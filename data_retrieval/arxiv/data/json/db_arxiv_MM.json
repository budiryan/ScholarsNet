[{
    "category": "cs.MM", 
    "author": "Francois Bar", 
    "title": "Open Access beyond cable: The case of Interactive TV", 
    "publish": "2001-09-20T23:48:11Z", 
    "summary": "In this paper we analyze the development of interactive TV in the U.S. and\nWestern Europe. We argue that despite the nascent character of the market there\nare important regulatory issues at stake, as exemplified by the AOL/TW merger\nand the British Interactive Broadcasting case. Absent rules that provide for\nnon-discriminatory access to network components (including terminal equipment\nspecifications), dominant platform operators are likely to leverage ownership\nof delivery infrastructure into market power over interactive TV services.\nWhile integration between platform operator, service provider and terminal\nvendor may facilitate the introduction of services in the short-term, the\nlasting result will be a collection of fragmented \"walled gardens\" offering\nlimited content and applications. Would interactive TV develop under such\nmodel, the exciting opportunities for broad-based innovation and extended\naccess to multiple information, entertainment and educational services opened\nby the new generation of broadcasting technologies will be foregone", 
    "link": "http://arxiv.org/pdf/cs/0109041v2", 
    "arxiv-id": "cs/0109041v2"
},{
    "category": "cs.MM", 
    "author": "Ph. D. Yuriy A. Chashkov", 
    "title": "Data Visualization on Shared Usage Multi-Screen Environment", 
    "publish": "2005-06-16T10:15:05Z", 
    "summary": "The modern multimedia technologies based on the whole palette of hardware and\nsoftware facilities of real-time high-speed information processing, in a\ncombination with effective facilities of the remote access to information\nresources, allow us to visualize diverse types of information. Data\nvisualization facilities &#8211; is the face of the Automated Control System on\nwhom often judge about their efficiency. They take a special place, providing\nvisualization of the diverse information necessary for decision-making by a\nfinal control link - the person allocated by certain powers.", 
    "link": "http://arxiv.org/pdf/cs/0506070v1", 
    "arxiv-id": "cs/0506070v1"
},{
    "category": "cs.MM", 
    "author": "M. S. Santhanam", 
    "title": "Digital watermarking in the singular vector domain", 
    "publish": "2006-03-31T19:36:03Z", 
    "summary": "Many current watermarking algorithms insert data in the spatial or transform\ndomains like the discrete cosine, the discrete Fourier, and the discrete\nwavelet transforms. In this paper, we present a data-hiding algorithm that\nexploits the singular value decomposition (SVD) representation of the data. We\ncompute the SVD of the host image and the watermark and embed the watermark in\nthe singular vectors of the host image. The proposed method leads to an\nimperceptible scheme for digital images, both in grey scale and color and is\nquite robust against attacks like noise and JPEG compression.", 
    "link": "http://arxiv.org/pdf/cs/0603130v1", 
    "arxiv-id": "cs/0603130v1"
},{
    "category": "cs.MM", 
    "author": "Denis Pellerin", 
    "title": "Un filtre temporel cr\u00e9dibiliste pour la reconnaissance d'actions   humaines dans les vid\u00e9os", 
    "publish": "2006-07-18T08:16:37Z", 
    "summary": "In the context of human action recognition in video sequences, a temporal\nbelief filter is presented. It allows to cope with human action disparity and\nlow quality videos. The whole system of action recognition is based on the\nTransferable Belief Model (TBM) proposed by P. Smets. The TBM allows to\nexplicitly model the doubt between actions. Furthermore, the TBM emphasizes the\nconflict which is exploited for action recognition. The filtering performance\nis assessed on real video sequences acquired by a moving camera and under\nseveral unknown view angles.", 
    "link": "http://arxiv.org/pdf/cs/0607087v1", 
    "arxiv-id": "cs/0607087v1"
},{
    "category": "cs.MM", 
    "author": "Baoyu Zheng", 
    "title": "A Fast Block Matching Algorithm for Video Motion Estimation Based on   Particle Swarm Optimization and Motion Prejudgment", 
    "publish": "2006-09-24T09:55:57Z", 
    "summary": "In this paper, we propose a fast 2-D block-based motion estimation algorithm\ncalled Particle Swarm Optimization - Zero-motion Prejudgment(PSO-ZMP) which\nconsists of three sequential routines: 1)Zero-motion prejudgment. The routine\naims at finding static macroblocks(MB) which do not need to perform remaining\nsearch thus reduces the computational cost; 2)Predictive image coding and 3)PSO\nmatching routine. Simulation results obtained show that the proposed PSO-ZMP\nalgorithm achieves over 10 times of computation less than Diamond Search(DS)\nand 5 times less than the recent proposed Adaptive Rood Pattern\nSearching(ARPS). Meanwhile the PSNR performances using PSO-ZMP are very close\nto that using DS and ARPS in some less-motioned sequences. While in some\nsequences containing dense and complex motion contents, the PSNR performances\nof PSO-ZMP are several dB lower than that using DS and ARPS but in an\nacceptable degree.", 
    "link": "http://arxiv.org/pdf/cs/0609131v1", 
    "arxiv-id": "cs/0609131v1"
},{
    "category": "cs.MM", 
    "author": "R. Sarmiento", 
    "title": "A High Quality/Low Computational Cost Technique for Block Matching   Motion Estimation", 
    "publish": "2007-10-25T12:03:15Z", 
    "summary": "Motion estimation is the most critical process in video coding systems. First\nof all, it has a definitive impact on the rate-distortion performance given by\nthe video encoder. Secondly, it is the most computationally intensive process\nwithin the encoding loop. For these reasons, the design of high-performance\nlow-cost motion estimators is a crucial task in the video compression field. An\nadaptive cost block matching (ACBM) motion estimation technique is presented in\nthis paper, featuring an excellent tradeoff between the quality of the\nreconstructed video sequences and the computational effort. Simulation results\ndemonstrate that the ACBM algorithm achieves a slight better rate-distortion\nperformance than the one given by the well-known full search algorithm block\nmatching algorithm with reductions of up to 95% in the computational load.", 
    "link": "http://arxiv.org/pdf/0710.4819v1", 
    "arxiv-id": "0710.4819v1"
},{
    "category": "cs.MM", 
    "author": "Wayne Wolf", 
    "title": "Multimedia Applications of Multiprocessor Systems-on-Chips", 
    "publish": "2007-10-25T12:04:15Z", 
    "summary": "This paper surveys the characteristics of multimedia systems. Multimedia\napplications today are dominated by compression and decompression, but\nmultimedia devices must also implement many other functions such as security\nand file management. We introduce some basic concepts of multimedia algorithms\nand the larger set of functions that multimedia systems-on-chips must\nimplement.", 
    "link": "http://arxiv.org/pdf/0710.4821v1", 
    "arxiv-id": "0710.4821v1"
},{
    "category": "cs.MM", 
    "author": "J. Lidon Simon", 
    "title": "A Coprocessor for Accelerating Visual Information Processing", 
    "publish": "2007-10-25T12:04:42Z", 
    "summary": "Visual information processing will play an increasingly important role in\nfuture electronics systems. In many applications, e.g. video surveillance\ncameras, data throughput of microprocessors is not sufficient and power\nconsumption is too high. Instruction profiling on a typical test algorithm has\nshown that pixel address calculations are the dominant operations to be\noptimized. Therefore AddressLib, a structured scheme for pixel addressing was\ndeveloped, that can be accelerated by AddressEngine, a coprocessor for visual\ninformation processing. In this paper, the architectural design of\nAddressEngine is described, which in the first step supports a subset of the\nAddressLib. Dataflow and memory organization are optimized during architectural\ndesign. AddressEngine was implemented in a FPGA and was tested with MPEG-7\nGlobal Motion Estimation algorithm. Results on processing speed and circuit\ncomplexity are given and compared to a pure software implementation. The next\nstep will be the support for the full AddressLib, including segment addressing.\nAn outlook on further investigations on dynamic reconfiguration capabilities is\ngiven.", 
    "link": "http://arxiv.org/pdf/0710.4823v1", 
    "arxiv-id": "0710.4823v1"
},{
    "category": "cs.MM", 
    "author": "Wojciech Mazurczyk", 
    "title": "SecMon: End-to-End Quality and Security Monitoring System", 
    "publish": "2008-04-01T10:46:26Z", 
    "summary": "The Voice over Internet Protocol (VoIP) is becoming a more available and\npopular way of communicating for Internet users. This also applies to\nPeer-to-Peer (P2P) systems and merging these two have already proven to be\nsuccessful (e.g. Skype). Even the existing standards of VoIP provide an\nassurance of security and Quality of Service (QoS), however, these features are\nusually optional and supported by limited number of implementations. As a\nresult, the lack of mandatory and widely applicable QoS and security guaranties\nmakes the contemporary VoIP systems vulnerable to attacks and network\ndisturbances. In this paper we are facing these issues and propose the SecMon\nsystem, which simultaneously provides a lightweight security mechanism and\nimproves quality parameters of the call. SecMon is intended specially for VoIP\nservice over P2P networks and its main advantage is that it provides\nauthentication, data integrity services, adaptive QoS and (D)DoS attack\ndetection. Moreover, the SecMon approach represents a low-bandwidth consumption\nsolution that is transparent to the users and possesses a self-organizing\ncapability. The above-mentioned features are accomplished mainly by utilizing\ntwo information hiding techniques: digital audio watermarking and network\nsteganography. These techniques are used to create covert channels that serve\nas transport channels for lightweight QoS measurement's results. Furthermore,\nthese metrics are aggregated in a reputation system that enables best route\npath selection in the P2P network. The reputation system helps also to mitigate\n(D)DoS attacks, maximize performance and increase transmission efficiency in\nthe network.", 
    "link": "http://arxiv.org/pdf/0804.0134v1", 
    "arxiv-id": "0804.0134v1"
},{
    "category": "cs.MM", 
    "author": "Prasanta K. Panigrahi", 
    "title": "A Reliable SVD based Watermarking Schem", 
    "publish": "2008-08-03T10:17:03Z", 
    "summary": "We propose a novel scheme for watermarking of digital images based on\nsingular value decomposition (SVD), which makes use of the fact that the SVD\nsubspace preserves significant amount of information of an image, as compared\nto its singular value matrix, Zhang and Li (2005). The principal components of\nthe watermark are embedded in the original image, leaving the detector with a\ncomplimentary set of singular vectors for watermark extraction. The above step\ninvariably ensures that watermark extraction from the embedded watermark image,\nusing a modified matrix, is not possible, thereby removing a major drawback of\nan earlier proposed algorithm by Liu and Tan (2002).", 
    "link": "http://arxiv.org/pdf/0808.0309v1", 
    "arxiv-id": "0808.0309v1"
},{
    "category": "cs.MM", 
    "author": "Bao Le Duc", 
    "title": "An Export Architecture for a Multimedia Authoring Environment", 
    "publish": "2008-09-30T09:56:35Z", 
    "summary": "In this paper, we propose an export architecture that provides a clear\nseparation of authoring services from publication services. We illustrate this\narchitecture with the LimSee3 authoring tool and several standard publication\nformats: Timesheets, SMIL, and XHTML.", 
    "link": "http://arxiv.org/pdf/0809.5154v1", 
    "arxiv-id": "0809.5154v1"
},{
    "category": "cs.MM", 
    "author": "Changjia Chen", 
    "title": "Initial Offset Placement in p2p Live Streaming Systems", 
    "publish": "2008-10-12T00:15:27Z", 
    "summary": "Initial offset placement in p2p streaming systems is studied in this paper.\nProportional placement (PP) scheme is proposed. In this scheme, peer places the\ninitial offset as the offset reported by other reference peer with a shift\nproportional to the buffer width or offset lag of this reference peer. This\nwill introduce a stable placement that supports larger buffer width for peers\nand small buffer width for tracker. Real deployed placement method in PPLive is\nstudied through measurement. It shows that, instead of based on offset lag, the\nplacement is based on buffer width of the reference peer to facilitate the\ninitial chunk fetching. We will prove that, such a PP scheme may not be stable\nunder arbitrary buffer occupation in the reference peer. The required average\nbuffer width then is derived. A simple good peer selection mechanism to check\nthe buffer occupation of reference peer is proposed for a stable PP scheme\nbased on buffer width", 
    "link": "http://arxiv.org/pdf/0810.2063v1", 
    "arxiv-id": "0810.2063v1"
},{
    "category": "cs.MM", 
    "author": "Charles A. B. Robert", 
    "title": "Characterization and collection of information from heterogeneous   multimedia sources with users' parameters for decision support", 
    "publish": "2008-11-12T20:10:00Z", 
    "summary": "No single information source can be good enough to satisfy the divergent and\ndynamic needs of users all the time. Integrating information from divergent\nsources can be a solution to deficiencies in information content. We present\nhow Information from multimedia document can be collected based on associating\na generic database to a federated database. Information collected in this way\nis brought into relevance by integrating the parameters of usage and user's\nparameter for decision making. We identified seven different classifications of\nmultimedia document.", 
    "link": "http://arxiv.org/pdf/0811.1959v1", 
    "arxiv-id": "0811.1959v1"
},{
    "category": "cs.MM", 
    "author": "Philippe Roose", 
    "title": "Kalinahia: Considering Quality of Service to Design and Execute   Distributed Multimedia Applications", 
    "publish": "2008-12-13T07:46:52Z", 
    "summary": "One of the current challenges of Information Systems is to ensure\nsemi-structured data transmission, such as multimedia data, in a distributed\nand pervasive environment. Information Sytems must then guarantee users a\nquality of service ensuring data accessibility whatever the hardware and\nnetwork conditions may be. They must also guarantee information coherence and\nparticularly intelligibility that imposes a personalization of the service.\nWithin this framework, we propose a design method based on original models of\nmultimedia applications and quality of service. We also define a supervision\nplatform Kalinahia using a user centered heuristic allowing us to define at any\nmoment which configuration of software components constitutes the best answers\nto users' wishes in terms of service.", 
    "link": "http://arxiv.org/pdf/0812.2529v1", 
    "arxiv-id": "0812.2529v1"
},{
    "category": "cs.MM", 
    "author": "Marc Dalmau", 
    "title": "The Korrontea Data Modeling", 
    "publish": "2008-12-16T07:46:49Z", 
    "summary": "Needs of multimedia systems evolved due to the evolution of their\narchitecture which is now distributed into heterogeneous contexts. A critical\nissue lies in the fact that they handle, process, and transmit multimedia data.\nThis data integrates several properties which should be considered since it\nholds a considerable part of its semantics, for instance the lips\nsynchronization in a video. In this paper, we focus on the definition of a\nmodel as a basic abstraction for describing and modeling media in multimedia\nsystems by taking into account their properties. This model will be used in\nsoftware architecture in order to handle data in efficient way. The provided\nmodel is an interesting solution for the integration of media into\napplications; we propose to consider and to handle them in a uniform way. This\nmodel is proposed with synchronization policies to ensure synchronous transport\nof media. Therefore, we use it in a component model that we develop for the\ndesign and deployment of distributed multimedia systems.", 
    "link": "http://arxiv.org/pdf/0812.2988v1", 
    "arxiv-id": "0812.2988v1"
},{
    "category": "cs.MM", 
    "author": "Marc Dalmau", 
    "title": "Heterogeneous component interactions: Sensors integration into   multimedia applications", 
    "publish": "2008-12-16T07:47:22Z", 
    "summary": "Resource-constrained embedded and mobile devices are becoming increasingly\ncommon. Since few years, some mobile and ubiquitous devices such as wireless\nsensor, able to be aware of their physical environment, appeared. Such devices\nenable proposing applications which adapt to user's need according the context\nevolution. It implies the collaboration of sensors and software components\nwhich differ on their nature and their communication mechanisms. This paper\nproposes a unified component model in order to easily design applications based\non software components and sensors without taking care of their nature. Then it\npresents a state of the art of communication problems linked to heterogeneous\ncomponents and proposes an interaction mechanism which ensures information\nexchanges between wireless sensors and software components.", 
    "link": "http://arxiv.org/pdf/0812.2989v1", 
    "arxiv-id": "0812.2989v1"
},{
    "category": "cs.MM", 
    "author": "Joerg Buehler", 
    "title": "Optimal Control of a Single Queue with Retransmissions: Delay-Dropping   Tradeoffs", 
    "publish": "2009-02-23T19:48:51Z", 
    "summary": "A single queue incorporating a retransmission protocol is investigated,\nassuming that the sequence of per effort success probabilities in the Automatic\nRetransmission reQuest (ARQ) chain is a priori defined and no channel state\ninformation at the transmitter is available. A Markov Decision Problem with an\naverage cost criterion is formulated where the possible actions are to either\ncontinue the retransmission process of an erroneous packet at the next time\nslot or to drop the packet and move on to the next packet awaiting for\ntransmission. The cost per slot is a linear combination of the current queue\nlength and a penalty term in case dropping is chosen as action. The\ninvestigation seeks policies that provide the best possible average packet\ndelay-dropping trade-off for Quality of Service guarantees. An optimal\ndeterministic stationary policy is shown to exist, several structural\nproperties of which are obtained. Based on that, a class of suboptimal\n<L,K>-policies is introduced. These suggest that it is almost optimal to use a\nK-truncated ARQ protocol as long as the queue length is lower than L, else send\nall packets in one shot. The work concludes with an evaluation of the optimal\ndelay-dropping tradeoff using dynamic programming and a comparison between the\noptimal and suboptimal policies.", 
    "link": "http://arxiv.org/pdf/0902.3979v1", 
    "arxiv-id": "0902.3979v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "A Systematic Framework for Dynamically Optimizing Multi-User Wireless   Video Transmission", 
    "publish": "2009-03-02T04:49:27Z", 
    "summary": "In this paper, we formulate the collaborative multi-user wireless video\ntransmission problem as a multi-user Markov decision process (MUMDP) by\nexplicitly considering the users' heterogeneous video traffic characteristics,\ntime-varying network conditions and the resulting dynamic coupling between the\nwireless users. These environment dynamics are often ignored in existing\nmulti-user video transmission solutions. To comply with the decentralized\nnature of wireless networks, we propose to decompose the MUMDP into local MDPs\nusing Lagrangian relaxation. Unlike in conventional multi-user video\ntransmission solutions stemming from the network utility maximization\nframework, the proposed decomposition enables each wireless user to\nindividually solve its own dynamic cross-layer optimization (i.e. the local\nMDP) and the network coordinator to update the Lagrangian multipliers (i.e.\nresource prices) based on not only current, but also future resource needs of\nall users, such that the long-term video quality of all users is maximized.\nHowever, solving the MUMDP requires statistical knowledge of the experienced\nenvironment dynamics, which is often unavailable before transmission time. To\novercome this obstacle, we then propose a novel online learning algorithm,\nwhich allows the wireless users to update their policies in multiple states\nduring one time slot. This is different from conventional learning solutions,\nwhich often update one state per time slot. The proposed learning algorithm can\nsignificantly improve the learning performance, thereby dramatically improving\nthe video quality experienced by the wireless users over time. Our simulation\nresults demonstrate the efficiency of the proposed MUMDP framework as compared\nto conventional multi-user video transmission solutions.", 
    "link": "http://arxiv.org/pdf/0903.0207v1", 
    "arxiv-id": "0903.0207v1"
},{
    "category": "cs.MM", 
    "author": "Antonio Ortega", 
    "title": "A Novel Approach for Compression of Images Captured using Bayer Color   Filter Arrays", 
    "publish": "2009-03-13T07:11:31Z", 
    "summary": "We propose a new approach for image compression in digital cameras, where the\ngoal is to achieve better quality at a given rate by using the characteristics\nof a Bayer color filter array. Most digital cameras produce color images by\nusing a single CCD plate, so that each pixel in an image has only one color\ncomponent and therefore an interpolation method is needed to produce a full\ncolor image. After the image processing stage, in order to reduce the memory\nrequirements of the camera, a lossless or lossy compression stage often\nfollows. But in this scheme, before decreasing redundancy through compression,\nredundancy is increased in an interpolation stage. In order to avoid increasing\nthe redundancy before compression, we propose algorithms for image compression\nin which the order of the compression and interpolation stages is reversed. We\nintroduce image transform algorithms, since non interpolated images cannot be\ndirectly compressed with general image coders. The simulation results show that\nour algorithm outperforms conventional methods with various color interpolation\nmethods in a wide range of compression ratios. Our proposed algorithm provides\nnot only better quality but also lower encoding complexity because the amount\nof luminance data used is only half of that in conventional methods.", 
    "link": "http://arxiv.org/pdf/0903.2272v1", 
    "arxiv-id": "0903.2272v1"
},{
    "category": "cs.MM", 
    "author": "Dorina Fera", 
    "title": "Virtual Reality", 
    "publish": "2009-03-25T12:16:29Z", 
    "summary": "This paper is focused on the presentation of Virtual Reality principles\ntogether with the main implementation methods and techniques. An overview of\nthe main development directions is included.", 
    "link": "http://arxiv.org/pdf/0903.4314v1", 
    "arxiv-id": "0903.4314v1"
},{
    "category": "cs.MM", 
    "author": "Dieter Penteliuc Cotosman", 
    "title": "The Multimedia Product - between Design and Information, Design and   Utility and Design and Entertainment", 
    "publish": "2009-04-23T13:50:36Z", 
    "summary": "The paper investigates the possible coherent and effective alternatives to\nsolve the problems related to the communication needs of any multimedia\nproduct. In essence, the presentation will focus on identifying the issues and\nprinciples governing three types of the design - in fact, the multimedia design\nin a broader sense - namely the information design - precisely aiming at ways\nof organization and presentation of information in a useful and significant\nform, the graphical user interface design, whose sub-domain consists of the\ninformation displayed on the monitor screen and of interactivity between user,\ncomputer and electronic devices, meaning, in fact, everything the user sees,\ntouches, hears and all the elements with which he interacts, the graphic\ndesign, whose main concern is to create an aesthetic layout arrangement (from\nthe visual and perceptive) information.", 
    "link": "http://arxiv.org/pdf/0904.3693v1", 
    "arxiv-id": "0904.3693v1"
},{
    "category": "cs.MM", 
    "author": "Dieter Penteliuc-Cotosman", 
    "title": "The new multimedia educational technologies, used in open and distance   learning", 
    "publish": "2009-04-23T13:57:23Z", 
    "summary": "This paper reviews and refers to the latest telematics technology that has\nturned the open system learning and helped it to become an institutional\nalternative to the face-to-face traditional one. Most technologies, briefly\npresented here, will be implemented in the \"ARTeFACt\" project - telematic\nsystem for vocational education system of open system learning, system which\nwill be officially launched at the end of 2006, in the institutional offer of\nthe Faculty of Arts of the University West of Timisoara. The scientific\ncoordination of the doctoral project \"ARTeFACt\" is done by Mr. Prof. Dr. Eng.\nSavi G. George, representing the Department of Mechatronics Faculty of\nMechanical Engineering from the University \"Politehnica\" of Timisoara, Romania", 
    "link": "http://arxiv.org/pdf/0904.3694v1", 
    "arxiv-id": "0904.3694v1"
},{
    "category": "cs.MM", 
    "author": "Miodrag Stoianovici", 
    "title": "Development and Optimization of a Multimedia Product", 
    "publish": "2009-05-26T14:06:05Z", 
    "summary": "This article presents a new concept of a multimedia interactive product. It\nis a multiuser versatile platform that can be used for different purposes. The\nfirst implementation of the platform is a multiplayer game called Texas Hold\n'em, which is a very popular community card game. The paper shows the product's\nmultimedia structure where Hardware and Software work together in creating a\nrealistic feeling for the users.", 
    "link": "http://arxiv.org/pdf/0905.4205v1", 
    "arxiv-id": "0905.4205v1"
},{
    "category": "cs.MM", 
    "author": "Adela Ionescu", 
    "title": "Web Publishing of the Files Obtained by Flash", 
    "publish": "2009-06-04T09:52:22Z", 
    "summary": "The aim of this article is to familiarize the user with the Web publishing of\nthe files obtained by Flash. The article contains an overview of Macromedia\nFlash 5, as well as the running of a Playing Flash movie, information on Flash\nand Generator, the publishing of Flash movies, a HTLM publishing for Flash\nPlayer files and publishing by Generator templates.", 
    "link": "http://arxiv.org/pdf/0906.0866v1", 
    "arxiv-id": "0906.0866v1"
},{
    "category": "cs.MM", 
    "author": "A. V. Sutagundar", 
    "title": "A Bandwidth Characterization Tool For MPEG-2 File", 
    "publish": "2009-06-25T05:40:46Z", 
    "summary": "This paper proposes the design and development of MPEG 2 Video Decoder to\noffer flexible and effective utilization of bandwidth services. The decoder is\ncapable of decoding the MPEG 2 bit stream on a single host machine. The present\ndecoder is designed to be simple, but yet effectively reconstruct the video\nfrom MPEG 2 bit stream.", 
    "link": "http://arxiv.org/pdf/0906.4607v1", 
    "arxiv-id": "0906.4607v1"
},{
    "category": "cs.MM", 
    "author": "P. Seethalakshmi", 
    "title": "TTSS Packet Classification Algorithm to enhance Multimedia Applications   in Network Processor based Router", 
    "publish": "2009-06-27T11:59:15Z", 
    "summary": "The objective of this paper is to implement the Trie based Tuple Space\nSearch(TTSS) packet classification algorithm for Network Processor(NP) based\nrouter to enhance multimedia applications. The performance is evaluated using\nIntel IXP2400 NP Simulator. The results demonstrate that, TTSS has better\nperformance than Tuple Space Search algorithm and is well suited to achieve\nhigh speed packet classification to support multimedia applications.", 
    "link": "http://arxiv.org/pdf/0906.5073v1", 
    "arxiv-id": "0906.5073v1"
},{
    "category": "cs.MM", 
    "author": "Anupam Shukla", 
    "title": "Enhanced Mode Selection Algorithm for H.264 encoder for Application in   Low Computational power devices", 
    "publish": "2009-09-01T18:57:06Z", 
    "summary": "The intent of the H.264 AVC project was to create a standard capable of\nproviding good video quality at substantially lower bit rates than previous\nstandards without increasing the complexity of design so much that it would be\nimpractical or excessively expensive to implement. An additional goal was to\nprovide enough flexibility to allow the standard to be applied to a wide\nvariety of applications. To achieve better coding efficiency, H.264 AVC uses\nseveral techniques such as inter mode and intra mode prediction with variable\nsize motion compensation, which adopts Rate Distortion Optimization (RDO). This\nincreases the computational complexity of the encoder especially for devices\nwith lower processing capabilities such as mobile and other handheld devices.\nIn this paper, we propose an algorithm to reduce the number of mode and sub\nmode evaluations in inter mode prediction. Experimental results show that this\nfast intra mode selection algorithm can lessen about 75 percent encoding time\nwith little loss of bit rate and visual quality.", 
    "link": "http://arxiv.org/pdf/0909.0245v1", 
    "arxiv-id": "0909.0245v1"
},{
    "category": "cs.MM", 
    "author": "Christian Feldbauer", 
    "title": "Efficient Quality-Based Playout Buffer Algorithm", 
    "publish": "2009-09-15T14:44:21Z", 
    "summary": "Playout buffers are used in VoIP systems to compensate for network delay\njitter by making a trade-off between delay and loss. In this work we propose a\nplayout buffer algorithm that makes the trade-off based on maximization of\nconversational speech quality, aiming to keep the computational complexity\nlowest possible. We model the network delay using a Pareto distribution and\nshow that it is a good compromise between providing an appropriate fit to the\nnetwork delay characteristics and yielding a low arithmetical complexity. We\nuse the ITU-T E-Model as the quality model and simplify its delay impairment\nfunction. The proposed playout buffer algorithm finds the optimum playout delay\nusing a closed-form solution that minimizes the sum of the simplified delay\nimpairment factor and the loss-dependent equipment impairment factor of the\nE-model. The simulation results show that our proposed algorithm outperforms\nexisting state-of-the-art algorithms with a reduced complexity for a\nquality-based algorithm.", 
    "link": "http://arxiv.org/pdf/0909.2816v1", 
    "arxiv-id": "0909.2816v1"
},{
    "category": "cs.MM", 
    "author": "T. R. GopalaKrishnan Nair", 
    "title": "Prefetching of VoD Programs Based On ART1 Requesting Clustering", 
    "publish": "2009-10-08T11:21:21Z", 
    "summary": "In this paper, we propose a novel approach to group users according to the\nVoD user request pattern. We cluster the user requests based on ART1 neural\nnetwork algorithm. The knowledge extracted from the cluster is used to prefetch\nthe multimedia object from each cluster before the users request. We have\ndeveloped an algorithm to cluster users according to the users request patterns\nbased on ART1 neural network algorithm that offers an unsupervised clustering.\nThis approach adapts to changes in user request patterns over period without\nlosing previous information. Each cluster is represented as prototype vector by\ngeneralizing the most frequently used URLs that are accessed by all the cluster\nmembers. The simulation results of our proposed clustering and prefetching\nalgorithm, shows enormous increase in the performance of streaming server. Our\nalgorithm helps the servers agent to learn user preferences and discover the\ninformation about the corresponding sources and other similar interested\nindividuals.", 
    "link": "http://arxiv.org/pdf/0910.1468v1", 
    "arxiv-id": "0910.1468v1"
},{
    "category": "cs.MM", 
    "author": "E. Ibnelhaj", 
    "title": "A Wavelet-Based Digital Watermarking for Video", 
    "publish": "2009-11-02T20:07:52Z", 
    "summary": "A novel video watermarking system operating in the three dimensional wavelet\ntransform is here presented. Specifically the video sequence is partitioned\ninto spatio temporal units and the single shots are projected onto the 3D\nwavelet domain. First a grayscale watermark image is decomposed into a series\nof bitplanes that are preprocessed with a random location matrix. After that\nthe preprocessed bitplanes are adaptively spread spectrum and added in 3D\nwavelet coefficients of the video shot. Our video watermarking algorithm is\nrobust against the attacks of frame dropping, averaging and swapping.\nFurthermore, it allows blind retrieval of embedded watermark which does not\nneed the original video and the watermark is perceptually invisible. The\nalgorithm design, evaluation, and experimentation of the proposed scheme are\ndescribed in this paper.", 
    "link": "http://arxiv.org/pdf/0911.0399v1", 
    "arxiv-id": "0911.0399v1"
},{
    "category": "cs.MM", 
    "author": "Francis Rousseaux", 
    "title": "How Do Interactive Virtual Operas Shift Relationships between Music,   Text and Image?", 
    "publish": "2009-12-24T15:28:33Z", 
    "summary": "In this paper we present the new genre of interactive operas implemented on\npersonal computers. They differ from traditional ones not only because they are\nvirtual, but mainly because they offer to composers and listeners new\nperspectives of combinations and interactions between music, text and visual\naspects.", 
    "link": "http://arxiv.org/pdf/0912.4880v1", 
    "arxiv-id": "0912.4880v1"
},{
    "category": "cs.MM", 
    "author": "Balakrishnan Ramadoss", 
    "title": "Modeling and Annotating the Expressive Semantics of Dance Videos", 
    "publish": "2010-01-04T05:41:01Z", 
    "summary": "Dance videos are interesting and semantics-intensive. At the same time, they\nare the complex type of videos compared to all other types such as sports, news\nand movie videos. In fact, dance video is the one which is less explored by the\nresearchers across the globe. Dance videos exhibit rich semantics such as macro\nfeatures and micro features and can be classified into several types. Hence,\nthe conceptual modeling of the expressive semantics of the dance videos is very\ncrucial and complex. This paper presents a generic Dance Video Semantics Model\n(DVSM) in order to represent the semantics of the dance videos at different\ngranularity levels, identified by the components of the accompanying song. This\nmodel incorporates both syntactic and semantic features of the videos and\nintroduces a new entity type called, Agent, to specify the micro features of\nthe dance videos. The instantiations of the model are expressed as graphs. The\nmodel is implemented as a tool using J2SE and JMF to annotate the macro and\nmicro features of the dance videos. Finally examples and evaluation results are\nprovided to depict the effectiveness of the proposed dance video model.\nKeywords: Agents, Dance videos, Macro features, Micro features, Video\nannotation, Video semantics.", 
    "link": "http://arxiv.org/pdf/1001.0442v1", 
    "arxiv-id": "1001.0442v1"
},{
    "category": "cs.MM", 
    "author": "Christian Guetl", 
    "title": "Discovering Knowledge from Multi-modal Lecture Recordings", 
    "publish": "2010-01-04T05:44:57Z", 
    "summary": "Educational media mining is the process of converting raw media data from\neducational systems to useful information that can be used to design learning\nsystems, answer research questions and allow personalized learning experiences.\nKnowledge discovery encompasses a wide range of techniques ranging from\ndatabase queries to more recent developments in machine learning and language\ntechnology. Educational media mining techniques are now being used in IT\nServices research worldwide. Multi-modal Lecture Recordings is one of the\nimportant types of educational media and this paper explores the research\nchallenges for mining lecture recordings for the efficient personalized\nlearning experiences. Keywords: Educational Media Mining; Lecture Recordings,\nMultimodal Information System, Personalized Learning; Online Course Ware;\nSkills and Competences;", 
    "link": "http://arxiv.org/pdf/1001.0443v1", 
    "arxiv-id": "1001.0443v1"
},{
    "category": "cs.MM", 
    "author": "T. R. Gopalakrishnan Nair", 
    "title": "Multicast Transmission Prefix and Popularity Aware Interval Caching   Based Admission Control Policy", 
    "publish": "2010-01-21T08:57:27Z", 
    "summary": "Admission control is a key component in multimedia servers, which will allow\nthe resources to be used by the client only when they are available. A problem\nfaced by numerous content serving machines is overload, when there are too many\nclients who need to be served, the server tends to slow down. An admission\ncontrol algorithm for a multimedia server is responsible for determining if a\nnew request can be accepted without violating the QoS requirements of the\nexisting requests in the system. By caching and streaming only the data in the\ninterval between two successive requests on the same object, the following\nrequest can be serviced directly from the buffer cache without disk operations\nand within the deadline of the request. An admission control strategy based on\nPopularity-aware interval caching for Prefix [3] scheme extends the interval\ncaching by considering different popularity of multimedia objects. The method\nof Prefix caching with multicast transmission of popular objects utilizes the\nhard disk and network bandwidth efficiently and increases the number of\nrequests being served.", 
    "link": "http://arxiv.org/pdf/1001.3744v1", 
    "arxiv-id": "1001.3744v1"
},{
    "category": "cs.MM", 
    "author": "T. R. Gopalakrishan Nair", 
    "title": "Cooperative Proxy Servers Architecture for VoD to Achieve High QoS with   Reduced Transmission Time and Cost", 
    "publish": "2010-01-21T11:08:42Z", 
    "summary": "- The aim of this paper is to propose a novel Voice On Demand (VoD)\narchitecture and implementation of an efficient load sharing algorithm to\nachieve Quality of Service (QoS). This scheme reduces the transmission cost\nfrom the Centralized Multimedia Sever (CMS) to Proxy Servers (PS) by sharing\nthe videos among the proxy servers of the Local Proxy Servers Group [LPSG] and\namong the neighboring LPSGs, which are interconnected in a ring fashion. This\nresults in very low request rejection ratio, reduction in transmission time and\ncost, reduction of load on the CMS and high QoS for the users. Simulation\nresults indicate acceptable initial startup latency, reduced transmission cost\nand time, load sharing among the proxy servers, among the LPSGs and between the\nCMS and the PS.", 
    "link": "http://arxiv.org/pdf/1001.3774v1", 
    "arxiv-id": "1001.3774v1"
},{
    "category": "cs.MM", 
    "author": "T. R. Gopalakrishnan Nair", 
    "title": "An Adaptive Dynamic Replacement Approach for a Multicast based   Popularity Aware Prefix Cache Memory System", 
    "publish": "2010-01-23T07:20:30Z", 
    "summary": "In this paper we have proposed an adaptive dynamic cache replacement\nalgorithm for a multimedia servers cache system. The goal is to achieve an\neffective utilization of the cache memory which stores the prefix of popular\nvideos. A replacement policy is usually evaluated using hit ratio, the\nfrequency with which any video is requested. Usually discarding the least\nrecently used page is the policy of choice in cache management. The adaptive\ndynamic replacement approach for prefix cache is a self tuning, low overhead\nalgorithm that responds online to changing access patterns. It constantly\nbalances between lru and lfu to improve combined result. It automatically\nadapts to evolving workloads. Since in our algorithm we have considered a\nprefix caching with multicast transmission of popular objects it utilizes the\nhard disk and network bandwidth efficiently and increases the number of\nrequests being served.", 
    "link": "http://arxiv.org/pdf/1001.4135v1", 
    "arxiv-id": "1001.4135v1"
},{
    "category": "cs.MM", 
    "author": "P. Jayarekha", 
    "title": "A Strategy to enable Prefix of Multicast VoD through dynamic buffer   allocation", 
    "publish": "2010-02-05T09:27:01Z", 
    "summary": "In this paper we have proposed a dynamic buffer allocation algorithm for the\nprefix, based on the popularity of the videos. More cache blocks are allocated\nfor most popular videos and a few cache blocks are allocated for less popular\nvideos. Buffer utilization is also maximized irrespective of the load on the\nVideo-on-Demand system. Overload can lead the server getting slowed down. By\nstoring the first few seconds of popular video clips, a multimedia local server\ncan shield the users from the delay, throughput, and loss properties of the\npath between the local server and the central server. The key idea of\ncontrolled multicast is used to allow clients to share a segment of a video\nstream even when the requests arrive at different times. This dynamic buffer\nallocation algorithm is simulated and its performance is evaluated based on the\nbuffer utilization by multimedia servers and average buffer allocation for the\nmost popular videos. Our simulation results shows efficient utilization of\nnetwork bandwidth and reduced hard disk utilization hence resulting in increase\nin the number of requests being served.", 
    "link": "http://arxiv.org/pdf/1002.1166v1", 
    "arxiv-id": "1002.1166v1"
},{
    "category": "cs.MM", 
    "author": "A. Ahaitouf", 
    "title": "Shape-Adaptive Motion Estimation Algorithm for MPEG-4 Video Coding", 
    "publish": "2010-02-05T09:31:58Z", 
    "summary": "This paper presents a gradient based motion estimation algorithm based on\nshape-motion prediction, which takes advantage of the correlation between\nneighboring Binary Alpha Blocks (BABs), to match with the Mpeg-4 shape coding\ncase and speed up the estimation process. The PSNR and computation time\nachieved by the proposed algorithm seem to be better than those obtained by\nmost popular motion estimation techniques.", 
    "link": "http://arxiv.org/pdf/1002.1168v1", 
    "arxiv-id": "1002.1168v1"
},{
    "category": "cs.MM", 
    "author": "M. Dakshayini", 
    "title": "Stochastic Model Based Proxy Servers Architecture for VoD to Achieve   Reduced Client Waiting Time", 
    "publish": "2010-02-05T10:54:05Z", 
    "summary": "In a video on demand system, the main video repository may be far away from\nthe user and generally has limited streaming capacities. Since a high quality\nvideo's size is huge, it requires high bandwidth for streaming over the\ninternet. In order to achieve a higher video hit ratio, reduced client waiting\ntime, distributed server's architecture can be used, in which multiple local\nservers are placed close to clients and, based on their regional demands video\ncontents are cached dynamically from the main server. As the cost of proxy\nserver is decreasing and demand for reduced waiting time is increasing day by\nday, newer architectures are explored, innovative schemes are arrived at. In\nthis paper we present novel 3 layer architecture, includes main multimedia\nserver, a Tracker and Proxy servers. This architecture targets to optimize the\nclient waiting time. We also propose an efficient prefix caching and load\nsharing algorithm at the proxy server to allocate the cache according to\nregional popularity of the video. The simulation results demonstrate that it\nachieves significantly lower client's waiting time, when compared to the other\nexisting algorithms.", 
    "link": "http://arxiv.org/pdf/1002.1195v1", 
    "arxiv-id": "1002.1195v1"
},{
    "category": "cs.MM", 
    "author": "Kanwalvir Singh Dhindsa", 
    "title": "Effect of Embedding Watermark on Compression of the Digital Images", 
    "publish": "2010-02-21T18:29:53Z", 
    "summary": "Image Compression plays a very important role in image processing especially\nwhen we are to send the image on the internet. The threat to the information on\nthe internet increases and image is no exception. Generally the image is sent\non the internet as the compressed image to optimally use the bandwidth of the\nnetwork. But as we are on the network, at any intermediate level the image can\nbe changed intentionally or unintentionally. To make sure that the correct\nimage is being delivered at the other end we embed the water mark to the image.\nThe watermarked image is then compressed and sent on the network. When the\nimage is decompressed at the other end we can extract the watermark and make\nsure that the image is the same that was sent by the other end. Though\nwatermarking the image increases the size of the uncompressed image but that\nhas to done to achieve the high degree of robustness i.e. how an image sustains\nthe attacks on it. The present paper is an attempt to make transmission of the\nimages secure from the intermediate attacks by applying the generally used\ncompression transforms.", 
    "link": "http://arxiv.org/pdf/1002.3984v1", 
    "arxiv-id": "1002.3984v1"
},{
    "category": "cs.MM", 
    "author": "T R GopalaKrishnan Nair", 
    "title": "An Optimal Prefix Replication Strategy for VoD Services", 
    "publish": "2010-03-22T03:31:50Z", 
    "summary": "In this paper we propose scalable proxy servers cluster architecture of\ninterconnected proxy servers for high quality and high availability services.\nWe also propose an optimal regional popularity based video prefix replication\nstrategy and a scene change based replica caching algorithm that utilizes the\nzipf-like video popularity distribution to maximize the availability of videos\ncloser to the client and request-servicing rate thereby reducing the client\nrejection ratio and the response time for the client. The simulation results of\nour proposed architecture and algorithm show the greater achievement in\nmaximizing the availability of videos, client request-servicing rate and in\nreduction of initial start-up latency and client rejection ratio.", 
    "link": "http://arxiv.org/pdf/1003.4049v1", 
    "arxiv-id": "1003.4049v1"
},{
    "category": "cs.MM", 
    "author": "I. Elamvazuthi", 
    "title": "Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient   (MFCC) and Dynamic Time Warping (DTW) Techniques", 
    "publish": "2010-03-22T06:39:55Z", 
    "summary": "Digital processing of speech signal and voice recognition algorithm is very\nimportant for fast and accurate automatic voice recognition technology. The\nvoice is a signal of infinite information. A direct analysis and synthesizing\nthe complex voice signal is due to too much information contained in the\nsignal. Therefore the digital signal processes such as Feature Extraction and\nFeature Matching are introduced to represent the voice signal. Several methods\nsuch as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM),\nArtificial Neural Network (ANN) and etc are evaluated with a view to identify a\nstraight forward and effective method for voice signal. The extraction and\nmatching process is implemented right after the Pre Processing or filtering\nsignal is performed. The non-parametric method for modelling the human auditory\nperception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as\nextraction techniques. The non linear sequence alignment known as Dynamic Time\nWarping (DTW) introduced by Sakoe Chiba has been used as features matching\ntechniques. Since it's obvious that the voice signal tends to have different\ntemporal rate, the alignment is important to produce the better\nperformance.This paper present the viability of MFCC to extract features and\nDTW to compare the test patterns.", 
    "link": "http://arxiv.org/pdf/1003.4083v1", 
    "arxiv-id": "1003.4083v1"
},{
    "category": "cs.MM", 
    "author": "Jintao Li", 
    "title": "Context-Oriented Web Video Tag Recommendation", 
    "publish": "2010-03-24T13:07:08Z", 
    "summary": "Tag recommendation is a common way to enrich the textual annotation of\nmultimedia contents. However, state-of-the-art recommendation methods are built\nupon the pair-wised tag relevance, which hardly capture the context of the web\nvideo, i.e., when who are doing what at where. In this paper we propose the\ncontext-oriented tag recommendation (CtextR) approach, which expands tags for\nweb videos under the context-consistent constraint. Given a web video, CtextR\nfirst collects the multi-form WWW resources describing the same event with the\nvideo, which produce an informative and consistent context; and then, the tag\nrecommendation is conducted based on the obtained context. Experiments on an\n80,031 web video collection show CtextR recommends various relevant tags to web\nvideos. Moreover, the enriched tags improve the performance of web video\ncategorization.", 
    "link": "http://arxiv.org/pdf/1003.4637v1", 
    "arxiv-id": "1003.4637v1"
},{
    "category": "cs.MM", 
    "author": "K. L. Shunmuganathan", 
    "title": "A reversible high embedding capacity data hiding technique for hiding   secret data in images", 
    "publish": "2010-04-10T03:42:06Z", 
    "summary": "As the multimedia and internet technologies are growing fast, the\ntransmission of digital media plays an important role in communication. The\nvarious digital media like audio, video and images are being transferred\nthrough internet. There are a lot of threats for the digital data that are\ntransferred through internet. Also, a number of security techniques have been\nemployed to protect the data that is transferred through internet. This paper\nproposes a new technique for sending secret messages securely, using\nsteganographic technique. Since the proposed system uses multiple level of\nsecurity for data hiding, where the data is hidden in an image file and the\nstego file is again concealed in another image. Previously, the secret message\nis being encrypted with the encryption algorithm which ensures the achievement\nof high security enabled data transfer through internet.", 
    "link": "http://arxiv.org/pdf/1004.1676v1", 
    "arxiv-id": "1004.1676v1"
},{
    "category": "cs.MM", 
    "author": "V. Sumathy", 
    "title": "Design And Implementation Of Multilevel Access Control In Medical Image   Transmission Using Symmetric Polynomial Based Audio Steganography", 
    "publish": "2010-04-10T04:29:01Z", 
    "summary": "...The steganography scheme makes it possible to hide the medical image in\ndifferent bit locations of host media without inviting suspicion. The Secret\nfile is embedded in a cover media with a key. At the receiving end the key can\nbe derived by all the classes which are higher in the hierarchy using symmetric\npolynomial and the medical image file can be retrieved. The system is\nimplemented and found to be secure, fast and scalable. Simulation results show\nthat the system is dynamic in nature and allows any type of hierarchy. The\nproposed approach performs better even during frequent member joins and leaves.\nThe computation cost is reduced as the same algorithm is used for key\ncomputation and descendant key derivation. Steganographic technique used in\nthis paper does not use the conventional LSB's and uses two bit positions and\nthe hidden data occurs only from a frame which is dictated by the key that is\nused. Hence the quality of stego data is improved.", 
    "link": "http://arxiv.org/pdf/1004.1682v1", 
    "arxiv-id": "1004.1682v1"
},{
    "category": "cs.MM", 
    "author": "R Manthalkar", 
    "title": "Review of Robust Video Watermarking Algorithms", 
    "publish": "2010-04-11T08:07:40Z", 
    "summary": "There has been a remarkable increase in the data exchange over web and the\nwidespread use of digital media. As a result, multimedia data transfers also\nhad a boost up. The mounting interest with reference to digital watermarking\nthroughout the last decade is certainly due to the increase in the need of\ncopyright protection of digital content. This is also enhanced due to\ncommercial prospective. Applications of video watermarking in copy control,\nbroadcast monitoring, fingerprinting, video authentication, copyright\nprotection etc is immensely rising. The main aspects of information hiding are\ncapacity, security and robustness. Capacity deals with the amount of\ninformation that can be hidden. The skill of anyone detecting the information\nis security and robustness refers to the resistance to modification of the\ncover content before concealed information is destroyed. Video watermarking\nalgorithms normally prefers robustness. In a robust algorithm it is not\npossible to eliminate the watermark without rigorous degradation of the cover\ncontent. In this paper, we introduce the notion of Video Watermarking and the\nfeatures required to design a robust watermarked video for a valuable\napplication. We review several algorithms, and introduce frequently used key\ntechniques. The aim of this paper is to focus on the various domains of video\nwatermarking techniques. The majority of the reviewed methods based on video\nwatermarking emphasize on the notion of robustness of the algorithm.", 
    "link": "http://arxiv.org/pdf/1004.1770v1", 
    "arxiv-id": "1004.1770v1"
},{
    "category": "cs.MM", 
    "author": "L. Ganesan", 
    "title": "Reversible Image data Hiding using Lifting wavelet Transform and   Histogram Shifting", 
    "publish": "2010-04-11T11:11:59Z", 
    "summary": "A method of lossless data hiding in images using integer wavelet transform\nand histogram shifting for gray scale images is proposed. The method shifts\npart of the histogram, to create space for embedding the watermark information\nbits. The method embeds watermark while maintaining the visual quality well.\nThe method is completely reversible. The original image and the watermark data\ncan be recovered without any loss.", 
    "link": "http://arxiv.org/pdf/1004.1791v1", 
    "arxiv-id": "1004.1791v1"
},{
    "category": "cs.MM", 
    "author": "Jigar Shah", 
    "title": "C Implementation & comparison of companding & silence audio compression   techniques", 
    "publish": "2010-04-19T18:26:09Z", 
    "summary": "Just about all the newest living room audio-video electronics and PC\nmultimedia products being designed today will incorporate some form of\ncompressed digitized-audio processing capability. Audio compression reduces the\nbit rate required to represent an analog audio signal while maintaining the\nperceived audio quality. Discarding inaudible data reduces the storage,\ntransmission and compute requirements of handling high-quality audio files.\nThis paper covers wave audio file format & algorithm of silence compression\nmethod and companding method to compress and decompress wave audio file. Then\nit compares the result of these two methods.", 
    "link": "http://arxiv.org/pdf/1004.3275v1", 
    "arxiv-id": "1004.3275v1"
},{
    "category": "cs.MM", 
    "author": "Mohsen Gerami", 
    "title": "Policies and Economics of Digital Multimedia Transmission", 
    "publish": "2010-04-20T20:14:19Z", 
    "summary": "There are different Standards of digital multimedia transmission, for example\nDVB in Europe and ISDB in Japan and DMB in Korea, with different delivery\nsystem (example MPEG-2, MPEG-4).This paper describe an overview of Digital\nMultimedia Transmission (DMT) technologies. The economic aspects of digital\ncontent & software solution industry as a strategic key in the future will be\ndiscussed. The study then focuses on some important policy and technology\nissues, such S-DMB, T-DMB, Digital Video Broadcasting Handheld (DVB-H) and\nconcludes DMT policies for convergence of telecommunications and broadcasting.", 
    "link": "http://arxiv.org/pdf/1004.3556v1", 
    "arxiv-id": "1004.3556v1"
},{
    "category": "cs.MM", 
    "author": "Alireza Nasiri Avanaki", 
    "title": "Error Concealment in Image Communication Using Edge Map Watermarking and   Spatial Smoothing", 
    "publish": "2010-04-24T00:29:00Z", 
    "summary": "We propose a novel error concealment algorithm to be used at the receiver\nside of a lossy image transmission system. Our algorithm involves hiding the\nedge map of the original image at the transmitter within itself using a robust\nwatermarking scheme. At the receiver, wherever a lost block is detected, the\nextracted edge information is used as border constraint for the spatial\nsmoothing employing the intact neighboring blocks in order to conceal errors.\nSimulation results show the superiority of our technique over existing methods\neven in case of high packet loss ratios in the communication network.", 
    "link": "http://arxiv.org/pdf/1004.4241v1", 
    "arxiv-id": "1004.4241v1"
},{
    "category": "cs.MM", 
    "author": "Yap Teck Ann", 
    "title": "Combination of Subtractive Clustering and Radial Basis Function in   Speaker Identification", 
    "publish": "2010-04-26T09:57:23Z", 
    "summary": "Speaker identification is the process of determining which registered speaker\nprovides a given utterance. Speaker identification required to make a claim on\nthe identity of speaker from the Ns trained speaker in its user database. In\nthis study, we propose the combination of clustering algorithm and the\nclassification technique - subtractive and Radial Basis Function (RBF). The\nproposed technique is chosen because RBF is a simpler network structures and\nfaster learning algorithm. RBF finds the input to output map using the local\napproximators which will combine the linear of the approximators and cause the\nlinear combiner have few weights. Besides that, RBF neural network model using\nsubtractive clustering algorithm for selecting the hidden node centers, which\ncan achieve faster training speed. In the meantime, the RBF network was trained\nwith a regularization term so as to minimize the variances of the nodes in the\nhidden layer and perform more accu-rate prediction.", 
    "link": "http://arxiv.org/pdf/1004.4457v1", 
    "arxiv-id": "1004.4457v1"
},{
    "category": "cs.MM", 
    "author": "S. G. Bhirud", 
    "title": "Visual Infrared Video Fusion for Night Vision using Background   Estimation", 
    "publish": "2010-04-26T10:01:53Z", 
    "summary": "Video fusion is a process that combines visual data from different sensors to\nobtain a single composite video preserving the information of the sources. The\navailability of a system, enhancing human ability to perceive the observed\nscenario, is crucial to improve the performance of a surveillance system. The\ninfrared (IR) camera captures thermal image of object in night-time\nenvironment, when only limited visual information can be captured by RGB\ncamera. The fusion of data recorded by an IR sensor and a visible RGB camera\ncan produce information otherwise not obtainable by viewing the sensor outputs\nseparately. In this paper we consider the problem of fusing two video streams\nacquired by an RGB camera and an IR sensor. The pedestrians, distinctly\ncaptured by IR video, are separated and fused with the RGB video. The\nalgorithms implemented involve estimation of the background, followed by\ndetection of object from the IR Video, after necessary denoising. Finally a\nsuitable fusion algorithm is employed to combine the extracted pedestrians with\nthe visual output. The obtained results clearly demonstrate the effectiveness\nof the proposed video fusion scheme, for night vision.", 
    "link": "http://arxiv.org/pdf/1004.4459v1", 
    "arxiv-id": "1004.4459v1"
},{
    "category": "cs.MM", 
    "author": "Elbey Bourennane", 
    "title": "Towards Hardware implementation of video applications in new   telecommunications devices", 
    "publish": "2010-04-26T19:14:07Z", 
    "summary": "Among the areas, most demanding in terms of calculation is the\ntelecommunication and video applications are now included in several\ntelecommunication devices such as set-top boxes, mobile phones. Embedded videos\napplications in new generations of telecommunication devices need a processing\ncapacity that can not be achieved by the conventional processor, to work around\nthis problem the use of programmable technology has a lot of interest. First,\nField Programmable Gate Arrays (FPGAs) present many performance benefits for\nreal-time image processing applications. The FPGA structure is able to exploit\nspatial and temporal parallelism. In this paper, we present a new method for\nimplementation of the Color Structure Descriptor (CSD) using the FPGA circuit.\nIn fact the (CSD) provides satisfactory image indexing and retrieval results\namong all colorbased descriptors in MPEG-7. But the real time implementation of\nthis descriptor is still having problems. In this paper we propose a method for\nadapting this descriptor for possible implementation under the constraints of\nthe video processing in real time. We have verified the real-time\nimplementation of the (CSD) with an image size of 120*80 pixels.", 
    "link": "http://arxiv.org/pdf/1005.0771v1", 
    "arxiv-id": "1005.0771v1"
},{
    "category": "cs.MM", 
    "author": "Toufik Ahmed", 
    "title": "Architecture for Cooperative Prefetching in P2P Video-on- Demand System", 
    "publish": "2010-05-11T08:42:18Z", 
    "summary": "Most P2P VoD schemes focused on service architectures and overlays\noptimization without considering segments rarity and the performance of\nprefetching strategies. As a result, they cannot better support VCRoriented\nservice in heterogeneous environment having clients using free VCR controls.\nDespite the remarkable popularity in VoD systems, there exist no prior work\nthat studies the performance gap between different prefetching strategies. In\nthis paper, we analyze and understand the performance of different prefetching\nstrategies. Our analytical characterization brings us not only a better\nunderstanding of several fundamental tradeoffs in prefetching strategies, but\nalso important insights on the design of P2P VoD system. On the basis of this\nanalysis, we finally proposed a cooperative prefetching strategy called\n\"cooching\". In this strategy, the requested segments in VCR interactivities are\nprefetched into session beforehand using the information collected through\ngossips. We evaluate our strategy through extensive simulations. The results\nindicate that the proposed strategy outperforms the existing prefetching\nmechanisms.", 
    "link": "http://arxiv.org/pdf/1005.1757v1", 
    "arxiv-id": "1005.1757v1"
},{
    "category": "cs.MM", 
    "author": "Pang Yee Yong", 
    "title": "A Study on Potential of Integrating Multimodal Interaction into Musical   Conducting Education", 
    "publish": "2010-05-21T17:18:29Z", 
    "summary": "With the rapid development of computer technology, computer music has begun\nto appear in the laboratory. Many potential utility of computer music is\ngradually increasing. The purpose of this paper is attempted to analyze the\npossibility of integrating multimodal interaction such as vision-based hand\ngesture and speech interaction into musical conducting education. To achieve\nthis purpose, this paper is focus on discuss some related research and the\ntraditional musical conducting education. To do so, six musical conductors had\nbeen interviewed to share their musical conducting learning/ teaching\nexperience. These interviews had been analyzed in this paper to show the\nsyllabus and the focus of musical conducting education for beginners.", 
    "link": "http://arxiv.org/pdf/1005.4014v1", 
    "arxiv-id": "1005.4014v1"
},{
    "category": "cs.MM", 
    "author": "T. R. Gopala Krishnan Nair", 
    "title": "Client-to-Client Streaming Scheme for VOD Applications", 
    "publish": "2010-05-29T08:00:39Z", 
    "summary": "In this paper, we propose an efficient client-to-client streaming approach to\ncooperatively stream the video using chaining technique with unicast\ncommunication among the clients. This approach considers two major issues of\nVoD 1) Prefix caching scheme to accommodate more number of videos closer to\nclient, so that the request-service delay for the user can be minimized. 2)\nCooperative proxy and client chaining scheme for streaming the videos using\nunicasting. This approach minimizes the client rejection rate and bandwidth\nrequirement on server to proxy and proxy to client path. Our simulation results\nshow that the proposed approach achieves reduced client waiting time and\noptimal prefix caching of videos minimizing server to proxy path bandwidth\nusage by utilizing the client to client bandwidth, which is occasionally used\nwhen compared to busy server to proxy path bandwidth.", 
    "link": "http://arxiv.org/pdf/1005.5436v1", 
    "arxiv-id": "1005.5436v1"
},{
    "category": "cs.MM", 
    "author": "Murtaza Ali Khan", 
    "title": "An Automated Algorithm for Approximation of Temporal Video Data Using   Linear B'EZIER Fitting", 
    "publish": "2010-05-31T08:11:59Z", 
    "summary": "This paper presents an efficient method for approximation of temporal video\ndata using linear Bezier fitting. For a given sequence of frames, the proposed\nmethod estimates the intensity variations of each pixel in temporal dimension\nusing linear Bezier fitting in Euclidean space. Fitting of each segment ensures\nupper bound of specified mean squared error. Break and fit criteria is employed\nto minimize the number of segments required to fit the data. The proposed\nmethod is well suitable for lossy compression of temporal video data and\nautomates the fitting process of each pixel. Experimental results show that the\nproposed method yields good results both in terms of objective and subjective\nquality measurement parameters without causing any blocking artifacts.", 
    "link": "http://arxiv.org/pdf/1005.5613v1", 
    "arxiv-id": "1005.5613v1"
},{
    "category": "cs.MM", 
    "author": "Jintao Li", 
    "title": "Web Video Categorization based on Wikipedia Categories and   Content-Duplicated Open Resources", 
    "publish": "2010-08-04T12:01:39Z", 
    "summary": "This paper presents a novel approach for web video categorization by\nleveraging Wikipedia categories (WikiCs) and open resources describing the same\ncontent as the video, i.e., content-duplicated open resources (CDORs). Note\nthat current approaches only col-lect CDORs within one or a few media forms and\nignore CDORs of other forms. We explore all these resources by utilizing WikiCs\nand commercial search engines. Given a web video, its discrimin-ative Wikipedia\nconcepts are first identified and classified. Then a textual query is\nconstructed and from which CDORs are collected. Based on these CDORs, we\npropose to categorize web videos in the space spanned by WikiCs rather than\nthat spanned by raw tags. Experimental results demonstrate the effectiveness of\nboth the proposed CDOR collection method and the WikiC voting catego-rization\nalgorithm. In addition, the categorization model built based on both WikiCs and\nCDORs achieves better performance compared with the models built based on only\none of them as well as state-of-the-art approach.", 
    "link": "http://arxiv.org/pdf/1008.0757v1", 
    "arxiv-id": "1008.0757v1"
},{
    "category": "cs.MM", 
    "author": "S. Athinarayanan", 
    "title": "A Block Based Scheme for Enhancing Low Luminated Images", 
    "publish": "2010-09-08T08:23:47Z", 
    "summary": "In this paper the background detection in images in poor lighting can be done\nby the use of morphological filters. Lately contrast image enhancement\ntechnique is used to detect the background in image which uses Weber's Law. The\nproposed technique is more effective one in which the background detection in\nimage can be done in color images. The given image obtained in this method is\nvery effective one. More enhancement can be obtained while comparing the\nresults. In this technique compressed domain enhancement has been used for\nbetter result.", 
    "link": "http://arxiv.org/pdf/1009.1478v1", 
    "arxiv-id": "1009.1478v1"
},{
    "category": "cs.MM", 
    "author": "F. Marvasti", 
    "title": "Improved Iterative Techniques to Compensate for Interpolation   Distortions", 
    "publish": "2010-09-20T12:15:46Z", 
    "summary": "In this paper a novel hybrid approach for compensating the distortion of any\ninterpolation has been proposed. In this hybrid method, a modular approach was\nincorporated in an iterative fashion. By using this approach we can get drastic\nimprovement with less computational complexity. The extension of the proposed\napproach to two dimensions was also studied. Both the simulation results and\nmathematical analyses confirmed the superiority of the hybrid method. The\nproposed method was also shown to be robust against additive noise.", 
    "link": "http://arxiv.org/pdf/1009.3785v1", 
    "arxiv-id": "1009.3785v1"
},{
    "category": "cs.MM", 
    "author": "Mohd. Dilshad Ansari", 
    "title": "Transmitting Video-on-Demand Effectively", 
    "publish": "2010-10-12T16:13:21Z", 
    "summary": "Now-a-days internet has become a vast source of entertainment & new services\nare available in quick succession which provides entertainment to the users.\nOne of this service i.e. Video-on-Demand is most hyped service in this context.\nTransferring the video over the network with less error is the main objective\nof the service providers. In this paper we present an algorithm for routing the\nvideo to the user in an effective manner along with a method that ensures less\nerror rate than others.", 
    "link": "http://arxiv.org/pdf/1010.2432v1", 
    "arxiv-id": "1010.2432v1"
},{
    "category": "cs.MM", 
    "author": "John Bosco Balaguru Rayappan", 
    "title": "Colour Guided Colour Image Steganography", 
    "publish": "2010-10-19T19:07:08Z", 
    "summary": "Information security has become a cause of concern because of the electronic\neavesdropping. Capacity, robustness and invisibility are important parameters\nin information hiding and are quite difficult to achieve in a single algorithm.\nThis paper proposes a novel steganography technique for digital color image\nwhich achieves the purported targets. The professed methodology employs a\ncomplete random scheme for pixel selection and embedding of data. Of the three\ncolour channels (Red, Green, Blue) in a given colour image, the least two\nsignificant bits of any one of the channels of the color image is used to\nchannelize the embedding capacity of the remaining two channels. We have\ndevised three approaches to achieve various levels of our desired targets. In\nthe first approach, Red is the default guide but it results in localization of\nMSE in the remaining two channels, which makes it slightly vulnerable. In the\nsecond approach, user gets the liberty to select the guiding channel (Red,\nGreen or Blue) to guide the remaining two channels. It will increase the\nrobustness and imperceptibility of the embedded image however the MSE factor\nwill still remain as a drawback. The third approach improves the performance\nfactor as a cyclic methodology is employed and the guiding channel is selected\nin a cyclic fashion. This ensures the uniform distribution of MSE, which gives\nbetter robustness and imperceptibility along with enhanced embedding capacity.\nThe imperceptibility has been enhanced by suitably adapting optimal pixel\nadjustment process (OPAP) on the stego covers.", 
    "link": "http://arxiv.org/pdf/1010.4007v1", 
    "arxiv-id": "1010.4007v1"
},{
    "category": "cs.MM", 
    "author": "Koichi Harada", 
    "title": "Haar Wavelet Based Approach for Image Compression and Quality Assessment   of Compressed Image", 
    "publish": "2010-10-20T01:52:05Z", 
    "summary": "With the increasing growth of technology and the entrance into the digital\nage, we have to handle a vast amount of information every time which often\npresents difficulties. So, the digital information must be stored and retrieved\nin an efficient and effective manner, in order for it to be put to practical\nuse. Wavelets provide a mathematical way of encoding information in such a way\nthat it is layered according to level of detail. This layering facilitates\napproximations at various intermediate stages. These approximations can be\nstored using a lot less space than the original data. Here a low complex 2D\nimage compression method using wavelets as the basis functions and the approach\nto measure the quality of the compressed image are presented. The particular\nwavelet chosen and used here is the simplest wavelet form namely the Haar\nWavelet. The 2D discret wavelet transform (DWT) has been applied and the detail\nmatrices from the information matrix of the image have been estimated. The\nreconstructed image is synthesized using the estimated detail matrices and\ninformation matrix provided by the Wavelet transform. The quality of the\ncompressed images has been evaluated using some factors like Compression Ratio\n(CR), Peak Signal to Noise Ratio (PSNR), Mean Opinion Score (MOS), Picture\nQuality Scale (PQS) etc.", 
    "link": "http://arxiv.org/pdf/1010.4084v1", 
    "arxiv-id": "1010.4084v1"
},{
    "category": "cs.MM", 
    "author": "Farokh Marvasti", 
    "title": "Compensating Interpolation Distortion by New Optimized Modular Method", 
    "publish": "2010-11-11T20:20:36Z", 
    "summary": "A modular method was suggested before to recover a band limited signal from\nthe sample and hold and linearly interpolated (or, in general, an\nnth-order-hold) version of the regular samples. In this paper a novel approach\nfor compensating the distortion of any interpolation based on modular method\nhas been proposed. In this method the performance of the modular method is\noptimized by adding only some simply calculated coefficients. This approach\ncauses drastic improvement in terms of SNRs with fewer modules compared to the\nclassical modular method. Simulation results clearly confirm the improvement of\nthe proposed method and also its superior robustness against additive noise.", 
    "link": "http://arxiv.org/pdf/1011.2753v1", 
    "arxiv-id": "1011.2753v1"
},{
    "category": "cs.MM", 
    "author": "F. Marvasti", 
    "title": "Image Inpainting Using Sparsity of the Transform Domain", 
    "publish": "2010-11-24T18:58:53Z", 
    "summary": "In this paper, we propose a new image inpainting method based on the property\nthat much of the image information in the transform domain is sparse. We add a\nredundancy to the original image by mapping the transform coefficients with\nsmall amplitudes to zero and the resultant sparsity pattern is used as the side\ninformation in the recovery stage. If the side information is not available,\nthe receiver has to estimate the sparsity pattern. At the end, the recovery is\ndone by consecutive projecting between two spatial and transform sets.\nExperimental results show that our method works well for both structural and\ntexture images and outperforms other techniques in objective and subjective\nperformance measures.", 
    "link": "http://arxiv.org/pdf/1011.5458v1", 
    "arxiv-id": "1011.5458v1"
},{
    "category": "cs.MM", 
    "author": "Farrokh Marvasti", 
    "title": "A proposed Optimized Spline Interpolation", 
    "publish": "2010-12-02T13:02:06Z", 
    "summary": "The goal of this paper is to design compact support basis spline functions\nthat best approximate a given filter (e.g., an ideal Lowpass filter). The\noptimum function is found by minimizing the least square problem ($\\ell$2 norm\nof the difference between the desired and the approximated filters) by means of\nthe calculus of variation; more precisely, the introduced splines give optimal\nfiltering properties with respect to their time support interval. Both\nmathematical analysis and simulation results confirm the superiority of these\nsplines.", 
    "link": "http://arxiv.org/pdf/1012.0397v2", 
    "arxiv-id": "1012.0397v2"
},{
    "category": "cs.MM", 
    "author": "Ulrike Sattler", 
    "title": "Evaluating Modelling Approaches for Medical Image Annotations", 
    "publish": "2010-12-08T21:45:16Z", 
    "summary": "Information system designers face many challenges w.r.t. selecting\nappropriate semantic technologies and deciding on a modelling approach for\ntheir system. However, there is no clear methodology yet to evaluate\n\"semantically enriched\" information systems. In this paper we present a case\nstudy on different modelling approaches for annotating medical images and\nintroduce a conceptual framework that can be used to analyse the fitness of\ninformation systems and help designers to spot the strengths and weaknesses of\nvarious modelling approaches as well as managing trade-offs between modelling\neffort and their potential benefits.", 
    "link": "http://arxiv.org/pdf/1012.1882v1", 
    "arxiv-id": "1012.1882v1"
},{
    "category": "cs.MM", 
    "author": "K. Venugopalan", 
    "title": "Digital watermarking : An approach based on Hilbert transform", 
    "publish": "2010-12-14T08:50:29Z", 
    "summary": "Most of the well known algorithms for watermarking of digital images involve\ntransformation of the image data to Fourier or singular vector space. In this\npaper, we introduce watermarking in Hilbert transform domain for digital media.\nGenerally, if the image is a matrix of order $m$ by $n$, then the transformed\nspace is also an image of the same order. However, with Hilbert transforms, the\ntransformed space is of order $2m$ by $2n$. This allows for more latitude in\nstoring the watermark in the host image. Based on this idea, we propose an\nalgorithm for embedding and extracting watermark in a host image and\nanalytically obtain a parameter related to this procedure. Using extensive\nsimulations, we show that the algorithm performs well even if the host image is\ncorrupted by various attacks.", 
    "link": "http://arxiv.org/pdf/1012.2965v1", 
    "arxiv-id": "1012.2965v1"
},{
    "category": "cs.MM", 
    "author": "Imon Mukherjee", 
    "title": "Image Sterilization to Prevent LSB-based Steganographic Transmission", 
    "publish": "2010-12-27T07:44:21Z", 
    "summary": "Sterilization is a very popular word used in biomedical testing (like removal\nof all microorganisms on surface of an article or in fluid using appropriate\nchemical products). Motivated by this biological analogy, we, for the first\ntime, introduce the concept of sterilization of an image, i.e., removing any\nsteganographic information embedded in the image. Experimental results show\nthat our technique succeeded in sterilizing around 76% to 91% of stego pixels\nin an image on average, where data is embedded using LSB-based steganography.", 
    "link": "http://arxiv.org/pdf/1012.5573v1", 
    "arxiv-id": "1012.5573v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Transmitting important bits and sailing high radio waves: a   decentralized cross-layer approach to cooperative video transmission", 
    "publish": "2011-02-26T19:11:27Z", 
    "summary": "We investigate the impact of cooperative relaying on uplink and downlink\nmulti-user (MU) wireless video transmissions. The objective is to maximize the\nlong-term sum of utilities across the video terminals in a decentralized\nfashion, by jointly optimizing the packet scheduling, the resource allocation,\nand the cooperation decisions, under the assumption that some nodes are willing\nto act as cooperative relays. A pricing-based distributed resource allocation\nframework is adopted, where the price reflects the expected future congestion\nin the network. Specifically, we formulate the wireless video transmission\nproblem as an MU Markov decision process (MDP) that explicitly considers the\ncooperation at the physical layer and the medium access control sublayer, the\nvideo users' heterogeneous traffic characteristics, the dynamically varying\nnetwork conditions, and the coupling among the users' transmission strategies\nacross time due to the shared wireless resource. Although MDPs notoriously\nsuffer from the curse of dimensionality, our study shows that, with appropriate\nsimplications and approximations, the complexity of the MU-MDP can be\nsignificantly mitigated. Our simulation results demonstrate that integrating\ncooperative decisions into the MU-MDP optimization can increase the resource\nprice in networks that only support low transmission rates and can decrease the\nprice in networks that support high transmission rates. Additionally, our\nresults show that cooperation allows users with feeble direct signals to\nachieve improvements in video quality on the order of 5-10 dB peak\nsignal-to-noise ratio (PSNR), with less than 0.8 dB quality loss by users with\nstrong direct signals, and with a moderate increase in total network energy\nconsumption that is significantly less than the energy that a distant node\nwould require to achieve an equivalent PSNR without exploiting cooperative\ndiversity.", 
    "link": "http://arxiv.org/pdf/1102.5437v2", 
    "arxiv-id": "1102.5437v2"
},{
    "category": "cs.MM", 
    "author": "Teri Brandenburg", 
    "title": "Multimedia Database Applications: Issues and Concerns for Classroom   Teaching", 
    "publish": "2011-02-28T20:47:08Z", 
    "summary": "The abundance of multimedia data and information is challenging educators to\neffectively search, browse, access, use, and store the data for their classroom\nteaching. However, many educators could still be accustomed to teaching or\nsearching for information using conventional methods, but often the\nconventional methods may not function well with multimedia data. Educators need\nto efficiently interact and manage a variety of digital media files too. The\npurpose of this study is to review current multimedia database applications in\nteaching and learning, and further discuss some of the issues or concerns that\neducators may have while incorporating multimedia data into their classrooms.\nSome strategies and recommendations are also provided in order for educators to\nbe able to use multimedia data more effectively in their teaching environments.", 
    "link": "http://arxiv.org/pdf/1102.5769v1", 
    "arxiv-id": "1102.5769v1"
},{
    "category": "cs.MM", 
    "author": "Dana Fusco", 
    "title": "Interdisciplinary Collaboration through Designing 3D Simulation Case   Studies", 
    "publish": "2011-03-01T02:02:54Z", 
    "summary": "Interdisciplinary collaboration is essential for the advance of research. As\ndomain subjects become more and more specialized, researchers need to cross\ndisciplines for insights from peers in other areas to have a broader and deeper\nunderstand of a topic at micro- and macro-levels. We developed a 3D virtual\nlearning environment that served as a platform for faculty to plan curriculum,\nshare educational beliefs, and conduct cross-discipline research for effective\nlearning. Based upon the scripts designed by faculty from five disciplines,\nvirtual doctors, nurses, or patients interact in a 3D virtual hospital. The\nteaching vignettes were then converted to video clips, allowing users to view,\npause, replay, or comment on the videos individually or in groups. Unlike many\nexisting platforms, we anticipated a value-added by adding a social networking\ncapacity to this virtual environment. The focus of this paper is on the\ncost-efficiency and system design of the virtual learning environment.", 
    "link": "http://arxiv.org/pdf/1103.0065v1", 
    "arxiv-id": "1103.0065v1"
},{
    "category": "cs.MM", 
    "author": "Sukumar Nandi", 
    "title": "Stage Staffing Scheme for Copyright Protection in Multimedia", 
    "publish": "2011-03-19T18:43:30Z", 
    "summary": "Copyright protection has become a need in today's world. To achieve a secure\ncopyright protection we embedded some information in images and videos and that\nimage or video is called copyright protected. The embedded information can't be\ndetected by human eye but some attacks and operations can tamper that\ninformation to breach protection. So in order to find a secure technique of\ncopyright protection, we have analyzed image processing techniques i.e. Spatial\nDomain (Least Significant Bit (LSB)), Transform Domain (Discrete Cosine\nTransform (DCT)), Discrete Wavelet Transform (DWT) and there are numerous\nalgorithm for watermarking using them. After having a good understanding of the\nsame we have proposed a novel algorithm named as Stage Staffing Algorithm that\ngenerates results with high effectiveness, additionally we can use self\nextracted-watermark technique to increase the security and automate the process\nof watermark image. The proposed algorithm provides protection in three stages.\nWe have implemented the algorithm and results of the simulations are shown. The\nvarious factors affecting spatial domain watermarking are also discussed.", 
    "link": "http://arxiv.org/pdf/1103.3802v1", 
    "arxiv-id": "1103.3802v1"
},{
    "category": "cs.MM", 
    "author": "Dr. P. G. Krishna Mohan", 
    "title": "Distributed Video Coding: Codec Architecture and Implementation", 
    "publish": "2011-03-24T09:50:26Z", 
    "summary": "Distributed Video Coding (DVC) is a new coding paradigm for video\ncompression, based on Slepian- Wolf (lossless coding) and Wyner-Ziv (lossy\ncoding) information theoretic results. DVC is useful for emerging applications\nsuch as wireless video cameras, wireless low-power surveillance networks and\ndisposable video cameras for medical applications etc. The primary objective of\nDVC is low-complexity video encoding, where bulk of computation is shifted to\nthe decoder, as opposed to low-complexity decoder in conventional video\ncompression standards such as H.264 and MPEG etc. There are couple of early\narchitectures and implementations of DVC from Stanford University[2][3] in\n2002, Berkeley University PRISM (Power-efficient, Robust, hIgh-compression,\nSyndrome-based Multimedia coding)[4][5] in 2002 and European project DISCOVER\n(DIStributed COding for Video SERvices)[6] in 2007. Primarily there are two\ntypes of DVC techniques namely pixel domain and transform domain based.\nTransform domain design will have better rate-distortion (RD) performance as it\nexploits spatial correlation between neighbouring samples and compacts the\nblock energy into as few transform coefficients as possible (aka energy\ncompaction). In this paper, architecture, implementation details and \"C\" model\nresults of our transform domain DVC are presented.", 
    "link": "http://arxiv.org/pdf/1103.4712v1", 
    "arxiv-id": "1103.4712v1"
},{
    "category": "cs.MM", 
    "author": "P. S. Hiremath", 
    "title": "SLDs for Visualizing Multicolor Elevation Contour Lines in Geo-Spatial   Web Applications", 
    "publish": "2011-04-05T11:06:30Z", 
    "summary": "This paper addresses the need for geospatial consumers (either humans or\nmachines) to visualize multicolored elevation contour poly lines with respect\ntheir different contour intervals and control the visual portrayal of the data\nwith which they work. The current OpenGIS Web Map Service (WMS) specification\nsupports the ability for an information provider to specify very basic styling\noptions by advertising a preset collection of visual portrayals for each\navailable data set. However, while a WMS currently can provide the user with a\nchoice of style options, the WMS can only tell the user the name of each style.\nIt cannot tell the user what portrayal will look like on the map. More\nimportantly, the user has no way of defining their own styling rules. The\nability for a human or machine client to define these rules requires a styling\nlanguage that the client and server can both understand. Defining this\nlanguage, called the StyledLayerDescriptor (SLD), is the main focus of this\npaper, and it can be used to portray the output of Web Map Servers, Web Feature\nServers and Web Coverage Servers.", 
    "link": "http://arxiv.org/pdf/1104.0809v1", 
    "arxiv-id": "1104.0809v1"
},{
    "category": "cs.MM", 
    "author": "Farrokh Marvasti", 
    "title": "Optimized Spline Interpolation", 
    "publish": "2011-04-29T20:05:52Z", 
    "summary": "In this paper, we investigate the problem of designing compact support\ninterpolation kernels for a given class of signals. By using calculus of\nvariations, we simplify the optimization problem from an infinite nonlinear\nproblem to a finite dimensional linear case, and then find the optimum compact\nsupport function that best approximates a given filter in the least square\nsense (l2 norm). The benefit of compact support interpolants is the low\ncomputational complexity in the interpolation process while the optimum compact\nsupport interpolant gaurantees the highest achivable Signal to Noise Ratio\n(SNR). Our simulation results confirm the superior performance of the proposed\nsplines compared to other conventional compact support interpolants such as\ncubic spline.", 
    "link": "http://arxiv.org/pdf/1105.0011v1", 
    "arxiv-id": "1105.0011v1"
},{
    "category": "cs.MM", 
    "author": "Lu Lu", 
    "title": "Survey of Cognitive Radio Techniques in Wireless Network", 
    "publish": "2011-04-29T21:34:21Z", 
    "summary": "In this report, I surveyed the cognitive radio technique in wireless\nnetworks. Researched several kinds of cognitive techniques about their\nadvantages and disadvantages.", 
    "link": "http://arxiv.org/pdf/1105.0023v1", 
    "arxiv-id": "1105.0023v1"
},{
    "category": "cs.MM", 
    "author": "Ali Reza Manashty", 
    "title": "Robust Sign Language Recognition System Using ToF Depth Cameras", 
    "publish": "2011-05-03T21:57:18Z", 
    "summary": "Sign language recognition is a difficult task, yet required for many\napplications in real-time speed. Using RGB cameras for recognition of sign\nlanguages is not very successful in practical situations and accurate 3D\nimaging requires expensive and complex instruments. With introduction of\nTime-of-Flight (ToF) depth cameras in recent years, it has become easier to\nscan the environment for accurate, yet fast depth images of the objects without\nthe need of any extra calibrating object. In this paper, a robust system for\nsign language recognition using ToF depth cameras is presented for converting\nthe recorded signs to a standard and portable XML sign language named SiGML for\neasy transferring and converting to real-time 3D virtual characters animations.\nFeature extraction using moments and classification using nearest neighbor\nclassifier are used to track hand gestures and significant result of 100% is\nachieved for the proposed approach.", 
    "link": "http://arxiv.org/pdf/1105.0699v1", 
    "arxiv-id": "1105.0699v1"
},{
    "category": "cs.MM", 
    "author": "Kai Xie", 
    "title": "Efficient Image Transmission Through Analog Error Correction", 
    "publish": "2011-05-09T00:07:56Z", 
    "summary": "This paper presents a new paradigm for image transmission through analog\nerror correction codes. Conventional schemes rely on digitizing images through\nquantization (which inevitably causes significant bandwidth expansion) and\ntransmitting binary bit-streams through digital error correction codes (which\ndo not automatically differentiate the different levels of significance among\nthe bits). To strike a better overall performance in terms of transmission\nefficiency and quality, we propose to use a single analog error correction code\nin lieu of digital quantization, digital code and digital modulation. The key\nis to get analog coding right. We show that this can be achieved by cleverly\nexploiting an elegant \"butterfly\" property of chaotic systems. Specifically, we\ndemonstrate a tail-biting triple-branch baker's map code and its\nmaximum-likelihood decoding algorithm. Simulations show that the proposed\nanalog code can actually outperform digital turbo code, one of the best codes\nknown to date. The results and findings discussed in this paper speak volume\nfor the promising potential of analog codes, in spite of their rather short\nhistory.", 
    "link": "http://arxiv.org/pdf/1105.1561v2", 
    "arxiv-id": "1105.1561v2"
},{
    "category": "cs.MM", 
    "author": "Najva Izadpanah", 
    "title": "Analytical Classification of Multimedia Index Structures by Using a   Partitioning Method-Based Framework", 
    "publish": "2011-05-10T13:55:31Z", 
    "summary": "Due to the advances in hardware technology and increase in production of\nmultimedia data in many applications, during the last decades, multimedia\ndatabases have become increasingly important. Contentbased multimedia retrieval\nis one of an important research area in the field of multimedia databases. Lots\nof research on this field has led to proposition of different kinds of index\nstructures to support fast and efficient similarity search to retrieve\nmultimedia data from these databases. Due to variety and plenty of proposed\nindex structures, we suggest a systematic framework based on partitioning\nmethod used in these structures to classify multimedia index structures, and\nthen we evaluated these structures based on important functional measures. We\nhope this proposed framework will lead to empirical and technical comparison of\nmultimedia index structures and development of more efficient structures at\nfuture.", 
    "link": "http://arxiv.org/pdf/1105.1948v1", 
    "arxiv-id": "1105.1948v1"
},{
    "category": "cs.MM", 
    "author": "Gert Lanckriet", 
    "title": "Learning content similarity for music recommendation", 
    "publish": "2011-05-12T00:43:46Z", 
    "summary": "Many tasks in music information retrieval, such as recommendation, and\nplaylist generation for online radio, fall naturally into the query-by-example\nsetting, wherein a user queries the system by providing a song, and the system\nresponds with a list of relevant or similar song recommendations. Such\napplications ultimately depend on the notion of similarity between items to\nproduce high-quality results. Current state-of-the-art systems employ\ncollaborative filter methods to represent musical items, effectively comparing\nitems in terms of their constituent users. While collaborative filter\ntechniques perform well when historical data is available for each item, their\nreliance on historical data impedes performance on novel or unpopular items. To\ncombat this problem, practitioners rely on content-based similarity, which\nnaturally extends to novel items, but is typically out-performed by\ncollaborative filter methods.\n  In this article, we propose a method for optimizing contentbased similarity\nby learning from a sample of collaborative filter data. The optimized\ncontent-based similarity metric can then be applied to answer queries on novel\nand unpopular items, while still maintaining high recommendation accuracy. The\nproposed system yields accurate and efficient representations of audio content,\nand experimental results show significant improvements in accuracy over\ncompeting content-based recommendation techniques.", 
    "link": "http://arxiv.org/pdf/1105.2344v1", 
    "arxiv-id": "1105.2344v1"
},{
    "category": "cs.MM", 
    "author": "Farokh Marvasti", 
    "title": "Fast restoration of natural images corrupted by high-density impulse   noise", 
    "publish": "2011-05-14T13:57:35Z", 
    "summary": "In this paper, we suggest a general model for the fixed-valued impulse noise\nand propose a two-stage method for high density noise suppression while\npreserving the image details. In the first stage, we apply an iterative impulse\ndetector, exploiting the image entropy, to identify the corrupted pixels and\nthen employ an Adaptive Iterative Mean filter to restore them. The filter is\nadaptive in terms of the number of iterations, which is different for each\nnoisy pixel, according to the Euclidean distance from the nearest uncorrupted\npixel. Experimental results show that the proposed filter is fast and\noutperforms the best existing techniques in both objective and subjective\nperformance measures.", 
    "link": "http://arxiv.org/pdf/1105.2899v2", 
    "arxiv-id": "1105.2899v2"
},{
    "category": "cs.MM", 
    "author": "Julien Pinquier", 
    "title": "Activities of Daily Living Indexing by Hierarchical HMM for Dementia   Diagnostics", 
    "publish": "2011-06-22T14:01:51Z", 
    "summary": "This paper presents a method for indexing human ac- tivities in videos\ncaptured from a wearable camera being worn by patients, for studies of\nprogression of the dementia diseases. Our method aims to produce indexes to\nfacilitate the navigation throughout the individual video recordings, which\ncould help doctors search for early signs of the dis- ease in the activities of\ndaily living. The recorded videos have strong motion and sharp lighting\nchanges, inducing noise for the analysis. The proposed approach is based on a\ntwo steps analysis. First, we propose a new approach to segment this type of\nvideo, based on apparent motion. Each segment is characterized by two original\nmotion de- scriptors, as well as color, and audio descriptors. Second, a\nHidden-Markov Model formulation is used to merge the multimodal audio and video\nfeatures, and classify the test segments. Experiments show the good properties\nof the ap- proach on real data.", 
    "link": "http://arxiv.org/pdf/1106.4451v1", 
    "arxiv-id": "1106.4451v1"
},{
    "category": "cs.MM", 
    "author": "J. Li", 
    "title": "Celerity: A Low-Delay Multi-Party Conferencing Solution", 
    "publish": "2011-07-06T14:14:41Z", 
    "summary": "In this paper, we attempt to revisit the problem of multi-party conferencing\nfrom a practical perspective, and to rethink the design space involved in this\nproblem. We believe that an emphasis on low end-to-end delays between any two\nparties in the conference is a must, and the source sending rate in a session\nshould adapt to bandwidth availability and congestion. We present Celerity, a\nmulti-party conferencing solution specifically designed to achieve our\nobjectives. It is entirely Peer-to-Peer (P2P), and as such eliminating the cost\nof maintaining centrally administered servers. It is designed to deliver video\nwith low end-to-end delays, at quality levels commensurate with available\nnetwork resources over arbitrary network topologies where bottlenecks can be\nanywhere in the network. This is in contrast to commonly assumed P2P scenarios\nwhere bandwidth bottlenecks reside only at the edge of the network. The\nhighlight in our design is a distributed and adaptive rate control protocol,\nthat can discover and adapt to arbitrary topologies and network conditions\nquickly, converging to efficient link rate allocations allowed by the\nunderlying network. In accordance with adaptive link rate control, source video\nencoding rates are also dynamically controlled to optimize video quality in\narbitrary and unpredictable network conditions. We have implemented Celerity in\na prototype system, and demonstrate its superior performance over existing\nsolutions in a local experimental testbed and over the Internet.", 
    "link": "http://arxiv.org/pdf/1107.1138v4", 
    "arxiv-id": "1107.1138v4"
},{
    "category": "cs.MM", 
    "author": "Vadim Zaytsev", 
    "title": "MediaWiki Grammar Recovery", 
    "publish": "2011-07-23T06:30:44Z", 
    "summary": "The paper describes in detail the recovery effort of one of the official\nMediaWiki grammars. Over two hundred grammar transformation steps are reported\nand annotated, leading to delivery of a level 2 grammar, semi-automatically\nextracted from a community created semi-formal text using at least five\ndifferent syntactic notations, several non-enforced naming conventions,\nmultiple misspellings, obsolete parsing technology idiosyncrasies and other\nproblems commonly encountered in grammars that were not engineered properly.\nHaving a quality grammar will allow to test and validate it further, without\nalienating the community with a separately developed grammar.", 
    "link": "http://arxiv.org/pdf/1107.4661v1", 
    "arxiv-id": "1107.4661v1"
},{
    "category": "cs.MM", 
    "author": "Nitin Rakesh", 
    "title": "Transmission of Successful Route Error Message(RERR) in Routing Aware   Multiple Description Video Coding over Mobile Ad-Hoc Network", 
    "publish": "2011-09-04T20:03:32Z", 
    "summary": "Video transmission over mobile ad-hoc networks is becoming important as these\nnetworks become more widely used in the wireless networks. We propose a\nrouting-aware multiple description video coding approach to support video\ntransmission over mobile ad-hoc networks with single and multiple path\ntransport. We build a model to estimate the packet loss probability of each\npacket transmitted over the network based on the standard ad-hoc routing\nmessages and network parameters without losing the RERR message. We then\ncalculate the frame loss probability in order to eliminate error without any\nloss of data.", 
    "link": "http://arxiv.org/pdf/1109.0753v1", 
    "arxiv-id": "1109.0753v1"
},{
    "category": "cs.MM", 
    "author": "Dr. Siddu. P. Algur", 
    "title": "A Survey on Web Multimedia Mining", 
    "publish": "2011-09-06T11:12:55Z", 
    "summary": "Modern developments in digital media technologies has made transmitting and\nstoring large amounts of multi/rich media data (e.g. text, images, music, video\nand their combination) more feasible and affordable than ever before. However,\nthe state of the art techniques to process, mining and manage those rich media\nare still in their infancy. Advances developments in multimedia acquisition and\nstorage technology the rapid progress has led to the fast growing incredible\namount of data stored in databases. Useful information to users can be revealed\nif these multimedia files are analyzed. Multimedia mining deals with the\nextraction of implicit knowledge, multimedia data relationships, or other\npatterns not explicitly stored in multimedia files. Also in retrieval, indexing\nand classification of multimedia data with efficient information fusion of the\ndifferent modalities is essential for the system's overall performance. The\npurpose of this paper is to provide a systematic overview of multimedia mining.\nThis article is also represents the issues in the application process component\nfor multimedia mining followed by the multimedia mining models.", 
    "link": "http://arxiv.org/pdf/1109.1145v1", 
    "arxiv-id": "1109.1145v1"
},{
    "category": "cs.MM", 
    "author": "Suresh N. Mali", 
    "title": "Secured color image watermarking technique in DWT-DCT domain", 
    "publish": "2011-09-11T16:24:00Z", 
    "summary": "The multilayer secured DWT-DCT and YIQ color space based image watermarking\ntechnique with robustness and better correlation is presented here. The\nsecurity levels are increased by using multiple pn sequences, Arnold\nscrambling, DWT domain, DCT domain and color space conversions. Peak signal to\nnoise ratio and Normalized correlations are used as measurement metrics. The\n512x512 sized color images with different histograms are used for testing and\nwatermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.\n'Haar' wavelet is used for decomposition and direct flexing factor is used. We\ngot PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum\nNC 0.9781 for flexing factor k=4 in Q color space. The comparative performance\nin Y, I and Q color space is presented. The technique is robust for different\nattacks like scaling, compression, rotation etc.", 
    "link": "http://arxiv.org/pdf/1109.2325v1", 
    "arxiv-id": "1109.2325v1"
},{
    "category": "cs.MM", 
    "author": "Aur\u00e9lie Bugeau", 
    "title": "Multi-Layer Local Graph Words for Object Recognition", 
    "publish": "2011-10-31T18:44:41Z", 
    "summary": "In this paper, we propose a new multi-layer structural approach for the task\nof object based image retrieval. In our work we tackle the problem of\nstructural organization of local features. The structural features we propose\nare nested multi-layered local graphs built upon sets of SURF feature points\nwith Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied\non these graphs, giving birth to a Bag-of-Graph-Words representation. The\nmulti-layer nature of the descriptors consists in scaling from trivial Delaunay\ngraphs - isolated feature points - by increasing the number of nodes layer by\nlayer up to graphs with maximal number of nodes. For each layer of graphs its\nown visual dictionary is built. The experiments conducted on the SIVAL and\nCaltech-101 data sets reveal that the graph features at different layers\nexhibit complementary performances on the same content and perform better than\nbaseline BoVW approach. The combination of all existing layers, yields\nsignificant improvement of the object recognition performance compared to\nsingle level approaches.", 
    "link": "http://arxiv.org/pdf/1110.6895v1", 
    "arxiv-id": "1110.6895v1"
},{
    "category": "cs.MM", 
    "author": "Jean-Fran\u00e7ois Dartigues", 
    "title": "Hierarchical Hidden Markov Model in Detecting Activities of Daily Living   in Wearable Videos for Studies of Dementia", 
    "publish": "2011-11-08T07:53:29Z", 
    "summary": "This paper presents a method for indexing activities of daily living in\nvideos obtained from wearable cameras. In the context of dementia diagnosis by\ndoctors, the videos are recorded at patients' houses and later visualized by\nthe medical practitioners. The videos may last up to two hours, therefore a\ntool for an efficient navigation in terms of activities of interest is crucial\nfor the doctors. The specific recording mode provides video data which are\nreally difficult, being a single sequence shot where strong motion and sharp\nlighting changes often appear. Our work introduces an automatic motion based\nsegmentation of the video and a video structuring approach in terms of\nactivities by a hierarchical two-level Hidden Markov Model. We define our\ndescription space over motion and visual characteristics of video and audio\nchannels. Experiments on real data obtained from the recording at home of\nseveral patients show the difficulty of the task and the promising results of\nour approach.", 
    "link": "http://arxiv.org/pdf/1111.1817v2", 
    "arxiv-id": "1111.1817v2"
},{
    "category": "cs.MM", 
    "author": "Hossein Reza Babae", 
    "title": "A Survey on Web-based AR Applications", 
    "publish": "2011-11-13T07:13:23Z", 
    "summary": "Due to the increase of interest in Augmented Reality (AR), the potential uses\nof AR are increasing also. It can benefit the user in various fields such as\neducation, business, medicine, and other. Augmented Reality supports the real\nenvironment with synthetic environment to give more details and meaning to the\nobjects in the real word. AR refers to a situation in which the goal is to\nsupplement a user's perception of the real-world through the addition of\nvirtual objects. This paper is an attempt to make a survey of web-based\nAugmented Reality applications and make a comparison among them.", 
    "link": "http://arxiv.org/pdf/1111.2993v1", 
    "arxiv-id": "1111.2993v1"
},{
    "category": "cs.MM", 
    "author": "Pascale S\u00e9billot", 
    "title": "A Scalable Video Search Engine Based on Audio Content Indexing and Topic   Segmentation", 
    "publish": "2011-11-27T15:08:36Z", 
    "summary": "One important class of online videos is that of news broadcasts. Most news\norganisations provide near-immediate access to topical news broadcasts over the\nInternet, through RSS streams or podcasts. Until lately, technology has not\nmade it possible for a user to automatically go to the smaller parts, within a\nlonger broadcast, that might interest them. Recent advances in both speech\nrecognition systems and natural language processing have led to a number of\nrobust tools that allow us to provide users with quicker, more focussed access\nto relevant segments of one or more news broadcast videos. Here we present our\nnew interface for browsing or searching news broadcasts (video/audio) that\nexploits these new language processing tools to (i) provide immediate access to\ntopical passages within news broadcasts, (ii) browse news broadcasts by events\nas well as by people, places and organisations, (iii) perform cross lingual\nsearch of news broadcasts, (iv) search for news through a map interface, (v)\nbrowse news by trending topics, and (vi) see automatically-generated textual\nclues for news segments, before listening. Our publicly searchable demonstrator\ncurrently indexes daily broadcast news content from 50 sources in English,\nFrench, Chinese, Arabic, Spanish, Dutch and Russian.", 
    "link": "http://arxiv.org/pdf/1111.6265v1", 
    "arxiv-id": "1111.6265v1"
},{
    "category": "cs.MM", 
    "author": "Mohd. Najib Mohd. Salleh", 
    "title": "A New Digital Watermarking Algorithm Using Combination of Least   Significant Bit (LSB) and Inverse Bit", 
    "publish": "2011-11-29T08:37:08Z", 
    "summary": "In this paper, we introduce a new digital watermarking algorithm using least\nsignificant bit (LSB). LSB is used because of its little effect on the image.\nThis new algorithm is using LSB by inversing the binary values of the watermark\ntext and shifting the watermark according to the odd or even number of pixel\ncoordinates of image before embedding the watermark. The proposed algorithm is\nflexible depending on the length of the watermark text. If the length of the\nwatermark text is more than ((MxN)/8)-2 the proposed algorithm will also embed\nthe extra of the watermark text in the second LSB. We compare our proposed\nalgorithm with the 1-LSB algorithm and Lee's algorithm using Peak\nsignal-to-noise ratio (PSNR). This new algorithm improved its quality of the\nwatermarked image. We also attack the watermarked image by using cropping and\nadding noise and we got good results as well.", 
    "link": "http://arxiv.org/pdf/1111.6727v1", 
    "arxiv-id": "1111.6727v1"
},{
    "category": "cs.MM", 
    "author": "ChoelHoon Lee", 
    "title": "Automatic Classification of X-rated Videos using Obscene Sound Analysis   based on a Repeated Curve-like Spectrum Feature", 
    "publish": "2011-12-09T07:05:49Z", 
    "summary": "This paper addresses the automatic classification of X-rated videos by\nanalyzing its obscene sounds. In this paper, obscene sounds refer to audio\nsignals generated from sexual moans and screams during sexual scenes. By\nanalyzing various sound samples, we determined the distinguishable\ncharacteristics of obscene sounds and propose a repeated curve-like spectrum\nfeature that represents the characteristics of such sounds. We constructed\n6,269 audio clips to evaluate the proposed feature, and separately constructed\n1,200 X-rated and general videos for classification. The proposed feature has\nan F1-score, precision, and recall rate of 96.6%, 98.2%, and 95.2%,\nrespectively, for the original dataset, and 92.6%, 97.6%, and 88.0% for a noisy\ndataset of 5dB SNR. And, in classifying videos, the feature has more than a 90%\nF1-score, 97% precision, and an 84% recall rate. From the measured performance,\nX-rated videos can be classified with only the audio features and the repeated\ncurve-like spectrum feature is suitable to detect obscene sounds.", 
    "link": "http://arxiv.org/pdf/1112.2027v1", 
    "arxiv-id": "1112.2027v1"
},{
    "category": "cs.MM", 
    "author": "Putra Sumari", 
    "title": "Statistical Information of the Increased Demand for Watch the VOD with   the Increased Sophistication in the Mobile Devices,Communications and   Internet Penetration in Asia", 
    "publish": "2011-12-09T08:26:23Z", 
    "summary": "As the rapid progress of the media streaming applications such as video\nstreaming can be classified into two types of streaming, Live video streaming,\nVideo on Demand (VoD). Live video streaming is a service which allows the\nclients to watch many TV channels over the internet and the clients able to use\none operation to perform is to switch the channels. Video on Demand (VoD) is\none of the most important applications for the internet of the future and has\nbecome an interactive multimedia service which allows the users to start\nwatching the video of their choice at anytime and anywhere, especially after\nthe rapid deployment of the wireless networks and mobile devices. In this paper\nprovide statistical information about the Internet, communications and mobile\ndevices etc. This has led to an increased demand for the development,\ncommunication and computational powers of many of the mobile wireless\nsubscribers/mobile devices such as laptops, PDAs, smart phones and notebook.\nThese techniques are utilized to obtain a video on demand service with higher\nresolution and quality. Another objective in this paper is to see Malaysia\nranked as a fully developed country by the year 2020.", 
    "link": "http://arxiv.org/pdf/1112.2042v1", 
    "arxiv-id": "1112.2042v1"
},{
    "category": "cs.MM", 
    "author": "Surekha Mariam Varghese", 
    "title": "Modelling Gesture Based Ubiquitous Applications", 
    "publish": "2011-12-09T08:35:14Z", 
    "summary": "A cost effective, gesture based modelling technique called Virtual\nInteractive Prototyping (VIP) is described in this paper. Prototyping is\nimplemented by projecting a virtual model of the equipment to be prototyped.\nUsers can interact with the virtual model like the original working equipment.\nFor capturing and tracking the user interactions with the model image and sound\nprocessing techniques are used. VIP is a flexible and interactive prototyping\nmethod that has much application in ubiquitous computing environments.\nDifferent commercial as well as socio-economic applications and extension to\ninteractive advertising of VIP are also discussed.", 
    "link": "http://arxiv.org/pdf/1112.2044v1", 
    "arxiv-id": "1112.2044v1"
},{
    "category": "cs.MM", 
    "author": "N. M. Jeya Nachiaban", 
    "title": "Lossless Digital Image Compression Method for Bitmap Images", 
    "publish": "2011-12-10T08:34:51Z", 
    "summary": "In this research paper, the authors propose a new approach to digital image\ncompression using crack coding This method starts with the original image and\ndevelop crack codes in a recursive manner, marking the pixels visited earlier\nand expanding the entropy in four directions. The proposed method is\nexperimented with sample bitmap images and results are tabulated. The method is\nimplemented in uni-processor machine using C language source code.", 
    "link": "http://arxiv.org/pdf/1112.2261v1", 
    "arxiv-id": "1112.2261v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Markov Decision Process Based Energy-Efficient On-Line Scheduling for   Slice-Parallel Video Decoders on Multicore Systems", 
    "publish": "2011-12-17T20:22:27Z", 
    "summary": "We consider the problem of energy-efficient on-line scheduling for\nslice-parallel video decoders on multicore systems. We assume that each of the\nprocessors are Dynamic Voltage Frequency Scaling (DVFS) enabled such that they\ncan independently trade off performance for power, while taking the video\ndecoding workload into account. In the past, scheduling and DVFS policies in\nmulti-core systems have been formulated heuristically due to the inherent\ncomplexity of the on-line multicore scheduling problem. The key contribution of\nthis report is that we rigorously formulate the problem as a Markov decision\nprocess (MDP), which simultaneously takes into account the on-line scheduling\nand per-core DVFS capabilities; the power consumption of the processor cores\nand caches; and the loss tolerant and dynamic nature of the video decoder's\ntraffic. In particular, we model the video traffic using a Direct Acyclic Graph\n(DAG) to capture the precedence constraints among frames in a Group of Pictures\n(GOP) structure, while also accounting for the fact that frames have different\ndisplay/decoding deadlines and non-deterministic decoding complexities. The\nobjective of the MDP is to minimize long-term power consumption subject to a\nminimum Quality of Service (QoS) constraint related to the decoder's\nthroughput. Although MDPs notoriously suffer from the curse of dimensionality,\nwe show that, with appropriate simplifications and approximations, the\ncomplexity of the MDP can be mitigated. We implement a slice-parallel version\nof H.264 on a multiprocessor ARM (MPARM) virtual platform simulator, which\nprovides cycle-accurate and bus signal-accurate simulation for different\nprocessors. We use this platform to generate realistic video decoding traces\nwith which we evaluate the proposed on-line scheduling algorithm in Matlab.", 
    "link": "http://arxiv.org/pdf/1112.4084v2", 
    "arxiv-id": "1112.4084v2"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Interactive multiview video system with non-complex navigation at the   decoder", 
    "publish": "2012-01-03T09:19:58Z", 
    "summary": "Multiview video with interactive and smooth view switching at the receiver is\na challenging application with several issues in terms of effective use of\nstorage and bandwidth resources, reactivity of the system, quality of the\nviewing experience and system complexity. The classical decoding system for\ngenerating virtual views first projects a reference or encoded frame to a given\nviewpoint and then fills in the holes due to potential occlusions. This last\nstep still constitutes a complex operation with specific software or hardware\nat the receiver and requires a certain quantity of information from the\nneighboring frames for insuring consistency between the virtual images. In this\nwork we propose a new approach that shifts most of the burden due to\ninteractivity from the decoder to the encoder, by anticipating the navigation\nof the decoder and sending auxiliary information that guarantees temporal and\ninterview consistency. This leads to an additional cost in terms of\ntransmission rate and storage, which we minimize by using optimization\ntechniques based on the user behavior modeling. We show by experiments that the\nproposed system represents a valid solution for interactive multiview systems\nwith classical decoders.", 
    "link": "http://arxiv.org/pdf/1201.0598v1", 
    "arxiv-id": "1201.0598v1"
},{
    "category": "cs.MM", 
    "author": "Saira Beg", 
    "title": "Stereo image Transference & Retrieval over SMS", 
    "publish": "2012-01-06T10:10:54Z", 
    "summary": "Paper presents the way of transferring stereo images using SMS over GSM\nnetwork. Generally, Stereo image is composed of two stereoscopic images in such\nway that gives three dimensional affect when viewed. GSM have two short\nmessaging services, which can transfer images and sounds etc. Such services are\nknown as; MMS (Multimedia Messaging Service) and EMS (Extended Messaging\nService). EMS can send Predefined sounds, animation and images but have\nlimitation that it does not support widely. MMS can send much higher contents\nthan EMS but need 3G and other network capability in order to send large size\ndata up to 1000 bytes. Other limitations are Portability, content adaption etc.\nOur major aim in this paper is to provide an alternative way of sending stereo\nimages over SMS which is widely supported than EMS. We develop an application\nusing J2ME Platform.", 
    "link": "http://arxiv.org/pdf/1201.1383v1", 
    "arxiv-id": "1201.1383v1"
},{
    "category": "cs.MM", 
    "author": "Mohammadreza Keyvanpour", 
    "title": "Identifying and Analysis of Scene Mining Methods Beased on Scenes   Extracted Features", 
    "publish": "2012-01-08T23:45:27Z", 
    "summary": "Scene mining is a subset of image mining in which scenes are classified to a\ndistinct set of classes based on analysis of their content. In other word in\nscene mining, a label is given to visual content of scene, for example,\nmountain, beach. Scene mining is used in applications such as medicine, movie,\ninformation retrieval, computer vision, recognition of traffic scene. Reviewing\nof represented methods shows there are various methods in scene mining. Scene\nmining applications extension and existence of various scenes, make comparison\nof methods hard. Scene mining can be followed by identifying scene mining\ncomponents and representing a framework to analyzing and evaluating methods. In\nthis paper, at first, components of scene mining are introduced, then a\nframework based on extracted features of scene is represented to classify scene\nmining methods. Finally, these methods are analyzed and evaluated via a\nproposal framework.", 
    "link": "http://arxiv.org/pdf/1201.1668v1", 
    "arxiv-id": "1201.1668v1"
},{
    "category": "cs.MM", 
    "author": "Yiannis Andreopoulos", 
    "title": "Throughput Scaling Of Convolution For Error-Tolerant Multimedia   Applications", 
    "publish": "2012-01-14T14:22:54Z", 
    "summary": "Convolution and cross-correlation are the basis of filtering and pattern or\ntemplate matching in multimedia signal processing. We propose two throughput\nscaling options for any one-dimensional convolution kernel in programmable\nprocessors by adjusting the imprecision (distortion) of computation. Our\napproach is based on scalar quantization, followed by two forms of tight\npacking in floating-point (one of which is proposed in this paper) that allow\nfor concurrent calculation of multiple results. We illustrate how our approach\ncan operate as an optional pre- and post-processing layer for off-the-shelf\noptimized convolution routines. This is useful for multimedia applications that\nare tolerant to processing imprecision and for cases where the input signals\nare inherently noisy (error tolerant multimedia applications). Indicative\nexperimental results with a digital music matching system and an MPEG-7 audio\ndescriptor system demonstrate that the proposed approach offers up to 175%\nincrease in processing throughput against optimized (full-precision)\nconvolution with virtually no effect in the accuracy of the results. Based on\nmarginal statistics of the input data, it is also shown how the throughput and\ndistortion can be adjusted per input block of samples under constraints on the\nsignal-to-noise ratio against the full-precision convolution.", 
    "link": "http://arxiv.org/pdf/1201.3018v1", 
    "arxiv-id": "1201.3018v1"
},{
    "category": "cs.MM", 
    "author": "M. Djoudi", 
    "title": "An Authoring System for Editing Lessons in Phonetic English in SMIL3.0", 
    "publish": "2012-01-25T14:39:39Z", 
    "summary": "One of the difficulties of teaching English is the prosody, including the\nstress. French learners have difficulties to encode this information about the\nword because it is irrelevant for them. Therefore, they have difficulty to\nproduce this stress when they speak that language. Studies in this area have\nconcluded that the dual-coding approach (auditory and visual) of a phonetic\nphenomenon helps a lot to improve its perception and memorization for novice\nlearners. The aim of our work is to provide English teachers with an authoring\nnamed SaCoPh for editing multimedia courses that support this approach. This\ncourse is based on a template that fits the educational aspects of phonetics,\nexploiting the features of version 3.0 of the standard SMIL (Synchronized\nMultimedia Integration Language) for the publication of this course on the web.", 
    "link": "http://arxiv.org/pdf/1201.5285v1", 
    "arxiv-id": "1201.5285v1"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Development Trends in Steganography", 
    "publish": "2012-02-23T20:39:38Z", 
    "summary": "Steganography is a general term referring to all methods for the embedding of\nadditional secret content into some form of carrier, with the aim of\nconcealment of the introduced alterations. The choice of the carrier is nearly\nunlimited, it may be an ancient piece of parchment, as well as a network\nprotocol header. Inspired by biological phenomena, adopted by man in the\nancient times, it has been developed over the ages. Present day steganographic\nmethods are far more sophisticated than their ancient predecessors, but the\nmain principles have remained unchanged. They typically rely on the utilization\nof digital media files or network protocols as a carrier, in which secret data\nis embedded. This paper presents the evolution of the hidden data carrier from\nthe ancient times till the present day and pinpoints the observed development\ntrends, with special emphasis on network steganography.", 
    "link": "http://arxiv.org/pdf/1202.5289v2", 
    "arxiv-id": "1202.5289v2"
},{
    "category": "cs.MM", 
    "author": "Ratnadeep R. Deshmukh", 
    "title": "Blind 3D Model Watermarking Based on Multi-Resolution Representation and   Fuzzy Logic", 
    "publish": "2012-03-12T13:33:03Z", 
    "summary": "Insertion of a text message, audio data or/and an image into another image or\n3D model is called as a watermarking process. Watermarking has variety of\napplications like: Copyright Protection, Owner Identification, Copy Protection\nand Data Hiding etc., depending upon the type of watermark insertion algorithm.\nWatermark remains in the content after applying various attacks without any\ndistortions. The blind watermarking method used in the system is based on a\nwavelet transform, a fuzzy inference system and a multi-resolution\nrepresentation (MRR) of the 3d model. The watermark scrambled by Arnold\nTransform is embedded in the wavelet coefficients at third resolution level of\nthe MRR. Fuzzy logic approach used in the method makes it to approximate the\nbest possible gain with an accurate scaling factor so that the watermark\nremains invisible. The fuzzy input variables are computed for each wavelet\ncoefficient in the 3D model. The output of the fuzzy system is a single value\nwhich is a perceptual value for each corresponding wavelet coefficient. Thus,\nthe fuzzy perceptual mask combines all these non-linear variables to build a\nsimple, easy to use HVS model. Results shows that the system is robust against\naffine transformations, smoothing, cropping and noise attacks.", 
    "link": "http://arxiv.org/pdf/1203.2485v1", 
    "arxiv-id": "1203.2485v1"
},{
    "category": "cs.MM", 
    "author": "Niraj Shakhakarmi", 
    "title": "Quantitative Multiscale Analysis using Different Wavelets in 1D Voice   Signal and 2D Image", 
    "publish": "2012-03-19T05:09:50Z", 
    "summary": "Mutiscale analysis represents multiresolution scrutiny of a signal to improve\nits signal quality. Multiresolution analysis of 1D voice signal and 2D image is\nconducted using DCT, FFT and different wavelets such as Haar, Deubachies,\nMorlet, Cauchy, Shannon, Biorthogonal, Symmlet and Coiflet deploying the\ncascaded filter banks based decomposition and reconstruction. The outstanding\nquantitative analysis of the specified wavelets is done to investigate the\nsignal quality, mean square error, entropy and peak-to-peak SNR at multiscale\nstage-4 for both 1D voice signal and 2D image. In addition, the 2D image\ncompression performance is significantly found 93.00% in DB-4, 93.68% in\nbior-4.4, 93.18% in Sym-4 and 92.20% in Coif-2 during the multiscale analysis.", 
    "link": "http://arxiv.org/pdf/1203.4035v1", 
    "arxiv-id": "1203.4035v1"
},{
    "category": "cs.MM", 
    "author": "Mehmet Sabih Aksoy", 
    "title": "I-SolFramework: An Integrated Solution Framework Six Layers Assessment   on Multimedia Information Security Architecture Policy Compliance", 
    "publish": "2012-03-31T02:06:00Z", 
    "summary": "Multimedia Information security becomes a important part for the\norganization's intangible assets. Level of confidence and stakeholder trusted\nare performance indicator as successes organization, it is imperative for\norganizations to use Information Security Management System (ISMS) to\neffectively manage their multimedia information assets. The main objective of\nthis paper is to Provide a novel practical framework approach to the\ndevelopment of ISMS, Called by the I-SolFramework, implemented in multimedia\ninformation security architecture (MISA), it divides a problem into six object\ndomains or six layers, namely organization,stakeholders, tool & technology,\npolicy, knowledge, and culture. In addition, this framework also introduced\nnovelty algorithm and mathematic models as measurement and assessment tools of\nMISA parameters.", 
    "link": "http://arxiv.org/pdf/1204.0056v1", 
    "arxiv-id": "1204.0056v1"
},{
    "category": "cs.MM", 
    "author": "Putra Sumari", 
    "title": "An Overview of Video Allocation Algorithms for Flash-based SSD Storage   Systems", 
    "publish": "2012-04-11T07:16:35Z", 
    "summary": "Despite the fact that Solid State Disk (SSD) data storage media had offered a\nrevolutionary property storages community, but the unavailability of a\ncomprehensive allocation strategy in SSDs storage media, leads to consuming the\navailable space, random writing processes, time-consuming reading processes,\nand system resources consumption. In order to overcome these challenges, an\nefficient allocation algorithm is a desirable option. In this paper, we had\nexecuted an intensive investigation on the SSD-based allocation algorithms that\nhad been proposed by the knowledge community. An explanatory comparison had\nbeen made between these algorithms. We reviewed these algorithms in order to\nbuilding advanced knowledge armature that would help in inventing new\nallocation algorithms for this type of storage media.", 
    "link": "http://arxiv.org/pdf/1204.2359v1", 
    "arxiv-id": "1204.2359v1"
},{
    "category": "cs.MM", 
    "author": "B B Meshram", 
    "title": "Content based video retrieval systems", 
    "publish": "2012-05-08T09:27:29Z", 
    "summary": "With the development of multimedia data types and available bandwidth there\nis huge demand of video retrieval systems, as users shift from text based\nretrieval systems to content based retrieval systems. Selection of extracted\nfeatures play an important role in content based video retrieval regardless of\nvideo attributes being under consideration. These features are intended for\nselecting, indexing and ranking according to their potential interest to the\nuser. Good features selection also allows the time and space costs of the\nretrieval process to be reduced. This survey reviews the interesting features\nthat can be extracted from video data for indexing and retrieval along with\nsimilarity measurement methods. We also identify present research issues in\narea of content based video retrieval systems.", 
    "link": "http://arxiv.org/pdf/1205.1641v1", 
    "arxiv-id": "1205.1641v1"
},{
    "category": "cs.MM", 
    "author": "K. L. Sudha", 
    "title": "Text Steganography using LSB insertion method along with Chaos Theory", 
    "publish": "2012-05-09T02:50:16Z", 
    "summary": "The art of information hiding has been around nearly as long as the need for\ncovert communication. Steganography, the concealing of information, arose early\non as an extremely useful method for covert information transmission.\nSteganography is the art of hiding secret message within a larger image or\nmessage such that the hidden message or an image is undetectable; this is in\ncontrast to cryptography, where the existence of the message itself is not\ndisguised, but the content is obscure. The goal of a steganographic method is\nto minimize the visually apparent and statistical differences between the cover\ndata and a steganogram while maximizing the size of the payload. Current\ndigital image steganography presents the challenge of hiding message in a\ndigital image in a way that is robust to image manipulation and attack. This\npaper explains about how a secret message can be hidden into an image using\nleast significant bit insertion method along with chaos.", 
    "link": "http://arxiv.org/pdf/1205.1859v1", 
    "arxiv-id": "1205.1859v1"
},{
    "category": "cs.MM", 
    "author": "Asif Perwej", 
    "title": "An Adaptive Watermarking Technique for the copyright of digital images   and Digital Image Protection", 
    "publish": "2012-05-12T17:53:36Z", 
    "summary": "The Internet as a whole does not use secure links, thus information in\ntransit may be vulnerable to interruption as well. The important of reducing a\nchance of the information being detected during the transmission is being an\nissue in the real world now days. The Digital watermarking method provides for\nthe quick and inexpensive distribution of digital information over the\nInternet. This method provides new ways of ensuring the sufficient protection\nof copyright holders in the intellectual property dispersion process. The\nproperty of digital watermarking images allows insertion of additional data in\nthe image without altering the value of the image.In this paper investigate the\nfollowing relevant concepts and terminology, history of watermarks and the\nproperties of a watermarking system and applications. We are proposing edge\ndetection using Gabor Filters. In this paper we are proposed least significant\nbit (LSB) substitution method to encrypt the message in the watermark image\nfile. The benefits of the LSB are its simplicity to embed the bits of the\nmessage directly into the LSB plane of cover-image and many techniques using\nthese methods. The LSB does not result in a human perceptible difference\nbecause the amplitude of the change is little therefore the human eye the\nresulting stego image will look identical to the cover image and this allows\nhigh perceptual transparency of the LSB. The spatial domain technique LSB\nsubstitution it would be able to use a pseudo-random number generator to\ndetermine the pixels to be used for embedding based on a given key. We are\nusing DCT transform watermark algorithms based on robustness. The watermarking\nrobustness have been calculated by the Peak Signal to Noise Ratio (PSNR) and\nNormalized cross correlation (NC) is used to quantify by the similarity between\nthe real watermark and after extracting watermark.", 
    "link": "http://arxiv.org/pdf/1205.2800v1", 
    "arxiv-id": "1205.2800v1"
},{
    "category": "cs.MM", 
    "author": "Huazhong Yang", 
    "title": "A Novel Video Compression Approach Based on Underdetermined Blind Source   Separation", 
    "publish": "2012-05-21T11:57:55Z", 
    "summary": "This paper develops a new video compression approach based on underdetermined\nblind source separation. Underdetermined blind source separation, which can be\nused to efficiently enhance the video compression ratio, is combined with\nvarious off-the-shelf codecs in this paper. Combining with MPEG-2, video\ncompression ratio could be improved slightly more than 33%. As for combing with\nH.264, 4X~12X more compression ratio could be achieved with acceptable PSNR,\naccording to different kinds of video sequences.", 
    "link": "http://arxiv.org/pdf/1205.4572v1", 
    "arxiv-id": "1205.4572v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "Perceptual quality comparison between single-layer and scalable videos   at the same spatial, temporal and amplitude resolutions", 
    "publish": "2012-06-08T20:15:30Z", 
    "summary": "In this paper, the perceptual quality difference between scalable and\nsingle-layer videos coded at the same spatial, temporal and amplitude\nresolution (STAR) is investigated through a subjective test using a mobile\nplatform. Three source videos are considered and for each source video\nsingle-layer and scalable video are compared at 9 different STARs. We utilize\npaired comparison methods with and without tie option. Results collected from\n10 subjects in the without \"tie\" option and 6 subjects in the with \"tie\" option\nshow that there is no significant quality difference between scalable and\nsinglelayer video when coded at the same STAR. An analysis of variance (ANOVA)\ntest is also performed to further confirm the finding.", 
    "link": "http://arxiv.org/pdf/1206.1866v1", 
    "arxiv-id": "1206.1866v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "Q-STAR:A Perceptual Video Quality Model Considering Impact of Spatial,   Temporal, and Amplitude Resolutions", 
    "publish": "2012-06-11T19:06:07Z", 
    "summary": "In this paper, we investigate the impact of spatial, temporal and amplitude\nresolution (STAR) on the perceptual quality of a compressed video. Subjective\nquality tests were carried out on a mobile device. Seven source sequences are\nincluded in the tests and for each source sequence we have 27 test\nconfigurations generated by JSVM encoder (3 QP levels, 3 spatial resolutions,\nand 3 temporal resolutions), resulting a total of 189 processed video sequences\n(PVSs). Videos coded at different spatial resolutions are displayed at the full\nscreen size of the mobile platform. Subjective data reveal that the impact of\nspatial resolution (SR), temporal resolution (TR) and quantization stepsize\n(QS) can each be captured by a function with a single content-dependent\nparameter. The joint impact of SR, TR and QS can be accurately modeled by the\nproduct of these three functions with only three parameters. We further find\nthat the quality decay rates with SR and QS, respectively are independent of\nTR, and likewise, the decay rate with TR is independent of SR and QS,\nrespectively. However, there is a significant interaction between the effects\nof SR and QS. The overall quality model is further validated on five other\ndatasets with very high accuracy. The complete model correlates well with the\nsubjective ratings with a Pearson Correlation Coefficient (PCC) of 0.991.", 
    "link": "http://arxiv.org/pdf/1206.2320v1", 
    "arxiv-id": "1206.2320v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "Rate Model for Compressed Video Considering Impacts Of Spatial, Temporal   and Amplitude Resolutions and Its Applications for Video Coding and   Adaptation", 
    "publish": "2012-06-12T19:13:02Z", 
    "summary": "In this paper, we investigate the impacts of spatial, temporal and amplitude\nresolution (STAR) on the bit rate of a compressed video. We propose an\nanalytical rate model in terms of the quantization stepsize, frame size and\nframe rate. Experimental results reveal that the increase of the video rate as\nthe individual resolution increases follows a power function. Hence, the\nproposed model expresses the rate as the product of power functions of the\nquantization stepsize, frame size and frame rate, respectively. The proposed\nrate model is analytically tractable, requiring only four content dependent\nparameters. We also propose methods for predicting the model parameters from\ncontent features that can be computed from original video. Simulation results\nshow that model predicted rates fit the measured data very well with high\nPearson correlation (PC) and small relative root mean square error (RRMSE). The\nsame model function works for different coding scenarios (including scalable\nand non-scalable video, temporal prediction using either hierarchical B or IPPP\nstructure, etc.) with very high accuracy (average PC $>$ 0.99), but the values\nof model parameters differ. Using the proposed rate model and the quality model\nintroduced in a separate work, we show how to optimize the STAR for a given\nrate constraint, which is important for both encoder rate control and scalable\nvideo adaptation. Furthermore, we demonstrate how to order the spatial,\ntemporal and amplitude layers of a scalable video in a rate-quality optimized\nway.", 
    "link": "http://arxiv.org/pdf/1206.2625v1", 
    "arxiv-id": "1206.2625v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Collaborative P2P Streaming of Interactive Live Free Viewpoint Video", 
    "publish": "2012-11-20T15:04:43Z", 
    "summary": "We study an interactive live streaming scenario where multiple peers pull\nstreams of the same free viewpoint video that are synchronized in time but not\nnecessarily in view. In free viewpoint video, each user can periodically select\na virtual view between two anchor camera views for display. The virtual view is\nsynthesized using texture and depth videos of the anchor views via\ndepth-image-based rendering (DIBR). In general, the distortion of the virtual\nview increases with the distance to the anchor views, and hence it is\nbeneficial for a peer to select the closest anchor views for synthesis. On the\nother hand, if peers interested in different virtual views are willing to\ntolerate larger distortion in using more distant anchor views, they can\ncollectively share the access cost of common anchor views.\n  Given anchor view access cost and synthesized distortion of virtual views\nbetween anchor views, we study the optimization of anchor view allocation for\ncollaborative peers. We first show that, if the network reconfiguration costs\ndue to view-switching are negligible, the problem can be optimally solved in\npolynomial time using dynamic programming. We then consider the case of\nnon-negligible reconfiguration costs (e.g., large or frequent view-switching\nleading to anchor-view changes). In this case, the view allocation problem\nbecomes NP-hard. We thus present a locally optimal and centralized allocation\nalgorithm inspired by Lloyd's algorithm in non-uniform scalar quantization. We\nalso propose a distributed algorithm with guaranteed convergence where each\npeer group independently make merge-and-split decisions with a well-defined\nfairness criteria. The results show that depending on the problem settings, our\nproposed algorithms achieve respective optimal and close-to-optimal performance\nin terms of total cost, and outperform a P2P scheme without collaborative\nanchor selection.", 
    "link": "http://arxiv.org/pdf/1211.4767v1", 
    "arxiv-id": "1211.4767v1"
},{
    "category": "cs.MM", 
    "author": "Kamlesh Sharma", 
    "title": "A Modified LSB Technique of Digital Watermarking in Spatial Domain", 
    "publish": "2013-03-29T10:20:44Z", 
    "summary": "Digital watermarking is a technique of embedding pieces of information into\ndigital data such as text, audio, video, and still images that can be detected\nor extracted later to show authentication about the data. Watermark is hidden\ninformation in the image(s) and is so designed that it does not degrade/distort\nthe quality of the image and still keeps the information. Digital watermarking\nis basically to protect ownership rights and to control of making illicit\ncopies of digital data. In this paper, we have discussed various watermarking\ntechniques and properties and have proposed a modified LSB technique. We have\nimplemented the proposed technique by following: 2-bits of 8-bit gray image is\nreplaced by luminance part, next 2-bits by red component, next 2-bits by green\ncomponent and next 2-bits by blue component of 32-bit image using secret key.\nThe advantage is that watermarking capacity has been increased and unaffected\nby various attacks e.g. zero out LSB bits, cropping etc. Watermark image is\nimperceptible in resultant image. We have tested this technique on several\nimages and found that it is quite satisfactory. This technique is secured as\nunauthorized user can not extract the watermarked contents easily from the\noriginal image and works well in adverse situations. We have implemented this\ntechnique on platform java 1.5.0.", 
    "link": "http://arxiv.org/pdf/1303.7353v1", 
    "arxiv-id": "1303.7353v1"
},{
    "category": "cs.MM", 
    "author": "Rabab K. Ward", 
    "title": "A local fingerprinting approach for audio copy detection", 
    "publish": "2013-04-02T20:35:11Z", 
    "summary": "This study proposes an audio copy detection system that is robust to various\nattacks. These include the severe pitch shift and tempo change attacks which\nexisting systems fail to detect. First, we propose a novel two dimensional\nrepresentation for audio signals called the time-chroma image. This image is\nbased on a modification of the concept of chroma in the music literature and is\nshown to achieve better performance in song identification. Then, we propose a\nnovel fingerprinting algorithm that extracts local fingerprints from the\ntime-chroma image. The proposed local fingerprinting algorithm is invariant to\ntime/frequency scale changes in audio signals. It also outperforms existing\nmethods like SIFT by a great extent. Finally, we introduce a song\nidentification algorithm that uses the proposed fingerprints. The resulting\ncopy detection system is shown to significantly outperform existing methods.\nBesides being able to detect whether a song (or a part of it) has been copied,\nthe proposed system can accurately estimate the amount of pitch shift and/or\ntempo change that might have been applied to a song.", 
    "link": "http://arxiv.org/pdf/1304.0793v1", 
    "arxiv-id": "1304.0793v1"
},{
    "category": "cs.MM", 
    "author": "Teresa Chambel", 
    "title": "Genetic Soundtracks: Creative Matching of Audio to Video", 
    "publish": "2013-04-09T17:29:56Z", 
    "summary": "The matching of the soundtrack in a movie or a video can have an enormous\ninfluence in the message being conveyed and its impact, in the sense of\ninvolvement and engagement, and ultimately in their aesthetic and entertainment\nqualities. Art is often associated with creativity, implying the presence of\ninspiration, originality and appropriateness. Evolutionary systems provides us\nwith the novelty, showing us new and subtly different solutions in every\ngeneration, possibly stimulating the creativity of the human using the system.\nIn this paper, we present Genetic Soundtracks, an evolutionary approach to the\ncreative matching of audio to a video. It analyzes both media to extract\nfeatures based on their content, and adopts genetic algorithms, with the\npurpose of truncating, combining and adjusting audio clips, to align and match\nthem with the video scenes.", 
    "link": "http://arxiv.org/pdf/1304.2671v1", 
    "arxiv-id": "1304.2671v1"
},{
    "category": "cs.MM", 
    "author": "Priya R. Kamath", 
    "title": "A Secure And High Capacity Image Steganography Technique", 
    "publish": "2013-04-11T14:45:51Z", 
    "summary": "Steganography is the science of invisible communication. The purpose of\nSteganography is to maintain secret communication between two parties. The\nsecret information can be concealed in content such as image, audio, or video.\nThis paper provides a novel image steganography technique to hide multiple\nsecret images and keys in color cover image using Integer Wavelet Transform\n(IWT). There is no visual difference between the stego image and the cover\nimage. The extracted secret images are also similar to the original secret\nimages. Very good PSNR (Peak Signal to Noise Ratio) values are obtained for\nboth stego and extracted secret images. The results are compared with the\nresults of other techniques, where single image is hidden and it is found that\nthe proposed technique is simple and gives better PSNR values than others.", 
    "link": "http://arxiv.org/pdf/1304.3629v1", 
    "arxiv-id": "1304.3629v1"
},{
    "category": "cs.MM", 
    "author": "Emmanuel Lochin", 
    "title": "Joint On-the-Fly Network Coding/Video Quality Adaptation for Real-Time   Delivery", 
    "publish": "2013-04-18T10:00:38Z", 
    "summary": "This paper introduces a redundancy adaptation algorithm for an on-the-fly\nerasure network coding scheme called Tetrys in the context of real-time video\ntransmission. The algorithm exploits the relationship between the redundancy\nratio used by Tetrys and the gain or loss in encoding bit rate from changing a\nvideo quality parameter called the Quantization Parameter (QP). Our evaluations\nshow that with equal or less bandwidth occupation, the video protected by\nTetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more 4\ndB compared to the video without Tetrys protection. We demonstrate that the\nTetrys redundancy adaptation algorithm performs well with the variations of\nboth loss pattern and delay induced by the networks. We also show that Tetrys\nwith the redundancy adaptation algorithm outperforms FEC with and without\nredundancy adaptation.", 
    "link": "http://arxiv.org/pdf/1304.5068v1", 
    "arxiv-id": "1304.5068v1"
},{
    "category": "cs.MM", 
    "author": "David E. Robillard", 
    "title": "Adaptive Software Radio Steganography", 
    "publish": "2013-04-27T05:05:52Z", 
    "summary": "This paper presents an adaptable steganography (information hiding) method\nfor digital radio communication. Many radio steganography methods exist, but\nmost are defined at higher levels of the protocol stack and are thus protocol\ndependent. In contrast, this method is defined at the physical layer, which\nmakes it widely applicable regardless of the protocols used at higher layers.\nThis approach is also adaptive; the covertness of the hidden channel is simple\nto control via a single continuous parameter either manually or automatically.\nSeveral variations are introduced, each with performance evaluated by\nsimulation. Results show this to be a feasible method with a reasonable\ntrade-off between performance and covertness.", 
    "link": "http://arxiv.org/pdf/1304.7324v1", 
    "arxiv-id": "1304.7324v1"
},{
    "category": "cs.MM", 
    "author": "Ben Liang", 
    "title": "Optimal Frame Transmission for Scalable Video with Hierarchical   Prediction Structure", 
    "publish": "2013-05-22T01:25:02Z", 
    "summary": "An optimal frame transmission scheme is presented for streaming scalable\nvideo over a link with limited capacity. The objective is to select a\ntransmission sequence of frames and their transmission schedule such that the\noverall video quality is maximized. The problem is solved for two general\nclasses of hierarchical prediction structures, which include as a special case\nthe popular dyadic structure. Based on a new characterization of the\ninterdependence among frames in terms of trees, structural properties of an\noptimal transmission schedule are derived. These properties lead to the\ndevelopment of a jointly optimal frame selection and scheduling algorithm,\nwhich has computational complexity that is quadratic in the number of frames.\nSimulation results show that the optimal scheme substantially outperforms three\nexisting alternatives.", 
    "link": "http://arxiv.org/pdf/1305.4999v2", 
    "arxiv-id": "1305.4999v2"
},{
    "category": "cs.MM", 
    "author": "Wai-tian Tan", 
    "title": "Loss-resilient Coding of Texture and Depth for Free-viewpoint Video   Conferencing", 
    "publish": "2013-05-21T18:06:04Z", 
    "summary": "Free-viewpoint video conferencing allows a participant to observe the remote\n3D scene from any freely chosen viewpoint. An intermediate virtual viewpoint\nimage is commonly synthesized using two pairs of transmitted texture and depth\nmaps from two neighboring captured viewpoints via depth-image-based rendering\n(DIBR). To maintain high quality of synthesized images, it is imperative to\ncontain the adverse effects of network packet losses that may arise during\ntexture and depth video transmission. Towards this end, we develop an\nintegrated approach that exploits the representation redundancy inherent in the\nmultiple streamed videos a voxel in the 3D scene visible to two captured views\nis sampled and coded twice in the two views. In particular, at the receiver we\nfirst develop an error concealment strategy that adaptively blends\ncorresponding pixels in the two captured views during DIBR, so that pixels from\nthe more reliable transmitted view are weighted more heavily. We then couple it\nwith a sender-side optimization of reference picture selection (RPS) during\nreal-time video coding, so that blocks containing samples of voxels that are\nvisible in both views are more error-resiliently coded in one view only, given\nadaptive blending will erase errors in the other view. Further, synthesized\nview distortion sensitivities to texture versus depth errors are analyzed, so\nthat relative importance of texture and depth code blocks can be computed for\nsystem-wide RPS optimization. Experimental results show that the proposed\nscheme can outperform the use of a traditional feedback channel by up to 0.82\ndB on average at 8% packet loss rate, and by as much as 3 dB for particular\nframes.", 
    "link": "http://arxiv.org/pdf/1305.5464v1", 
    "arxiv-id": "1305.5464v1"
},{
    "category": "cs.MM", 
    "author": "Md. Maklachur Rahman", 
    "title": "A dwt, dct and svd based watermarking technique to protect the image   piracy", 
    "publish": "2013-07-11T23:35:44Z", 
    "summary": "With the rapid development of information technology and multimedia, the use\nof digital data is increasing day by day. So it becomes very essential to\nprotect multimedia information from piracy and also it is challenging. A great\ndeal of Copyright owners is worried about protecting any kind of illegal\nrepetition of their information. Hence, facing all these kinds of problems\ndevelopment of the techniques is very important. Digital watermarking\nconsidered as a solution to prevent the multimedia data. In this paper, an idea\nof watermarking is proposed and implemented. In proposed watermarking method,\nthe original image is rearranged using zigzag sequence and DWT is applied on\nrearranged image. Then DCT and SVD are applied on all high bands LH, HL and HH.\nWatermark is then embedded by modifying the singular values of these bands.\nExtraction of watermark is performed by the inversion of watermark embedding\nprocess. For choosing of these three bands it gives facility of mid-band and\npure high band that ensures good imperceptibility and more robustness against\ndifferent kinds of attacks.", 
    "link": "http://arxiv.org/pdf/1307.3294v1", 
    "arxiv-id": "1307.3294v1"
},{
    "category": "cs.MM", 
    "author": "Md. Iqbal Hasan Sarker", 
    "title": "Digital Watermarking for Image AuthenticationBased on Combined DCT, DWT   and SVD Transformation", 
    "publish": "2013-07-24T08:32:38Z", 
    "summary": "This paper presents a hybrid digital image watermarking based on Discrete\nWavelet Transform (DWT), Discrete Cosine Transform (DCT) and Singular Value\nDecomposition (SVD) in a zigzag order. From DWT we choose the high band to\nembed the watermark that facilities to add more information, gives more\ninvisibility and robustness against some attacks. Such as geometric attack.\nZigzag method is applied to map DCT coefficients into four quadrants that\nrepresent low, mid and high bands. Finally, SVD is applied to each quadrant.", 
    "link": "http://arxiv.org/pdf/1307.6328v1", 
    "arxiv-id": "1307.6328v1"
},{
    "category": "cs.MM", 
    "author": "Vinay Kumar", 
    "title": "A simple technique for steganography", 
    "publish": "2013-07-31T16:55:35Z", 
    "summary": "A new technique for data hiding in digital image is proposed in this paper.\nSteganography is a well known technique for hiding data in an image, but\ngenerally the format of image plays a pivotal role in it, and the scheme is\nformat dependent. In this paper we will discuss a new technique where\nirrespective of the format of image, we can easily hide a large amount of data\nwithout deteriorating the quality of the image. The data to be hidden is\nenciphered with the help of a secret key. This enciphered data is then embedded\nat the end of the image. The enciphered data bits are extracted and then\ndeciphered with the help of same key used for encryption. Simulation results\nshow that Image Quality Measures of this proposed scheme are better than the\nconventional LSB replacing technique. The proposed method is simple and is easy\nto implement.", 
    "link": "http://arxiv.org/pdf/1307.8385v1", 
    "arxiv-id": "1307.8385v1"
},{
    "category": "cs.MM", 
    "author": "Xiao-Hong Peng", 
    "title": "Model and Performance of a No-Reference Quality Assessment Metric for   Video Streaming", 
    "publish": "2013-08-08T13:10:46Z", 
    "summary": "Video streaming via TCP networks has become a popular and highly demanded\nservice, but its quality assessment in both objective and subjective terms has\nnot been properly addressed. In this paper, based on statistical analysis a\nfull analytic model of a no-reference objective metric, namely Pause Intensity,\nfor video quality assessment is presented. The model characterizes the video\nplayout buffer behavior in connection with the network performance (throughput)\nand the video playout rate. This allows for instant quality measurement and\ncontrol without requiring a reference video. Pause intensity specifically\naddresses the need for assessing the quality issue in terms of the continuity\nin the playout of TCP streaming videos, which cannot be properly measured by\nother objective metrics such as PSNR, SSIM and buffer underrun or pause\nfrequency. The performance of the analytical model is rigidly verified by\nsimulation results and subjective tests using a range of video clips. It is\ndemonstrated that pause intensity is closely correlated with viewer opinion\nscores regardless of the vastly different composition of individual elements,\nsuch as pause duration and pause frequency which jointly constitute this new\nquality metric. It is also shown that the correlation performance of pause\nintensity is consistent and content independent.", 
    "link": "http://arxiv.org/pdf/1308.1839v1", 
    "arxiv-id": "1308.1839v1"
},{
    "category": "cs.MM", 
    "author": "Giulio Giunta", 
    "title": "Compressive Sampling for the Packet Loss Recovery in Audio Multimedia   Streaming", 
    "publish": "2013-08-20T09:02:57Z", 
    "summary": "The aim of this paper is to introduce a new schema, based on a Compressive\nSampling technique, for the recovery of lost data in multimedia streaming. The\naudio streaming data are encapsuled in different packets by using an\ninterleaving technique. The Compressive Sampling technique is used to recover\naudio information in case of lost packets. Experimental results are presented\non speech and musical audio signals to illustrate the performances and the\ncapabilities of the proposed methodology.", 
    "link": "http://arxiv.org/pdf/1308.4263v1", 
    "arxiv-id": "1308.4263v1"
},{
    "category": "cs.MM", 
    "author": "Xiaolin Wu", 
    "title": "Coded Acquisition of High Frame Rate Video", 
    "publish": "2013-08-21T01:13:46Z", 
    "summary": "High frame video (HFV) is an important investigational tool in sciences,\nengineering and military. In ultra-high speed imaging, the obtainable temporal,\nspatial and spectral resolutions are limited by the sustainable throughput of\nin-camera mass memory, the lower bound of exposure time, and illumination\nconditions. In order to break these bottlenecks, we propose a new coded video\nacquisition framework that employs K > 2 conventional cameras, each of which\nmakes random measurements of the 3D video signal in both temporal and spatial\ndomains. For each of the K cameras, this multi-camera strategy greatly relaxes\nthe stringent requirements in memory speed, shutter speed, and illumination\nstrength. The recovery of HFV from these random measurements is posed and\nsolved as a large scale l1 minimization problem by exploiting joint temporal\nand spatial sparsities of the 3D signal. Three coded video acquisition\ntechniques of varied trade offs between performance and hardware complexity are\ndeveloped: frame-wise coded acquisition, pixel-wise coded acquisition, and\ncolumn-row-wise coded acquisition. The performances of these techniques are\nanalyzed in relation to the sparsity of the underlying video signal.\nSimulations of these new HFV capture techniques are carried out and\nexperimental results are reported.", 
    "link": "http://arxiv.org/pdf/1308.4458v1", 
    "arxiv-id": "1308.4458v1"
},{
    "category": "cs.MM", 
    "author": "Huazhong Yang", 
    "title": "Increasing Compression Ratio of Low Complexity Compressive Sensing Video   Encoder with Application-Aware Configurable Mechanism", 
    "publish": "2013-11-06T15:24:17Z", 
    "summary": "With the development of embedded video acquisition nodes and wireless video\nsurveillance systems, traditional video coding methods could not meet the needs\nof less computing complexity any more, as well as the urgent power consumption.\nSo, a low-complexity compressive sensing video encoder framework with\napplication-aware configurable mechanism is proposed in this paper, where novel\nencoding methods are exploited based on the practical purposes of the real\napplications to reduce the coding complexity effectively and improve the\ncompression ratio (CR). Moreover, the group of processing (GOP) size and the\nmeasurement matrix size can be configured on the encoder side according to the\npost-analysis requirements of an application example of object tracking to\nincrease the CR of encoder as best as possible. Simulations show the proposed\nframework of encoder could achieve 60X of CR when the tracking successful rate\n(SR) is still keeping above 90%.", 
    "link": "http://arxiv.org/pdf/1311.1419v1", 
    "arxiv-id": "1311.1419v1"
},{
    "category": "cs.MM", 
    "author": "Dattatray Bade", 
    "title": "Image Steganography using Karhunen-Loeve Transform and Least Bit   Substitution", 
    "publish": "2013-11-07T14:45:30Z", 
    "summary": "As communication channels are increasing in number, reliability of faithful\ncommunication is reducing. Hacking and tempering of data are two major issues\nfor which security should be provided by channel. This raises the importance of\nsteganography. In this paper, a novel method to encode the message information\ninside a carrier image has been described. It uses Karhunen-Lo\\`eve Transform\nfor compression of data and Least Bit Substitution for data encryption.\nCompression removes redundancy and thus also provides encoding to a level. It\nis taken further by means of Least Bit Substitution. The algorithm used for\nthis purpose uses pixel matrix which serves as a best tool to work on. Three\ndifferent sets of images were used with three different numbers of bits to be\nsubstituted by message information. The experimental results show that\nalgorithm is time efficient and provides high data capacity. Further, it can\ndecrypt the original data effectively. Parameters such as carrier error and\nmessage error were calculated for each set and were compared for performance\nanalysis.", 
    "link": "http://arxiv.org/pdf/1311.1700v1", 
    "arxiv-id": "1311.1700v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Optimal Foresighted Multi-User Wireless Video", 
    "publish": "2013-11-17T22:37:16Z", 
    "summary": "Recent years have seen an explosion in wireless video communication systems.\nOptimization in such systems is crucial - but most existing methods intended to\noptimize the performance of multi-user wireless video transmission are\ninefficient. Some works (e.g. Network Utility Maximization (NUM)) are myopic:\nthey choose actions to maximize instantaneous video quality while ignoring the\nfuture impact of these actions. Such myopic solutions are known to be inferior\nto foresighted solutions that optimize the long-term video quality.\nAlternatively, foresighted solutions such as rate-distortion optimized packet\nscheduling focus on single-user wireless video transmission, while ignoring the\nresource allocation among the users.\n  In this paper, we propose an optimal solution for performing joint\nforesighted resource allocation and packet scheduling among multiple users\ntransmitting video over a shared wireless network. A key challenge in\ndeveloping foresighted solutions for multiple video users is that the users'\ndecisions are coupled. To decouple the users' decisions, we adopt a novel dual\ndecomposition approach, which differs from the conventional optimization\nsolutions such as NUM, and determines foresighted policies. Specifically, we\npropose an informationally-decentralized algorithm in which the network manager\nupdates resource \"prices\" (i.e. the dual variables associated with the resource\nconstraints), and the users make individual video packet scheduling decisions\nbased on these prices. Because a priori knowledge of the system dynamics is\nalmost never available at run-time, the proposed solution can learn online,\nconcurrently with performing the foresighted optimization. Simulation results\nshow 7 dB and 3 dB improvements in Peak Signal-to-Noise Ratio (PSNR) over\nmyopic solutions and existing foresighted solutions, respectively.", 
    "link": "http://arxiv.org/pdf/1311.4227v1", 
    "arxiv-id": "1311.4227v1"
},{
    "category": "cs.MM", 
    "author": "Lina J. Karam", 
    "title": "Traffic and Statistical Multiplexing Characterization of 3D Video   Representation Formats (Extended Version)", 
    "publish": "2013-11-22T18:09:25Z", 
    "summary": "The network transport of 3D video, which contains two views of a video scene,\nposes significant challenges due to the increased video data compared to\nconventional single-view video. Addressing these challenges requires a thorough\nunderstanding of the traffic and multiplexing characteristics of the different\nrepresentation formats of 3D video. We examine the average bitrate-distortion\n(RD) and bitrate variability-distortion (VD) characteristics of three main\nrepresentation formats. Specifically, we compare multiview video (MV)\nrepresentation and encoding, frame sequential (FS) representation, and\nside-by-side (SBS) representation, whereby conventional single-view encoding is\nemployed for the FS and SBS representations. Our results for long 3D videos in\nfull HD format indicate that the MV representation and encoding achieves the\nhighest RD efficiency, while exhibiting the highest bitrate variabilities. We\nexamine the impact of these bitrate variabilities on network transport through\nextensive statistical multiplexing simulations. We find that when multiplexing\na small number of streams, the MV and FS representations require the same\nbandwidth. However, when multiplexing a large number of streams or smoothing\ntraffic, the MV representation and encoding reduces the bandwidth requirement\nrelative to the FS representation.", 
    "link": "http://arxiv.org/pdf/1311.5834v1", 
    "arxiv-id": "1311.5834v1"
},{
    "category": "cs.MM", 
    "author": "Alan C. Bovik", 
    "title": "Modeling the Time-varying Subjective Quality of HTTP Video Streams with   Rate Adaptations", 
    "publish": "2013-11-25T20:31:44Z", 
    "summary": "Newly developed HTTP-based video streaming technologies enable flexible\nrate-adaptation under varying channel conditions. Accurately predicting the\nusers' Quality of Experience (QoE) for rate-adaptive HTTP video streams is thus\ncritical to achieve efficiency. An important aspect of understanding and\nmodeling QoE is predicting the up-to-the-moment subjective quality of a video\nas it is played, which is difficult due to hysteresis effects and\nnonlinearities in human behavioral responses. This paper presents a\nHammerstein-Wiener model for predicting the time-varying subjective quality\n(TVSQ) of rate-adaptive videos. To collect data for model parameterization and\nvalidation, a database of longer-duration videos with time-varying distortions\nwas built and the TVSQs of the videos were measured in a large-scale subjective\nstudy. The proposed method is able to reliably predict the TVSQ of rate\nadaptive videos. Since the Hammerstein-Wiener model has a very simple\nstructure, the proposed method is suitable for on-line TVSQ prediction in HTTP\nbased streaming.", 
    "link": "http://arxiv.org/pdf/1311.6441v1", 
    "arxiv-id": "1311.6441v1"
},{
    "category": "cs.MM", 
    "author": "Robert W. Heath Jr", 
    "title": "Rate Adaptation and Admission Control for Video Transmission with   Subjective Quality Constraints", 
    "publish": "2013-11-25T20:42:33Z", 
    "summary": "Adapting video data rate during streaming can effectively reduce the risk of\nplayback interruptions caused by channel throughput fluctuations. The\nvariations in rate, however, also introduce video quality fluctuations and thus\npotentially affects viewers' Quality of Experience (QoE). We show how the QoE\nof video users can be improved by rate adaptation and admission control. We\nconducted a subjective study wherein we found that viewers' QoE was strongly\ncorrelated with the empirical cumulative distribution function (eCDF) of the\npredicted video quality. Based on this observation, we propose a\nrate-adaptation algorithm that can incorporate QoE constraints on the empirical\ncumulative quality distribution per user. We then propose a threshold-based\nadmission control policy to block users whose empirical cumulative quality\ndistribution is not likely to satisfy their QoE constraint. We further devise\nan online adaptation algorithm to automatically optimize the threshold.\nExtensive simulation results show that the proposed scheme can reduce network\nresource consumption by $40\\%$ over conventional average-quality maximized\nrate-adaptation algorithms.", 
    "link": "http://arxiv.org/pdf/1311.6453v1", 
    "arxiv-id": "1311.6453v1"
},{
    "category": "cs.MM", 
    "author": "Radu Arsinte", 
    "title": "A Study on the Optimal Implementation of Statistical Multiplexing in DVB   Distribution Systems", 
    "publish": "2014-02-04T18:05:37Z", 
    "summary": "The paper presents an overview of the main methods used to improve the\nefficiency of DVB systems, based on multiplexing, through a study on the impact\nof the multiplexing methods used in DVB, having as a final goal a better usage\nof the data capacity and the possibility to insert new services into the\noriginal DVB Transport Stream. This study revealed that not all DVB providers\nare using statistical multiplexing. Based on this study, we were able to\npropose a method to improve the original DVB stream, originated from DVB-S or\nDVB-T providers. This method is proposing the detection of null packets,\nremoval and reinserting a new service, with a VBR content. The method developed\nin this research can be implemented even in optimized statistical multiplexing\nsystems, due to a residual use of null packets for data rate adjustment. There\nis no need to have access in the original stream multiplexer, since the method\nallows the implementation on the fly, near to the end user. The proposed method\nis proposed to be applied in DVB-S to DVB-C translation, using the computing\npower of a PC or in a FPGA implementation.", 
    "link": "http://arxiv.org/pdf/1402.0812v1", 
    "arxiv-id": "1402.0812v1"
},{
    "category": "cs.MM", 
    "author": "Vishal Monga", 
    "title": "Twofold Video Hashing with Automatic Synchronization", 
    "publish": "2014-02-21T21:02:01Z", 
    "summary": "Video hashing finds a wide array of applications in content authentication,\nrobust retrieval and anti-piracy search. While much of the existing research\nhas focused on extracting robust and secure content descriptors, a significant\nopen challenge still remains: Most existing video hashing methods are fallible\nto temporal desynchronization. That is, when the query video results by\ndeleting or inserting some frames from the reference video, most existing\nmethods assume the positions of the deleted (or inserted) frames are either\nperfectly known or reliably estimated. This assumption may be okay under\ntypical transcoding and frame-rate changes but is highly inappropriate in\nadversarial scenarios such as anti-piracy video search. For example, an illegal\nuploader will try to bypass the 'piracy check' mechanism of YouTube/Dailymotion\netc by performing a cleverly designed non-uniform resampling of the video. We\npresent a new solution based on dynamic time warping (DTW), which can implement\nautomatic synchronization and can be used together with existing video hashing\nmethods. The second contribution of this paper is to propose a new robust\nfeature extraction method called flow hashing (FH), based on frame averaging\nand optical flow descriptors. Finally, a fusion mechanism called distance\nboosting is proposed to combine the information extracted by DTW and FH.\nExperiments on real video collections show that such a hash extraction and\ncomparison enables unprecedented robustness under both spatial and temporal\nattacks.", 
    "link": "http://arxiv.org/pdf/1402.5422v1", 
    "arxiv-id": "1402.5422v1"
},{
    "category": "cs.MM", 
    "author": "Reza Rahimi", 
    "title": "A Methodology for Implementation of MMS Client on Embedded Platforms", 
    "publish": "2014-03-17T16:42:43Z", 
    "summary": "MMS (Multimedia Messaging Service) is the next generation of messaging\nservices in multimedia mobile communications. MMS enables messaging with full\nmultimedia content including images, audios, videos, texts and data, from\nclient to client or e-mail. MMS is based on WAP technology, so it is technology\nindependent. This means that enabling messages from a GSM/GPRS network to be\nsent to a TDMA or WCDMA network. In this paper a methodology for implementing\nMMS client on embedded platforms especially on Wince OS is described.", 
    "link": "http://arxiv.org/pdf/1403.4158v1", 
    "arxiv-id": "1403.4158v1"
},{
    "category": "cs.MM", 
    "author": "Shanthakumari R", 
    "title": "An Adaptive Watermarking Process in Hadamard Transform", 
    "publish": "2014-05-12T16:45:21Z", 
    "summary": "An adaptive visible/invisible watermarking scheme is done to prevent the\nprivacy and preserving copyright protection of digital data using Hadamard\ntransform based on the scaling factor of the image. The value of scaling factor\ndepends on the control parameter. The scaling factor is calculated to embedded\nthe watermark. Depend upon the control parameter the visible and invisible\nwatermarking is determined. The proposed Hadamard transform domain method is\nmore robust again image/signal processing attacks. Furthermore, it also shows\nthat the proposed method confirm the efficiency through various performance\nanalysis and experimental results.", 
    "link": "http://arxiv.org/pdf/1405.3207v1", 
    "arxiv-id": "1405.3207v1"
},{
    "category": "cs.MM", 
    "author": "Mari Carmen Aguayo-Torres", 
    "title": "YouTube QoE Evaluation Tool for Android Wireless Terminals", 
    "publish": "2014-05-19T13:10:09Z", 
    "summary": "In this paper, we present an Android application which is able to evaluate\nand analyze the perceived Quality of Experience (QoE) for YouTube service in\nwireless terminals. To achieve this goal, the application carries out\nmeasurements of objective Quality of Service (QoS) parameters, which are then\nmapped onto subjective QoE (in terms of Mean Opinion Score, MOS) by means of a\nutility function. Our application also informs the user about potential causes\nthat lead to a low MOS as well as provides some hints to improve it. After each\nYouTube session, the users may optionally qualify the session through an online\nopinion survey. This information has been used in a pilot experience to\ncorrelate the theoretical QoE model with real user feedback. Results from such\nan experience have shown that the theoretical model (taken from the literature)\nprovides slightly more pessimistic results compared to user feedback. Users\nseem to be more indulgent with wireless connections, increasing the MOS from\nthe opinion survey in about 20% compared to the theoretical model, which was\nobtained from wired scenarios.", 
    "link": "http://arxiv.org/pdf/1405.4709v1", 
    "arxiv-id": "1405.4709v1"
},{
    "category": "cs.MM", 
    "author": "Diego Oliva", 
    "title": "Block matching algorithm based on Differential Evolution for motion   estimation", 
    "publish": "2014-05-16T19:41:14Z", 
    "summary": "Motion estimation is one of the major problems in developing video coding\napplications. Among all motion estimation approaches, Block matching (BM)\nalgorithms are the most popular methods due to their effectiveness and\nsimplicity for both software and hardware implementations. A BM approach\nassumes that the movement of pixels within a defined region of the current\nframe (Macro-Block, MB) can be modeled as a translation of pixels contained in\nthe previous frame. In this procedure, the motion vector is obtained by\nminimizing the sum of absolute differences (SAD) produced by the MB of the\ncurrent frame over a determined search window from the previous frame. The SAD\nevaluation is computationally expensive and represents the most consuming\noperation in the BM process. The most straightforward BM method is the full\nsearch algorithm (FSA) which finds the most accurate motion vector, calculating\nexhaustively the SAD values for all elements of the search window. Over this\ndecade, several fast BM algorithms have been proposed to reduce the number of\nSAD operations by calculating only a fixed subset of search locations at the\nprice of a poor accuracy. In this paper, a new algorithm based on Differential\nEvolution (DE) is proposed to reduce the number of search locations in the BM\nprocess. In order to avoid computing several search locations, the algorithm\nestimates the SAD values (fitness) for some locations using the SAD values of\npreviously calculated neighboring positions. Since the proposed algorithm does\nnot consider any fixed search pattern or other different assumption, a high\nprobability for finding the true minimum (accurate motion vector) is expected.\nIn comparison to other fast BM algorithms, the proposed method deploys more\naccurate motion vectors yet delivering competitive time rates.", 
    "link": "http://arxiv.org/pdf/1405.4721v1", 
    "arxiv-id": "1405.4721v1"
},{
    "category": "cs.MM", 
    "author": "J P Gupta", 
    "title": "A hybrid video quality metric for analyzing quality degradation due to   frame drop", 
    "publish": "2014-05-21T09:01:32Z", 
    "summary": "In last decade, ever growing internet technologies provided platform to share\nthe multimedia data among different communities. As the ultimate users are\nhuman subjects who are concerned about quality of visual information, it is\noften desired to have good resumed perceptual quality of videos, thus arises\nthe need of quality assessment. This paper presents a full reference hybrid\nvideo quality metric which is capable to analyse the video quality for\nspatially or temporally (frame drop) or spatio-temporally distorted video\nsequences. Simulated results show that the metric efficiently analyses the\nquality degradation and more closer to the developed human visual system", 
    "link": "http://arxiv.org/pdf/1405.5340v1", 
    "arxiv-id": "1405.5340v1"
},{
    "category": "cs.MM", 
    "author": "Huazhong Yang", 
    "title": "Low-complexity video encoder for smart eyes based on underdetermined   blind signal separation", 
    "publish": "2014-05-23T02:24:11Z", 
    "summary": "This paper presents a low complexity video coding method based on\nUnderdetermined Blind Signal Separation (UBSS). The detailed coding framework\nis designed. Three key techniques are proposed to enhance the compression ratio\nand the quality of the decoded frames. The experiments validate that the\nproposed method costs 30ms encoding time less than DISCOVER. The simulation\nshows that this new method can save 50% energy compared with H.264.", 
    "link": "http://arxiv.org/pdf/1405.5948v1", 
    "arxiv-id": "1405.5948v1"
},{
    "category": "cs.MM", 
    "author": "Wesam Ahmed", 
    "title": "Jpeg Image Compression Using Discrete Cosine Transform - A Survey", 
    "publish": "2014-05-08T08:04:21Z", 
    "summary": "Due to the increasing requirements for transmission of images in computer,\nmobile environments, the research in the field of image compression has\nincreased significantly. Image compression plays a crucial role in digital\nimage processing, it is also very important for efficient transmission and\nstorage of images. When we compute the number of bits per image resulting from\ntypical sampling rates and quantization methods, we find that Image compression\nis needed. Therefore development of efficient techniques for image compression\nhas become necessary .This paper is a survey for lossy image compression using\nDiscrete Cosine Transform, it covers JPEG compression algorithm which is used\nfor full-colour still image applications and describes all the components of\nit.", 
    "link": "http://arxiv.org/pdf/1405.6147v1", 
    "arxiv-id": "1405.6147v1"
},{
    "category": "cs.MM", 
    "author": "D H Rao", 
    "title": "A scenario based approach for dealing with challenges in a pervasive   computing environment", 
    "publish": "2014-05-08T07:16:46Z", 
    "summary": "With the surge in modern research focus towards Pervasive Computing, lot of\ntechniques and challenges needs to be addressed so as to effectively create\nsmart spaces and achieve miniaturization. In the process of scaling down to\ncompact devices, the real things to ponder upon are the Information Retrieval\nchallenges. In this work, we discuss the aspects of multimedia which makes\ninformation access challenging. An Example Pattern Recognition scenario is\npresented and the mathematical techniques that can be used to model uncertainty\nare also presented for developing a system that can sense, compute and\ncommunicate in a way that can make human life easy with smart objects assisting\nfrom around his surroundings.", 
    "link": "http://arxiv.org/pdf/1405.6661v1", 
    "arxiv-id": "1405.6661v1"
},{
    "category": "cs.MM", 
    "author": "Jiwu Huang", 
    "title": "JPEG Noises beyond the First Compression Cycle", 
    "publish": "2014-05-29T14:43:55Z", 
    "summary": "This paper focuses on the JPEG noises, which include the quantization noise\nand the rounding noise, during a JPEG compression cycle. The JPEG noises in the\nfirst compression cycle have been well studied; however, so far less attention\nhas been paid on the JPEG noises in higher compression cycles. In this work, we\npresent a statistical analysis on JPEG noises beyond the first compression\ncycle. To our knowledge, this is the first work on this topic. We find that the\nnoise distributions in higher compression cycles are different from those in\nthe first compression cycle, and they are dependent on the quantization\nparameters used between two successive cycles. To demonstrate the benefits from\nthe statistical analysis, we provide two applications that can employ the\nderived noise distributions to uncover JPEG compression history with\nstate-of-the-art performance.", 
    "link": "http://arxiv.org/pdf/1405.7571v1", 
    "arxiv-id": "1405.7571v1"
},{
    "category": "cs.MM", 
    "author": "Samir Medjiah", 
    "title": "QoE assessment for SVC streaming in ENVISION", 
    "publish": "2014-05-28T17:15:51Z", 
    "summary": "Scalable video coding has drawn great interest in content delivery in many\nmultimedia services thanks to its capability to handle terminal heterogeneity\nand network conditions variation. In our previous work, and under the umbrella\nof ENVISION, we have proposed a playout smoothing mechanism to ensure the\nuniform delivery of the layered stream, by reducing the quality changes that\nthe stream undergoes when adapting to changing network conditions. In this\npaper we study the resulting video quality, from the final user perception\nunder different network conditions of loss and delays. For that we have adopted\nthe Double Stimulus Impairment Scale (DSIS) method. The results show that the\nMean Opinion Score for the smoothed video clips was higher under different\nnetwork configuration. This confirms the effectiveness of the proposed\nsmoothing mechanism.", 
    "link": "http://arxiv.org/pdf/1405.7629v1", 
    "arxiv-id": "1405.7629v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "Perceptual Quality of Video with Periodic Frame Rate and Quantization   Variation-Subjective Studies and Analytical Modeling", 
    "publish": "2014-06-08T19:37:06Z", 
    "summary": "In networked video applications, the frame rate (FR) and quantization\nstepsize (QS) of a compressed video are often adapted in response to the\nchanges of the available bandwidth. It is important to understand how do the\nvariation of FR and QS and their variation pattern affect the video quality. In\nthis paper, we investigate the impact of temporal variation of FR and QS on the\nperceptual video quality. Among all possible variation patterns, we focus on\nvideos in which two FR's (or QS's) alternate over a fixed interval. We explore\nthe human responses to such variation by conducting subjective evaluation of\ntest videos with different variation magnitudes and frequencies. We further\nanalyze statistical significance of the impact of variation magnitude,\nvariation frequency, video content, and their interactions. By analyzing the\nsubjective ratings, we propose two models for predicting the quality of video\nwith alternating FR and QS, respectively, The proposed models have simple\nmathematical forms with a few content-dependent parameters. The models fit the\nmeasured data very well using parameters determined by least square fitting\nwith the measured data. We further propose some guidelines for adaptation of FR\nand QS based on trends observed from subjective test results.", 
    "link": "http://arxiv.org/pdf/1406.2018v1", 
    "arxiv-id": "1406.2018v1"
},{
    "category": "cs.MM", 
    "author": "Putra Sumari", 
    "title": "real-time audio translation module between iax and rsw", 
    "publish": "2014-06-11T05:09:59Z", 
    "summary": "At the last few years, multimedia communication has been developed and\nimproved rapidly in order to enable users to communicate between each other\nover the internet. Generally, multimedia communication consists of audio and\nvideo communication. However, this research concentrates on audio conferencing\nonly. The audio translation between protocols is a very critical issue, because\nit solves the communication problems between any two protocols. So, it enables\npeople around the world to talk with each other even they use different\nprotocols. In this research, a real time audio translation module between two\nprotocols has been done. These two protocols are: InterAsterisk eXchange\nProtocol (IAX) and Real-Time Switching Control Protocol (RSW), which they are\nwidely used to provide two ways audio transfer feature. The solution here is to\nprovide inter-working between the two protocols which they have different media\ntransports, audio codecs, header formats and different transport protocols for\nthe audio transmission. This translation will help bridging the gap between the\ntwo protocols by providing inter-working capability between the two audio\nstreams of IAX and RSW. Some related works have been done to provide\ntranslation between IAX and RSW control signalling messages. But, this research\npaper concentrates on the translation that depends on the media transfer. The\nproposed translation module was tested and evaluated in different scenarios in\norder to examine its performance. The obtained results showed that the\nReal-Time Audio Translation Module produces lower rates of packet delay and\njitter than the acceptance values for each of the mentioned performance\nmetrics.", 
    "link": "http://arxiv.org/pdf/1406.2779v1", 
    "arxiv-id": "1406.2779v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Optimized Adaptive Streaming Representations based on System Dynamics", 
    "publish": "2014-06-12T09:24:32Z", 
    "summary": "Adaptive streaming addresses the increasing and heterogenous demand of\nmultimedia content over the Internet by offering several encoded versions for\neach video sequence. Each version (or representation) has a different\nresolution and bit rate, aimed at a specific set of users, like TV or mobile\nphone clients. While most existing works on adaptive streaming deal with\neffective playout-control strategies at the client side, we take in this paper\na providers' perspective and propose solutions to improve user satisfaction by\noptimizing the encoding rates of the video sequences. We formulate an integer\nlinear program that maximizes users' average satisfaction, taking into account\nthe network dynamics, the video content information, and the user population\ncharacteristics. The solution of the optimization is a set of encoding\nparameters that permit to create different streams to robustly satisfy users'\nrequests over time. We simulate multiple adaptive streaming sessions\ncharacterized by realistic network connections models, where the proposed\nsolution outperforms commonly used vendor recommendations, in terms of user\nsatisfaction but also in terms of fairness and outage probability. The\nsimulation results further show that video content information as well as\nnetwork constraints and users' statistics play a crucial role in selecting\nproper encoding parameters to provide fairness a mong users and to reduce\nnetwork resource usage. We finally propose a few practical guidelines that can\nbe used to choose the encoding parameters based on the user base\ncharacteristics, the network capacity and the type of video content.", 
    "link": "http://arxiv.org/pdf/1406.3161v2", 
    "arxiv-id": "1406.3161v2"
},{
    "category": "cs.MM", 
    "author": "Ganga Holi", 
    "title": "Securing Medical Images by Watermarking Using DWT DCT and SVD", 
    "publish": "2014-06-26T18:27:36Z", 
    "summary": "Telemedicine is well known application where enormous amount of medical data\nneed to be transferred securely over network and manipulate effectively.\nSecurity of digital data, especially medical images, becomes important for many\nreasons such as confidentiality, authentication and integrity. Digital\nwatermarking has emerged as a advanced technology to enhance the security of\ndigital images. The insertion of watermark in medical images can authenticate\nit and guarantee its integrity. The watermark must be generally hidden does not\naffect the quality of the medical image. In this paper, we propose blind\nwatermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine\nTransform (DCT) and Singular Value Decomposition (SVD), we compare the\nperformance of this technique with watermarking based DWT and SVD. The proposed\nmethod DWT, DCT and SVD comparatively better than DWT and SVD method.", 
    "link": "http://arxiv.org/pdf/1406.7226v1", 
    "arxiv-id": "1406.7226v1"
},{
    "category": "cs.MM", 
    "author": "Rohit Tanwar", 
    "title": "Genetic Algorithm in Audio Steganography", 
    "publish": "2014-07-10T09:07:03Z", 
    "summary": "With the advancement of communication technology,data is exchanged digitally\nover the network. At the other side the technology is also proven as a tool for\nunauthorized access to attackers. Thus the security of data to be transmitted\ndigitally should get prime focus. Data hiding is the common approach to secure\ndata. In steganography technique, the existence of data is concealed. GA is an\nemerging component of AI to provide suboptimal solutions. In this paper the use\nof GA in Steganography is explored to find future scope of research.", 
    "link": "http://arxiv.org/pdf/1407.2729v1", 
    "arxiv-id": "1407.2729v1"
},{
    "category": "cs.MM", 
    "author": "Vishal Shrivastava", 
    "title": "A Survey of Digital Watermarking Techniques and its Applications", 
    "publish": "2014-07-17T16:56:15Z", 
    "summary": "Digital media is the need of a people now a day as the alternate of paper\nmedia.As the technology grown up digital media required protection while\ntransferring through internet or others mediums.Watermarking techniques have\nbeen developed to fulfill this requirement.This paper aims to provide a\ndetailed survey of all watermarking techniques specially focuses on image\nwatermarking types and its applications in today world.", 
    "link": "http://arxiv.org/pdf/1407.4735v1", 
    "arxiv-id": "1407.4735v1"
},{
    "category": "cs.MM", 
    "author": "Jay Prakash Gupta", 
    "title": "Robust Lossless Semi Fragile Information Protection in Images", 
    "publish": "2014-07-18T01:37:21Z", 
    "summary": "Internet security finds it difficult to keep the information secure and to\nmaintain the integrity of the data. Sending messages over the internet secretly\nis one of the major tasks as it is widely used for passing the message.", 
    "link": "http://arxiv.org/pdf/1407.4865v4", 
    "arxiv-id": "1407.4865v4"
},{
    "category": "cs.MM", 
    "author": "M. C. Adhikary", 
    "title": "An Easy yet Effective Method for Detecting Spatial Domain LSB   Steganography", 
    "publish": "2014-07-25T12:58:23Z", 
    "summary": "Digitization of image was a revolutionary step for the fields of photography\nand Image processing as this made the editing of images much effortless and\neasier. Image editing was not an issue until it was limited to corrective\nediting procedures used to enhance the quality of an image such as, contrast\nstretching, noise filtering, sharpening etc. But, it became a headache for many\nfields when image editing became manipulative. Digital images have become an\neasier source of tampering and forgery during last few decades. Today users and\nediting specialists, equipped with easily available image editing software,\nmanipulate digital images with varied goals. Photo journalists often tamper\nphotographs to give dramatic effect to their stories. Scientists and\nresearchers use this trick to get theirs works published. Patients' diagnoses\nare misrepresented by manipulating medical imageries. Lawyers and Politicians\nuse tampered images to direct the opinion of people or court to their favor.\nTerrorists, anti-social groups use manipulated Stego images for secret\ncommunication. In this paper we present an effective method for detecting\nspatial domain Steganography.", 
    "link": "http://arxiv.org/pdf/1407.6877v1", 
    "arxiv-id": "1407.6877v1"
},{
    "category": "cs.MM", 
    "author": "M. C. Adhikary", 
    "title": "Detection of Clones in Digital Images", 
    "publish": "2014-07-25T13:00:50Z", 
    "summary": "During the recent years, tampering of digital images has become a general\nhabit among people and professionals. As a result, establishment of image\nauthenticity has become a key issue in fields those make use of digital images.\nAuthentication of an image involves separation of original camera outputs from\ntheir tampered or Stego counterparts. Digital image cloning being a popular\ntype of image tampering, in this paper we have experimentally analyzed seven\ndifferent algorithms of cloning detection such as the simple overlapped block\nmatching with lexicographic sorting (SOBMwLS) algorithm, block matching with\ndiscrete cosine transformation, principal component analysis, discrete wavelet\ntransformation and singular value decomposition performed on the blocks (DCT,\nDWT, PCA, SVD), two combination models where, DCT and DWT are combined with\nsingular value decomposition (DCTSVD and DWTSVD. A comparative study of all\nthese techniques with respect to their time complexities and robustness of\ndetection against various post processing operations such as cropping,\nbrightness and contrast adjustments are presented in the paper.", 
    "link": "http://arxiv.org/pdf/1407.6879v1", 
    "arxiv-id": "1407.6879v1"
},{
    "category": "cs.MM", 
    "author": "Yuan Yao", 
    "title": "Fast Adaptive Algorithm for Robust Evaluation of Quality of Experience", 
    "publish": "2014-07-29T06:20:42Z", 
    "summary": "Outlier detection is an integral part of robust evaluation for\ncrowdsourceable Quality of Experience (QoE) and has attracted much attention in\nrecent years. In QoE for multimedia, outliers happen because of different test\nconditions, human errors, abnormal variations in context, {etc}. In this paper,\nwe propose a simple yet effective algorithm for outlier detection and robust\nQoE evaluation named iterative Least Trimmed Squares (iLTS). The algorithm\nassigns binary weights to samples, i.e., 0 or 1 indicating if a sample is an\noutlier, then the outlier-trimmed subset least squares solutions give robust\nranking scores. An iterative optimization is carried alternatively between\nupdating weights and ranking scores which converges to a local optimizer in\nfinite steps. In our test setting, iLTS is up to 190 times faster than\nLASSO-based methods with a comparable performance. Moreover, a varied version\nof this method shows adaptation in outlier detection, which provides an\nautomatic detection to determine whether a data sample is an outlier without\n\\emph{a priori} knowledge about the amount of the outliers. The effectiveness\nand efficiency of iLTS are demonstrated on both simulated examples and\nreal-world applications. A Matlab package is provided to researchers exploiting\ncrowdsourcing paired comparison data for robust ranking.", 
    "link": "http://arxiv.org/pdf/1407.7636v2", 
    "arxiv-id": "1407.7636v2"
},{
    "category": "cs.MM", 
    "author": "Christian Wolff", 
    "title": "Impact of video quality and wireless network interface on power   consumption of mobile devices", 
    "publish": "2014-07-29T09:10:05Z", 
    "summary": "During the last years, many improvements were made to the hardware capability\nof mobile devices. As mobile software also became more interactive and data\nprocessing intensive, the increased power demand could not be compensated by\nthe improvements on battery technology. Adaptive systems can help to balance\nthe demand of applications with the limitations of battery resources. For\neffective systems, the influence of multimedia quality on power consumption of\nthe components of mobile devices needs to be better understood. In this paper,\nwe analyze the impact of video quality and wireless network type on the energy\nconsumption of a mobile device. We have found that the additional power\nconsumption is up to 38% higher when a movie is played over a WiFi network\ninstead from internal memory and 64% higher in case of a mobile network (3G).\nWe have also discovered that a higher movie quality not only affects the power\nconsumption of the CPU but also the power consumption of the WiFi unit by up to\n58% and up to 72% respectively on mobile networks.", 
    "link": "http://arxiv.org/pdf/1407.7667v2", 
    "arxiv-id": "1407.7667v2"
},{
    "category": "cs.MM", 
    "author": "Mike Stannett", 
    "title": "Developing a Video Steganography Toolkit", 
    "publish": "2014-09-17T07:33:46Z", 
    "summary": "Although techniques for separate image and audio steganography are widely\nknown, relatively little has been described concerning the hiding of\ninformation within video streams (\"video steganography\"). In this paper we\nreview the current state of the art in this field, and describe the key issues\nwe have encountered in developing a practical video steganography system. A\nsupporting video is also available online at\nhttp://www.youtube.com/watch?v=YhnlHmZolRM", 
    "link": "http://arxiv.org/pdf/1409.4883v1", 
    "arxiv-id": "1409.4883v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "A Novel No-reference Video Quality Metric for Evaluating Temporal   Jerkiness due to Frame Freezing", 
    "publish": "2014-11-05T16:29:30Z", 
    "summary": "In this work, we propose a novel no-reference (NR) video quality metric that\nevaluates the impact of frame freezing due to either packet loss or late\narrival. Our metric uses a trained neural network acting on features that are\nchosen to capture the impact of frame freezing on the perceived quality. The\nconsidered features include the number of freezes, freeze duration statistics,\ninter-freeze distance statistics, frame difference before and after the freeze,\nnormal frame difference, and the ratio of them. We use the neural network to\nfind the mapping between features and subjective test scores. We optimize the\nnetwork structure and the feature selection through a cross validation\nprocedure, using training samples extracted from both VQEG and LIVE video\ndatabases. The resulting feature set and network structure yields accurate\nquality prediction for both the training data containing 54 test videos and a\nseparate testing dataset including 14 videos, with Pearson Correlation\nCoefficients greater than 0.9 and 0.8 for the training set and the testing set,\nrespectively. Our proposed metric has low complexity and could be utilized in a\nsystem with realtime processing constraint.", 
    "link": "http://arxiv.org/pdf/1411.1705v1", 
    "arxiv-id": "1411.1705v1"
},{
    "category": "cs.MM", 
    "author": "Pedro M. Q. Aguiar", 
    "title": "Maximizing compression efficiency through block rotation", 
    "publish": "2014-11-16T18:52:54Z", 
    "summary": "The Discrete Cosine Transform (DCT) is widely used in lossy image and video\ncompression schemes, e.g., JPEG and MPEG. In this paper, we show that the\ncompression efficiency of the DCT is dependent on the edge directions within a\nblock. In particular, higher compression ratios are achieved when edges are\naligned with the image axes. To maximize compression for general images, we\npropose a rotated block DCT method. It consists of rotating each block, before\napplying the DCT, by an angle that aligns the edges, and rotating back the\nblock in the decompression stage. We show how to compute the rotation angle and\nanalyze two alternative block rotation approaches. Our experiments show that\nour method enables both a perceptual improvement and a PSNR increase of up to\n2dB, compared with the standard DCT, for low and medium bit rates.", 
    "link": "http://arxiv.org/pdf/1411.4290v1", 
    "arxiv-id": "1411.4290v1"
},{
    "category": "cs.MM", 
    "author": "Xin Gao", 
    "title": "A Tag Identification Approach Based On Fragile Watermark", 
    "publish": "2014-11-25T17:23:19Z", 
    "summary": "This paper proposes a tag identify approach based on fragile Watermark that\nbased on Least significant bit of the replacement that we first use a special\nway to initialize the cover to ensure that we can use random positions to embed\nthe information of tag. Using this way enhance the security of other to get the\nright information of this tag. Finally as long as the covered information can\nbe decoded, the completeness and accuracy of the tag information can be\nguaranteed. the result of simulation experiment show that this approach has\nhigh sensitivity and security .", 
    "link": "http://arxiv.org/pdf/1411.6928v1", 
    "arxiv-id": "1411.6928v1"
},{
    "category": "cs.MM", 
    "author": "P. P. Sarkar", 
    "title": "A novel technique for image steganography based on Block-DCT and Huffman   Encoding", 
    "publish": "2010-06-07T06:59:18Z", 
    "summary": "Image steganography is the art of hiding information into a cover image. This\npaper presents a novel technique for Image steganography based on Block-DCT,\nwhere DCT is used to transform original image (cover image) blocks from spatial\ndomain to frequency domain. Firstly a gray level image of size M x N is divided\ninto no joint 8 x 8 blocks and a two dimensional Discrete Cosine Transform (2-d\nDCT) is performed on each of the P = MN / 64 blocks. Then Huffman encoding is\nalso performed on the secret messages/images before embedding and each bit of\nHuffman code of secret message/image is embedded in the frequency domain by\naltering the least significant bit of each of the DCT coefficients of cover\nimage blocks. The experimental results show that the algorithm has a high\ncapacity and a good invisibility. Moreover PSNR of cover image with stego-image\nshows the better results in comparison with other existing steganography\napproaches. Furthermore, satisfactory security is maintained since the secret\nmessage/image cannot be extracted without knowing decoding rules and Huffman\ntable.", 
    "link": "http://arxiv.org/pdf/1006.1186v1", 
    "arxiv-id": "1006.1186v1"
},{
    "category": "cs.MM", 
    "author": "Indra Kanta Maitra", 
    "title": "An Alternative Approach of Steganography using Reference Image", 
    "publish": "2010-07-01T09:40:40Z", 
    "summary": "This paper is to create a practical steganographic implementation for 4-bit\nimages.The proposed technique converts 4 bit image into 4 shaded Gray Scale\nimage. This image will be act as reference image to hide the text. Using this\ngrey scale reference image any text can be hidden. Single character of a text\ncan be represented by 8-bit. The 8-bit character can be split into 4X2 bit\ninformation. If the reference image and the data file are transmitted through\nnetwork separately, we can achieve the effect of Steganography. Here the image\nis not at all distorted because said image is only used for referencing. Any\nhuge mount of text material can be hidden using a very small image. Decipher\nthe text is not possible intercepting the image or data file separately. So, it\nis more secure.", 
    "link": "http://arxiv.org/pdf/1007.1233v1", 
    "arxiv-id": "1007.1233v1"
},{
    "category": "cs.MM", 
    "author": "Ming-Shing Hsieh", 
    "title": "Perceptual Copyright Protection Using Multiresolution Wavelet-Based   Watermarking And Fuzzy Logic", 
    "publish": "2010-07-29T07:41:51Z", 
    "summary": "In this paper, an efficiently DWT-based watermarking technique is proposed to\nembed signatures in images to attest the owner identification and discourage\nthe unauthorized copying. This paper deals with a fuzzy inference filter to\nchoose the larger entropy of coefficients to embed watermarks. Unlike most\nprevious watermarking frameworks which embedded watermarks in the larger\ncoefficients of inner coarser subbands, the proposed technique is based on\nutilizing a context model and fuzzy inference filter by embedding watermarks in\nthe larger-entropy coefficients of coarser DWT subbands. The proposed\napproaches allow us to embed adaptive casting degree of watermarks for\ntransparency and robustness to the general image-processing attacks such as\nsmoothing, sharpening, and JPEG compression. The approach has no need the\noriginal host image to extract watermarks. Our schemes have been shown to\nprovide very good results in both image transparency and robustness.", 
    "link": "http://arxiv.org/pdf/1007.5136v1", 
    "arxiv-id": "1007.5136v1"
},{
    "category": "cs.MM", 
    "author": "Serguei A. Mokhov", 
    "title": "Alchymical Mirror: Real-time Interactive Sound- and Simple   Motion-Tracking Set of Jitter/Max/MSP Patches", 
    "publish": "2011-01-05T18:43:30Z", 
    "summary": "This document supplements an experimental Jitter / Max/MSP collection of\nimplementation patches that set its goal to simulate an alchemical process for\na person standing in front of a mirror-like screen while interacting with it.\nThe work involved takes some patience and has three stages to go through. At\nthe final stage the \"alchemist\" in the mirror wearing sharp-colored gloves (for\nmotion tracking) is to extract the final ultimate shining sparkle (FFT-based\nvisualization) in the nexus of the hands. The more the hands are apart, the\nlarge the sparkle should be. Moving hands around should make the sparkle\nfollow. To achieve the desired visual effect and the feedback mechanism, the\nJitter lattice-based intensional programming model is used to work on\n4-dimensional (A+R+G+B) video matrices and sound signals in order to apply some\nwell-known alchemical techniques to the video at real-time to get a mirror\neffect and accompanying transmutation and transformation stages of the video\nbased on the stability of the sound produced for some duration of time in\nreal-time. There is an accompanying video of the result with the interaction\nwith the tool and the corresponding programming patches.", 
    "link": "http://arxiv.org/pdf/1101.2219v1", 
    "arxiv-id": "1101.2219v1"
},{
    "category": "cs.MM", 
    "author": "Debin Zhao", 
    "title": "Exploiting Image Local And Nonlocal Consistency For Mixed   Gaussian-Impulse Noise Removal", 
    "publish": "2012-08-18T02:11:20Z", 
    "summary": "Most existing image denoising algorithms can only deal with a single type of\nnoise, which violates the fact that the noisy observed images in practice are\noften suffered from more than one type of noise during the process of\nacquisition and transmission. In this paper, we propose a new variational\nalgorithm for mixed Gaussian-impulse noise removal by exploiting image local\nconsistency and nonlocal consistency simultaneously. Specifically, the local\nconsistency is measured by a hyper-Laplace prior, enforcing the local\nsmoothness of images, while the nonlocal consistency is measured by\nthree-dimensional sparsity of similar blocks, enforcing the nonlocal\nself-similarity of natural images. Moreover, a Split-Bregman based technique is\ndeveloped to solve the above optimization problem efficiently. Extensive\nexperiments for mixed Gaussian plus impulse noise show that significant\nperformance improvements over the current state-of-the-art schemes have been\nachieved, which substantiates the effectiveness of the proposed algorithm.", 
    "link": "http://arxiv.org/pdf/1208.3718v1", 
    "arxiv-id": "1208.3718v1"
},{
    "category": "cs.MM", 
    "author": "Djamel Benazzouz", 
    "title": "Behavioral Systel Level Power Consumption Modeling of Mobile Video   Streaming applications", 
    "publish": "2012-08-31T06:31:01Z", 
    "summary": "Nowadays, the use of mobile applications and terminals faces fundamental\nchallenges related to energy constraint. This is due to the limited battery\nlifetime as compared to the increasing hardware evolution. Video streaming is\none of the most energy consuming applications in a mobile system because of its\nintensive use of bandwidth, memory and processing power. In this work, we aim\nto propose a methodology for building and validating a high level global power\nconsumption model including a hardware and software elements. Our approach is\nbased on exploiting the interactions between power consumption sub-models of\nstandalone systems in the perspective to build more accurate global model. The\ninteractions are studied within the exclusive context of video streaming\napplications that are one of the most used mobile applications.", 
    "link": "http://arxiv.org/pdf/1208.6389v1", 
    "arxiv-id": "1208.6389v1"
},{
    "category": "cs.MM", 
    "author": "Gustavo de Veciana", 
    "title": "A Markov Decision Model for Adaptive Scheduling of Stored Scalable   Videos", 
    "publish": "2012-09-10T17:19:45Z", 
    "summary": "We propose two scheduling algorithms that seek to optimize the quality of\nscalably coded videos that have been stored at a video server before\ntransmission.} The first scheduling algorithm is derived from a Markov Decision\nProcess (MDP) formulation developed here. We model the dynamics of the channel\nas a Markov chain and reduce the problem of dynamic video scheduling to a\ntractable Markov decision problem over a finite state space. Based on the MDP\nformulation, a near-optimal scheduling policy is computed that minimize the\nmean square error. Using insights taken from the development of the optimal\nMDP-based scheduling policy, the second proposed scheduling algorithm is an\nonline scheduling method that only requires easily measurable knowledge of the\nchannel dynamics, and is thus viable in practice. Simulation results show that\nthe performance of both scheduling algorithms is close to a performance upper\nbound also derived in this paper.", 
    "link": "http://arxiv.org/pdf/1209.2067v2", 
    "arxiv-id": "1209.2067v2"
},{
    "category": "cs.MM", 
    "author": "Oscar Martinez-Bonastre", 
    "title": "Surveying the Social, Smart and Converged TV Landscape: Where is   Television Research Headed?", 
    "publish": "2012-09-13T14:24:45Z", 
    "summary": "The TV is dead motto of just a few years ago has been replaced by the\nprospect of Internet Protocol (IP) television experiences over converged\nnetworks to become one of the great technology opportunities in the next few\nyears. As an introduction to the Special Issue on Smart, Social and Converged\nTelevision, this extended editorial intends to review the current IP television\nlandscape in its many realizations: operator-based, over-the-top, and user\ngenerated. We will address new services like social TV and recommendation\nengines, dissemination including new paradigms built on peer to peer and\ncontent centric networks, as well as the all important quality of experience\nthat challenges services and networks alike. But we intend to go further than\njust review the existing work by proposing areas for the future of television\nresearch. These include strategies to provide services that are more efficient\nin network and energy usage while being socially engaging, novel services that\nwill provide consumers with a broader choice of content and devices, and\nmetrics that will enable operators and users alike to define the level of\nservice they require or that they are ready to provide. These topics are\naddressed in this survey paper that attempts to create a unifying framework to\nlink them all together. Not only is television not dead, it is well alive,\nthriving and fostering innovation and this paper will hopefully prove it.", 
    "link": "http://arxiv.org/pdf/1209.2905v1", 
    "arxiv-id": "1209.2905v1"
},{
    "category": "cs.MM", 
    "author": "Suayb S. Arslan", 
    "title": "Minimum Distortion Variance Concatenated Block Codes for Embedded Source   Transmission", 
    "publish": "2012-10-10T06:54:32Z", 
    "summary": "Some state-of-art multimedia source encoders produce embedded source bit\nstreams that upon the reliable reception of only a fraction of the total bit\nstream, the decoder is able reconstruct the source up to a basic quality.\nReliable reception of later source bits gradually improve the reconstruction\nquality. Examples include scalable extensions of H.264/AVC and progressive\nimage coders such as JPEG2000. To provide an efficient protection for embedded\nsource bit streams, a concatenated block coding scheme using a minimum mean\ndistortion criterion was considered in the past. Although, the original design\nwas shown to achieve better mean distortion characteristics than previous\nstudies, the proposed coding structure was leading to dramatic quality\nfluctuations. In this paper, a modification of the original design is first\npresented and then the second order statistics of the distortion is taken into\naccount in the optimization. More specifically, an extension scheme is proposed\nusing a minimum distortion variance optimization criterion. This robust system\ndesign is tested for an image transmission scenario. Numerical results show\nthat the proposed extension achieves significantly lower variance than the\noriginal design, while showing similar mean distortion performance using both\nconvolutional codes and low density parity check codes.", 
    "link": "http://arxiv.org/pdf/1210.2815v2", 
    "arxiv-id": "1210.2815v2"
},{
    "category": "cs.MM", 
    "author": "Prasanta K. Panigrahi", 
    "title": "Non-uniform Quantization of Detail Components in Wavelet Transformed   Image for Lossy JPEG2000 Compression", 
    "publish": "2012-10-30T21:05:16Z", 
    "summary": "The paper introduces the idea of non-uniform quantization in the detail\ncomponents of wavelet transformed image. It argues that most of the\ncoefficients of horizontal, vertical and diagonal components lie near to zeros\nand the coefficients representing large differences are few at the extreme ends\nof histogram. Therefore, this paper advocates need for variable step size\nquantization scheme which preserves the edge information at the edge of\nhistogram and removes redundancy with the minimal number of quantized values.\nTo support the idea, preliminary results are provided using a non-uniform\nquantization algorithm. We believe that successful implementation of\nnon-uniform quantization in detail components in JPEG-2000 still image standard\nwill improve image quality and compression efficiency with lesser number of\nquantized values.", 
    "link": "http://arxiv.org/pdf/1210.8165v1", 
    "arxiv-id": "1210.8165v1"
},{
    "category": "cs.MM", 
    "author": "Attoumani Mohamed Karim", 
    "title": "Blind and robust images watermarking based on wavelet and edge insertion", 
    "publish": "2013-10-09T10:24:07Z", 
    "summary": "This paper gives a new scheme of watermarking technique related to insert the\nmark by adding edge in HH sub-band of the host image after wavelet\ndecomposition. Contrary to most of the watermarking algorithms in wavelet\ndomain, our method is blind and results show that it is robust against the JPEG\nand GIF compression, histogram and spectrum spreading, noise adding and small\nrotation. Its robustness against compression is better than others watermarking\nalgorithms reported in the literature. The algorithm is flexible because its\ncapacity or robustness can be improved by modifying some parameters.", 
    "link": "http://arxiv.org/pdf/1310.5653v1", 
    "arxiv-id": "1310.5653v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Multiview Navigation based on Extended Layered Depth Image   Representation", 
    "publish": "2013-10-22T18:41:45Z", 
    "summary": "Emerging applications in multiview streaming look for providing interactive\nnavigation services to video players. The user can ask for information from any\nviewpoint with a minimum transmission delay. The purpose is to provide user\nwith as much information as possible with least number of redundancies. The\nrecent concept of navigation segment representation consists of regrouping a\ngiven number of viewpoints in one signal and transmitting them to the users\naccording to their navigation path. The question of the best description\nstrategy of these navigation segments is however still open. In this paper, we\npropose to represent and code navigation segments by a method that extends the\nrecent layered depth image (LDI) format. It consists of describing the scene\nfrom a viewpoint with multiple images organized in layers corresponding to the\ndifferent levels of occluded objects. The notion of extended LDI comes from the\nfact that the size of this image is adapted to take into account the sides of\nthe scene also, in contrary to classical LDI. The obtained results show a\nsignificant rate-distortion gain compared to classical multiview compression\napproaches in navigation scenario.", 
    "link": "http://arxiv.org/pdf/1310.6377v1", 
    "arxiv-id": "1310.6377v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Graph-based representation for multiview image coding", 
    "publish": "2013-12-20T19:55:33Z", 
    "summary": "In this paper, we propose a new representation for multiview image sets. Our\napproach relies on graphs to describe geometry information in a compact and\ncontrollable way. The links of the graph connect pixels in different images and\ndescribe the proximity between pixels in the 3D space. These connections are\ndependent on the geometry of the scene and provide the right amount of\ninformation that is necessary for coding and reconstructing multiple views.\nThis multiview image representation is very compact and adapts the transmitted\ngeometry information as a function of the complexity of the prediction\nperformed at the decoder side. To achieve this, our GBR adapts the accuracy of\nthe geometry representation, in contrast with depth coding, which directly\ncompresses with losses the original geometry signal. We present the principles\nof this graph-based representation (GBR) and we build a complete prototype\ncoding scheme for multiview images. Experimental results demonstrate the\npotential of this new representation as compared to a depth-based approach. GBR\ncan achieve a gain of 2 dB in reconstructed quality over depth-based schemes\noperating at similar rates.", 
    "link": "http://arxiv.org/pdf/1312.6090v1", 
    "arxiv-id": "1312.6090v1"
},{
    "category": "cs.MM", 
    "author": "Alessandra M. Coelho", 
    "title": "State-of-the Art Motion Estimation in the Context of 3D TV", 
    "publish": "2013-12-23T09:43:09Z", 
    "summary": "Progress in image sensors and computation power has fueled studies to improve\nacquisition, processing, and analysis of 3D streams along with 3D\nscenes/objects reconstruction. The role of motion compensation/motion\nestimation (MCME) in 3D TV from end-to-end user is investigated in this\nchapter. Motion vectors (MVs) are closely related to the concept of\ndisparities, and they can help improving dynamic scene acquisition, content\ncreation, 2D to 3D conversion, compression coding, decompression/decoding,\nscene rendering, error concealment, virtual/augmented reality handling,\nintelligent content retrieval, and displaying. Although there are different 3D\nshape extraction methods, this chapter focuses mostly on shape-from-motion\n(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D\nshape information from a single camera data.", 
    "link": "http://arxiv.org/pdf/1312.6497v1", 
    "arxiv-id": "1312.6497v1"
},{
    "category": "cs.MM", 
    "author": "G. Umamaheswari", 
    "title": "A Study of Various Steganographic Techniques Used for Information Hiding", 
    "publish": "2014-01-22T05:58:55Z", 
    "summary": "Steganography derives from the Greek word steganos, meaning covered or\nsecret, and graphy (writing or drawing). Steganography is a technology where\nmodern data compression, information theory, spread spectrum, and cryptography\ntechnologies are brought together to satisfy the need for privacy on the\nInternet. This paper is an attempt to analyse the various techniques used in\nsteganography and to identify areas in which this technique can be applied, so\nthat the human race can be benefited at large.", 
    "link": "http://arxiv.org/pdf/1401.5561v1", 
    "arxiv-id": "1401.5561v1"
},{
    "category": "cs.MM", 
    "author": "Michel Kieffer", 
    "title": "Control of Multiple Remote Servers for Quality-Fair Delivery of   Multimedia Contents", 
    "publish": "2014-01-24T14:56:42Z", 
    "summary": "This paper proposes a control scheme for the quality-fair delivery of several\nencoded video streams to mobile users sharing a common wireless resource. Video\nquality fairness, as well as similar delivery delays are targeted among\nstreams. The proposed controller is implemented within some aggregator located\nnear the bottleneck of the network. The transmission rate among streams is\nadapted based on the quality of the already encoded and buffered packets in the\naggregator. Encoding rate targets are evaluated by the aggregator and fed back\nto each remote video server (fully centralized solution), or directly evaluated\nby each server in a distributed way (partially distributed solution). Each\nencoding rate target is adjusted for each stream independently based on the\ncorresponding buffer level or buffering delay in the aggregator. Communication\ndelays between the servers and the aggregator are taken into account. The\ntransmission and encoding rate control problems are studied with a\ncontrol-theoretic perspective. The system is described with a multi-input\nmulti-output model. Proportional Integral (PI) controllers are used to adjust\nthe video quality and control the aggregator buffer levels. The system\nequilibrium and stability properties are studied. This provides guidelines for\nchoosing the parameters of the PI controllers. Experimental results show the\nconvergence of the proposed control system and demonstrate the improvement in\nvideo quality fairness compared to a classical transmission rate fair streaming\nsolution and to a utility max-min fair approach.", 
    "link": "http://arxiv.org/pdf/1401.6361v1", 
    "arxiv-id": "1401.6361v1"
},{
    "category": "cs.MM", 
    "author": "E. S. Yakovleva", 
    "title": "Color to Gray and Back transformation for distributing color digital   images", 
    "publish": "2014-03-31T14:19:04Z", 
    "summary": "The Color to Gray and Back transformation watermarking with a secrete key is\nconsidered. Color is embedded into the bit planes of the luminosity component\nof the YUV color space with the help of a block algorithm that allows using not\nonly the least significant bits. An application of the problem of distributing\ncolor digital images from a data base among legitimate users is discussed. The\nproposed protocol can protect original images from unauthorized copying.", 
    "link": "http://arxiv.org/pdf/1404.1313v1", 
    "arxiv-id": "1404.1313v1"
},{
    "category": "cs.MM", 
    "author": "Dr. Timur Mirzoev", 
    "title": "Analysis of Computer Hardware Affecting Video Transmission via IEEE   1394a connection", 
    "publish": "2014-04-09T01:23:31Z", 
    "summary": "When 60 de-interlaced fields per second digital uncompressed video is\nstreamed to a computer, some video fields are lost and not able to be stored on\na computer s hard drive successfully. Additionally, this problem amplifies once\nmultiple video sources are deployed. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it. If a loss of video fields occurs, then a quality analysis of\nvideo is not possible.", 
    "link": "http://arxiv.org/pdf/1404.2344v1", 
    "arxiv-id": "1404.2344v1"
},{
    "category": "cs.MM", 
    "author": "Yonggang Wen", 
    "title": "Enhancing User Experience for Multi-Screen Social TV Streaming over   Wireless Networks", 
    "publish": "2014-04-11T07:13:22Z", 
    "summary": "Recently, multi-screen cloud social TV is invented to transform TV into\nsocial experience. People watching the same content on social TV may come from\ndifferent locations, while freely interact with each other through text, image,\naudio and video. This crucial virtual living-room experience adds social\naspects into existing performance metrics. In this paper, we parse social TV\nuser experience into three elements (i.e., inter-user delay, video quality of\nexperience (QoE), and resource efficiency), and provide a joint analytical\nframework to enhance user experience. Specifically, we propose a cloud-based\noptimal playback rate allocation scheme to maximize the overall QoE while upper\nbounding inter-user delay. Experiment results show that our algorithm achieves\nnear-optimal tradeoff between inter-user delay and video quality, and\ndemonstrates resilient performance even under very fast wireless channel\nfading.", 
    "link": "http://arxiv.org/pdf/1404.3018v1", 
    "arxiv-id": "1404.3018v1"
},{
    "category": "cs.MM", 
    "author": "Alfred M. Bruckstein", 
    "title": "Improving Low Bit-Rate Video Coding using Spatio-Temporal Down-Scaling", 
    "publish": "2014-04-15T19:26:13Z", 
    "summary": "Good quality video coding for low bit-rate applications is important for\ntransmission over narrow-bandwidth channels and for storage with limited memory\ncapacity. In this work, we develop a previous analysis for image compression at\nlow bit-rates to adapt it to video signals. Improving compression using\ndown-scaling in the spatial and temporal dimensions is examined. We show, both\ntheoretically and experimentally, that at low bit-rates, we benefit from\napplying spatio-temporal scaling. The proposed method includes down-scaling\nbefore the compression and a corresponding up-scaling afterwards, while the\ncodec itself is left unmodified. We propose analytic models for low bit-rate\ncompression and spatio-temporal scaling operations. Specifically, we use\ntheoretic models of motion-compensated prediction of available and absent\nframes as in coding and frame-rate up-conversion (FRUC) applications,\nrespectively. The proposed models are designed for multi-resolution analysis.\nIn addition, we formulate a bit-allocation procedure and propose a method for\nestimating good down-scaling factors of a given video based on its second-order\nstatistics and the given bit-budget. We validate our model with experimental\nresults of H.264 compression.", 
    "link": "http://arxiv.org/pdf/1404.4026v2", 
    "arxiv-id": "1404.4026v2"
},{
    "category": "cs.MM", 
    "author": "Behrang QasemiZadeh", 
    "title": "A Novel Approach for Video Temporal Annotation", 
    "publish": "2014-04-17T14:42:03Z", 
    "summary": "Recent advances in computing, communication, and data storage have led to an\nincreasing number of large digital libraries publicly available on the\nInternet. Main problem of content-based video retrieval is inferring semantics\nfrom raw video data. Video data play an important role in these libraries.\nInstead of words, a video retrieval system deals with collections of video\nrecords. Therefore, the system is confronted with the problem of video\nunderstanding. Because machine understanding of the video data is still an\nunsolved research problem, text annotations are usually used to describe the\ncontent of video data according to the annotator's understanding and the\npurpose of that video data. Most of proposed systems for video annotation are\ndomain dependent. In addition, in many of these systems, an important feature\nof video data, temporality, is disregarded. In this paper, we proposed a\nframework for video temporal annotation. The proposed system uses domain\nknowledge and a time ontology to perform temporal annotation of input video.", 
    "link": "http://arxiv.org/pdf/1404.4543v1", 
    "arxiv-id": "1404.4543v1"
},{
    "category": "cs.MM", 
    "author": "Rajasekhar R. Kurra", 
    "title": "A Smart Intelligent Way of Video Authentication Using Classification and   Decomposition of Watermarking Methods", 
    "publish": "2014-04-29T05:20:25Z", 
    "summary": "Video Watermarking serves as a new technology mainly used to provide security\nto the illegal distribution of digital video over the web. The purpose of any\nvideo watermarking scheme is to embed extra information into video in such a\nway that must be perceptually undetectable while still holding enough\ninformation in order to extract the watermark beginning with the resultant\nvideo. Information which is embedded within the original image is a Digital\nWatermark, which could be visible or invisible. To improved more security,\nembedding and extraction Watermark process should be complex against attackers.\nRecent research indicates SVD (Singular Value Decomposition) algorithms are\nemployed owing to their simple scheme with mathematical function. In this\nproposed work an advanced SVD transformation algorithm is used for embedding\nand extraction process. Experimental results show proposed watermarking process\nis more secured than existing SVD approach.", 
    "link": "http://arxiv.org/pdf/1404.7237v1", 
    "arxiv-id": "1404.7237v1"
},{
    "category": "cs.MM", 
    "author": "Minh N. Do", 
    "title": "ITEM: Immersive Telepresence for Entertainment and Meetings - A   Practical Approach", 
    "publish": "2014-08-04T08:16:09Z", 
    "summary": "This paper presents an Immersive Telepresence system for Entertainment and\nMeetings (ITEM). The system aims to provide a radically new video communication\nexperience by seamlessly merging participants into the same virtual space to\nallow a natural interaction among them and shared collaborative contents. With\nthe goal to make a scalable, flexible system for various business solutions as\nwell as easily accessible by massive consumers, we address the challenges in\nthe whole pipeline of media processing, communication, and displaying in our\ndesign and realization of such a system. Particularly, in this paper we focus\non the system aspects that maximize the end-user experience, optimize the\nsystem and network resources, and enable various teleimmersive application\nscenarios. In addition, we also present a few key technologies, i.e. fast\nobject-based video coding for real world data and spatialized audio capture and\n3D sound localization for group teleconferencing. Our effort is to investigate\nand optimize the key system components and provide an efficient end-to-end\noptimization and integration by considering user needs and preferences.\nExtensive experiments show the developed system runs reliably and comfortably\nin real time with a minimal setup requirement (e.g. a webcam and/or a depth\ncamera, an optional microphone array, a laptop/desktop connected to the public\nInternet) for teleimmersive communication. With such a really minimal\ndeployment requirement, we present a variety of interesting applications and\nuser experiences created by ITEM.", 
    "link": "http://arxiv.org/pdf/1408.0605v1", 
    "arxiv-id": "1408.0605v1"
},{
    "category": "cs.MM", 
    "author": "M. C. Adhikary", 
    "title": "Digital Image Data Hiding Techniques: A Comparative Study", 
    "publish": "2014-08-15T15:33:40Z", 
    "summary": "With the advancements in the field of digital image processing during the\nlast decade, digital image data hiding techniques such as watermarking,\nSteganography have gained wide popularity. Digital image watermarking\ntechniques hide a small amount of data into a digital image which, later can be\nretrieved using some specific retrieval algorithms to prove the copyright of a\npiece of digital information whereas, Steganographic techniques are used to\nhide a large amount of data secretly into some innocuous looking digital\nmedium. In this paper we are providing an up-to-date review of these data\nhiding techniques.", 
    "link": "http://arxiv.org/pdf/1408.3564v1", 
    "arxiv-id": "1408.3564v1"
},{
    "category": "cs.MM", 
    "author": "Niu Xiamu", 
    "title": "A New Method for Estimating the Widths of JPEG Images", 
    "publish": "2014-10-08T13:24:06Z", 
    "summary": "Image width is important for image understanding. We propose a novel method\nto estimate widths for JPEG images when their widths are not available. The key\nidea is that the distance between two decoded MCUs (Minimum Coded Unit)\nadjacent in the vertical direction is usually small, which is measured by the\naverage Euclidean distance between the pixels from the bottom row of the top\nMCU and the top row of the bottom MCU. On PASCAL VOC 2010 challenge dataset and\nUSC-SIPI image database, experimental results show the high performance of the\nproposed approach.", 
    "link": "http://arxiv.org/pdf/1410.2100v1", 
    "arxiv-id": "1410.2100v1"
},{
    "category": "cs.MM", 
    "author": "Charul Bhatnagar", 
    "title": "An Efficient Bit Plane X-OR Algorithm for Irreversible Image   Steganography", 
    "publish": "2014-10-12T16:47:55Z", 
    "summary": "The science of hiding secret information in another message is known as\nSteganography; hence the presence of secret information is concealed. It is the\nmethod of hiding cognitive content in same or another media to avoid\nrecognition by the intruders. This paper introduces new method wherein\nirreversible steganography is used to hide an image in the same medium so that\nthe secret data is masked. The secret image is known as payload and the carrier\nis known as cover image. X-OR operation is used amongst mid level bit planes of\ncarrier image and high level bit planes of data image to generate new low level\nbit planes of the stego image. Recovery process includes the X-ORing of low\nlevel bit planes and mid level bit planes of the stego image. Based on the\nresult of the recovery, subsequent data image is generated. A RGB color image\nis used as carrier and the data image is a grayscale image of dimensions less\nthan or equal to the dimensions of the carrier image. The proposed method\ngreatly increases the embedding capacity without significantly decreasing the\nPSNR value.", 
    "link": "http://arxiv.org/pdf/1410.3117v1", 
    "arxiv-id": "1410.3117v1"
},{
    "category": "cs.MM", 
    "author": "Charul Bhatnagar", 
    "title": "Secret Image Sharing Using Grayscale Payload Decomposition and   Irreversible Image Steganography", 
    "publish": "2014-10-12T17:35:37Z", 
    "summary": "To provide an added security level most of the existing reversible as well as\nirreversible image steganography schemes emphasize on encrypting the secret\nimage (payload) before embedding it to the cover image. The complexity of\nencryption for a large payload where the embedding algorithm itself is complex\nmay adversely affect the steganographic system. Schemes that can induce same\nlevel of distortion, as any standard encryption technique with lower\ncomputational complexity, can improve the performance of stego systems. In this\npaper we propose a secure secret image sharing scheme, which bears minimal\ncomputational complexity. The proposed scheme, as a replacement for encryption,\ndiversifies the payload into different matrices which are embedded into carrier\nimage (cover image) using bit X-OR operation. A payload is a grayscale image\nwhich is divided into frequency matrix, error matrix, and sign matrix. The\nfrequency matrix is scaled down using a mapping algorithm to produce Down\nScaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix\nare then embedded in different cover images using bit X-OR operation between\nthe bit planes of the matrices and respective cover images. Analysis of the\nproposed scheme shows that it effectively camouflages the payload with minimum\ncomputation time.", 
    "link": "http://arxiv.org/pdf/1410.3122v1", 
    "arxiv-id": "1410.3122v1"
},{
    "category": "cs.MM", 
    "author": "De-Nian Yang", 
    "title": "Multi-View 3D Video Multicast for Broadband IP Networks", 
    "publish": "2014-10-15T09:09:10Z", 
    "summary": "With the recent emergence of 3D-supported TVs, video service providers now\nface an opportunity to provide high resolution multi-view 3D videos over IP\nnetworks. One simple way to support efficient communications between a video\nserver and multiple clients is to deliver each desired view in a multicast\nstream. Nevertheless, it is expected that significantly increased bandwidth\nwill be required to support the transmission of all views in multi-view 3D\nvideos. However, the recent emergence of a new video synthesis technique called\nDepth-Image-Based Rendering (DIBR) suggests that multi-view 3D video does not\nnecessarily require the transmission of all views. Therefore, we formulate a\nnew problem, named Multi-view and Multicast Delivery Selection Problem (MMDS),\nand design an algorithm, called MMDEA, to find the optimal solution. Simulation\nresults manifest that using DIBR can effectively reduce bandwidth consumption\nby 35% compared to the original multicast delivery scheme.", 
    "link": "http://arxiv.org/pdf/1410.3977v1", 
    "arxiv-id": "1410.3977v1"
},{
    "category": "cs.MM", 
    "author": "Ying He", 
    "title": "Human Motion Capture Data Tailored Transform Coding", 
    "publish": "2014-10-17T14:16:08Z", 
    "summary": "Human motion capture (mocap) is a widely used technique for digitalizing\nhuman movements. With growing usage, compressing mocap data has received\nincreasing attention, since compact data size enables efficient storage and\ntransmission. Our analysis shows that mocap data have some unique\ncharacteristics that distinguish themselves from images and videos. Therefore,\ndirectly borrowing image or video compression techniques, such as discrete\ncosine transform, does not work well. In this paper, we propose a novel\nmocap-tailored transform coding algorithm that takes advantage of these\nfeatures. Our algorithm segments the input mocap sequences into clips, which\nare represented in 2D matrices. Then it computes a set of data-dependent\northogonal bases to transform the matrices to frequency domain, in which the\ntransform coefficients have significantly less dependency. Finally, the\ncompression is obtained by entropy coding of the quantized coefficients and the\nbases. Our method has low computational cost and can be easily extended to\ncompress mocap databases. It also requires neither training nor complicated\nparameter setting. Experimental results demonstrate that the proposed scheme\nsignificantly outperforms state-of-the-art algorithms in terms of compression\nperformance and speed.", 
    "link": "http://arxiv.org/pdf/1410.4730v1", 
    "arxiv-id": "1410.4730v1"
},{
    "category": "cs.MM", 
    "author": "Xiaoyu Liu", 
    "title": "Comparing CSI and PCA in Amalgamation with JPEG for Spectral Image   Compression", 
    "publish": "2014-10-19T16:51:34Z", 
    "summary": "Continuing our previous research on color image compression, we move towards\nspectral image compression. This enormous amount of data needs more space to\nstore and more time to transmit. To manage this sheer amount of data,\nresearchers have investigated different techniques so that image quality can be\nconserved and compressibility can be improved. The principle component analysis\n(PCA) can be employed to reduce the dimensions of spectral images to achieve\nhigh compressibility and performance. Due to processing complexity of PCA, a\nsimple interpolation technique called cubic spline interpolation (CSI) was\nconsidered to reduce the dimensionality of spectral domain of spectral images.\nThe CSI and PCA were employed one by one in the spectral domain and were\namalgamated with the JPEG, which was employed in spatial domain. Three measures\nincluding compression rate (CR), processing time (Tp) and color difference\nCIEDE2000 were used for performance analysis. Test results showed that for a\nfixed value of compression rate, CSI based algorithm performed poor in terms of\ndE00, in comparison with PCA, but is still reliable because of small color\ndifference. On the other hand it has lower complexity and is computationally\nmuch better as compared to PCA based algorithm, especially for spectral images\nwith large size.", 
    "link": "http://arxiv.org/pdf/1410.5092v4", 
    "arxiv-id": "1410.5092v4"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Optimized Packet Scheduling in Multiview Video Navigation Systems", 
    "publish": "2014-12-02T16:02:12Z", 
    "summary": "In multiview video systems, multiple cameras generally acquire the same scene\nfrom different perspectives, such that users have the possibility to select\ntheir preferred viewpoint. This results in large amounts of highly redundant\ndata, which needs to be properly handled during encoding and transmission over\nresource-constrained channels. In this work, we study coding and transmission\nstrategies in multicamera systems, where correlated sources send data through a\nbottleneck channel to a central server, which eventually transmits views to\ndifferent interactive users. We propose a dynamic correlation-aware packet\nscheduling optimization under delay, bandwidth, and interactivity constraints.\nThe optimization relies both on a novel rate-distortion model, which captures\nthe importance of each view in the 3D scene reconstruction, and on an objective\nfunction that optimizes resources based on a client navigation model. The\nlatter takes into account the distortion experienced by interactive clients as\nwell as the distortion variations that might be observed by clients during\nmultiview navigation. We solve the scheduling problem with a novel\ntrellis-based solution, which permits to formally decompose the multivariate\noptimization problem thereby significantly reducing the computation complexity.\nSimulation results show the gain of the proposed algorithm compared to baseline\nscheduling policies. More in details, we show the gain offered by our dynamic\nscheduling policy compared to static camera allocation strategies and to\nschemes with constant coding strategies. Finally, we show that the best\nscheduling policy consistently adapts to the most likely user navigation path\nand that it minimizes distortion variations that can be very disturbing for\nusers in traditional navigation systems.", 
    "link": "http://arxiv.org/pdf/1412.0954v1", 
    "arxiv-id": "1412.0954v1"
},{
    "category": "cs.MM", 
    "author": "Farokh Marvasti", 
    "title": "Multi-Hypothesis Compressed Video Sensing Technique", 
    "publish": "2014-12-15T13:19:24Z", 
    "summary": "In this paper, we present a compressive sampling and Multi-Hypothesis (MH)\nreconstruction strategy for video sequences which has a rather simple encoder,\nwhile the decoding system is not that complex. We introduce a convex cost\nfunction that incorporates the MH technique with the sparsity constraint and\nthe Tikhonov regularization. Consequently, we derive a new iterative algorithm\nbased on these criteria. This algorithm surpasses its counterparts (Elasticnet\nand Tikhonov) in the recovery performance. Besides it is computationally much\nfaster than the Elasticnet and comparable to the Tikhonov. Our extensive\nsimulation results confirm these claims.", 
    "link": "http://arxiv.org/pdf/1412.4576v1", 
    "arxiv-id": "1412.4576v1"
},{
    "category": "cs.MM", 
    "author": "Jiangchuan Liu", 
    "title": "Mobile Instant Video Clip Sharing: Modeling and Enhancing View   Experience", 
    "publish": "2014-12-24T03:44:21Z", 
    "summary": "With the rapid development of wireless networking and mobile devices, anytime\nand anywhere data access becomes readily available nowadays. Given the\ncrowdsourced content capturing and sharing, the preferred content length\nbecomes shorter and shorter, even for such multimedia data as video. A\nrepresentative is Twitter's Vine service, which, mainly targeting mobile users,\nenables them to create ultra-short video clips and instantly post and share\nwith their followers. In this paper, we present an initial study on this new\ngeneration of instant video clip sharing service enabled by mobile platforms\nand explore the potentials towards its further enhancement. We closely\ninvestigate its unique mobile interface, revealing the key differences between\nVine-enabled anytime anywhere data access patterns and that of traditional\ncounterparts. We then examine the scheduling policy to maximize the user\nwatching experience as well as the efficiency on the monetary and energy costs.\nWe show that the generic scheduling problem involves two subproblems, namely,\npre-fetching scheduling and watch-time download scheduling, and develop\neffective solutions towards both of them. The superiority of our solution is\ndemonstrated by extensive trace-driven simulations. To the best of our\nknowledge, this is the first work on modeling and optimizing the instant video\nclip sharing on mobile devices.", 
    "link": "http://arxiv.org/pdf/1412.7595v3", 
    "arxiv-id": "1412.7595v3"
},{
    "category": "cs.MM", 
    "author": "Wai Ho Mow", 
    "title": "A Systematic Scheme for Measuring the Performance of the Display-Camera   Channel", 
    "publish": "2015-01-12T03:13:34Z", 
    "summary": "Display-camera communication has become a promising direction in both\ncomputer vision and wireless communication communities. However, the\nconsistency of the channel measurement is an open issue since precise\ncalibration of the experimental setting has not been fully studied in the\nliteratures. This paper focuses on establishing a scheme for precise\ncalibration of the display-camera channel performance. To guarantee high\nconsistency of the experiment, we propose an accurate measurement scheme for\nthe geometric parameters, and identify some unstable channel factors, e.g.,\nMoire effect, rolling shutter effect, blocking artifacts, inconsistency in\nauto-focus, trembling and vibration. In the experiment, we first define the\nconsistency criteria according to the error-prone region in bit error rate\n(BER) plots of the channel measurements. It is demonstrated that the\nconsistency of the experimental result can be improved by the proposed precise\ncalibration scheme.", 
    "link": "http://arxiv.org/pdf/1501.02528v1", 
    "arxiv-id": "1501.02528v1"
},{
    "category": "cs.MM", 
    "author": "Vlasios Kasapakis", 
    "title": "PacMap: Transferring PacMan to the Physical Realm", 
    "publish": "2015-01-12T14:27:37Z", 
    "summary": "This paper discusses the implementation of the pervasive game PacMap.\nOpenness and portability have been the main design objectives for PacMap. We\nelaborate on programming techniques which may be applicable to a broad range of\nlocation-based games that involve the movement of virtual characters over map\ninterfaces. In particular, we present techniques to execute shortest path\nalgorithms on spatial environments bypassing the restrictions imposed by\ncommercial mapping services. Last, we present ways to improve the movement and\nenhance the intelligence of virtual characters taking into consideration the\nactions and position of players in location-based games.", 
    "link": "http://arxiv.org/pdf/1501.02659v1", 
    "arxiv-id": "1501.02659v1"
},{
    "category": "cs.MM", 
    "author": "Stavros D. Nikolopoulos", 
    "title": "Watermarking PDF Documents using Various Representations of   Self-inverting Permutations", 
    "publish": "2015-01-12T16:09:48Z", 
    "summary": "This work provides to web users copyright protection of their Portable\nDocument Format (PDF) documents by proposing efficient and easily implementable\ntechniques for PDF watermarking; our techniques are based on the ideas of our\nrecently proposed watermarking techniques for software, image, and audio,\nexpanding thus the digital objects that can be efficiently watermarked through\nthe use of self-inverting permutations. In particular, we present various\nrepresentations of a self-inverting permutation $\\pi^*$ namely\n1D-representation, 2D-representation, and RPG-representation, and show that\ntheses representations can be efficiently applied to PDF watermarking. Indeed,\nwe first present an audio-based technique for marking a PDF document $T$ by\nexploiting the 1D-representation of a permutation $\\pi^*$, and then, since\npages of a PDF document $T$ are 2D objects, we present an image-based algorithm\nfor encoding $\\pi^*$ into $T$ by first mapping the elements of $\\pi^*$ into a\nmatrix $A^*$ and then using the information stored in $A^*$ to mark invisibly\nspecific areas of PDF document $T$. Finally, we describe a graph-based\nwatermarking algorithm for embedding a self-inverting permutation $\\pi^*$ into\nthe document structure of a PDF file $T$ by exploiting the RPG-representation\nof $\\pi^*$ and the structure of a PDF document. We have evaluated the embedding\nand extracting algorithms by testing them on various and different in\ncharacteristics PDF documents.", 
    "link": "http://arxiv.org/pdf/1501.02686v1", 
    "arxiv-id": "1501.02686v1"
},{
    "category": "cs.MM", 
    "author": "E. M. Kainarova", 
    "title": "Embedding of binary image in the Gray planes", 
    "publish": "2015-01-28T08:58:20Z", 
    "summary": "For watermarking of the digital grayscale image its Gray planes have been\nused. With the help of the introduced representation over Gray planes the LSB\nembedding method and detection have been discussed. It found that data, a\nbinary image, hidden in the Gray planes is more robust to JPEG lossy\ncompression than in the bit planes.", 
    "link": "http://arxiv.org/pdf/1501.07034v1", 
    "arxiv-id": "1501.07034v1"
},{
    "category": "cs.MM", 
    "author": "Edis Franca", 
    "title": "Wavelet based Watermarking approach in the Compressive Sensing Scenario", 
    "publish": "2015-02-06T19:16:45Z", 
    "summary": "Due to the wide distribution and usage of digital media, an important issue\nis protection of the digital content. There is a number of algorithms and\ntechniques developed for the digital watermarking.In this paper, the invisible\nimage watermark procedure is considered. Watermark is created as a pseudo\nrandom sequence, embedded in the certain region of the image, obtained using\nHaar wavelet decomposition. Generally, the watermarking procedure should be\nrobust to the various attacks-filtering, noise etc. Here we assume the\nCompressive sensing scenario as a new signal processing technique that may\ninfluence the robustness. The focus of this paper was the possibility of the\nwatermark detection under Compressive Sensing attack with different number of\navailable image coefficients. The quality of the reconstructed images has been\nevaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with\nexperimental results.", 
    "link": "http://arxiv.org/pdf/1502.01996v1", 
    "arxiv-id": "1502.01996v1"
},{
    "category": "cs.MM", 
    "author": "Tianjing Tang", 
    "title": "A DCT And SVD based Watermarking Technique To Identify Tag", 
    "publish": "2015-02-10T16:23:07Z", 
    "summary": "With the rapid development of the multimedia,the secure of the multimedia is\nget more concerned. as far as we know , Digital watermarking is an effective\nway to protect copyright. The watermark must be generally hidden does not\naffect the quality of the original image. In this paper,a novel way based on\ndiscrete cosine transform(DCT) and singular value decomposition(SVD) .In the\nproposed way,we decomposition the image into 8*8 blocks, next we use the DCT to\nget the transformed block,then we choose the diagonal to embed the information,\nafter we do this, we recover the image and then we decomposition the image to\n8*8 blocks,we use the SVD way to get the diagonal matrix and embed the\ninformation in the matrix. next we extract the information use both inverse of\nDCT and SVD, as we all know,after we embed the information seconded time , the\ninformation we first information we embed must be changed, we choose a measure\nway called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the\ntwo image, and set a threshold to ensure whether the information is same or\nnot.", 
    "link": "http://arxiv.org/pdf/1502.02969v1", 
    "arxiv-id": "1502.02969v1"
},{
    "category": "cs.MM", 
    "author": "Yao Wang", 
    "title": "A two-stage video coding framework with both self-adaptive redundant   dictionary and adaptively orthonormalized DCT basis", 
    "publish": "2015-02-12T20:41:15Z", 
    "summary": "In this work, we propose a two-stage video coding framework, as an extension\nof our previous one-stage framework in [1]. The two-stage frameworks consists\ntwo different dictionaries. Specifically, the first stage directly finds the\nsparse representation of a block with a self-adaptive dictionary consisting of\nall possible inter-prediction candidates by solving an L0-norm minimization\nproblem using an improved orthogonal matching pursuit with embedded\northonormalization (eOMP) algorithm, and the second stage codes the residual\nusing DCT dictionary adaptively orthonormalized to the subspace spanned by the\nfirst stage atoms. The transition of the first stage and the second stage is\ndetermined based on both stages' quantization stepsizes and a threshold. We\nfurther propose a complete context adaptive entropy coder to efficiently code\nthe locations and the coefficients of chosen first stage atoms. Simulation\nresults show that the proposed coder significantly improves the RD performance\nover our previous one-stage coder. More importantly, the two-stage coder, using\na fixed block size and inter-prediction only, outperforms the H.264 coder\n(x264) and is competitive with the HEVC reference coder (HM) over a large rate\nrange.", 
    "link": "http://arxiv.org/pdf/1502.03802v1", 
    "arxiv-id": "1502.03802v1"
},{
    "category": "cs.MM", 
    "author": "Jiangchuan Liu", 
    "title": "On Crowdsourced Interactive Live Streaming: A Twitch.TV-Based   Measurement Study", 
    "publish": "2015-02-16T19:06:52Z", 
    "summary": "Empowered by today's rich tools for media generation and collaborative\nproduction, the multimedia service paradigm is shifting from the conventional\nsingle source, to multi-source, to many sources, and now toward {\\em\ncrowdsource}. Such crowdsourced live streaming platforms as Twitch.tv allow\ngeneral users to broadcast their content to massive viewers, thereby greatly\nexpanding the content and user bases. The resources available for these\nnon-professional broadcasters however are limited and unstable, which\npotentially impair the streaming quality and viewers' experience. The diverse\nlive interactions among the broadcasters and viewers can further aggravate the\nproblem.\n  In this paper, we present an initial investigation on the modern crowdsourced\nlive streaming systems. Taking Twitch as a representative, we outline their\ninside architecture using both crawled data and captured traffic of local\nbroadcasters/viewers. Closely examining the access data collected in a\ntwo-month period, we reveal that the view patterns are determined by both\nevents and broadcasters' sources. Our measurements explore the unique source-\nand event-driven views, showing that the current delay strategy on the viewer's\nside substantially impacts the viewers' interactive experience, and there is\nsignificant disparity between the long broadcast latency and the short live\nmessaging latency. On the broadcaster's side, the dynamic uploading capacity is\na critical challenge, which noticeably affects the smoothness of live streaming\nfor viewers.", 
    "link": "http://arxiv.org/pdf/1502.04666v2", 
    "arxiv-id": "1502.04666v2"
},{
    "category": "cs.MM", 
    "author": "Radu Arsinte", 
    "title": "Evaluating QoS Parameters for IPTV Distribution in Heterogeneous   Networks", 
    "publish": "2015-02-21T07:26:33Z", 
    "summary": "The present work presents an architecture developed to evaluate the QoS\nparameters for the IPTV heterogeneous network. At its very basic level lie two\nsoftware technologies: Video LAN and Windows Media Services with two operating\nsystems: Windows and Linux. Three types of streams are analyzed, which will be\ntransmitted to a Linux VLC client through means of the aggregation and access\nservers. The first stream is generated in real time by a capture camera,\nprocessed by the encapsulated VC-1 encoder and sent to the Media Server, while\nthe second one is of VoD(Video on Demand) type and the third one will be\nhandled by DVBViewer through the MPEG TS form. The first stream is transcoded\nin H.264-AAC such that the Linux stations will recognize its format. Through\nthe simultaneous transmission of the three streams, we are analyzing their\nperformance from a QoS parameters point of view by means of an application\nimplemented in C programming language. The stream transporting the DVB-S\ntelevision content was proven to ensure the best performance regarding loss of\npackets, delays and jitter.", 
    "link": "http://arxiv.org/pdf/1502.06078v1", 
    "arxiv-id": "1502.06078v1"
},{
    "category": "cs.MM", 
    "author": "Nemanja Ivanovic", 
    "title": "Compressive sensing based velocity estimation in video data", 
    "publish": "2015-02-21T13:19:34Z", 
    "summary": "This paper considers the use of compressive sensing based algorithms for\nvelocity estimation of moving vehicles. The procedure is based on sparse\nreconstruction algorithms combined with time-frequency analysis applied to\nvideo data. This algorithm provides an accurate estimation of object's velocity\neven in the case of a very reduced number of available video frames. The\ninfluence of crucial parameters is analysed for different types of moving\nvehicles.", 
    "link": "http://arxiv.org/pdf/1502.06103v1", 
    "arxiv-id": "1502.06103v1"
},{
    "category": "cs.MM", 
    "author": "Ming-Ting Sun", 
    "title": "A Computation Control Motion Estimation Method for Complexity-Scalable   Video Coding", 
    "publish": "2015-02-28T06:59:42Z", 
    "summary": "In this paper, a new Computation-Control Motion Estimation (CCME) method is\nproposed which can perform Motion Estimation (ME) adaptively under different\ncomputation or power budgets while keeping high coding performance. We first\npropose a new class-based method to measure the Macroblock (MB) importance\nwhere MBs are classified into different classes and their importance is\nmeasured by combining their class information as well as their initial matching\ncost information. Based on the new MB importance measure, a complete CCME\nframework is then proposed to allocate computation for ME. The proposed method\nperforms ME in a one-pass flow. Experimental results demonstrate that the\nproposed method can allocate computation more accurately than previous methods\nand thus has better performance under the same computation budget.", 
    "link": "http://arxiv.org/pdf/1503.00083v1", 
    "arxiv-id": "1503.00083v1"
},{
    "category": "cs.MM", 
    "author": "Hongxiang Li", 
    "title": "A Fast Sub-Pixel Motion Estimation Algorithm for H.264/AVC Video Coding", 
    "publish": "2015-02-28T07:08:03Z", 
    "summary": "Motion Estimation (ME) is one of the most time-consuming parts in video\ncoding. The use of multiple partition sizes in H.264/AVC makes it even more\ncomplicated when compared to ME in conventional video coding standards. It is\nimportant to develop fast and effective sub-pixel ME algorithms since (a) The\ncomputation overhead by sub-pixel ME has become relatively significant while\nthe complexity of integer-pixel search has been greatly reduced by fast\nalgorithms, and (b) Reducing sub-pixel search points can greatly save the\ncomputation for sub-pixel interpolation. In this paper, a novel fast sub-pixel\nME algorithm is proposed which performs a 'rough' sub-pixel search before the\npartition selection, and performs a 'precise' sub-pixel search for the best\npartition. By reducing the searching load for the large number of non-best\npartitions, the computation complexity for sub-pixel search can be greatly\ndecreased. Experimental results show that our method can reduce the sub-pixel\nsearch points by more than 50% compared to existing fast sub-pixel ME methods\nwith negligible quality degradation.", 
    "link": "http://arxiv.org/pdf/1503.00085v1", 
    "arxiv-id": "1503.00085v1"
},{
    "category": "cs.MM", 
    "author": "Xiaozhen Zheng", 
    "title": "An Efficient Coding Method for Coding Region-of-Interest Locations in   AVS2", 
    "publish": "2015-02-28T11:42:42Z", 
    "summary": "Region-of-Interest (ROI) location information in videos has many practical\nusages in video coding field, such as video content analysis and user\nexperience improvement. Although ROI-based coding has been studied widely by\nmany researchers to improve coding efficiency for video contents, the ROI\nlocation information itself is seldom coded in video bitstream. In this paper,\nwe will introduce our proposed ROI location coding tool which has been adopted\nin surveillance profile of AVS2 video coding standard (surveillance profile).\nOur tool includes three schemes: direct-coding scheme, differential- coding\nscheme, and reconstructed-coding scheme. We will illustrate the details of\nthese schemes, and perform analysis of their advantages and disadvantages,\nrespectively.", 
    "link": "http://arxiv.org/pdf/1503.00118v1", 
    "arxiv-id": "1503.00118v1"
},{
    "category": "cs.MM", 
    "author": "Ming-Ting Sun", 
    "title": "Region-Based Rate-Control for H.264/AVC for Low Bit-Rate Applications", 
    "publish": "2015-02-28T11:57:21Z", 
    "summary": "Rate-control plays an important role in video coding. However, in the\nconventional rate-control algorithms, the number and position of Macroblocks\n(MBs) inside one basic unit for rate-control is inflexible and predetermined.\nThe different characteristics of the MBs are not fully considered. Also, there\nis no overall optimization of the coding of basic units. This paper proposes a\nnew region-based rate-control scheme for H.264/AVC to improve the coding\nefficiency. The inter-frame information is explored to objectively divide one\nframe into multiple regions based on their rate-distortion behaviors. The MBs\nwith the similar characteristics are classified into the same region, and the\nentire region instead of a single MB or a group of contiguous MBs is treated as\na basic unit for rate-control. A linear rate-quantization stepsize model and a\nlinear distortion-quantization stepsize model are proposed to accurately\ndescribe the rate-distortion characteristics for the region-based basic units.\nMoreover, based on the above linear models, an overall optimization model is\nproposed to obtain suitable Quantization Parameters (QPs) for the region-based\nbasic units. Experimental results demonstrate that the proposed region-based\nrate-control approach can achieve both better subjective and objective quality\nby performing the rate-control adaptively with the content, compared to the\nconventional rate-control approaches.", 
    "link": "http://arxiv.org/pdf/1503.00121v1", 
    "arxiv-id": "1503.00121v1"
},{
    "category": "cs.MM", 
    "author": "Muhammad Zubair", 
    "title": "A Novel Image Steganographic Approach for Hiding Text in Color Images   using HSI Color Model", 
    "publish": "2015-03-02T01:41:39Z", 
    "summary": "Image Steganography is the process of embedding text in images such that its\nexistence cannot be detected by Human Visual System (HVS) and is known only to\nsender and receiver. This paper presents a novel approach for image\nsteganography using Hue-Saturation-Intensity (HSI) color space based on Least\nSignificant Bit (LSB). The proposed method transforms the image from RGB color\nspace to Hue-Saturation-Intensity (HSI) color space and then embeds secret data\ninside the Intensity Plane (I-Plane) and transforms it back to RGB color model\nafter embedding. The said technique is evaluated by both subjective and\nObjective Analysis. Experimentally it is found that the proposed method have\nlarger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and\nmultiple security levels which shows its superiority as compared to several\nexisting methods", 
    "link": "http://arxiv.org/pdf/1503.00388v1", 
    "arxiv-id": "1503.00388v1"
},{
    "category": "cs.MM", 
    "author": "Patrice Brault", 
    "title": "Gaussian Mixture Model Based Contrast Enhancement", 
    "publish": "2015-03-05T12:31:36Z", 
    "summary": "In this paper, a method for enhancing low contrast images is proposed. This\nmethod, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),\nbrings into play the Gaussian mixture modeling of histograms to model the\ncontent of the images. Based on the fact that each homogeneous area in natural\nimages has a Gaussian-shaped histogram, it decomposes the narrow histogram of\nlow contrast images into a set of scaled and shifted Gaussians. The individual\nhistograms are then stretched by increasing their variance parameters, and are\ndiffused on the entire histogram by scattering their mean parameters, to build\na broad version of the histogram. The number of Gaussians as well as their\nparameters are optimized to set up a GMM with lowest approximation error and\nhighest similarity to the original histogram. Compared to the existing\nhistogram-based methods, the experimental results show that the quality of\nGMMCE enhanced pictures are mostly consistent and outperform other benchmark\nmethods. Additionally, the computational complexity analysis show that GMMCE is\na low complexity method.", 
    "link": "http://arxiv.org/pdf/1503.01620v2", 
    "arxiv-id": "1503.01620v2"
},{
    "category": "cs.MM", 
    "author": "Prasanta K. Panigrahi", 
    "title": "Reliable SVD based Semi-blind and Invisible Watermarking Schemes", 
    "publish": "2015-03-06T12:42:32Z", 
    "summary": "A semi-blind watermarking scheme is presented based on Singular Value\nDecomposition (SVD), which makes essential use of the fact that, the SVD\nsubspace preserves significant amount of information of an image and is a one\nway decomposition. The principal components are used, along with the\ncorresponding singular vectors of the watermark image to watermark the target\nimage. For further security, the semi-blind scheme is extended to an invisible\nhash based watermarking scheme. The hash based scheme commits a watermark with\na key such that, it is incoherent with the actual watermark, and can only be\nextracted using the key. Its security is analyzed in the random oracle model\nand shown to be unforgeable, invisible and satisfying the property of\nnon-repudiation.", 
    "link": "http://arxiv.org/pdf/1503.01934v1", 
    "arxiv-id": "1503.01934v1"
},{
    "category": "cs.MM", 
    "author": "Ajit Danti", 
    "title": "A novel hash based least significant bit (2-3-3) image steganography in   spatial domain", 
    "publish": "2015-03-12T11:16:18Z", 
    "summary": "This paper presents a novel 2-3-3 LSB insertion method. The image\nsteganography takes the advantage of human eye limitation. It uses color image\nas cover media for embedding secret message.The important quality of a\nsteganographic system is to be less distortive while increasing the size of the\nsecret message. In this paper a method is proposed to embed a color secret\nimage into a color cover image. A 2-3-3 LSB insertion method has been used for\nimage steganography. Experimental results show an improvement in the Mean\nsquared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the\nproposed technique over the base technique of hash based 3-3-2 LSB insertion.", 
    "link": "http://arxiv.org/pdf/1503.03674v1", 
    "arxiv-id": "1503.03674v1"
},{
    "category": "cs.MM", 
    "author": "Junkyun Choi", 
    "title": "User Centric Content Management System for Open IPTV Over SNS (ICTC2012)", 
    "publish": "2015-03-14T03:56:25Z", 
    "summary": "Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have\nrecently been researched. Web-based content providers and telecommunications\ncompany (Telecom) based Internet protocol television (IPTV) providers have\nstruggled against each other to accommodate more three-screen service\nsubscribers. Since the advent of Web 2.0, more abundant reproduced content can\nbe circulated. However, because according to increasing device's resolution and\ncontent formats IPTV providers transcode content in advance, network bandwidth,\nstorage and operation costs for content management systems (CMSs) are wasted.\nIn this paper, we present a user centric CMS for open IPTV, which integrates\nSOA and Web 2.0. Considering content popularity based on a Zipf-like\ndistribution to solve these problems, we analyze the performance between the\nuser centric CMS and the conventional Web syndication system for normalized\ncosts. Based on the user centric CMS, we implement a social Web TV with\ndevice-aware function, which can aggregate, transcode, and deploy content over\nsocial networking service (SNS) independently.", 
    "link": "http://arxiv.org/pdf/1503.04263v1", 
    "arxiv-id": "1503.04263v1"
},{
    "category": "cs.MM", 
    "author": "Jiwu Huang", 
    "title": "Identification of Image Operations Based on Steganalytic Features", 
    "publish": "2015-03-16T16:34:48Z", 
    "summary": "Image forensics have attracted wide attention during the past decade. Though\nmany forensic methods have been proposed to identify image forgeries, most of\nthem are targeted ones, since their proposed features are highly dependent on\nthe image operation under investigation. The performance of the well-designed\nfeatures for detecting the targeted operation usually degrades significantly\nfor other operations. On the other hand, a wise attacker can perform\nanti-forensics to fool the existing forensic methods, making countering\nanti-forensics become an urgent need. In this paper, we try to find a universal\nfeature to detect various image processing and anti-forensic operations. Based\non our extensive experiments and analysis, we find that any image\nprocessing/anti-forensic operations would inevitably modify many image pixels.\nThis would change some inherent statistics within original images, which is\nsimilar to the case of steganography. Therefore, we model image\nprocessing/anti-forensic operations as steganography problems, and propose a\ndetection strategy by applying steganalytic features. With some advanced\nsteganalytic features, we are able to detect various image operations and\nfurther identify their types. In our experiments, we have tested several\nsteganalytic features on 11 different kinds of typical image processing\noperations and 4 kinds of anti-forensic operations. The experimental results\nhave shown that the proposed strategy significantly outperforms the existing\nforensic methods in both effectiveness and universality.", 
    "link": "http://arxiv.org/pdf/1503.04718v2", 
    "arxiv-id": "1503.04718v2"
},{
    "category": "cs.MM", 
    "author": "O. V. Pavlovskaya", 
    "title": "The blind detection for palette image watermarking without changing the   color", 
    "publish": "2015-03-17T09:09:48Z", 
    "summary": "To hide a binary pattern in the palette image a steganographic scheme with\nblind detection is considered. The embedding algorithm uses the Lehmer code by\npalette color permutations for which the cover image palette is generally\nrequired. The found transformation between the palette and RGB images allows to\nextract the hidden data without any cover work.", 
    "link": "http://arxiv.org/pdf/1503.04958v1", 
    "arxiv-id": "1503.04958v1"
},{
    "category": "cs.MM", 
    "author": "Wanjiun Liao", 
    "title": "Error-Resilient Multicasting for Multi-View 3D Videos in Wireless   Networks", 
    "publish": "2015-03-30T16:06:39Z", 
    "summary": "With the emergence of naked-eye 3D mobile devices, mobile 3D video services\nare becoming increasingly important for video service providers, such as\nYoutube and Netflix, while multi-view 3D videos have the potential to inspire a\nvariety of innovative applications. However, enabling multi-view 3D video\nservices may overwhelm WiFi networks when every view of a video are\nmulticasted. In this paper, therefore, we propose to incorporate\ndepth-image-based rendering (DIBR), which allows each mobile client to\nsynthesize the desired view from nearby left and right views, in order to\neffectively reduce the bandwidth consumption. Moreover, when each client\nsuffers from packet losses, retransmissions incur additional bandwidth\nconsumption and excess delay, which in turn undermines the quality of\nexperience in video applications. To address the above issue, we first discover\nthe merit of view protection via DIBR for multi-view video multicast using a\nmathematical analysis and then design a new protocol, named Multi-View Group\nManagement Protocol (MVGMP), to support the dynamic join and leave of users and\nthe change of desired views. The simulation results demonstrate that our\nprotocol effectively reduces bandwidth consumption and increases the\nprobability for each client to successfully playback the desired views in a\nmulti-view 3D video.", 
    "link": "http://arxiv.org/pdf/1503.08726v4", 
    "arxiv-id": "1503.08726v4"
},{
    "category": "cs.MM", 
    "author": "Mohammad Ali Akhaee", 
    "title": "Optimum Decoder for an Additive Video Watermarking with Laplacian Noise   in H.264", 
    "publish": "2015-06-04T08:10:13Z", 
    "summary": "In this paper, we investigate an additive video watermarking method in H.264\nstandard in presence of the Laplacian noise. In some applications, due to the\nloss of some pixels or a region of a frame, we resort to Laplacian noise rather\nthan Gaussian one. The embedding is performed in the transform domain; while an\noptimum and a sub-optimum decoder are derived for the proposed Laplacian model.\nSimulation results show that the proposed watermarking scheme has suitable\nperformance with enough transparency required for watermarking applications.", 
    "link": "http://arxiv.org/pdf/1506.01501v1", 
    "arxiv-id": "1506.01501v1"
},{
    "category": "cs.MM", 
    "author": "Sung Wook Baik", 
    "title": "A novel magic LSB substitution method (M-LSB-SM) using multi-level   encryption and achromatic component of an image", 
    "publish": "2015-06-06T01:50:29Z", 
    "summary": "Image Steganography is a thriving research area of information security where\nsecret data is embedded in images to hide its existence while getting the\nminimum possible statistical detectability. This paper proposes a novel magic\nleast significant bit substitution method (M-LSB-SM) for RGB images. The\nproposed method is based on the achromatic component (I-plane) of the\nhue-saturation-intensity (HSI) color model and multi-level encryption (MLE) in\nthe spatial domain. The input image is transposed and converted into an HSI\ncolor space. The I-plane is divided into four sub-images of equal size,\nrotating each sub-image with a different angle using a secret key. The secret\ninformation is divided into four blocks, which are then encrypted using an MLE\nalgorithm (MLEA). Each sub-block of the message is embedded into one of the\nrotated sub-images based on a specific pattern using magic LSB substitution.\nExperimental results validate that the proposed method not only enhances the\nvisual quality of stego images but also provides good imperceptibility and\nmultiple security levels as compared to several existing prominent methods.", 
    "link": "http://arxiv.org/pdf/1506.02100v1", 
    "arxiv-id": "1506.02100v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Optimal Layered Representation for Adaptive Interactive Multiview Video   Streaming", 
    "publish": "2015-06-25T17:24:13Z", 
    "summary": "We consider an interactive multiview video streaming (IMVS) system where\nclients select their preferred viewpoint in a given navigation window. To\nprovide high quality IMVS, many high quality views should be transmitted to the\nclients. However, this is not always possible due to the limited and\nheterogeneous capabilities of the clients. In this paper, we propose a novel\nadaptive IMVS solution based on a layered multiview representation where camera\nviews are organized into layered subsets to match the different clients\nconstraints. We formulate an optimization problem for the joint selection of\nthe views subsets and their encoding rates. Then, we propose an optimal and a\nreduced computational complexity greedy algorithms, both based on\ndynamic-programming. Simulation results show the good performance of our novel\nalgorithms compared to a baseline algorithm, proving that an effective IMVS\nadaptive solution should consider the scene content and the client capabilities\nand their preferences in navigation.", 
    "link": "http://arxiv.org/pdf/1506.07823v1", 
    "arxiv-id": "1506.07823v1"
},{
    "category": "cs.MM", 
    "author": "Ying He", 
    "title": "Low-latency compression of mocap data using learned spatial   decorrelation transform", 
    "publish": "2015-06-29T23:47:00Z", 
    "summary": "Due to the growing needs of human motion capture (mocap) in movie, video\ngames, sports, etc., it is highly desired to compress mocap data for efficient\nstorage and transmission. This paper presents two efficient frameworks for\ncompressing human mocap data with low latency. The first framework processes\nthe data in a frame-by-frame manner so that it is ideal for mocap data\nstreaming and time critical applications. The second one is clip-based and\nprovides a flexible tradeoff between latency and compression performance. Since\nmocap data exhibits some unique spatial characteristics, we propose a very\neffective transform, namely learned orthogonal transform (LOT), for reducing\nthe spatial redundancy. The LOT problem is formulated as minimizing square\nerror regularized by orthogonality and sparsity and solved via alternating\niteration. We also adopt a predictive coding and temporal DCT for temporal\ndecorrelation in the frame- and clip-based frameworks, respectively.\nExperimental results show that the proposed frameworks can produce higher\ncompression performance at lower computational cost and latency than the\nstate-of-the-art methods.", 
    "link": "http://arxiv.org/pdf/1506.08898v3", 
    "arxiv-id": "1506.08898v3"
},{
    "category": "cs.MM", 
    "author": "Ying He", 
    "title": "SLRMA: Sparse Low-Rank Matrix Approximation for Data Compression", 
    "publish": "2015-07-07T04:36:19Z", 
    "summary": "Low-rank matrix approximation (LRMA) is a powerful technique for signal\nprocessing and pattern analysis. However, its potential for data compression\nhas not yet been fully investigated in the literature. In this paper, we\npropose sparse low-rank matrix approximation (SLRMA), an effective\ncomputational tool for data compression. SLRMA extends the conventional LRMA by\nexploring both the intra- and inter-coherence of data samples simultaneously.\nWith the aid of prescribed orthogonal transforms (e.g., discrete cosine/wavelet\ntransform and graph transform), SLRMA decomposes a matrix into a product of two\nsmaller matrices, where one matrix is made of extremely sparse and orthogonal\ncolumn vectors, and the other consists of the transform coefficients.\nTechnically, we formulate SLRMA as a constrained optimization problem, i.e.,\nminimizing the approximation error in the least-squares sense regularized by\n$\\ell_0$-norm and orthogonality, and solve it using the inexact augmented\nLagrangian multiplier method. Through extensive tests on real-world data, such\nas 2D image sets and 3D dynamic meshes, we observe that (i) SLRMA empirically\nconverges well; (ii) SLRMA can produce approximation error comparable to LRMA\nbut in a much sparse form; (iii) SLRMA-based compression schemes significantly\noutperform the state-of-the-art in terms of rate-distortion performance.", 
    "link": "http://arxiv.org/pdf/1507.01673v2", 
    "arxiv-id": "1507.01673v2"
},{
    "category": "cs.MM", 
    "author": "JayatiGhosh Dastidar", 
    "title": "Data Hiding in Video using Triangularization LSB Technique", 
    "publish": "2015-07-19T03:05:26Z", 
    "summary": "The importance of data hiding in the field of Information Technology is a\nwidely accepted. The challenge is to be able to pass information in a manner\nthat the very existence of the message is unknown in order to repel attention\nof the potential attacker. Steganography is a technique that has been widely\nused to achieve this objective. However Steganography is often found to be\nlacking when it comes to hiding bulk data. Attempting to hide data in a video\novercomes this problem because of the large sized cover object (video) as\ncompared to an image in the case of steganography. This paper attempts to\npropose a scheme using which data can be hidden in a video. We focus on the\nTriangularization method and make use of the Least Significant Bit (LSB)\ntechnique in hiding messages in a video.", 
    "link": "http://arxiv.org/pdf/1507.05242v1", 
    "arxiv-id": "1507.05242v1"
},{
    "category": "cs.MM", 
    "author": "Hyoung Joong Kim", 
    "title": "Low Bit-Rate and High Fidelity Reversible Data Hiding", 
    "publish": "2015-07-29T09:25:44Z", 
    "summary": "An accurate predictor is crucial for histogram-shifting (HS) based reversible\ndata hiding methods. The embedding capacity is increased and the embedding\ndistortion is decreased simultaneously if the predictor can generate accurate\npredictions. In this paper, we propose an accurate linear predictor based on\nweighted least squares (WLS) estimation. The robustness of WLS helps the\nproposed predictor generate accurate predictions, especially in complex texture\nareas of an image, where other predictors usually fail. To further reduce the\nembedding distortion, we propose a new embedding method called dynamic\nhistogram shifting with pixel selection (DHS-PS) that selects not only the\nproper histogram bins but also the proper pixel locations to embed the given\ndata. As a result, the proposed method can obtain very high fidelity marked\nimages with low bit-rate data embedded. The experimental results show that the\nproposed method outperforms the state-of-the-art low bit-rate reversible data\nhiding method.", 
    "link": "http://arxiv.org/pdf/1507.08075v1", 
    "arxiv-id": "1507.08075v1"
},{
    "category": "cs.MM", 
    "author": "Lamiaa A. Elrefaei", 
    "title": "Arabic Text Watermarking: A Review", 
    "publish": "2015-08-06T18:39:19Z", 
    "summary": "The using of the internet with its technologies and applications have been\nincreased rapidly. So, protecting the text from illegal use is too needed .\nText watermarking is used for this purpose. Arabic text has many\ncharacteristics such existing of diacritics , kashida (extension character) and\npoints above or under its letters .Each of Arabic letters can take different\nshapes with different Unicode. These characteristics are utilized in the\nwatermarking process. In this paper, several methods are discussed in the area\nof Arabic text watermarking with its advantages and disadvantages .Comparison\nof these methods is done in term of capacity, robustness and Imperceptibility.", 
    "link": "http://arxiv.org/pdf/1508.01485v1", 
    "arxiv-id": "1508.01485v1"
},{
    "category": "cs.MM", 
    "author": "Roman Starosolski", 
    "title": "Reversible Denoising and Lifting Based Color Component Transformation   for Lossless Image Compression", 
    "publish": "2015-08-25T11:17:54Z", 
    "summary": "An undesirable side effect of reversible color space transformation, which\nconsists of lifting steps, is that while removing correlation it contaminates\ntransformed components with noise from other components. To remove correlation\nwithout increasing noise, we integrate denoising into the lifting steps and\nobtain a reversible image component transformation. For JPEG-LS, JPEG 2000, and\nJPEG XR algorithms in lossless mode, we find that the proposed method applied\nto the RDgDb color space transformation with a simple denoising filter is\nespecially effective for images in the native optical resolutions of\nacquisition devices, but may lead to increased bitrates for typical images. We\nalso present an efficient estimator of image component transformation effects.", 
    "link": "http://arxiv.org/pdf/1508.06106v2", 
    "arxiv-id": "1508.06106v2"
},{
    "category": "cs.MM", 
    "author": "Seyed Mehdi Hosseini Andargoli", 
    "title": "Compressive Video Sensing via Dictionary Learning and Forward Prediction", 
    "publish": "2015-08-30T21:47:00Z", 
    "summary": "In this paper, we propose a new framework for compressive video sensing (CVS)\nthat exploits the inherent spatial and temporal redundancies of a video\nsequence, effectively. The proposed method splits the video sequence into the\nkey and non-key frames followed by dividing each frame into small\nnon-overlapping blocks of equal sizes. At the decoder side, the key frames are\nreconstructed using adaptively learned sparsifying (ALS) basis via $\\ell_0$\nminimization, in order to exploit the spatial redundancy. Also, the\neffectiveness of three well-known dictionary learning algorithms is\ninvestigated in our method. For recovery of the non-key frames, a prediction of\nthe current frame is initialized, by using the previous reconstructed frame, in\norder to exploit the temporal redundancy. The prediction is employed in a\nproper optimization problem to recover the current non-key frame. To compare\nour experimental results with the results of some other methods, we employ peak\nsignal to noise ratio (PSNR) and structural similarity (SSIM) index as the\nquality assessor. The numerical results show the adequacy of our proposed\nmethod in CVS.", 
    "link": "http://arxiv.org/pdf/1508.07640v1", 
    "arxiv-id": "1508.07640v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "In-Network View Synthesis for Interactive Multiview Video Systems", 
    "publish": "2015-09-01T19:57:33Z", 
    "summary": "To enable Interactive multiview video systems with a minimum view-switching\ndelay, multiple camera views are sent to the users, which are used as reference\nimages to synthesize additional virtual views via depth-image-based rendering.\nIn practice, bandwidth constraints may however restrict the number of reference\nviews sent to clients per time unit, which may in turn limit the quality of the\nsynthesized viewpoints. We argue that the reference view selection should\nideally be performed close to the users, and we study the problem of in-network\nreference view synthesis such that the navigation quality is maximized at the\nclients. We consider a distributed cloud network architecture where data stored\nin a main cloud is delivered to end users with the help of cloudlets, i.e.,\nresource-rich proxies close to the users. In order to satisfy last-hop\nbandwidth constraints from the cloudlet to the users, a cloudlet re-samples\nviewpoints of the 3D scene into a discrete set of views (combination of\nreceived camera views and virtual views synthesized) to be used as reference\nfor the synthesis of additional virtual views at the client. This in-network\nsynthesis leads to better viewpoint sampling given a bandwidth constraint\ncompared to simple selection of camera views, but it may however carry a\ndistortion penalty in the cloudlet-synthesized reference views. We therefore\ncast a new reference view selection problem where the best subset of views is\ndefined as the one minimizing the distortion over a view navigation window\ndefined by the user under some transmission bandwidth constraints. We show that\nthe view selection problem is NP-hard, and propose an effective polynomial time\nalgorithm using dynamic programming to solve the optimization problem.\nSimulation results finally confirm the performance gain offered by virtual view\nsynthesis in the network.", 
    "link": "http://arxiv.org/pdf/1509.00464v1", 
    "arxiv-id": "1509.00464v1"
},{
    "category": "cs.MM", 
    "author": "Avinash K. Gulve", 
    "title": "Audio Steganography: LSB Technique Using a Pyramid Structure and Range   of Bytes", 
    "publish": "2015-09-09T04:00:59Z", 
    "summary": "The demand for keeping the information secure and confidential simultaneously\nhas been progressively increasing. Among various techniques- Audio\nSteganography, a technique of embedding information transparently in a digital\nmedia thereby restricting the access to such information has been prominently\ndeveloped. Imperceptibility, robustness, and payload or hiding capacity are the\nmain character for it. In earlier, LSB techniques increased payload capacity\nwould hamper robustness as well as imperceptibility of the cover media and vice\nversa. The proposed technique overcomes the problem. It provides relatively\ngood improvement in the payload capacity by dividing the bytes of cover media\ninto ranges to hide the bits of secret message appropriately. As well as due to\nthe use of ranges of bytes the robustness of cover media has maintained and\nimperceptibility preserved by using a pyramid structure.", 
    "link": "http://arxiv.org/pdf/1509.02630v1", 
    "arxiv-id": "1509.02630v1"
},{
    "category": "cs.MM", 
    "author": "Oscar C. Au", 
    "title": "Merge Frame Design for Video Stream Switching using Piecewise Constant   Functions", 
    "publish": "2015-09-10T03:27:33Z", 
    "summary": "The ability to efficiently switch from one pre-encoded video stream to\nanother (e.g., for bitrate adaptation or view switching) is important for many\ninteractive streaming applications. Recently, stream-switching mechanisms based\non distributed source coding (DSC) have been proposed. In order to reduce the\noverall transmission rate, these approaches provide a \"merge\" mechanism, where\ninformation is sent to the decoder such that the exact same frame can be\nreconstructed given that any one of a known set of side information (SI) frames\nis available at the decoder (e.g., each SI frame may correspond to a different\nstream from which we are switching). However, the use of bit-plane coding and\nchannel coding in many DSC approaches leads to complex coding and decoding. In\nthis paper, we propose an alternative approach for merging multiple SI frames,\nusing a piecewise constant (PWC) function as the merge operator. In our\napproach, for each block to be reconstructed, a series of parameters of these\nPWC merge functions are transmitted in order to guarantee identical\nreconstruction given the known side information blocks. We consider two\ndifferent scenarios. In the first case, a target frame is first given, and then\nmerge parameters are chosen so that this frame can be reconstructed exactly at\nthe decoder. In contrast, in the second scenario, the reconstructed frame and\nmerge parameters are jointly optimized to meet a rate-distortion criteria.\nExperiments show that for both scenarios, our proposed merge techniques can\noutperform both a recent approach based on DSC and the SP-frame approach in\nH.264, in terms of compression efficiency and decoder complexity.", 
    "link": "http://arxiv.org/pdf/1509.02995v1", 
    "arxiv-id": "1509.02995v1"
},{
    "category": "cs.MM", 
    "author": "S. Abolfazl Hosseini", 
    "title": "A New Method For Digital Watermarking Based on Combination of DCT and   PCA", 
    "publish": "2015-09-10T19:01:14Z", 
    "summary": "In the digital watermarking with DCT method,the watermark is located within a\nrange of DCT coefficients of the cover image. In this paper to use the\nlow-frequency band, a new method is proposed by using a combination of the DCT\nand PCA transform. The proposed method is compared to other DCT methods, our\nmethod is robust and keeps the quality of cover image, also increases capacity\nof the watermarking.", 
    "link": "http://arxiv.org/pdf/1509.03278v1", 
    "arxiv-id": "1509.03278v1"
},{
    "category": "cs.MM", 
    "author": "Indrajit Chakrabarti", 
    "title": "Hardware Implementation of Compressed Sensing based Low Complex Video   Encoder", 
    "publish": "2015-09-13T11:39:09Z", 
    "summary": "This paper presents a memory efficient VLSI architecture of low complex video\nencoder using three dimensional (3-D) wavelet and Compressed Sensing (CS) is\nproposed for space and low power video applications. Majority of the\nconventional video coding schemes are based on hybrid model, which requires\ncomplex operations like transform coding (DCT), motion estimation and\ndeblocking filter at the encoder. Complexity of the proposed encoder is reduced\nby replacing those complex operations by 3-D DWT and CS at the encoder. The\nproposed architecture uses 3-D DWT to enable the scalability with levels of\nwavelet decomposition and also to exploit the spatial and the temporal\nredundancies. CS provides the good error resilience and coding efficiency. At\nthe first stage of the proposed architecture for encoder, 3-D DWT has been\napplied (Lifting based 2-D DWT in spatial domain and Haar wavelet in temporal\ndomain) on each frame of the group of frames (GOF), and in the second stage CS\nmodule exploits the sparsity of the wavelet coefficients. Small set of linear\nmeasurements are extracted by projecting the sparse 3-D wavelet coefficients\nonto random Bernoulli matrix at the encoder. Compared with the best existing\n3-D DWT architectures, the proposed architecture for 3-D DWT requires less\nmemory and provide high throughput. For an N?N image, the proposed 3-D DWT\narchitecture consumes a total of only 2?(3N +40P) words of on-chip memory for\nthe one level of decomposition. The proposed architecture for an encoder is\nfirst of its kind and to the best of my knowledge, no architecture is noted for\ncomparison. The proposed VLSI architecture of the encoder has been synthesized\non 90-nm CMOS process technology and results show that it consumes 90.08 mW\npower and occupies an area equivalent to 416.799 K equivalent gate at frequency\nof 158 MHz.", 
    "link": "http://arxiv.org/pdf/1509.03836v1", 
    "arxiv-id": "1509.03836v1"
},{
    "category": "cs.MM", 
    "author": "Roch Glitho", 
    "title": "A Resource Allocation Mechanism for Video Mixing as a Cloud Computing   Service in Multimedia Conferencing Applications", 
    "publish": "2015-09-22T21:44:28Z", 
    "summary": "Multimedia conferencing is the conversational exchange of multimedia content\nbetween multiple parties. It has a wide range of applications (e.g. Massively\nMultiplayer Online Games (MMOGs) and distance learning). Many multimedia\nconferencing applications use video extensively, thus video mixing in\nconferencing settings is of critical importance. Cloud computing is a\ntechnology that can solve the scalability issue in multimedia conferencing,\nwhile bringing other benefits, such as, elasticity, efficient use of resources,\nrapid development, and introduction of new applications. However, proposed\ncloud-based multimedia conferencing approaches so far have several deficiencies\nwhen it comes to efficient resource usage while meeting Quality of Service\n(QoS) requirements. We propose a solution to optimize resource allocation for\ncloud-based video mixing service in multimedia conferencing applications, which\ncan support scalability in terms of number of users, while guaranteeing QoS. We\nformulate the resource allocation problem mathematically as an Integer Linear\nProgramming (ILP) problem and design a heuristic for it. Simulation results\nshow that our resource allocation model can support more participants compared\nto the state-of-the-art, while honoring QoS, with respect to end-to-end delay.", 
    "link": "http://arxiv.org/pdf/1509.06792v1", 
    "arxiv-id": "1509.06792v1"
},{
    "category": "cs.MM", 
    "author": "Dimitris Maroulis", 
    "title": "CVC: The Contourlet Video Compression algorithm for real-time   applications", 
    "publish": "2015-10-02T11:08:45Z", 
    "summary": "Nowadays, real-time video communication over the internet through video\nconferencing applications has become an invaluable tool in everyone's\nprofessional and personal life. This trend underlines the need for video coding\nalgorithms that provide acceptable quality on low bitrates and can support\nvarious resolutions inside the same stream in order to cope with limitations on\ncomputational resources and network bandwidth. In this work, a novel scalable\nvideo coding algorithm based on the contourlet transform is presented. The\nalgorithm utilizes both lossy and lossless methods in order to achieve\ncompression. One of its most notable features is that due to the transform\nutilised, it does not suffer from blocking artifacts that occur with many\nwidely adopted compression algorithms. The proposed algorithm takes advantage\nof the vast computational capabilities of modern GPUs, in order to achieve\nreal-time performance and provide satisfactory encoding and decoding times at\nrelatively low cost, making it suitable for applications like video\nconferencing. Experiments show that the proposed algorithm performs\nsatisfactorily in terms of compression ratio and speed, while it outperforms\nstandard methods in terms of perceptual quality on lower bitrates.", 
    "link": "http://arxiv.org/pdf/1510.00561v1", 
    "arxiv-id": "1510.00561v1"
},{
    "category": "cs.MM", 
    "author": "Eckehard Steinbach", 
    "title": "A System for Precise End-to-End Delay Measurements in Video   Communication", 
    "publish": "2015-10-05T13:04:55Z", 
    "summary": "Low delay video transmission is becoming increasingly important. Delay\ncritical, video enabled applications range from teleoperation scenarios such as\ncontrolling drones or telesurgery to autonomous control through computer vision\nalgorithms applied on real-time video. To judge the quality of the video\ntransmission in such a system, it is important to be able to precisely measure\nthe end-to-end (E2E) delay of the transmitted video. We present a\nlow-complexity system that automatically takes pairwise independent\nmeasurements of E2E delay. The precision can be far below the millisecond\norder, mainly limited by the sampling rate of the measurement system. In our\nimplementation, we achieve a precision of 0.5 milliseconds with a sampling rate\nof 2kHz.", 
    "link": "http://arxiv.org/pdf/1510.01134v2", 
    "arxiv-id": "1510.01134v2"
},{
    "category": "cs.MM", 
    "author": "Jean-Claude Dufourd", 
    "title": "The Virtual Splitter: Refactoring Web Applications for the Multiscreen   Environment", 
    "publish": "2015-10-19T09:40:17Z", 
    "summary": "Creating web applications for the multiscreen environment is still a\nchallenge. One approach is to transform existing single-screen applications but\nthis has not been done yet automatically or generically. This paper proposes a\nrefactor-ing system. It consists of a generic and extensible mapping phase that\nautomatically analyzes the application content based on a semantic or a visual\ncriterion determined by the author or the user, and prepares it for the\nsplitting process. The system then splits the application and as a result\ndelivers two instrumented applications ready for distribution across devices.\nDuring runtime, the system uses a mirroring phase to maintain the functionality\nof the distributed application and to support a dynamic splitting process.\nDeveloped as a Chrome extension, our approach is validated on several web\napplications, including a YouTube page and a video application from Mozilla.", 
    "link": "http://arxiv.org/pdf/1510.05405v1", 
    "arxiv-id": "1510.05405v1"
},{
    "category": "cs.MM", 
    "author": "Christophe Guyeux", 
    "title": "A new chaos-based watermarking algorithm", 
    "publish": "2015-10-31T12:30:52Z", 
    "summary": "This paper introduces a new watermarking algorithm based on discrete chaotic\niterations. After defining some coefficients deduced from the description of\nthe carrier medium, chaotic discrete iterations are used to mix the watermark\nand to embed it in the carrier medium. It can be proved that this procedure\ngenerates topological chaos, which ensures that desired properties of a\nwatermarking algorithm are satisfied.", 
    "link": "http://arxiv.org/pdf/1511.00118v1", 
    "arxiv-id": "1511.00118v1"
},{
    "category": "cs.MM", 
    "author": "Truong Cong Thang", 
    "title": "A Novel Adaptation Method for HTTP Streaming of VBR Videos over Mobile   Networks", 
    "publish": "2015-11-09T12:20:11Z", 
    "summary": "Recently, HTTP streaming has become very popular for delivering video over\nthe Internet. For adaptivity, a provider should generate multiple versions of a\nvideo as well as the related metadata. Various adaptation methods have been\nproposed to support a streaming client in coping with strong bandwidth\nvariations. However, most of existing methods target at constant bitrate (CBR)\nvideos only. In this paper, we present a new method for quality adaptation in\non-demand streaming of variable bitrate (VBR) videos. To cope with strong\nvariations of VBR bitrate, we use a local average bitrate as the representative\nbitrate of a version. A buffer-based algorithm is then proposed to\nconservatively adapt video quality. Through experiments, we show that our\nmethod can provide quality stability as well as buffer stability even under\nvery strong variations of bandwidth and video bitrates.", 
    "link": "http://arxiv.org/pdf/1511.02656v1", 
    "arxiv-id": "1511.02656v1"
},{
    "category": "cs.MM", 
    "author": "Lei Zhang", 
    "title": "Optimization of the Block-level Bit Allocation in Perceptual Video   Coding based on MINMAX", 
    "publish": "2015-11-15T12:27:43Z", 
    "summary": "In video coding, it is expected that the encoder could adaptively select the\nencoding parameters (e.g., quantization parameter) to optimize the bit\nallocation to different sources under the given constraint. However, in hybrid\nvideo coding, the dependency between sources brings high complexity for the bit\nallocation optimization, especially in the block-level, and existing\noptimization methods mostly focus on frame-level bit allocation. In this paper,\nwe propose a macroblock (MB) level bit allocation method based on the minimum\nmaximum (MINMAX) criterion, which has acceptable encoding complexity for\noffline applications. An iterative-based algorithm, namely maximum distortion\ndescend (MDD), is developed to reduce quality fluctuation among MBs within a\nframe, where the Structure SIMilarity (SSIM) index is used to measure the\nperceptual distortion of MBs. Our extensive experimental results on benchmark\nvideo sequences show that the proposed method can greatly enhance the encoding\nperformance in terms of both bits saving and perceptual quality improvement.", 
    "link": "http://arxiv.org/pdf/1511.04691v1", 
    "arxiv-id": "1511.04691v1"
},{
    "category": "cs.MM", 
    "author": "Khan Muhammad", 
    "title": "Steganography: A Secure way for Transmission in Wireless Sensor Networks", 
    "publish": "2015-11-28T04:00:05Z", 
    "summary": "Addressing the security concerns in wireless sensor networks (WSN) is a\nchallenging task, which has attracted the attention of many researchers from\nthe last few decades. Researchers have presented various schemes in WSN,\naddressing the problems of processing, bandwidth, load balancing, and efficient\nrouting. However, little work has been done on security aspects of WSN. In a\ntypical WSN network, the tiny nodes installed on different locations sense the\nsurrounding environment, send the collected data to their neighbors, which in\nturn is forwarded to a sink node. The sink node aggregate the data received\nfrom different sensors and send it to the base station for further processing\nand necessary actions. In highly critical sensor networks such as military and\nlaw enforcement agencies networks, the transmission of such aggregated data via\nthe public network Internet is very sensitive and vulnerable to various attacks\nand risks. Therefore, this paper provides a solution for addressing these\nsecurity issues based on steganography, where the aggregated data can be\nembedded as a secret message inside an innocent-looking cover image. The stego\nimage containing the embedded data can be then sent to fusion center using\nInternet. At the fusion center, the hidden data is extracted from the image,\nthe required processing is performed and decision is taken accordingly.\nExperimentally, the proposed method is evaluated by objective analysis using\npeak signal-to-noise ratio (PSNR), mean square error (MSE), normalized cross\ncorrelation (NCC), and structural similarity index metric (SSIM), providing\npromising results in terms of security and image quality, thus validating its\nsuperiority.", 
    "link": "http://arxiv.org/pdf/1511.08865v1", 
    "arxiv-id": "1511.08865v1"
},{
    "category": "cs.MM", 
    "author": "Qifei Wang", 
    "title": "An Overview of Emerging Technologies for High Efficiency 3D Video Coding", 
    "publish": "2015-12-30T05:41:21Z", 
    "summary": "3D video coding is one of the most popular research area in multimedia. This\npaper reviews the recent progress of the coding technologies for multiview\nvideo (MVV) and free view-point video (FVV) which is represented by MVV and\ndepth maps. We first discuss the traditional multiview video coding (MVC)\nframework with different prediction structures. The rate-distortion performance\nand the view switching delay of the three main coding prediction structures are\nanalyzed. We further introduce the joint coding technologies for MVV and depth\nmaps and evaluate the rate-distortion performance of them. The scalable 3D\nvideo coding technologies are reviewed by the quality and view scalability,\nrespectively. Finally, we summarize the bit allocation work of 3D video coding.\nThis paper also points out some future research problems in high efficiency 3D\nvideo coding such as the view switching latency optimization in coding\nstructure and bit allocation.", 
    "link": "http://arxiv.org/pdf/1512.08854v1", 
    "arxiv-id": "1512.08854v1"
},{
    "category": "cs.MM", 
    "author": "Mostafa Charmi", 
    "title": "Capacity Enlargement Of The PVD Steganography Method Using The GLM   Technique", 
    "publish": "2016-01-03T14:23:32Z", 
    "summary": "In most steganographic methods, increasing in the capacity leads to decrease\nin the quality of the stego-image, so in this paper, we propose to combine two\nexisting techniques, Pixel value differencing and Gray Level Modification, to\ncome up with a hybrid steganography scheme which can hide more information\nwithout having to compromise much on the quality of the stego-image.\nExperimental results demonstrate that the proposed approach has larger capacity\nwhile its results are imperceptible. In comparison with original PVD method\ncriterion of the quality is declined by 2% dB averagely while the capacity is\nincreased around 25%.", 
    "link": "http://arxiv.org/pdf/1601.00299v1", 
    "arxiv-id": "1601.00299v1"
},{
    "category": "cs.MM", 
    "author": "Zahoor Jan", 
    "title": "A New Image Steganographic Technique using Pattern based Bits Shuffling   and Magic LSB for Grayscale Images", 
    "publish": "2016-01-07T03:32:27Z", 
    "summary": "Image Steganography is a growing research area of information security where\nsecret information is embedded in innocent-looking public communication. This\npaper proposes a novel crystographic technique for grayscale images in spatial\ndomain. The secret data is encrypted and shuffled using pattern based bits\nshuffling algorithm (PBSA) and a secret key. The encrypted data is then\nembedded in the cover image using magic least significant bit (M-LSB) method.\nExperimentally, the proposed method is evaluated by qualitative and\nquantitative analysis which validates the effectiveness of the proposed method\nin contrast to several state-of-the-art methods.", 
    "link": "http://arxiv.org/pdf/1601.01386v1", 
    "arxiv-id": "1601.01386v1"
},{
    "category": "cs.MM", 
    "author": "Hannan Xiao", 
    "title": "Comparison of cinepak, intel, microsoft video and indeo codec for video   compression", 
    "publish": "2016-01-07T06:15:16Z", 
    "summary": "The file size and picture quality are factors to be considered for streaming,\nstorage and transmitting videos over networks. This work compares Cinepak,\nIntel, Microsoft Video and Indeo Codec for video compression. The peak signal\nto noise ratio is used to compare the quality of such video compressed using\nAVI codecs. The most widely used objective measurement by developers of video\nprocessing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise\nRation is measured on a logarithmic scale and depends on the mean squared error\n(MSE) between an original and an impaired image or video, relative to (2n-1)2.\n  Previous research done regarding assessing of video quality has been mainly\nby the use of subjective methods, and there is still no standard method for\nobjective assessments. Although it has been considered that compression might\nnot be significant in future as storage and transmission capabilities improve,\nbut at low bandwidths compression makes communication possible.", 
    "link": "http://arxiv.org/pdf/1601.01408v1", 
    "arxiv-id": "1601.01408v1"
},{
    "category": "cs.MM", 
    "author": "Niranjan Kulkarni", 
    "title": "An Enhanced Edge Adaptive Steganography Approach Using Threshold Value   for Region Selection", 
    "publish": "2016-01-09T04:48:08Z", 
    "summary": "This paper attempts to improve the quality and the modification rate of a\nStego Image. The input image provided for estimating the quality of an image\nand the modified rate is a bitmap image. The threshold value is used as a\nparameter for selecting the high frequency pixels from the Cover Image. The\ndata embedding process are performed on the pixels that are found with the help\nof Threshold value by using LSBMR. The quality of an image is estimated by the\nvalue of PSNR and the modification rate of an image is estimated by the value\nof MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the\nquality of an image and about 4 to 10 % of improvement in the modification rate\nof an image compared to the edge detection techniques such as Sobel and Canny.", 
    "link": "http://arxiv.org/pdf/1601.02076v1", 
    "arxiv-id": "1601.02076v1"
},{
    "category": "cs.MM", 
    "author": "Fatih Kamisli", 
    "title": "Lossless Intra Coding in HEVC with 3-tap Filters", 
    "publish": "2016-01-18T11:22:29Z", 
    "summary": "This paper presents a pixel-by-pixel spatial prediction method for lossless\nintra coding within High Efficiency Video Coding (HEVC). A well-known previous\npixel-by-pixel spatial prediction method uses only two neighboring pixels for\nprediction, based on the angular projection idea borrowed from block-based\nintra prediction in lossy coding. This paper explores a method which uses three\nneighboring pixels for prediction according to a two-dimensional correlation\nmodel, and the used neighbor pixels and prediction weights change depending on\nintra mode. To find the best prediction weights for each intra mode, a\ntwo-stage offline optimization algorithm is used and a number of implementation\naspects are discussed to simplify the proposed prediction method. The proposed\nmethod is implemented in the HEVC reference software and experimental results\nshow that the explored 3-tap filtering method can achieve an average 11.34%\nbitrate reduction over the default lossless intra coding in HEVC. The proposed\nmethod also decreases average decoding time by 12.7% while it increases average\nencoding time by 9.7%", 
    "link": "http://arxiv.org/pdf/1601.04473v1", 
    "arxiv-id": "1601.04473v1"
},{
    "category": "cs.MM", 
    "author": "Wei Liu", 
    "title": "Multiple Watermarking Algorithm Based on Spread Transform Dither   Modulation", 
    "publish": "2016-01-18T14:15:06Z", 
    "summary": "Multiple watermarking technique, embedding several watermarks in one carrier,\nhas enabled many interesting applications. In this study, a novel multiple\nwatermarking algorithm is proposed based on the spirit of spread transform\ndither modulation (STDM). It can embed multiple watermarks into the same region\nand the same transform domain of one image; meanwhile, the embedded watermarks\ncan be extracted independently and blindly in the detector without any\ninterference. Furthermore, to improve the fidelity of the watermarked image,\nthe properties of the dither modulation quantizer and the proposed multiple\nwatermarks embedding strategy are investigated, and two practical optimization\nmethods are proposed. Finally, to enhance the application flexibility, an\nextension of the proposed algorithm is proposed which can sequentially embeds\ndifferent watermarks into one image during each stage of its circulation.\nCompared with the pioneering multiple watermarking algorithms, the proposed one\nowns more flexibility in practical application and is more robust against\ndistortion due to basic operations such as random noise, JPEG compression and\nvolumetric scaling.", 
    "link": "http://arxiv.org/pdf/1601.04522v1", 
    "arxiv-id": "1601.04522v1"
},{
    "category": "cs.MM", 
    "author": "Bihan Wen", 
    "title": "Revisiting copy-move forgery detection by considering realistic image   with similar but genuine objects", 
    "publish": "2016-01-27T04:58:16Z", 
    "summary": "Many images, of natural or man-made scenes often contain Similar but Genuine\nObjects (SGO). This poses a challenge to existing Copy-Move Forgery Detection\n(CMFD) methods which match the key points / blocks, solely based on the pair\nsimilarity in the scene. To address such issue, we propose a novel CMFD method\nusing Scaled Harris Feature Descriptors (SHFD) that preform consistently well\non forged images with SGO. It involves the following main steps: (i) Pyramid\nscale space and orientation assignment are used to keep scaling and rotation\ninvariance; (ii) Combined features are applied for precise texture description;\n(iii) Similar features of two points are matched and RANSAC is used to remove\nthe false matches. The experimental results indicate that the proposed\nalgorithm is effective in detecting SGO and copy-move forgery, which compares\nfavorably to existing methods. Our method exhibits high robustness even when an\nimage is operated by geometric transformation and post-processing", 
    "link": "http://arxiv.org/pdf/1601.07262v1", 
    "arxiv-id": "1601.07262v1"
},{
    "category": "cs.MM", 
    "author": "Jose Sepulveda", 
    "title": "GECKA3D: A 3D Game Engine for Commonsense Knowledge Acquisition", 
    "publish": "2016-02-03T03:32:31Z", 
    "summary": "Commonsense knowledge representation and reasoning is key for tasks such as\nartificial intelligence and natural language understanding. Since commonsense\nconsists of information that humans take for granted, gathering it is an\nextremely difficult task. In this paper, we introduce a novel 3D game engine\nfor commonsense knowledge acquisition (GECKA3D) which aims to collect\ncommonsense from game designers through the development of serious games.\nGECKA3D integrates the potential of serious games and games with a purpose.\nThis provides a platform for the acquisition of re-usable and multi-purpose\nknowledge, and also enables the development of games that can provide\nentertainment value and teach players something meaningful about the actual\nworld they live in.", 
    "link": "http://arxiv.org/pdf/1602.01178v1", 
    "arxiv-id": "1602.01178v1"
},{
    "category": "cs.MM", 
    "author": "Ori Mashiach", 
    "title": "Adaptation Logic for HTTP Dynamic Adaptive Streaming using   Geo-Predictive Crowdsourcing", 
    "publish": "2016-02-05T14:17:52Z", 
    "summary": "The increasing demand for video streaming services with high Quality of\nExperience (QoE) has prompted a lot of research on client-side adaptation logic\napproaches. However, most algorithms use the client's previous download\nexperience and do not use a crowd knowledge database generated by users of a\nprofessional service. We propose a new crowd algorithm that maximizes the QoE.\nAdditionally, we show how crowd information can be integrated into existing\nalgorithms and illustrate this with two state-of-the-art algorithms. We\nevaluate our algorithm and state-of-the-art algorithms (including our modified\nalgorithms) on a large, real-life crowdsourcing dataset that contains 336,551\nsamples on network performance. The dataset was provided by WeFi LTD. Our new\nalgorithm outperforms all other methods in terms of QoS (eMOS).", 
    "link": "http://arxiv.org/pdf/1602.02030v1", 
    "arxiv-id": "1602.02030v1"
},{
    "category": "cs.MM", 
    "author": "Timothy B. Terriberry", 
    "title": "Perceptual Vector Quantization For Video Coding", 
    "publish": "2016-02-16T21:27:58Z", 
    "summary": "This paper applies energy conservation principles to the Daala video codec\nusing gain-shape vector quantization to encode a vector of AC coefficients as a\nlength (gain) and direction (shape). The technique originates from the CELT\nmode of the Opus audio codec, where it is used to conserve the spectral\nenvelope of an audio signal. Conserving energy in video has the potential to\npreserve textures rather than low-passing them. Explicitly quantizing a gain\nallows a simple contrast masking model with no signaling cost. Vector\nquantizing the shape keeps the number of degrees of freedom the same as scalar\nquantization, avoiding redundancy in the representation. We demonstrate how to\npredict the vector by transforming the space it is encoded in, rather than\nsubtracting off the predictor, which would make energy conservation impossible.\nWe also derive an encoding of the vector-quantized codewords that takes\nadvantage of their non-uniform distribution. We show that the resulting\ntechnique outperforms scalar quantization by an average of 0.90 dB on still\nimages, equivalent to a 24.8% reduction in bitrate at equal quality, while for\nvideos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in\nbitrate.", 
    "link": "http://arxiv.org/pdf/1602.05209v1", 
    "arxiv-id": "1602.05209v1"
},{
    "category": "cs.MM", 
    "author": "Jean-Marc Valin", 
    "title": "The Daala Directional Deringing Filter", 
    "publish": "2016-02-18T21:14:25Z", 
    "summary": "This paper presents the deringing filter used in the Daala royalty-free video\ncodec. The filter is based on a non-linear conditional replacement filter and\nis designed for vectorization efficiency. It takes into account the direction\nof edges and patterns being filtered. The filter works by identifying the\ndirection of each block and then adaptively filtering along the identified\ndirection. In a second pass, the blocks are also filtered in a different\ndirection, with more conservative thresholds to avoid blurring edges. The\nproposed deringing filter is shown to improve the quality of both Daala and the\nAlliance for Open Media (AOM) AV1 video codec.", 
    "link": "http://arxiv.org/pdf/1602.05975v2", 
    "arxiv-id": "1602.05975v2"
},{
    "category": "cs.MM", 
    "author": "Timothy B. Terriberry", 
    "title": "Daala: A Perceptually-Driven Next Generation Video Codec", 
    "publish": "2016-03-10T02:40:05Z", 
    "summary": "The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not, and what we've learned from\nthem. The result is a codec which compares favorably with HEVC on still images,\nand is on a path to do so for video as well.", 
    "link": "http://arxiv.org/pdf/1603.03129v1", 
    "arxiv-id": "1603.03129v1"
},{
    "category": "cs.MM", 
    "author": "Jiang Yu", 
    "title": "Towards Coordinated Bandwidth Adaptations for Hundred-Scale 3D   Tele-Immersive Systems", 
    "publish": "2016-03-19T11:43:01Z", 
    "summary": "3D tele-immersion improves the state of collaboration among geographically\ndistributed participants. Unlike the traditional 2D videos, a 3D tele-immersive\nsystem employs multiple 3D cameras based in each physical site to cover a much\nlarger field of view, generating a very large amount of stream data. One of the\nmajor challenges is how to efficiently transmit these bulky 3D streaming data\nto bandwidth-constrained sites. In this paper, we study an adaptive Human\nVisual System (HVS) -compliant bandwidth management framework for efficient\ndelivery of hundred-scale streams produced from distributed 3D tele-immersive\nsites to a receiver site with limited bandwidth budget. Our adaptation\nframework exploits the semantics link of HVS with multiple 3D streams in the 3D\ntele-immersive environment. We developed TELEVIS, a visual simulation tool to\nshowcase a HVS-aware tele-immersive system for realistic cases. Our evaluation\nresults show that the proposed adaptation can improve the total quality per\nunit of bandwidth used to deliver streams in 3D tele-immersive systems.", 
    "link": "http://arxiv.org/pdf/1603.06083v2", 
    "arxiv-id": "1603.06083v2"
},{
    "category": "cs.MM", 
    "author": "Fernando Pereira", 
    "title": "Optimal Lagrange Multipliers for Dependent Rate Allocation in Video   Coding", 
    "publish": "2016-03-19T18:02:17Z", 
    "summary": "In a typical video rate allocation problem, the objective is to optimally\ndistribute a source rate budget among a set of (in)dependently coded data units\nto minimize the total distortion of all units. Conventional Lagrangian\napproaches convert the lone rate constraint to a linear rate penalty scaled by\na multiplier in the objective, resulting in a simpler unconstrained\nformulation. However, the search for the \"optimal\" multiplier, one that results\nin a distortion-minimizing solution among all Lagrangian solutions that satisfy\nthe original rate constraint, remains an elusive open problem in the general\nsetting. To address this problem, we propose a computation-efficient search\nstrategy to identify this optimal multiplier numerically. Specifically, we\nfirst formulate a general rate allocation problem where each data unit can be\ndependently coded at different quantization parameters (QP) using a previous\nunit as predictor, or left uncoded at the encoder and subsequently interpolated\nat the decoder using neighboring coded units. After converting the original\nrate constrained problem to the unconstrained Lagrangian counterpart, we design\nan efficient dynamic programming (DP) algorithm that finds the optimal\nLagrangian solution for a fixed multiplier. Finally, within the DP framework,\nwe iteratively compute neighboring singular multiplier values, each resulting\nin multiple simultaneously optimal Lagrangian solutions, to drive the rates of\nthe computed Lagrangian solutions towards the bit budget. We terminate when a\nsingular multiplier value results in two Lagrangian solutions with rates below\nand above the bit budget. In extensive monoview and multiview video coding\nexperiments, we show that our DP algorithm and selection of optimal multipliers\non average outperform comparable rate control solutions used in video\ncompression standards such as HEVC that do not skip frames in Y-PSNR.", 
    "link": "http://arxiv.org/pdf/1603.06123v1", 
    "arxiv-id": "1603.06123v1"
},{
    "category": "cs.MM", 
    "author": "Ramesh Jain", 
    "title": "A framework for event co-occurrence detection in event streams", 
    "publish": "2016-03-30T01:16:37Z", 
    "summary": "This paper shows that characterizing co-occurrence between events is an\nimportant but non-trivial and neglected aspect of discovering potential causal\nrelationships in multimedia event streams. First an introduction to the notion\nof event co-occurrence and its relation to co-occurrence pattern detection is\ngiven. Then a finite state automaton extended with a time model and event\nparameterization is introduced to convert high level co-occurrence pattern\ndefinition to its corresponding pattern matching automaton. Finally a\nprocessing algorithm is applied to count the occurrence frequency of a\ncollection of patterns with only one pass through input event streams. The\nmethod proposed in this paper can be used for detecting co-occurrences between\nboth events of one event stream (Auto co-occurrence), and events from multiple\nevent streams (Cross co-occurrence). Some fundamental results concerning the\ncharacterization of event co-occurrence are presented in form of a visual co-\noccurrence matrix. Reusable causality rules can be extracted easily from\nco-occurrence matrix and fed into various analysis tools, such as\nrecommendation systems and complex event processing systems for further\nanalysis.", 
    "link": "http://arxiv.org/pdf/1603.09012v1", 
    "arxiv-id": "1603.09012v1"
},{
    "category": "cs.MM", 
    "author": "Mohammad Ali Zare Chahooki", 
    "title": "Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet   Transforms", 
    "publish": "2016-03-30T22:03:19Z", 
    "summary": "With the growth of digital networks such as the Internet, digital media have\nbeen explosively developed in e-commerce and online services. This causes\nproblems such as illegal copy and fake ownership. Watermarking is proposed as\none of the solutions to such cases. Among different watermarking techniques,\nthe wavelet transform has been used more because of its good ability in\nmodeling the human visual system. Recently, Shearlet transform as an extension\nof Wavelet transform which is based on multi-resolution and multi-directional\nanalysis is introduced. The most important feature of this transform is the\nappropriate representation of image edges. In this paper a hybrid scheme using\nDiscrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is\npresented. In this way, the host image is decomposed using DWT, and then its\nlow frequency sub-band is decomposed by DST. After that, the bidiagonal\nsingular value decomposition (BSVD) is applied on the selected sub-band from\nShearlet transform and the gray-scale watermark image is embedded into its\nbidiagonal singular values. The proposed method is examined on the images with\ndifferent textures and resistance is evaluated against various attacks like\nimage processing and geometric attacks. The results show good transparency and\nhigh robustness in proposed method.", 
    "link": "http://arxiv.org/pdf/1603.09396v1", 
    "arxiv-id": "1603.09396v1"
},{
    "category": "cs.MM", 
    "author": "Alberto Arribas", 
    "title": "A Practical Approach to Spatiotemporal Data Compression", 
    "publish": "2016-04-13T08:33:36Z", 
    "summary": "Datasets representing the world around us are becoming ever more unwieldy as\ndata volumes grow. This is largely due to increased measurement and modelling\nresolution, but the problem is often exacerbated when data are stored at\nspuriously high precisions. In an effort to facilitate analysis of these\ndatasets, computationally intensive calculations are increasingly being\nperformed on specialised remote servers before the reduced data are transferred\nto the consumer. Due to bandwidth limitations, this often means data are\ndisplayed as simple 2D data visualisations, such as scatter plots or images. We\npresent here a novel way to efficiently encode and transmit 4D data fields\non-demand so that they can be locally visualised and interrogated. This nascent\n\"4D video\" format allows us to more flexibly move the boundary between data\nserver and consumer client. However, it has applications beyond purely\nscientific visualisation, in the transmission of data to virtual and augmented\nreality.", 
    "link": "http://arxiv.org/pdf/1604.03688v2", 
    "arxiv-id": "1604.03688v2"
},{
    "category": "cs.MM", 
    "author": "Yun-Qing Shi", 
    "title": "Prediction-error of Prediction Error (PPE)-based Reversible Data Hiding", 
    "publish": "2016-04-18T04:52:27Z", 
    "summary": "This paper presents a novel reversible data hiding (RDH) algorithm for\ngray-scaled images, in which the prediction-error of prediction error (PPE) of\na pixel is used to carry the secret data. In the proposed method, the pixels to\nbe embedded are firstly predicted with their neighboring pixels to obtain the\ncorresponding prediction errors (PEs). Then, by exploiting the PEs of the\nneighboring pixels, the prediction of the PEs of the pixels can be determined.\nAnd, a sorting technique based on the local complexity of a pixel is used to\ncollect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will\nbe processed first for data embedding. By reversibly shifting the PPE histogram\n(PPEH) with optimized parameters, the pixels corresponding to the altered PPEH\nbins can be finally modified to carry the secret data. Experimental results\nhave implied that the proposed method can benefit from the prediction procedure\nof the PEs, sorting technique as well as parameters selection, and therefore\noutperform some state-of-the-art works in terms of payload-distortion\nperformance when applied to different images.", 
    "link": "http://arxiv.org/pdf/1604.04984v5", 
    "arxiv-id": "1604.04984v5"
},{
    "category": "cs.MM", 
    "author": "Fatih Kamisli", 
    "title": "Lossless Intra Coding in HEVC with Adaptive 3-tap Filters", 
    "publish": "2016-04-24T16:58:08Z", 
    "summary": "In pixel-by-pixel spatial prediction methods for lossless intra coding, the\nprediction is obtained by a weighted sum of neighbouring pixels. The proposed\nprediction approach in this paper uses a weighted sum of three neighbor pixels\naccording to a two-dimensional correlation model. The weights are obtained\nafter a three step optimization procedure. The first two stages are offline\nprocedures where the computed prediction weights are obtained offline from\ntraining sequences. The third stage is an online optimization procedure where\nthe offline obtained prediction weights are further fine-tuned and adapted to\neach encoded block during encoding using a rate-distortion optimized method and\nthe modification in this third stage is transmitted to the decoder as side\ninformation. The results of the simulations show average bit rate reductions of\n12.02% and 3.28% over the default lossless intra coding in HEVC and the\nwell-known Sample-based Angular Prediction (SAP) method, respectively.", 
    "link": "http://arxiv.org/pdf/1604.07051v1", 
    "arxiv-id": "1604.07051v1"
},{
    "category": "cs.MM", 
    "author": "Yufeng Shan", 
    "title": "Compressed-domain visual saliency models: A comparative study", 
    "publish": "2016-04-25T17:39:25Z", 
    "summary": "Computational modeling of visual saliency has become an important research\nproblem in recent years, with applications in video quality estimation, video\ncompression, object tracking, retargeting, summarization, and so on. While most\nvisual saliency models for dynamic scenes operate on raw video, several models\nhave been developed for use with compressed-domain information such as motion\nvectors and transform coefficients. This paper presents a comparative study of\neleven such models as well as two high-performing pixel-domain saliency models\non two eye-tracking datasets using several comparison metrics. The results\nindicate that highly accurate saliency estimation is possible based only on a\npartially decoded video bitstream. The strategies that have shown success in\ncompressed-domain saliency modeling are highlighted, and certain challenges are\nidentified as potential avenues for further improvement.", 
    "link": "http://arxiv.org/pdf/1604.07339v1", 
    "arxiv-id": "1604.07339v1"
},{
    "category": "cs.MM", 
    "author": "Sumohana S. Channappayya", 
    "title": "Subjective Assessment of H.264 Compressed Stereoscopic Video", 
    "publish": "2016-04-26T05:02:33Z", 
    "summary": "The tremendous growth in 3D (stereo) imaging and display technologies has led\nto stereoscopic content (video and image) becoming increasingly popular.\nHowever, both the subjective and the objective evaluation of stereoscopic video\ncontent has not kept pace with the rapid growth of the content. Further, the\navailability of standard stereoscopic video databases is also quite limited. In\nthis work, we attempt to alleviate these shortcomings. We present a\nstereoscopic video database and its subjective evaluation. We have created a\ndatabase containing a set of 144 distorted videos. We limit our attention to\nH.264 compression artifacts. The distorted videos were generated using 6\nuncompressed pristine videos of left and right views originally created by\nGoldmann et al. at EPFL [1]. Further, 19 subjects participated in the\nsubjective assessment task. Based on the subjective study, we have formulated a\nrelation between the 2D and stereoscopic subjective scores as a function of\ncompression rate and depth range. We have also evaluated the performance of\npopular 2D and 3D image/video quality assessment (I/VQA) algorithms on our\ndatabase.", 
    "link": "http://arxiv.org/pdf/1604.07519v1", 
    "arxiv-id": "1604.07519v1"
},{
    "category": "cs.MM", 
    "author": "Faisal Baig", 
    "title": "Compress Voice Transference over low Signal Strength in Satellite   Communication", 
    "publish": "2016-04-26T09:57:37Z", 
    "summary": "This paper presents the comparison of compression algorithms for voice\ntransferring method over SMS in satellite communication. Voice transferring\nmethod over SMS is useful in situations when signal strength is low and due to\npoor signal strength voice call connection is not possible to initiate or\nsignal dropped during voice call. This method has one serious flaw that it\nproduces large number of SMS while converting voice into SMS. Such issue is\ncatered to some extend by employing any compression algorithm. In this paper\nour major aim is to find best compression scheme for said method, for that\npurpose we compare 6 different types of compression algorithms which are; LZW\n(Lempel-Ziv-Welch), Huffman coding, PPM (Prediction by partial matching),\nArithmetic Coding (AC), BWT (Burrows-Wheeler-Transform), LZMA\n(Lempel-Ziv-Markov chain). This comparison shows that PPM compression method\noffers better compression ratio and produce small number of SMS. For\nexperimentation we use Thuraya SG-2520 satellite phone. Moreover, we develop an\napplication using J2ME platform[Ref:a]. We tested that application more than\n100 times and then we compare the result in terms of compression ratio of each\nalgorithm and number of connected SMS produce after each compression method.\nThe result of this study will help developers to choose better compression\nscheme for their respective applications.\nhttp://www.learnrnd.com/news.php?id=ISSUES_IN_MOLECULAR_COMMUNICATIONS", 
    "link": "http://arxiv.org/pdf/1604.07593v1", 
    "arxiv-id": "1604.07593v1"
},{
    "category": "cs.MM", 
    "author": "Dinei Florencio", 
    "title": "Context Tree based Image Contour Coding using A Geometric Prior", 
    "publish": "2016-04-27T10:00:41Z", 
    "summary": "If object contours in images are coded efficiently as side information, then\nthey can facilitate advanced image / video coding techniques, such as graph\nFourier transform coding or motion prediction of arbitrarily shaped pixel\nblocks. In this paper, we study the problem of lossless and lossy compression\nof detected contours in images. Specifically, we first convert a detected\nobject contour composed of contiguous between-pixel edges to a sequence of\ndirectional symbols drawn from a small alphabet. To encode the symbol sequence\nusing arithmetic coding, we compute an optimal variable-length context tree\n(VCT) $\\mathcal{T}$ via a maximum a posterior (MAP) formulation to estimate\nsymbols' conditional probabilities. MAP prevents us from overfitting given a\nsmall training set $\\mathcal{X}$ of past symbol sequences by identifying a VCT\n$\\mathcal{T}$ that achieves a high likelihood $P(\\mathcal{X}|\\mathcal{T})$ of\nobserving $\\mathcal{X}$ given $\\mathcal{T}$, and a large geometric prior\n$P(\\mathcal{T})$ stating that image contours are more often straight than\ncurvy. For the lossy case, we design efficient dynamic programming (DP)\nalgorithms that optimally trade off coding rate of an approximate contour\n$\\hat{\\mathbf{x}}$ given a VCT $\\mathcal{T}$ with two notions of distortion of\n$\\hat{\\mathbf{x}}$ with respect to the original contour $\\mathbf{x}$. To reduce\nthe size of the DP tables, a total suffix tree is derived from a given VCT\n$\\mathcal{T}$ for compact table entry indexing, reducing complexity.\nExperimental results show that for lossless contour coding, our proposed\nalgorithm outperforms state-of-the-art context-based schemes consistently for\nboth small and large training datasets. For lossy contour coding, our\nalgorithms outperform comparable schemes in the literature in rate-distortion\nperformance.", 
    "link": "http://arxiv.org/pdf/1604.08001v1", 
    "arxiv-id": "1604.08001v1"
},{
    "category": "cs.MM", 
    "author": "Faisal Baig", 
    "title": "Text writing in the air", 
    "publish": "2016-04-27T21:11:22Z", 
    "summary": "This paper presents a real time video based pointing method which allows\nsketching and writing of English text over air in front of mobile camera.\nProposed method have two main tasks: first it track the colored finger tip in\nthe video frames and then apply English OCR over plotted images in order to\nrecognize the written characters. Moreover, proposed method provides a natural\nhuman-system interaction in such way that it do not require keypad, stylus, pen\nor glove etc for character input. For the experiments, we have developed an\napplication using OpenCv with JAVA language. We tested the proposed method on\nSamsung Galaxy3 android mobile. Results show that proposed algorithm gains the\naverage accuracy of 92.083% when tested for different shaped alphabets. Here,\nmore than 3000 different Magnetic 3D shaped characters were used [Ref:\nhttp://learnrnd.com/news.php?id=Magnetic_3D_Bio_Printing]. Our proposed system\nis the software based approach and relevantly very simple, fast and easy. It\ndoes not require sensors or any hardware rather than camera and red tape.\nMoreover, proposed methodology can be applicable for all disconnected languages\nbut having one issue that it is color sensitive in such a way that existence of\nany red color in the background before starting the character writing can lead\nto false results.", 
    "link": "http://arxiv.org/pdf/1604.08245v1", 
    "arxiv-id": "1604.08245v1"
},{
    "category": "cs.MM", 
    "author": "Iyad F. Jafar", 
    "title": "Efficient Reversible Data Hiding Algorithms Based on Dual Prediction", 
    "publish": "2016-05-09T14:39:02Z", 
    "summary": "In this paper, a new reversible data hiding (RDH) algorithm that is based on\nthe concept of shifting of prediction error histograms is proposed. The\nalgorithm extends the efficient modification of prediction errors (MPE)\nalgorithm by incorporating two predictors and using one prediction error value\nfor data embedding. The motivation behind using two predictors is driven by the\nfact that predictors have different prediction accuracy which is directly\nrelated to the embedding capacity and quality of the stego image. The key\nfeature of the proposed algorithm lies in using two predictors without the need\nto communicate additional overhead with the stego image. Basically, the\nidentification of the predictor that is used during embedding is done through a\nset of rules. The proposed algorithm is further extended to use two and three\nbins in the prediction errors histogram in order to increase the embedding\ncapacity. Performance evaluation of the proposed algorithm and its extensions\nshowed the advantage of using two predictors in boosting the embedding capacity\nwhile providing competitive quality for the stego image.", 
    "link": "http://arxiv.org/pdf/1605.02605v1", 
    "arxiv-id": "1605.02605v1"
},{
    "category": "cs.MM", 
    "author": "Satoshi Goto", 
    "title": "Frame-level quality and memory traffic allocation for lossy embedded   compression in video codec systems", 
    "publish": "2016-05-10T12:29:18Z", 
    "summary": "For mobile video codecs, the huge energy dissipation for external memory\ntraffic is a critical challenge under the battery power constraint. Lossy\nembedded compression (EC), as a solution to this challenge, is considered in\nthis paper. While previous studies in EC mostly focused on compression\nalgorithms at the block level, this work, to the best of our knowledge, is the\nfirst one that addresses the allocation of video quality and memory traffic at\nthe frame level. For lossy EC, a main difficulty of its application lies in the\nerror propagation from quality degradation of reference frames. Instinctively,\nit is preferred to perform more lossy EC in non-reference frames to minimize\nthe quality loss. The analysis and experiments in this paper, however, will\nshow lossy EC should actually be distributed to more frames. Correspondingly,\nfor hierarchical-B GOPs, we developed an efficient allocation that outperforms\nthe non-reference-only allocation by up to 4.5 dB in PSNR. In comparison, the\nproposed allocation also delivers more consistent quality between frames by\nhaving lower PSNR fluctuation.", 
    "link": "http://arxiv.org/pdf/1605.02976v1", 
    "arxiv-id": "1605.02976v1"
},{
    "category": "cs.MM", 
    "author": "Yukihiko Yamashita", 
    "title": "Regression-based Intra-prediction for Image and Video Coding", 
    "publish": "2016-05-12T10:50:18Z", 
    "summary": "By utilizing previously known areas in an image, intra-prediction techniques\ncan find a good estimate of the current block. This allows the encoder to store\nonly the error between the original block and the generated estimate, thus\nleading to an improvement in coding efficiency. Standards such as AVC and HEVC\ndescribe expert-designed prediction modes operating in certain angular\norientations alongside separate DC and planar prediction modes. Being designed\npredictors, while these techniques have been demonstrated to perform well in\nimage and video coding applications, they do not necessarily fully utilize\nnatural image structures. In this paper, we describe a novel system for\ndeveloping predictors derived from natural image blocks. The proposed algorithm\nis seeded with designed predictors (e.g. HEVC-style prediction) and allowed to\niteratively refine these predictors through regularized regression. The\nresulting prediction models show significant improvements in estimation quality\nover their designed counterparts across all conditions while maintaining\nreasonable computational complexity. We also demonstrate how the proposed\nalgorithm handles the worst-case scenario of intra-prediction with no error\nreporting.", 
    "link": "http://arxiv.org/pdf/1605.03754v1", 
    "arxiv-id": "1605.03754v1"
},{
    "category": "cs.MM", 
    "author": "Stefan Valentin", 
    "title": "Backward-Shifted Strategies Based on SVC for HTTP Adaptive Video   Streaming", 
    "publish": "2016-05-12T14:01:14Z", 
    "summary": "Although HTTP-based video streaming can easily penetrate firewalls and profit\nfrom Web caches, the underlying TCP may introduce large delays in case of a\nsudden capacity loss. To avoid an interruption of the video stream in such\ncases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video\nCoding (SVC), BSC adds a time-shifted layer of redundancy to the video stream\nsuch that future frames are downloaded at any instant. This pre-fetched content\nmaintains a fluent video stream even under highly variant network conditions\nand leads to high Quality of Experience (QoE). We characterize this QoE gain by\nanalyzing initial buffering time, re-buffering time and content resolution\nusing the Ballot theorem. The probability generating functions of the playback\ninterruption and of the initial buffering latency are provided in closed form.\nWe further compute the quasi-stationary distribution of the video quality, in\norder to compute the average quality, as well as temporal variability in video\nquality. Employing these analytic results to optimize QoE shows interesting\ntrade-offs and video streaming at outstanding fluency.", 
    "link": "http://arxiv.org/pdf/1605.03815v1", 
    "arxiv-id": "1605.03815v1"
},{
    "category": "cs.MM", 
    "author": "Christopher Montgomery", 
    "title": "Daala: A Perceptually-Driven Still Picture Codec", 
    "publish": "2016-05-16T20:12:02Z", 
    "summary": "Daala is a new royalty-free video codec based on perceptually-driven coding\ntechniques. We explore using its keyframe format for still picture coding and\nshow how it has improved over the past year. We believe the technology used in\nDaala could be the basis of an excellent, royalty-free image format.", 
    "link": "http://arxiv.org/pdf/1605.04930v1", 
    "arxiv-id": "1605.04930v1"
},{
    "category": "cs.MM", 
    "author": "Fatih Kamisli", 
    "title": "Lossless Compression in HEVC with Integer-to-Integer Transforms", 
    "publish": "2016-05-17T11:32:13Z", 
    "summary": "Many approaches have been proposed to support lossless coding within video\ncoding standards that are primarily designed for lossy coding. The simplest\napproach is to just skip transform and quantization and directly entropy code\nthe prediction residual, which is used in HEVC version 1. However, this simple\napproach is inefficient for compression. More efficient approaches include\nprocessing the residual with DPCM prior to entropy coding. This paper explores\nan alternative approach based on processing the residual with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. Experiments with the HEVC reference software show competitive results.", 
    "link": "http://arxiv.org/pdf/1605.05118v1", 
    "arxiv-id": "1605.05118v1"
},{
    "category": "cs.MM", 
    "author": "Fatih Kamisli", 
    "title": "Lossless Intra Coding in HEVC with Integer-to-Integer DST", 
    "publish": "2016-05-17T11:54:04Z", 
    "summary": "It is desirable to support efficient lossless coding within video coding\nstandards, which are primarily designed for lossy coding, with as little\nmodification as possible. A simple approach is to skip transform and\nquantization, and directly entropy code the prediction residual, but this is\ninefficient for compression. A more efficient and popular approach is to\nprocess the residual block with DPCM prior to entropy coding. This paper\nexplores an alternative approach based on processing the residual block with\ninteger-to-integer (i2i) transforms. I2i transforms map integers to integers,\nhowever, unlike the integer transforms used in HEVC for lossy coding, they do\nnot increase the dynamic range at the output and can be used in lossless\ncoding. We use both an i2i DCT from the literature and a novel i2i\napproximation of the DST. Experiments with the HEVC reference software show\ncompetitive results.", 
    "link": "http://arxiv.org/pdf/1605.05319v1", 
    "arxiv-id": "1605.05319v1"
},{
    "category": "cs.MM", 
    "author": "Michel Salomon", 
    "title": "Steganalysis via a Convolutional Neural Network using Large Convolution   Filters for Embedding Process with Same Stego Key", 
    "publish": "2016-05-25T15:58:57Z", 
    "summary": "For the past few years, in the race between image steganography and\nsteganalysis, deep learning has emerged as a very promising alternative to\nsteganalyzer approaches based on rich image models combined with ensemble\nclassifiers. A key knowledge of image steganalyzer, which combines relevant\nimage features and innovative classification procedures, can be deduced by a\ndeep learning approach called Convolutional Neural Networks (CNN). These kind\nof deep learning networks is so well-suited for classification tasks based on\nthe detection of variations in 2D shapes that it is the state-of-the-art in\nmany image recognition problems. In this article, we design a CNN-based\nsteganalyzer for images obtained by applying steganography with a unique\nembedding key. This one is quite different from the previous study of {\\em Qian\net al.} and its successor, namely {\\em Pibre et al.} The proposed architecture\nembeds less convolutions, with much larger filters in the final convolutional\nlayer, and is more general: it is able to deal with larger images and lower\npayloads. For the \"same embedding key\" scenario, our proposal outperforms all\nother steganalyzers, in particular the existing CNN-based ones, and defeats\nmany state-of-the-art image steganography schemes.", 
    "link": "http://arxiv.org/pdf/1605.07946v3", 
    "arxiv-id": "1605.07946v3"
},{
    "category": "cs.MM", 
    "author": "Ruiqin Xiong", 
    "title": "Efficient Multiple Line-Based Intra Prediction for HEVC", 
    "publish": "2016-05-26T14:45:35Z", 
    "summary": "Traditional intra prediction usually utilizes the nearest reference line to\ngenerate the predicted block when considering strong spatial correlation.\nHowever, this kind of single line-based method does not always work well due to\nat least two issues. One is the incoherence caused by the signal noise or the\ntexture of other object, where this texture deviates from the inherent texture\nof the current block. The other reason is that the nearest reference line\nusually has worse reconstruction quality in block-based video coding. Due to\nthese two issues, this paper proposes an efficient multiple line-based intra\nprediction scheme to improve coding efficiency. Besides the nearest reference\nline, further reference lines are also utilized. The further reference lines\nwith relatively higher quality can provide potential better prediction. At the\nsame time, the residue compensation is introduced to calibrate the prediction\nof boundary regions in a block when we utilize further reference lines. To\nspeed up the encoding process, this paper designs several fast algorithms.\nExperimental results show that, compared with HM-16.9, the proposed fast search\nmethod achieves 2.0% bit saving on average and up to 3.7%, with increasing the\nencoding time by 112%.", 
    "link": "http://arxiv.org/pdf/1605.08308v2", 
    "arxiv-id": "1605.08308v2"
},{
    "category": "cs.MM", 
    "author": "Shiqiang Yang", 
    "title": "Improving Crowdsourced Live Streaming with Aggregated Edge Networks", 
    "publish": "2016-05-29T06:03:26Z", 
    "summary": "Recent years have witnessed a dramatic increase of user-generated video\nservices. In such user-generated video services, crowdsourced live streaming\n(e.g., Periscope, Twitch) has significantly challenged today's edge network\ninfrastructure: today's edge networks (e.g., 4G, Wi-Fi) have limited uplink\ncapacity support, making high-bitrate live streaming over such links\nfundamentally impossible. In this paper, we propose to let broadcasters (i.e.,\nusers who generate the video) upload crowdsourced video streams using\naggregated network resources from multiple edge networks. There are several\nchallenges in the proposal: First, how to design a framework that aggregates\nbandwidth from multiple edge networks? Second, how to make this framework\ntransparent to today's crowdsourced live streaming services? Third, how to\nmaximize the streaming quality for the whole system? We design a\nmulti-objective and deployable bandwidth aggregation system BASS to address\nthese challenges: (1) We propose an aggregation framework transparent to\ntoday's crowdsourced live streaming services, using an edge proxy box and\naggregation cloud paradigm; (2) We dynamically allocate geo-distributed cloud\naggregation servers to enable MPTCP (i.e., multi-path TCP), according to\nlocation and network characteristics of both broadcasters and the original\nstreaming servers; (3) We maximize the overall performance gain for the whole\nsystem, by matching streams with the best aggregation paths.", 
    "link": "http://arxiv.org/pdf/1605.08969v1", 
    "arxiv-id": "1605.08969v1"
},{
    "category": "cs.MM", 
    "author": "Shiqiang Yang", 
    "title": "Drone Streaming with Wi-Fi Grid Aggregation for Virtual Tour", 
    "publish": "2016-05-31T03:59:03Z", 
    "summary": "To provide a live, active and high-quality virtual touring streaming\nexperience, we propose an unmanned drone stereoscopic streaming paradigm using\na control and streaming infrastructure of a 2.4GHz Wi-Fi grid. Our system\nallows users to actively control the streaming captured by a drone, receive and\nwatch the streaming using a head mount display (HMD); a Wi-Fi grid is deployed\nacross the remote scene with multi-channel support to enable high-bitrate\nstream- ing broadcast from the drones. The system adopt a joint view adaptation\nand drone control scheme to enable fast viewer movement including both head\nrotation and touring. We implement the prototype on Dji M100 quadcopter and HTC\nVive in a demo scene.", 
    "link": "http://arxiv.org/pdf/1605.09486v1", 
    "arxiv-id": "1605.09486v1"
},{
    "category": "cs.MM", 
    "author": "Benjamin Rainer", 
    "title": "Which Adaptation Logic? An Objective and Subjective Performance   Evaluation of HTTP-based Adaptive Media Streaming Systems", 
    "publish": "2016-06-01T16:05:14Z", 
    "summary": "Multimedia content delivery over the Internet is predominantly using the\nHypertext Transfer Protocol (HTTP) as its primary protocol and multiple\nproprietary solutions exits. The MPEG standard Dynamic Adaptive Streaming over\nHTTP (DASH) provides an interoperable solution and in recent years various\nadaptation logics/algorithms have been proposed. However, to the best of our\nknowledge, there is no comprehensive evaluation of the various\nlogics/algorithms. Therefore, this paper provides a comprehensive evaluation of\nten different adaptation logics/algorithms, which have been proposed in the\npast years. The evaluation is done both objectively and subjectively. The\nformer is using a predefined bandwidth trajectory within a controlled\nenvironment and the latter is done in a real-world environment adopting\ncrowdsourcing. The results shall provide insights about which strategy can be\nadopted in actual deployment scenarios. Additionally, the evaluation\nmethodology described in this paper can be used to evaluate any other/new\nadaptation logic and to compare it directly with the results reported here.", 
    "link": "http://arxiv.org/pdf/1606.00341v1", 
    "arxiv-id": "1606.00341v1"
},{
    "category": "cs.MM", 
    "author": "Yun-Qing Shi", 
    "title": "Can Machine Learn Steganography? - Implementing LSB Substitution and   Matrix Coding Steganography with Feed-Forward Neural Networks", 
    "publish": "2016-06-16T17:52:59Z", 
    "summary": "In recent years, due to the powerful abilities to deal with highly complex\ntasks, the artificial neural networks (ANNs) have been studied in the hope of\nachieving human-like performance in many applications. Since the ANNs have the\nability to approximate complex functions from observations, it is\nstraightforward to consider the ANNs for steganography. In this paper, we aim\nto implement the well-known LSB substitution and matrix coding steganography\nwith the feed-forward neural networks (FNNs). Our experimental results have\nshown that, the used FNNs can achieve the data embedding operation of the LSB\nsubstitution and matrix coding steganography. For steganography with the ANNs,\nthough there may be some challenges to us, it would be very promising and\nvaluable to pay attention to the ANNs for steganography, which may be a new\ndirection for steganography.", 
    "link": "http://arxiv.org/pdf/1606.05294v1", 
    "arxiv-id": "1606.05294v1"
},{
    "category": "cs.MM", 
    "author": "Mohamed Cheriet", 
    "title": "A Note on Efficiency of Downsampling and Color Transformation in Image   Quality Assessment", 
    "publish": "2016-06-20T14:52:47Z", 
    "summary": "Several existing and successful full reference image quality assessment (IQA)\nmodels use linear color transformation and downsampling before measuring\nsimilarity or quality of images. This paper indicates to the right order of\nthese two procedures and that the existing models have not chosen the more\nefficient approach. In addition, efficiency of these metrics is not compared in\na fair basis in the literature.", 
    "link": "http://arxiv.org/pdf/1606.06152v1", 
    "arxiv-id": "1606.06152v1"
},{
    "category": "cs.MM", 
    "author": "Irfan Essa", 
    "title": "Leveraging Contextual Cues for Generating Basketball Highlights", 
    "publish": "2016-06-29T05:04:27Z", 
    "summary": "The massive growth of sports videos has resulted in a need for automatic\ngeneration of sports highlights that are comparable in quality to the\nhand-edited highlights produced by broadcasters such as ESPN. Unlike previous\nworks that mostly use audio-visual cues derived from the video, we propose an\napproach that additionally leverages contextual cues derived from the\nenvironment that the game is being played in. The contextual cues provide\ninformation about the excitement levels in the game, which can be ranked and\nselected to automatically produce high-quality basketball highlights. We\nintroduce a new dataset of 25 NCAA games along with their play-by-play stats\nand the ground-truth excitement data for each basket. We explore the\ninformativeness of five different cues derived from the video and from the\nenvironment through user studies. Our experiments show that for our study\nparticipants, the highlights produced by our system are comparable to the ones\nproduced by ESPN for the same games.", 
    "link": "http://arxiv.org/pdf/1606.08955v1", 
    "arxiv-id": "1606.08955v1"
},{
    "category": "cs.MM", 
    "author": "Sebastian M\u00f6ller", 
    "title": "Formal Definition of QoE Metrics", 
    "publish": "2016-07-01T17:19:27Z", 
    "summary": "This technical report formally defines the QoE metrics which are introduced\nand discussed in the article \"QoE Beyond the MOS: An In-Depth Look at QoE via\nBetter Metrics and their Relation to MOS\" by Tobias Ho{\\ss}feld, Poul E.\nHeegaard, Martin Varela, Sebastian M\\\"oller, accepted for publication in the\nSpringer journal \"Quality and User Experience\". Matlab scripts for computing\nthe QoE metrics for given data sets are available in GitHub.", 
    "link": "http://arxiv.org/pdf/1607.00321v1", 
    "arxiv-id": "1607.00321v1"
},{
    "category": "cs.MM", 
    "author": "Lifeng Sun", 
    "title": "Dynamic Flow Scheduling Strategy in Multihoming Video CDNs", 
    "publish": "2016-07-05T14:23:57Z", 
    "summary": "Multihoming for a video Content Delivery Network (CDN) allows edge peering\nservers to deliver video chunks through different Internet Service Providers\n(ISPs), to achieve an improved quality of service (QoS) for video streaming\nusers. However, since traditional strategies for a multihoming video CDN are\nsimply designed according to static rules, e.g., simply sending traffic via a\nISP which is the same as the ISP of client, they fail to dynamically allocate\nresources among different ISPs over time. In this paper, we perform measurement\nstudies to demonstrate that such static allocation mechanism is inefficient to\nmake full utilization of multiple ISPs' resources. To address this problem, we\npropose a dynamic flow scheduling strategy for multihoming video CDN. The\nchallenge is to find the control parameters that can guide the ISP selection\nwhen performing flow scheduling. Using a data-driven approach, we find factors\nthat have a major impact on the performance improvement in the dynamic flow\nscheduling. We further utilize an information gain approach to generate\nparameter combinations that can be used to guide the flow scheduling, i.e., to\ndetermine the ISP each request should be responded by. Our evaluation results\ndemonstrate that our design effectively performs the flow scheduling. In\nparticular, our design yields near optimal performance in a simulation of\nreal-world multihoming setup.", 
    "link": "http://arxiv.org/pdf/1607.01261v1", 
    "arxiv-id": "1607.01261v1"
},{
    "category": "cs.MM", 
    "author": "Stavros D. Nikolopoulos", 
    "title": "Two RPG Flow-graphs for Software Watermarking using Bitonic Sequences of   Self-inverting Permutations", 
    "publish": "2016-07-08T09:12:07Z", 
    "summary": "Software watermarking has received considerable attention and was adopted by\nthe software development community as a technique to prevent or discourage\nsoftware piracy and copyright infringement. A wide range of software\nwatermarking techniques has been proposed among which the graph-based methods\nthat encode watermarks as graph structures. Following up on our recently\nproposed methods for encoding watermark numbers $w$ as reducible permutation\nflow-graphs $F[\\pi^*]$ through the use of self-inverting permutations $\\pi^*$,\nin this paper, we extend the types of flow-graphs available for software\nwatermarking by proposing two different reducible permutation flow-graphs\n$F_1[\\pi^*]$ and $F_2[\\pi^*]$ incorporating important properties which are\nderived from the bitonic subsequences composing the self-inverting permutation\n$\\pi^*$. We show that a self-inverting permutation $\\pi^*$ can be efficiently\nencoded into either $F_1[\\pi^*]$ or $F_2[\\pi^*]$ and also efficiently decoded\nfrom theses graph structures. The proposed flow-graphs $F_1[\\pi^*]$ and\n$F_2[\\pi^*]$ enrich the repository of graphs which can encode the same\nwatermark number $w$ and, thus, enable us to embed multiple copies of the same\nwatermark $w$ into an application program $P$. Moreover, the enrichment of that\nrepository with new flow-graphs increases our ability to select a graph\nstructure more similar to the structure of a given application program $P$\nthereby enhancing the resilience of our codec system to attacks.", 
    "link": "http://arxiv.org/pdf/1607.02281v2", 
    "arxiv-id": "1607.02281v2"
},{
    "category": "cs.MM", 
    "author": "Ngan King Ngi", 
    "title": "Hybrid Video Signal Coding Technologies: Past, Current and Future", 
    "publish": "2016-07-20T03:23:49Z", 
    "summary": "The growing needs for high-quality video applications have resulted in a lot\nof studies and developments in video signal coding. This chapter presents some\nadvanced techniques in enhancing the rate-distortion performance of the\nblock-based hybrid video coding systems. Additionally, as can be seen from the\ndevelopments of H.264/AVC and HEVC, most of the current coding tools, such as\nprediction, transformation and entropy coding, have less room to improve in the\ncompression performance. On the other hand, loop filer in the modern video\nstandards shows the promising results. Thus, we believe that loop filter can be\nthe candidate in contributing to higher video compression for the\nnext-generation video coding. Specifically, improvements on ALF and SAO are\nalso introduced, and the simulation results show that the proposed methods\noutperform the existing method, which offer new degrees of freedom to improve\nthe overall rate-distortion performance. As a result, they can be the candidate\ncoding tools for the next-generation video codec.", 
    "link": "http://arxiv.org/pdf/1607.05808v1", 
    "arxiv-id": "1607.05808v1"
},{
    "category": "cs.MM", 
    "author": "Mansour Jamzad", 
    "title": "Restoring highly corrupted images by impulse noise using radial basis   functions interpolation", 
    "publish": "2016-07-22T19:44:01Z", 
    "summary": "Preserving details in restoring images highly corrupted by impulse noise\nremains a challenging problem. We proposed an algorithm based on radial basis\nfunctions (RBF) interpolation which estimates the intensities of corrupted\npixels by their neighbors. In this algorithm, first intensity values of noisy\npixels in the corrupted image are estimated using RBFs. Next, the image is\nsmoothed. The proposed algorithm can effectively remove the highly dense\nimpulse noise. Experimental results show the superiority of the proposed\nalgorithm in comparison to the recent similar methods both in noise suppression\nand detail preservation. Extensive simulations show better results in measure\nof peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM),\nespecially when the image is corrupted by very highly dense impulse noise.", 
    "link": "http://arxiv.org/pdf/1607.06803v3", 
    "arxiv-id": "1607.06803v3"
},{
    "category": "cs.MM", 
    "author": "Roman Starosolski", 
    "title": "Skipping Selected Steps of DWT Computation in Lossless JPEG 2000 for   Improved Bitrates", 
    "publish": "2016-08-01T20:59:30Z", 
    "summary": "In order to improve bitrates of lossless JPEG 2000, we propose to modify the\ndiscrete wavelet transform (DWT) by skipping selected steps of its computation.\nWe employ a heuristic to construct the skipped steps DWT (SS-DWT) in an\nimage-adaptive way and define fixed SS-DWT variants. For a large and diverse\nset of images, we find that SS-DWT significantly improves bitrates of\nnon-photographic images. From a practical standpoint, the most interesting\nresults are obtained by applying entropy estimation of coding effects for\nselecting among the fixed SS-DWT variants. This way we get the compression\nscheme that, as opposed to the general SS-DWT case, is compliant with the JPEG\n2000 part 2 standard. It provides average bitrate improvement of roughly 5% for\nthe entire test-set, whereas the overall compression time becomes only 3%\ngreater than that of the unmodified JPEG 2000. Bitrates of photographic and\nnon-photographic images are improved by roughly 0.5% and 14%, respectively. At\na significantly increased cost of exploiting a heuristic, selecting the steps\nto be skipped based on the actual bitrate instead of an estimated one, and by\napplying reversible denoising and lifting steps to SS-DWT, we have attained\ngreater bitrate improvements of up to about 17.5% for non-photographic images.", 
    "link": "http://arxiv.org/pdf/1608.00613v2", 
    "arxiv-id": "1608.00613v2"
},{
    "category": "cs.MM", 
    "author": "Michael Bebenita", 
    "title": "Daala: Building A Next-Generation Video Codec From Unconventional   Technology", 
    "publish": "2016-08-05T17:36:51Z", 
    "summary": "Daala is a new royalty-free video codec that attempts to compete with\nstate-of-the-art royalty-bearing codecs. To do so, it must achieve good\ncompression while avoiding all of their patented techniques. We use technology\nthat is as different as possible from traditional approaches to achieve this.\nThis paper describes the technology behind Daala and discusses where it fits in\nthe newly created AV1 codec from the Alliance for Open Media. We show that\nDaala is approaching the performance level of more mature, state-of-the art\nvideo codecs and can contribute to improving AV1.", 
    "link": "http://arxiv.org/pdf/1608.01947v1", 
    "arxiv-id": "1608.01947v1"
},{
    "category": "cs.MM", 
    "author": "Busra Canturk", 
    "title": "MT3S: Mobile Turkish Scene Text-to-Speech System for the Visually   Impaired", 
    "publish": "2016-08-17T19:24:23Z", 
    "summary": "Reading text is one of the essential needs of the visually impaired people.\nWe developed a mobile system that can read Turkish scene and book text, using a\nfast gradient-based multi-scale text detection algorithm for real-time\noperation and Tesseract OCR engine for character recognition. We evaluated the\nOCR accuracy and running time of our system on a new, publicly available mobile\nTurkish scene text dataset we constructed and also compared with\nstate-of-the-art systems. Our system proved to be much faster, able to run on a\nmobile device, with OCR accuracy comparable to the state-of-the-art.", 
    "link": "http://arxiv.org/pdf/1608.05054v1", 
    "arxiv-id": "1608.05054v1"
},{
    "category": "cs.MM", 
    "author": "Feng Wu", 
    "title": "A Convolutional Neural Network Approach for Post-Processing in HEVC   Intra Coding", 
    "publish": "2016-08-24T02:15:06Z", 
    "summary": "Lossy image and video compression algorithms yield visually annoying\nartifacts including blocking, blurring, and ringing, especially at low\nbit-rates. To reduce these artifacts, post-processing techniques have been\nextensively studied. Recently, inspired by the great success of convolutional\nneural network (CNN) in computer vision, some researches were performed on\nadopting CNN in post-processing, mostly for JPEG compressed images. In this\npaper, we present a CNN-based post-processing algorithm for High Efficiency\nVideo Coding (HEVC), the state-of-the-art video coding standard. We redesign a\nVariable-filter-size Residue-learning CNN (VRCNN) to improve the performance\nand to accelerate network training. Experimental results show that using our\nVRCNN as post-processing leads to on average 4.6% bit-rate reduction compared\nto HEVC baseline. The VRCNN outperforms previously studied networks in\nachieving higher bit-rate reduction, lower memory cost, and multiplied\ncomputational speedup.", 
    "link": "http://arxiv.org/pdf/1608.06690v2", 
    "arxiv-id": "1608.06690v2"
},{
    "category": "cs.MM", 
    "author": "Haiyang Wang", 
    "title": "Towards Hybrid Cloud-assisted Crowdsourced Live Streaming: Measurement   and Analysis", 
    "publish": "2016-08-31T21:14:57Z", 
    "summary": "Crowdsourced Live Streaming (CLS), most notably Twitch.tv, has seen explosive\ngrowth in its popularity in the past few years. In such systems, any user can\nlively broadcast video content of interest to others, e.g., from a game player\nto many online viewers. To fulfill the demands from both massive and\nheterogeneous broadcasters and viewers, expensive server clusters have been\ndeployed to provide video ingesting and transcoding services. Despite the\nexistence of highly popular channels, a significant portion of the channels is\nindeed unpopular. Yet as our measurement shows, these broadcasters are\nconsuming considerable system resources; in particular, 25% (resp. 30%) of\nbandwidth (resp. computation) resources are used by the broadcasters who do not\nhave any viewers at all. In this paper, we closely examine the challenge of\nhandling unpopular live-broadcasting channels in CLS systems and present a\ncomprehensive solution for service partitioning on hybrid cloud. The\ntrace-driven evaluation shows that our hybrid cloud-assisted design can smartly\nassign ingesting and transcoding tasks to the elastic cloud virtual machines,\nproviding flexible system deployment cost-effectively.", 
    "link": "http://arxiv.org/pdf/1609.00045v1", 
    "arxiv-id": "1609.00045v1"
},{
    "category": "cs.MM", 
    "author": "Alejandro Jaimes", 
    "title": "To Click or Not To Click: Automatic Selection of Beautiful Thumbnails   from Videos", 
    "publish": "2016-09-06T04:33:34Z", 
    "summary": "Thumbnails play such an important role in online videos. As the most\nrepresentative snapshot, they capture the essence of a video and provide the\nfirst impression to the viewers; ultimately, a great thumbnail makes a video\nmore attractive to click and watch. We present an automatic thumbnail selection\nsystem that exploits two important characteristics commonly associated with\nmeaningful and attractive thumbnails: high relevance to video content and\nsuperior visual aesthetic quality. Our system selects attractive thumbnails by\nanalyzing various visual quality and aesthetic metrics of video frames, and\nperforms a clustering analysis to determine the relevance to video content,\nthus making the resulting thumbnails more representative of the video. On the\ntask of predicting thumbnails chosen by professional video editors, we\ndemonstrate the effectiveness of our system against six baseline methods, using\na real-world dataset of 1,118 videos collected from Yahoo Screen. In addition,\nwe study what makes a frame a good thumbnail by analyzing the statistical\nrelationship between thumbnail frames and non-thumbnail frames in terms of\nvarious image quality features. Our study suggests that the selection of a good\nthumbnail is highly correlated with objective visual quality metrics, such as\nthe frame texture and sharpness, implying the possibility of building an\nautomatic thumbnail selection system based on visual aesthetics.", 
    "link": "http://arxiv.org/pdf/1609.01388v1", 
    "arxiv-id": "1609.01388v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Optimal Representations for Adaptive Streaming in Interactive Multi-View   Video Systems", 
    "publish": "2016-09-14T10:01:17Z", 
    "summary": "Interactive multi-view video streaming (IMVS) services permit to remotely\nimmerse within a 3D scene. This is possible by transmitting a set of reference\ncamera views (anchor views), which are used by the clients to freely navigate\nin the scene and possibly synthesize additional viewpoints of interest. From a\nnetworking perspective, the big challenge in IMVS systems is to deliver to each\nclient the best set of anchor views that maximizes the navigation quality,\nminimizes the view-switching delay and yet satisfies the network constraints.\nIntegrating adaptive streaming solutions in free-viewpoint systems offers a\npromising solution to deploy IMVS in large and heterogeneous scenarios, as long\nas the multi-view video representations on the server are properly selected. We\ntherefore propose to optimize the multi-view data at the server by minimizing\nthe overall resource requirements, yet offering a good navigation quality to\nthe different users. We propose a video representation set optimization for\nmultiview adaptive streaming systems and we show that it is NP-hard. We\ntherefore introduce the concept of multi-view navigation segment that permits\nto cast the video representation set selection as an integer linear programming\nproblem with a bounded computational complexity. We then show that the proposed\nsolution reduces the computational complexity while preserving optimality in\nmost of the 3D scenes. We then provide simulation results for different classes\nof users and show the gain offered by an optimal multi-view video\nrepresentation selection compared to recommended representation sets (e.g.,\nNetflix and Apple ones) or to a baseline representation selection algorithm\nwhere the encoding parameters are decided a priori for all the views.", 
    "link": "http://arxiv.org/pdf/1609.04196v1", 
    "arxiv-id": "1609.04196v1"
},{
    "category": "cs.MM", 
    "author": "Victor Sanchez", 
    "title": "Color-Based Coding Unit Level Adaptive Quantization for HEVC", 
    "publish": "2016-09-15T15:28:41Z", 
    "summary": "HEVC HM 16 includes a Coding Unit (CU) level perceptual quantization\ntechnique named AdaptiveQP. AdaptiveQP adjusts the Quantization Parameter (QP)\nat the CU level based on the spatial activity of samples in the four\nconstituent NxN sub-blocks of the luma Coding Block (CB), which is contained\nwithin a 2Nx2N CU. In this paper, we propose C-BAQ, which, in contrast to\nAdaptiveQP, adjusts the CU level QP according to the spatial activity of\nsamples in the four constituent NxN sub-blocks of both the luma and chroma CBs.\nBy computing the sum of luma, chroma Cb and chroma Cr spatial activity in a CU,\na richer reflection of spatial activity in the CU is attained. Therefore, a\nmore appropriate CU level QP can be selected, thus leading to important\nimprovements in terms of coding efficiency. We evaluate the proposed technique\nin HEVC HM 16.7 using 4:4:4, 4:2:2 and 4:2:0 YCbCr sequences. Both subjective\nand objective evaluations are undertaken during which we compare C-BAQ with\nAdaptiveQP. The objective evaluation reveals that C-BAQ attains a maximum\nBD-Rate reduction of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a\nmaximum decoding time reduction of 11.0%.", 
    "link": "http://arxiv.org/pdf/1609.06302v2", 
    "arxiv-id": "1609.06302v2"
},{
    "category": "cs.MM", 
    "author": "Victor Sanchez", 
    "title": "Minimizing Compression Artifacts for High Resolutions with Adaptive   Quantization Matrices for HEVC", 
    "publish": "2016-09-21T07:30:18Z", 
    "summary": "Visual Display Units (VDUs), capable of displaying video data at High\nDefinition (HD) and Ultra HD (UHD) resolutions, are frequently employed in a\nvariety of technological domains. Quantization-induced video compression\nartifacts, which are usually unnoticeable in low resolution environments, are\ntypically conspicuous on high resolution VDUs and video data. The default\nquantization matrices (QMs) in HEVC do not take into account specific display\nresolutions of VDUs or video data to determine the appropriate levels of\nquantization required to reduce unwanted compression artifacts. Therefore, we\npropose a novel, adaptive quantization matrix technique for the HEVC standard\nincluding Scalable HEVC (SHVC). Our technique, which is based on a refinement\nof the current QM technique in HEVC, takes into consideration specific display\nresolutions of the target VDUs in order to minimize compression artifacts. We\nundertake a thorough evaluation of the proposed technique by utilizing SHVC SHM\n9.0 (two-layered bit-stream) and the BD-Rate and SSIM metrics. For the BD-Rate\nevaluation, the proposed method achieves maximum BD-Rate reductions of 56.5% in\nthe enhancement layer. For the SSIM evaluation, our technique achieves a\nmaximum structural improvement of 0.8660 vs. 0.8538.", 
    "link": "http://arxiv.org/pdf/1609.06442v1", 
    "arxiv-id": "1609.06442v1"
},{
    "category": "cs.MM", 
    "author": "Jean-Charles Gr\u00e9goire", 
    "title": "Multimedia Communication Quality Assessment Testbeds", 
    "publish": "2016-09-21T15:59:59Z", 
    "summary": "We make an intensive use of multimedia frameworks in our research on modeling\nthe perceived quality estimation in streaming services and real-time\ncommunications. In our preliminary work, we have used the VLC VOD software to\ngenerate reference audiovisual files with various degree of coding and network\ndegradations. We have successfully built machine learning based models on the\nsubjective quality dataset we have generated using these files. However,\nimperfections in the dataset introduced by the multimedia framework we have\nused prevented us from achieving the full potential of these models.\n  In order to develop better models, we have re-created our end-to-end\nmultimedia pipeline using the GStreamer framework for audio and video\nstreaming. A GStreamer based pipeline proved to be significantly more robust to\nnetwork degradations than the VLC VOD framework and allowed us to stream a\nvideo flow at a loss rate up to 5\\% packet very easily. GStreamer has also\nenabled us to collect the relevant RTCP statistics that proved to be more\naccurate than network-deduced information. This dataset is free to the public.\nThe accuracy of the statistics eventually helped us to generate better\nperforming perceived quality estimation models.\n  In this paper, we present the implementation of these VLC and GStreamer-based\nmultimedia communication quality assessment testbeds with the references to\ntheir publicly available code bases.", 
    "link": "http://arxiv.org/pdf/1609.06612v1", 
    "arxiv-id": "1609.06612v1"
},{
    "category": "cs.MM", 
    "author": "Jared J. Stein", 
    "title": "Location-Based and Audience-Aware Storytelling", 
    "publish": "2016-09-26T05:09:32Z", 
    "summary": "While the daily user of digital, Internet-enabled devices has some explicit\ncontrol over what they read and see, the providers fulfilling searches,\noffering options, and presenting material are using increasingly sophisticated\nreal-time algorithms that tune and target content for the particular user. They\nredefine the historical relationships between tellers and users, providing a\nresponsiveness paralleled only by forms of live performance incorporating\nelements of improvisation and audience interaction. The general accessibility\nof algorithmically driven content delivery techniques suggests significant\nuntapped potential for new approaches to narrative beyond advertising and\ncommercially orientated customization.", 
    "link": "http://arxiv.org/pdf/1609.07848v1", 
    "arxiv-id": "1609.07848v1"
},{
    "category": "cs.MM", 
    "author": "Jacob Chakareski", 
    "title": "Viewport-Adaptive Navigable 360-Degree Video Delivery", 
    "publish": "2016-09-26T16:10:48Z", 
    "summary": "The delivery and display of ultra high resolution 360-degree videos on\nHead-Mounted Displays (HMDs) presents a number of technical challenges.\n360-degree videos are high resolution spherical videos that contain an\nomnidirectional view of the scene, however only a portion of this scene is\ndisplayed at any time on the user's HMD. The delivery of such videos wastes\nnetwork resources since most of the pixel data are never used. With high\nrefresh rates, HMDs need to respond in less than 10 ms to head movements. The\nrequired ultra low-latency response prevents dynamic adjustments of video\nquality at the server based on client feedback. Instead, an entire 360-degree\nvideo scene needs to be delivered to the client to extract the appropriate\nfraction of this scene on the client's end. To reduce the required immense\nvideo bit-rate, while still providing an immersive experience to the user, a\nviewpoint-adaptive 360-degree video streaming system is proposed. In this\nsystem, the server prepares multiple video representations that differ not only\nby their bit-rate, but also by the qualities of different scene regions they\nsupport. The client chooses a representation for the next segment such that its\nbit-rate fits the available throughput and a full quality region matches its\nviewing direction. We investigate the impact of various spherical-to-plane\nprojections and quality arrangements on the video quality displayed to the\nuser, showing that the cube map layout offers the best quality for the given\nbit-rate budget. An evaluation with a dataset of users navigating 360-degree\nvideos demonstrates that segments need to be short enough to enable frequent\nview switches.", 
    "link": "http://arxiv.org/pdf/1609.08042v1", 
    "arxiv-id": "1609.08042v1"
},{
    "category": "cs.MM", 
    "author": "Francesco De Pellegrini", 
    "title": "Backward-Shifted Coding (BSC) based on Scalable Video Coding for HAS", 
    "publish": "2016-10-07T13:01:30Z", 
    "summary": "The main task of HTTP Adaptive Streaming is to adapt video quality\ndynamically under variable network conditions. This is a key feature for\nmultimedia delivery especially when quality of service cannot be granted\nnetwork-wide and, e.g., throughput may suffer short term fluctuations.\n  Hence, robust bitrate adaptation schemes become crucial in order to improve\nvideo quality. The objective, in this context, is to control the filling level\nof the playback buffer and maximize the quality of the video, while avoiding\nunnecessary video quality variations.\n  In this paper we study bitrate adaptation algorithms based on\nBackward-Shifted Coding (BSC), a scalable video coding scheme able to greatly\nimprove video quality. We design bitrate adaptation algorithms that balance\nvideo rate smoothness and high network capacity utilization, leveraging both on\nthroughput-based and buffer-based adaptation mechanisms.\n  Extensive simulations using synthetic and real-world video traffic traces\nshow that the proposed scheme performs remarkably well even under challenging\nnetwork conditions.", 
    "link": "http://arxiv.org/pdf/1610.02263v1", 
    "arxiv-id": "1610.02263v1"
},{
    "category": "cs.MM", 
    "author": "Derrick Newton", 
    "title": "mPDF: Framework for Watermarking PDF Files using Image Watermarking   Algorithms", 
    "publish": "2016-10-07T23:06:09Z", 
    "summary": "The advancement in digital technologies have made it possible to produce\nperfect copies of digital content. In this environment, malicious users\nreproduce the digital content and share it without compensation to the content\nowner. Content owners are concerned about the potential loss of revenue and\nreputation from piracy, especially when the content is available over the\nInternet. Digital watermarking has emerged as a deterrent measure towards such\nmalicious activities. Several methods have been proposed for copyright\nprotection and fingerprinting of digital images. However, these methods are not\napplicable to text documents as these documents lack rich texture information\nwhich is abundantly available in digital images. In this paper, a framework\n(mPDF) is proposed which facilitates the usage of digital image watermarking\nalgorithms on text documents. The proposed method divides a text document into\ntexture and non-texture blocks using an energy-based approach. After\nclassification, a watermark is embedded inside the texture blocks in a content\nadaptive manner. The proposed method is integrated with five known image\nwatermarking methods and its performance is studied in terms of quality and\nrobustness. Experiments are conducted on documents in 11 different languages.\nExperimental results clearly show that the proposed method facilitates the\nusage of image watermarking algorithms on text documents and is robust against\nattacks such as print & scan, print screen, and skew. Also, the proposed method\novercomes the drawbacks of existing text watermarking methods such as manual\ninspection and language dependency.", 
    "link": "http://arxiv.org/pdf/1610.02443v1", 
    "arxiv-id": "1610.02443v1"
},{
    "category": "cs.MM", 
    "author": "Jean-Marc Valin", 
    "title": "Perceptually-Driven Video Coding with the Daala Video Codec", 
    "publish": "2016-10-08T05:34:56Z", 
    "summary": "The Daala project is a royalty-free video codec that attempts to compete with\nthe best patent-encumbered codecs. Part of our strategy is to replace core\ntools of traditional video codecs with alternative approaches, many of them\ndesigned to take perceptual aspects into account, rather than optimizing for\nsimple metrics like PSNR. This paper documents some of our experiences with\nthese tools, which ones worked and which did not. We evaluate which tools are\neasy to integrate into a more traditional codec design, and show results in the\ncontext of the codec being developed by the Alliance for Open Media.", 
    "link": "http://arxiv.org/pdf/1610.02488v1", 
    "arxiv-id": "1610.02488v1"
},{
    "category": "cs.MM", 
    "author": "Xiaoming Tao", 
    "title": "Saliency-Guided Complexity Control for HEVC Decoding", 
    "publish": "2016-10-08T12:09:38Z", 
    "summary": "The latest High Efficiency Video Coding (HEVC) standard significantly\nimproves coding efficiency over its previous video coding standards. The\nexpense of such improvement is enormous computational complexity, from both\nencoding and decoding perspectives. Since the capability and capacity of power\nare diverse across portable devices, it is necessary to reduce decoding\ncomplexity to a target with tolerable quality loss, so called complexity\ncontrol. This paper proposes a Saliency-Guided Complexity Control (SGCC)\napproach for HEVC decoding, which reduces the decoding complexity to the target\nwith minimal perceptual quality loss. First, an HEVC domain method is developed\nto detect video saliency from HEVC bitstreams, as the preliminary for assessing\nperceptual quality. Based on detected saliency, we establish the SGCC\nformulation to minimize perceptual quality loss at the constraint on reduced\ndecoding complexity, which is achieved via disabling Deblocking Filter (DF) and\nsimplifying Motion Compensation (MC) of some non-salient Largest Coding Units\n(LCUs). One important component in this formulation is the modelled\nrelationship between decoding complexity reduction and DF disabling/MC\nsimplification, which determines the control accuracy of our approach. Another\ncomponent is the modelled relationship between quality loss and DF disabling/MC\nsimplification, responsible for optimizing perceptual quality. By solving the\nSGCC formulation, we can obtain the DF and MC states of each LCU given a target\ncomplexity, and then the decoding complexity can be reduced to the target.\nFinally, the experimental results show the effectiveness of our SGCC approach,\nfrom the aspects of control performance, complexity-distortion performance and\nsubjective quality.", 
    "link": "http://arxiv.org/pdf/1610.02516v2", 
    "arxiv-id": "1610.02516v2"
},{
    "category": "cs.MM", 
    "author": "Sebastiano Battiato", 
    "title": "A Classification Engine for Image Ballistics of Social Data", 
    "publish": "2016-10-20T10:27:10Z", 
    "summary": "Image Forensics has already achieved great results for the source camera\nidentification task on images. Standard approaches for data coming from Social\nNetwork Platforms cannot be applied due to different processes involved (e.g.,\nscaling, compression, etc.). Over 1 billion images are shared each day on the\nInternet and obtaining information about their history from the moment they\nwere acquired could be exploited for investigation purposes. In this paper, a\nclassification engine for the reconstruction of the history of an image, is\npresented. Specifically, exploiting K-NN and decision trees classifiers and\na-priori knowledge acquired through image analysis, we propose an automatic\napproach that can understand which Social Network Platform has processed an\nimage and the software application used to perform the image upload. The engine\nmakes use of proper alterations introduced by each platform as features.\nResults, in terms of global accuracy on a dataset of 2720 images, confirm the\neffectiveness of the proposed strategy.", 
    "link": "http://arxiv.org/pdf/1610.06347v1", 
    "arxiv-id": "1610.06347v1"
},{
    "category": "cs.MM", 
    "author": "Shohreh Kasaei", 
    "title": "An Efficient Adaptive Boundary Matching Algorithm for Video Error   Concealment", 
    "publish": "2016-10-24T12:30:52Z", 
    "summary": "Sending compressed video data in error-prone environments (like the Internet\nand wireless networks) might cause data degradation. Error concealment\ntechniques try to conceal the received data in the decoder side. In this paper,\nan adaptive boundary matching algorithm is presented for recovering the damaged\nmotion vectors (MVs). This algorithm uses an outer boundary matching or\ndirectional temporal boundary matching method to compare every boundary of\ncandidate macroblocks (MBs), adaptively. It gives a specific weight according\nto the accuracy of each boundary of the damaged MB. Moreover, if each of the\nadjacent MBs is already concealed, different weights are given to the\nboundaries. Finally, the MV with minimum adaptive boundary distortion is\nselected as the MV of the damaged MB. Experimental results show that the\nproposed algorithm can improve both objective and subjective quality of\nreconstructed frames without any considerable computational complexity. The\naverage PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88,\n4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching,\ndirectional boundary matching, directional temporal boundary matching, outer\nboundary matching, and dynamical temporal error concealment algorithm,\nrespectively.", 
    "link": "http://arxiv.org/pdf/1610.07386v1", 
    "arxiv-id": "1610.07386v1"
},{
    "category": "cs.MM", 
    "author": "Christos G. Bampis", 
    "title": "Recover Subjective Quality Scores from Noisy Measurements", 
    "publish": "2016-11-06T01:34:47Z", 
    "summary": "Simple quality metrics such as PSNR are known to not correlate well with\nsubjective quality when tested across a wide spectrum of video content or\nquality regime. Recently, efforts have been made in designing objective quality\nmetrics trained on subjective data, demonstrating better correlation with video\nquality perceived by human. Clearly, the accuracy of such a metric heavily\ndepends on the quality of the subjective data that it is trained on. In this\npaper, we propose a new approach to recover subjective quality scores from\nnoisy raw measurements, by jointly estimating the subjective quality of\nimpaired videos, the bias and consistency of test subjects, and the ambiguity\nof video contents all together. Compared to previous methods which partially\nexploit the subjective information, our approach is able to exploit the\ninformation in full, yielding better handling of outliers without the need for\nz-scoring or subject rejection. It also handles missing data more gracefully.\nLastly, as side information, it provides interesting insights on the test\nsubjects and video contents.", 
    "link": "http://arxiv.org/pdf/1611.01715v2", 
    "arxiv-id": "1611.01715v2"
},{
    "category": "cs.MM", 
    "author": "Jiwu Huang", 
    "title": "Large-scale JPEG steganalysis using hybrid deep-learning framework", 
    "publish": "2016-11-10T09:33:12Z", 
    "summary": "Deep learning frameworks have recently achieved superior performance in many\npattern recognition problems. However, adoption of deep learning in image\nsteganalysis is still in its initial stage. In this paper we propose a hybrid\ndeep-learning framework for JPEG steganalysis incorporating the domain\nknowledge behind rich steganalytic models. We prove that the convolution phase\nand the quantization & truncation phase of the rich models are not learnable in\ndeep convolutional neural networks. Based on theoretical analysis, our proposed\nframework involves two main stages. The first stage is hand-crafted,\ncorresponding to the convolution phase and the quantization & truncation phase\nof the rich models. The second stage is a compound deep neural network\ncontaining three deep subnets in which the model parameters are learned in the\ntraining procedure. By doing so, we ably combine some merits of rich models\ninto our proposed deep-learning framework. We have conducted extensive\nexperiments on a large-scale dataset extracted from ImageNet. The primary\ndataset used in our experiments contains 500,000 cover images, while our\nlargest dataset contains five million cover images. Our experiments show that\nthe proposed framework outperforms all other state-of-the-art steganalytic\nmodels either hand-crafted or learned using deep networks in the literature.\nFurthermore, we demonstrate that our framework is insensitive to JPEG blocking\nartifact alterations and the learned model can be easily transferred to a\ndifferent attacking target. Both of these properties are of critical importance\nin practical applications. According to our best knowledge, This is the first\nreport of deep learning in image steganalysis validated with large-scale test\ndata.", 
    "link": "http://arxiv.org/pdf/1611.03233v2", 
    "arxiv-id": "1611.03233v2"
},{
    "category": "cs.MM", 
    "author": "Shih-Fu Chang", 
    "title": "Columbia MVSO Image Sentiment Dataset", 
    "publish": "2016-11-14T16:48:12Z", 
    "summary": "The Multilingual Visual Sentiment Ontology (MVSO) consists of 15,600 concepts\nin 12 different languages that are strongly related to emotions and sentiments\nexpressed in images. These concepts are defined in the form of Adjective-Noun\nPair (ANP), which are crawled and discovered from online image forum Flickr. In\nthis work, we used Amazon Mechanical Turk as a crowd-sourcing platform to\ncollect human judgments on sentiments expressed in images that are uniformly\nsampled over 3,911 English ANPs extracted from a tag-restricted subset of MVSO.\nOur goal is to use the dataset as a benchmark for the evaluation of systems\nthat automatically predict sentiments in images or ANPs.", 
    "link": "http://arxiv.org/pdf/1611.04455v1", 
    "arxiv-id": "1611.04455v1"
},{
    "category": "cs.MM", 
    "author": "Christophe Guyeux", 
    "title": "A Second Order Derivatives based Approach for Steganography", 
    "publish": "2016-11-25T09:38:10Z", 
    "summary": "Steganography schemes are designed with the objective of minimizing a defined\ndistortion function. In most existing state of the art approaches, this\ndistortion function is based on image feature preservation. Since smooth\nregions or clean edges define image core, even a small modification in these\nareas largely modifies image features and is thus easily detectable. On the\ncontrary, textures, noisy or chaotic regions are so difficult to model that the\nfeatures having been modified inside these areas are similar to the initial\nones. These regions are characterized by disturbed level curves. This work\npresents a new distortion function for steganography that is based on second\norder derivatives, which are mathematical tools that usually evaluate level\ncurves. Two methods are explained to compute these partial derivatives and have\nbeen completely implemented. The first experiments show that these approaches\nare promising.", 
    "link": "http://arxiv.org/pdf/1611.08397v1", 
    "arxiv-id": "1611.08397v1"
},{
    "category": "cs.MM", 
    "author": "Farah Torkamani-Azar", 
    "title": "A novel Adaptive weighted Kronecker Compressive Sensing", 
    "publish": "2016-12-04T13:08:04Z", 
    "summary": "Recently, multidimensional signal reconstruction using a low number of\nmeasurements is of great interest. Therefore, an effective sampling scheme\nwhich should acquire the most information of signal using a low number of\nmeasurements is required. In this paper, we study a novel cube-based method for\nsampling and reconstruction of multidimensional signals. First, inspired by the\nblock-based compressive sensing (BCS), we divide a group of pictures (GoP) in a\nvideo sequence into cubes. By this way, we can easily store the measurement\nmatrix and also easily can generate the sparsifying basis. The reconstruction\nprocess also can be done in parallel. Second, along with the Kronecker\nstructure of the sampling matrix, we design a weight matrix based on the human\nvisuality system, i.e. perceptually. We will also benefit from different\nweighted $\\ell_1$-minimization methods for reconstruction. Furthermore,\nconventional methods for BCS consider an equal number of samples for all\nblocks. However, the sparsity order of blocks in natural images could be\ndifferent and, therefore, a various number of samples could be required for\ntheir reconstruction. Motivated by this point, we will adaptively allocate the\nsamples for each cube in a video sequence. Our aim is to show that our simple\nlinear sampling approach can be competitive with the other state-of-the-art\nmethods.", 
    "link": "http://arxiv.org/pdf/1612.01113v2", 
    "arxiv-id": "1612.01113v2"
},{
    "category": "cs.MM", 
    "author": "Houqiang Li", 
    "title": "Pseudo Sequence based 2-D hierarchical reference structure for   Light-Field Image Compression", 
    "publish": "2016-12-21T20:31:57Z", 
    "summary": "In this paper, we present a novel pseudo sequence based 2-D hierarchical\nreference structure for light-field image compression. In the proposed scheme,\nwe first decompose the light-field image into multiple views and organize them\ninto a 2-D coding structure according to the spatial coordinates of the\ncorresponding microlens. Then we mainly develop three technologies to optimize\nthe 2-D coding structure. First, we divide all the views into four quadrants,\nand all the views are encoded one quadrant after another to reduce the\nreference buffer size as much as possible. Inside each quadrant, all the views\nare encoded hierarchically to fully exploit the correlations between different\nviews. Second, we propose to use the distance between the current view and its\nreference views as the criteria for selecting better reference frames for each\ninter view. Third, we propose to use the spatial relative positions between\ndifferent views to achieve more accurate motion vector scaling. The whole\nscheme is implemented in the reference software of High Efficiency Video\nCoding. The experimental results demonstrate that the proposed novel\npseudo-sequence based 2-D hierarchical structure can achieve maximum 14.2%\nbit-rate savings compared with the state-of-the-art light-field image\ncompression method.", 
    "link": "http://arxiv.org/pdf/1612.07309v1", 
    "arxiv-id": "1612.07309v1"
},{
    "category": "cs.MM", 
    "author": "Hong Vicky Zhao", 
    "title": "Object Shape Approximation & Contour Adaptive Depth Image Coding for   Virtual View Synthesis", 
    "publish": "2016-12-23T04:32:33Z", 
    "summary": "A depth image provides partial geometric information of a 3D scene, namely\nthe shapes of physical objects as observed from a particular viewpoint. This\ninformation is important when synthesizing images of different virtual camera\nviewpoints via depth-image-based rendering (DIBR). It has been shown that depth\nimages can be efficiently coded using contour-adaptive codecs that preserve\nedge sharpness, resulting in visually pleasing DIBR-synthesized images.\nHowever, contours are typically losslessly coded as side information (SI),\nwhich is expensive if the object shapes are complex.\n  In this paper, we pursue a new paradigm in depth image coding for\ncolor-plus-depth representation of a 3D scene: we pro-actively simplify object\nshapes in a depth and color image pair to reduce depth coding cost, at a\npenalty of a slight increase in synthesized view distortion. Specifically, we\nfirst mathematically derive a distortion upper-bound proxy for 3DSwIM---a\nquality metric tailored for DIBR-synthesized images. This proxy reduces\ninterdependency among pixel rows in a block to ease optimization. We then\napproximate object contours via a dynamic programming (DP) algorithm to\noptimally trade off coding cost of contours using arithmetic edge coding (AEC)\nwith our proposed view synthesis distortion proxy. We modify the depth and\ncolor images according to the approximated object contours in an inter-view\nconsistent manner. These are then coded respectively using a contour-adaptive\nimage codec based on graph Fourier transform (GFT) for edge preservation and\nHEVC intra. Experimental results show that by maintaining sharp but simplified\nobject contours during contour-adaptive coding, for the same visual quality of\nDIBR-synthesized virtual views, our proposal can reduce depth image coding rate\nby up to 22% compared to alternative coding strategies such as HEVC intra.", 
    "link": "http://arxiv.org/pdf/1612.07872v1", 
    "arxiv-id": "1612.07872v1"
},{
    "category": "cs.MM", 
    "author": "Victor Sanchez", 
    "title": "Cross-Color Channel Perceptually Adaptive Quantization for HEVC", 
    "publish": "2016-12-23T07:57:34Z", 
    "summary": "HEVC includes a Coding Unit (CU) level luminance-based perceptual\nquantization technique known as AdaptiveQP. AdaptiveQP perceptually adjusts the\nQuantization Parameter (QP) at the CU level based on the spatial activity of\nraw input video data in a luma Coding Block (CB). In this paper, we propose a\nnovel cross-color channel adaptive quantization scheme which perceptually\nadjusts the CU level QP according to the spatial activity of raw input video\ndata in the constituent luma and chroma CBs; i.e., the combined spatial\nactivity across all three color channels (the Y, Cb and Cr channels). Our\ntechnique is evaluated in HM 16 with 4:4:4, 4:2:2 and 4:2:0 YCbCr JCT-VC test\nsequences. Both subjective and objective visual quality evaluations are\nundertaken during which we compare our method with AdaptiveQP. Our technique\nachieves considerable coding efficiency improvements, with maximum BD-Rate\nreductions of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a maximum\ndecoding time reduction of 11.0%.", 
    "link": "http://arxiv.org/pdf/1612.07893v3", 
    "arxiv-id": "1612.07893v3"
},{
    "category": "cs.MM", 
    "author": "Mohamed Hefeeda", 
    "title": "Streaming Virtual Reality Content", 
    "publish": "2016-12-26T09:40:45Z", 
    "summary": "The recent rise of interest in Virtual Reality (VR) came with the\navailability of commodity commercial VR prod- ucts, such as the Head Mounted\nDisplays (HMD) created by Oculus and other vendors. To accelerate the user\nadoption of VR headsets, content providers should focus on producing high\nquality immersive content for these devices. Similarly, multimedia streaming\nservice providers should enable the means to stream 360 VR content on their\nplatforms. In this study, we try to cover different aspects related to VR\ncontent representation, streaming, and quality assessment that will help\nestablishing the basic knowledge of how to build a VR streaming system.", 
    "link": "http://arxiv.org/pdf/1612.08350v1", 
    "arxiv-id": "1612.08350v1"
},{
    "category": "cs.MM", 
    "author": "Michel Salomon", 
    "title": "Improving Blind Steganalysis in Spatial Domain using a Criterion to   Choose the Appropriate Steganalyzer between CNN and SRM+EC", 
    "publish": "2016-12-28T13:44:19Z", 
    "summary": "Conventional state-of-the-art image steganalysis approaches usually consist\nof a classifier trained with features provided by rich image models. As both\nfeatures extraction and classification steps are perfectly embodied in the deep\nlearning architecture called Convolutional Neural Network (CNN), different\nstudies have tried to design a CNN-based steganalyzer. The network designed by\nXu et al. is the first competitive CNN with the combination Spatial Rich Models\n(SRM) and Ensemble Classifier (EC) providing detection performances of the same\norder. In this work we propose a criterion to choose either the CNN or the\nSRM+EC method for a given input image. Our approach is studied with three\ndifferent steganographic spatial domain algorithms: S-UNIWARD, MiPOD, and HILL,\nusing the Tensorflow computing platform, and exhibits detection capabilities\nbetter than each method alone. Furthermore, as SRM+EC and the CNN are both only\ntrained with a single embedding algorithm, namely MiPOD, the proposed method\ncan be seen as an approach for blind steganalysis. In blind detection, error\nrates are respectively of 16% for S-UNIWARD, 16% for MiPOD, and 17% for HILL on\nthe BOSSBase with a payload of 0.4 bpp. For 0.1 bpp, the respective\ncorresponding error rates are of 39%, 38%, and 41%, and are always better than\nthe ones provided by SRM+EC.", 
    "link": "http://arxiv.org/pdf/1612.08882v2", 
    "arxiv-id": "1612.08882v2"
},{
    "category": "cs.MM", 
    "author": "Meysam Ghaffari", 
    "title": "Duplicate matching and estimating features for detection of copy-move   images forgery", 
    "publish": "2017-01-02T18:25:45Z", 
    "summary": "Copy-move forgery is the most popular and simplest image manipulation method.\nIn this type of forgery, an area from the image copied, then after post\nprocessing such as rotation and scaling, placed on the destination. The goal of\nCopy-move forgery is to hide or duplicate one or more objects in the image.\nKey-point based Copy-move forgery detection methods have five main steps:\npreprocessing, feature extraction, matching, transform estimation and post\nprocessing that matching and transform estimation have important effect on the\ndetection. More over the error could happens in some steps due to the noise.\nThe existing methods process these steps separately and in case of having an\nerror in a step, this error could be propagated to the following steps and\naffects the detection. To solve the above mentioned problem, in this paper the\nsteps of the detection system interact with each other and if an error happens\nin a step, following steps are trying to detect and solve it. We formulate this\ninteraction by defining and optimizing a cost function. This function includes\nmatching and transform estimation steps. Then in an iterative procedure the\nsteps are executed and in case of detecting error, the error will be corrected.\nThe efficiency of the proposed method analyzed in diverse cases such as pixel\nimage precision level on the simple forgery images, robustness to the rotation\nand scaling, detecting professional forgery images and the precision of the\ntransformation matrix. The results indicate the better efficiency of the\nproposed method.", 
    "link": "http://arxiv.org/pdf/1701.00474v1", 
    "arxiv-id": "1701.00474v1"
},{
    "category": "cs.MM", 
    "author": "C. -C. Jay Kuo", 
    "title": "VideoSet: A Large-Scale Compressed Video Quality Dataset Based on JND   Measurement", 
    "publish": "2017-01-05T23:14:01Z", 
    "summary": "A new methodology to measure coded image/video quality using the\njust-noticeable-difference (JND) idea was proposed. Several small JND-based\nimage/video quality datasets were released by the Media Communications Lab at\nthe University of Southern California. In this work, we present an effort to\nbuild a large-scale JND-based coded video quality dataset. The dataset consists\nof 220 5-second sequences in four resolutions (i.e., $1920 \\times 1080$, $1280\n\\times 720$, $960 \\times 540$ and $640 \\times 360$). For each of the 880 video\nclips, we encode it using the H.264 codec with $QP=1, \\cdots, 51$ and measure\nthe first three JND points with 30+ subjects. The dataset is called the\n\"VideoSet\", which is an acronym for \"Video Subject Evaluation Test (SET)\". This\nwork describes the subjective test procedure, detection and removal of outlying\nmeasured data, and the properties of collected JND data. Finally, the\nsignificance and implications of the VideoSet to future video coding research\nand standardization efforts are pointed out. All source/coded video clips as\nwell as measured JND data included in the VideoSet are available to the public\nin the IEEE DataPort.", 
    "link": "http://arxiv.org/pdf/1701.01500v2", 
    "arxiv-id": "1701.01500v2"
},{
    "category": "cs.MM", 
    "author": "Mohammad H. Kayvanrad", 
    "title": "A Comprehensive Review of Audio Steganalysis Methods", 
    "publish": "2017-01-19T21:43:07Z", 
    "summary": "Recently merging signal processing techniques with information security\nservices has found lots of attentions. Steganography and steganalysis are among\nthese emerging trends. Like their counterparts in cryptology, steganography and\nsteganalysis are in a constant battle- steganography methods try to hide the\npresence of covert messages in innocuous-looking data, whereas steganalysis\nmethods try to reveal existence of such messages and to break steganography\nmethods. The stream nature of audio signals, their popularity, and their wide\nspread usage makes them very suitable media for steganography. This has led to\na very rich literature on both steganography and steganalysis of audio signals.\nThis review intends to conduct a comprehensive survey of audio steganalysis\nmethods aggregated over near fifteen years. Furthermore, we implement some of\nthe most recent audio steganalysis methods and conduct a comparative analysis\non their performances. Finally, the paper provides some possible directions for\nfuture researches on audio steganalysis.", 
    "link": "http://arxiv.org/pdf/1701.05611v1", 
    "arxiv-id": "1701.05611v1"
},{
    "category": "cs.MM", 
    "author": "Meisam Khalil Arjmandi", 
    "title": "Universal Audio Steganalysis Based on Calibration and Reversed Frequency   Resolution of Human Auditory System", 
    "publish": "2017-01-19T21:49:06Z", 
    "summary": "Calibration and higher order statistics (HOS) are standard components of many\nimage steganalysis systems. These techniques have not yet found adequate\nattention in audio steganalysis context. Specifically, most of current works\nare either non-calibrated or only based on noise removal approach. This paper\naims to fill these gaps by proposing a new set of calibrated features based on\nre-embedding technique. Additionally, we show that least significant bit (LSB)\nis the most sensitive bit-plane to data hiding algorithms and therefore it can\nbe employed as a universal embedding method. Furthermore, the proposed features\nare based on a model that has the maximum deviation from human auditory system\n(HAS), and therefore are more suitable for the purpose of steganalysis.\nPerformance of the proposed method is evaluated on a wide range of data hiding\nalgorithms in both targeted and universal paradigms. Simulation results show\nthat the proposed method can detect the finest traces of data hiding algorithms\nand in very low embedding rates. The system detects steghide at capacity of\n0.06 bit per symbol (BPS) with sensitivity of 98.6% (music) and 78.5% (speech).\nThese figures are respectively 7.1% and 27.5% higher than state-of-the-art\nresults based on RMFCC.", 
    "link": "http://arxiv.org/pdf/1701.05614v1", 
    "arxiv-id": "1701.05614v1"
},{
    "category": "cs.MM", 
    "author": "Viswanathan Swaminathan", 
    "title": "Adaptive 360 VR Video Streaming based on MPEG-DASH SRD", 
    "publish": "2017-01-23T17:11:32Z", 
    "summary": "We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system\nbased on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR\nvideos, and showcase a dynamic view-aware adaptation technique to tackle the\nhigh bandwidth demands of streaming 360 VR videos to wireless VR headsets. We\nspatially partition the underlying 3D mesh into multiple 3D sub-meshes, and\nconstruct an efficient 3D geometry mesh called hexaface sphere to optimally\nrepresent tiled 360 VR videos in the 3D space. We then spatially divide the 360\nvideos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to\ndescribe the spatial relationship of tiles in the 3D space, and prioritize the\ntiles in the Field of View (FoV) for view-aware adaptation. Our initial\nevaluation results show that we can save up to 72% of the required bandwidth on\n360 VR video streaming with minor negative quality impacts compared to the\nbaseline scenario when no adaptations is applied.", 
    "link": "http://arxiv.org/pdf/1701.06509v1", 
    "arxiv-id": "1701.06509v1"
},{
    "category": "cs.MM", 
    "author": "Arslan Ahmad", 
    "title": "Analysis of challenges faced by WebRTC videoconferencing and a remedial   architecture", 
    "publish": "2017-01-01T18:09:08Z", 
    "summary": "Lately, World Wide Web came up with an evolution in the niche of\nvideoconference applications. Latest technologies give browsers a capacity to\ninitiate real-time communications. WebRTC is one of the free and open source\nprojects that aim at providing the users freedom to enjoy real-time\ncommunications, and it does so by following and redefining the standards.\nHowever, WebRTC is still a new project and it lacks some high-end\nvideoconferencing features such as media mixing, recording of a session and\ndifferent network conditions adaptation. This paper is an attempt at analyzing\nthe shortcomings and challenges faced by WebRTC and proposing a Multipoint\nControl Unit or traditional communications entity based architecture as a\nsolution.", 
    "link": "http://arxiv.org/pdf/1701.09182v2", 
    "arxiv-id": "1701.09182v2"
},{
    "category": "cs.MM", 
    "author": "Mohsen Moradi", 
    "title": "Combining and Steganography of 3D Face Textures", 
    "publish": "2017-02-04T18:55:16Z", 
    "summary": "One of the serious issues in communication between people is hiding\ninformation from others, and the best way for this, is deceiving them. Since\nnowadays face images are mostly used in three dimensional format, in this paper\nwe are going to steganography 3D face images, detecting which by curious people\nwill be impossible. As in detecting face only its texture is important, we\nseparate texture from shape matrices, for eliminating half of the extra\ninformation, steganography is done only for face texture, and for\nreconstructing 3D face, we can use any other shape. Moreover, we will indicate\nthat, by using two textures, how two 3D faces can be combined. For a complete\ndescription of the process, first, 2D faces are used as an input for building\n3D faces, and then 3D textures are hidden within other images.", 
    "link": "http://arxiv.org/pdf/1702.01325v1", 
    "arxiv-id": "1702.01325v1"
},{
    "category": "cs.MM", 
    "author": "Farah Torkamani-Azar", 
    "title": "Perceptual Compressive Sensing based on Contrast Sensitivity Function:   Can we avoid non-visible redundancies acquisition?", 
    "publish": "2017-02-19T08:21:20Z", 
    "summary": "Conventional compressive sensing (CS) attempts to acquire the most important\npart of a signal directly. In fact, CS avoids acquisition of existed\n\\textit{statistical redundancies} of a signal. Since the sensitivity of the\nhuman eye is different for each frequency, in addition to statistical\nredundancies, there exist \\textit{perceptual redundancies} in an image which\nhuman eye could not detect them. In this paper, we propose a novel CS approach\nin which the acquisition of non-visible information is also avoided. Hence, we\ncould expect a better compression performance. We deploy the weighted CS idea\nto consider these perceptual redundancies in our model. Moreover, the\nblock-based compressed sensing is favorable since it has some advantages: (a)\nIt needs low memory to store the sensing matrix and sparsifying basis. (b) All\nblocks can be reconstructed in parallel. Therefore, we apply our proposed\nscheme in the block-based framework to make it practical to use. Simulation\nresults verify the superiority of our proposed method compared to the other\nstate-of-the-art methods.", 
    "link": "http://arxiv.org/pdf/1702.05718v1", 
    "arxiv-id": "1702.05718v1"
},{
    "category": "cs.MM", 
    "author": "Feng Wu", 
    "title": "An Efficient Four-Parameter Affine Motion Model for Video Coding", 
    "publish": "2017-02-21T09:04:15Z", 
    "summary": "In this paper, we study a simplified affine motion model based coding\nframework to overcome the limitation of translational motion model and maintain\nlow computational complexity. The proposed framework mainly has three key\ncontributions. First, we propose to reduce the number of affine motion\nparameters from 6 to 4. The proposed four-parameter affine motion model can not\nonly handle most of the complex motions in natural videos but also save the\nbits for two parameters. Second, to efficiently encode the affine motion\nparameters, we propose two motion prediction modes, i.e., advanced affine\nmotion vector prediction combined with a gradient-based fast affine motion\nestimation algorithm and affine model merge, where the latter attempts to reuse\nthe affine motion parameters (instead of the motion vectors) of neighboring\nblocks. Third, we propose two fast affine motion compensation algorithms. One\nis the one-step sub-pixel interpolation, which reduces the computations of each\ninterpolation. The other is the interpolation-precision-based adaptive block\nsize motion compensation, which performs motion compensation at the block level\nrather than the pixel level to reduce the interpolation times. Our proposed\ntechniques have been implemented based on the state-of-the-art high efficiency\nvideo coding standard, and the experimental results show that the proposed\ntechniques altogether achieve on average 11.1% and 19.3% bits saving for random\naccess and low delay configurations, respectively, on typical video sequences\nthat have rich rotation or zooming motions. Meanwhile, the computational\ncomplexity increases of both encoder and decoder are within an acceptable\nrange.", 
    "link": "http://arxiv.org/pdf/1702.06297v1", 
    "arxiv-id": "1702.06297v1"
},{
    "category": "cs.MM", 
    "author": "Feng Wu", 
    "title": "Convolutional Neural Network-Based Block Up-sampling for Intra Frame   Coding", 
    "publish": "2017-02-22T09:51:49Z", 
    "summary": "Inspired by the recent advances of image super-resolution using convolutional\nneural network (CNN), we propose a CNN-based block up-sampling scheme for intra\nframe coding. A block can be down-sampled before being compressed by normal\nintra coding, and then up-sampled to its original resolution. Different from\nprevious studies on down/up-sampling based coding, the up-sampling\ninterpolation filters in our scheme have been designed by training CNN instead\nof hand-crafted. We explore a new CNN structure for up-sampling, which features\ndeconvolution of feature maps, multi-scale fusion, and residue learning, making\nthe network both compact and efficient. We also design different networks for\nthe up-sampling of luma and chroma components, respectively, where the chroma\nup-sampling CNN utilizes the luma information to boost its performance. In\naddition, we design a two-stage up-sampling process, the first stage being\nwithin the block-by-block coding loop, and the second stage being performed on\nthe entire frame, so as to refine block boundaries. We also empirically study\nhow to set the coding parameters of down-sampled blocks for pursuing the\nframe-level rate-distortion optimization. Our proposed scheme is implemented\ninto the High Efficiency Video Coding (HEVC) reference software, and a\ncomprehensive set of experiments have been performed to evaluate our methods.\nExperimental results show that our scheme achieves significant bits saving\ncompared with HEVC anchor especially at low bit rates, leading to on average\n5.5% BD-rate on common test sequences and on average 9.0% BD-rate on ultra high\ndefinition (UHD) test sequences.", 
    "link": "http://arxiv.org/pdf/1702.06728v1", 
    "arxiv-id": "1702.06728v1"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Wegner", 
    "title": "Analysis of video quality losses in the homogenous HEVC video   transcoding", 
    "publish": "2017-02-24T11:53:22Z", 
    "summary": "The paper presents quantitative analysis of the video quality losses in the\nhomogenous HEVC video transcoder. With the use of HM15.0 reference software and\na set of test video sequences, cascaded pixel domain video transcoder (CPDT)\nconcept has been used to gather all the necessary data needed for the analysis.\nThis experiment was done for wide range of source and target bitrates. The\nessential result of the work is extensive evaluation of CPDT, commonly used as\na reference in works on effective video transcoding. Until now no such\nextensively performed study have been made available in the literature. Quality\ndegradation between transcoded video and the video that would be result of\ndirect compression of the original video at the same bitrate as the transcoded\none have been reported. The dependency between quality degradation caused by\ntranscoding and the bitrate changes of the transcoded data stream are clearly\npresented on graphs.", 
    "link": "http://arxiv.org/pdf/1702.07548v1", 
    "arxiv-id": "1702.07548v1"
},{
    "category": "cs.MM", 
    "author": "Wenwu Zhu", 
    "title": "Understanding Performance of Edge Content Caching for Mobile Video   Streaming", 
    "publish": "2017-02-24T15:28:20Z", 
    "summary": "Today's Internet has witnessed an increase in the popularity of mobile video\nstreaming, which is expected to exceed 3/4 of the global mobile data traffic by\n2019. To satisfy the considerable amount of mobile video requests, video\nservice providers have been pushing their content delivery infrastructure to\nedge networks--from regional CDN servers to peer CDN servers (e.g.,\nsmartrouters in users' homes)--to cache content and serve users with storage\nand network resources nearby. Among the edge network content caching paradigms,\nWi-Fi access point caching and cellular base station caching have become two\nmainstream solutions. Thus, understanding the effectiveness and performance of\nthese solutions for large-scale mobile video delivery is important. However,\nthe characteristics and request patterns of mobile video streaming are unclear\nin practical wireless network. In this paper, we use real-world datasets\ncontaining 50 million trace items of nearly 2 million users viewing more than\n0.3 million unique videos using mobile devices in a metropolis in China over 2\nweeks, not only to understand the request patterns and user behaviors in mobile\nvideo streaming, but also to evaluate the effectiveness of Wi-Fi and\ncellular-based edge content caching solutions. To understand performance of\nedge content caching for mobile video streaming, we first present temporal and\nspatial video request patterns, and we analyze their impacts on caching\nperformance using frequency-domain and entropy analysis approaches. We then\nstudy the behaviors of mobile video users, including their mobility and\ngeographical migration behaviors. Using trace-driven experiments, we compare\nstrategies for edge content caching including LRU and LFU, in terms of\nsupporting mobile video requests. Moreover, we design an efficient caching\nstrategy based on the measurement insights and experimentally evaluate its\nperformance.", 
    "link": "http://arxiv.org/pdf/1702.07627v1", 
    "arxiv-id": "1702.07627v1"
},{
    "category": "cs.MA", 
    "author": "A. Zaslavsky", 
    "title": "Document Archiving, Replication and Migration Container for Mobile Web   Users", 
    "publish": "1998-09-20T12:48:43Z", 
    "summary": "With the increasing use of mobile workstations for a wide variety of tasks\nand associated information needs, and with many variations of available\nnetworks, access to data becomes a prime consideration. This paper discusses\nissues of workstation mobility and proposes a solution wherein the data\nstructures are accessed in an encapsulated form - through the Portable File\nSystem (PFS) wrapper. The paper discusses an implementation of the Portable\nFile System, highlighting the architecture and commenting upon performance of\nan experimental system. Although investigations have been focused upon mobile\naccess of WWW documents, this technique could be applied to any mobile data\naccess situation.", 
    "link": "http://arxiv.org/pdf/cs/9809036v1", 
    "arxiv-id": "cs/9809036v1"
},{
    "category": "cs.NI", 
    "author": "Tatsuya Suda", 
    "title": "Adaptive Multicast of Multi-Layered Video: Rate-Based and Credit-Based   Approaches", 
    "publish": "1998-09-24T18:12:21Z", 
    "summary": "Network architectures that can efficiently transport high quality, multicast\nvideo are rapidly becoming a basic requirement of emerging multimedia\napplications. The main problem complicating multicast video transport is\nvariation in network bandwidth constraints. An attractive solution to this\nproblem is to use an adaptive, multi-layered video encoding mechanism. In this\npaper, we consider two such mechanisms for the support of video multicast; one\nis a rate-based mechanism that relies on explicit rate congestion feedback from\nthe network, and the other is a credit-based mechanism that relies on\nhop-by-hop congestion feedback. The responsiveness, bandwidth utilization,\nscalability and fairness of the two mechanisms are evaluated through\nsimulations. Results suggest that while the two mechanisms exhibit performance\ntrade-offs, both are capable of providing a high quality video service in the\npresence of varying bandwidth constraints.", 
    "link": "http://arxiv.org/pdf/cs/9809104v1", 
    "arxiv-id": "cs/9809104v1"
},{
    "category": "cs.MM", 
    "author": "John Derrick", 
    "title": "Stochastic Model Checking for Multimedia", 
    "publish": "2000-02-04T18:42:13Z", 
    "summary": "Modern distributed systems include a class of applications in which\nnon-functional requirements are important. In particular, these applications\ninclude multimedia facilities where real time constraints are crucial to their\ncorrect functioning. In order to specify such systems it is necessary to\ndescribe that events occur at times given by probability distributions and\nstochastic automata have emerged as a useful technique by which such systems\ncan be specified and verified.\n  However, stochastic descriptions are very general, in particular they allow\nthe use of general probability distribution functions, and therefore their\nverification can be complex. In the last few years, model checking has emerged\nas a useful verification tool for large systems.\n  In this paper we describe two model checking algorithms for stochastic\nautomata. These algorithms consider how properties written in a simple\nprobabilistic real-time logic can be checked against a given stochastic\nautomaton.", 
    "link": "http://arxiv.org/pdf/cs/0002004v1", 
    "arxiv-id": "cs/0002004v1"
},{
    "category": "cs.IR", 
    "author": "Giordano B. Beretta", 
    "title": "A Benchmark for Image Retrieval using Distributed Systems over the   Internet: BIRDS-I", 
    "publish": "2000-12-22T23:38:37Z", 
    "summary": "The performance of CBIR algorithms is usually measured on an isolated\nworkstation. In a real-world environment the algorithms would only constitute a\nminor component among the many interacting components. The Internet\ndramati-cally changes many of the usual assumptions about measuring CBIR\nperformance. Any CBIR benchmark should be designed from a networked systems\nstandpoint. These benchmarks typically introduce communication overhead because\nthe real systems they model are distributed applications. We present our\nimplementation of a client/server benchmark called BIRDS-I to measure image\nretrieval performance over the Internet. It has been designed with the trend\ntoward the use of small personalized wireless systems in mind. Web-based CBIR\nimplies the use of heteroge-neous image sets, imposing certain constraints on\nhow the images are organized and the type of performance metrics applicable.\nBIRDS-I only requires controlled human intervention for the compilation of the\nimage collection and none for the generation of ground truth in the measurement\nof retrieval accuracy. Benchmark image collections need to be evolved\nincrementally toward the storage of millions of images and that scaleup can\nonly be achieved through the use of computer-aided compilation. Finally, our\nscoring metric introduces a tightly optimized image-ranking window.", 
    "link": "http://arxiv.org/pdf/cs/0012021v1", 
    "arxiv-id": "cs/0012021v1"
},{
    "category": "cs.NI", 
    "author": "T. C. Schmidt", 
    "title": "Media Objects in Time - A Multimedia Streaming System", 
    "publish": "2001-12-28T20:19:09Z", 
    "summary": "The widespread availability of networked multimedia potentials embedded in an\ninfrastructure of qualitative superior kind gives rise to new approaches in the\nareas of teleteaching and internet presentation: The distribution of\nprofessionally styled multimedia streams has fallen in the realm of\npossibility. This paper presents a prototype - both model and runtime\nenvironment - of a time directed media system treating any kind of\npresentational contribution as reusable media object components. The plug-in\nfree runtime system is based on a database and allows for a flexible support of\nstatic media types as well as for easy extensions by streaming media servers.\nThe prototypic implementation includes a preliminary Web Authoring platform.", 
    "link": "http://arxiv.org/pdf/cs/0112024v1", 
    "arxiv-id": "cs/0112024v1"
},{
    "category": "cs.MM", 
    "author": "Jane Hunter", 
    "title": "Reconciling MPEG-7 and MPEG-21 Semantics through a Common Event-Aware   Metadata Model", 
    "publish": "2002-10-22T02:16:57Z", 
    "summary": "The \"event\" concept appears repeatedly when developing metadata models for\nthe description and management of multimedia content. During the typical life\ncycle of multimedia content, events occur at many different levels - from the\nevents which happen during content creation (directing, acting, camera panning\nand zooming) to the events which happen to the physical form (acquisition,\nrelocation, damage of film or video) to the digital conversion, reformatting,\nediting and repackaging events, to the events which are depicted in the actual\ncontent (political, news, sporting) to the usage, ownership and copyright\nagreement events and even the metadata attribution events. Support is required\nwithin both MPEG-7 and MPEG-21 for the clear and unambiguous description of all\nof these event types which may occur at widely different levels of nesting and\ngranularity. In this paper we first describe an event-aware model (the ABC\nmodel) which is capable of modeling and yet clearly differentiating between all\nof these, often recursive and overlapping events. We then illustrate how this\nmodel can be used as the foundation to facilitate semantic interoperability\nbetween MPEG-7 and MPEG-21. By expressing the semantics of both MPEG-7 and\nMPEG-21 metadata terms in RDF Schema (and some DAML+OIL extensions) and\nattaching the MPEG-7 and MPEG-21 class and property hierarchies to the\nappropriate top-level classes and properties of the ABC model, we are\nessentially able to define a single distributed machine-understandable\nontology, which will enable interoperability of data and services across the\nentire multimedia content delivery chain.", 
    "link": "http://arxiv.org/pdf/cs/0210021v1", 
    "arxiv-id": "cs/0210021v1"
},{
    "category": "cs.MM", 
    "author": "David Adamczyk", 
    "title": "Global Platform for Rich Media Conferencing and Collaboration", 
    "publish": "2003-06-19T22:57:42Z", 
    "summary": "The Virtual Rooms Videoconferencing Service (VRVS) provides a worldwide\nvideoconferencing service and collaborative environment to the research and\neducation communities. This system provides a low cost, bandwidth-efficient,\nextensible means for videoconferencing and remote collaboration over networks\nwithin the High Energy and Nuclear Physics communities (HENP). VRVS has become\na standard part of the toolset used daily by a large sector of HENP, and it is\nused increasingly for other DoE/NSF-supported programs. The current features\nincluded multi-protocol, multi-OS support for all significant video enabled\nclients including: H.323, Mbone, QuickTime, MPEG2, Java Media Framework, and\nother clients. The current architecture makes VRVS a distributed, highly\nfunctional, and efficient software-only system for multipoint audio, video and\nweb conferencing and collaboration over global IP networks. VRVS has developed\nthe VRVS-AG Reflector and a specialized Web interface that enables end users to\nconnect to any Access Grid (AG) session, in any of the AG \"virtual venues\" from\nanywhere worldwide. The VRVS system has now been running for the last five and\nhalf years, offering to the HENP community a working and reliable tool for\ncollaboration within groups and among physicists dispersed world-wide. The goal\nof this ongoing effort is to develop the next generation collaborative systems\nrunning over next generation networks. The new developments area integrate\nemerging standards, include all security aspects, and will extend the range of\nVRVS video technologies supported to cover the latest high end standards\nquality. We will focus the discussion on the new capability provides by the\nlatest version V3.0 and its future evolution.", 
    "link": "http://arxiv.org/pdf/cs/0306116v2", 
    "arxiv-id": "cs/0306116v2"
},{
    "category": "cs.CL", 
    "author": "Paul Piwek", 
    "title": "A Flexible Pragmatics-driven Language Generator for Animated Agents", 
    "publish": "2003-12-22T16:23:34Z", 
    "summary": "This paper describes the NECA MNLG; a fully implemented Multimodal Natural\nLanguage Generation module. The MNLG is deployed as part of the NECA system\nwhich generates dialogues between animated agents. The generation module\nsupports the seamless integration of full grammar rules, templates and canned\ntext. The generator takes input which allows for the specification of\nsyntactic, semantic and pragmatic constraints on the output.", 
    "link": "http://arxiv.org/pdf/cs/0312050v1", 
    "arxiv-id": "cs/0312050v1"
},{
    "category": "cs.IR", 
    "author": "John R. Kender", 
    "title": "Analysis and Visualization of Index Words from Audio Transcripts of   Instructional Videos", 
    "publish": "2004-08-27T20:45:32Z", 
    "summary": "We introduce new techniques for extracting, analyzing, and visualizing\ntextual contents from instructional videos of low production quality. Using\nAutomatic Speech Recognition, approximate transcripts (H75% Word Error Rate)\nare obtained from the originally highly compressed videos of university\ncourses, each comprising between 10 to 30 lectures. Text material in the form\nof books or papers that accompany the course are then used to filter meaningful\nphrases from the seemingly incoherent transcripts. The resulting index into the\ntranscripts is tied together and visualized in 3 experimental graphs that help\nin understanding the overall course structure and provide a tool for localizing\ncertain topics for indexing. We specifically discuss a Transcript Index Map,\nwhich graphically lays out key phrases for a course, a Textbook Chapter to\nTranscript Match, and finally a Lecture Transcript Similarity graph, which\nclusters semantically similar lectures. We test our methods and tools on 7 full\ncourses with 230 hours of video and 273 transcripts. We are able to extract up\nto 98 unique key terms for a given transcript and up to 347 unique key terms\nfor an entire course. The accuracy of the Textbook Chapter to Transcript Match\nexceeds 70% on average. The methods used can be applied to genres of video in\nwhich there are recurrent thematic words (news, sports, meetings,...)", 
    "link": "http://arxiv.org/pdf/cs/0408063v1", 
    "arxiv-id": "cs/0408063v1"
},{
    "category": "cs.MM", 
    "author": "Vita Hinze-Hoare", 
    "title": "From Digital Television to Internet?", 
    "publish": "2004-09-30T19:08:55Z", 
    "summary": "This paper provides a general technical overview of the Multimedia Home\nPlatform (MHP) specifications. MHP is a generic interface between digital\napplications and user machines, whether they happen to be set top boxes,\ndigital TV sets or Multimedia PC's. MHP extends the DVB open standards.\nAddressed are MHP architexture, System core and MHP Profiles.", 
    "link": "http://arxiv.org/pdf/cs/0409059v1", 
    "arxiv-id": "cs/0409059v1"
},{
    "category": "cs.MM", 
    "author": "H. Pirker", 
    "title": "RRL: A Rich Representation Language for the Description of Agent   Behaviour in NECA", 
    "publish": "2004-10-11T12:34:02Z", 
    "summary": "In this paper, we describe the Rich Representation Language (RRL) which is\nused in the NECA system. The NECA system generates interactions between two or\nmore animated characters. The RRL is an XML compliant framework for\nrepresenting the information that is exchanged at the interfaces between the\nvarious NECA system modules. The full XML Schemas for the RRL are available at\nhttp://www.ai.univie.ac.at/NECA/RRL", 
    "link": "http://arxiv.org/pdf/cs/0410022v1", 
    "arxiv-id": "cs/0410022v1"
},{
    "category": "cs.NI", 
    "author": "Henning Schulzrinne", 
    "title": "An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol", 
    "publish": "2004-12-05T03:56:57Z", 
    "summary": "Skype is a peer-to-peer VoIP client developed by KaZaa in 2003. Skype claims\nthat it can work almost seamlessly across NATs and firewalls and has better\nvoice quality than the MSN and Yahoo IM applications. It encrypts calls\nend-to-end, and stores user information in a decentralized fashion. Skype also\nsupports instant messaging and conferencing. This report analyzes key Skype\nfunctions such as login, NAT and firewall traversal, call establishment, media\ntransfer, codecs, and conferencing under three different network setups.\nAnalysis is performed by careful study of Skype network traffic.", 
    "link": "http://arxiv.org/pdf/cs/0412017v1", 
    "arxiv-id": "cs/0412017v1"
},{
    "category": "cs.MM", 
    "author": "Vitorino Ramos", 
    "title": "Self-Organizing the Abstract: Canvas as a Swarm Habitat for Collective   Memory, Perception and Cooperative Distributed Creativity", 
    "publish": "2004-12-17T15:36:05Z", 
    "summary": "Past experiences under the designation of \"Swarm Paintings\" conducted in\n2001, not only confirmed the possibility of realizing an artificial art (thus\nnon-human), as introduced into the process the questioning of creative\nmigration, specifically from the computer monitors to the canvas via a robotic\nharm. In more recent self-organized based research we seek to develop and\nprofound the initial ideas by using a swarm of autonomous robots (ARTsBOT\nproject 2002-03), that \"live\" avoiding the purpose of being merely a simple\nperpetrator of order streams coming from an external computer, but instead,\nthat actually co-evolve within the canvas space, acting (that is, laying ink)\naccording to simple inner threshold stimulus response functions, reacting\nsimultaneously to the chromatic stimulus present in the canvas environment done\nby the passage of their team-mates, as well as by the distributed feedback,\naffecting their future collective behaviour. In parallel, and in what respects\nto certain types of collective systems, we seek to confirm, in a physically\nembedded way, that the emergence of order (even as a concept) seems to be found\nat a lower level of complexity, based on simple and basic interchange of\ninformation, and on the local dynamic of parts, who, by self-organizing\nmechanisms tend to form an lived whole, innovative and adapting, allowing for\nemergent open-ended creative and distributed production. KEYWORDS: ArtSBots\nProject, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm\nPaintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation,\nArt and Complexity, ArtBots: The Robot Talent Show.", 
    "link": "http://arxiv.org/pdf/cs/0412073v1", 
    "arxiv-id": "cs/0412073v1"
},{
    "category": "cs.AI", 
    "author": "Vitorino Ramos", 
    "title": "On the Implicit and on the Artificial - Morphogenesis and Emergent   Aesthetics in Autonomous Collective Systems", 
    "publish": "2004-12-17T16:15:08Z", 
    "summary": "Imagine a \"machine\" where there is no pre-commitment to any particular\nrepresentational scheme: the desired behaviour is distributed and roughly\nspecified simultaneously among many parts, but there is minimal specification\nof the mechanism required to generate that behaviour, i.e. the global behaviour\nevolves from the many relations of multiple simple behaviours. A machine that\nlives to and from/with Synergy. An artificial super-organism that avoids\nspecific constraints and emerges within multiple low-level implicit\nbio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm\nIntelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot\nPaintings, Non-Human Art, Painting Emergence and Cooperation, Art and\nComplexity, ArtBots: The Robot Talent Show.", 
    "link": "http://arxiv.org/pdf/cs/0412077v1", 
    "arxiv-id": "cs/0412077v1"
},{
    "category": "cs.AI", 
    "author": "Vitorino Ramos", 
    "title": "The MC2 Project [Machines of Collective Conscience]: A possible walk, up   to Life-like Complexity and Behaviour, from bottom, basic and simple   bio-inspired heuristics - a walk, up into the morphogenesis of information", 
    "publish": "2004-12-17T16:28:26Z", 
    "summary": "Synergy (from the Greek word synergos), broadly defined, refers to combined\nor co-operative effects produced by two or more elements (parts or\nindividuals). The definition is often associated with the holistic conviction\nquote that \"the whole is greater than the sum of its parts\" (Aristotle, in\nMetaphysics), or the whole cannot exceed the sum of the energies invested in\neach of its parts (e.g. first law of thermodynamics) even if it is more\naccurate to say that the functional effects produced by wholes are different\nfrom what the parts can produce alone. Synergy is a ubiquitous phenomena in\nnature and human societies alike. One well know example is provided by the\nemergence of self-organization in social insects, via direct or indirect\ninteractions. The latter types are more subtle and defined as stigmergy to\nexplain task coordination and regulation in the context of nest reconstruction\nin termites. An example, could be provided by two individuals, who interact\nindirectly when one of them modifies the environment and the other responds to\nthe new environment at a later time. In other words, stigmergy could be defined\nas a particular case of environmental or spatial synergy. The system is purely\nholistic, and their properties are intrinsically emergent and autocatalytic. On\nthe present work we present a \"machine\" where there is no precommitment to any\nparticular representational scheme: the desired behaviour is distributed and\nroughly specified simultaneously among many parts, but there is minimal\nspecification of the mechanism required to generate that behaviour, i.e. the\nglobal behaviour evolves from the many relations of multiple simple behaviours.", 
    "link": "http://arxiv.org/pdf/cs/0412079v1", 
    "arxiv-id": "cs/0412079v1"
},{
    "category": "cs.DL", 
    "author": "Chris Wilper", 
    "title": "Fedora: An Architecture for Complex Objects and their Relationships", 
    "publish": "2005-01-07T17:57:05Z", 
    "summary": "The Fedora architecture is an extensible framework for the storage,\nmanagement, and dissemination of complex objects and the relationships among\nthem. Fedora accommodates the aggregation of local and distributed content into\ndigital objects and the association of services with objects. This al-lows an\nobject to have several accessible representations, some of them dy-namically\nproduced. The architecture includes a generic RDF-based relation-ship model\nthat represents relationships among objects and their components. Queries\nagainst these relationships are supported by an RDF triple store. The\narchitecture is implemented as a web service, with all aspects of the complex\nobject architecture and related management functions exposed through REST and\nSOAP interfaces. The implementation is available as open-source soft-ware,\nproviding the foundation for a variety of end-user applications for digital\nlibraries, archives, institutional repositories, and learning object systems.", 
    "link": "http://arxiv.org/pdf/cs/0501012v6", 
    "arxiv-id": "cs/0501012v6"
},{
    "category": "cs.MM", 
    "author": "Kwok-Tung Lo", 
    "title": "On the Design of Perceptual MPEG-Video Encryption Algorithms", 
    "publish": "2005-01-08T15:55:30Z", 
    "summary": "In this paper, some existing perceptual encryption algorithms of MPEG videos\nare reviewed and some problems, especially security defects of two recently\nproposed MPEG-video perceptual encryption schemes, are pointed out. Then, a\nsimpler and more effective design is suggested, which selectively encrypts\nfixed-length codewords (FLC) in MPEG-video bitstreams under the control of\nthree perceptibility factors. The proposed design is actually an encryption\nconfiguration that can work with any stream cipher or block cipher. Compared\nwith the previously-proposed schemes, the new design provides more useful\nfeatures, such as strict size-preservation, on-the-fly encryption and multiple\nperceptibility, which make it possible to support more applications with\ndifferent requirements. In addition, four different measures are suggested to\nprovide better security against known/chosen-plaintext attacks.", 
    "link": "http://arxiv.org/pdf/cs/0501014v3", 
    "arxiv-id": "cs/0501014v3"
},{
    "category": "cs.MM", 
    "author": "John R. Kender", 
    "title": "Augmented Segmentation and Visualization for Presentation Videos", 
    "publish": "2005-01-20T17:12:05Z", 
    "summary": "We investigate methods of segmenting, visualizing, and indexing presentation\nvideos by separately considering audio and visual data. The audio track is\nsegmented by speaker, and augmented with key phrases which are extracted using\nan Automatic Speech Recognizer (ASR). The video track is segmented by visual\ndissimilarities and augmented by representative key frames. An interactive user\ninterface combines a visual representation of audio, video, text, and key\nframes, and allows the user to navigate a presentation video. We also explore\nclustering and labeling of speaker data and present preliminary results.", 
    "link": "http://arxiv.org/pdf/cs/0501044v1", 
    "arxiv-id": "cs/0501044v1"
},{
    "category": "cs.CV", 
    "author": "Francisco B. Rodriguez", 
    "title": "A hybrid MLP-PNN architecture for fast image superresolution", 
    "publish": "2005-03-22T11:45:59Z", 
    "summary": "Image superresolution methods process an input image sequence of a scene to\nobtain a still image with increased resolution. Classical approaches to this\nproblem involve complex iterative minimization procedures, typically with high\ncomputational costs. In this paper is proposed a novel algorithm for\nsuper-resolution that enables a substantial decrease in computer load. First, a\nprobabilistic neural network architecture is used to perform a scattered-point\ninterpolation of the image sequence data. The network kernel function is\noptimally determined for this problem by a multi-layer perceptron trained on\nsynthetic data. Network parameters dependence on sequence noise level is\nquantitatively analyzed. This super-sampled image is spatially filtered to\ncorrect finite pixel size effects, to yield the final high-resolution estimate.\nResults on a real outdoor sequence are presented, showing the quality of the\nproposed method.", 
    "link": "http://arxiv.org/pdf/cs/0503053v1", 
    "arxiv-id": "cs/0503053v1"
},{
    "category": "cs.CV", 
    "author": "Javier Santamaria", 
    "title": "Semi-automatic vectorization of linear networks on rasterized   cartographic maps", 
    "publish": "2005-03-22T20:16:52Z", 
    "summary": "A system for semi-automatic vectorization of linear networks (roads, rivers,\netc.) on rasterized cartographic maps is presented. In this system, human\nintervention is limited to a graphic, interactive selection of the color\nattributes of the information to be obtained. Using this data, the system\nperforms a preliminary extraction of the linear network, which is subsequently\ncompleted, refined and vectorized by means of an automatic procedure. Results\non maps of different sources and scales are included.\n  -----\n  Se presenta un sistema semi-automatico de vectorizacion de redes de objetos\nlineales (carreteras, rios, etc.) en mapas cartograficos digitalizados. En este\nsistema, la intervencion humana queda reducida a la seleccion grafica\ninteractiva de los atributos de color de la informacion a obtener. Con estos\ndatos, el sistema realiza una extraccion preliminar de la red lineal, que se\ncompleta, refina y vectoriza mediante un procedimiento automatico. Se presentan\nresultados de la aplicacion del sistema sobre imagenes digitalizadas de mapas\nde distinta procedencia y escala.", 
    "link": "http://arxiv.org/pdf/cs/0503056v1", 
    "arxiv-id": "cs/0503056v1"
},{
    "category": "cs.DC", 
    "author": "G Gordon Worley III", 
    "title": "Wikis in Tuple Spaces", 
    "publish": "2005-04-27T23:04:35Z", 
    "summary": "We consider storing the pages of a wiki in a tuple space and the effects this\nmight have on the wiki experience. In particular, wiki pages are stored in\ntuples with a few identifying values such as title, author, revision date,\ncontent, etc. and pages are retrieved by sending the tuple space templates,\nsuch as one that gives the title but nothing else, leaving the tuple space to\nresolve to a single tuple. We use a tuple space wiki to avoid deadlocks,\ninfinite loops, and wasted efforts when page edit contention arises and examine\nhow a tuple space wiki changes the wiki experience.", 
    "link": "http://arxiv.org/pdf/cs/0504105v1", 
    "arxiv-id": "cs/0504105v1"
},{
    "category": "cs.MM", 
    "author": "Henrik Regensburg", 
    "title": "A Distributed Multimedia Communication System and its Applications to   E-Learning", 
    "publish": "2005-04-28T13:40:09Z", 
    "summary": "In this paper we report on a multimedia communication system including a\nVCoIP (Video Conferencing over IP) software with a distributed architecture and\nits applications for teaching scenarios. It is a simple, ready-to-use scheme\nfor distributed presenting, recording and streaming multimedia content. We also\nintroduce and investigate concepts and experiments to IPv6 user and session\nmobility, with the special focus on real-time video group communication.", 
    "link": "http://arxiv.org/pdf/cs/0504106v1", 
    "arxiv-id": "cs/0504106v1"
},{
    "category": "cs.CR", 
    "author": "Zbigniew Kotulski", 
    "title": "Alternative security architecture for IP Telephony based on digital   watermarking", 
    "publish": "2005-06-18T21:41:13Z", 
    "summary": "Problems with securing IP Telephony systems, insufficient standardization and\nlack of security mechanisms emerged the need for new approaches and solutions.\nIn this paper a new, alternative security architecture for voice-systems is\npresented. It is based on digital watermarking: a new, flexible and powerful\ntechnology that is increasingly gaining more and more attention. Besides known\napplications e.g. to solve copyright protection problems, we propose to use\ndigital watermarking to secure not only transmitted audio but also signaling\nprotocol that IP Telephony is based on.", 
    "link": "http://arxiv.org/pdf/cs/0506076v1", 
    "arxiv-id": "cs/0506076v1"
},{
    "category": "cs.MM", 
    "author": "Kwok-Tung Lo", 
    "title": "Cryptanalysis of an MPEG-Video Encryption Scheme Based on Secret Huffman   Tables", 
    "publish": "2005-09-13T10:44:31Z", 
    "summary": "This paper studies the security of a recently-proposed MPEG-video encryption\nscheme based on secret Huffman tables. Our cryptanalysis shows that: 1) the key\nspace of the encryption scheme is not sufficiently large against\ndivide-and-conquer (DAC) attack and known-plaintext attack; 2) it is possible\nto decrypt a cipher-video with a partially-known key, thus dramatically\nreducing the complexity of the DAC brute-force attack in some cases; 3) its\nsecurity against the chosen-plaintext attack is very weak. Some experimental\nresults are included to support the cryptanalytic results with a brief discuss\non how to improve this MPEG-video encryption scheme.", 
    "link": "http://arxiv.org/pdf/cs/0509035v2", 
    "arxiv-id": "cs/0509035v2"
},{
    "category": "cs.CR", 
    "author": "Kwok-Tung Lo", 
    "title": "Security Problems with Improper Implementations of Improved FEA-M", 
    "publish": "2005-09-13T10:35:52Z", 
    "summary": "This paper reports security problems with improper implementations of an\nimproved version of FEA-M (fast encryption algorithm for multimedia). It is\nfound that an implementation-dependent differential chosen-plaintext attack or\nits chosen-ciphertext counterpart can reveal the secret key of the\ncryptosystem, if the involved (pseudo-)random process can be tampered (for\nexample, through a public time service). The implementation-dependent\ndifferential attack is very efficient in complexity and needs only $O(n^2)$\nchosen plaintext or ciphertext bits. In addition, this paper also points out a\nminor security problem with the selection of the session key. In real\nimplementations of the cryptosystem, these security problems should be\ncarefully avoided, or the cryptosystem has to be further enhanced to work under\nsuch weak implementations.", 
    "link": "http://arxiv.org/pdf/cs/0509036v2", 
    "arxiv-id": "cs/0509036v2"
},{
    "category": "cs.CR", 
    "author": "Zbigniew Kotulski", 
    "title": "New security and control protocol for VoIP based on steganography and   digital watermarking", 
    "publish": "2006-02-10T22:49:53Z", 
    "summary": "In this paper new security and control protocol for Voice over Internet\nProtocol (VoIP) service is presented. It is the alternative for the IETF's\n(Internet Engineering Task Force) RTCP (Real-Time Control Protocol) for\nreal-time application's traffic. Additionally this solution offers\nauthentication and integrity, it is capable of exchanging and verifying QoS and\nsecurity parameters. It is based on digital watermarking and steganography that\nis why it does not consume additional bandwidth and the data transmitted is\ninseparably bound to the voice content.", 
    "link": "http://arxiv.org/pdf/cs/0602042v1", 
    "arxiv-id": "cs/0602042v1"
},{
    "category": "cs.MM", 
    "author": "Guang-Can Guo", 
    "title": "A Hybrid Quantum Encoding Algorithm of Vector Quantization for Image   Compression", 
    "publish": "2006-04-30T13:35:54Z", 
    "summary": "Many classical encoding algorithms of Vector Quantization (VQ) of image\ncompression that can obtain global optimal solution have computational\ncomplexity O(N). A pure quantum VQ encoding algorithm with probability of\nsuccess near 100% has been proposed, that performs operations 45sqrt(N) times\napproximately. In this paper, a hybrid quantum VQ encoding algorithm between\nclassical method and quantum algorithm is presented. The number of its\noperations is less than sqrt(N) for most images, and it is more efficient than\nthe pure quantum algorithm.\n  Key Words: Vector Quantization, Grover's Algorithm, Image Compression,\nQuantum Algorithm", 
    "link": "http://arxiv.org/pdf/cs/0605002v3", 
    "arxiv-id": "cs/0605002v3"
},{
    "category": "cs.MM", 
    "author": "Teddy Furon", 
    "title": "A constructive and unifying framework for zero-bit watermarking", 
    "publish": "2006-06-08T13:28:07Z", 
    "summary": "In the watermark detection scenario, also known as zero-bit watermarking, a\nwatermark, carrying no hidden message, is inserted in content. The watermark\ndetector checks for the presence of this particular weak signal in content. The\narticle looks at this problem from a classical detection theory point of view,\nbut with side information enabled at the embedding side. This means that the\nwatermark signal is a function of the host content. Our study is twofold. The\nfirst step is to design the best embedding function for a given detection\nfunction, and the best detection function for a given embedding function. This\nyields two conditions, which are mixed into one `fundamental' partial\ndifferential equation. It appears that many famous watermarking schemes are\nindeed solution to this `fundamental' equation. This study thus gives birth to\na constructive framework unifying solutions, so far perceived as very\ndifferent.", 
    "link": "http://arxiv.org/pdf/cs/0606034v2", 
    "arxiv-id": "cs/0606034v2"
},{
    "category": "cs.NI", 
    "author": "Emin Gabrielyan", 
    "title": "Fault-Tolerant Real-Time Streaming with FEC thanks to Capillary   Multi-Path Routing", 
    "publish": "2006-07-17T08:10:28Z", 
    "summary": "Erasure resilient FEC codes in off-line packetized streaming rely on time\ndiversity. This requires unrestricted buffering time at the receiver. In\nreal-time streaming the playback buffering time must be very short. Path\ndiversity is an orthogonal strategy. However, the large number of long paths\nincreases the number of underlying links and consecutively the overall link\nfailure rate. This may increase the overall requirement in redundant FEC\npackets for combating the link failures. We introduce the Redundancy Overall\nRequirement (ROR) metric, a routing coefficient specifying the total number of\nFEC packets required for compensation of all underlying link failures. We\npresent a capillary routing algorithm for constructing layer by layer steadily\ndiversifying multi-path routing patterns. By measuring the ROR coefficients of\na dozen of routing layers on hundreds of network samples, we show that the\nnumber of required FEC packets decreases substantially when the path diversity\nis increased by the capillary routing construction algorithm.", 
    "link": "http://arxiv.org/pdf/cs/0607077v1", 
    "arxiv-id": "cs/0607077v1"
},{
    "category": "cs.CR", 
    "author": "Guanrong Chen", 
    "title": "Cryptanalysis of an Encryption Scheme Based on Blind Source Separation", 
    "publish": "2006-08-04T11:32:44Z", 
    "summary": "Recently Lin et al. proposed a method of using the underdetermined BSS (blind\nsource separation) problem to realize image and speech encryption. In this\npaper, we give a cryptanalysis of this BSS-based encryption and point out that\nit is not secure against known/chosen-plaintext attack and chosen-ciphertext\nattack. In addition, there exist some other security defects: low sensitivity\nto part of the key and the plaintext, a ciphertext-only differential attack,\ndivide-and-conquer (DAC) attack on part of the key. We also discuss the role of\nBSS in Lin et al.'s efforts towards cryptographically secure ciphers.", 
    "link": "http://arxiv.org/pdf/cs/0608024v1", 
    "arxiv-id": "cs/0608024v1"
},{
    "category": "cs.MM", 
    "author": "Zhiquan Wang", 
    "title": "Security Analysis of A Chaos-based Image Encryption Algorithm", 
    "publish": "2006-08-30T03:22:06Z", 
    "summary": "The security of Fridrich Image Encryption Algorithm against brute-force\nattack, statistical attack, known-plaintext attack and select-plaintext attack\nis analyzed by investigating the properties of the involved chaotic maps and\ndiffusion functions. Based on the given analyses, some means are proposed to\nstrengthen the overall performance of the focused cryptosystem.", 
    "link": "http://arxiv.org/pdf/cs/0608119v1", 
    "arxiv-id": "cs/0608119v1"
},{
    "category": "cs.CR", 
    "author": "Wing-Shing Law", 
    "title": "A Fast Image Encryption Scheme based on Chaotic Standard Map", 
    "publish": "2006-09-29T07:36:29Z", 
    "summary": "In recent years, a variety of effective chaos-based image encryption schemes\nhave been proposed. The typical structure of these schemes has the permutation\nand the diffusion stages performed alternatively. The confusion and diffusion\neffect is solely contributed by the permutation and the diffusion stage,\nrespectively. As a result, more overall rounds than necessary are required to\nachieve a certain level of security. In this paper, we suggest to introduce\ncertain diffusion effect in the confusion stage by simple sequential\nadd-and-shift operations. The purpose is to reduce the workload of the\ntime-consuming diffusion part so that fewer overall rounds and hence a shorter\nencryption time is needed. Simulation results show that at a similar\nperformance level, the proposed cryptosystem needs less than one-third the\nencryption time of an existing cryptosystem. The effective acceleration of the\nencryption speed is thus achieved.", 
    "link": "http://arxiv.org/pdf/cs/0609158v1", 
    "arxiv-id": "cs/0609158v1"
},{
    "category": "cs.NI", 
    "author": "Olivier Fourmaux", 
    "title": "P2P IPTV Measurement: A Comparison Study", 
    "publish": "2006-10-23T13:55:28Z", 
    "summary": "With the success of P2P file sharing, new emerging P2P applications arise on\nthe Internet for streaming content like voice (VoIP) or live video (IPTV).\nNowadays, there are lots of works measuring P2P file sharing or P2P telephony\nsystems, but there is still no comprehensive study about P2P IPTV, whereas it\nshould be massively used in the future. During the last FIFA world cup, we\nmeasured network traffic generated by P2P IPTV applications like PPlive,\nPPstream, TVants and Sopcast. In this paper we analyze some of our results\nduring the same games for the applications. We focus on traffic statistics and\nchurn of peers within these P2P networks. Our objectives are threefold: we\npoint out the traffic generated to understand the impact they will have on the\nnetwork, we try to infer the mechanisms of such applications and highlight\ndifferences, and we give some insights about the users' behavior.", 
    "link": "http://arxiv.org/pdf/cs/0610133v4", 
    "arxiv-id": "cs/0610133v4"
},{
    "category": "cs.CR", 
    "author": "Zbigniew Kotulski", 
    "title": "Lightweight security mechanism for PSTN-VoIP cooperation", 
    "publish": "2006-12-08T22:05:21Z", 
    "summary": "In this paper we describe a new, lightweight security mechanism for PSTN-VoIP\ncooperation that is based on two information hiding techniques: digital\nwatermarking and steganography. Proposed scheme is especially suitable for\nPSTN-IP-PSTN (toll-by-passing) scenario which nowadays is very popular\napplication of IP Telephony systems. With the use of this mechanism we\nauthenticate end-to-end transmitted voice between PSTN users. Additionally we\nimprove IP part traffic security (both media stream and VoIP signalling\nmessages). Exemplary scenario is presented for SIP signalling protocol along\nwith SIP-T extension and H.248/Megaco protocol.", 
    "link": "http://arxiv.org/pdf/cs/0612054v1", 
    "arxiv-id": "cs/0612054v1"
},{
    "category": "cs.SD", 
    "author": "John R. Kender", 
    "title": "Accommodating Sample Size Effect on Similarity Measures in Speaker   Clustering", 
    "publish": "2006-12-28T06:39:55Z", 
    "summary": "We investigate the symmetric Kullback-Leibler (KL2) distance in speaker\nclustering and its unreported effects for differently-sized feature matrices.\nSpeaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)\nvectors, and features are compared using the KL2 metric to form clusters of\nspeech segments for each speaker. We make two observations with respect to\nclustering based on KL2: 1.) The accuracy of clustering is strongly dependent\non the absolute lengths of the speech segments and their extracted feature\nvectors. 2.) The accuracy of the similarity measure strongly degrades with the\nlength of the shorter of the two speech segments. These effects of length can\nbe attributed to the measure of covariance used in KL2. We demonstrate an\nempirical correction of this sample-size effect that increases clustering\naccuracy. We draw parallels to two Vector Quantization-based (VQ) similarity\nmeasures, one which exhibits an equivalent effect of sample size, and the\nsecond being less influenced by it.", 
    "link": "http://arxiv.org/pdf/cs/0612138v1", 
    "arxiv-id": "cs/0612138v1"
},{
    "category": "cs.SD", 
    "author": "John R. Kender", 
    "title": "Alignment of Speech to Highly Imperfect Text Transcriptions", 
    "publish": "2006-12-28T06:45:43Z", 
    "summary": "We introduce a novel and inexpensive approach for the temporal alignment of\nspeech to highly imperfect transcripts from automatic speech recognition (ASR).\nTranscripts are generated for extended lecture and presentation videos, which\nin some cases feature more than 30 speakers with different accents, resulting\nin highly varying transcription qualities. In our approach we detect a subset\nof phonemes in the speech track, and align them to the sequence of phonemes\nextracted from the transcript. We report on the results for 4 speech-transcript\nsets ranging from 22 to 108 minutes. The alignment performance is promising,\nshowing a correct matching of phonemes within 10, 20, 30 second error margins\nfor more than 60%, 75%, 90% of text, respectively, on average.", 
    "link": "http://arxiv.org/pdf/cs/0612139v1", 
    "arxiv-id": "cs/0612139v1"
},{
    "category": "cs.AI", 
    "author": "Laurent Romary", 
    "title": "Multimodal Meaning Representation for Generic Dialogue Systems   Architectures", 
    "publish": "2007-03-16T15:37:47Z", 
    "summary": "An unified language for the communicative acts between agents is essential\nfor the design of multi-agents architectures. Whatever the type of interaction\n(linguistic, multimodal, including particular aspects such as force feedback),\nwhatever the type of application (command dialogue, request dialogue, database\nquerying), the concepts are common and we need a generic meta-model. In order\nto tend towards task-independent systems, we need to clarify the modules\nparameterization procedures. In this paper, we focus on the characteristics of\na meta-model designed to represent meaning in linguistic and multimodal\napplications. This meta-model is called MMIL for MultiModal Interface Language,\nand has first been specified in the framework of the IST MIAMM European\nproject. What we want to test here is how relevant is MMIL for a completely\ndifferent context (a different task, a different interaction type, a different\nlinguistic domain). We detail the exploitation of MMIL in the framework of the\nIST OZONE European project, and we draw the conclusions on the role of MMIL in\nthe parameterization of task-independent dialogue managers.", 
    "link": "http://arxiv.org/pdf/cs/0703091v1", 
    "arxiv-id": "cs/0703091v1"
},{
    "category": "quant-ph", 
    "author": "Jose I. Latorre", 
    "title": "Image compression and entanglement", 
    "publish": "2005-10-04T14:54:22Z", 
    "summary": "The pixel values of an image can be casted into a real ket of a Hilbert space\nusing an appropriate block structured addressing. The resulting state can then\nbe rewritten in terms of its matrix product state representation in such a way\nthat quantum entanglement corresponds to classical correlations between\ndifferent coarse-grained textures. A truncation of the MPS representation is\ntantamount to a compression of the original image. The resulting algorithm can\nbe improved adding a discrete Fourier transform preprocessing and a further\nentropic lossless compression.", 
    "link": "http://arxiv.org/pdf/quant-ph/0510031v1", 
    "arxiv-id": "quant-ph/0510031v1"
},{
    "category": "cs.NI", 
    "author": "Kave Salamatian", 
    "title": "Characterization of P2P IPTV Traffic: Scaling Analysis", 
    "publish": "2007-04-24T16:18:13Z", 
    "summary": "P2P IPTV applications arise on the Internet and will be massively used in the\nfuture. It is expected that P2P IPTV will contribute to increase the overall\nInternet traffic. In this context, it is important to measure the impact of P2P\nIPTV on the networks and to characterize this traffic. Dur- ing the 2006 FIFA\nWorld Cup, we performed an extensive measurement campaign. We measured network\ntraffic generated by broadcasting soc- cer games by the most popular P2P IPTV\napplications, namely PPLive, PPStream, SOPCast and TVAnts. From the collected\ndata, we charac- terized the P2P IPTV traffic structure at different time\nscales by using wavelet based transform method. To the best of our knowledge,\nthis is the first work, which presents a complete multiscale analysis of the\nP2P IPTV traffic. Our results show that the scaling properties of the TCP\ntraffic present periodic behavior whereas the UDP traffic is stationary and\nlead to long- range depedency characteristics. For all the applications, the\ndownload traffic has different characteristics than the upload traffic. The\nsignaling traffic has a significant impact on the download traffic but it has\nnegligible impact on the upload. Both sides of the traffic and its granularity\nhas to be taken into account to design accurate P2P IPTV traffic models.", 
    "link": "http://arxiv.org/pdf/0704.3228v2", 
    "arxiv-id": "0704.3228v2"
},{
    "category": "cs.MM", 
    "author": "Shangteng Huang", 
    "title": "Double Sided Watermark Embedding and Detection with Perceptual Analysis", 
    "publish": "2007-05-14T12:23:43Z", 
    "summary": "In our previous work, we introduced a double-sided technique that utilizes\nbut not reject the host interference. Due to its nice property of utilizing but\nnot rejecting the host interference, it has a big advantage over the host\ninterference schemes in that the perceptual analysis can be easily implemented\nfor our scheme to achieve the locally bounded maximum embedding strength. Thus,\nin this work, we detail how to implement the perceptual analysis in our\ndouble-sided schemes since the perceptual analysis is very important for\nimproving the fidelity of watermarked contents. Through the extensive\nperformance comparisons, we can further validate the performance advantage of\nour double-sided schemes.", 
    "link": "http://arxiv.org/pdf/0705.1925v1", 
    "arxiv-id": "0705.1925v1"
},{
    "category": "cs.MM", 
    "author": "Jidong Zhong", 
    "title": "Watermark Embedding and Detection", 
    "publish": "2007-06-02T05:37:34Z", 
    "summary": "The embedder and the detector (or decoder) are the two most important\ncomponents of the digital watermarking systems. Thus in this work, we discuss\nhow to design a better embedder and detector (or decoder). I first give a\nsummary of the prospective applications of watermarking technology and major\nwatermarking schemes in the literature. My review on the literature closely\ncenters upon how the side information is exploited at both embedders and\ndetectors. In Chapter 3, I explore the optimum detector or decoder according to\na particular probability distribution of the host signals. We found that the\nperformance of both multiplicative and additive spread spectrum schemes depends\non the shape parameter of the host signals. For spread spectrum schemes, the\nperformance of the detector or the decoder is reduced by the host interference.\nThus I present a new host-interference rejection technique for the\nmultiplicative spread spectrum schemes. Its embedding rule is tailored to the\noptimum detection or decoding rule. Though the host interference rejection\nschemes enjoy a big performance gain over the traditional spread spectrum\nschemes, their drawbacks that it is difficult for them to be implemented with\nthe perceptual analysis to achieve the maximum allowable embedding level\ndiscourage their use in real scenarios. Thus, in the last chapters of this\nwork, I introduce a double-sided technique to tackle this drawback. It differs\nfrom the host interference rejection schemes in that it utilizes but does not\nreject the host interference at its embedder. The perceptual analysis can be\neasily implemented in our scheme to achieve the maximum allowable level of\nembedding strength.", 
    "link": "http://arxiv.org/pdf/0706.0427v1", 
    "arxiv-id": "0706.0427v1"
},{
    "category": "cs.MM", 
    "author": "Steffen Rothkugel", 
    "title": "Multimedia Content Distribution in Hybrid Wireless Networks using   Weighted Clustering", 
    "publish": "2007-06-08T09:31:46Z", 
    "summary": "Fixed infrastructured networks naturally support centralized approaches for\ngroup management and information provisioning. Contrary to infrastructured\nnetworks, in multi-hop ad-hoc networks each node acts as a router as well as\nsender and receiver. Some applications, however, requires hierarchical\narrangements that-for practical reasons-has to be done locally and\nself-organized. An additional challenge is to deal with mobility that causes\npermanent network partitioning and re-organizations. Technically, these\nproblems can be tackled by providing additional uplinks to a backbone network,\nwhich can be used to access resources in the Internet as well as to inter-link\nmultiple ad-hoc network partitions, creating a hybrid wireless network. In this\npaper, we present a prototypically implemented hybrid wireless network system\noptimized for multimedia content distribution. To efficiently manage the ad-hoc\ncommunicating devices a weighted clustering algorithm is introduced. The\nproposed localized algorithm deals with mobility, but does not require\ngeographical information or distances.", 
    "link": "http://arxiv.org/pdf/0706.1141v1", 
    "arxiv-id": "0706.1141v1"
},{
    "category": "cs.MM", 
    "author": "Haila Wang", 
    "title": "On the Performance of Joint Fingerprint Embedding and Decryption Scheme", 
    "publish": "2007-06-21T02:44:44Z", 
    "summary": "Till now, few work has been done to analyze the performances of joint\nfingerprint embedding and decryption schemes. In this paper, the security of\nthe joint fingerprint embedding and decryption scheme proposed by Kundur et al.\nis analyzed and improved. The analyses include the security against\nunauthorized customer, the security against authorized customer, the\nrelationship between security and robustness, the relationship between\nsecu-rity and imperceptibility and the perceptual security. Based these\nanalyses, some means are proposed to strengthen the system, such as multi-key\nencryp-tion and DC coefficient encryption. The method can be used to analyze\nother JFD schemes. It is expected to provide valuable information to design JFD\nschemes.", 
    "link": "http://arxiv.org/pdf/0706.3076v1", 
    "arxiv-id": "0706.3076v1"
},{
    "category": "cs.CR", 
    "author": "Jiwu Huang", 
    "title": "Robust Audio Watermarking Against the D/A and A/D conversions", 
    "publish": "2007-07-03T11:49:03Z", 
    "summary": "Audio watermarking has played an important role in multimedia security. In\nmany applications using audio watermarking, D/A and A/D conversions (denoted by\nDA/AD in this paper) are often involved. In previous works, however, the\nrobustness issue of audio watermarking against the DA/AD conversions has not\ndrawn sufficient attention yet. In our extensive investigation, it has been\nfound that the degradation of a watermarked audio signal caused by the DA/AD\nconversions manifests itself mainly in terms of wave magnitude distortion and\nlinear temporal scaling, making the watermark extraction failed. Accordingly, a\nDWT-based audio watermarking algorithm robust against the DA/AD conversions is\nproposed in this paper. To resist the magnitude distortion, the relative energy\nrelationships among different groups of the DWT coefficients in the\nlow-frequency sub-band are utilized in watermark embedding by adaptively\ncontrolling the embedding strength. Furthermore, the resynchronization is\ndesigned to cope with the linear temporal scaling. The time-frequency\nlocalization characteristics of DWT are exploited to save the computational\nload in the resynchronization. Consequently, the proposed audio watermarking\nalgorithm is robust against the DA/AD conversions, other common audio\nprocessing manipulations, and the attacks in StirMark Benchmark for Audio,\nwhich has been verified by experiments.", 
    "link": "http://arxiv.org/pdf/0707.0397v1", 
    "arxiv-id": "0707.0397v1"
},{
    "category": "cs.NI", 
    "author": "Jiangchuan Liu", 
    "title": "Understanding the Characteristics of Internet Short Video Sharing:   YouTube as a Case Study", 
    "publish": "2007-07-25T05:39:44Z", 
    "summary": "Established in 2005, YouTube has become the most successful Internet site\nproviding a new generation of short video sharing service. Today, YouTube alone\ncomprises approximately 20% of all HTTP traffic, or nearly 10% of all traffic\non the Internet. Understanding the features of YouTube and similar video\nsharing sites is thus crucial to their sustainable development and to network\ntraffic engineering. In this paper, using traces crawled in a 3-month period,\nwe present an in-depth and systematic measurement study on the characteristics\nof YouTube videos. We find that YouTube videos have noticeably different\nstatistics compared to traditional streaming videos, ranging from length and\naccess pattern, to their active life span, ratings, and comments. The series of\ndatasets also allows us to identify the growth trend of this fast evolving\nInternet site in various aspects, which has seldom been explored before. We\nalso look closely at the social networking aspect of YouTube, as this is a key\ndriving force toward its success. In particular, we find that the links to\nrelated videos generated by uploaders' choices form a small-world network. This\nsuggests that the videos have strong correlations with each other, and creates\nopportunities for developing novel caching or peer-to-peer distribution schemes\nto efficiently deliver videos to end users.", 
    "link": "http://arxiv.org/pdf/0707.3670v1", 
    "arxiv-id": "0707.3670v1"
},{
    "category": "cs.MM", 
    "author": "Shiguo Lian", 
    "title": "Image Authentication Based on Neural Networks", 
    "publish": "2007-07-31T02:27:10Z", 
    "summary": "Neural network has been attracting more and more researchers since the past\ndecades. The properties, such as parameter sensitivity, random similarity,\nlearning ability, etc., make it suitable for information protection, such as\ndata encryption, data authentication, intrusion detection, etc. In this paper,\nby investigating neural networks' properties, the low-cost authentication\nmethod based on neural networks is proposed and used to authenticate images or\nvideos. The authentication method can detect whether the images or videos are\nmodified maliciously. Firstly, this chapter introduces neural networks'\nproperties, such as parameter sensitivity, random similarity, diffusion\nproperty, confusion property, one-way property, etc. Secondly, the chapter\ngives an introduction to neural network based protection methods. Thirdly, an\nimage or video authentication scheme based on neural networks is presented, and\nits performances, including security, robustness and efficiency, are analyzed.\nFinally, conclusions are drawn, and some open issues in this field are\npresented.", 
    "link": "http://arxiv.org/pdf/0707.4524v1", 
    "arxiv-id": "0707.4524v1"
},{
    "category": "cs.HC", 
    "author": "Matthew McCool", 
    "title": "An Application of Chromatic Prototypes", 
    "publish": "2007-08-04T02:38:19Z", 
    "summary": "This paper has been withdrawn.", 
    "link": "http://arxiv.org/pdf/0708.0598v2", 
    "arxiv-id": "0708.0598v2"
},{
    "category": "cs.MM", 
    "author": "Hiroshi Murase", 
    "title": "A quick search method for audio signals based on a piecewise linear   representation of feature trajectories", 
    "publish": "2007-10-23T03:06:53Z", 
    "summary": "This paper presents a new method for a quick similarity-based search through\nlong unlabeled audio streams to detect and locate audio clips provided by\nusers. The method involves feature-dimension reduction based on a piecewise\nlinear representation of a sequential feature trajectory extracted from a long\naudio stream. Two techniques enable us to obtain a piecewise linear\nrepresentation: the dynamic segmentation of feature trajectories and the\nsegment-based Karhunen-L\\'{o}eve (KL) transform. The proposed search method\nguarantees the same search results as the search method without the proposed\nfeature-dimension reduction method in principle. Experiment results indicate\nsignificant improvements in search speed. For example the proposed method\nreduced the total search time to approximately 1/12 that of previous methods\nand detected queries in approximately 0.3 seconds from a 200-hour audio\ndatabase.", 
    "link": "http://arxiv.org/pdf/0710.4180v1", 
    "arxiv-id": "0710.4180v1"
},{
    "category": "cs.AR", 
    "author": "J. T. J. Van Eijndhoven", 
    "title": "Compositional Memory Systems for Multimedia Communicating Tasks", 
    "publish": "2007-10-25T08:35:10Z", 
    "summary": "Conventional cache models are not suited for real-time parallel processing\nbecause tasks may flush each other's data out of the cache in an unpredictable\nmanner. In this way the system is not compositional so the overall performance\nis difficult to predict and the integration of new tasks expensive. This paper\nproposes a new method that imposes compositionality to the system?s performance\nand makes different memory hierarchy optimizations possible for multimedia\ncommunicating tasks when running on embedded multiprocessor architectures. The\nmethod is based on a cache allocation strategy that assigns sets of the unified\ncache exclusively to tasks and to the communication buffers. We also\nanalytically formulate the problem and describe a method to compute the cache\npartitioning ratio for optimizing the throughput and the consumed power. When\napplied to a multiprocessor with memory hierarchy our technique delivers also\nperformance gain. Compared to the shared cache case, for an application\nconsisting of two jpeg decoders and one edge detection algorithm 5 times less\nmisses are experienced and for an mpeg2 decoder 6.5 times less misses are\nexperienced.", 
    "link": "http://arxiv.org/pdf/0710.4658v1", 
    "arxiv-id": "0710.4658v1"
},{
    "category": "cs.AR", 
    "author": "Youn-Long Lin", 
    "title": "Integration, Verification and Layout of a Complex Multimedia SOC", 
    "publish": "2007-10-25T08:44:47Z", 
    "summary": "We present our experience of designing a single-chip controller for advanced\ndigital still camera from specification all the way to mass production. The\nprocess involves collaboration with camera system designer, IP vendors, EDA\nvendors, silicon wafer foundry, package and testing houses, and camera maker.\nWe also co-work with academic research groups to develop a JPEG codec IP and\nmemory BIST and SOC testing methodology. In this presentation, we cover the\nproblems encountered, our solutions, and lessons learned.", 
    "link": "http://arxiv.org/pdf/0710.4667v1", 
    "arxiv-id": "0710.4667v1"
},{
    "category": "cs.MM", 
    "author": "G. Pravadelli", 
    "title": "An Integrated Design and Verification Methodology for Reconfigurable   Multimedia Systems", 
    "publish": "2007-10-25T12:24:16Z", 
    "summary": "Recently a lot of multimedia applications are emerging on portable\nappliances. They require both the flexibility of upgradeable devices\n(traditionally software based) and a powerful computing engine (typically\nhardware). In this context, programmable HW and dynamic reconfiguration allow\nnovel approaches to the migration of algorithms from SW to HW. Thus, in the\nframe of the Symbad project, we propose an industrial design flow for\nreconfigurable SoC's. The goal of Symbad consists of developing a system level\ndesign platform for hardware and software SoC systems including formal and\nsemi-formal verification techniques.", 
    "link": "http://arxiv.org/pdf/0710.4846v1", 
    "arxiv-id": "0710.4846v1"
},{
    "category": "cs.MM", 
    "author": "Shiguo Lian", 
    "title": "Secure Fractal Image Coding", 
    "publish": "2007-11-22T03:53:29Z", 
    "summary": "In recent work, various fractal image coding methods are reported, which\nadopt the self-similarity of images to compress the size of images. However,\ntill now, no solutions for the security of fractal encoded images have been\nprovided. In this paper, a secure fractal image coding scheme is proposed and\nevaluated, which encrypts some of the fractal parameters during fractal\nencoding, and thus, produces the encrypted and encoded image. The encrypted\nimage can only be recovered by the correct key. To keep secure and efficient,\nonly the suitable parameters are selected and encrypted through in-vestigating\nthe properties of various fractal parameters, including parameter space,\nparameter distribu-tion and parameter sensitivity. The encryption process does\nnot change the file format, keeps secure in perception, and costs little time\nor computational resources. These properties make it suitable for secure image\nencoding or transmission.", 
    "link": "http://arxiv.org/pdf/0711.3500v1", 
    "arxiv-id": "0711.3500v1"
},{
    "category": "cs.CR", 
    "author": "Wolfgang A. Halang", 
    "title": "Cryptanalysis of an Image Encryption Scheme Based on a Compound Chaotic   Sequence", 
    "publish": "2007-12-24T04:02:35Z", 
    "summary": "Recently, an image encryption scheme based on a compound chaotic sequence was\nproposed. In this paper, the security of the scheme is studied and the\nfollowing problems are found: (1) a differential chosen-plaintext attack can\nbreak the scheme with only three chosen plain-images; (2) there is a number of\nweak keys and some equivalent keys for encryption; (3) the scheme is not\nsensitive to the changes of plain-images; and (4) the compound chaotic sequence\ndoes not work as a good random number resource.", 
    "link": "http://arxiv.org/pdf/0712.3964v2", 
    "arxiv-id": "0712.3964v2"
},{
    "category": "cs.MM", 
    "author": "Shiguo Lian", 
    "title": "On the Robustness of the Delay-Based Fingerprint Embedding Scheme", 
    "publish": "2008-01-04T03:15:56Z", 
    "summary": "The delay-based fingerprint embedding was recently proposed to support more\nusers in secure media distribution scenario. In this embedding scheme, some\nusers are assigned the same fingerprint code with only different embedding\ndelay. The algorithm's robustness against collusion attacks is investigated.\nHowever, its robustness against common desynchronization attacks, e.g.,\ncropping and time shifting, is not considered. In this paper, desynchronization\nattacks are used to break the delay-based fingerprint embedding algorithm. To\nimprove the robustness, two means are proposed to keep the embedded fingerprint\ncodes synchronized, i.e., adding a synchronization fingerprint and adopting the\nrelative delay to detect users. Analyses and experiments are given to show the\nimprovements.", 
    "link": "http://arxiv.org/pdf/0801.0625v1", 
    "arxiv-id": "0801.0625v1"
},{
    "category": "cs.CV", 
    "author": "Ciprian Ilioaei", 
    "title": "Some Aspects of Testing Process for Transport Streams in Digital Video   Broadcasting", 
    "publish": "2008-02-22T10:48:44Z", 
    "summary": "This paper presents some aspects related to the DVB (Digital Video\nBroadcasting) investigation. The basic aspects of DVB are presented, with an\nemphasis on DVB-T version of standard. The main purpose of this research is to\nanalyze the way that the transmission of the transport streams is realized in\ncase of the Terrestrial Digital Video Broadcasting (DVB-T). To accomplish this,\nfirst, Digital Video Broadcasting standard is presented, and then the main\naspects of DVB testing and analysis of the transport streams are investigated.\nThe paper presents also the results obtained using two programs designed for\nDVB analysis: Mosalina and TSA.", 
    "link": "http://arxiv.org/pdf/0802.3285v1", 
    "arxiv-id": "0802.3285v1"
},{
    "category": "cs.CV", 
    "author": "Radu Arsinte", 
    "title": "Implementing a Test Strategy for an Advanced Video Acquisition and   Processing Architecture", 
    "publish": "2008-02-22T10:54:59Z", 
    "summary": "This paper presents some aspects related to test process of an advanced video\nsystem used in remote IP surveillance. The system is based on a Pentium\ncompatible architecture using the industrial standard PC104+. First the overall\narchitecture of the system is presented, involving both hardware or software\naspects. The acquisition board which is developed in a special, nonstandard\narchitecture, is also briefly presented. The main purpose of this research was\nto set a coherent set of procedures in order to test all the aspects of the\nvideo acquisition board. To accomplish this, it was necessary to set-up a\nprocedure in two steps: stand alone video board test (functional test) and an\nin-system test procedure verifying the compatibility with both OS: Linux and\nWindows. The paper presents also the results obtained using this procedure.", 
    "link": "http://arxiv.org/pdf/0802.3288v1", 
    "arxiv-id": "0802.3288v1"
},{
    "category": "cs.CV", 
    "author": "Costin Miron", 
    "title": "Acquisition Accuracy Evaluation in Visual Inspection Systems - a   Practical Approach", 
    "publish": "2008-03-03T08:57:10Z", 
    "summary": "This paper draws a proposal of a set of parameters and methods for accuracy\nevaluation of visual inspection systems. The case of a monochrome board is\ntreated, but practically all conclusions and methods may be extended for colour\nacquisition. Basically, the proposed parameters are grouped in five sets as\nfollows:Internal noise;Video ADC cuantisation parameters;Analogue processing\nsection parameters;Dominant frequencies;Synchronisation (lock-in) accuracy. On\nbasis of this set of parameters was developed a software environment, in\nconjunction with a test signal generator that allows the \"test\" images. The\npaper also presents conclusions of evaluation for two types of video\nacquisition boards", 
    "link": "http://arxiv.org/pdf/0803.0194v1", 
    "arxiv-id": "0803.0194v1"
},{
    "category": "cs.MM", 
    "author": "Giulia Menconi", 
    "title": "Multi-dimensional sparse time series: feature extraction", 
    "publish": "2008-03-04T10:27:59Z", 
    "summary": "We show an analysis of multi-dimensional time series via entropy and\nstatistical linguistic techniques. We define three markers encoding the\nbehavior of the series, after it has been translated into a multi-dimensional\nsymbolic sequence. The leading component and the trend of the series with\nrespect to a mobile window analysis result from the entropy analysis and label\nthe dynamical evolution of the series. The diversification formalizes the\ndifferentiation in the use of recurrent patterns, from a Zipf law point of\nview. These markers are the starting point of further analysis such as\nclassification or clustering of large database of multi-dimensional time\nseries, prediction of future behavior and attribution of new data. We also\npresent an application to economic data. We deal with measurements of money\ninvestments of some business companies in advertising market for different\nmedia sources.", 
    "link": "http://arxiv.org/pdf/0803.0405v1", 
    "arxiv-id": "0803.0405v1"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Steganography of VoIP Streams", 
    "publish": "2008-05-19T20:46:38Z", 
    "summary": "The paper concerns available steganographic techniques that can be used for\ncreating covert channels for VoIP (Voice over Internet Protocol) streams. Apart\nfrom characterizing existing steganographic methods we provide new insights by\npresenting two new techniques. The first one is network steganography solution\nwhich exploits free/unused protocols' fields and is known for IP, UDP or TCP\nprotocols but has never been applied to RTP (Real-Time Transport Protocol) and\nRTCP (Real-Time Control Protocol) which are characteristic for VoIP. The second\nmethod, called LACK (Lost Audio Packets Steganography), provides hybrid\nstorage-timing covert channel by utilizing delayed audio packets. The results\nof the experiment, that was performed to estimate a total amount of data that\ncan be covertly transferred during typical VoIP conversation phase, regardless\nof steganalysis, are also included in this paper.", 
    "link": "http://arxiv.org/pdf/0805.2938v2", 
    "arxiv-id": "0805.2938v2"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Covert Channels in SIP for VoIP signalling", 
    "publish": "2008-05-22T20:43:38Z", 
    "summary": "In this paper, we evaluate available steganographic techniques for SIP\n(Session Initiation Protocol) that can be used for creating covert channels\nduring signaling phase of VoIP (Voice over IP) call. Apart from characterizing\nexisting steganographic methods we provide new insights by introducing new\ntechniques. We also estimate amount of data that can be transferred in\nsignalling messages for typical IP telephony call.", 
    "link": "http://arxiv.org/pdf/0805.3538v1", 
    "arxiv-id": "0805.3538v1"
},{
    "category": "cs.NI", 
    "author": "Mehul Motani", 
    "title": "Avatar Mobility in Networked Virtual Environments: Measurements,   Analysis, and Implications", 
    "publish": "2008-07-15T09:51:46Z", 
    "summary": "We collected mobility traces of 84,208 avatars spanning 22 regions over two\nmonths in Second Life, a popular networked virtual environment. We analyzed the\ntraces to characterize the dynamics of the avatars mobility and behavior, both\ntemporally and spatially. We discuss the implications of the our findings to\nthe design of peer-to-peer networked virtual environments, interest management,\nmobility modeling of avatars, server load balancing and zone partitioning,\nclient-side caching, and prefetching.", 
    "link": "http://arxiv.org/pdf/0807.2328v1", 
    "arxiv-id": "0807.2328v1"
},{
    "category": "cs.MM", 
    "author": "Eric Engle", 
    "title": "Computer Art in the Former Soviet Bloc", 
    "publish": "2008-09-02T21:48:28Z", 
    "summary": "Documents early computer art in the Soviet bloc and describes Marxist art\ntheory.", 
    "link": "http://arxiv.org/pdf/0809.0524v1", 
    "arxiv-id": "0809.0524v1"
},{
    "category": "cs.MM", 
    "author": "Christian Jutten", 
    "title": "A First Step to Convolutive Sparse Representation", 
    "publish": "2008-09-20T05:52:47Z", 
    "summary": "In this paper an extension of the sparse decomposition problem is considered\nand an algorithm for solving it is presented. In this extension, it is known\nthat one of the shifted versions of a signal s (not necessarily the original\nsignal itself) has a sparse representation on an overcomplete dictionary, and\nwe are looking for the sparsest representation among the representations of all\nthe shifted versions of s. Then, the proposed algorithm finds simultaneously\nthe amount of the required shift, and the sparse representation. Experimental\nresults emphasize on the performance of our algorithm.", 
    "link": "http://arxiv.org/pdf/0809.3485v1", 
    "arxiv-id": "0809.3485v1"
},{
    "category": "cs.MM", 
    "author": "Jean Fran\u00e7ois Nezan", 
    "title": "Optimization of automatically generated multi-core code for the LTE   RACH-PD algorithm", 
    "publish": "2008-11-04T19:36:16Z", 
    "summary": "Embedded real-time applications in communication systems require high\nprocessing power. Manual scheduling devel-oped for single-processor\napplications is not suited to multi-core architectures. The Algorithm\nArchitecture Matching (AAM) methodology optimizes static application\nimplementation on multi-core architectures. The Random Access Channel Preamble\nDetection (RACH-PD) is an algorithm for non-synchronized access of Long Term\nEvolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency\nof the next generation cellular system. This paper de-scribes a complete\nmethodology for implementing the RACH-PD. AAM prototyping is applied to the\nRACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient\nimplemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then\nexplained. Benchmarks for the solution are given.", 
    "link": "http://arxiv.org/pdf/0811.0582v1", 
    "arxiv-id": "0811.0582v1"
},{
    "category": "cs.CR", 
    "author": "Jozef Lubacz", 
    "title": "LACK - a VoIP Steganographic Method", 
    "publish": "2008-11-25T17:25:13Z", 
    "summary": "The paper presents a new steganographic method called LACK (Lost Audio\nPaCKets Steganography) which is intended mainly for VoIP. The method is\npresented in a broader context of network steganography and of VoIP\nsteganography in particular. The analytical results presented in the paper\nconcern the influence of LACK's hidden data insertion procedure on the method's\nimpact on quality of voice transmission and its resistance to steganalysis.", 
    "link": "http://arxiv.org/pdf/0811.4138v1", 
    "arxiv-id": "0811.4138v1"
},{
    "category": "cs.DS", 
    "author": "Jian Pei", 
    "title": "Fast and Quality-Guaranteed Data Streaming in Resource-Constrained   Sensor Networks", 
    "publish": "2008-11-28T20:59:55Z", 
    "summary": "In many emerging applications, data streams are monitored in a network\nenvironment. Due to limited communication bandwidth and other resource\nconstraints, a critical and practical demand is to online compress data streams\ncontinuously with quality guarantee. Although many data compression and digital\nsignal processing methods have been developed to reduce data volume, their\nsuper-linear time and more-than-constant space complexity prevents them from\nbeing applied directly on data streams, particularly over resource-constrained\nsensor networks. In this paper, we tackle the problem of online quality\nguaranteed compression of data streams using fast linear approximation (i.e.,\nusing line segments to approximate a time series). Technically, we address two\nversions of the problem which explore quality guarantees in different forms. We\ndevelop online algorithms with linear time complexity and constant cost in\nspace. Our algorithms are optimal in the sense they generate the minimum number\nof segments that approximate a time series with the required quality guarantee.\nTo meet the resource constraints in sensor networks, we also develop a fast\nalgorithm which creates connecting segments with very simple computation. The\nlow cost nature of our methods leads to a unique edge on the applications of\nmassive and fast streaming environment, low bandwidth networks, and heavily\nconstrained nodes in computational power. We implement and evaluate our methods\nin the application of an acoustic wireless sensor network.", 
    "link": "http://arxiv.org/pdf/0811.4672v1", 
    "arxiv-id": "0811.4672v1"
},{
    "category": "cs.GR", 
    "author": "Fran\u00e7ois Cayre", 
    "title": "The Good, the Bad, and the Ugly: three different approaches to break   their watermarking system", 
    "publish": "2008-11-28T16:31:12Z", 
    "summary": "The Good is Blondie, a wandering gunman with a strong personal sense of\nhonor. The Bad is Angel Eyes, a sadistic hitman who always hits his mark. The\nUgly is Tuco, a Mexican bandit who's always only looking out for himself.\nAgainst the backdrop of the BOWS contest, they search for a watermark in gold\nburied in three images. Each knows only a portion of the gold's exact location,\nso for the moment they're dependent on each other. However, none are\nparticularly inclined to share...", 
    "link": "http://arxiv.org/pdf/0811.4681v1", 
    "arxiv-id": "0811.4681v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Decomposition Principles and Online Learning in Cross-Layer Optimization   for Delay-Sensitive Applications", 
    "publish": "2008-12-05T23:14:41Z", 
    "summary": "In this paper, we propose a general cross-layer optimization framework in\nwhich we explicitly consider both the heterogeneous and dynamically changing\ncharacteristics of delay-sensitive applications and the underlying time-varying\nnetwork conditions. We consider both the independently decodable data units\n(DUs, e.g. packets) and the interdependent DUs whose dependencies are captured\nby a directed acyclic graph (DAG). We first formulate the cross-layer design as\na non-linear constrained optimization problem by assuming complete knowledge of\nthe application characteristics and the underlying network conditions. The\nconstrained cross-layer optimization is decomposed into several cross-layer\noptimization subproblems for each DU and two master problems. The proposed\ndecomposition method determines the necessary message exchanges between layers\nfor achieving the optimal cross-layer solution. However, the attributes (e.g.\ndistortion impact, delay deadline etc) of future DUs as well as the network\nconditions are often unknown in the considered real-time applications. The\nimpact of current cross-layer actions on the future DUs can be characterized by\na state-value function in the Markov decision process (MDP) framework. Based on\nthe dynamic programming solution to the MDP, we develop a low-complexity\ncross-layer optimization algorithm using online learning for each DU\ntransmission. This online algorithm can be implemented in real-time in order to\ncope with unknown source characteristics, network dynamics and resource\nconstraints. Our numerical results demonstrate the efficiency of the proposed\nonline algorithm.", 
    "link": "http://arxiv.org/pdf/0812.1244v1", 
    "arxiv-id": "0812.1244v1"
},{
    "category": "cs.MM", 
    "author": "Christian Jutten", 
    "title": "A New Trend in Optimization on Multi Overcomplete Dictionary toward   Inpainting", 
    "publish": "2008-12-12T15:56:42Z", 
    "summary": "Recently, great attention was intended toward overcomplete dictionaries and\nthe sparse representations they can provide. In a wide variety of signal\nprocessing problems, sparsity serves a crucial property leading to high\nperformance. Inpainting, the process of reconstructing lost or deteriorated\nparts of images or videos, is an interesting application which can be handled\nby suitably decomposition of an image through combination of overcomplete\ndictionaries. This paper addresses a novel technique of such a decomposition\nand investigate that through inpainting of images. Simulations are presented to\ndemonstrate the validation of our approach.", 
    "link": "http://arxiv.org/pdf/0812.2405v1", 
    "arxiv-id": "0812.2405v1"
},{
    "category": "cs.MM", 
    "author": "SeyedMajid Valiollahzadeh", 
    "title": "Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel   Recognition in Continues Speech", 
    "publish": "2008-12-12T16:08:04Z", 
    "summary": "In this paper, we discuss the issues in automatic recognition of vowels in\nPersian language. The present work focuses on new statistical method of\nrecognition of vowels as a basic unit of syllables. First we describe a vowel\ndetection system then briefly discuss how the detected vowels can feed to\nrecognition unit. According to pattern recognition, Support Vector Machines\n(SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a\ngenerative model classifier are two most popular techniques. Current\nstate-ofthe- art systems try to combine them together for achieving more power\nof classification and improving the performance of the recognition systems. The\nmain idea of the study is to combine probabilistic SVM and traditional GMM\npattern classification with some characteristic of speech like band-pass energy\nto achieve better classification rate. This idea has been analytically\nformulated and tested on a FarsDat based vowel recognition system. The results\nshow inconceivable increases in recognition accuracy. The tests have been\ncarried out by various proposed vowel recognition algorithms and the results\nhave been compared.", 
    "link": "http://arxiv.org/pdf/0812.2411v1", 
    "arxiv-id": "0812.2411v1"
},{
    "category": "cs.CV", 
    "author": "Alireza Avanaki", 
    "title": "Over-enhancement Reduction in Local Histogram Equalization using its   Degrees of Freedom", 
    "publish": "2009-02-02T10:41:53Z", 
    "summary": "A well-known issue of local (adaptive) histogram equalization (LHE) is\nover-enhancement (i.e., generation of spurious details) in homogenous areas of\nthe image. In this paper, we show that the LHE problem has many solutions due\nto the ambiguity in ranking pixels with the same intensity. The LHE solution\nspace can be searched for the images having the maximum PSNR or structural\nsimilarity (SSIM) with the input image. As compared to the results of the prior\nart, these solutions are more similar to the input image while offering the\nsame local contrast.\n  Index Terms: histogram modification or specification, contrast enhancement", 
    "link": "http://arxiv.org/pdf/0902.0221v2", 
    "arxiv-id": "0902.0221v2"
},{
    "category": "cs.PF", 
    "author": "Stefano Salsano", 
    "title": "Fundamental delay bounds in peer-to-peer chunk-based real-time streaming   systems", 
    "publish": "2009-02-09T10:05:48Z", 
    "summary": "This paper addresses the following foundational question: what is the maximum\ntheoretical delay performance achievable by an overlay peer-to-peer streaming\nsystem where the streamed content is subdivided into chunks? As shown in this\npaper, when posed for chunk-based systems, and as a consequence of the\nstore-and-forward way in which chunks are delivered across the network, this\nquestion has a fundamentally different answer with respect to the case of\nsystems where the streamed content is distributed through one or more flows\n(sub-streams). To circumvent the complexity emerging when directly dealing with\ndelay, we express performance in term of a convenient metric, called \"stream\ndiffusion metric\". We show that it is directly related to the end-to-end\nminimum delay achievable in a P2P streaming network. In a homogeneous scenario,\nwe derive a performance bound for such metric, and we show how this bound\nrelates to two fundamental parameters: the upload bandwidth available at each\nnode, and the number of neighbors a node may deliver chunks to. In this bound,\nk-step Fibonacci sequences do emerge, and appear to set the fundamental laws\nthat characterize the optimal operation of chunk-based systems.", 
    "link": "http://arxiv.org/pdf/0902.1394v2", 
    "arxiv-id": "0902.1394v2"
},{
    "category": "cs.MM", 
    "author": "Yujun Li", 
    "title": "Gradient-based adaptive interpolation in super-resolution image   restoration", 
    "publish": "2009-03-24T01:33:15Z", 
    "summary": "This paper presents a super-resolution method based on gradient-based\nadaptive interpolation. In this method, in addition to considering the distance\nbetween the interpolated pixel and the neighboring valid pixel, the\ninterpolation coefficients take the local gradient of the original image into\naccount. The smaller the local gradient of a pixel is, the more influence it\nshould have on the interpolated pixel. And the interpolated high resolution\nimage is finally deblurred by the application of wiener filter. Experimental\nresults show that our proposed method not only substantially improves the\nsubjective and objective quality of restored images, especially enhances edges,\nbut also is robust to the registration error and has low computational\ncomplexity.", 
    "link": "http://arxiv.org/pdf/0903.3995v1", 
    "arxiv-id": "0903.3995v1"
},{
    "category": "cs.NI", 
    "author": "Vincent Roca", 
    "title": "On-the-fly erasure coding for real-time video applications", 
    "publish": "2009-04-27T16:09:33Z", 
    "summary": "This paper introduces a robust point-to-point transmission scheme: Tetrys,\nthat relies on a novel on-the-fly erasure coding concept which reduces the\ndelay for recovering lost data at the receiver side. In current erasure coding\nschemes, the packets that are not rebuilt at the receiver side are either lost\nor delayed by at least one RTT before transmission to the application. The\npresent contribution aims at demonstrating that Tetrys coding scheme can fill\nthe gap between real-time applications requirements and full reliability.\nIndeed, we show that in several cases, Tetrys can recover lost packets below\none RTT over lossy and best-effort networks. We also show that Tetrys allows to\nenable full reliability without delay compromise and as a result: significantly\nimproves the performance of time constrained applications. For instance, our\nevaluations present that video-conferencing applications obtain a PSNR gain up\nto 7dB compared to classic block-based erasure codes.", 
    "link": "http://arxiv.org/pdf/0904.4202v4", 
    "arxiv-id": "0904.4202v4"
},{
    "category": "cs.CV", 
    "author": "Hanzi Wang", 
    "title": "Generalized Kernel-based Visual Tracking", 
    "publish": "2009-05-15T03:26:52Z", 
    "summary": "In this work we generalize the plain MS trackers and attempt to overcome\nstandard mean shift trackers' two limitations.\n  It is well known that modeling and maintaining a representation of a target\nobject is an important component of a successful visual tracker.\n  However, little work has been done on building a robust template model for\nkernel-based MS tracking. In contrast to building a template from a single\nframe, we train a robust object representation model from a large amount of\ndata. Tracking is viewed as a binary classification problem, and a\ndiscriminative classification rule is learned to distinguish between the object\nand background. We adopt a support vector machine (SVM) for training. The\ntracker is then implemented by maximizing the classification score. An\niterative optimization scheme very similar to MS is derived for this purpose.", 
    "link": "http://arxiv.org/pdf/0905.2463v2", 
    "arxiv-id": "0905.2463v2"
},{
    "category": "cs.MM", 
    "author": "Fausto Rabitti", 
    "title": "CoPhIR: a Test Collection for Content-Based Image Retrieval", 
    "publish": "2009-05-28T12:14:07Z", 
    "summary": "The scalability, as well as the effectiveness, of the different Content-based\nImage Retrieval (CBIR) approaches proposed in literature, is today an important\nresearch issue. Given the wealth of images on the Web, CBIR systems must in\nfact leap towards Web-scale datasets. In this paper, we report on our\nexperience in building a test collection of 100 million images, with the\ncorresponding descriptive features, to be used in experimenting new scalable\ntechniques for similarity searching, and comparing their results. In the\ncontext of the SAPIR (Search on Audio-visual content using Peer-to-peer\nInformation Retrieval) European project, we had to experiment our distributed\nsimilarity searching technology on a realistic data set. Therefore, since no\nlarge-scale collection was available for research purposes, we had to tackle\nthe non-trivial process of image crawling and descriptive feature extraction\n(we used five MPEG-7 features) using the European EGEE computer GRID. The\nresult of this effort is CoPhIR, the first CBIR test collection of such scale.\nCoPhIR is now open to the research community for experiments and comparisons,\nand access to the collection was already granted to more than 50 research\ngroups worldwide.", 
    "link": "http://arxiv.org/pdf/0905.4627v2", 
    "arxiv-id": "0905.4627v2"
},{
    "category": "cs.MM", 
    "author": "Harald Kosch", 
    "title": "Quality assessment of the MPEG-4 scalable video CODEC", 
    "publish": "2009-06-03T09:31:34Z", 
    "summary": "In this paper, the performance of the emerging MPEG-4 SVC CODEC is evaluated.\nIn the first part, a brief introduction on the subject of quality assessment\nand the development of the MPEG-4 SVC CODEC is given. After that, the used test\nmethodologies are described in detail, followed by an explanation of the actual\ntest scenarios. The main part of this work concentrates on the performance\nanalysis of the MPEG-4 SVC CODEC - both objective and subjective.", 
    "link": "http://arxiv.org/pdf/0906.0667v1", 
    "arxiv-id": "0906.0667v1"
},{
    "category": "cs.MM", 
    "author": "Bruno Sadeg", 
    "title": "A New Approach to Manage QoS in Distributed Multimedia Systems", 
    "publish": "2009-06-26T13:40:38Z", 
    "summary": "Dealing with network congestion is a criterion used to enhance quality of\nservice (QoS) in distributed multimedia systems. The existing solutions for the\nproblem of network congestion ignore scalability considerations because they\nmaintain a separate classification for each video stream. In this paper, we\npropose a new method allowing to control QoS provided to clients according to\nthe network congestion, by discarding some frames when needed. The technique\nproposed, called (m,k)-frame, is scalable with little degradation in\napplication performances. (m,k)-frame method is issued from the notion of\n(m,k)-firm realtime constraints which means that among k invocations of a task,\nm invocations must meet their deadline. Our simulation studies show the\nusefulness of (m,k)-frame method to adapt the QoS to the real conditions in a\nmultimedia application, according to the current system load. Notably, the\nsystem must adjust the QoS provided to active clients1 when their number\nvaries, i.e. dynamic arrival of clients.", 
    "link": "http://arxiv.org/pdf/0906.4936v1", 
    "arxiv-id": "0906.4936v1"
},{
    "category": "cs.LG", 
    "author": "Mihaela van der Schaar", 
    "title": "Online Reinforcement Learning for Dynamic Multimedia Systems", 
    "publish": "2009-06-29T17:48:40Z", 
    "summary": "In our previous work, we proposed a systematic cross-layer framework for\ndynamic multimedia systems, which allows each layer to make autonomous and\nforesighted decisions that maximize the system's long-term performance, while\nmeeting the application's real-time delay constraints. The proposed solution\nsolved the cross-layer optimization offline, under the assumption that the\nmultimedia system's probabilistic dynamics were known a priori. In practice,\nhowever, these dynamics are unknown a priori and therefore must be learned\nonline. In this paper, we address this problem by allowing the multimedia\nsystem layers to learn, through repeated interactions with each other, to\nautonomously optimize the system's long-term performance at run-time. We\npropose two reinforcement learning algorithms for optimizing the system under\ndifferent design constraints: the first algorithm solves the cross-layer\noptimization in a centralized manner, and the second solves it in a\ndecentralized manner. We analyze both algorithms in terms of their required\ncomputation, memory, and inter-layer communication overheads. After noting that\nthe proposed reinforcement learning algorithms learn too slowly, we introduce a\ncomplementary accelerated learning algorithm that exploits partial knowledge\nabout the system's dynamics in order to dramatically improve the system's\nperformance. In our experiments, we demonstrate that decentralized learning can\nperform as well as centralized learning, while enabling the layers to act\nautonomously. Additionally, we show that existing application-independent\nreinforcement learning algorithms, and existing myopic learning algorithms\ndeployed in multimedia systems, perform significantly worse than our proposed\napplication-aware and foresighted learning methods.", 
    "link": "http://arxiv.org/pdf/0906.5325v1", 
    "arxiv-id": "0906.5325v1"
},{
    "category": "cs.NI", 
    "author": "Sateesh Addepalli", 
    "title": "Interworking Scheme Using Optimized SIP Mobility for MultiHomed Mobile   Nodes in Wireless Heterogeneous Networks", 
    "publish": "2009-08-05T13:55:36Z", 
    "summary": "Nowadays, mobile users wish to use their multi-interface mobile devices to\naccess the Internet through network points of attachment (PoA) based on\nheterogeneous wireless technologies. They also wish to seamlessly change the\nPoAs during their ongoing sessions to improve service quality and/or reduce\nmonetary cost. If appropriately handled, multihomed mobile nodes offer a\npotential solution to this issue. In this sense, the management of multihomed\nmobile nodes in heterogeneous environment is a key research topic. In this\npaper, we present an improvement of SIP mobility (pre-call plus mid-call\nmobility) to support seamless mobility of multihomed mobile nodes in\nheterogeneous wireless networks. Pre-call mobility is extended to associate\nuser identifier (i.e. SIP URI) and interface identifiers (i.e. IP addresses).\nThe multiple addresses of a mobile device are weighted by the user to create a\npriority list in the SIP server so as to guarantee resilient reachability of\nmobile nodes and to avoid unnecessary signaling through wireless links, thus\nsaving radio resources. Then, three variations of mid-call mobility, called\nhard, hybrid and soft procedures, are also proposed. Their main aim is to\nminimize, or even avoid, packet losses during interface switching at the mobile\nnode. The proposed solutions have been implemented in a wireless heterogeneous\ntestbed composed of 802.11 WLAN plus 3.5 cellular network, which are fully\ncontrolled and configurable. The testbed has been used to study the performance\nand the robustness of the three proposed mid-call mobility procedures.", 
    "link": "http://arxiv.org/pdf/0908.0667v1", 
    "arxiv-id": "0908.0667v1"
},{
    "category": "cs.MM", 
    "author": "Bela Genge", 
    "title": "Component based platform for multimedia applications", 
    "publish": "2009-08-21T09:46:07Z", 
    "summary": "We propose a platform for distributed multimedia applications which\nsimplifies the development process and at the same time ensures application\nportability, flexibility and performance. The platform is implemented using the\nNetscape Portable Runtime (NSPR) and the Cross-Platform Component Object Model\n(XPCOM).", 
    "link": "http://arxiv.org/pdf/0908.3082v1", 
    "arxiv-id": "0908.3082v1"
},{
    "category": "cs.CR", 
    "author": "Manesh Kokare", 
    "title": "Optimization of Bit Plane Combination for Efficient Digital Image   Watermarking", 
    "publish": "2009-08-27T18:52:26Z", 
    "summary": "In view of the frequent multimedia data transfer authentication and\nprotection of images has gained importance in todays world. In this paper we\npropose a new watermarking technique, based on bit plane, which enhances\nrobustness and capacity of the watermark, as well as maintains transparency of\nthe watermark and fidelity of the image. In the proposed technique, higher\nstrength bit plane of digital signature watermark is embedded in to a\nsignificant bit plane of the original image. The combination of bit planes\n(image and watermark) selection is an important issue. Therefore, a mechanism\nis developed for appropriate bit plane selection. Ten different attacks are\nselected to test different alternatives. These attacks are given different\nweightings as appropriate to user requirement. A weighted correlation\ncoefficient for retrieved watermark is estimated for each of the alternatives.\nBased on these estimated values optimal bit plane combination is identified for\na given user requirement. The proposed method is found to be useful for\nauthentication and to prove legal ownership. We observed better results by our\nproposed method in comparison with the previously reported work on pseudorandom\nwatermark embedded in least significant bit (LSB) plane.", 
    "link": "http://arxiv.org/pdf/0908.4062v1", 
    "arxiv-id": "0908.4062v1"
},{
    "category": "cs.IR", 
    "author": "Namita Srivastava", 
    "title": "Retrieval of Remote Sensing Images Using Colour and Texture Attribute", 
    "publish": "2009-08-27T19:21:11Z", 
    "summary": "Grouping images into semantically meaningful categories using low-level\nvisual feature is a challenging and important problem in content-based image\nretrieval. The groupings can be used to build effective indices for an image\ndatabase. Digital image analysis techniques are being used widely in remote\nsensing assuming that each terrain surface category is characterized with\nspectral signature observed by remote sensors. Even with the remote sensing\nimages of IRS data, integration of spatial information is expected to assist\nand to improve the image analysis of remote sensing data. In this paper we\npresent a satellite image retrieval based on a mixture of old fashioned ideas\nand state of the art learning tools. We have developed a methodology to\nclassify remote sensing images using HSV color features and Haar wavelet\ntexture features and then grouping them on the basis of particular threshold\nvalue. The experimental results indicate that the use of color and texture\nfeature extraction is very useful for image retrieval.", 
    "link": "http://arxiv.org/pdf/0908.4074v1", 
    "arxiv-id": "0908.4074v1"
},{
    "category": "cs.MM", 
    "author": "S. Sujatha", 
    "title": "Dynamic Multimedia Content Retrieval System in Distributed Environment", 
    "publish": "2009-09-01T08:19:18Z", 
    "summary": "WiCoM enables remote management of web resources. Our application Mobile\nreporter is aimed at Journalist, who will be able to capture the events in\nreal-time using their mobile phones and update their web server on the latest\nevent. WiCoM has been developed using J2ME technology on the client side and\nPHP on the server side. The communication between the client and the server is\nestablished through GPRS. Mobile reporter will be able to upload, edit and\nremove both textual as well as multimedia contents in the server.", 
    "link": "http://arxiv.org/pdf/0909.0118v1", 
    "arxiv-id": "0909.0118v1"
},{
    "category": "cs.MM", 
    "author": "Raman Kumar", 
    "title": "Robustness of the Digital Image Watermarking Techniques against   Brightness and Rotation Attack", 
    "publish": "2009-09-19T03:06:57Z", 
    "summary": "The recent advent in the field of multimedia proposed a many facilities in\ntransport, transmission and manipulation of data. Along with this advancement\nof facilities there are larger threats in authentication of data, its licensed\nuse and protection against illegal use of data. A lot of digital image\nwatermarking techniques have been designed and implemented to stop the illegal\nuse of the digital multimedia images. This paper compares the robustness of\nthree different watermarking schemes against brightness and rotation attacks.\nThe robustness of the watermarked images has been verified on the parameters of\nPSNR (Peak Signal to Noise Ratio), RMSE (Root Mean Square Error) and MAE (Mean\nAbsolute Error).", 
    "link": "http://arxiv.org/pdf/0909.3554v1", 
    "arxiv-id": "0909.3554v1"
},{
    "category": "cs.MM", 
    "author": "Y. Elawdy", 
    "title": "Analysis, Design and Simulation of a New System for Internet Multimedia   Transmission Guarantee", 
    "publish": "2009-10-01T14:03:51Z", 
    "summary": "QoS is a very important issue for multimedia communication systems. In this\npaper, a new system that reinstalls the relation between the QoS elements\n(RSVP, routing protocol, sender, and receiver) during the multimedia\ntransmission is proposed, then an alternative path is created in case of\noriginal multimedia path failure. The suggested system considers the resulting\nproblems that may be faced within and after the creation of rerouting path.\nFinally, the proposed system is simulated using OPNET 11.5 simulation package.\nSimulation results show that our proposed system outperforms the old one in\nterms of QoS parameters like packet loss and delay jitter.", 
    "link": "http://arxiv.org/pdf/0910.0179v1", 
    "arxiv-id": "0910.0179v1"
},{
    "category": "cs.MM", 
    "author": "T R GopalaKrishnan Nair", 
    "title": "Prefix based Chaining Scheme for Streaming Popular Videos using Proxy   servers in VoD", 
    "publish": "2009-10-08T11:26:01Z", 
    "summary": "Streaming high quality videos consumes significantly large amount of network\nresources. In this context request to service delay, network traffic,\ncongestion and server overloading are the main parameters to be considered in\nvideo streaming over the internet that effect the quality of service (QoS). In\nthis paper, we propose an efficient architecture as a cluster of proxy servers\nand clients that uses a peer to peer (P2P) approach to cooperatively stream the\nvideo using chaining technique. We consider the following two key issues in the\nproposed architecture (1) Prefix caching technique to accommodate more number\nof videos close to client (2) Cooperative client and proxy chaining to achieve\nthe network efficiency. Our simulation results shows that the proposed approach\nyields a prefix caching close to the optimal solution minimizing WAN bandwidth\nusage on server-proxy path by utilizing the proxy-client and client-client path\nbandwidth, which is much cheaper than the expensive server proxy path\nbandwidth, server load, and client rejection ratio significantly using\nchaining.", 
    "link": "http://arxiv.org/pdf/0910.1471v1", 
    "arxiv-id": "0910.1471v1"
},{
    "category": "cs.NI", 
    "author": "Mihaela van der Schaar", 
    "title": "Media-TCP: A Quality-Centric TCP-Friendly Congestion Control for   Multimedia Transmission", 
    "publish": "2009-10-21T21:53:25Z", 
    "summary": "In this paper, we propose a quality-centric congestion control for multimedia\nstreaming over IP networks, which we refer to as media-TCP. Unlike existing\ncongestion control schemes that adapt a user's sending rate merely to the\nnetwork condition, our solution adapts the sending rate to both the network\ncondition and the application characteristics by explicitly considering the\ndistortion impacts, delay deadlines, and interdependencies of different video\npacket classes. Hence, our media-aware solution is able to provide differential\nservices for transmitting various packet classes and thereby, further improves\nthe multimedia streaming quality. We model this problem using a Finite-Horizon\nMarkov Decision Process (FHMDP) and determine the optimal congestion control\npolicy that maximizes the long-term multimedia quality, while adhering to the\nhorizon- TCP-friendliness constraint, which ensures long-term fairness with\nexisting TCP applications. We show that the FHMDP problem can be decomposed\ninto multiple optimal stopping problems, which admit a low-complexity\nthreshold-based solution. Moreover, unlike existing congestion control\napproaches, which focus on maintaining throughput-based fairness among users,\nthe proposed media-TCP aims to achieve quality-based fairness among multimedia\nusers. We also derive sufficient conditions for multiple multimedia users to\nachieve quality-based fairness using media-TCP congestion control. Our\nsimulation results show that the proposed media-TCP achieves more than 3dB\nimprovement in terms of PSNR over the conventional TCP congestion control\napproaches, with the largest improvements observed for real-time streaming\napplications requiring stringent playback delays.", 
    "link": "http://arxiv.org/pdf/0910.4186v1", 
    "arxiv-id": "0910.4186v1"
},{
    "category": "cs.MM", 
    "author": "K. Ganesan", 
    "title": "A Reliable Replication Strategy for VoD System using Markov Chain", 
    "publish": "2009-12-05T13:03:17Z", 
    "summary": "In this paper we have investigated on the reliability of streams for a VoD\nsystem. The objective of the paper is to maximize the availability of streams\nfor the peers in the VoD system. We have achieved this by using data\nreplication technique in the peers. Hence, we proposed a new data replication\ntechnique to optimally store the videos in the peers. The new data replication\ntechnique generates more number of replicas than the existing techniques such\nas random, minimum request and maximize hit. We have also investigated by\napplying the CTMC model for the reliability of replications during the peer\nfailures. Our result shows that the mean lifetime of replicas are more under\nvarious circumstances. We have addressed the practical issues of efficient\nutilization of overall bandwidth and buffer in the VoD system. We achieved\ngreater success playback probability of videos than the existing techniques.", 
    "link": "http://arxiv.org/pdf/0912.1011v1", 
    "arxiv-id": "0912.1011v1"
},{
    "category": "cs.CR", 
    "author": "Kwok-Tung Lo", 
    "title": "Breaking a modified substitution-diffusion image cipher based on chaotic   standard and logistic maps", 
    "publish": "2009-12-16T03:52:35Z", 
    "summary": "Recently, an image encryption scheme based on chaotic standard and logistic\nmaps was proposed by Patidar et al. It was later reported by Rhouma et al. that\nan equivalent secret key can be reconstructed with only one\nknown/chosen-plaintext and the corresponding ciphertext. Patidar et al. soon\nmodified the original scheme and claimed that the modified scheme is secure\nagainst Rhouma et al.'s attack. In this paper, we point out that the modified\nscheme is still insecure against the same known/chosen-plaintext attack. In\naddition, some other security defects existing in both the original and the\nmodified schemes are also reported.", 
    "link": "http://arxiv.org/pdf/0912.3050v2", 
    "arxiv-id": "0912.3050v2"
},{
    "category": "cs.CL", 
    "author": "Abdelkader Benyettou", 
    "title": "Speech Recognition Oriented Vowel Classification Using Temporal Radial   Basis Functions", 
    "publish": "2009-12-19T18:29:43Z", 
    "summary": "The recent resurgence of interest in spatio-temporal neural network as speech\nrecognition tool motivates the present investigation. In this paper an approach\nwas developed based on temporal radial basis function \"TRBF\" looking to many\nadvantages: few parameters, speed convergence and time invariance. This\napplication aims to identify vowels taken from natural speech samples from the\nTimit corpus of American speech. We report a recognition accuracy of 98.06\npercent in training and 90.13 in test on a subset of 6 vowel phonemes, with the\npossibility to expend the vowel sets in future.", 
    "link": "http://arxiv.org/pdf/0912.3917v1", 
    "arxiv-id": "0912.3917v1"
},{
    "category": "cs.MM", 
    "author": "Alain Bonardi", 
    "title": "Music-ripping: des pratiques qui provoquent la musicologie", 
    "publish": "2009-12-24T15:28:53Z", 
    "summary": "Out of the scope of the usual positions of computing in the field of music\nand musicology, one notices the emergence of human-computer systems that do\nexist by breaking off. Though these singular systems take effect in the usual\nfields of expansion of music, they do not make any systematic reference to\nknown musicological categories. On the contrary, they make possible experiments\nthat open uses where listening, composition and musical transmission get merged\nin a gesture sometimes named as ?music-ripping?. We will show in which way the\nmusic-ripping practices provoke traditional musicology, whose canonical\ncategories happen to be ineffectual to explain here. To achieve that purpose,\nwe shall need: - to make explicit a minimal set of categories that is\nsufficient to underlie the usual models of computer assisted music;- to do the\nsame for human-computer systems (anti-musicological?) that disturb us; - to\nexamine the possibility conditions of reduction of the second set to the first;\n- to conclude on the nature of music-ripping.", 
    "link": "http://arxiv.org/pdf/0912.4881v1", 
    "arxiv-id": "0912.4881v1"
},{
    "category": "cs.IR", 
    "author": "Balakrishnan Ramadoss", 
    "title": "Tutoring System for Dance Learning", 
    "publish": "2010-01-04T05:24:31Z", 
    "summary": "Recent advances in hardware sophistication related to graphics display, audio\nand video devices made available a large number of multimedia and hypermedia\napplications. These multimedia applications need to store and retrieve the\ndifferent forms of media like text, hypertext, graphics, still images,\nanimations, audio and video. Dance is one of the important cultural forms of a\nnation and dance video is one such multimedia types. Archiving and retrieving\nthe required semantics from these dance media collections is a crucial and\ndemanding multimedia application. This paper summarizes the difference dance\nvideo archival techniques and systems. Keywords: Multimedia, Culture Media,\nMetadata archival and retrieval systems, MPEG-7, XML.", 
    "link": "http://arxiv.org/pdf/1001.0440v1", 
    "arxiv-id": "1001.0440v1"
},{
    "category": "cs.MM", 
    "author": "Balakrishnan Ramadoss", 
    "title": "Semantic Modeling and Retrieval of Dance Video Annotations", 
    "publish": "2010-01-04T05:32:05Z", 
    "summary": "Dance video is one of the important types of narrative videos with semantic\nrich content. This paper proposes a new meta model, Dance Video Content Model\n(DVCM) to represent the expressive semantics of the dance videos at multiple\ngranularity levels. The DVCM is designed based on the concepts such as video,\nshot, segment, event and object, which are the components of MPEG-7 MDS. This\npaper introduces a new relationship type called Temporal Semantic Relationship\nto infer the semantic relationships between the dance video objects. Inverted\nfile based index is created to reduce the search time of the dance queries. The\neffectiveness of containment queries using precision and recall is depicted.\nKeywords: Dance Video Annotations, Effectiveness Metrics, Metamodeling,\nTemporal Semantic Relationships.", 
    "link": "http://arxiv.org/pdf/1001.0441v1", 
    "arxiv-id": "1001.0441v1"
},{
    "category": "cs.MM", 
    "author": "Bernd Girod", 
    "title": "Distributed Rate Allocation Policies for Multi-Homed Video Streaming   over Heterogeneous Access Networks", 
    "publish": "2010-01-07T01:11:44Z", 
    "summary": "We consider the problem of rate allocation among multiple simultaneous video\nstreams sharing multiple heterogeneous access networks. We develop and evaluate\nan analytical framework for optimal rate allocation based on observed available\nbit rate (ABR) and round-trip time (RTT) over each access network and video\ndistortion-rate (DR) characteristics. The rate allocation is formulated as a\nconvex optimization problem that minimizes the total expected distortion of all\nvideo streams. We present a distributed approximation of its solution and\ncompare its performance against H-infinity optimal control and two heuristic\nschemes based on TCP-style additive-increase-multiplicative decrease (AIMD)\nprinciples. The various rate allocation schemes are evaluated in simulations of\nmultiple high-definition (HD) video streams sharing multiple access networks.\nOur results demonstrate that, in comparison with heuristic AIMD-based schemes,\nboth media-aware allocation and H-infinity optimal control benefit from\nproactive congestion avoidance and reduce the average packet loss rate from 45%\nto below 2%. Improvement in average received video quality ranges between 1.5\nto 10.7 dB in PSNR for various background traffic loads and video playout\ndeadlines. Media-aware allocation further exploits its knowledge of the video\nDR characteristics to achieve a more balanced video quality among all streams.", 
    "link": "http://arxiv.org/pdf/1001.1013v1", 
    "arxiv-id": "1001.1013v1"
},{
    "category": "cs.MM", 
    "author": "Hamed Rasouli", 
    "title": "Designing a Truly Integrated (Onsite and Online) Conference: Concept,   Processes, Solutions", 
    "publish": "2010-01-12T05:06:16Z", 
    "summary": "Web conferencing tools have entered the mainstream of business applications.\nUsing web conferencing for IEEE conferences has a good potential of adding\nvalue to both organizers and participants. Authors propose a concept of Truly\nIntegrated Conference (TIC) according to which a multi-point\nworldwide-distributed network of conference online authors/participants will\nenhance the standard (centralized) IEEE conference model, which requires\nattendance of the participants in person at the main conference location. The\nconcept entails seamless integration of the onsite and online conference\nsystems, including data/presentation, video, audio channels. Benefits and\nchallenges of the TIC concept are analyzed. Requirements to the web\nconferencing system capable of supporting the TIC conference are presented and\nreviewed against commercial web conferencing tools. Case study of the IEEE\nToronto International Conference ? Science and Technology for Humanity, which\nwas the first realization of TIC, is presented which analyzes various aspects\n(organizational, technological, and financial) of the integrated conference.", 
    "link": "http://arxiv.org/pdf/1001.1794v1", 
    "arxiv-id": "1001.1794v1"
},{
    "category": "cs.MM", 
    "author": "Asu Ozdaglar", 
    "title": "Avoiding Interruptions - QoE Trade-offs in Block-coded Streaming Media   Applications", 
    "publish": "2010-01-12T16:13:50Z", 
    "summary": "We take an analytical approach to study Quality of user Experience (QoE) for\nvideo streaming applications. First, we show that random linear network coding\napplied to blocks of video frames can significantly simplify the packet\nrequests at the network layer and save resources by avoiding duplicate packet\nreception. Network coding allows us to model the receiver's buffer as a queue\nwith Poisson arrivals and deterministic departures. We consider the probability\nof interruption in video playback as well as the number of initially buffered\npackets (initial waiting time) as the QoE metrics. We characterize the optimal\ntrade-off between these metrics by providing upper and lower bounds on the\nminimum initial buffer size, required to achieve certain level of interruption\nprobability for different regimes of the system parameters. Our bounds are\nasymptotically tight as the file size goes to infinity.", 
    "link": "http://arxiv.org/pdf/1001.1937v2", 
    "arxiv-id": "1001.1937v2"
},{
    "category": "cs.MM", 
    "author": "Geeta Sikka", 
    "title": "A New Image Steganography Based On First Component Alteration Technique", 
    "publish": "2010-01-12T18:30:20Z", 
    "summary": "In this paper, A new image steganography scheme is proposed which is a kind\nof spatial domain technique. In order to hide secret data in cover-image, the\nfirst component alteration technique is used. Techniques used so far focuses\nonly on the two or four bits of a pixel in a image (at the most five bits at\nthe edge of an image) which results in less peak to signal noise ratio and high\nroot mean square error. In this technique, 8 bits of blue components of pixels\nare replaced with secret data bits. Proposed scheme can embed more data than\nprevious schemes and shows better image quality. To prove this scheme, several\nexperiments are performed, and are compared the experimental results with the\nrelated previous works.", 
    "link": "http://arxiv.org/pdf/1001.1972v1", 
    "arxiv-id": "1001.1972v1"
},{
    "category": "cs.MM", 
    "author": "M. Shahid Khalil", 
    "title": "Evaluating Effectiveness of Tamper Proofing on Dynamic Graph Software   Watermarks", 
    "publish": "2010-01-12T18:34:24Z", 
    "summary": "For enhancing the protection level of dynamic graph software watermarks and\nfor the purpose of conducting the analysis which evaluates the effect of\nintegrating two software protection techniques such as software watermarking\nand tamper proofing, constant encoding technique along with the enhancement\nthrough the idea of constant splitting is proposed. In this paper Thomborson\ntechnique has been implemented with the scheme of breaking constants which\nenables to encode all constants without having any consideration about their\nvalues with respect to the value of watermark tree. Experimental analysis which\nhave been conducted and provided in this paper concludes that the constant\nencoding process significantly increases the code size, heap space usage, and\nexecution time, while making the tamper proofed code resilient to variety of\nsemantic preserving program transformation attacks.", 
    "link": "http://arxiv.org/pdf/1001.1974v1", 
    "arxiv-id": "1001.1974v1"
},{
    "category": "cs.NI", 
    "author": "Ahmad M. Manasrah", 
    "title": "Comparative Evaluation and Analysis of IAX and RSW", 
    "publish": "2010-01-13T19:29:02Z", 
    "summary": "Voice over IP (VoIP) is a technology to transport media over IP networks such\nas the Internet. VoIP has the capability of connecting people over packet\nswitched networks instead of traditional circuit switched networks. Recently,\nthe InterAsterisk Exchange Protocol (IAX) has emerged as a new VoIP which is\ngaining popularity among VoIP products. IAX is known for its simplicity, NAT\nfriendliness, efficiency, and robustness. More recently, the Real time\nSwitching (RSW) control criterion has emerged as a multimedia conferencing\nprotocol. In this paper, we made a comparative evaluation and analysis of IAX\nand RSW using Mean Opinion Score rating (MOS) and found that they both perform\nwell under different network packet delays in ms.", 
    "link": "http://arxiv.org/pdf/1001.2280v1", 
    "arxiv-id": "1001.2280v1"
},{
    "category": "cs.MM", 
    "author": "C. -C. Jay Kuo", 
    "title": "An Improved DC Recovery Method from AC Coefficients of DCT-Transformed   Images", 
    "publish": "2010-02-08T22:05:04Z", 
    "summary": "Motivated by the work of Uehara et al. [1], an improved method to recover DC\ncoefficients from AC coefficients of DCT-transformed images is investigated in\nthis work, which finds applications in cryptanalysis of selective multimedia\nencryption. The proposed under/over-flow rate minimization (FRM) method employs\nan optimization process to get a statistically more accurate estimation of\nunknown DC coefficients, thus achieving a better recovery performance. It was\nshown by experimental results based on 200 test images that the proposed DC\nrecovery method significantly improves the quality of most recovered images in\nterms of the PSNR values and several state-of-the-art objective image quality\nassessment (IQA) metrics such as SSIM and MS-SSIM.", 
    "link": "http://arxiv.org/pdf/1002.1727v3", 
    "arxiv-id": "1002.1727v3"
},{
    "category": "cs.MM", 
    "author": "Prof. Kore S. N", 
    "title": "Image Retrieval Techniques based on Image Features, A State of Art   approach for CBIR", 
    "publish": "2010-02-09T19:43:44Z", 
    "summary": "The purpose of this Paper is to describe our research on different feature\nextraction and matching techniques in designing a Content Based Image Retrieval\n(CBIR) system. Due to the enormous increase in image database sizes, as well as\nits vast deployment in various applications, the need for CBIR development\narose. Firstly, this paper outlines a description of the primitive feature\nextraction techniques like, texture, colour, and shape. Once these features are\nextracted and used as the basis for a similarity check between images, the\nvarious matching techniques are discussed. Furthermore, the results of its\nperformance are illustrated by a detailed example.", 
    "link": "http://arxiv.org/pdf/1002.1951v1", 
    "arxiv-id": "1002.1951v1"
},{
    "category": "cs.MM", 
    "author": "A. Nirmal kumar", 
    "title": "The Fast Haar Wavelet Transform for Signal & Image Processing", 
    "publish": "2010-02-10T19:27:25Z", 
    "summary": "A method for the design of Fast Haar wavelet for signal processing and image\nprocessing has been proposed. In the proposed work, the analysis bank and\nsynthesis bank of Haar wavelet is modified by using polyphase structure.\nFinally, the Fast Haar wavelet was designed and it satisfies alias free and\nperfect reconstruction condition. Computational time and computational\ncomplexity is reduced in Fast Haar wavelet transform.", 
    "link": "http://arxiv.org/pdf/1002.2184v1", 
    "arxiv-id": "1002.2184v1"
},{
    "category": "cs.MM", 
    "author": "Amr M. Sauber", 
    "title": "Using Statistical Moment Invariants and Entropy in Image Retrieval", 
    "publish": "2010-02-10T19:52:12Z", 
    "summary": "Although content-based image retrieval (CBIR) is not a new subject, it keeps\nattracting more and more attention, as the amount of images grow tremendously\ndue to internet, inexpensive hardware and automation of image acquisition. One\nof the applications of CBIR is fetching images from a database. This paper\npresents a new method for automatic image retrieval using moment invariants and\nimage entropy, our technique could be used to find semi or perfect matches\nbased on query by example manner, experimental results demonstrate that the\npurposed technique is scalable and efficient.", 
    "link": "http://arxiv.org/pdf/1002.2193v1", 
    "arxiv-id": "1002.2193v1"
},{
    "category": "cs.MM", 
    "author": "K. Thaiyalnayaki", 
    "title": "Dual Watermarking Scheme with Encryption", 
    "publish": "2010-02-11T20:05:22Z", 
    "summary": "Digital Watermarking is used for copyright protection and authentication. In\nthe proposed system, a Dual Watermarking Scheme based on DWT SVD with chaos\nencryption algorithm, will be developed to improve the robustness and\nprotection along with security. DWT and SVD have been used as a mathematical\ntool to embed watermark in the image. Two watermarks are embedded in the host\nimage. The secondary is embedded into primary watermark and the resultant\nwatermarked image is encrypted using chaos based logistic map. This provides an\nefficient and secure way for image encryption and transmission. The watermarked\nimage is decrypted and a reliable watermark extraction scheme is developed for\nthe extraction of the primary as well as secondary watermark from the distorted\nimage.", 
    "link": "http://arxiv.org/pdf/1002.2414v1", 
    "arxiv-id": "1002.2414v1"
},{
    "category": "cs.CR", 
    "author": "B. B. Zaidan", 
    "title": "New System for Secure Cover File of Hidden Data in the Image Page within   Executable File Using Statistical Steganography Techniques", 
    "publish": "2010-02-11T20:28:15Z", 
    "summary": "A Previously traditional methods were sufficient to protect the information,\nsince it is simplicity in the past does not need complicated methods but with\nthe progress of information technology, it become easy to attack systems, and\ndetection of encryption methods became necessary to find ways parallel with the\ndiffering methods used by hackers, so the embedding methods could be under\nsurveillance from system managers in an organization that requires the high\nlevel of security. This fact requires researches on new hiding methods and\ncover objects which hidden information is embedded in. It is the result from\nthe researches to embed information in executable files, but when will use the\nexecutable file for cover they have many challenges must be taken into\nconsideration which is any changes made to the file will be firstly detected by\nuntie viruses, secondly the functionality of the file is not still functioning.\nIn this paper, a new information hiding system is presented. The aim of the\nproposed system is to hide information (data file) within image page of\nexecution file (EXEfile) to make sure changes made to the file will not be\ndetected by universe and the functionality of the exe.file is still functioning\nafter hiding process. Meanwhile, since the cover file might be used to identify\nhiding information, the proposed system considers overcoming this dilemma by\nusing the execution file as a cover file.", 
    "link": "http://arxiv.org/pdf/1002.2416v1", 
    "arxiv-id": "1002.2416v1"
},{
    "category": "cs.CV", 
    "author": "Alireza Avanaki", 
    "title": "Iterative exact global histogram specification and SSIM gradient ascent:   a proof of convergence, step size and parameter selection", 
    "publish": "2010-02-17T18:29:09Z", 
    "summary": "The SSIM-optimized exact global histogram specification (EGHS) is shown to\nconverge in the sense that the first order approximation of the result's\nquality (i.e., its structural similarity with input) does not decrease in an\niteration, when the step size is small. Each iteration is composed of SSIM\ngradient ascent and basic EGHS with the specified target histogram. Selection\nof step size and other parameters is also discussed.", 
    "link": "http://arxiv.org/pdf/1002.3344v1", 
    "arxiv-id": "1002.3344v1"
},{
    "category": "cs.MM", 
    "author": "Hamdan. O. Alanazi", 
    "title": "Optimization Digital Image Watermarking Technique for Patent Protection", 
    "publish": "2010-02-22T03:19:07Z", 
    "summary": "The rapid development of multimedia and internet allows for wide distribution\nof digital media data. It becomes much easier to edit, modify and duplicate\ndigital information besides that, digital documents are also easy to copy and\ndistribute, therefore it will be faced by many threats. It is a big security\nand privacy issue. Another problem with digital document and video is that\nundetectable modifications can be made with very simple and widely available\nequipment, which put the digital material for evidential purposes under\nquestion With the large flood of information and the development of the digital\nformat, it become necessary to find appropriate protection because of the\nsignificance, accuracy and sensitivity of the information, therefore multimedia\ntechnology and popularity of internet communications they have great interest\nin using digital watermarks for the purpose of copy protection and content\nauthentication. Digital watermarking is a technique used to embed a known piece\nof digital data within another piece of digital data .A digital data may\nrepresent a digital signature or digital watermark that is embedded in the host\nmedia. The signature or watermark is hidden such that it's perceptually and\nstatistically undetectable. Then this signature or watermark can be extracted\nfrom the host media and used to identify the owner of the media.", 
    "link": "http://arxiv.org/pdf/1002.4049v1", 
    "arxiv-id": "1002.4049v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "What are suspicious VoIP delays?", 
    "publish": "2010-02-23T11:22:50Z", 
    "summary": "Voice over IP (VoIP) is unquestionably the most popular real-time service in\nIP networks today. Recent studies have shown that it is also a suitable carrier\nfor information hiding. Hidden communication may pose security concerns as it\ncan lead to confidential information leakage. In VoIP, RTP (Real-time Transport\nProtocol) in particular, which provides the means for the successful transport\nof voice packets through IP networks, is suitable for steganographic purposes.\nIt is characterised by a high packet rate compared to other protocols used in\nIP telephony, resulting in a potentially high steganographic bandwidth. The\nmodification of an RTP packet stream provides many opportunities for hidden\ncommunication as the packets may be delayed, reordered or intentionally lost.\nIn this paper, to enable the detection of steganographic exchanges in VoIP, we\nexamined real RTP traffic traces to answer the questions, what do the \"normal\"\ndelays in RTP packet streams look like? and, is it possible to detect the use\nof known RTP steganographic methods based on this knowledge?", 
    "link": "http://arxiv.org/pdf/1002.4303v1", 
    "arxiv-id": "1002.4303v1"
},{
    "category": "cs.MM", 
    "author": "P. Musa", 
    "title": "An Algorithm for Index Multimedia Data (Video) using the Movement   Oriented Method for Real-time Online Services", 
    "publish": "2010-03-16T05:07:10Z", 
    "summary": "Multimedia data is a form of data that can represent all types of data\n(images, sound and text). The use of multimedia data for the online application\nrequires a more comprehensive database in the use of storage media, Sorting /\nindexing, search and system / data searching. This is necessary in order to\nhelp providers and users to access multimedia data online. Systems that use of\nthe index image as a reference requires storage media so that the rules and\nrequire special expertise to obtain the desired file. Changes in multimedia\ndata into a series of stories / storyboard in the form of a text will help\nreduce the consumption of media storage, system index / sorting and search\napplications. Oriented Movement is one method that is being developed to change\nthe form of multimedia data into a storyboard.", 
    "link": "http://arxiv.org/pdf/1003.3080v1", 
    "arxiv-id": "1003.3080v1"
},{
    "category": "cs.MM", 
    "author": "Frederic Andres", 
    "title": "Towards Automated Lecture Capture, Navigation and Delivery System for   Web-Lecture on Demand", 
    "publish": "2010-03-18T09:27:16Z", 
    "summary": "Institutions all over the world are continuously exploring ways to use ICT in\nimproving teaching and learning effectiveness. The use of course web pages,\ndiscussion groups, bulletin boards, and e-mails have shown considerable impact\non teaching and learning in significant ways, across all disciplines. ELearning\nhas emerged as an alternative to traditional classroom-based education and\ntraining and web lectures can be a powerful addition to traditional lectures.\nThey can even serve as a main content source for learning, provided users can\nquickly navigate and locate relevant pages in a web lecture. A web lecture\nconsists of video and audio of the presenter and slides complemented with\nscreen capturing. In this paper, an automated approach for recording live\nlectures and for browsing available web lectures for on-demand applications by\nend users is presented.", 
    "link": "http://arxiv.org/pdf/1003.3533v1", 
    "arxiv-id": "1003.3533v1"
},{
    "category": "cs.MM", 
    "author": "Zaidoon Kh. AL-Ani", 
    "title": "New Classification Methods for Hiding Information into Two Parts:   Multimedia Files and Non Multimedia Files", 
    "publish": "2010-03-22T06:42:05Z", 
    "summary": "With the rapid development of various multimedia technologies, more and more\nmultimedia data are generated and transmitted in the medical, commercial, and\nmilitary fields, which may include some sensitive information which should not\nbe accessed by or can only be partially exposed to the general users.\nTherefore, security and privacy has become an important, Another problem with\ndigital document and video is that undetectable modifications can be made with\nvery simple and widely available equipment, which put the digital material for\nevidential purposes under question .With the large flood of information and the\ndevelopment of the digital format Information hiding considers one of the\ntechniques which used to protect the important information. The main goals for\nthis paper, provides a general overview of the New Classification Methods for\nHiding Information into Two Parts: Multimedia Files and Non Multimedia Files.", 
    "link": "http://arxiv.org/pdf/1003.4084v1", 
    "arxiv-id": "1003.4084v1"
},{
    "category": "cs.CR", 
    "author": "Hamdan. O. Alanazi", 
    "title": "Overview: Main Fundamentals for Steganography", 
    "publish": "2010-03-22T06:46:04Z", 
    "summary": "The rapid development of multimedia and internet allows for wide distribution\nof digital media data. It becomes much easier to edit, modify and duplicate\ndigital information .Besides that, digital documents are also easy to copy and\ndistribute, therefore it will be faced by many threats. It is a big security\nand privacy issue, it become necessary to find appropriate protection because\nof the significance, accuracy and sensitivity of the information. Steganography\nconsiders one of the techniques which used to protect the important\ninformation. The main goals for this paper, to recognize the researchers for\nthe main fundamentals of steganography. In this paper provides a general\noverview of the following subject areas: Steganography types, General\nSteganography system, Characterization of Steganography Systems and\nClassification of Steganography Techniques.", 
    "link": "http://arxiv.org/pdf/1003.4086v1", 
    "arxiv-id": "1003.4086v1"
},{
    "category": "cs.CV", 
    "author": "S. Srinivas Kumar", 
    "title": "Image Compression and Watermarking scheme using Scalar Quantization", 
    "publish": "2010-03-29T06:51:17Z", 
    "summary": "This paper presents a new compression technique and image watermarking\nalgorithm based on Contourlet Transform (CT). For image compression, an energy\nbased quantization is used. Scalar quantization is explored for image\nwatermarking. Double filter bank structure is used in CT. The Laplacian Pyramid\n(LP) is used to capture the point discontinuities, and then followed by a\nDirectional Filter Bank (DFB) to link point discontinuities. The coefficients\nof down sampled low pass version of LP decomposed image are re-ordered in a\npre-determined manner and prediction algorithm is used to reduce entropy\n(bits/pixel). In addition, the coefficients of CT are quantized based on the\nenergy in the particular band. The superiority of proposed algorithm to JPEG is\nobserved in terms of reduced blocking artifacts. The results are also compared\nwith wavelet transform (WT). Superiority of CT to WT is observed when the image\ncontains more contours. The watermark image is embedded in the low pass image\nof contourlet decomposition. The watermark can be extracted with minimum error.\nIn terms of PSNR, the visual quality of the watermarked image is exceptional.\nThe proposed algorithm is robust to many image attacks and suitable for\ncopyright protection applications.", 
    "link": "http://arxiv.org/pdf/1003.5435v1", 
    "arxiv-id": "1003.5435v1"
},{
    "category": "cs.HC", 
    "author": "Anja Lee Pedersen", 
    "title": "Psychophysiological Correlations with Gameplay Experience Dimensions", 
    "publish": "2010-04-01T21:52:36Z", 
    "summary": "In this paper, we report a case study using two easy-to-deploy\npsychophysiological measures - electrodermal activity (EDA) and heart rate (HR)\n- and correlating them with a gameplay experience questionnaire (GEQ) in an\nattempt to establish this mixed-methods approach for rapid application in a\ncommercial game development context. Results indicate that there is a\nstatistically significant correlation (p < 0.01) between measures of\npsychophysiological arousal (HR, EDA) and self-reported UX in games (GEQ), with\nsome variation between the EDA and HR measures. Results are consistent across\nthree major commercial First-Person Shooter (FPS) games.", 
    "link": "http://arxiv.org/pdf/1004.0243v1", 
    "arxiv-id": "1004.0243v1"
},{
    "category": "cs.HC", 
    "author": "Craig A. Lindley", 
    "title": "Affective Ludology, Flow and Immersion in a First- Person Shooter:   Measurement of Player Experience", 
    "publish": "2010-04-01T22:24:47Z", 
    "summary": "Gameplay research about experiential phenomena is a challenging undertaking,\ngiven the variety of experiences that gamers encounter when playing and which\ncurrently do not have a formal taxonomy, such as flow, immersion, boredom, and\nfun. These informal terms require a scientific explanation. Ludologists also\nacknowledge the need to understand cognition, emotion, and goal- oriented\nbehavior of players from a psychological perspective by establishing rigorous\nmethodologies. This paper builds upon and extends prior work in an area for\nwhich we would like to coin the term \"affective ludology.\" The area is\nconcerned with the affective measurement of player-game interaction. The\nexperimental study reported here investigated different traits of gameplay\nexperience using subjective (i.e., questionnaires) and objective (i.e.,\npsychophysiological) measures. Participants played three Half-Life 2 game level\ndesign modifications while measures such as electromyography (EMG),\nelectrodermal activity (EDA) were taken and questionnaire responses were\ncollected. A level designed for combat-oriented flow experience demonstrated\nsignificant high-arousal positive affect emotions. This method shows that\nemotional patterns emerge from different level designs, which has great\npotential for providing real-time emotional profiles of gameplay that may be\ngenerated together with self- reported subjective player experience\ndescriptions.", 
    "link": "http://arxiv.org/pdf/1004.0248v1", 
    "arxiv-id": "1004.0248v1"
},{
    "category": "cs.HC", 
    "author": "Lennart E. Nacke", 
    "title": "From Playability to a Hierarchical Game Usability Model", 
    "publish": "2010-04-01T23:42:11Z", 
    "summary": "This paper presents a brief review of current game usability models. This\nleads to the conception of a high-level game development-centered usability\nmodel that integrates current usability approaches in game industry and game\nresearch.", 
    "link": "http://arxiv.org/pdf/1004.0256v1", 
    "arxiv-id": "1004.0256v1"
},{
    "category": "cs.HC", 
    "author": "Craig A. Lindley", 
    "title": "Gameplay experience in a gaze interaction game", 
    "publish": "2010-04-01T23:52:50Z", 
    "summary": "Assessing gameplay experience for gaze interaction games is a challenging\ntask. For this study, a gaze interaction Half-Life 2 game modification was\ncreated that allowed eye tracking control. The mod was deployed during an\nexperiment at Dreamhack 2007, where participants had to play with gaze\nnavigation and afterwards rate their gameplay experience. The results show low\ntension and negative affects scores on the gameplay experience questionnaire as\nwell as high positive challenge, immersion and flow ratings. The correlation\nbetween spatial presence and immersion for gaze interaction was high and yields\nfurther investigation. It is concluded that gameplay experience can be\ncorrectly assessed with the methodology presented in this paper.", 
    "link": "http://arxiv.org/pdf/1004.0259v1", 
    "arxiv-id": "1004.0259v1"
},{
    "category": "cs.CR", 
    "author": "Hossam Afifi", 
    "title": "Integrating identity-based cryptography in IMS service authentication", 
    "publish": "2010-04-06T03:38:59Z", 
    "summary": "Nowadays, the IP Multimedia Subsystem (IMS) is a promising research field.\nMany ongoing works related to the security and the performances of its\nemployment are presented to the research community. Although, the security and\ndata privacy aspects are very important in the IMS global objectives, they\nobserve little attention so far. Secure access to multimedia services is based\non SIP and HTTP digest on top of IMS architecture. The standard deploys AKA-MD5\nfor the terminal authentication. The third Generation Partnership Project\n(3GPP) provided Generic Bootstrapping Architecture (GBA) to authenticate the\nsubscriber before accessing multimedia services over HTTP. In this paper, we\npropose a new IMS Service Authentication scheme using Identity Based\ncryptography (IBC). This new scheme will lead to better performances when there\nare simultaneous authentication requests using Identity-based Batch\nVerification. We analyzed the security of our new protocol and we presented a\nperformance evaluation of its cryptographic operations", 
    "link": "http://arxiv.org/pdf/1004.0762v1", 
    "arxiv-id": "1004.0762v1"
},{
    "category": "cs.MM", 
    "author": "V. Balamurugan", 
    "title": "Feature-Based Adaptive Tolerance Tree (FATT): An Efficient Indexing   Technique for Content-Based Image Retrieval Using Wavelet Transform", 
    "publish": "2010-04-08T02:56:58Z", 
    "summary": "This paper introduces a novel indexing and access method, called Feature-\nBased Adaptive Tolerance Tree (FATT), using wavelet transform is proposed to\norganize large image data sets efficiently and to support popular image access\nmechanisms like Content Based Image Retrieval (CBIR).Conventional database\nsystems are designed for managing textual and numerical data and retrieving\nsuch data is often based on simple comparisons of text or numerical values.\nHowever, this method is no longer adequate for images, since the digital\npresentation of images does not convey the reality of images. Retrieval of\nimages become difficult when the database is very large. This paper addresses\nsuch problems and presents a novel indexing technique, Feature Based Adaptive\nTolerance Tree (FATT), which is designed to bring an effective solution\nespecially for indexing large databases. The proposed indexing scheme is then\nused along with a query by image content, in order to achieve the ultimate goal\nfrom the user point of view that is retrieval of all relevant images. FATT\nindexing technique, features of the image is extracted using 2-dimensional\ndiscrete wavelet transform (2DDWT) and index code is generated from the\ndeterminant value of the features. Multiresolution analysis technique using\n2D-DWT can decompose the image into components at different scales, so that the\ncoarest scale components carry the global approximation information while the\nfiner scale components contain the detailed information. Experimental results\nshow that the FATT outperforms M-tree upto 200%, Slim-tree up to 120% and HCT\nupto 89%. FATT indexing technique is adopted to increase the efficiently of\ndata storage and retrieval.", 
    "link": "http://arxiv.org/pdf/1004.1229v1", 
    "arxiv-id": "1004.1229v1"
},{
    "category": "cs.NI", 
    "author": "S. Radhakrishnan", 
    "title": "Processor Based Active Queue Management for providing QoS in Multimedia   Application", 
    "publish": "2010-04-11T03:44:30Z", 
    "summary": "The objective of this paper is to implement the Active Network based Active\nQueue Management Technique for providing Quality of Service (QoS) using Network\nProcessor(NP) based router to enhance multimedia applications. The performance\nis evaluated using Intel IXP2400 NP Simulator. The results demonstrate that,\nActive Network based Active Queue Management has better performance than RED\nalgorithm in case of congestion and is well suited to achieve high speed packet\nclassification to support multimedia applications with minimum delay and Queue\nloss. Using simulation, we show that the proposed system can provide assurance\nfor prioritized flows with improved network utilization where bandwidth is\nshared among the flows according to the levels of priority. We first analyze\nthe feasibility and optimality of the load distribution schemes and then\npresent separate solutions for non-delay sensitive streams and delay-sensitive\nstreams. Rigorous simulations and experiments have been carried out to evaluate\nthe performance.", 
    "link": "http://arxiv.org/pdf/1004.1757v1", 
    "arxiv-id": "1004.1757v1"
},{
    "category": "cs.MM", 
    "author": "Tanuja K. Sarode", 
    "title": "SAR Image Segmentation using Vector Quantization Technique on Entropy   Images", 
    "publish": "2010-04-11T11:05:33Z", 
    "summary": "The development and application of various remote sensing platforms result in\nthe production of huge amounts of satellite image data. Therefore, there is an\nincreasing need for effective querying and browsing in these image databases.\nIn order to take advantage and make good use of satellite images data, we must\nbe able to extract meaningful information from the imagery. Hence we proposed a\nnew algorithm for SAR image segmentation. In this paper we propose segmentation\nusing vector quantization technique on entropy image. Initially, we obtain\nentropy image and in second step we use Kekre's Fast Codebook Generation (KFCG)\nalgorithm for segmentation of the entropy image. Thereafter, a codebook of size\n128 was generated for the Entropy image. These code vectors were further\nclustered in 8 clusters using same KFCG algorithm and converted into 8 images.\nThese 8 images were displayed as a result. This approach does not lead to over\nsegmentation or under segmentation. We compared these results with well known\nGray Level Co-occurrence Matrix. The proposed algorithm gives better\nsegmentation with less complexity.", 
    "link": "http://arxiv.org/pdf/1004.1789v1", 
    "arxiv-id": "1004.1789v1"
},{
    "category": "math.OC", 
    "author": "Srinivas Shakkottai", 
    "title": "Access-Network Association Policies for Media Streaming in Heterogeneous   Environments", 
    "publish": "2010-04-20T18:32:58Z", 
    "summary": "We study the design of media streaming applications in the presence of\nmultiple heterogeneous wireless access methods with different throughputs and\ncosts. Our objective is to analytically characterize the trade-off between the\nusage cost and the Quality of user Experience (QoE), which is represented by\nthe probability of interruption in media playback and the initial waiting time.\nWe model each access network as a server that provides packets to the user\naccording to a Poisson process with a certain rate and cost. Blocks are coded\nusing random linear codes to alleviate the duplicate packet reception problem.\nUsers must take decisions on how many packets to buffer before playout, and\nwhich networks to access during playout. We design, analyze and compare several\ncontrol policies with a threshold structure. We formulate the problem of\nfinding the optimal control policy as an MDP with a probabilistic constraint.\nWe present the HJB equation for this problem by expanding the state space, and\nexploit it as a verification method for optimality of the proposed control law.", 
    "link": "http://arxiv.org/pdf/1004.3523v1", 
    "arxiv-id": "1004.3523v1"
},{
    "category": "cs.NI", 
    "author": "P. Calyam", 
    "title": "Influence of distortions of key frames on video transfer in wireless   networks", 
    "publish": "2010-05-01T17:15:36Z", 
    "summary": "In this paper it is shown that for substantial increase of video quality in\nwireless network it is necessary to execute two obligatory points on\nmodernization of the communication scheme. The player on the received part\nshould throw back automatically duplicated RTP packets, server of streaming\nvideo should duplicate the packets containing the information of key frames.\nCoefficients of the mathematical model describing video quality in wireless\nnetwork have been found for WiFi and 3G standards and codecs MPEG-2 and MPEG-4\n(DivX). The special experimental technique which has allowed collecting and\nprocessing the data has been developed for calculation of values of factors.", 
    "link": "http://arxiv.org/pdf/1005.0092v1", 
    "arxiv-id": "1005.0092v1"
},{
    "category": "cs.MM", 
    "author": "Gulfishan Firdose Ahmed", 
    "title": "Content Base Image Retrieval Using Phong Shading", 
    "publish": "2010-05-24T07:28:24Z", 
    "summary": "The digital image data is rapidly expanding in quantity and heterogeneity.\nThe traditional information retrieval techniques does not meet the user's\ndemand, so there is need to develop an efficient system for content based image\nretrieval. Content based image retrieval means retrieval of images from\ndatabase on the basis of visual features of image like as color, texture etc.\nIn our proposed method feature are extracted after applying Phong shading on\ninput image. Phong shading, flattering out the dull surfaces of the image The\nfeatures are extracted using color, texture & edge density methods. Feature\nextracted values are used to find the similarity between input query image and\nthe data base image. It can be measure by the Euclidean distance formula. The\nexperimental result shows that the proposed approach has a better retrieval\nresults with phong shading.", 
    "link": "http://arxiv.org/pdf/1005.4267v1", 
    "arxiv-id": "1005.4267v1"
},{
    "category": "cs.CR", 
    "author": "N. Kamaraj", 
    "title": "Optimized Image Steganalysis through Feature Selection using MBEGA", 
    "publish": "2010-08-17T05:57:36Z", 
    "summary": "Feature based steganalysis, an emerging branch in information forensics, aims\nat identifying the presence of a covert communication by employing the\nstatistical features of the cover and stego image as clues/evidences. Due to\nthe large volumes of security audit data as well as complex and dynamic\nproperties of steganogram behaviours, optimizing the performance of\nsteganalysers becomes an important open problem. This paper is focussed at fine\ntuning the performance of six promising steganalysers in this field, through\nfeature selection. We propose to employ Markov Blanket-Embedded Genetic\nAlgorithm (MBEGA) for stego sensitive feature selection process. In particular,\nthe embedded Markov blanket based memetic operators add or delete features (or\ngenes) from a genetic algorithm (GA) solution so as to quickly improve the\nsolution and fine-tune the search. Empirical results suggest that MBEGA is\neffective and efficient in eliminating irrelevant and redundant features based\non both Markov blanket and predictive power in classifier model. Observations\nshow that the proposed method is superior in terms of number of selected\nfeatures, classification accuracy and computational cost than their existing\ncounterparts.", 
    "link": "http://arxiv.org/pdf/1008.2824v1", 
    "arxiv-id": "1008.2824v1"
},{
    "category": "cs.MM", 
    "author": "Lihua Li", 
    "title": "Reliable Multicasting for Device-to-Device Radio Underlaying Cellular   Networks", 
    "publish": "2010-08-23T03:06:22Z", 
    "summary": "This paper proposes Leader in Charge (LiC), a reliable multicast architecture\nfor device-to-device (D2D) radio underlaying cellular networks. The\nmulticast-requesting user equipments (UEs) in close proximity form a D2D\ncluster to receive the multicast packets through cooperation. In addition to\nreceiving the multicast packets from the eNB, UEs share what they received from\nthe multicast on short-range links among UEs, namely the D2D links, to exploit\nthe wireless resources a more efficient way. Consequently, we show that\nutilizing the D2D links in cellular networks increases the throughput of a\nmulticast session by means of simulation. We also discuss some practical issues\nfacing the integration of LiC into the current cellular networks. In\nparticular, we propose efficient delay control mechanism to reduce the average\nand maximum delay experienced by LiC users, which is further confirmed by the\nsimulation results.", 
    "link": "http://arxiv.org/pdf/1008.3741v2", 
    "arxiv-id": "1008.3741v2"
},{
    "category": "cs.NI", 
    "author": "Zygmunt J. Haas", 
    "title": "Network evolution and QOS provisioning for integrated   femtocell/macrocell networks", 
    "publish": "2010-09-13T12:39:02Z", 
    "summary": "Integrated femtocell/macrocell networks, comprising a conventional cellular\nnetwork overlaid with femtocells, offer an economically appealing way to\nimprove coverage, quality of service, and access network capacity. The key\nelement to successful femtocells/macrocell integration lies in its\nself-organizing capability. Provisioning of quality of service is the main\ntechnical challenge of the femtocell/macrocell integrated networks, while the\nmain administrative challenge is the choice of the proper evolutionary path\nfrom the existing macrocellular networks to the integrated network. In this\narticle, we introduce three integrated network architectures which, while\nincreasing the access capacity, they also reduce the deployment and operational\ncosts. Then, we discuss a number of technical issues, which are key to making\nsuch integration a reality, and we offer possible approaches to their solution.\nThese issues include efficient frequency and interference management, quality\nof service provisioning of the xDSL-based backhaul networks, and intelligent\nhandover control.", 
    "link": "http://arxiv.org/pdf/1009.2368v1", 
    "arxiv-id": "1009.2368v1"
},{
    "category": "cs.NI", 
    "author": "Yeong Min Jang", 
    "title": "Handover Control for WCDMA Femtocell Networks", 
    "publish": "2010-09-20T11:50:21Z", 
    "summary": "The ability to seamlessly switch between the macro networks and femtocell\nnetworks is a key driver for femtocell network deployment. The handover\nprocedures for the integrated femtocell/macrocell networks differ from the\nexisting handovers. Some modifications of existing network and protocol\narchitecture for the integration of femtocell networks with the existing\nmacrocell networks are also essential. These modifications change the signal\nflow for handover procedures due to different 2-tier cell (macrocell and\nfemtocell) environment. The handover between two networks should be performed\nwith minimum signaling. A frequent and unnecessary handover is another problem\nfor hierarchical femtocell/macrocell network environment that must be\nminimized. This work studies the details mobility management schemes for small\nand medium scale femtocell network deployment. To do that, firstly we present\ntwo different network architectures for small scale and medium scale WCDMA\nfemtocell deployment. The details handover call flow for these two network\narchitectures and CAC scheme to minimize the unnecessary handovers are proposed\nfor the integrated femtocell/macrocell networks. The numerical analysis for the\nproposed M/M/N/N queuing scheme and the simulation results of the proposed CAC\nscheme demonstrate the handover call control performances for femtocell\nenvironment.", 
    "link": "http://arxiv.org/pdf/1009.3779v1", 
    "arxiv-id": "1009.3779v1"
},{
    "category": "cs.DC", 
    "author": "Helen D. Karatza", 
    "title": "A Gossip-based optimistic replication for efficient delay-sensitive   streaming using an interactive middleware support system", 
    "publish": "2010-09-09T09:14:37Z", 
    "summary": "While sharing resources the efficiency is substantially degraded as a result\nof the scarceness of availability of the requested resources in a multiclient\nsupport manner. These resources are often aggravated by many factors like the\ntemporal constraints for availability or node flooding by the requested\nreplicated file chunks. Thus replicated file chunks should be efficiently\ndisseminated in order to enable resource availability on-demand by the mobile\nusers. This work considers a cross layered middleware support system for\nefficient delay-sensitive streaming by using each device's connectivity and\nsocial interactions in a cross layered manner. The collaborative streaming is\nachieved through the epidemically replicated file chunk policy which uses a\ntransition-based approach of a chained model of an infectious disease with\nsusceptible, infected, recovered and death states. The Gossip-based stateful\nmodel enforces the mobile nodes whether to host a file chunk or not or, when no\nlonger a chunk is needed, to purge it. The proposed model is thoroughly\nevaluated through experimental simulation taking measures for the effective\nthroughput Eff as a function of the packet loss parameter in contrast with the\neffectiveness of the replication Gossip-based policy.", 
    "link": "http://arxiv.org/pdf/1009.4642v1", 
    "arxiv-id": "1009.4642v1"
},{
    "category": "cs.MM", 
    "author": "Pedro M. Q. Aguiar", 
    "title": "Alternatives to speech in low bit rate communication systems", 
    "publish": "2010-10-19T15:24:33Z", 
    "summary": "This paper describes a framework and a method with which speech communication\ncan be analyzed. The framework consists of a set of low bit rate, short-range\nacoustic communication systems, such as speech, but that are quite different\nfrom speech. The method is to systematically compare these systems according to\ndifferent objective functions such as data rate, computational overhead,\npsychoacoustic effects and semantics. One goal of this study is to better\nunderstand the nature of human communication. Another goal is to identify\nacoustic communication systems that are more efficient than human speech for\nsome specific purposes.", 
    "link": "http://arxiv.org/pdf/1010.3951v1", 
    "arxiv-id": "1010.3951v1"
},{
    "category": "cs.NI", 
    "author": "Patrick Seeling", 
    "title": "Web Conferencing Traffic - An Analysis using DimDim as Example", 
    "publish": "2010-11-12T12:11:51Z", 
    "summary": "In this paper, we present an evaluation of the Ethernet traffic for host and\nattendees of the popular opensource web conferencing system DimDim. While\ntraditional Internet-centric approaches such as the MBONE have been used over\nthe past decades, current trends for web-based conference systems make\nexclusive use of application-layer multicast. To allow for network dimensioning\nand QoS provisioning, an understanding of the underlying traffic\ncharacteristics is required. We find in our exemplary evaluations that the host\nof a web conference session produces a large amount of Ethernet traffic,\nlargely due to the required control of the conference session, that is\nheavily-tailed distributed and exhibits additionally long-range dependence. For\ndifferent groups of activities within a web conference session, we find\ndistinctive characteristics of the generated traffic.", 
    "link": "http://arxiv.org/pdf/1011.2893v1", 
    "arxiv-id": "1011.2893v1"
},{
    "category": "cs.CV", 
    "author": "N. Anbazhagan", 
    "title": "An Effective Method of Image Retrieval using Image Mining Techniques", 
    "publish": "2010-12-01T15:34:50Z", 
    "summary": "The present research scholars are having keen interest in doing their\nresearch activities in the area of Data mining all over the world. Especially,\n[13]Mining Image data is the one of the essential features in this present\nscenario since image data plays vital role in every aspect of the system such\nas business for marketing, hospital for surgery, engineering for construction,\nWeb for publication and so on. The other area in the Image mining system is the\nContent-Based Image Retrieval (CBIR) which performs retrieval based on the\nsimilarity defined in terms of extracted features with more objectiveness. The\ndrawback in CBIR is the features of the query image alone are considered.\nHence, a new technique called Image retrieval based on optimum clusters is\nproposed for improving user interaction with image retrieval systems by fully\nexploiting the similarity information. The index is created by describing the\nimages according to their color characteristics, with compact feature vectors,\nthat represent typical color distributions [12].", 
    "link": "http://arxiv.org/pdf/1012.0223v1", 
    "arxiv-id": "1012.0223v1"
},{
    "category": "cs.CV", 
    "author": "Xiaolin Wu", 
    "title": "Image Deblurring and Super-resolution by Adaptive Sparse Domain   Selection and Adaptive Regularization", 
    "publish": "2010-12-06T14:37:14Z", 
    "summary": "As a powerful statistical image modeling technique, sparse representation has\nbeen successfully used in various image restoration applications. The success\nof sparse representation owes to the development of l1-norm optimization\ntechniques, and the fact that natural images are intrinsically sparse in some\ndomain. The image restoration quality largely depends on whether the employed\nsparse domain can represent well the underlying image. Considering that the\ncontents can vary significantly across different images or different patches in\na single image, we propose to learn various sets of bases from a pre-collected\ndataset of example image patches, and then for a given patch to be processed,\none set of bases are adaptively selected to characterize the local sparse\ndomain. We further introduce two adaptive regularization terms into the sparse\nrepresentation framework. First, a set of autoregressive (AR) models are\nlearned from the dataset of example image patches. The best fitted AR models to\na given patch are adaptively selected to regularize the image local structures.\nSecond, the image non-local self-similarity is introduced as another\nregularization term. In addition, the sparsity regularization parameter is\nadaptively estimated for better image restoration performance. Extensive\nexperiments on image deblurring and super-resolution validate that by using\nadaptive sparse domain selection and adaptive regularization, the proposed\nmethod achieves much better results than many state-of-the-art algorithms in\nterms of both PSNR and visual perception.", 
    "link": "http://arxiv.org/pdf/1012.1184v1", 
    "arxiv-id": "1012.1184v1"
},{
    "category": "cs.NI", 
    "author": "Shomik Bhattacharya", 
    "title": "A Survey on Cross-Layer Design Frameworks for Multimedia Applications   over Wireless Networks", 
    "publish": "2010-12-12T07:40:07Z", 
    "summary": "In the last few years, the Internet throughput, usage and reliability have\nincreased almost exponentially. The introduction of broadband wireless mobile\nad hoc networks (MANETs) and cellular networks together with increased\ncomputational power have opened the door for a new breed of applications to be\ncreated, namely real-time multimedia applications. Delivering real-time\nmultimedia traffic over a complex network like the Internet is a particularly\nchallenging task since these applications have strict quality -of-service (QoS)\nrequirements on bandwidth, delay, and delay jitter. Traditional IP-based best\neffort service will not be able to meet these stringent requirements. The\ntime-varying nature of wireless channels and resource constrained wireless\ndevices make the problem even more difficult. To improve perceived media\nquality by end users over wireless Internet, QoS supports can be addressed in\ndifferent layers, including application layer, transport layer and link layer.\nCross layer design is a well-known approach to achieve this adaptation. In\ncross-layer design, the challenges from the physical wireless medium and the\nQoS-demands from the applications are taken into account so that the rate,\npower, and coding at the physical layer can adapted to meet the requirements of\nthe applications given the current channel and network conditions. A number of\npropositions for cross-layer designs exist in the literature. In this paper, an\nextensive review has been made on these cross-layer architectures that combine\nthe application-layer, transport layer and the link layer controls.\nParticularly the issues like channel estimation techniques, adaptive controls\nat the application and link layers for energy efficiency, priority based\nscheduling, transmission rate control at the transport layer, and adaptive\nautomatic repeat request (ARQ) are discussed in detail.", 
    "link": "http://arxiv.org/pdf/1012.2518v1", 
    "arxiv-id": "1012.2518v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "On Steganography in Lost Audio Packets", 
    "publish": "2011-01-31T21:57:34Z", 
    "summary": "The paper presents a new hidden data insertion procedure based on estimated\nprobability of the remaining time of the call for steganographic method called\nLACK (Lost Audio PaCKets steganography). LACK provides hidden communication for\nreal-time services like Voice over IP. The analytical results presented in this\npaper concern the influence of LACK's hidden data insertion procedures on the\nmethod's impact on quality of voice transmission and its resistance to\nsteganalysis. The proposed hidden data insertion procedure is also compared to\nprevious steganogram insertion approach based on estimated remaining average\ncall duration.", 
    "link": "http://arxiv.org/pdf/1102.0023v1", 
    "arxiv-id": "1102.0023v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Peer-to-Peer Multimedia Sharing based on Social Norms", 
    "publish": "2011-02-08T04:36:32Z", 
    "summary": "Empirical data shows that in the absence of incentives, a peer participating\nin a Peer-to-Peer (P2P) network wishes to free-riding. Most solutions for\nproviding incentives in P2P networks are based on direct reciprocity, which are\nnot appropriate for most P2P multimedia sharing networks due to the unique\nfeatures exhibited by such networks: large populations of anonymous agents\ninteracting infrequently, asymmetric interests of peers, network errors, and\nmultiple concurrent transactions. In this paper, we design and rigorously\nanalyze a new family of incentive protocols that utilizes indirect reciprocity\nwhich is based on the design of efficient social norms. In the proposed P2P\nprotocols, the social norms consist of a social strategy, which represents the\nrule prescribing to the peers when they should or should not provide content to\nother peers, and a reputation scheme, which rewards or punishes peers depending\non whether they comply or not with the social strategy. We first define the\nconcept of a sustainable social norm, under which no peer has an incentive to\ndeviate. We then formulate the problem of designing optimal social norms, which\nselects the social norm that maximizes the network performance among all\nsustainable social norms. Hence, we prove that it becomes in the self-interest\nof peers to contribute their content to the network rather than to free-ride.\nWe also investigate the impact of various punishment schemes on the social\nwelfare as well as how should the optimal social norms be designed if\naltruistic and malicious peers are active in the network. Our results show that\noptimal social norms are capable of providing significant improvements in the\nsharing efficiency of multimedia P2P networks.", 
    "link": "http://arxiv.org/pdf/1102.1503v1", 
    "arxiv-id": "1102.1503v1"
},{
    "category": "cs.NI", 
    "author": "Farhat Masood", 
    "title": "A Study on Digital Video Broadcasting to a Handheld Device (DVB-H),   Operating in UHF Band", 
    "publish": "2011-02-08T13:22:55Z", 
    "summary": "In this paper, we will understand that the development of the Digital Video\nBroadcasting to a Handheld (DVB-H) standard makes it possible to deliver live\nbroadcast television to a mobile handheld device. Building upon the strengths\nof the Digital Video Broadcasting - Terrestrial (DVB-T) standard in use in\nmillions of homes, DVB-H recognizes the trend towards the personal consumption\nof media.", 
    "link": "http://arxiv.org/pdf/1102.1600v1", 
    "arxiv-id": "1102.1600v1"
},{
    "category": "cs.MM", 
    "author": "Sina Jafarpour", 
    "title": "Quasi-Optimal Network Utility Maximization for Scalable Video Streaming", 
    "publish": "2011-02-13T15:35:30Z", 
    "summary": "This paper addresses rate control for transmission of scalable video streams\nvia Network Utility Maximization (NUM) formulation. Due to stringent QoS\nrequirements of video streams and specific characterization of utility\nexperienced by end-users, one has to solve nonconvex and even nonsmooth NUM\nformulation for such streams, where dual methods often prove incompetent.\nConvexification plays an important role in this work as it permits the use of\nexisting dual methods to solve an approximate to the NUM problem iteratively\nand distributively. Hence, to tackle the nonsmoothness and nonconvexity, we aim\nat reformulating the NUM problem through approximation and transformation of\nthe ideal discretely adaptive utility function for scalable video streams. The\nreformulated problem is shown to be a D.C. (Difference of Convex) problem. We\nleveraged Sequential Convex Programming (SCP) approach to replace the nonconvex\nD.C. problem by a sequence of convex problems that aim to approximate the\noriginal D.C. problem. We then solve each convex problem produced by SCP\napproach using existing dual methods. This procedure is the essence of two\ndistributed iterative rate control algorithms proposed in this paper, for which\none can show the convergence to a locally optimal point of the nonconvex D.C.\nproblem and equivalently to a locally optimal point of an approximate to the\noriginal nonconvex problem. Our experimental results show that the proposed\nrate control algorithms converge with tractable convergence behavior.", 
    "link": "http://arxiv.org/pdf/1102.2604v2", 
    "arxiv-id": "1102.2604v2"
},{
    "category": "cs.NI", 
    "author": "Chen Chang-Jia", 
    "title": "On the Capacity of p2p Multipoint Video Conference", 
    "publish": "2011-02-16T02:27:45Z", 
    "summary": "In this paper, The structure of video conference is formulated and the\npeer-assisted distribution scheme is constructed to achieve optimal video\ndelivery rate in each sub-conference. The capacity of conference is proposed to\nreferee the video rate that can be supported in every possible scenario. We\nhave proved that, in case of one user watching only one video, 5/6 is a lower\nbound of the capacity which is much larger than 1/2, the achievable rate of\nchained approach in [2]. Almost all proofs in this paper are constructive. They\ncan be applied into real implementation directly with a few modifications.", 
    "link": "http://arxiv.org/pdf/1102.3219v1", 
    "arxiv-id": "1102.3219v1"
},{
    "category": "cs.MM", 
    "author": "Neha Tyagi", 
    "title": "Ontology based approach for video transmission over the network", 
    "publish": "2011-02-28T16:13:20Z", 
    "summary": "With the increase in the bandwidth & the transmission speed over the\ninternet, transmission of multimedia objects like video, audio, images has\nbecome an easier work. In this paper we provide an approach that can be useful\nfor transmission of video objects over the internet without much fuzz. The\napproach provides a ontology based framework that is used to establish an\nautomatic deployment of video transmission system. Further the video is\ncompressed using the structural flow mechanism that uses the wavelet principle\nfor compression of video frames. Finally the video transmission algorithm known\nas RRDBFSF algorithm is provided that makes use of the concept of restrictive\nflooding to avoid redundancy thereby increasing the efficiency.", 
    "link": "http://arxiv.org/pdf/1102.5699v1", 
    "arxiv-id": "1102.5699v1"
},{
    "category": "cs.CV", 
    "author": "Ling Shao", 
    "title": "An Algorithm for Repairing Low-Quality Video Enhancement Techniques   Based on Trained Filter", 
    "publish": "2011-03-02T20:50:22Z", 
    "summary": "Multifarious image enhancement algorithms have been used in different\napplications. Still, some algorithms or modules are imperfect for practical\nuse. When the image enhancement modules have been fixed or combined by a series\nof algorithms, we need to repair them as a whole part without changing the\ninside. This report aims to find an algorithm based on trained filters to\nrepair low-quality image enhancement modules. A brief review on basic image\nenhancement techniques and pixel classification methods will be presented, and\nthe procedure of trained filters will be described step by step. The\nexperiments and result comparisons for this algorithm will be described in\ndetail.", 
    "link": "http://arxiv.org/pdf/1103.0540v1", 
    "arxiv-id": "1103.0540v1"
},{
    "category": "cs.MM", 
    "author": "Rajesh Kumar Tiwari", 
    "title": "Hiding Secret Information in Movie Clip: A Steganographic Approach", 
    "publish": "2011-03-04T05:38:59Z", 
    "summary": "Establishing hidden communication is an important subject of discussion that\nhas gained increasing importance nowadays with the development of the internet.\nOne of the key methods for establishing hidden communication is steganography.\nModern day steganography mainly deals with hiding information within files like\nimage, text, html, binary files etc. These file contains small irrelevant\ninformation that can be substituted for small secret data. To store a high\ncapacity secret data these carrier files are not very supportive. To overcome\nthe problem of storing the high capacity secret data with the utmost security\nfence, we have proposed a novel methodology for concealing a voluminous data\nwith high levels of security wall by using movie clip as a carrier file.", 
    "link": "http://arxiv.org/pdf/1103.0829v1", 
    "arxiv-id": "1103.0829v1"
},{
    "category": "cs.MM", 
    "author": "Yeong Min Jang", 
    "title": "Priority based Interface Selection for Overlaying Heterogeneous Networks", 
    "publish": "2011-03-04T07:40:44Z", 
    "summary": "Offering of different attractive opportunities by different wireless\ntechnologies trends the convergence of heterogeneous networks for the future\nwireless communication system. To make a seamless handover among the\nheterogeneous networks, the optimization of the power consumption, and optimal\nselection of interface are the challenging issues for convergence networks. The\naccess of multi interfaces simultaneously reduces the handover latency and data\nloss in heterogeneous handover. The mobile node (MN) maintains one interface\nconnection while other interface is used for handover process. However, it\ncauses much battery power consumption. In this paper we propose an efficient\ninterface selection scheme including interface selection algorithms, interface\nselection procedures considering battery power consumption and user mobility\nwith other existing parameters for overlaying networks. We also propose a\npriority based network selection scheme according to the service types. MN's\nbattery power level, provision of QoS/QoE in the target network and our\nproposed priority parameters are considered as more important parameters for\nour interface selection algorithm. The performances of the proposed scheme are\nverified using numerical analysis.", 
    "link": "http://arxiv.org/pdf/1103.0837v1", 
    "arxiv-id": "1103.0837v1"
},{
    "category": "cs.GR", 
    "author": "Francesco Pagano", 
    "title": "Rendering of 3D Dynamic Virtual Environments", 
    "publish": "2011-03-22T14:16:07Z", 
    "summary": "In this paper we present a framework for the rendering of dynamic 3D virtual\nenvironments which can be integrated in the development of videogames. It\nincludes methods to manage sounds and particle effects, paged static\ngeometries, the support of a physics engine and various input systems. It has\nbeen designed with a modular structure to allow future expansions. We exploited\nsome open-source state-of-the-art components such as OGRE, PhysX,\nParticleUniverse, etc.; all of them have been properly integrated to obtain\npeculiar physical and environmental effects. The stand-alone version of the\napplication is fully compatible with Direct3D and OpenGL APIs and adopts OpenAL\nAPIs to manage audio cards. Concluding, we devised a showcase demo which\nreproduces a dynamic 3D environment, including some particular effects: the\nalternation of day and night infuencing the lighting of the scene, the\nrendering of terrain, water and vegetation, the reproduction of sounds and\natmospheric agents.", 
    "link": "http://arxiv.org/pdf/1103.4271v2", 
    "arxiv-id": "1103.4271v2"
},{
    "category": "cs.NI", 
    "author": "Andrei M. Sukhov", 
    "title": "Duplication of Key Frames of Video Streams in Wireless Networks", 
    "publish": "2011-04-26T16:29:15Z", 
    "summary": "In this paper technological solutions for improving the quality of video\ntransfer along wireless networks are investigated. Tools have been developed to\nallow packets to be duplicated with key frames data. In the paper we tested\nvideo streams with duplication of all frames, with duplication of key frames,\nand without duplication. The experiments showed that the best results are\nobtained by duplication of packages which contain key frames. The paper also\nprovides an overview of the coefficients describing the dependence of video\nquality on packet loss and delay variation (network jitter).", 
    "link": "http://arxiv.org/pdf/1104.4959v1", 
    "arxiv-id": "1104.4959v1"
},{
    "category": "cs.CV", 
    "author": "Arnaldo Araujo", 
    "title": "Content-Based Spam Filtering on Video Sharing Social Networks", 
    "publish": "2011-04-28T03:16:42Z", 
    "summary": "In this work we are concerned with the detection of spam in video sharing\nsocial networks. Specifically, we investigate how much visual content-based\nanalysis can aid in detecting spam in videos. This is a very challenging task,\nbecause of the high-level semantic concepts involved; of the assorted nature of\nsocial networks, preventing the use of constrained a priori information; and,\nwhat is paramount, of the context dependent nature of spam. Content filtering\nfor social networks is an increasingly demanded task: due to their popularity,\nthe number of abuses also tends to increase, annoying the user base and\ndisrupting their services. We systematically evaluate several approaches for\nprocessing the visual information: using static and dynamic (motionaware)\nfeatures, with and without considering the context, and with or without latent\nsemantic analysis (LSA). Our experiments show that LSA is helpful, but taking\nthe context into consideration is paramount. The whole scheme shows good\nresults, showing the feasibility of the concept.", 
    "link": "http://arxiv.org/pdf/1104.5284v1", 
    "arxiv-id": "1104.5284v1"
},{
    "category": "stat.AP", 
    "author": "Hyokun Yun", 
    "title": "Using Logistic Regression to Analyze the Balance of a Game: The Case of   StarCraft II", 
    "publish": "2011-05-04T08:15:20Z", 
    "summary": "Recently, the market size of online game has been increasing astonishingly\nfast, and so does the importance of good game design. In online games, usually\na human user competes with others, so the fairness of the game system to all\nusers is of great importance not to lose interests of users on the game.\nFurthermore, the emergence and success of electronic sports (e-sports) and\nprofessional gaming which specially talented gamers compete with others draws\nmore attention on whether they are competing in the fair environment. No matter\nhow fierce the debates are in the game-design community, it is rarely the case\nthat one employs statistical analysis to answer this question seriously. But\nconsidering the fact that we can easily gather large amount of user behavior\ndata on games, it seems potentially beneficial to make use of this data to aid\nmaking decisions on design problems of games. Actually, modern games do not aim\nto perfectly design the game at once: rather, they first release the game, and\nthen monitor users' behavior to better balance the game. In such a scenario,\nstatistical analysis can be particularly helpful. Specifically, we chose to\nanalyze the balance of StarCraft II, which is a very successful\nrecently-released real-time strategy (RTS) game. It is a central icon in\ncurrent e-Sports and professional gaming community: from April 1st to 15th,\nthere were 18 tournaments of StarCraft II. However, there is endless debate on\nwhether the winner of the tournament is actually superior to others, or it is\nlargely due to certain design flaws of the game. In this paper, we aim to\nanswer such a question using traditional statistical tool, logistic regression.", 
    "link": "http://arxiv.org/pdf/1105.0755v1", 
    "arxiv-id": "1105.0755v1"
},{
    "category": "cs.MM", 
    "author": "Eugen Lupu", 
    "title": "Streaming Multimedia Information Using the Features of the DVB-S Card", 
    "publish": "2011-05-04T13:46:31Z", 
    "summary": "This paper presents a study of audio-video streaming using the additional\npossibilities of a DVB-S card. The board used for experiments (Technisat\nSkyStar 2) is one of the most frequently used cards for this purpose. Using the\nmain blocks of the board's software support it is possible the implement a\nreally useful and full functional system for audio-video streaming. The\nstreaming is possible to be implemented either for decoded MPEG stream or for\ntransport stream. In this last case it is possible to view not only a program,\nbut any program from the same multiplex. This allows us to implement", 
    "link": "http://arxiv.org/pdf/1105.0826v1", 
    "arxiv-id": "1105.0826v1"
},{
    "category": "cs.SD", 
    "author": "Goutam Saha", 
    "title": "Improving Performance of Speaker Identification System Using   Complementary Information Fusion", 
    "publish": "2011-05-13T16:32:44Z", 
    "summary": "Feature extraction plays an important role as a front-end processing block in\nspeaker identification (SI) process. Most of the SI systems utilize like\nMel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP),\nLinear Predictive Cepstral Coefficients (LPCC), as a feature for representing\nspeech signal. Their derivations are based on short term processing of speech\nsignal and they try to capture the vocal tract information ignoring the\ncontribution from the vocal cord. Vocal cord cues are equally important in SI\ncontext, as the information like pitch frequency, phase in the residual signal,\netc could convey important speaker specific attributes and are complementary to\nthe information contained in spectral feature sets. In this paper we propose a\nnovel feature set extracted from the residual signal of LP modeling.\nHigher-order statistical moments are used here to find the nonlinear\nrelationship in residual signal. To get the advantages of complementarity vocal\ncord based decision score is fused with the vocal tract based score. The\nexperimental results on two public databases show that fused mode system\noutperforms single spectral features.", 
    "link": "http://arxiv.org/pdf/1105.2770v2", 
    "arxiv-id": "1105.2770v2"
},{
    "category": "cs.CV", 
    "author": "Y\u00fccel Yemez", 
    "title": "View subspaces for indexing and retrieval of 3D models", 
    "publish": "2011-05-13T18:24:10Z", 
    "summary": "View-based indexing schemes for 3D object retrieval are gaining popularity\nsince they provide good retrieval results. These schemes are coherent with the\ntheory that humans recognize objects based on their 2D appearances. The\nviewbased techniques also allow users to search with various queries such as\nbinary images, range images and even 2D sketches. The previous view-based\ntechniques use classical 2D shape descriptors such as Fourier invariants,\nZernike moments, Scale Invariant Feature Transform-based local features and 2D\nDigital Fourier Transform coefficients. These methods describe each object\nindependent of others. In this work, we explore data driven subspace models,\nsuch as Principal Component Analysis, Independent Component Analysis and\nNonnegative Matrix Factorization to describe the shape information of the\nviews. We treat the depth images obtained from various points of the view\nsphere as 2D intensity images and train a subspace to extract the inherent\nstructure of the views within a database. We also show the benefit of\ncategorizing shapes according to their eigenvalue spread. Both the shape\ncategorization and data-driven feature set conjectures are tested on the PSB\ndatabase and compared with the competitor view-based 3D shape retrieval\nalgorithms", 
    "link": "http://arxiv.org/pdf/1105.2795v1", 
    "arxiv-id": "1105.2795v1"
},{
    "category": "cs.CV", 
    "author": "Asim Imdad Wagan", 
    "title": "Salient Local 3D Features for 3D Shape Retrieval", 
    "publish": "2011-05-13T18:25:15Z", 
    "summary": "In this paper we describe a new formulation for the 3D salient local features\nbased on the voxel grid inspired by the Scale Invariant Feature Transform\n(SIFT). We use it to identify the salient keypoints (invariant points) on a 3D\nvoxelized model and calculate invariant 3D local feature descriptors at these\nkeypoints. We then use the bag of words approach on the 3D local features to\nrepresent the 3D models for shape retrieval. The advantages of the method are\nthat it can be applied to rigid as well as to articulated and deformable 3D\nmodels. Finally, this approach is applied for 3D Shape Retrieval on the McGill\narticulated shape benchmark and then the retrieval results are presented and\ncompared to other methods.", 
    "link": "http://arxiv.org/pdf/1105.2796v1", 
    "arxiv-id": "1105.2796v1"
},{
    "category": "cs.MM", 
    "author": "Won Ryu", 
    "title": "Service Level Agreement for the QoS Guaranteed Mobile IPTV Services over   Mobile WiMAX Networks", 
    "publish": "2011-05-23T08:30:58Z", 
    "summary": "While mobile IPTV services are supported through the mobile WiMAX networks,\nthere must need some guaranteed bandwidth for the IPTV services especially if\nIPTV and non-IPTV services are simultaneously supported by the mobile WiMAX\nnetworks. The quality of an IPTV service definitely depends on the allocated\nbandwidth for that channel. However, due to the high quality IPTV services and\nto support of huge non-IPTV traffic over mobile WiMAX networks, it is not\npossible to guarantee the sufficient amount of the limited mobile WiMAX\nbandwidth for the mobile IPTV services every time. A Service Level Agreement\n(SLA) between the mobile IPTV service provider and mobile WiMAX network\noperator to reserve sufficient bandwidth for the IPTV calls can increase the\nsatisfaction level of the mobile IPTV users. In this paper, we propose a SLA\nnegotiation procedure for mobile IPTV users over mobile WiMAX networks. The\nBandwidth Broker controls the allocated bandwidth for IPTV and non-IPTV users.\nThe proposed dynamically reserved bandwidth for the IPTV services increases the\nIPTV user's satisfaction level. The simulation results state that, our proposed\nscheme is able to provide better user satisfaction level for the IPTV users.", 
    "link": "http://arxiv.org/pdf/1105.4431v1", 
    "arxiv-id": "1105.4431v1"
},{
    "category": "cs.NI", 
    "author": "Xie Renhong", 
    "title": "A Frequency-domain Compensation Scheme for IQ-Imbalance in OFDM   Receivers", 
    "publish": "2011-05-27T13:08:30Z", 
    "summary": "A pilot pattern across two OFDM symbols with special structure is devised for\nchannel estimation in OFDM systems with IQ imbalance at receiver. Based on this\npilot pattern, a high-efficiency time-domain (TD) least square (LS) channel\nestimator is proposed to significantly suppress channel noise by a factor\nN/(L+1) in comparison with the frequency-domain LS one in [1] where N and L+1\nare the total number of subcarriers and the length of cyclic prefix,\nrespectively. Following this, a low-complexity frequency-domain (FD) Gaussian\nelimination (GE) equalizer is proposed to eliminate IQ distortion by using only\n2N complex multiplications per OFDM symbol. From simulation, the proposed\nscheme TD-LS/FD-GE using only two pilot OFDM symbols achieves the same bit\nerror rate (BER) performance under ideal channel knowledge and no IQ imbalances\nat low and medium signal-to-noise ratio (SNR) regions whereas these\ncompensation schemes including FD-LS/Post-FFT LS, FD-LS/Pre-FFT Corr, and\nSPP/Pre-FFT Corr in [1] require about twenty OFDM training symbols to reach the\nsame performance where A/B denotes compensation scheme with A being channel\nestimator and B being equalizer.", 
    "link": "http://arxiv.org/pdf/1105.5553v3", 
    "arxiv-id": "1105.5553v3"
},{
    "category": "cs.DC", 
    "author": "Jeevan Eranti", 
    "title": "High Quality of Service on Video Streaming in P2P Networks using FST-MDC", 
    "publish": "2011-05-27T13:34:42Z", 
    "summary": "Video streaming applications have newly attracted a large number of\nparticipants in a distribution network. Traditional client-server based video\nstreaming solutions sustain precious bandwidth provision rate on the server.\nRecently, several P2P streaming systems have been organized to provide\non-demand and live video streaming services on the wireless network at reduced\nserver cost. Peer-to-Peer (P2P) computing is a new pattern to construct\ndisseminated network applications. Typical error control techniques are not\nvery well matched and on the other hand error prone channels has increased\ngreatly for video transmission e.g., over wireless networks and IP. These two\nfacts united together provided the essential motivation for the development of\na new set of techniques (error concealment) capable of dealing with\ntransmission errors in video systems. In this paper, we propose an flexible\nmultiple description coding method named as Flexible Spatial-Temporal (FST)\nwhich improves error resilience in the sense of frame loss possibilities over\nindependent paths. It introduces combination of both spatial and temporal\nconcealment technique at the receiver and to conceal the lost frames more\neffectively. Experimental results show that, proposed approach attains\nreasonable quality of video performance over P2P wireless network.", 
    "link": "http://arxiv.org/pdf/1105.5641v1", 
    "arxiv-id": "1105.5641v1"
},{
    "category": "cs.MM", 
    "author": "Mandis S. Beigi", 
    "title": "Scale-Invariant Local Descriptor for Event Recognition in 1D Sensor   Signals", 
    "publish": "2011-05-28T00:44:54Z", 
    "summary": "In this paper, we introduce a shape-based, time-scale invariant feature\ndescriptor for 1-D sensor signals. The time-scale invariance of the feature\nallows us to use feature from one training event to describe events of the same\nsemantic class which may take place over varying time scales such as walking\nslow and walking fast. Therefore it requires less training set. The descriptor\ntakes advantage of the invariant location detection in the scale space theory\nand employs a high level shape encoding scheme to capture invariant local\nfeatures of events. Based on this descriptor, a scale-invariant classifier with\n\"R\" metric (SIC-R) is designed to recognize multi-scale events of human\nactivities. The R metric combines the number of matches of keypoint in scale\nspace with the Dynamic Time Warping score. SICR is tested on various types of\n1-D sensors data from passive infrared, accelerometer and seismic sensors with\nmore than 90% classification accuracy.", 
    "link": "http://arxiv.org/pdf/1105.5675v1", 
    "arxiv-id": "1105.5675v1"
},{
    "category": "cs.MM", 
    "author": "R\u00e9mi M\u00e9gret", 
    "title": "Nested Graph Words for Object Recognition", 
    "publish": "2011-06-14T14:43:02Z", 
    "summary": "In this paper, we propose a new, scalable approach for the task of object\nbased image search or object recognition. Despite the very large literature\nexisting on the scalability issues in CBIR in the sense of retrieval\napproaches, the scalability of media and scalability of features remain an\nissue. In our work we tackle the problem of scalability and structural\norganization of features. The proposed features are nested local graphs built\nupon sets of SURF feature points with Delaunay triangulation. A\nBag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth\nto a Bag-of-Graph-Words representation. The nested nature of the descriptors\nconsists in scaling from trivial Delaunay graphs - isolated feature points - by\nincreasing the number of nodes layer by layer up to graphs with maximal number\nof nodes. For each layer of graphs its proper visual dictionary is built. The\nexperiments conducted on the SIVAL data set reveal that the graph features at\ndifferent layers exhibit complementary performances on the same content. The\nnested approach, the combination of all existing layers, yields significant\nimprovement of the object recognition performance compared to single level\napproaches.", 
    "link": "http://arxiv.org/pdf/1106.2729v2", 
    "arxiv-id": "1106.2729v2"
},{
    "category": "cs.NI", 
    "author": "Wilhelm Wimmreuter", 
    "title": "SIP APIs for Voice and Video Communications on the Web", 
    "publish": "2011-06-30T18:40:48Z", 
    "summary": "Existing standard protocols for the web and Internet telephony fail to\ndeliver real-time interactive communication from within a web browser. In\nparticular, the client-server web protocol over reliable TCP is not always\nsuitable for end-to-end low latency media path needed for interactive voice and\nvideo communication. To solve this, we compare the available platform options\nusing the existing technologies such as modifying the web programming language\nand protocol, using an existing web browser plugin, and a separate host\nresident application that the web browser can talk to. We argue that using a\nseparate application as an adaptor is a promising short term as well as\nlong-term strategy for voice and video communications on the web. Our project\naims at developing the open technology and sample implementations for web-based\nreal-time voice and video communication applications. We describe the\narchitecture of our project including (1) a RESTful web communication API over\nHTTP inspired by SIP message flows, (2) a web-friendly set of metadata for\nsession description, and (3) an UDP-based end-to-end media path. All other\ntelephony functions reside in the web application itself and/or in web feature\nservers. The adaptor approach allows us to easily add new voice and video\ncodecs and NAT traversal technologies such as Host Identity Protocol. We want\nto make web-based communication accessible to millions of web developers,\nmaximize the end user experience and security, and preserve the huge global\ninvestment in and experience from SIP systems while adhering to web standards\nand development tools as much as possible. We have created an open source\nprototype that allows you to freely use the conference application by directing\na browser to the conference URL.", 
    "link": "http://arxiv.org/pdf/1106.6333v1", 
    "arxiv-id": "1106.6333v1"
},{
    "category": "cs.NI", 
    "author": "Carol Davids", 
    "title": "Flash-based Audio and Video Communication in the Cloud", 
    "publish": "2011-06-30T20:06:50Z", 
    "summary": "Internet telephony and multimedia communication protocols have matured over\nthe last fifteen years. Recently, the web is evolving as a popular platform for\neverything we do on the Internet including email, text chat, voice calls,\ndiscussions, enterprise apps and multi-party collaboration. Unfortunately,\nthere is a disconnect between web and traditional Internet telephony protocols\nas they have ignored the constraints and requirements of each other.\nConsequently, the Flash Player is being used as a web browser plugin by many\ndevelopers for web-based voice and video calls. We describe the challenges of\nvideo communication using a web browser, present a simple API using a Flash\nPlayer application, show how it supports wide range of web communication\nscenarios in the cloud, and describe how it can interoperate with Session\nInitiation Protocol (SIP)-based systems. We describe both the advantages and\nchallenges of Flash Player based communication applications. The presented API\ncould guide future work on communication-related web protocol extensions.", 
    "link": "http://arxiv.org/pdf/1107.0011v1", 
    "arxiv-id": "1107.0011v1"
},{
    "category": "cs.GR", 
    "author": "Francesco Pagano", 
    "title": "A Framework for Designing 3D Virtual Environments", 
    "publish": "2011-07-04T17:51:00Z", 
    "summary": "The process of design and development of virtual environments can be\nsupported by tools and frameworks, to save time in technical aspects and\nfocusing on the content. In this paper we present an academic framework which\nprovides several levels of abstraction to ease this work. It includes\nstate-of-the-art components we devised or integrated adopting open-source\nsolutions in order to face specific problems. Its architecture is modular and\ncustomizable, the code is open-source.", 
    "link": "http://arxiv.org/pdf/1107.0690v1", 
    "arxiv-id": "1107.0690v1"
},{
    "category": "cs.MM", 
    "author": "Radu Arsinte", 
    "title": "Study of a Hybrid - Analog TV and Ethernet- Home Data Link using a   Coaxial Cable", 
    "publish": "2011-07-12T09:21:37Z", 
    "summary": "The paper presents an implementation and compatibility tests of a simple home\nnetwork implemented in a nonconventional manner using a CATV coaxial cable.\nReusing the cable, normally designated to supply RF modulated TV signals from\ncable TV networks, makes possible to add data services as well. A short\npresentation of the technology is given with an investigation of the main\nperformances obtained using this technique. The measurements revealed that this\nsimple solution makes possible to have both TV and data services with\nperformances close to traditional home data services: cable modems or ADSL,\nwith minimal investments. This technology keeps also open the possibility for\nfuture improvements of the network: DVB-C or Data via Cable Modems.", 
    "link": "http://arxiv.org/pdf/1107.2222v1", 
    "arxiv-id": "1107.2222v1"
},{
    "category": "cs.NI", 
    "author": "Radu Arsinte", 
    "title": "Aspects of Entertainment Distribution in an Intelligent Home Environment", 
    "publish": "2011-07-13T17:47:28Z", 
    "summary": "The paper presents an implementation and tests of a simple home entertainment\ndistribution architecture (server + multiple clients) implemented using two\nconventional cabling architectures: CATV coaxial cable and conventional\nEthernet. This architecture is created taking into account the \"Home gateway\"\nconcept present in most attempts to solve the problem of the \"Intelligent\nhome\". A short presentation of the experimental is given with an investigation\nof the main performances obtained using this architecture. The experiments\nrevealed that this simple solution makes possible to have entertainment and\ndata services with performances close to traditional data services in a\ncost-effective architecture", 
    "link": "http://arxiv.org/pdf/1107.2615v1", 
    "arxiv-id": "1107.2615v1"
},{
    "category": "cs.MM", 
    "author": "Ramesh Jain", 
    "title": "Label-Specific Training Set Construction from Web Resource for Image   Annotation", 
    "publish": "2011-07-14T15:52:21Z", 
    "summary": "Recently many research efforts have been devoted to image annotation by\nleveraging on the associated tags/keywords of web images as training labels. A\nkey issue to resolve is the relatively low accuracy of the tags. In this paper,\nwe propose a novel semi-automatic framework to construct a more accurate and\neffective training set from these web media resources for each label that we\nwant to learn. Experiments conducted on a real-world dataset demonstrate that\nthe constructed training set can result in higher accuracy for image\nannotation.", 
    "link": "http://arxiv.org/pdf/1107.2859v1", 
    "arxiv-id": "1107.2859v1"
},{
    "category": "cs.CR", 
    "author": "Wojciech Mazurczyk", 
    "title": "Lost Audio Packets Steganography: The First Practical Evaluation", 
    "publish": "2011-07-20T19:16:37Z", 
    "summary": "This paper presents first experimental results for an IP telephony-based\nsteganographic method called LACK (Lost Audio PaCKets steganography). This\nmethod utilizes the fact that in typical multimedia communication protocols\nlike RTP (Real-Time Transport Protocol), excessively delayed packets are not\nused for the reconstruction of transmitted data at the receiver, i.e. these\npackets are considered useless and discarded. The results presented in this\npaper were obtained basing on a functional LACK prototype and show the method's\nimpact on the quality of voice transmission. Achievable steganographic\nbandwidth for the different IP telephony codecs is also calculated.", 
    "link": "http://arxiv.org/pdf/1107.4076v1", 
    "arxiv-id": "1107.4076v1"
},{
    "category": "cs.DC", 
    "author": "Toni Anwar", 
    "title": "Optimization and Evaluation of a Multimedia Streaming Service on Hybrid   Telco cloud", 
    "publish": "2011-09-07T21:34:26Z", 
    "summary": "With recent developments in cloud computing, a paradigm shift from rather\nstatic deployment of resources to more dynamic, on-demand practices means more\nflexibility and better utilization of resources. This demands new ways to\nefficiently configure networks.\n  In this paper, we will characterize a class of competitive cloud services\nthat telecom operators could provide based on the characteristics of telecom\ninfrastructure through an applicable streaming service architecture. Then, we\nwill model this architecture as a cost-based mathematic model. This model\nprovides a tool to evaluate and compare the cost of software services for\ndifferent telecom network topologies and deployment strategies. Additionally,\nwith each topology it acts as a means to characterize the deployment solution\nthat yields the lowest resource usage over the entire network. These\napplications are illustrated through numerical analysis. Finally, a\nproof-of-concept prototype is deployed to shows dynamic properties of the\nservice in the architecture and the model above.", 
    "link": "http://arxiv.org/pdf/1109.1583v1", 
    "arxiv-id": "1109.1583v1"
},{
    "category": "cs.MM", 
    "author": "Mohammad Sadegh Talebi", 
    "title": "Content-Aware Rate Control for Video Transmission with Buffer   Constraints in Multipath Networks", 
    "publish": "2011-09-30T15:26:30Z", 
    "summary": "Being an integral part of the network traffic, nowadays it's vital to design\nrobust mechanisms to provide QoS for multimedia applications. The main goal of\nthis paper is to provide an efficient solution to support content-aware video\ntransmission mechanism with buffer underflow avoidance at the receiver in\nmultipath networks. Towards this, we introduce a content-aware time-varying\nutility function, where the quality impacts of video content is incorporated\ninto its definition. Using the proposed utility function, we formulate a\nmultipath Dynamic Network Utility Maximization (DNUM) problem for the rate\nallocation of video streams, where it takes into account QoS demand of video\nstreams in terms of buffer underflow avoidance. Finally, using primal-dual\nmethod, we propose a distributed solution that optimally allocates the shared\nbandwidth to video streams. The numerical examples demonstrate the efficacy of\nthe proposed content-aware rate allocation algorithm for video sources in both\nsingle and multiple path network models.", 
    "link": "http://arxiv.org/pdf/1109.6851v2", 
    "arxiv-id": "1109.6851v2"
},{
    "category": "cs.IR", 
    "author": "P. M. Kamade", 
    "title": "Video OCR for Video Indexing", 
    "publish": "2011-09-30T15:51:12Z", 
    "summary": "Video OCR is a technique that can greatly help to locate the topics of\ninterest in video via the automatic extraction and reading of captions and\nannotations. Text in video can provide key indexing information. Recognizing\nsuch text for search application is critical. Major difficult problem for\ncharacter recognition for videos is degraded and deformated characters, low\nresolution characters or very complex background. To tackle the problem\npreprocessing on text image plays vital role. Most of the OCR engines are\nworking on the binary image so to find a better binarization procedure for\nimage to get a desired result is important.Accurate binarization process\nminimizes the error rate of video OCR.", 
    "link": "http://arxiv.org/pdf/1109.6862v1", 
    "arxiv-id": "1109.6862v1"
},{
    "category": "cs.NI", 
    "author": "Jaydip Sen", 
    "title": "Cross-Layer Protocols for Multimedia Communications over Wireless   Networks", 
    "publish": "2011-10-02T03:35:20Z", 
    "summary": "In the last few years, the Internet throughput, usage and reliability have\nincreased almost exponentially. The introduction of broadband wireless mobile\nad hoc networks (MANETs) and cellular networks together with increased\ncomputational power have opened the door for a new breed of applications to be\ncreated, namely real-time multimedia applications. Delivering real-time\nmultimedia traffic over a complex network like the Internet is a particularly\nchallenging task since these applications have strict quality-of-service (QoS)\nrequirements on bandwidth, delay, and delay jitter. Traditional Internet\nprotocol (IP)-based best effort service is not able to meet these stringent\nrequirements. The time-varying nature of wireless channels and resource\nconstrained wireless devices make the problem even more difficult. To improve\nperceived media quality by end users over wireless Internet, QoS supports can\nbe addressed in different layers, including application layer, transport layer\nand link layer. Cross layer design is a well-known approach to achieve this\nadaptation. In cross-layer design, the challenges from the physical wireless\nmedium and the QoS-demands from the applications are taken into account so that\nthe rate, power, and coding at the physical (PHY) layer can adapted to meet the\nrequirements of the applications given the current channel and network\nconditions. A number of propositions for cross-layer designs exist in the\nliterature. In this chapter, an extensive review has been made on these\ncross-layer architectures that combine the application-layer, transport layer\nand the link layer controls. Particularly, the issues like channel estimation\ntechniques, adaptive controls at the application and link layers for energy\nefficiency, priority based scheduling, transmission rate control at the\ntransport layer, and adaptive automatic repeat request (ARQ) are discussed in\ndetail.", 
    "link": "http://arxiv.org/pdf/1110.0147v2", 
    "arxiv-id": "1110.0147v2"
},{
    "category": "cs.NI", 
    "author": "Walid Dabbous", 
    "title": "Network Characteristics of Video Streaming Traffic", 
    "publish": "2011-11-03T19:02:51Z", 
    "summary": "Video streaming represents a large fraction of Internet traffic.\nSurprisingly, little is known about the network characteristics of this\ntraffic. In this paper, we study the network characteristics of the two most\npopular video streaming services, Netflix and YouTube. We show that the\nstreaming strategies vary with the type of the application (Web browser or\nnative mobile application), and the type of container (Silverlight, Flash, or\nHTML5) used for video streaming. In particular, we identify three different\nstreaming strategies that produce traffic patterns from non-ack clocked ON-OFF\ncycles to bulk TCP transfer. We then present an analytical model to study the\npotential impact of these streaming strategies on the aggregate traffic and\nmake recommendations accordingly.", 
    "link": "http://arxiv.org/pdf/1111.0948v1", 
    "arxiv-id": "1111.0948v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Using Transcoding for Hidden Communication in IP Telephony", 
    "publish": "2011-11-04T21:07:49Z", 
    "summary": "The paper presents a new steganographic method for IP telephony called\nTranSteg (Transcoding Steganography). Typically, in steganographic\ncommunication it is advised for covert data to be compressed in order to limit\nits size. In TranSteg it is the overt data that is compressed to make space for\nthe steganogram. The main innovation of TranSteg is to, for a chosen voice\nstream, find a codec that will result in a similar voice quality but smaller\nvoice payload size than the originally selected. Then, the voice stream is\ntranscoded. At this step the original voice payload size is intentionally\nunaltered and the change of the codec is not indicated. Instead, after placing\nthe transcoded voice payload, the remaining free space is filled with hidden\ndata. TranSteg proof of concept implementation was designed and developed. The\nobtained experimental results are enclosed in this paper. They prove that the\nproposed method is feasible and offers a high steganographic bandwidth.\nTranSteg detection is difficult to perform when performing inspection in a\nsingle network localisation.", 
    "link": "http://arxiv.org/pdf/1111.1250v1", 
    "arxiv-id": "1111.1250v1"
},{
    "category": "cs.NI", 
    "author": "Yucel Altunbasak", 
    "title": "Channel Reordering with Time-shifted Streams to Improve Channel Change   Latency in IPTV Networks", 
    "publish": "2011-11-16T03:39:19Z", 
    "summary": "In IPTV networks, channel change latency is considered as a major obstacle in\nachieving broadcast-level quality video delivery. Because of the bandwidth\nlimitations observed at the client side, users typically have access to a\nlimited number of channels. As a result, channel change requests oftentimes\nneed to go through the network, thereby leading to significant delays. In this\npaper, we address this problem by proposing a resource-efficient time-shifted\nchannel reordering mechanism to minimize the channel change latency. The\nproposed framework exploits the differing key-frame delivery times for the\nadjacent sessions to dynamically arrange the switching order during the surfing\nperiods. The simulation results show that, with the proposed framework, more\nthan 50% improvement can be achieved in channel change latency without\nintroducing any overhead in the network.", 
    "link": "http://arxiv.org/pdf/1111.3711v2", 
    "arxiv-id": "1111.3711v2"
},{
    "category": "cs.CV", 
    "author": "Soumen Kanrar", 
    "title": "Enhancement of Image Resolution by Binarization", 
    "publish": "2011-11-21T09:47:17Z", 
    "summary": "Image segmentation is one of the principal approaches of image processing.\nThe choice of the most appropriate Binarization algorithm for each case proved\nto be a very interesting procedure itself. In this paper, we have done the\ncomparison study between the various algorithms based on Binarization\nalgorithms and propose a methodologies for the validation of Binarization\nalgorithms. In this work we have developed two novel algorithms to determine\nthreshold values for the pixels value of the gray scale image. The performance\nestimation of the algorithm utilizes test images with, the evaluation metrics\nfor Binarization of textual and synthetic images. We have achieved better\nresolution of the image by using the Binarization method of optimum\nthresholding techniques.", 
    "link": "http://arxiv.org/pdf/1111.4800v1", 
    "arxiv-id": "1111.4800v1"
},{
    "category": "cs.NI", 
    "author": "Chun Tung Chou", 
    "title": "A Frame Rate Optimization Framework For Improving Continuity In Video   Streaming", 
    "publish": "2011-11-22T13:15:33Z", 
    "summary": "This paper aims to reduce the prebuffering requirements, while maintaining\ncontinuity, for video streaming. Current approaches do this by making use of\nadaptive media playout (AMP) to reduce the playout rate. However, this\nintroduces playout distortion to the viewers and increases the viewing latency.\nWe approach this by proposing a frame rate optimization framework that adjusts\nboth the encoder frame generation rate and the decoder playout frame rate.\nFirstly, we model this problem as the joint adjustment of the encoder frame\ngeneration interval and the decoder playout frame interval. This model is used\nwith a discontinuity penalty virtual buffer to track the accumulated difference\nbetween the receiving frame interval and the playout frame interval. We then\napply Lyapunov optimization to the model to systematically derive a pair of\ndecoupled optimization policies. We show that the occupancy of the\ndiscontinuity penalty virtual buffer is correlated to the video discontinuity\nand that this framework produces a very low playout distortion in addition to a\nsignificant reduction in the prebuffering requirements compared to existing\napproaches. Secondly, we introduced a delay constraint into the framework by\nusing a delay accumulator virtual buffer. Simulation results show that the the\ndelay constrained framework provides a superior tradeoff between the video\nquality and the delay introduced compared to the existing approach. Finally, we\nanalyzed the impact of delayed feedback between the receiver and the sender on\nthe optimization policies. We show that the delayed feedbacks have a minimal\nimpact on the optimization policies.", 
    "link": "http://arxiv.org/pdf/1111.5189v1", 
    "arxiv-id": "1111.5189v1"
},{
    "category": "cs.CV", 
    "author": "Pascal Frossard", 
    "title": "Distributed Representation of Geometrically Correlated Images with   Compressed Linear Measurements", 
    "publish": "2011-11-23T15:54:23Z", 
    "summary": "This paper addresses the problem of distributed coding of images whose\ncorrelation is driven by the motion of objects or positioning of the vision\nsensors. It concentrates on the problem where images are encoded with\ncompressed linear measurements. We propose a geometry-based correlation model\nin order to describe the common information in pairs of images. We assume that\nthe constitutive components of natural images can be captured by visual\nfeatures that undergo local transformations (e.g., translation) in different\nimages. We first identify prominent visual features by computing a sparse\napproximation of a reference image with a dictionary of geometric basis\nfunctions. We then pose a regularized optimization problem to estimate the\ncorresponding features in correlated images given by quantized linear\nmeasurements. The estimated features have to comply with the compressed\ninformation and to represent consistent transformation between images. The\ncorrelation model is given by the relative geometric transformations between\ncorresponding features. We then propose an efficient joint decoding algorithm\nthat estimates the compressed images such that they stay consistent with both\nthe quantized measurements and the correlation model. Experimental results show\nthat the proposed algorithm effectively estimates the correlation between\nimages in multi-view datasets. In addition, the proposed algorithm provides\neffective decoding performance that compares advantageously to independent\ncoding solutions as well as state-of-the-art distributed coding schemes based\non disparity learning.", 
    "link": "http://arxiv.org/pdf/1111.5612v1", 
    "arxiv-id": "1111.5612v1"
},{
    "category": "cs.CR", 
    "author": "Subhasis Chaudhuri", 
    "title": "Estimation of the Embedding Capacity in Pixel-pair based Watermarking   Schemes", 
    "publish": "2011-11-24T00:04:50Z", 
    "summary": "Estimation of the Embedding capacity is an important problem specifically in\nreversible multi-pass watermarking and is required for analysis before any\nimage can be watermarked. In this paper, we propose an efficient method for\nestimating the embedding capacity of a given cover image under multi-pass\nembedding, without actually embedding the watermark. We demonstrate this for a\nclass of reversible watermarking schemes which operate on a disjoint group of\npixels, specifically for pixel pairs. The proposed algorithm iteratively\nupdates the co-occurrence matrix at every stage, to estimate the multi-pass\nembedding capacity, and is much more efficient vis-a-vis actual watermarking.\nWe also suggest an extremely efficient, pre-computable tree based\nimplementation which is conceptually similar to the co-occurrence based method,\nbut provides the estimates in a single iteration, requiring a complexity akin\nto that of single pass capacity estimation. We also provide bounds on the\nembedding capacity. We finally show how our method can be easily used on a\nnumber of watermarking algorithms and specifically evaluate the performance of\nour algorithms on the benchmark watermarking schemes of Tian [11] and Coltuc\n[6].", 
    "link": "http://arxiv.org/pdf/1111.5653v1", 
    "arxiv-id": "1111.5653v1"
},{
    "category": "cs.NI", 
    "author": "Manjaiah D. H", 
    "title": "Peer-to-Peer Live Streaming and Video On Demand Design Issues and its   Challenges", 
    "publish": "2011-11-29T09:06:57Z", 
    "summary": "Peer-to-Peer Live streaming and Video on Demand is the most popular media\napplications over the Internet in recent years. These systems reduce the load\non the server and provide a scalable content distribution. A new paradigm of\nP2P network collaborates to build large distributed video applications on\nexisting networks .But, the problem of designing the system are at par with the\nP2P media streaming, live and Video on demand systems. Hence a comprehensive\ndesign comparison is needed to build such kind of system architecture.\nTherefore, in this paper we elaborately studied the traditional approaches for\nP2P streaming architectures, and its critical design issues, as well as\npracticable challenges. Thus, our studies in this paper clearly point the\ntangible design issues and its challenges, and other intangible issues for\nproviding P2P VoD services.", 
    "link": "http://arxiv.org/pdf/1111.6735v1", 
    "arxiv-id": "1111.6735v1"
},{
    "category": "cs.MM", 
    "author": "Nedunchezhian R", 
    "title": "Recent Trends and Research Issues in Video Association Mining", 
    "publish": "2011-12-09T08:16:03Z", 
    "summary": "With the ever-growing digital libraries and video databases, it is\nincreasingly important to understand and mine the knowledge from video database\nautomatically. Discovering association rules between items in a large video\ndatabase plays a considerable role in the video data mining research areas.\nBased on the research and development in the past years, application of\nassociation rule mining is growing in different domains such as surveillance,\nmeetings, broadcast news, sports, archives, movies, medical data, as well as\npersonal and online media collections. The purpose of this paper is to provide\ngeneral framework of mining the association rules from video database. This\narticle is also represents the research issues in video association mining\nfollowed by the recent trends.", 
    "link": "http://arxiv.org/pdf/1112.2040v1", 
    "arxiv-id": "1112.2040v1"
},{
    "category": "cs.MM", 
    "author": "Teoh Suk Kuan", 
    "title": "Steganography Algorithm to Hide Secret Message inside an Image", 
    "publish": "2011-12-13T06:56:25Z", 
    "summary": "In this paper, the authors propose a new algorithm to hide data inside image\nusing steganography technique. The proposed algorithm uses binary codes and\npixels inside an image. The zipped file is used before it is converted to\nbinary codes to maximize the storage of data inside the image. By applying the\nproposed algorithm, a system called Steganography Imaging System (SIS) is\ndeveloped. The system is then tested to see the viability of the proposed\nalgorithm. Various sizes of data are stored inside the images and the PSNR\n(Peak signal-to-noise ratio) is also captured for each of the images tested.\nBased on the PSNR value of each images, the stego image has a higher PSNR\nvalue. Hence this new steganography algorithm is very efficient to hide the\ndata inside the image.", 
    "link": "http://arxiv.org/pdf/1112.2809v1", 
    "arxiv-id": "1112.2809v1"
},{
    "category": "cs.LG", 
    "author": "Shiwen Deng", 
    "title": "Online Learning for Classification of Low-rank Representation Features   and Its Applications in Audio Segment Classification", 
    "publish": "2011-12-19T05:29:18Z", 
    "summary": "In this paper, a novel framework based on trace norm minimization for audio\nsegment is proposed. In this framework, both the feature extraction and\nclassification are obtained by solving corresponding convex optimization\nproblem with trace norm regularization. For feature extraction, robust\nprinciple component analysis (robust PCA) via minimization a combination of the\nnuclear norm and the $\\ell_1$-norm is used to extract low-rank features which\nare robust to white noise and gross corruption for audio segments. These\nlow-rank features are fed to a linear classifier where the weight and bias are\nlearned by solving similar trace norm constrained problems. For this\nclassifier, most methods find the weight and bias in batch-mode learning, which\nmakes them inefficient for large-scale problems. In this paper, we propose an\nonline framework using accelerated proximal gradient method. This framework has\na main advantage in memory cost. In addition, as a result of the regularization\nformulation of matrix classification, the Lipschitz constant was given\nexplicitly, and hence the step size estimation of general proximal gradient\nmethod was omitted in our approach. Experiments on real data sets for\nlaugh/non-laugh and applause/non-applause classification indicate that this\nnovel framework is effective and noise robust.", 
    "link": "http://arxiv.org/pdf/1112.4243v1", 
    "arxiv-id": "1112.4243v1"
},{
    "category": "cs.CV", 
    "author": "MohammadReza Keyvanpour", 
    "title": "A New Color Feature Extraction Method Based on Dynamic Color   Distribution Entropy of Neighborhoods", 
    "publish": "2012-01-08T23:36:19Z", 
    "summary": "One of the important requirements in image retrieval, indexing,\nclassification, clustering and etc. is extracting efficient features from\nimages. The color feature is one of the most widely used visual features. Use\nof color histogram is the most common way for representing color feature. One\nof disadvantage of the color histogram is that it does not take the color\nspatial distribution into consideration. In this paper dynamic color\ndistribution entropy of neighborhoods method based on color distribution\nentropy is presented, which effectively describes the spatial information of\ncolors. The image retrieval results in compare to improved color distribution\nentropy show the acceptable efficiency of this approach.", 
    "link": "http://arxiv.org/pdf/1201.3337v1", 
    "arxiv-id": "1201.3337v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Influence of Speech Codecs Selection on Transcoding Steganography", 
    "publish": "2012-01-30T14:16:35Z", 
    "summary": "The typical approach to steganography is to compress the covert data in order\nto limit its size, which is reasonable in the context of a limited\nsteganographic bandwidth. TranSteg (Trancoding Steganography) is a new IP\ntelephony steganographic method that was recently proposed that offers high\nsteganographic bandwidth while retaining good voice quality. In TranSteg,\ncompression of the overt data is used to make space for the steganogram. In\nthis paper we focus on analyzing the influence of the selection of speech\ncodecs on hidden transmission performance, that is, which codecs would be the\nmost advantageous ones for TranSteg. Therefore, by considering the codecs which\nare currently most popular for IP telephony we aim to find out which codecs\nshould be chosen for transcoding to minimize the negative influence on voice\nquality while maximizing the obtained steganographic bandwidth.", 
    "link": "http://arxiv.org/pdf/1201.6218v1", 
    "arxiv-id": "1201.6218v1"
},{
    "category": "cs.CR", 
    "author": "J. K. Mandal", 
    "title": "A Frequency Domain Steganography using Z Transform (FDSZT)", 
    "publish": "2012-02-20T08:11:11Z", 
    "summary": "Image steganography is art of hiding information onto the cover image. In\nthis proposal a transformed domain based gray scale image authentication/data\nhiding technique using Z transform (ZT) termed as FDSZT, has been proposed.\nZTransform is applied on 2x2 masks of the source image in row major order to\ntransform original sub image (cover image) block to its corresponding frequency\ndomain. One bit of the hidden image is embedded in each mask of the source\nimage onto the fourth LSB of transformed coefficient based on median value of\nthe mask. A delicate handle has also been performed as post embedding operation\nfor proper decoding. Stego sub image is obtained through a reverse transform as\nfinal step of embedding in a mask. During the process of embedding, dimension\nof the hidden image followed by the content of the message/hidden image are\nembedded. Reverse process is followed during decoding. High PSNR obtained for\nvarious images conform the quality of invisible watermark of FDSZT.", 
    "link": "http://arxiv.org/pdf/1202.4245v1", 
    "arxiv-id": "1202.4245v1"
},{
    "category": "cs.MM", 
    "author": "Munchurl Kim", 
    "title": "Real-time detection and tracking of multiple objects with partial   decoding in H.264/AVC bitstream domain", 
    "publish": "2012-02-21T20:42:17Z", 
    "summary": "In this paper, we show that we can apply probabilistic spatiotemporal\nmacroblock filtering (PSMF) and partial decoding processes to effectively\ndetect and track multiple objects in real time in H.264|AVC bitstreams with\nstationary background. Our contribution is that our method cannot only show\nfast processing time but also handle multiple moving objects that are\narticulated, changing in size or internally have monotonous color, even though\nthey contain a chaotic set of non-homogeneous motion vectors inside. In\naddition, our partial decoding process for H.264|AVC bitstreams enables to\nimprove the accuracy of object trajectories and overcome long occlusion by\nusing extracted color information.", 
    "link": "http://arxiv.org/pdf/1202.4743v1", 
    "arxiv-id": "1202.4743v1"
},{
    "category": "cs.MM", 
    "author": "G. R. Sinha", 
    "title": "A new hybrid jpeg image compression scheme using symbol reduction   technique", 
    "publish": "2012-02-22T15:43:56Z", 
    "summary": "Lossy JPEG compression is a widely used compression technique. Normally the\nJPEG standard technique uses three process mapping reduces interpixel\nredundancy, quantization, which is lossy process and entropy encoding, which is\nconsidered lossless process. In this paper, a new technique has been proposed\nby combining the JPEG algorithm and Symbol Reduction Huffman technique for\nachieving more compression ratio. The symbols reduction technique reduces the\nnumber of symbols by combining together to form a new symbol. As a result of\nthis technique the number of Huffman code to be generated also reduced. It is\nsimple fast and easy to implement. The result shows that the performance of\nstandard JPEG method can be improved by proposed method. This hybrid approach\nachieves about 20% more compression ratio than the Standard JPEG.", 
    "link": "http://arxiv.org/pdf/1202.4943v1", 
    "arxiv-id": "1202.4943v1"
},{
    "category": "cs.NI", 
    "author": "Soumen Kanrar", 
    "title": "Analysis and implementation of the Large Scale Video-on-Demand System", 
    "publish": "2012-02-23T06:25:16Z", 
    "summary": "Next Generation Network (NGN) provides multimedia services over broadband\nbased networks, which supports high definition TV (HDTV), and DVD quality\nvideo-on-demand content. The video services are thus seen as merging mainly\nthree areas such as computing, communication, and broadcasting. It has numerous\nadvantages and more exploration for the large-scale deployment of\nvideo-on-demand system is still needed. This is due to its economic and design\nconstraints. It's need significant initial investments for full service\nprovision. This paper presents different estimation for the different\ntopologies and it require efficient planning for a VOD system network. The\nmethodology investigates the network bandwidth requirements of a VOD system\nbased on centralized servers, and distributed local proxies. Network traffic\nmodels are developed to evaluate the VOD system's operational bandwidth\nrequirements for these two network architectures. This paper present an\nefficient estimation of the of the bandwidth requirement for the different\narchitectures.", 
    "link": "http://arxiv.org/pdf/1202.5094v1", 
    "arxiv-id": "1202.5094v1"
},{
    "category": "cs.CV", 
    "author": "S. Remya", 
    "title": "Enhancement Techniques for Local Content Preservation and Contrast   Improvement in Images", 
    "publish": "2012-03-08T15:12:34Z", 
    "summary": "There are several images that do not have uniform brightness which pose a\nchallenging problem for image enhancement systems. As histogram equalization\nhas been successfully used to correct for uniform brightness problems, a\nhistogram equalization method that utilizes human visual system based\nthresholding(human vision thresholding) as well as logarithmic processing\ntechniques were introduced later . But these methods are not good for\npreserving the local content of the image which is a major factor for various\nimages like medical and aerial images. Therefore new method is proposed here.\nThis method is referred as \"Human vision thresholding with enhancement\ntechnique for dark blurred images for local content preservation\". It uses\nhuman vision thresholding together with an existing enhancement method for dark\nblurred images. Furthermore a comparative study with another method for local\ncontent preservation is done which is further extended to make it suitable for\ncontrast improvement. Experimental results shows that the proposed methods\noutperforms the former existing methods in preserving the local content for\nstandard images, medical and aerial images.", 
    "link": "http://arxiv.org/pdf/1203.1823v1", 
    "arxiv-id": "1203.1823v1"
},{
    "category": "cs.CR", 
    "author": "Aroop Dasgupta", 
    "title": "Experimenting with the Novel Approaches in Text Steganography", 
    "publish": "2012-03-16T09:28:58Z", 
    "summary": "As is commonly known, the steganographic algorithms employ images, audio,\nvideo or text files as the medium to ensure hidden exchange of information\nbetween multiple contenders to protect the data from the prying eyes. However,\nusing text as the target medium is relatively difficult as compared to the\nother target media, because of the lack of available redundant information in a\ntext file. In this paper, in the backdrop of the limitations in the prevalent\ntext based steganographic approaches, we propose simple, yet novel approaches\nthat overcome the same. Our approaches are based on combining the random\ncharacter sequence and feature coding methods to hide a character. We also\nanalytically evaluate the approaches based on metrics viz. hiding strength,\ntime overhead and memory overhead entailed. As compared to other methods, we\nbelieve the approaches proposed impart increased randomness and thus aid higher\nsecurity at lower overhead.", 
    "link": "http://arxiv.org/pdf/1203.3644v1", 
    "arxiv-id": "1203.3644v1"
},{
    "category": "cs.CR", 
    "author": "Wojciech Mazurczyk", 
    "title": "VoIP Steganography and Its Detection - A Survey", 
    "publish": "2012-03-20T10:29:14Z", 
    "summary": "Steganography is an ancient art that encompasses various techniques of\ninformation hiding, the aim of which is to secret information into a carrier\nmessage. Steganographic methods are usually aimed at hiding the very existence\nof the communication. Due to the rise in popularity of IP telephony, together\nwith the large volume of data and variety of protocols involved, it is\ncurrently attracting the attention of the research community as a perfect\ncarrier for steganographic purposes. This paper is a survey of the existing\nVoIP steganography (steganophony) methods and their countermeasures.", 
    "link": "http://arxiv.org/pdf/1203.4374v4", 
    "arxiv-id": "1203.4374v4"
},{
    "category": "cs.NI", 
    "author": "Jerome Lacan", 
    "title": "Online multipath convolutional coding for real-time transmission", 
    "publish": "2012-04-06T07:59:27Z", 
    "summary": "Most of multipath multimedia streaming proposals use Forward Error Correction\n(FEC) approach to protect from packet losses. However, FEC does not sustain\nwell burst of losses even when packets from a given FEC block are spread over\nmultiple paths. In this article, we propose an online multipath convolutional\ncoding for real-time multipath streaming based on an on-the-fly coding scheme\ncalled Tetrys. We evaluate the benefits brought out by this coding scheme\ninside an existing FEC multipath load splitting proposal known as Encoded\nMultipath Streaming (EMS). We demonstrate that Tetrys consistently outperforms\nFEC in both uniform and burst losses with EMS scheme. We also propose a\nmodification of the standard EMS algorithm that greatly improves the\nperformance in terms of packet recovery. Finally, we analyze different\nspreading policies of the Tetrys redundancy traffic between available paths and\nobserve that the longer propagation delay path should be preferably used to\ncarry repair packets.", 
    "link": "http://arxiv.org/pdf/1204.1428v1", 
    "arxiv-id": "1204.1428v1"
},{
    "category": "cs.MM", 
    "author": "Manas S", 
    "title": "Genetic Algorithm to Make Persistent Security and Quality of Image in   Steganography from RS Analysis", 
    "publish": "2012-04-12T04:43:08Z", 
    "summary": "Retention of secrecy is one of the significant features during communication\nactivity. Steganography is one of the popular methods to achieve secret\ncommunication between sender and receiver by hiding message in any form of\ncover media such as an audio, video, text, images etc. Least significant bit\nencoding is the simplest encoding method used by many steganography programs to\nhide secret message in 24bit, 8bit colour images and grayscale images.\nSteganalysis is a method of detecting secret message hidden in a cover media\nusing steganography. RS steganalysis is one of the most reliable steganalysis\nwhich performs statistical analysis of the pixels to successfully detect the\nhidden message in an image. However, existing steganography method protects the\ninformation against RS steganalysis in grey scale images. This paper presents a\nsteganography method using genetic algorithm to protect against the RS attack\nin colour images. Stego image is divided into number of blocks. Subsequently,\nwith the implementation of natural evolution on the stego image using genetic\nalgorithm enables to achieve optimized security and image quality.", 
    "link": "http://arxiv.org/pdf/1204.2616v1", 
    "arxiv-id": "1204.2616v1"
},{
    "category": "cs.CV", 
    "author": "Farokh Marvasti", 
    "title": "Compensating Interpolation Distortion by Using New Optimized Modular   Method", 
    "publish": "2012-04-13T20:16:11Z", 
    "summary": "A modular method was suggested before to recover a band limited signal from\nthe sample and hold and linearly interpolated (or, in general, an\nnth-order-hold) version of the regular samples. In this paper a novel approach\nfor compensating the distortion of any interpolation based on modular method\nhas been proposed. In this method the performance of the modular method is\noptimized by adding only some simply calculated coefficients. This approach\ncauses drastic improvement in terms of signal-to-noise ratios with fewer\nmodules compared to the classical modular method. Simulation results clearly\nconfirm the improvement of the proposed method and also its superior robustness\nagainst additive noise.", 
    "link": "http://arxiv.org/pdf/1204.3618v1", 
    "arxiv-id": "1204.3618v1"
},{
    "category": "cs.MM", 
    "author": "Soumen Kanrar", 
    "title": "Image Enhancement with Statistical Estimation", 
    "publish": "2012-05-07T12:49:42Z", 
    "summary": "Contrast enhancement is an important area of research for the image analysis.\nOver the decade, the researcher worked on this domain to develop an efficient\nand adequate algorithm. The proposed method will enhance the contrast of image\nusing Binarization method with the help of Maximum Likelihood Estimation (MLE).\nThe paper aims to enhance the image contrast of bimodal and multi-modal images.\nThe proposed methodology use to collect mathematical information retrieves from\nthe image. In this paper, we are using binarization method that generates the\ndesired histogram by separating image nodes. It generates the enhanced image\nusing histogram specification with binarization method. The proposed method has\nshowed an improvement in the image contrast enhancement compare with the other\nimage.", 
    "link": "http://arxiv.org/pdf/1205.1365v1", 
    "arxiv-id": "1205.1365v1"
},{
    "category": "cs.IR", 
    "author": "Hima Bindu Maringanti", 
    "title": "Architecture for Automated Tagging and Clustering of Song Files   According to Mood", 
    "publish": "2012-06-12T10:28:11Z", 
    "summary": "Music is one of the basic human needs for recreation and entertainment. As\nsong files are digitalized now a days, and digital libraries are expanding\ncontinuously, which makes it difficult to recall a song. Thus need of a new\nclassification system other than genre is very obvious and mood based\nclassification system serves the purpose very well. In this paper we will\npresent a well-defined architecture to classify songs into different mood-based\ncategories, using audio content analysis, affective value of song lyrics to map\na song onto a psychological-based emotion space and information from online\nsources. In audio content analysis we will use music features such as\nintensity, timbre and rhythm including their subfeatures to map music in a\n2-Dimensional emotional space. In lyric based classification 1-Dimensional\nemotional space is used. Both the results are merged onto a 2-Dimensional\nemotional space, which will classify song into a particular mood category.\nFinally clusters of mood based song files are formed and arranged according to\ndata acquired from various Internet sources.", 
    "link": "http://arxiv.org/pdf/1206.2484v1", 
    "arxiv-id": "1206.2484v1"
},{
    "category": "cs.CR", 
    "author": "Mehdi Khalili", 
    "title": "A Novel Effective, Secure and Robust CDMA Digital Image Watermarking in   YUV Color Space Using DWT2", 
    "publish": "2012-06-19T16:30:49Z", 
    "summary": "This paper is allocated to CDMA digital images watermarking for ownership\nverification and image authentication applications, which for more security,\nwatermark W is converted to a sequence and then a random binary sequence R of\nsize n is adopted to encrypt the watermark; where n is the size of the\nwatermark. This adopting process uses a pseudo-random number generator to\ndetermine the pixel to be used on a given key. After converting the host image\nto YUV color space and then wavelet decomposition of Y channel, this adopted\nwatermark is embedded into the selected subbands coefficients of Y channel\nusing the correlation properties of additive pseudo- random noise patterns. The\nexperimental results show that the proposed approach provides extra\nimperceptibility, security and robustness against JPEG compression and\ndifferent noises attacks compared to the similar proposed methods. Moreover,\nthe proposed approach has no need of the original image to extract watermarks.", 
    "link": "http://arxiv.org/pdf/1206.4256v1", 
    "arxiv-id": "1206.4256v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Joint Reconstruction of Multi-view Compressed Images", 
    "publish": "2012-06-19T20:16:04Z", 
    "summary": "The distributed representation of correlated multi-view images is an\nimportant problem that arise in vision sensor networks. This paper concentrates\non the joint reconstruction problem where the distributively compressed\ncorrelated images are jointly decoded in order to improve the reconstruction\nquality of all the compressed images. We consider a scenario where the images\ncaptured at different viewpoints are encoded independently using common coding\nsolutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among\ndifferent cameras. A central decoder first estimates the underlying correlation\nmodel from the independently compressed images which will be used for the joint\nsignal recovery. The joint reconstruction is then cast as a constrained convex\noptimization problem that reconstructs total-variation (TV) smooth images that\ncomply with the estimated correlation model. At the same time, we add\nconstraints that force the reconstructed images to be consistent with their\ncompressed versions. We show by experiments that the proposed joint\nreconstruction scheme outperforms independent reconstruction in terms of image\nquality, for a given target bit rate. In addition, the decoding performance of\nour proposed algorithm compares advantageously to state-of-the-art distributed\ncoding schemes based on disparity learning and on the DISCOVER.", 
    "link": "http://arxiv.org/pdf/1206.4326v1", 
    "arxiv-id": "1206.4326v1"
},{
    "category": "cs.CR", 
    "author": "David Asatryan", 
    "title": "Improved DWT Based Watermarking Using JPEG-YCbCr", 
    "publish": "2012-06-20T14:30:08Z", 
    "summary": "In this paper a blind, Secure, imperceptible and robust watermarking\nalgorithm based on wavelet transform domain is proposed in which for more\nsecurity, the watermark W is converted to a sequence and then a random binary\nsequence R of size n is adopted to encrypt the watermark, where n is the size\nof the watermark image. Afterwards, the encrypted watermark sequence W1 is\ngenerated by executing exclusive-OR operation on W and R. This generated\nwatermark embeds into low frequency selected coefficients of Y channel wavelet\ndecomposition of JPEG-YCbCr using LSB insertion technique. The experimental\nresults show that the proposed algorithm increases the security and\nimperceptibility of watermark and has better robustness against wavelet\ncompression and cropping attacks compared to the earlier work in [1].", 
    "link": "http://arxiv.org/pdf/1206.4513v1", 
    "arxiv-id": "1206.4513v1"
},{
    "category": "cs.CR", 
    "author": "David Asatryan", 
    "title": "Effective Digital Image Watermarking in YCbCr Color Space Accompanied by   Presenting a Novel Technique Using DWT", 
    "publish": "2012-06-20T14:54:26Z", 
    "summary": "In this paper, a quantization based watermark casting and blind watermark\nretrieval algorithm operating in YCbCr color space using discrete wavelet\ntransform (DWT), for ownership verification and image authentication\napplications is implemented. This method uses implicit visual masking by\ninserting watermark bits into only the wavelet coefficients of high magnitude,\nin Y channel of YCbCr color space. A blind watermark retrieval technique that\ncan detect the embedded watermark without the help from the original\nuncorrupted image is devised which is computationally efficient. The new\nwatermarking algorithm combines and adapts various aspects from existing\nwatermarking methods. Experimental results show that the proposed technique to\nembed watermark provides extra imperceptibility and robustness against various\nsignal processing attacks in comparison with the same technique in RGB color\nspace.", 
    "link": "http://arxiv.org/pdf/1206.4520v1", 
    "arxiv-id": "1206.4520v1"
},{
    "category": "cs.CR", 
    "author": "Mehdi Khalili", 
    "title": "A Comparison between Digital Image Watermarking in Tow Different Color   Spaces Using DWT2", 
    "publish": "2012-06-20T19:11:10Z", 
    "summary": "A novel digital watermarking for ownership verification and image\nauthentication applications using discrete wavelet transform (DWT) is proposed\nin this paper. Most previous proposed watermarking algorithms embed sequences\nof random numbers as watermarks. Here binary images are taken as watermark for\nembedding. In the proposed approach, the host image is converted into the YCbCr\ncolor space and then its Y channel decomposed into wavelet coefficients. The\nselected approximation coefficients are quantized and then their four least\nsignificant bits of the quantized coefficients are replaced by the watermark\nusing LSB insertion technique. At last, the watermarked image is synthesized\nfrom the changed and unchanged DWT coefficients. The experiments show that the\nproposed approach provides extra imperceptibility and robustness against\nwavelet compression compared to the traditional embedding methods in RGB color\nspace. Moreover, the proposed approach has no need of the original image to\nextract watermarks.", 
    "link": "http://arxiv.org/pdf/1206.4582v1", 
    "arxiv-id": "1206.4582v1"
},{
    "category": "cs.MM", 
    "author": "Med Salim Bouhlel", 
    "title": "Improvement of ISOM by using filter", 
    "publish": "2012-07-10T08:49:48Z", 
    "summary": "Image compression helps in storing the transmitted data in proficient way by\ndecreasing its redundancy. This technique helps in transferring more digital or\nmultimedia data over internet as it increases the storage space. It is\nimportant to maintain the image quality even if it is compressed to certain\nextent. Depend upon this the image compression is classified into two\ncategories : lossy and lossless image compression. There are many lossy digital\nimage compression techniques exists. Among this Incremental Self Organizing Map\nis a familiar one. The good pictures quality can be retrieved if image\ndenoising technique is used for compression and also provides better\ncompression ratio. Image denoising is an important pre-processing step for many\nimage analysis and computer vision system. It refers to the task of recovering\na good estimate of the true image from a degraded observation without altering\nand changing useful structure in the image such as discontinuities and edges.\nMany approaches have been proposed to remove the noise effectively while\npreserving the original image details and features as much as possible. This\npaper proposes a technique for image compression using Incremental Self\nOrganizing Map (ISOM) with Discret Wavelet Transform (DWT) by applying\nfiltering techniques which play a crucial role in enhancing the quality of a\nreconstructed image. The experimental result shows that the proposed technique\nobtained better compression ratio value.", 
    "link": "http://arxiv.org/pdf/1207.2268v1", 
    "arxiv-id": "1207.2268v1"
},{
    "category": "cs.NI", 
    "author": "Vincent Roca", 
    "title": "Erasure Coding and Congestion Control for Interactive Real-Time   Communication", 
    "publish": "2012-07-12T07:30:45Z", 
    "summary": "The use of real-time applications over the Internet is a challenging problem\nthat the QoS epoch attempted to solve by proposing the DiffServ architecture.\nToday, the only existing service provided by the Internet is still best-effort.\nAs a result, multimedia applications often perform on top of a transport layer\nthat provides a variable sending rate. In an obvious manner, this variable\nsending rate is an issue for these applications with strong delay constraint.\nIn a real-time context where retransmission can not be used to ensure\nreliability, video quality suffers from any packet losses. In this position\npaper, we discuss this problem and motivate why we want to bring out a certain\nclass of erasure coding scheme inside multimedia congestion control protocols\nsuch as TFRC.", 
    "link": "http://arxiv.org/pdf/1207.2863v1", 
    "arxiv-id": "1207.2863v1"
},{
    "category": "cs.CR", 
    "author": "Gadadhar Sahoo", 
    "title": "Some New Methodologies for Image Hiding using Steganographic Techniques", 
    "publish": "2012-11-02T06:36:12Z", 
    "summary": "Security and memory management are the major demands for electronics devices\nlike ipods, cell phones, pmps, iphones and digital cameras. In this paper, we\nhave suggested a high level of security mechanism by considering the concept of\nsteganography along with the principle of cryptography. Four different methods\nthat can save a considerable amount of memory space have been discussed. Based\non these methods, we have constructed secured stego image creator and secured\nmulti image viewer in Microsoft platform so as to provide high level of\nsecurity and using less memory space for storage of image files in the above\nsaid electronic devices", 
    "link": "http://arxiv.org/pdf/1211.0377v1", 
    "arxiv-id": "1211.0377v1"
},{
    "category": "cs.CG", 
    "author": "Bata Vasic", 
    "title": "Ordered Statistics Vertex Extraction and Tracing Algorithm (OSVETA)", 
    "publish": "2012-11-06T18:39:50Z", 
    "summary": "We propose an algorithm for identifying vertices from three dimensional (3D)\nmeshes that are most important for a geometric shape creation. Extracting such\na set of vertices from a 3D mesh is important in applications such as digital\nwatermarking, but also as a component of optimization and triangulation. In the\nfirst step, the Ordered Statistics Vertex Extraction and Tracing Algorithm\n(OSVETA) estimates precisely the local curvature, and most important\ntopological features of mesh geometry. Using the vertex geometric importance\nranking, the algorithm traces and extracts a vector of vertices, ordered by\ndecreasing index of importance.", 
    "link": "http://arxiv.org/pdf/1211.1345v2", 
    "arxiv-id": "1211.1345v2"
},{
    "category": "cs.MM", 
    "author": "Mufti Mahmud", 
    "title": "A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete   Wavelet Transform Domain using Two Subbands", 
    "publish": "2012-11-12T17:15:41Z", 
    "summary": "Digital watermarking is the process to hide digital pattern directly into a\ndigital content. Digital watermarking techniques are used to address digital\nrights management, protect information and conceal secrets. An invisible\nnon-blind watermarking approach for gray scale images is proposed in this\npaper. The host image is decomposed into 3-levels using Discrete Wavelet\nTransform. Based on the parent-child relationship between the wavelet\ncoefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression\nalgorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the\nsignificant coefficients. The most significant coefficients of LH2 and HL2\nbands are selected to embed a binary watermark image. The selected significant\ncoefficients are modulated using Noise Visibility Function, which is considered\nas the best strength to ensure better imperceptibility. The approach is tested\nagainst various image processing attacks such as addition of noise, filtering,\ncropping, JPEG compression, histogram equalization and contrast adjustment. The\nexperimental results reveal the high effectiveness of the method.", 
    "link": "http://arxiv.org/pdf/1211.2699v1", 
    "arxiv-id": "1211.2699v1"
},{
    "category": "cs.MM", 
    "author": "Athina Markopoulou", 
    "title": "Network Coding Meets Multimedia: a Review", 
    "publish": "2012-11-18T10:43:54Z", 
    "summary": "While every network node only relays messages in a traditional communication\nsystem, the recent network coding (NC) paradigm proposes to implement simple\nin-network processing with packet combinations in the nodes. NC extends the\nconcept of \"encoding\" a message beyond source coding (for compression) and\nchannel coding (for protection against errors and losses). It has been shown to\nincrease network throughput compared to traditional networks implementation, to\nreduce delay and to provide robustness to transmission errors and network\ndynamics. These features are so appealing for multimedia applications that they\nhave spurred a large research effort towards the development of\nmultimedia-specific NC techniques. This paper reviews the recent work in NC for\nmultimedia applications and focuses on the techniques that fill the gap between\nNC theory and practical applications. It outlines the benefits of NC and\npresents the open challenges in this area. The paper initially focuses on\nmultimedia-specific aspects of network coding, in particular delay, in-network\nerror control, and media-specific error control. These aspects permit to handle\nvarying network conditions as well as client heterogeneity, which are critical\nto the design and deployment of multimedia systems. After introducing these\ngeneral concepts, the paper reviews in detail two applications that lend\nthemselves naturally to NC via the cooperation and broadcast models, namely\npeer-to-peer multimedia streaming and wireless networking.", 
    "link": "http://arxiv.org/pdf/1211.4206v1", 
    "arxiv-id": "1211.4206v1"
},{
    "category": "cs.CV", 
    "author": "Hind E. Qassim", 
    "title": "Five Modulus Method For Image Compression", 
    "publish": "2012-11-19T21:02:58Z", 
    "summary": "Data is compressed by reducing its redundancy, but this also makes the data\nless reliable, more prone to errors. In this paper a novel approach of image\ncompression based on a new method that has been created for image compression\nwhich is called Five Modulus Method (FMM). The new method consists of\nconverting each pixel value in an 8-by-8 block into a multiple of 5 for each of\nthe R, G and B arrays. After that, the new values could be divided by 5 to get\nnew values which are 6-bit length for each pixel and it is less in storage\nspace than the original value which is 8-bits. Also, a new protocol for\ncompression of the new values as a stream of bits has been presented that gives\nthe opportunity to store and transfer the new compressed image easily.", 
    "link": "http://arxiv.org/pdf/1211.4591v1", 
    "arxiv-id": "1211.4591v1"
},{
    "category": "cs.MM", 
    "author": "B. B. Meshram", 
    "title": "Content based video retrieval", 
    "publish": "2012-11-20T08:22:56Z", 
    "summary": "Content based video retrieval is an approach for facilitating the searching\nand browsing of large image collections over World Wide Web. In this approach,\nvideo analysis is conducted on low level visual properties extracted from video\nframe. We believed that in order to create an effective video retrieval system,\nvisual perception must be taken into account. We conjectured that a technique\nwhich employs multiple features for indexing and retrieval would be more\neffective in the discrimination and search tasks of videos. In order to\nvalidate this claim, content based indexing and retrieval systems were\nimplemented using color histogram, various texture features and other\napproaches. Videos were stored in Oracle 9i Database and a user study measured\ncorrectness of response.", 
    "link": "http://arxiv.org/pdf/1211.4683v1", 
    "arxiv-id": "1211.4683v1"
},{
    "category": "cs.CV", 
    "author": "Rowayda A. Sadek", 
    "title": "SVD Based Image Processing Applications: State of The Art, Contributions   and Research Challenges", 
    "publish": "2012-11-29T21:52:00Z", 
    "summary": "Singular Value Decomposition (SVD) has recently emerged as a new paradigm for\nprocessing different types of images. SVD is an attractive algebraic transform\nfor image processing applications. The paper proposes an experimental survey\nfor the SVD as an efficient transform in image processing applications. Despite\nthe well-known fact that SVD offers attractive properties in imaging, the\nexploring of using its properties in various image applications is currently at\nits infancy. Since the SVD has many attractive properties have not been\nutilized, this paper contributes in using these generous properties in newly\nimage applications and gives a highly recommendation for more research\nchallenges. In this paper, the SVD properties for images are experimentally\npresented to be utilized in developing new SVD-based image processing\napplications. The paper offers survey on the developed SVD based image\napplications. The paper also proposes some new contributions that were\noriginated from SVD properties analysis in different image processing. The aim\nof this paper is to provide a better understanding of the SVD in image\nprocessing and identify important various applications and open research\ndirections in this increasingly important area; SVD based image processing in\nthe future research.", 
    "link": "http://arxiv.org/pdf/1211.7102v1", 
    "arxiv-id": "1211.7102v1"
},{
    "category": "cs.HC", 
    "author": "Kre\u0161imir \u0106osi\u0107", 
    "title": "Towards semantic and affective coupling in emotionally annotated   databases", 
    "publish": "2012-12-01T23:36:14Z", 
    "summary": "Emotionally annotated databases are repositories of multimedia documents with\nannotated affective content that elicit emotional responses in exposed human\nsubjects. They are primarily used in research of human emotions, attention and\ndevelopment of stress-related mental disorders. This can be successfully\nexploited in larger processes like selection, evaluation and training of\npersonnel for occupations involving high stress levels. Emotionally annotated\ndatabases are also used in multimodal affective user interfaces to facilitate\nricher and more intuitive human-computer interaction. Multimedia documents in\nemotionally annotated databases must have maximum personal ego relevance to be\nthe most effective in all these applications. For this reason flexible\nconstruction of subject-specific of emotionally annotated databases is\nimperative. But current construction process is lengthy and labor intensive\nbecause it inherently includes an elaborate tagging experiment involving a team\nof human experts. This is unacceptable since the creation of new databases or\nmodification of the existing ones becomes slow and difficult. We identify a\npositive correlation between the affect and semantics in the existing\nemotionally annotated databases and propose to exploit this feature with an\ninteractive relevance feedback for a more efficient construction of emotionally\nannotated databases. Automatic estimation of affective annotations from\nexisting semantics enhanced with information refinement processes may lead to\nan efficient construction of high-quality emotionally annotated databases.", 
    "link": "http://arxiv.org/pdf/1212.0169v1", 
    "arxiv-id": "1212.0169v1"
},{
    "category": "cs.CR", 
    "author": "Youssef Bassil", 
    "title": "An Image Steganography Scheme using Randomized Algorithm and   Context-Free Grammar", 
    "publish": "2012-12-10T13:54:44Z", 
    "summary": "Currently, cryptography is in wide use as it is being exploited in various\ndomains from data confidentiality to data integrity and message authentication.\nBasically, cryptography shuffles data so that they become unreadable by\nunauthorized parties. However, clearly visible encrypted messages, no matter\nhow unbreakable, will arouse suspicions. A better approach would be to hide the\nvery existence of the message using steganography. Fundamentally, steganography\nconceals secret data into innocent-looking mediums called carriers which can\nthen travel from the sender to the receiver safe and unnoticed. This paper\nproposes a novel steganography scheme for hiding digital data into uncompressed\nimage files using a randomized algorithm and a context-free grammar. Besides,\nthe proposed scheme uses two mediums to deliver the secret data: a carrier\nimage into which the secret data are hidden into random pixels, and a\nwell-structured English text that encodes the location of the random carrier\npixels. The English text is generated at runtime using a context-free grammar\ncoupled with a lexicon of English words. The proposed scheme is stealthy, and\nhard to be noticed, detected, and recovered. Experiments conducted showed how\nthe covering and the uncovering processes of the proposed scheme work. As\nfuture work, a semantic analyzer is to be developed so as to make the English\ntext medium semantically correct, and consequently safer to be transmitted\nwithout drawing any attention.", 
    "link": "http://arxiv.org/pdf/1212.2064v1", 
    "arxiv-id": "1212.2064v1"
},{
    "category": "cs.CR", 
    "author": "J. K. Mandal", 
    "title": "Self Authentication of color image through Wavelet Transformation   Technique (SAWT)", 
    "publish": "2012-12-11T18:41:22Z", 
    "summary": "In this paper a self organized legal document/content authentication,\ncopyright protection in composite domain has been proposed without using any\nexternal information. Average values of transformed red and green components in\nfrequency domain generated through wavelet transform are embedded into the blue\ncomponent of the color image matrix in spatial domain. A reverse transformation\nis made in RG matrix to obtain embedded image in association with blue\ncomponent in spatial domain. Reverse procedure is done during decoding where\ntransformed average values are obtained from red and green components and\ncompared with the same from blue component for authentication. Results are\ncompared with existing technique which shows better performance interns of\nPSNR, MSE & IF.", 
    "link": "http://arxiv.org/pdf/1212.2582v1", 
    "arxiv-id": "1212.2582v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Multi-View Video Packet Scheduling", 
    "publish": "2012-12-18T18:42:52Z", 
    "summary": "In multiview applications, multiple cameras acquire the same scene from\ndifferent viewpoints and generally produce correlated video streams. This\nresults in large amounts of highly redundant data. In order to save resources,\nit is critical to handle properly this correlation during encoding and\ntransmission of the multiview data. In this work, we propose a\ncorrelation-aware packet scheduling algorithm for multi-camera networks, where\ninformation from all cameras are transmitted over a bottleneck channel to\nclients that reconstruct the multiview images. The scheduling algorithm relies\non a new rate-distortion model that captures the importance of each view in the\nscene reconstruction. We propose a problem formulation for the optimization of\nthe packet scheduling policies, which adapt to variations in the scene content.\nThen, we design a low complexity scheduling algorithm based on a trellis search\nthat selects the subset of candidate packets to be transmitted towards\neffective multiview reconstruction at clients. Extensive simulation results\nconfirm the gain of our scheduling algorithm when inter-source correlation\ninformation is used in the scheduler, compared to scheduling policies with no\ninformation about the correlation or non-adaptive scheduling policies. We\nfinally show that increasing the optimization horizon in the packet scheduling\nalgorithm improves the transmission performance, especially in scenarios where\nthe level of correlation rapidly varies with time.", 
    "link": "http://arxiv.org/pdf/1212.4455v2", 
    "arxiv-id": "1212.4455v2"
},{
    "category": "cs.CR", 
    "author": "Youssef Bassil", 
    "title": "Image Steganography Method Based on Brightness Adjustment", 
    "publish": "2012-12-23T14:54:10Z", 
    "summary": "Steganography is an information hiding technique in which secret data are\nsecured by covering them into a computer carrier file without damaging the file\nor changing its size. The difference between steganography and cryptography is\nthat steganography is a stealthy method of communication that only the\ncommunicating parties are aware of; while, cryptography is an overt method of\ncommunication that anyone is aware of, despite its payload is scribbled.\nTypically, an irrecoverable steganography algorithm is the algorithm that makes\nit hard for malicious third parties to discover how it works and how to recover\nthe secret data out of the carrier file. One popular way to achieve\nirrecoverability is to digitally process the carrier file after hiding the\nsecret data into it. However, such process is irreversible as it would destroy\nthe concealed data. This paper proposes a new image steganography method for\ntextual data, as well as for any form of digital data, based on adjusting the\nbrightness of the carrier image after covering the secret data into it. The\nalgorithm used is parameterized as it can be configured using three different\nparameters defined by the communicating parties. They include the amount of\nbrightness to apply on the carrier image after the completion of the covering\nprocess, the color channels whose brightness should be adjusted, and the bytes\nthat should carry in the secret data. The novelty of the proposed method is\nthat it embeds bits of the secret data into the three LSBs of the bytes that\ncompose the carrier image in such a way that does not destroy the secret data\nwhen restoring back the original brightness of the carrier image. The\nsimulation conducted proved that the proposed algorithm is valid and correct.", 
    "link": "http://arxiv.org/pdf/1212.5801v1", 
    "arxiv-id": "1212.5801v1"
},{
    "category": "cs.MM", 
    "author": "Debin Zhao", 
    "title": "High Quality Image Interpolation via Local Autoregressive and Nonlocal   3-D Sparse Regularization", 
    "publish": "2012-12-25T14:28:52Z", 
    "summary": "In this paper, we propose a novel image interpolation algorithm, which is\nformulated via combining both the local autoregressive (AR) model and the\nnonlocal adaptive 3-D sparse model as regularized constraints under the\nregularization framework. Estimating the high-resolution image by the local AR\nregularization is different from these conventional AR models, which weighted\ncalculates the interpolation coefficients without considering the rough\nstructural similarity between the low-resolution (LR) and high-resolution (HR)\nimages. Then the nonlocal adaptive 3-D sparse model is formulated to regularize\nthe interpolated HR image, which provides a way to modify these pixels with the\nproblem of numerical stability caused by AR model. In addition, a new\nSplit-Bregman based iterative algorithm is developed to solve the above\noptimization problem iteratively. Experiment results demonstrate that the\nproposed algorithm achieves significant performance improvements over the\ntraditional algorithms in terms of both objective quality and visual perception", 
    "link": "http://arxiv.org/pdf/1212.6058v1", 
    "arxiv-id": "1212.6058v1"
},{
    "category": "cs.SD", 
    "author": "Mart\u00edn Varela", 
    "title": "Single-sided Real-time PESQ Score Estimation", 
    "publish": "2012-12-27T11:31:16Z", 
    "summary": "For several years now, the ITU-T's Perceptual Evaluation of Speech Quality\n(PESQ) has been the reference for objective speech quality assessment. It is\nwidely deployed in commercial QoE measurement products, and it has been well\nstudied in the literature. While PESQ does provide reasonably good correlation\nwith subjective scores for VoIP applications, the algorithm itself is not\nusable in a real-time context, since it requires a reference signal, which is\nusually not available in normal conditions. In this paper we provide an\nalternative technique for estimating PESQ scores in a single-sided fashion,\nbased on the Pseudo Subjective Quality Assessment (PSQA) technique.", 
    "link": "http://arxiv.org/pdf/1212.6350v1", 
    "arxiv-id": "1212.6350v1"
},{
    "category": "cs.MM", 
    "author": "Stefania Colonnese", 
    "title": "A Poisson Hidden Markov Model for Multiview Video Traffic", 
    "publish": "2013-01-02T22:18:06Z", 
    "summary": "Multiview video has recently emerged as a means to improve user experience in\nnovel multimedia services. We propose a new stochastic model to characterize\nthe traffic generated by a Multiview Video Coding (MVC) variable bit rate\nsource. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in\nwhich the first (hidden) layer represents the evolution of the video activity\nand the second layer represents the frame sizes of the multiple encoded views.\nWe propose a method for estimating the model parameters in long MVC sequences.\nWe then present extensive numerical simulations assessing the model's ability\nto produce traffic with realistic characteristics for a general class of MVC\nsequences. We then extend our framework to network applications where we show\nthat our model is able to accurately describe the sender and receiver buffers\nbehavior in MVC transmission. Finally, we derive a model of user behavior for\ninteractive view selection, which, in conjunction with our traffic model, is\nable to accurately predict actual network load in interactive multiview\nservices.", 
    "link": "http://arxiv.org/pdf/1301.0344v1", 
    "arxiv-id": "1301.0344v1"
},{
    "category": "cs.MM", 
    "author": "Abdelmajid ben Hamadou", 
    "title": "Content-Based Video Browsing by Text Region Localization and   Classification", 
    "publish": "2013-01-10T16:22:40Z", 
    "summary": "The amount of digital video data is increasing over the world. It highlights\nthe need for efficient algorithms that can index, retrieve and browse this data\nby content. This can be achieved by identifying semantic description captured\nautomatically from video structure. Among these descriptions, text within video\nis considered as rich features that enable a good way for video indexing and\nbrowsing. Unlike most video text detection and extraction methods that treat\nvideo sequences as collections of still images, we propose in this paper\nspatiotemporal. video-text localization and identification approach which\nproceeds in two main steps: text region localization and text region\nclassification. In the first step we detect the significant appearance of the\nnew objects in a frame by a split and merge processes applied on binarized edge\nframe pair differences. Detected objects are, a priori, considered as text.\nThey are then filtered according to both local contrast variation and texture\ncriteria in order to get the effective ones. The resulted text regions are\nclassified based on a visual grammar descriptor containing a set of semantic\ntext class regions characterized by visual features. A visual table of content\nis then generated based on extracted text regions occurring within video\nsequence enriched by a semantic identification. The experimentation performed\non a variety of video sequences shows the efficiency of our approach.", 
    "link": "http://arxiv.org/pdf/1301.2172v1", 
    "arxiv-id": "1301.2172v1"
},{
    "category": "cs.MM", 
    "author": "Walid Mahdi", 
    "title": "AViTExt: Automatic Video Text Extraction, A new Approach for video   content indexing Application", 
    "publish": "2013-01-10T16:29:11Z", 
    "summary": "In this paper, we propose a spatial temporal video-text detection technique\nwhich proceed in two principal steps:potential text region detection and a\nfiltering process. In the first step we divide dynamically each pair of\nconsecutive video frames into sub block in order to detect change. A\nsignificant difference between homologous blocks implies the appearance of an\nimportant object which may be a text region. The temporal redundancy is then\nused to filter these regions and forms an effective text region. The\nexperimentation driven on a variety of video sequences shows the effectiveness\nof our approach by obtaining a 89,39% as precision rate and 90,19 as recall.", 
    "link": "http://arxiv.org/pdf/1301.2173v1", 
    "arxiv-id": "1301.2173v1"
},{
    "category": "cs.MM", 
    "author": "Walid Mahdi", 
    "title": "A Visual Grammar Approach for TV Program Identification", 
    "publish": "2013-01-10T17:56:02Z", 
    "summary": "Automatic identification of TV programs within TV streams is an important\ntask for archive exploitation. This paper proposes a new spatial-temporal\napproach to identify programs in TV streams in two main steps: First, a\nreference catalogue for video grammars of visual jingles is constructed. We\nexploit visual grammars characterizing instances of the same program type in\norder to identify the various program types in the TV stream. The role of video\ngrammar is to represent the visual invariants for each visual jingle using a\nset of descriptors appropriate for each TV program. Secondly, programs in TV\nstreams are identified by examining the similarity of the video signal to the\nvisual grammars in the catalogue. The main idea of identification process\nconsists in comparing the visual similarity of the video signal signature in TV\nstream to the catalogue elements. After presenting the proposed approach, the\npaper overviews the encouraging experimental results on several streams\nextracted from different channels and composed of several programs.", 
    "link": "http://arxiv.org/pdf/1301.2200v1", 
    "arxiv-id": "1301.2200v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "SkyDe: a Skype-based Steganographic Method", 
    "publish": "2013-01-16T09:21:11Z", 
    "summary": "This paper introduces SkyDe (Skype Hide), a new steganographic method that\nutilizes Skype encrypted packets with silence to provide the means for\nclandestine communication. It is possible to reuse packets that do not carry\nvoice signals for steganographic purposes because Skype does not use any\nsilence suppression mechanism. The method's proof-of-concept implementation and\nfirst experimental results are presented. They prove that the method is\nfeasible and offers steganographic bandwidth as high as 2.8 kbps.", 
    "link": "http://arxiv.org/pdf/1301.3632v1", 
    "arxiv-id": "1301.3632v1"
},{
    "category": "cs.MM", 
    "author": "Ashish Jani", 
    "title": "A Novel Digital Watermarking Algorithm using Random Matrix Image", 
    "publish": "2013-01-18T10:16:21Z", 
    "summary": "The availability of bandwidth for internet access is sufficient enough to\ncommunicate digital assets. These digital assets are subjected to various types\nof threats. [19] As a result of this, protection mechanism required for the\nprotection of digital assets is of priority in research. The threat of current\nfocus is unauthorized copying of digital assets which give boost to piracy.\nThis under the copyright act is illegal and a robust mechanism is required to\ncurb this kind of unauthorized copy. To safeguard the copyright digital assets,\na robust digital watermarking technique is needed. The existing digital\nwatermarking techniques protect digital assets by embedding a digital watermark\ninto a host digital image. This embedding does induce slight distortion in the\nhost image but the distortion is usually too small to be noticed. At the same\ntime the embedded watermark must be robust enough to with stand deliberate\nattacks. There are various techniques of digital watermarking but researchers\nare making constant efforts to increase the robustness of the watermark image.\nThe layered approach of watermarking based on Huffman coding [5] can soon\nincrease the robustness of digital watermark.[11] Ultimately, increasing the\nsecurity of copyright of protection. The proposed work is in similar direction\nwhere in RMI (Random Matrix Image) is used in place of Huffman coding. This\ninnovative algorithm has considerably increased the robustness in digital\nwatermark while also enhancing security of production", 
    "link": "http://arxiv.org/pdf/1301.4337v2", 
    "arxiv-id": "1301.4337v2"
},{
    "category": "cs.MM", 
    "author": "Juan M. Lopez-Soler", 
    "title": "Video Tester -- A multiple-metric framework for video quality assessment   over IP networks", 
    "publish": "2013-01-24T14:31:10Z", 
    "summary": "This paper presents an extensible and reusable framework which addresses the\nproblem of video quality assessment over IP networks. The proposed tool\n(referred to as Video-Tester) supports raw uncompressed video encoding and\ndecoding. It also includes different video over IP transmission methods (i.e.:\nRTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it\nis furnished with a rich set of offline analysis capabilities. Video-Tester\nanalysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,\npacket inter-arrival time, jitter and loss rate, as well as GOP size and\nI-frame loss rate). Our design facilitates the integration of virtually any\nexisting video quality metric thanks to the adopted Python-based modular\napproach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video\nquality metric, DIV and PSNR-based MOS estimations. In order to promote its use\nand extension, Video-Tester is open and publicly available.", 
    "link": "http://arxiv.org/pdf/1301.5793v1", 
    "arxiv-id": "1301.5793v1"
},{
    "category": "cs.MM", 
    "author": "Tino Jameskutty", 
    "title": "Secure Video Streaming Plug-In", 
    "publish": "2013-03-07T14:17:23Z", 
    "summary": "Video sharing sites like YouTube, Metacafe, Dailymotion, Vimeo, etc. provide\na platform for media content sharing among its users. Some of these videos are\ncopyright protected and restricted from being downloaded and saved. But users\ncan use various download managers or application programs to download and save\nthese videos. This affects the incoming traffic on these websites reducing\ntheir hit rate and consequently reducing their revenue. Adobe Flash Player is\nthe most commonly used player for watching online videos. It uses RTMP (Real\nTime Messaging Protocol) to stream audio, video and data over the Internet,\nbetween a Flash Player and Adobe Flash Media Server.Here, we propose a plug-in\nthat enables the site owner control over downloading of videos from such\nwebsite. The plug-in will be installed at the client side with the consent of\nthe user. When the video is being played this plug-in will send unique keys to\nthe media server. The server will continue streaming the video after verifying\nthe keys. Download managers or application programs will not be able to\ndownload the videos as they wont be able to create the unique keys that need to\nbe sent to the server.", 
    "link": "http://arxiv.org/pdf/1303.1697v1", 
    "arxiv-id": "1303.1697v1"
},{
    "category": "cs.MM", 
    "author": "Sheli Sinha Chaudhuri", 
    "title": "Medical Information Embedding in Compressed Watermarked Intravascular   Ultrasound Video", 
    "publish": "2013-03-09T14:08:23Z", 
    "summary": "In medical field, intravascular ultrasound (IVUS) is a tomographic imaging\nmodality, which can identify the boundaries of different layers of blood\nvessels. IVUS can detect myocardial infarction (heart attack) that remains\nignored and unattended when only angioplasty is done. During the past decade,\nit became easier for some individuals or groups to copy and transmits digital\ninformation without the permission of the owner. For increasing authentication\nand security of copyrights, digital watermarking, an information hiding\ntechnique, was introduced. Achieving watermarking technique with lesser amount\nof distortion in biomedical data is a challenging task. Watermark can be\nembedded into an image or in a video. As video data is a huge amount of\ninformation, therefore a large storage area is needed which is not feasible. In\nthis case motion vector based video compression is done to reduce size. In this\npresent paper, an Electronic Patient Record (EPR) is embedded as watermark\nwithin an IVUS video and then motion vector is calculated. This proposed method\nproves robustness as the extracted watermark has good PSNR value and less MSE.", 
    "link": "http://arxiv.org/pdf/1303.2211v1", 
    "arxiv-id": "1303.2211v1"
},{
    "category": "cs.MM", 
    "author": "D. Venkataraman", 
    "title": "Image compression using anti-forensics method", 
    "publish": "2013-03-10T15:29:26Z", 
    "summary": "A large number of image forensics methods are available which are capable of\nidentifying image tampering. But these techniques are not capable of addressing\nthe anti-forensics method which is able to hide the trace of image tampering.\nIn this paper anti-forensics method for digital image compression has been\nproposed. This anti-forensics method is capable of removing the traces of image\ncompression. Additionally, technique is also able to remove the traces of\nblocking artifact that are left by image compression algorithms that divide an\nimage into segments during compression process. This method is targeted to\nremove the compression fingerprints of JPEG compression.", 
    "link": "http://arxiv.org/pdf/1303.2330v1", 
    "arxiv-id": "1303.2330v1"
},{
    "category": "cs.CR", 
    "author": "Aniket More", 
    "title": "Proposed Video Encryption Algorithm v/s Other Existing Algorithms: A   Comparative Study", 
    "publish": "2013-03-14T15:52:12Z", 
    "summary": "Securing multimedia data has become of utmost importance especially in the\napplications related to military purposes. With the rise in development in\ncomputer and internet technology, multimedia data has become the most\nconvenient method for military training. An innovative encryption algorithm for\nvideos compressed using H.264 was proposed to safely exchange highly\nconfidential videos. To maintain a balance between security and computational\ntime, the proposed algorithm shuffles the video frames along with the audio,\nand then AES is used to selectively encrypt the sensitive video codewords.\nUsing this approach unauthorized viewing of the video file can be prevented and\nhence this algorithm provides a high level of security. A comparative study of\nthe proposed algorithm with other existing algorithms has been put forward in\nthis paper to prove the effectiveness of the proposed algorithm.", 
    "link": "http://arxiv.org/pdf/1303.3485v1", 
    "arxiv-id": "1303.3485v1"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "StegTorrent: a Steganographic Method for the P2P File Sharing Service", 
    "publish": "2013-03-18T14:36:53Z", 
    "summary": "The paper proposes StegTorrent a new network steganographic method for the\npopular P2P file transfer service-BitTorrent. It is based on modifying the\norder of data packets in the peer-peer data exchange protocol. Unlike other\nexisting steganographic methods that modify the packets' order it does not\nrequire any synchronization. Experimental results acquired from prototype\nimplementation proved that it provides high steganographic bandwidth of up to\n270 b/s while introducing little transmission distortion and providing\ndifficult detectability.", 
    "link": "http://arxiv.org/pdf/1303.4264v1", 
    "arxiv-id": "1303.4264v1"
},{
    "category": "cs.MM", 
    "author": "Kre\u0161imir \u0106osi\u0107", 
    "title": "Multimedia stimuli databases usage patterns: a survey report", 
    "publish": "2013-03-20T10:18:50Z", 
    "summary": "Multimedia documents such as images, sounds or videos can be used to elicit\nemotional responses in exposed human subjects. These stimuli are stored in\naffective multimedia databases and successfully used for a wide variety of\nresearch in affective computing, human-computer interaction and cognitive\nsciences. Affective multimedia databases are simple repositories of multimedia\ndocuments with annotated high-level semantics and affective content. Although\nimportant all affective multimedia databases have numerous deficiencies which\nimpair their applicability. To establish a better understanding of how experts\nuse affective multimedia databases an online survey was conducted into the\nsubject. The survey results are statistically significant and indicate that\ncontemporary databases lack stimuli with rich semantic and emotional content.\n73.33% of survey participants find the databases lacking at least some\nimportant semantic or emotion content. Most of the participants consider\nstimuli descriptions to be inadequate. Overall, 1-2h or more than 24h are\ngenerally needed to construct a single stimulation sequence. Almost 84% of the\nsurvey participants would like to use real-life videos in their research.\nExperts unequivocally recognize the need for an intelligent stimuli retrieval\napplication that would assist them in experimentation. Almost all experts agree\nsuch applications could be useful in their work.", 
    "link": "http://arxiv.org/pdf/1303.4893v2", 
    "arxiv-id": "1303.4893v2"
},{
    "category": "cs.MM", 
    "author": "Nilanjan Dey", 
    "title": "Odd-Even Embedding Scheme Based Modified Reversible Watermarking   Technique using Blueprint", 
    "publish": "2013-03-24T17:52:45Z", 
    "summary": "Digital watermarking is a technique of information adding or information\nhiding in order to identify the owner of the data in multimedia content. It\nseems that a signal or digital image can permanently embed over another digital\ndata providing a good way to protect intellectual property from illegal\nreplication. The cover data that is transmitted through the internet hides the\nwatermark in a computer aided assertion method such that it becomes\nundetectable. Finally it stands as a hindrance over many operations without\nharming the embedded host document. Unfortunately, many owners of the digital\nmaterials such as images, text, audio and video are reluctant to the spreading\nof their documents on the web or other networked environment, because the ease\nof duplicating digital materials facilitates copyright violation. Digital media\ndistribution occurs through various channels. The cover data may or may not\nhold any relation with the watermark information. In the last two decades, a\nconsiderable amount of research has been done on the digital watermarking of\nmultimedia files such as audio, video, images and text. Different type of\nwatermarking algorithms has been proposed by the researchers to achieve high\nlevel of security and authenticity. In our proposed method, a modified\nreversible watermarking technique is introduced, which employs a blueprint\ngeneration of original image based on odd-even embedding methodology to yield\nlarge data hiding capacity, security as well as high watermarked quality. The\nexperimental results demonstrate that, no matter how much secret data is\nembedded, the watermarked quality is about 51dB in this proposed scheme.", 
    "link": "http://arxiv.org/pdf/1303.5972v1", 
    "arxiv-id": "1303.5972v1"
},{
    "category": "cs.NI", 
    "author": "Sabu M Thampi", 
    "title": "A Review on P2P Video Streaming", 
    "publish": "2013-04-04T03:15:04Z", 
    "summary": "The main objective of this article is to provide an overview of P2P based\nVideo-on-Demand and live streaming services. The article starts with an\nintroduction to media streaming and its simplified architecture. Various\nsolutions offering video streaming in the context of widespread usage of\nInternet are discussed. This is followed by a short introduction to P2P\nnetworks and its applications. A broad discussion on various P2P streaming\nschemes and P2P streaming applications are the main focus of this chapter.\nFinally, the security issues and solutions for P2P video streaming are\ndiscussed briefly.", 
    "link": "http://arxiv.org/pdf/1304.1235v1", 
    "arxiv-id": "1304.1235v1"
},{
    "category": "cs.MM", 
    "author": "Firas A. Jassim", 
    "title": "Hiding Image in Image by Five Modulus Method for Image Steganography", 
    "publish": "2013-04-04T22:17:47Z", 
    "summary": "This paper is to create a practical steganographic implementation to hide\ncolor image (stego) inside another color image (cover). The proposed technique\nuses Five Modulus Method to convert the whole pixels within both the cover and\nthe stego images into multiples of five. Since each pixels inside the stego\nimage is divisible by five then the whole stego image could be divided by five\nto get new range of pixels 0..51. Basically, the reminder of each number that\nis not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,\nthen a 4-by-4 window size has been implemented to accommodate the proposed\ntechnique. For each 4-by-4 window inside the cover image, a number from 1 to 4\ncould be embedded secretly from the stego image. The previous discussion must\nbe applied separately for each of the R, G, and B arrays. Moreover, a stego-key\ncould be combined with the proposed algorithm to make it difficult for any\nadversary to extract the secret image from the cover image. Based on the PSNR\nvalue, the extracted stego image has high PSNR value. Hence this new\nsteganography algorithm is very efficient to hide color images.", 
    "link": "http://arxiv.org/pdf/1304.1571v1", 
    "arxiv-id": "1304.1571v1"
},{
    "category": "cs.MM", 
    "author": "Gaurav Pande", 
    "title": "Metrics for Video Quality Assessment in Mobile Scenarios", 
    "publish": "2013-04-13T01:59:01Z", 
    "summary": "With exponential increase in the volumes of video traffic in cellular\nnet-works, there is an increasing need for optimizing the quality of video\ndelivery. 4G networks (Long Term Evolution Advanced or LTE A) are being\nintroduced in many countries worldwide, which allow a downlink speed of upto 1\nGbps and uplink of 100 Mbps over a single base station. This makes a strong\npush towards video broadcasting over LTE networks, characterizing its\nperformance and developing metrics which can be deployed to provide user\nfeedback of video quality and feed-back them to network operators to fine tune\nthe network. In this paper, we characterize the performance of video\ntransmission over LTE A physical layer using popular video quality metrics such\nas SSIM, Blocking, Blurring, NIQE and BRISQUE. We conduct experiments to find a\nsuitable no-reference metrics for mobile scenario and find that Blocking\nMetrics is most promising in case of channel or modulation variations but it\ndoes not perform well to quantize variations in compression ratios. The metrics\nBRISQUE is very efficient in quantizing this distortion and performs well in\ncase of network variations also.", 
    "link": "http://arxiv.org/pdf/1304.3758v1", 
    "arxiv-id": "1304.3758v1"
},{
    "category": "cs.MM", 
    "author": "Urmila Shrawankar", 
    "title": "Security Issues In Speech Watermarking for Information Transmission", 
    "publish": "2013-04-25T10:53:39Z", 
    "summary": "The secure transmission of speech information is a significant issue faced by\nmany security professionals and individuals. By applying voice-encryption\ntechnique any kind of encrypted sensitive speech data such as password can be\ntransmitted. But this has the serious disadvantage that by means of\ncryptanalysis attack encrypted data can be compromised. Increasing the strength\nof encryption/decryption results in an associated increased in the cost.\nAdditional techniques like stenography and digital watermarking can be used to\nconceal information in an undetectable way in audio data. However this\nwatermarked audio data has to be send through unreliable media and an\neavesdropper might get hold of secret message and can also determine the\nidentity of a speaker who is sending the information since human voice contains\ninformation based on its characteristics such as frequency, pitch, and energy.\nThis paper proposes Normalized Speech Watermarking technique. Speech signal is\nnormalized to hide the identity of the speaker who is sending the information\nand then speech watermarking technique is applied on this normalized signal\nthat contains the message (password) so that what information is transmitted\nshould not be unauthorizedly revealed.", 
    "link": "http://arxiv.org/pdf/1304.6872v1", 
    "arxiv-id": "1304.6872v1"
},{
    "category": "cs.MM", 
    "author": "V. M Thakare", 
    "title": "Secure Transmission of Password Using Speech Watermarking", 
    "publish": "2013-04-25T10:42:04Z", 
    "summary": "Internet is one of the most valuable resources for information communication\nand retrievals. Most multimedia signals today are in digital formats. The\ndigital data can be duplicated and edited with great ease which has led to a\nneed for data integrity and protection of digital data. The security\nrequirements such as integrity or data authentication can be met by\nimplementing security measures using digital watermarking techniques. In this\npaper a blind speech watermarking algorithm that embeds the watermark signal\ndata in the musical (sequence) host signal by using frequency masking is used.\nA different logarithmic approach is proposed. In this regard a logarithmic\nfunction is first applied to watermark data. Then the transformed signal is\nembedded to the converted version of host signal which is obtained by applying\nFast Fourier transform method. Finally using inverse Fast Fourier Transform and\nantilogarithmic function watermark signal is retrieved.", 
    "link": "http://arxiv.org/pdf/1304.8080v1", 
    "arxiv-id": "1304.8080v1"
},{
    "category": "cs.CV", 
    "author": "Firas A. Jassim", 
    "title": "Image Compression By Embedding Five Modulus Method Into JPEG", 
    "publish": "2013-04-30T20:28:37Z", 
    "summary": "The standard JPEG format is almost the optimum format in image compression.\nThe compression ratio in JPEG sometimes reaches 30:1. The compression ratio of\nJPEG could be increased by embedding the Five Modulus Method (FMM) into the\nJPEG algorithm. The novel algorithm gives twice the time as the standard JPEG\nalgorithm or more. The novel algorithm was called FJPEG (Five-JPEG). The\nquality of the reconstructed image after compression is approximately\napproaches the JPEG. Standard test images have been used to support and\nimplement the suggested idea in this paper and the error metrics have been\ncomputed and compared with JPEG.", 
    "link": "http://arxiv.org/pdf/1305.0020v1", 
    "arxiv-id": "1305.0020v1"
},{
    "category": "cs.CV", 
    "author": "Amir Averbuch", 
    "title": "Video Segmentation via Diffusion Bases", 
    "publish": "2013-05-01T16:22:55Z", 
    "summary": "Identifying moving objects in a video sequence, which is produced by a static\ncamera, is a fundamental and critical task in many computer-vision\napplications. A common approach performs background subtraction, which\nidentifies moving objects as the portion of a video frame that differs\nsignificantly from a background model. A good background subtraction algorithm\nhas to be robust to changes in the illumination and it should avoid detecting\nnon-stationary background objects such as moving leaves, rain, snow, and\nshadows. In addition, the internal background model should quickly respond to\nchanges in background such as objects that start to move or stop. We present a\nnew algorithm for video segmentation that processes the input video sequence as\na 3D matrix where the third axis is the time domain. Our approach identifies\nthe background by reducing the input dimension using the \\emph{diffusion bases}\nmethodology. Furthermore, we describe an iterative method for extracting and\ndeleting the background. The algorithm has two versions and thus covers the\ncomplete range of backgrounds: one for scenes with static backgrounds and the\nother for scenes with dynamic (moving) backgrounds.", 
    "link": "http://arxiv.org/pdf/1305.0218v1", 
    "arxiv-id": "1305.0218v1"
},{
    "category": "cs.MM", 
    "author": "Gaurav Pande", 
    "title": "Performance Evaluation of Video Communications over 4G Network", 
    "publish": "2013-05-08T17:19:50Z", 
    "summary": "With exponential increase in the volumes of video traffic in cellular\nnet-works, there is an increasing need for optimizing the quality of video\ndelivery. 4G networks (Long Term Evolution Advanced or LTE A) are being\nintroduced in many countries worldwide, which allow a downlink speed of upto 1\nGbps and uplink of 100 Mbps over a single base station. In this paper, we\ncharacterize the performance of LTE A physical layer in terms of transmitted\nvideo quality when the channel condi-tions and LTE settings are varied. We test\nthe performance achieved as the channel quality is changed and HARQ features\nare enabled in physical layer. Blocking and blurring metrics were used to model\nimage quality.", 
    "link": "http://arxiv.org/pdf/1305.1887v1", 
    "arxiv-id": "1305.1887v1"
},{
    "category": "cs.MM", 
    "author": "Prasanta K. Panigrahi", 
    "title": "An Adaptive Statistical Non-uniform Quantizer for Detail Wavelet   Components in Lossy JPEG2000 Image Compression", 
    "publish": "2013-05-09T01:10:11Z", 
    "summary": "The paper presents a non-uniform quantization method for the Detail\ncomponents in the JPEG2000 standard. Incorporating the fact that the\ncoefficients lying towards the ends of the histogram plot of each Detail\ncomponent represent the structural information of an image, the quantization\nstep sizes become smaller at they approach the ends of the histogram plot. The\nvariable quantization step sizes are determined by the actual statistics of the\nwavelet coefficients. Mean and standard deviation are the two statistical\nparameters used iteratively to obtain the variable step sizes. Moreover, the\nmean of the coefficients lying within the step size is chosen as the quantized\nvalue, contrary to the deadzone uniform quantizer which selects the midpoint of\nthe quantization step size as the quantized value. The experimental results of\nthe deadzone uniform quantizer and the proposed non-uniform quantizer are\nobjectively compared by using Mean-Squared Error (MSE) and Mean Structural\nSimilarity Index Measure (MSSIM), to evaluate the quantization error and\nreconstructed image quality, respectively. Subjective analysis of the\nreconstructed images is also carried out. Through the objective and subjective\nassessments, it is shown that the non-uniform quantizer performs better than\nthe deadzone uniform quantizer in the perceptual quality of the reconstructed\nimage, especially at low bitrates. More importantly, unlike the deadzone\nuniform quantizer, the non-uniform quantizer accomplishes better visual quality\nwith a few quantized values.", 
    "link": "http://arxiv.org/pdf/1305.1986v3", 
    "arxiv-id": "1305.1986v3"
},{
    "category": "cs.MM", 
    "author": "Prasanta K. Panigrahi", 
    "title": "Quantum Image Representation Through Two-Dimensional Quantum States and   Normalized Amplitude", 
    "publish": "2013-05-10T02:53:07Z", 
    "summary": "We propose a novel method for image representation in quantum computers,\nwhich uses the two-dimensional (2-D) quantum states to locate each pixel in an\nimage through row-location and column-location vectors for identifying each\npixel location. The quantum state of an image is the linear superposition of\nthe tensor product of the m-qubits row-location vector and the n-qubits\ncolumn-location vector of each pixel. It enables the natural quantum\nrepresentation of rectangular images that other methods lack. The\namplitude/intensity of each pixel is incorporated into the coefficient values\nof the pixel's quantum state, without using any qubits. Due to the fact that\nlinear superposition, tensor product and qubits form the fundamental basis of\nquantum computing, the proposed method presents the machine level\nrepresentation of images on quantum computers. Unlike other methods, this\nmethod is a pure quantum representation without any classical components.", 
    "link": "http://arxiv.org/pdf/1305.2251v4", 
    "arxiv-id": "1305.2251v4"
},{
    "category": "cs.MM", 
    "author": "Khizar Hyat", 
    "title": "Wave Atom Based Watermarking", 
    "publish": "2013-05-14T05:27:52Z", 
    "summary": "Watermarking helps in ensuring originality, ownership and copyrights of a\ndigital image. This paper aims at embedding a Watermark in an image using Wave\nAtom Transform. Preference of Wave Atoms on other transformations has been due\nto its sparser expansion, adaptability to the direction of local pattern, and\nsharp frequency localization. In this scheme, we had tried to spread the\nwatermark in an image so that the information at one place is very small and\nundetectable. In order to extract the watermark and verify ownership of an\nimage, one would have the advantage of prior knowledge of embedded locations. A\nnoise of high amplitude will be needed to be added to the image for watermark\ndistortion. Furthermore, the information spread will ensure the robustness of\nthe watermark data. The proposed scheme has the ability to withstand malicious\noperations and attacks.", 
    "link": "http://arxiv.org/pdf/1305.3021v2", 
    "arxiv-id": "1305.3021v2"
},{
    "category": "cs.MM", 
    "author": "Shareeda Mohammed", 
    "title": "Using Bias Optimization for Reversible Data Hiding Using Image   Interpolation", 
    "publish": "2013-04-11T14:30:46Z", 
    "summary": "In this paper, we propose a reversible data hiding method in the spatial\ndomain for compressed grayscale images. The proposed method embeds secret bits\ninto a compressed thumbnail of the original image by using a novel\ninterpolation method and the Neighbour Mean Interpolation (NMI) technique as\nscaling up to the original image occurs. Experimental results presented in this\npaper show that the proposed method has significantly improved embedding\ncapacities over the approach proposed by Jung and Yoo.", 
    "link": "http://arxiv.org/pdf/1305.4102v1", 
    "arxiv-id": "1305.4102v1"
},{
    "category": "cs.MM", 
    "author": "John Woods", 
    "title": "Survey on QoE\\QoS Correlation Models For Multimedia Services", 
    "publish": "2013-06-02T15:53:26Z", 
    "summary": "This paper presents a brief review of some existing correlation models which\nattempt to map Quality of Service (QoS) to Quality of Experience (QoE) for\nmultimedia services. The term QoS refers to deterministic network behaviour, so\nthat data can be transported with a minimum of packet loss, delay and maximum\nbandwidth. QoE is a subjective measure that involves human dimensions; it ties\ntogether user perception, expectations, and experience of the application and\nnetwork performance. The Holy Grail of subjective measurement is to predict it\nfrom the objective measurements; in other words predict QoE from a given set of\nQoS parameters or vice versa. Whilst there are many quality models for\nmultimedia, most of them are only partial solutions to predicting QoE from a\ngiven QoS. This contribution analyses a number of previous attempts and\noptimisation techniquesthat can reliably compute the weighting coefficients for\nthe QoS/QoE mapping.", 
    "link": "http://arxiv.org/pdf/1306.0221v1", 
    "arxiv-id": "1306.0221v1"
},{
    "category": "cs.DC", 
    "author": "Naison Gasela", 
    "title": "CUDA Based Performance Evaluation of the Computational Efficiency of the   DCT Image Compression Technique on Both the CPU and GPU", 
    "publish": "2013-06-06T11:11:16Z", 
    "summary": "Recent advances in computing such as the massively parallel GPUs (Graphical\nProcessing Units),coupled with the need to store and deliver large quantities\nof digital data especially images, has brought a number of challenges for\nComputer Scientists, the research community and other stakeholders. These\nchallenges, such as prohibitively large costs to manipulate the digital data\namongst others, have been the focus of the research community in recent years\nand has led to the investigation of image compression techniques that can\nachieve excellent results. One such technique is the Discrete Cosine Transform,\nwhich helps separate an image into parts of differing frequencies and has the\nadvantage of excellent energy-compaction.\n  This paper investigates the use of the Compute Unified Device Architecture\n(CUDA) programming model to implement the DCT based Cordic based Loeffler\nalgorithm for efficient image compression. The computational efficiency is\nanalyzed and evaluated under both the CPU and GPU. The PSNR (Peak Signal to\nNoise Ratio) is used to evaluate image reconstruction quality in this paper.\nThe results are presented and discussed.", 
    "link": "http://arxiv.org/pdf/1306.1373v1", 
    "arxiv-id": "1306.1373v1"
},{
    "category": "cs.CV", 
    "author": "Rabab K Ward", 
    "title": "Sparse Representation-based Image Quality Assessment", 
    "publish": "2013-06-12T07:22:36Z", 
    "summary": "A successful approach to image quality assessment involves comparing the\nstructural information between a distorted and its reference image. However,\nextracting structural information that is perceptually important to our visual\nsystem is a challenging task. This paper addresses this issue by employing a\nsparse representation-based approach and proposes a new metric called the\n\\emph{sparse representation-based quality} (SPARQ) \\emph{index}. The proposed\nmethod learns the inherent structures of the reference image as a set of basis\nvectors, such that any structure in the image can be represented by a linear\ncombination of only a few of those basis vectors. This sparse strategy is\nemployed because it is known to generate basis vectors that are qualitatively\nsimilar to the receptive field of the simple cells present in the mammalian\nprimary visual cortex. The visual quality of the distorted image is estimated\nby comparing the structures of the reference and the distorted images in terms\nof the learnt basis vectors resembling cortical cells. Our approach is\nevaluated on six publicly available subject-rated image quality assessment\ndatasets. The proposed SPARQ index consistently exhibits high correlation with\nthe subjective ratings on all datasets and performs better or at par with the\nstate-of-the-art.", 
    "link": "http://arxiv.org/pdf/1306.2727v1", 
    "arxiv-id": "1306.2727v1"
},{
    "category": "physics.optics", 
    "author": "Song-Jin Im", 
    "title": "Document watermarking based on digital holographic principle", 
    "publish": "2013-06-21T07:43:30Z", 
    "summary": "A new method for document watermarking based on the digital Fourier hologram\nis proposed. It applies the methods of digital image watermarking based on\nholographic principle presented previously in several papers into printed\ndocuments. Experimental results show that the proposed method can not only meet\nthe demand on invisibility, robustness and non-reproducibility of the document\nwatermark, and but also has other advantages compared with the conventional\nmethods for document securities such as embossed hologram, Lippmann photograph\nand halftone modulation.", 
    "link": "http://arxiv.org/pdf/1306.5066v1", 
    "arxiv-id": "1306.5066v1"
},{
    "category": "cs.MM", 
    "author": "Saloni Talwar", 
    "title": "Enhanced Tiny Encryption Algorithm with Embedding (ETEA)", 
    "publish": "2013-06-03T10:23:38Z", 
    "summary": "As computer systems become more pervasive and complex, security is\nincreasingly important. Secure Transmission refers to the transfer of data such\nas confidential or proprietary information over a secure channel. Many secure\ntransmission methods require a type of encryption. Secure transmissions are put\nin place to prevent attacks such as ARP spoofing and general data loss. Hence,\nin order to provide a better security mechanism, in this paper we propose\nEnhanced Tiny Encryption Algorithm with Embedding (ETEA), a data hiding\ntechnique called steganography along with the technique of encryption\n(Cryptography). The advantage of ETEA is that it incorporates cryptography and\nsteganography. The advantage proposed algorithm is that it hides the messages.", 
    "link": "http://arxiv.org/pdf/1306.6920v1", 
    "arxiv-id": "1306.6920v1"
},{
    "category": "cs.CV", 
    "author": "Firas A. Jassim", 
    "title": "Increasing Compression Ratio in PNG Images by k-Modulus Method for Image   Transformation", 
    "publish": "2013-06-28T22:01:48Z", 
    "summary": "Image compression is an important filed in image processing. The science\nwelcomes any tinny contribution that may increase the compression ratio by\nwhichever insignificant percentage. Therefore, the essential contribution in\nthis paper is to increase the compression ratio for the well known Portable\nNetwork Graphics (PNG) image file format. The contribution starts with\nconverting the original PNG image into k-Modulus Method (k-MM). Practically,\ntaking k equals to ten, and then the pixels in the constructed image will be\nintegers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm,\nthen the ability to reduce file size will increase according to the repetition\nin pixels in each k-by-k window according to the transformation done by k-MM.\nExperimental results show that the proposed technique (k-PNG) produces high\ncompression ratio with smaller file size in comparison to the original PNG\nfile.", 
    "link": "http://arxiv.org/pdf/1307.0036v1", 
    "arxiv-id": "1307.0036v1"
},{
    "category": "cs.MM", 
    "author": "Firas A. Jassim", 
    "title": "A Novel Steganography Algorithm for Hiding Text in Image using Five   Modulus Method", 
    "publish": "2013-07-02T09:33:43Z", 
    "summary": "The needs for steganographic techniques for hiding secret message inside\nimages have been arise. This paper is to create a practical steganographic\nimplementation to hide text inside grey scale images. The secret message is\nhidden inside the cover image using Five Modulus Method. The novel algorithm is\ncalled (ST-FMM. FMM which consists of transforming all the pixels within the\n5X5 window size into its corresponding multiples of 5. After that, the secret\nmessage is hidden inside the 5X5 window as a non-multiples of 5. Since the\nmodulus of non-multiples of 5 are 1,2,3 and 4, therefore; if the reminder is\none of these, then this pixel represents a secret character. The secret key\nthat has to be sent is the window size. The main advantage of this novel\nalgorithm is to keep the size of the cover image constant while the secret\nmessage increased in size. Peak signal-to-noise ratio is captured for each of\nthe images tested. Based on the PSNR value of each images, the stego image has\nhigh PSNR value. Hence this new steganography algorithm is very efficient to\nhide the data inside the image.", 
    "link": "http://arxiv.org/pdf/1307.0642v1", 
    "arxiv-id": "1307.0642v1"
},{
    "category": "cs.NI", 
    "author": "Bruce Hajek", 
    "title": "Single Video Performance Analysis for Video-on-Demand Systems", 
    "publish": "2013-07-02T20:56:37Z", 
    "summary": "We study the content placement problem for cache delivery video-on-demand\nsystems under static random network topologies with fixed heavy-tailed video\ndemand. The performance measure is the amount of server load; we wish to\nminimize the total download rate for all users from the server and maximize the\nrate from caches. Our approach reduces the analysis for multiple videos to\nconsideration of decoupled systems with a single video each. For each placement\npolicy, insights gained from the single video analysis carry back to the\noriginal multiple video content placement problem. Finally, we propose a hybrid\nplacement technique that achieves near optimal performance with low complexity.", 
    "link": "http://arxiv.org/pdf/1307.0849v1", 
    "arxiv-id": "1307.0849v1"
},{
    "category": "cs.CV", 
    "author": "Firas A. Jassim", 
    "title": "A Novel Robust Method to Add Watermarks to Bitmap Images by Fading   Technique", 
    "publish": "2013-07-03T21:35:17Z", 
    "summary": "Digital water marking is one of the essential fields in image security and\ncopyright protection. The proposed technique in this paper was based on the\nprinciple of protecting images by hide an invisible watermark in the image. The\ntechnique starts with merging the cover image and the watermark image with\nsuitable ratios, i.e., 99% from the cover image will be merged with 1% from the\nwatermark image. Technically, the fading process is irreversible but with the\nproposed technique, the probability to reconstruct the original watermark image\nis great. There is no perceptible difference between the original and\nwatermarked image by human eye. The experimental results show that the proposed\ntechnique proven its ability to hide images that have the same size of the\ncover image. Three performance measures were implemented to support the\nproposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the three\nmeasures have excellent values.", 
    "link": "http://arxiv.org/pdf/1307.1166v1", 
    "arxiv-id": "1307.1166v1"
},{
    "category": "cs.MM", 
    "author": "Sunil Bhooshan", 
    "title": "Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image   Fusion", 
    "publish": "2013-07-10T15:07:26Z", 
    "summary": "We develop a multiexposure image fusion method based on texture features,\nwhich exploits the edge preserving and intraregion smoothing property of\nnonlinear diffusion filters based on partial differential equations (PDE). With\nthe captured multiexposure image series, we first decompose images into base\nlayers and detail layers to extract sharp details and fine details,\nrespectively. The magnitude of the gradient of the image intensity is utilized\nto encourage smoothness at homogeneous regions in preference to inhomogeneous\nregions. Then, we have considered texture features of the base layer to\ngenerate a mask (i.e., decision mask) that guides the fusion of base layers in\nmultiresolution fashion. Finally, well-exposed fused image is obtained that\ncombines fused base layer and the detail layers at each scale across all the\ninput exposures. Proposed algorithm skipping complex High Dynamic Range Image\n(HDRI) generation and tone mapping steps to produce detail preserving image for\ndisplay on standard dynamic range display devices. Moreover, our technique is\neffective for blending flash/no-flash image pair and multifocus images, that\nis, images focused on different targets.", 
    "link": "http://arxiv.org/pdf/1307.2818v1", 
    "arxiv-id": "1307.2818v1"
},{
    "category": "cs.MM", 
    "author": "A. Renuka", 
    "title": "Comparison of secure and high capacity color image steganography   techniques in RGB and YCbCr domains", 
    "publish": "2013-07-11T09:26:21Z", 
    "summary": "Steganography is one of the methods used for secret communication.\nSteganography attempts to hide the existence of the information. The object\nused to hide the secret information is called as cover object. Images are the\nmost popular cover objects used for steganography. Different techniques have to\nbe used for color image steganography and grey scale image steganography since\nthey are stored in different ways. Color image are normally stored with 24 bit\ndepth and grey scale images are stored with 8 bit depth. Color images can hold\nlarge amount of secret information since they have three color components.\nDifferent color spaces namely RGB (Red Green Blue), HSV (Hue, Saturation,\nValue), YUV, YIQ, YCbCr (Luminance, Chrominance) etc. are used to represent\ncolor images. Color image steganography can be done in any color space domain.\nIn this paper color image steganography in RGB and YCbCr domain are compared.\nThe secret information considered is grey scale image. Since RGB is the common\nmethod of representation, hiding secret information in this format is not\nsecure.", 
    "link": "http://arxiv.org/pdf/1307.3026v1", 
    "arxiv-id": "1307.3026v1"
},{
    "category": "cs.MM", 
    "author": "Dah Ming Chiu", 
    "title": "Smart Streaming for Online Video Services", 
    "publish": "2013-07-17T11:23:09Z", 
    "summary": "Bandwidth consumption is a significant concern for online video service\nproviders. Practical video streaming systems usually use some form of HTTP\nstreaming (progressive download) to let users download the video at a faster\nrate than the video bitrate. Since users may quit before viewing the complete\nvideo, however, much of the downloaded video will be \"wasted\". To the extent\nthat users' departure behavior can be predicted, we develop smart streaming\nthat can be used to improve user QoE with limited server bandwidth or save\nbandwidth cost with unlimited server bandwidth. Through measurement, we extract\ncertain user behavior properties for implementing such smart streaming, and\ndemonstrate its advantage using prototype implementation as well as\nsimulations.", 
    "link": "http://arxiv.org/pdf/1307.4581v4", 
    "arxiv-id": "1307.4581v4"
},{
    "category": "cs.CR", 
    "author": "Rahul R Upadhyay", 
    "title": "Study of Encryption and Decryption of Wave File in Image Formats", 
    "publish": "2013-07-25T12:12:47Z", 
    "summary": "This paper describes a novel method of encrypting wave files in popular image\nformats like JPEG, TIF and PNG along with retrieving them from these image\nfiles. MATLAB software is used to perform matrix manipulation to encrypt and\ndecrypt sound files into and from image files. This method is not only a\nstenographic means but also a data compression technique.", 
    "link": "http://arxiv.org/pdf/1307.6711v2", 
    "arxiv-id": "1307.6711v2"
},{
    "category": "cs.NI", 
    "author": "Gustavo de Veciana", 
    "title": "NOVA: QoE-driven Optimization of DASH-based Video Delivery in Networks", 
    "publish": "2013-07-27T02:39:20Z", 
    "summary": "We consider the problem of optimizing video delivery for a network supporting\nvideo clients streaming stored video. Specifically, we consider the problem of\njointly optimizing network resource allocation and video quality adaptation.\nOur objective is to fairly maximize video clients' Quality of Experience (QoE)\nrealizing tradeoffs among the mean quality, temporal variability in quality,\nand fairness, incorporating user preferences on rebuffering and cost of video\ndelivery. We present a simple asymptotically optimal online algorithm, NOVA, to\nsolve the problem. NOVA is asynchronous, and using minimal communication,\ndistributes the tasks of resource allocation to network controller, and quality\nadaptation to respective video clients. Video quality adaptation in NOVA is\nalso optimal for standalone video clients, and is well suited for use with DASH\nframework. Further, we extend NOVA for use with more general QoE models,\nnetworks shared with other traffic loads and networks using fixed/legacy\nresource allocation.", 
    "link": "http://arxiv.org/pdf/1307.7210v3", 
    "arxiv-id": "1307.7210v3"
},{
    "category": "cs.MM", 
    "author": "Adel M. Alimi", 
    "title": "MAS for video objects segmentation and tracking based on active contours   and SURF descriptor", 
    "publish": "2013-08-01T19:45:23Z", 
    "summary": "In computer vision, video segmentation and tracking is an important\nchallenging issue. In this paper, we describe a new video sequences\nsegmentation and tracking algorithm based on MAS \"multi-agent systems\" and SURF\n\"Speeded Up Robust Features\". Our approach consists in modelling a multi-agent\nsystem for segmenting the first image from a video sequence and tracking\nobjects in the video sequences. The used agents are supervisor and explorator\nagents, they are communicating between them and they inspire in their behavior\nfrom active contours approaches. The tracking of objects is based on SURF\ndescriptors \"Speed Up Robust Features\". We used the DIMA platform and \"API\nAteji PX\" (an extension of the Java language to facilitate parallel programming\non heterogeneous architectures) to implement this algorithm. The experimental\nresults indicate that the proposed algorithm is more robust and faster than\nprevious approaches.", 
    "link": "http://arxiv.org/pdf/1308.0315v1", 
    "arxiv-id": "1308.0315v1"
},{
    "category": "cs.MM", 
    "author": "Paul Auguste Randriamitantsoa", 
    "title": "Improved Watermarking Scheme Using Discrete Cosine Transform and Schur   Decomposition", 
    "publish": "2013-08-02T08:20:13Z", 
    "summary": "Watermarking is a technique which consists in introducing a brand, the name\nor the logo of the author, in an image in order to protect it against illegal\ncopy. The capacity of the existing watermark channel is often limited. We\npropose in this paper a new robust method which consists in adding the\ntriangular matrix of the mark obtained after the Schur decomposition to the DCT\ntransform of the host image. The unitary matrix acts as secret key for the\nextraction of the mark. Unlike most watermarking algorithms, the host image and\nthe mark have the same size. The results show that our method is robust against\nattack techniques as : JPEG compression, colors reducing, adding noise,\nfiltering, cropping, low rotations, and histogram spreading.", 
    "link": "http://arxiv.org/pdf/1308.0435v1", 
    "arxiv-id": "1308.0435v1"
},{
    "category": "cs.CR", 
    "author": "Quansheng Liu", 
    "title": "Image Integrity Authentication Scheme Based On Fixed Point Theory", 
    "publish": "2013-08-03T10:22:35Z", 
    "summary": "Based on fixed point theory, this paper proposes a new scheme for image\nintegrity authentication, which is different from Digital Signature and Fragile\nWatermarking. A realization of the new scheme is given based on Gaussian\nConvolution and Deconvolution (GCD) functions. For a given image, if it is\ninvariant under a GCD function, we call it GCD fixed point image. An existence\ntheorem of fixed points for GCD functions is proved and an iterative algorithm\nis presented for finding fixed points. Experiments show that GCD fixed point\nimages perform well in transparence, fragility, security and tampering\nlocalization.", 
    "link": "http://arxiv.org/pdf/1308.0679v1", 
    "arxiv-id": "1308.0679v1"
},{
    "category": "cs.MM", 
    "author": "Adel M. Alimi", 
    "title": "Multimodal Approach for Video Surveillance Indexing and Retrieval", 
    "publish": "2013-08-06T01:21:35Z", 
    "summary": "In this paper, we present an overview of a multimodal system to indexing and\nsearching video sequence by the content that has been developed within the\nREGIMVid project. A large part of our system has been developed as part of\nTRECVideo evaluation. The MAVSIR platform provides High-level feature\nextraction from audio-visual content and concept/event-based video retrieval.\nWe illustrate the architecture of the system as well as provide an overview of\nthe descriptors supported to date. Then we demonstrate the usefulness of the\ntoolbox in the context of feature extraction, concepts/events learning and\nretrieval in large collections of video surveillance dataset. The results are\nencouraging as we are able to get good results on several event categories,\nwhile for all events we have gained valuable insights and experience.", 
    "link": "http://arxiv.org/pdf/1308.1150v1", 
    "arxiv-id": "1308.1150v1"
},{
    "category": "cs.IR", 
    "author": "Sebastian Michel", 
    "title": "Benchmarking Soundtrack Recommendation Systems with SRBench", 
    "publish": "2013-08-06T10:10:22Z", 
    "summary": "In this work, a benchmark to evaluate the retrieval performance of soundtrack\nrecommendation systems is proposed. Such systems aim at finding songs that are\nplayed as background music for a given set of images. The proposed benchmark is\nbased on preference judgments, where relevance is considered a continuous\nordinal variable and judgments are collected for pairs of songs with respect to\na query (i.e., set of images). To capture a wide variety of songs and images,\nwe use a large space of possible music genres, different emotions expressed\nthrough music, and various query-image themes. The benchmark consists of two\ntypes of relevance assessments: (i) judgments obtained from a user study, that\nserve as a \"gold standard\" for (ii) relevance judgments gathered through\nAmazon's Mechanical Turk. We report on an analysis of relevance judgments based\non different levels of user agreement and investigate the performance of two\nstate-of-the-art soundtrack recommendation systems using the proposed\nbenchmark.", 
    "link": "http://arxiv.org/pdf/1308.1224v1", 
    "arxiv-id": "1308.1224v1"
},{
    "category": "cs.MM", 
    "author": "Damodaram Avula", 
    "title": "An Efficient Transport Protocol for delivery of Multimedia An Efficient   Transport Protocol for delivery of Multimedia Content in Wireless Grids", 
    "publish": "2013-08-11T13:29:06Z", 
    "summary": "A grid computing system is designed for solving complicated scientific and\ncommercial problems effectively,whereas mobile computing is a traditional\ndistributed system having computing capability with mobility and adopting\nwireless communications. Media and Entertainment fields can take advantage from\nboth paradigms by applying its usage in gaming applications and multimedia data\nmanagement. Multimedia data has to be stored and retrieved in an efficient and\neffective manner to put it in use. In this paper, we proposed an application\nlayer protocol for delivery of multimedia data in wireless girds i.e.\nmultimedia grid protocol (MMGP). To make streaming efficient a new video\ncompression algorithm called dWave is designed and embedded in the proposed\nprotocol. This protocol will provide faster, reliable access and render an\nimperceptible QoS in delivering multimedia in wireless grid environment and\ntackles the challenging issues such as i) intermittent connectivity, ii) device\nheterogeneity, iii) weak security and iv) device mobility.", 
    "link": "http://arxiv.org/pdf/1308.2393v1", 
    "arxiv-id": "1308.2393v1"
},{
    "category": "cs.CR", 
    "author": "Beijing Chen", 
    "title": "A Novel Method for Image Integrity Authentication Based on Fixed Point   Theory", 
    "publish": "2013-08-24T13:38:24Z", 
    "summary": "Based on fixed point theory, this paper proposes a simple but efficient\nmethod for image integrity authentication, which is different from Digital\nSignature and Fragile Watermarking. By this method, any given image can be\ntransformed into a fixed point of a well-chosen function, which can be\nconstructed with periodic functions. The authentication can be realized due to\nthe fragility of the fixed points. The experiments show that 'Fixed Point\nImage' performs well in security, transparence, fragility and tampering\nlocalization.", 
    "link": "http://arxiv.org/pdf/1308.5326v1", 
    "arxiv-id": "1308.5326v1"
},{
    "category": "cs.NI", 
    "author": "R. Srikant", 
    "title": "Achieving the Optimal Steaming Capacity and Delay Using Random Regular   Digraphs in P2P Networks", 
    "publish": "2013-08-30T17:49:55Z", 
    "summary": "In earlier work, we showed that it is possible to achieve $O(\\log N)$\nstreaming delay with high probability in a peer-to-peer network, where each\npeer has as little as four neighbors, while achieving any arbitrary fraction of\nthe maximum possible streaming rate. However, the constant in the $O(log N)$\ndelay term becomes rather large as we get closer to the maximum streaming rate.\nIn this paper, we design an alternative pairing and chunk dissemination\nalgorithm that allows us to transmit at the maximum streaming rate while\nensuring that all, but a negligible fraction of the peers, receive the data\nstream with $O(\\log N)$ delay with high probability. The result is established\nby examining the properties of graph formed by the union of two or more random\n1-regular digraphs, i.e., directed graphs in which each node has an incoming\nand an outgoing node degree both equal to one.", 
    "link": "http://arxiv.org/pdf/1308.6807v1", 
    "arxiv-id": "1308.6807v1"
},{
    "category": "cs.CV", 
    "author": "Zhenyang Wu", 
    "title": "Tracking Deformable Parts via Dynamic Conditional Random Fields", 
    "publish": "2013-10-30T09:33:55Z", 
    "summary": "Despite the success of many advanced tracking methods in this area, tracking\ntargets with drastic variation of appearance such as deformation, view change\nand partial occlusion in video sequences is still a challenge in practical\napplications. In this letter, we take these serious tracking problems into\naccount simultaneously, proposing a dynamic graph based model to track object\nand its deformable parts at multiple resolutions. The method introduces well\nlearned structural object detection models into object tracking applications as\nprior knowledge to deal with deformation and view change. Meanwhile, it\nexplicitly formulates partial occlusion by integrating spatial potentials and\ntemporal potentials with an unparameterized occlusion handling mechanism in the\ndynamic conditional random field framework. Empirical results demonstrate that\nthe method outperforms state-of-the-art trackers on different challenging video\nsequences.", 
    "link": "http://arxiv.org/pdf/1311.0262v1", 
    "arxiv-id": "1311.0262v1"
},{
    "category": "cs.MM", 
    "author": "Neha Satam", 
    "title": "An Efficient Method for Image and Audio Steganography using Least   Significant Bit (LSB) Substitution", 
    "publish": "2013-09-26T12:01:18Z", 
    "summary": "In order to improve the data hiding in all types of multimedia data formats\nsuch as image and audio and to make hidden message imperceptible, a novel\nmethod for steganography is introduced in this paper. It is based on Least\nSignificant Bit (LSB) manipulation and inclusion of redundant noise as secret\nkey in the message. This method is applied to data hiding in images. For data\nhiding in audio, Discrete Cosine Transform (DCT) and Discrete Wavelet Transform\n(DWT) both are used. All the results displayed prove to be time-efficient and\neffective. Also the algorithm is tested for various numbers of bits. For those\nvalues of bits, Mean Square Error (MSE) and Peak-Signal-to-Noise-Ratio (PSNR)\nare calculated and plotted. Experimental results show that the stego-image is\nvisually indistinguishable from the original cover-image when n<=4, because of\nbetter PSNR which is achieved by this technique. The final results obtained\nafter steganography process does not reveal presence of any hidden message,\nthus qualifying the criteria of imperceptible message.", 
    "link": "http://arxiv.org/pdf/1311.1083v1", 
    "arxiv-id": "1311.1083v1"
},{
    "category": "cs.MM", 
    "author": "Sasu Tarkoma", 
    "title": "Mobile Multimedia Streaming Techniques : QoE and Energy Consumption   Perspective", 
    "publish": "2013-11-18T10:20:25Z", 
    "summary": "Multimedia streaming to mobile devices is challenging for two reasons. First,\nthe way content is delivered to a client must ensure that the user does not\nexperience a long initial playback delay or a distorted playback in the middle\nof a streaming session. Second, multimedia streaming applications are among the\nmost energy hungry applications in smartphones. The energy consumption mostly\ndepends on the delivery techniques and on the power management techniques of\nwireless access technologies (Wi-Fi, 3G, and 4G). In order to provide insights\non what kind of streaming techniques exist, how they work on different mobile\nplatforms, their efforts in providing smooth quality of experience, and their\nimpact on energy consumption of mobile phones, we did a large set of active\nmeasurements with several smartphones having both Wi-Fi and cellular network\naccess. Our analysis reveals five different techniques to deliver the content\nto the video players. The selection of a technique depends on the mobile\nplatform, device, player, quality, and service. The results from our traffic\nand power measurements allow us to conclude that none of the identified\ntechniques is optimal because they take none of the following facts into\naccount: access technology used, user behavior, and user preferences concerning\ndata waste. We point out the technique with optimal playback buffer\nconfiguration, which provides the most attractive trade-offs in particular\nsituations.", 
    "link": "http://arxiv.org/pdf/1311.4317v2", 
    "arxiv-id": "1311.4317v2"
},{
    "category": "cs.NI", 
    "author": "Carlo Kleber Da Silva Rodrigues", 
    "title": "Analyzing Peer Selection Policies for BitTorrent Multimedia On-Demand   Streaming Systems in Internet", 
    "publish": "2014-02-10T15:36:41Z", 
    "summary": "The adaptation of the BitTorrent protocol to multimedia on-demand streaming\nsystems essentially lies on the modification of its two core algorithms, namely\nthe piece and the peer selection policies, respectively. Much more attention\nhas though been given to the piece selection policy. Within this context, this\narticle proposes three novel peer selection policies for the design of\nBitTorrent-like protocols targeted at that type of systems: Select Balanced\nNeighbour Policy (SBNP), Select Regular Neighbour Policy (SRNP), and Select\nOptimistic Neighbour Policy (SONP). These proposals are validated through a\ncompetitive analysis based on simulations which encompass a variety of\nmultimedia scenarios, defined in function of important characterization\nparameters such as content type, content size, and client interactivity\nprofile. Service time, number of clients served and efficiency retrieving\ncoefficient are the performance metrics assessed in the analysis. The final\nresults mainly show that the novel proposals constitute scalable solutions that\nmay be considered for real project designs. Lastly, future work is included in\nthe conclusion of this paper.", 
    "link": "http://arxiv.org/pdf/1402.2187v1", 
    "arxiv-id": "1402.2187v1"
},{
    "category": "cs.MM", 
    "author": "J Hengmeechai", 
    "title": "Pervasive Image Computation: A Mobile Phone Application for getting   Information of the Images", 
    "publish": "2014-03-17T17:03:01Z", 
    "summary": "Although many of the information processing systems are text-based, much of\nthe information in the real life is generally multimedia objects, so there is a\nneed to define and standardize the frame works for multimedia-based information\nprocessing systems. In this paper we consider the application of such a system\nnamely pervasive image computation system, in which the user uses the cellphone\nfor taking the picture of the objects, and he wants to get some information\nabout them. We have implemented two architectures, the first one, called online\narchitecture, which the user sends the picture to the server and server sends\nthe picture information directly back to him. In the second one, which is\ncalled offline architecture, the user uploads the image in one public image\ndatabase such as Flickr and sends the ID of the image in this database to the\nserver. The server processes the image and adds the information of the image in\nthe database, and finally the user can connect to the database and download the\nimage information. The implementation results show that these architectures are\nvery flexible and could be easily extended to be used in more complicated\npervasive multimedia systems.", 
    "link": "http://arxiv.org/pdf/1403.4169v1", 
    "arxiv-id": "1403.4169v1"
},{
    "category": "cs.MM", 
    "author": "Stavros D. Nikolopoulos", 
    "title": "WaterRPG: A Graph-based Dynamic Watermarking Model for Software   Protection", 
    "publish": "2014-03-17T11:31:49Z", 
    "summary": "Software watermarking involves embedding a unique identifier or,\nequivalently, a watermark value within a software to prove owner's authenticity\nand thus to prevent or discourage copyright infringement. Towards the embedding\nprocess, several graph theoretic watermarking algorithmic techniques encode the\nwatermark values as graph structures and embed them in application programs.\nRecently, we presented an efficient codec system for encoding a watermark\nnumber $w$ as a reducible permutation graph $F[\\pi^*]$ through the use of\nself-inverting permutations $\\pi^*$. In this paper, we propose a dynamic\nwatermarking model, which we call WaterRPG, for embedding the watermark graph\n$F[\\pi^*]$ into an application program $P$. The main idea behind the proposed\nwatermarking model is a systematic use of appropriate calls of specific\nfunctions of the program $P$. More precisely, for a specific input $I_{key}$ of\nthe program $P$, our model takes the dynamic call-graph $G(P, I_{key})$ of $P$\nand the watermark graph $F[\\pi^*]$, and produces the watermarked program $P^*$\nhaving the following key property: its dynamic call-graph $G(P^*, I_{key})$ is\nisomorphic to the watermark graph $F[\\pi^*]$. Within this idea the program\n$P^*$ is produced by only altering appropriate calls of specific functions of\nthe input application program $P$. We have implemented our watermarking model\nWaterRPG in real application programs and evaluated its functionality under\nvarious and broadly used watermarking assessment criteria. The evaluation\nresults show that our model efficiently watermarks Java application programs\nwith respect to several watermarking metrics like data-rate, bytecode\ninstructions overhead, resiliency, time and space efficiency. Moreover, the\nembedded watermarks withstand several software obfuscation and optimization\nattacks.", 
    "link": "http://arxiv.org/pdf/1403.6658v1", 
    "arxiv-id": "1403.6658v1"
},{
    "category": "cs.NI", 
    "author": "Stefan Valentin", 
    "title": "Energy-Efficient Adaptive Video Transmission: Exploiting Rate   Predictions in Wireless Networks", 
    "publish": "2014-03-31T15:41:30Z", 
    "summary": "The unprecedented growth of mobile video traffic is adding significant\npressure to the energy drain at both the network and the end user. Energy\nefficient video transmission techniques are thus imperative to cope with the\nchallenge of satisfying user demand at sustainable costs. In this paper, we\ninvestigate how predicted user rates can be exploited for energy efficient\nvideo streaming with the popular HTTP-based Adaptive Streaming (AS) protocols\n(e.g. DASH). To this end, we develop an energy-efficient Predictive Green\nStreaming (PGS) optimization framework that leverages predictions of wireless\ndata rates to achieve the following objectives 1) minimize the required\ntransmission airtime without causing streaming interruptions, 2) minimize total\ndownlink Base Station (BS) power consumption for cases where BSs can be\nswitched off in deep sleep, and 3) enable a trade-off between AS quality and\nenergy consumption. Our framework is first formulated as a Mixed Integer Linear\nProgram (MILP) where decisions on multi-user rate allocation, video segment\nquality, and BS transmit power are jointly optimized. Then, to provide an\nonline solution, we present a polynomial-time heuristic algorithm that\ndecouples the PGS problem into multiple stages. We provide a performance\nanalysis of the proposed methods by simulations, and numerical results\ndemonstrate that the PGS framework yields significant energy savings.", 
    "link": "http://arxiv.org/pdf/1403.8055v1", 
    "arxiv-id": "1403.8055v1"
},{
    "category": "cs.CV", 
    "author": "Nagia M. Ghanem", 
    "title": "VSCAN: An Enhanced Video Summarization using Density-based Spatial   Clustering", 
    "publish": "2014-05-01T14:36:35Z", 
    "summary": "In this paper, we present VSCAN, a novel approach for generating static video\nsummaries. This approach is based on a modified DBSCAN clustering algorithm to\nsummarize the video content utilizing both color and texture features of the\nvideo frames. The paper also introduces an enhanced evaluation method that\ndepends on color and texture features. Video Summaries generated by VSCAN are\ncompared with summaries generated by other approaches found in the literature\nand those created by users. Experimental results indicate that the video\nsummaries generated by VSCAN have a higher quality than those generated by\nother approaches.", 
    "link": "http://arxiv.org/pdf/1405.0174v1", 
    "arxiv-id": "1405.0174v1"
},{
    "category": "cs.MM", 
    "author": "Wen Gao", 
    "title": "Image Restoration Using Joint Statistical Modeling in Space-Transform   Domain", 
    "publish": "2014-05-11T08:45:07Z", 
    "summary": "This paper presents a novel strategy for high-fidelity image restoration by\ncharacterizing both local smoothness and nonlocal self-similarity of natural\nimages in a unified statistical manner. The main contributions are three-folds.\nFirst, from the perspective of image statistics, a joint statistical modeling\n(JSM) in an adaptive hybrid space-transform domain is established, which offers\na powerful mechanism of combining local smoothness and nonlocal self-similarity\nsimultaneously to ensure a more reliable and robust estimation. Second, a new\nform of minimization functional for solving image inverse problem is formulated\nusing JSM under regularization-based framework. Finally, in order to make JSM\ntractable and robust, a new Split-Bregman based algorithm is developed to\nefficiently solve the above severely underdetermined inverse problem associated\nwith theoretical proof of convergence. Extensive experiments on image\ninpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise\nremoval applications verify the effectiveness of the proposed algorithm.", 
    "link": "http://arxiv.org/pdf/1405.3173v1", 
    "arxiv-id": "1405.3173v1"
},{
    "category": "cs.NI", 
    "author": "Athina Markopoulou", 
    "title": "MicroCast: Cooperative Video Streaming using Cellular and D2D   Connections", 
    "publish": "2014-05-14T18:55:26Z", 
    "summary": "We consider a group of mobile users, within proximity of each other, who are\ninterested in watching the same online video at roughly the same time. The\ncommon practice today is that each user downloads the video independently on\nher mobile device using her own cellular connection, which wastes access\nbandwidth and may also lead to poor video quality. We propose a novel\ncooperative system where each mobile device uses simultaneously two network\ninterfaces: (i) the cellular to connect to the video server and download parts\nof the video and (ii) WiFi to connect locally to all other devices in the group\nand exchange those parts. Devices cooperate to efficiently utilize all network\nresources and are able to adapt to varying wireless network conditions. In the\nlocal WiFi network, we exploit overhearing, and we further combine it with\nnetwork coding. The end result is savings in cellular bandwidth and improved\nuser experience (faster download) by a factor on the order up to the group\nsize.\n  We follow a complete approach, from theory to practice. First, we formulate\nthe problem using a network utility maximization (NUM) framework, decompose the\nproblem, and provide a distributed solution. Then, based on the structure of\nthe NUM solution, we design a modular system called MicroCast and we implement\nit as an Android application. We provide both simulation results of the NUM\nsolution and experimental evaluation of MicroCast on a testbed consisting of\nAndroid phones. We demonstrate that the proposed approach brings significant\nperformance benefits without battery penalty.", 
    "link": "http://arxiv.org/pdf/1405.3622v1", 
    "arxiv-id": "1405.3622v1"
},{
    "category": "cs.SD", 
    "author": "Daniele Giacobello", 
    "title": "Trends and Perspectives for Signal Processing in Consumer Audio", 
    "publish": "2014-05-19T19:10:15Z", 
    "summary": "The trend in media consumption towards streaming and portability offers new\nchallenges and opportunities for signal processing in audio and acoustics. The\nmost significant embodiment of this trend is that most music consumption now\nhappens on-the-go which has recently led to an explosion in headphone sales and\nsmall portable speakers. In particular, premium headphones offer a gateway for\na younger generation to experience high quality sound. Additionally, through\ntechnologies incorporating head-related transfer functions headphones can also\noffer unique new experiences in gaming, augmented reality, and surround sound\nlistening. Home audio has also seen a transition to smaller sound systems in\nthe form of sound bars. This speaker configuration offers many exciting\nchallenges for surround sound reproduction which has traditionally used five\nspeakers surrounding the listener. Furthermore, modern home entertainment\nsystems offer more than just content delivery; users now expect wireless and\nconnected smart devices with video conferencing, gaming, and other interactive\ncapabilities. With this comes challenges for voice interaction at a distance\nand in demanding conditions, e.g., during content playback, and opportunities\nfor new smart interactive experiences based on awareness of environment and\nuser biometrics.", 
    "link": "http://arxiv.org/pdf/1405.4843v1", 
    "arxiv-id": "1405.4843v1"
},{
    "category": "cs.MM", 
    "author": "Sugata Sanyal", 
    "title": "Steganalysis: Detecting LSB Steganographic Techniques", 
    "publish": "2014-05-20T15:21:52Z", 
    "summary": "Steganalysis means analysis of stego images. Like cryptanalysis, steganalysis\nis used to detect messages often encrypted using secret key from stego images\nproduced by steganography techniques. Recently lots of new and improved\nsteganography techniques are developed and proposed by researchers which\nrequire robust steganalysis techniques to detect the stego images having\nminimum false alarm rate. This paper discusses about the different Steganalysis\ntechniques and help to understand how, where and when this techniques can be\nused based on different situations.", 
    "link": "http://arxiv.org/pdf/1405.5119v1", 
    "arxiv-id": "1405.5119v1"
},{
    "category": "cs.CR", 
    "author": "Munetoshi Iwakiri", 
    "title": "An Incomplete Cryptography based Digital Rights Management with DCFF", 
    "publish": "2014-01-21T03:45:22Z", 
    "summary": "In general, DRM (Digital Rights Management) system is responsible for the\nsafe distribution of digital content, however, DRM system is achieved with\nindividual function modules of cryptography, watermarking and so on. In this\ntypical system flow, it has a problem that all original digital contents are\ntemporarily disclosed with perfect condition via decryption process. In this\npaper, we propose the combination of the differential codes and fragile\nfingerprinting (DCFF) method based on incomplete cryptography that holds\npromise for a better compromise between practicality and security for emerging\ndigital rights management applications. Experimental results with simulation\nconfirmed that DCFF keeps compatibility with standard JPEG codec, and revealed\nthat the proposed method is suitable for DRM in the network distribution\nsystem.", 
    "link": "http://arxiv.org/pdf/1405.5203v1", 
    "arxiv-id": "1405.5203v1"
},{
    "category": "cs.MM", 
    "author": "Trevor Darrell", 
    "title": "Detection Bank: An Object Detection Based Video Representation for   Multimedia Event Recognition", 
    "publish": "2014-05-28T02:07:29Z", 
    "summary": "While low-level image features have proven to be effective representations\nfor visual recognition tasks such as object recognition and scene\nclassification, they are inadequate to capture complex semantic meaning\nrequired to solve high-level visual tasks such as multimedia event detection\nand recognition. Recognition or retrieval of events and activities can be\nimproved if specific discriminative objects are detected in a video sequence.\nIn this paper, we propose an image representation, called Detection Bank, based\non the detection images from a large number of windowed object detectors where\nan image is represented by different statistics derived from these detections.\nThis representation is extended to video by aggregating the key frame level\nimage representations through mean and max pooling. We empirically show that it\ncaptures complementary information to state-of-the-art representations such as\nSpatial Pyramid Matching and Object Bank. These descriptors combined with our\nDetection Bank representation significantly outperforms any of the\nrepresentations alone on TRECVID MED 2011 data.", 
    "link": "http://arxiv.org/pdf/1405.7102v2", 
    "arxiv-id": "1405.7102v2"
},{
    "category": "cs.MM", 
    "author": "Patrick Seeling", 
    "title": "Towards Quality of Experience Determination for Video in Augmented   Binocular Vision Scenarios", 
    "publish": "2014-06-04T00:14:06Z", 
    "summary": "With the continuous growth in the consumer markets of mobile smartphones and\nincreasingly in augmented reality wearable devices, several avenues of research\ninvestigate the relationships between the quality perceived by mobile users and\nthe delivery mechanisms at play to support a high quality of experience for\nmobile users. In this paper, we present the first study that evaluates the\nrelationships of mobile movie quality and the viewer-perceived quality thereof\nin an augmented reality setting with see-through devices. We find that\nparticipants tend to overestimate the video quality and exhibit a significant\nvariation of accuracy that leans onto the movie content and its dynamics. Our\nfindings, thus, can broadly impact future media adaptation and delivery\nmechanisms for this new display format of mobile multimedia.", 
    "link": "http://arxiv.org/pdf/1406.0912v3", 
    "arxiv-id": "1406.0912v3"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "On Importance of Steganographic Cost For Network Steganography", 
    "publish": "2014-06-10T12:07:22Z", 
    "summary": "Network steganography encompasses the information hiding techniques that can\nbe applied in communication network environments and that utilize hidden data\ncarriers for this purpose. In this paper we introduce a characteristic called\nsteganographic cost which is an indicator for the degradation or distortion of\nthe carrier caused by the application of the steganographic method. Based on\nexemplary cases for single- and multi-method steganographic cost analyses we\nobserve that it can be an important characteristic that allows to express\nhidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR\n(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.\nSteganographic cost can moreover be helpful to analyse the relationships\nbetween two or more steganographic methods applied to the same hidden data\ncarrier.", 
    "link": "http://arxiv.org/pdf/1406.2519v1", 
    "arxiv-id": "1406.2519v1"
},{
    "category": "cs.HC", 
    "author": "Amy Banic", 
    "title": "Low-cost Augmented Reality prototype for controlling network devices", 
    "publish": "2014-06-12T04:46:32Z", 
    "summary": "With the evolution of mobile devices, and smart-phones in particular, comes\nthe ability to create new experiences that enhance the way we see, interact,\nand manipulate objects, within the world that surrounds us. It is now possible\nto blend data from our senses and our devices in numerous ways that simply were\nnot possible before using Augmented Reality technology. In a near future, when\nall of the office devices as well as your personal electronic gadgets are on a\ncommon wireless network, operating them using a universal remote controller\nwould be possible. This paper presents an off-the-shelf, low-cost prototype\nthat leverages the Augmented Reality technology to deliver a novel and\ninteractive way of operating office network devices around using a mobile\ndevice. We believe this type of system may provide benefits to controlling\nmultiple integrated devices and visualizing interconnectivity or utilizing\nvisual elements to pass information from one device to another, or may be\nespecially beneficial to control devices when interacting with them physically\nmay be difficult or pose danger or harm.", 
    "link": "http://arxiv.org/pdf/1406.3117v1", 
    "arxiv-id": "1406.3117v1"
},{
    "category": "cs.HC", 
    "author": "Amy Banic", 
    "title": "3DTouch: A wearable 3D input device with an optical sensor and a 9-DOF   inertial measurement unit", 
    "publish": "2014-06-21T06:32:35Z", 
    "summary": "We present 3DTouch, a novel 3D wearable input device worn on the fingertip\nfor 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D\ninput device that is self-contained, mobile, and universally working across\nvarious 3D platforms. This paper presents a low-cost solution to designing and\nimplementing such a device. Our approach relies on relative positioning\ntechnique using an optical laser sensor and a 9-DOF inertial measurement unit.\n  3DTouch is self-contained, and designed to universally work on various 3D\nplatforms. The device employs touch input for the benefits of passive haptic\nfeedback, and movement stability. On the other hand, with touch interaction,\n3DTouch is conceptually less fatiguing to use over many hours than 3D spatial\ninput devices. We propose a set of 3D interaction techniques including\nselection, translation, and rotation using 3DTouch. An evaluation also\ndemonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for\nsubtle touch interaction in 3D space. Modular solutions like 3DTouch opens up a\nwhole new design space for interaction techniques to further develop on.", 
    "link": "http://arxiv.org/pdf/1406.5581v2", 
    "arxiv-id": "1406.5581v2"
},{
    "category": "cs.MM", 
    "author": "Georg Groh", 
    "title": "Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis", 
    "publish": "2014-06-23T18:29:59Z", 
    "summary": "In this contribution, we will discuss a prototype that allows a group of\nusers to design sound collaboratively in real time using a multi-touch\ntabletop. We make use of a machine learning method to generate a mapping from\nperceptual audio features to synthesis parameters. This mapping is then used\nfor visualization and interaction. Finally, we discuss the results of a\ncomparative evaluation study.", 
    "link": "http://arxiv.org/pdf/1406.6012v1", 
    "arxiv-id": "1406.6012v1"
},{
    "category": "cs.MM", 
    "author": "Sakuntala S. Pillai", 
    "title": "Performance Comparison of Linear Prediction based Vocoders in Linux   Platform", 
    "publish": "2014-06-25T06:45:02Z", 
    "summary": "Linear predictive coders form an important class of speech coders. This paper\ndescribes the software level implementation of linear prediction based\nvocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)\nand Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s\nand 2.4 kb/s respectively. The C programs of the vocoders have been compiled\nand executed in Linux platform. Subjective testing with the help of Mean\nOpinion Score test has been performed. Waveform analysis has been done using\nPraat and Adobe Audition software. The results show that MELP and CELP produce\ncomparable quality while the quality of LD-CELP coder is much higher, at the\nexpense of higher bit rate.", 
    "link": "http://arxiv.org/pdf/1406.6473v1", 
    "arxiv-id": "1406.6473v1"
},{
    "category": "cs.NI", 
    "author": "Ramin Khalili", 
    "title": "MSPlayer: Multi-Source and multi-Path LeverAged YoutubER", 
    "publish": "2014-06-26T05:34:19Z", 
    "summary": "Online video streaming through mobile devices has become extremely popular\nnowadays. YouTube, for example, reported that the percentage of its traffic\nstreaming to mobile devices has soared from 6% to more than 40% over the past\ntwo years. Moreover, people are constantly seeking to stream high quality video\nfor better experience while often suffering from limited bandwidth. Thanks to\nthe rapid deployment of content delivery networks (CDNs), popular videos are\nnow replicated at different sites, and users can stream videos from close-by\nlocations with low latencies. As mobile devices nowadays are equipped with\nmultiple wireless interfaces (e.g., WiFi and 3G/4G), aggregating bandwidth for\nhigh definition video streaming has become possible.\n  We propose a client-based video streaming solution, MSPlayer, that takes\nadvantage of multiple video sources as well as multiple network paths through\ndifferent interfaces. MSPlayer reduces start-up latency and provides high\nquality video streaming and robust data transport in mobile scenarios. We\nexperimentally demonstrate our solution on a testbed and through the YouTube\nvideo service.", 
    "link": "http://arxiv.org/pdf/1406.6772v2", 
    "arxiv-id": "1406.6772v2"
},{
    "category": "cs.MM", 
    "author": "Shahram Shirani", 
    "title": "Subjective and Objective Quality Assessment of Image: A Survey", 
    "publish": "2014-06-30T16:25:00Z", 
    "summary": "With the increasing demand for image-based applications, the efficient and\nreliable evaluation of image quality has increased in importance. Measuring the\nimage quality is of fundamental importance for numerous image processing\napplications, where the goal of image quality assessment (IQA) methods is to\nautomatically evaluate the quality of images in agreement with human quality\njudgments. Numerous IQA methods have been proposed over the past years to\nfulfill this goal. In this paper, a survey of the quality assessment methods\nfor conventional image signals, as well as the newly emerged ones, which\nincludes the high dynamic range (HDR) and 3-D images, is presented. A\ncomprehensive explanation of the subjective and objective IQA and their\nclassification is provided. Six widely used subjective quality datasets, and\nperformance measures are reviewed. Emphasis is given to the full-reference\nimage quality assessment (FR-IQA) methods, and 9 often-used quality measures\n(including mean squared error (MSE), structural similarity index (SSIM),\nmulti-scale structural similarity index (MS-SSIM), visual information fidelity\n(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),\nfeature similarity measure for color images (FSIMC), dynamic range independent\nmeasure (DRIM), and tone-mapped images quality index (TMQI)) are carefully\ndescribed, and their performance and computation time on four subjective\nquality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is\nprovided and the issues related to this area of research are reviewed.", 
    "link": "http://arxiv.org/pdf/1406.7799v1", 
    "arxiv-id": "1406.7799v1"
},{
    "category": "cs.MM", 
    "author": "Val\u00e9rie Gouranton", 
    "title": "Sonic interaction with a virtual orchestra of factory machinery", 
    "publish": "2014-07-06T06:29:34Z", 
    "summary": "This paper presents an immersive application where users receive sound and\nvisual feedbacks on their interactions with a virtual environment. In this\napplication, the users play the part of conductors of an orchestra of factory\nmachines since each of their actions on interaction devices triggers a pair of\nvisual and audio responses. Audio stimuli were spatialized around the listener.\nThe application was exhibited during the 2013 Science and Music day and\ndesigned to be used in a large immersive system with head tracking, shutter\nglasses and a 10.2 loudspeaker configuration.", 
    "link": "http://arxiv.org/pdf/1407.2221v1", 
    "arxiv-id": "1407.2221v1"
},{
    "category": "cs.CR", 
    "author": "Vishal Shrivastava", 
    "title": "Analysis of Attacks on Hybrid DWT-DCT Algorithm for Digital Image   Watermarking With MATLAB", 
    "publish": "2014-07-17T17:07:13Z", 
    "summary": "Watermarking algorithms needs properties of robustness and perceptibility.\nBut these properties are affected by different -2 types of attacks performed on\nwatermarked images. The goal of performing attacks is destroy the information\nof watermark hidden in the watermarked image. So every Algorithms should be\npreviously tested by developers so that it would not affected by attacks.", 
    "link": "http://arxiv.org/pdf/1407.4738v1", 
    "arxiv-id": "1407.4738v1"
},{
    "category": "cs.HC", 
    "author": "Wenping Wang", 
    "title": "Speaker-following Video Subtitles", 
    "publish": "2014-07-19T05:06:16Z", 
    "summary": "We propose a new method for improving the presentation of subtitles in video\n(e.g. TV and movies). With conventional subtitles, the viewer has to constantly\nlook away from the main viewing area to read the subtitles at the bottom of the\nscreen, which disrupts the viewing experience and causes unnecessary eyestrain.\nOur method places on-screen subtitles next to the respective speakers to allow\nthe viewer to follow the visual content while simultaneously reading the\nsubtitles. We use novel identification algorithms to detect the speakers based\non audio and visual information. Then the placement of the subtitles is\ndetermined using global optimization. A comprehensive usability study indicated\nthat our subtitle placement method outperformed both conventional\nfixed-position subtitling and another previous dynamic subtitling method in\nterms of enhancing the overall viewing experience and reducing eyestrain.", 
    "link": "http://arxiv.org/pdf/1407.5145v1", 
    "arxiv-id": "1407.5145v1"
},{
    "category": "cs.HC", 
    "author": "Mohamed Cheriet", 
    "title": "Quality of Experience (QoE) beyond Quality of Service (QoS) as its   baseline: QoE at the Interface of Experience Domains", 
    "publish": "2014-07-18T15:13:06Z", 
    "summary": "In this work, a new approach to the definition of the quality of experience\nis presented. By considering the quality of service as a baseline, that portion\nof the QoE that can be inferred from the QoS is excluded, and then the rest of\nthe QoE is approached with the notion of QoE at a Boundary (QoEaaB). With the\nQoEaaB as the core of the proposed approach, various potential boundaries, and\ntheir associated unseen opportunities to improve the QoE are discussed. In\nparticular, property, contract, SLA, and content are explored in terms of their\nboundaries and also their associated QoEaaB. With an interest in online video\ndelivery, management of resource sharing and isolation associated with\nmulti-tenant operations is considered. It is concluded that the proposed QoEaaB\ncan bring a new perspective in QoE modeling and assessment toward a more\nenriched approach to improving the experience based on innovation and deep\nconnectivity among actors.", 
    "link": "http://arxiv.org/pdf/1407.5527v4", 
    "arxiv-id": "1407.5527v4"
},{
    "category": "cs.MM", 
    "author": "Jichun Yang", 
    "title": "A Digital Watermarking Approach Based on DCT Domain Combining QR Code   and Chaotic Theory", 
    "publish": "2014-07-28T07:04:04Z", 
    "summary": "This paper proposes a robust watermarking approach based on Discrete Cosine\nTransform domain that combines Quick Response Code and chaotic system.", 
    "link": "http://arxiv.org/pdf/1407.7337v1", 
    "arxiv-id": "1407.7337v1"
},{
    "category": "cs.MM", 
    "author": "H. Hernan Moraldo", 
    "title": "An Approach for Text Steganography Based on Markov Chains", 
    "publish": "2014-09-02T22:59:52Z", 
    "summary": "A text steganography method based on Markov chains is introduced, together\nwith a reference implementation. This method allows for information hiding in\ntexts that are automatically generated following a given Markov model. Other\nMarkov - based systems of this kind rely on big simplifications of the language\nmodel to work, which produces less natural looking and more easily detectable\ntexts. The method described here is designed to generate texts within a good\napproximation of the original language model provided.", 
    "link": "http://arxiv.org/pdf/1409.0915v1", 
    "arxiv-id": "1409.0915v1"
},{
    "category": "cs.MM", 
    "author": "Hosssam S. Hassenein", 
    "title": "Toward Green Media Delivery: Location-Aware Opportunities and Approaches", 
    "publish": "2014-09-03T16:28:36Z", 
    "summary": "Mobile media has undoubtedly become the predominant source of traffic in\nwireless networks. The result is not only congestion and poor\nQuality-of-Experience, but also an unprecedented energy drain at both the\nnetwork and user devices. In order to sustain this continued growth, novel\ndisruptive paradigms of media delivery are urgently needed. We envision that\ntwo key contemporary advancements can be leveraged to develop greener media\ndelivery platforms: 1) the proliferation of navigation hardware and software in\nmobile devices has created an era of location-awareness, where both the current\nand future user locations can be predicted; and 2) the rise of context-aware\nnetwork architectures and self-organizing functionalities is enabling context\nsignaling and in-network adaptation. With these developments in mind, this\narticle investigates the opportunities of exploiting location-awareness to\nenable green end-to-end media delivery. In particular, we discuss and propose\napproaches for location-based adaptive video quality planning, in-network\ncaching, content prefetching, and long-term radio resource management. To\nprovide insights on the energy savings, we then present a cross-layer framework\nthat jointly optimizes resource allocation and multi-user video quality using\nlocation predictions. Finally, we highlight some of the future research\ndirections for location-aware media delivery in the conclusion.", 
    "link": "http://arxiv.org/pdf/1409.1148v1", 
    "arxiv-id": "1409.1148v1"
},{
    "category": "cs.IR", 
    "author": "Martha Larson", 
    "title": "A Crowdsourcing Procedure for the Discovery of Non-Obvious Attributes of   Social Image", 
    "publish": "2014-09-06T21:26:28Z", 
    "summary": "Research on mid-level image representations has conventionally concentrated\nrelatively obvious attributes and overlooked non-obvious attributes, i.e.,\ncharacteristics that are not readily observable when images are viewed\nindependently of their context or function. Non-obvious attributes are not\nnecessarily easily nameable, but nonetheless they play a systematic role in\npeople`s interpretation of images. Clusters of related non-obvious attributes,\ncalled interpretation dimensions, emerge when people are asked to compare\nimages, and provide important insight on aspects of social images that are\nconsidered relevant. In contrast to aesthetic or affective approaches to image\nanalysis, non-obvious attributes are not related to the personal perspective of\nthe viewer. Instead, they encode a conventional understanding of the world,\nwhich is tacit, rather than explicitly expressed. This paper introduces a\nprocedure for discovering non-obvious attributes using crowdsourcing. We\ndiscuss this procedure using a concrete example of a crowdsourcing task on\nAmazon Mechanical Turk carried out in the domain of fashion. An analysis\ncomparing discovered non-obvious attributes with user tags demonstrated the\nadded value delivered by our procedure.", 
    "link": "http://arxiv.org/pdf/1409.2668v1", 
    "arxiv-id": "1409.2668v1"
},{
    "category": "cs.MM", 
    "author": "Med Salim Bouhlel", 
    "title": "A new Watermarking Technique for Medical Image using Hierarchical   Encryption", 
    "publish": "2014-09-16T11:27:38Z", 
    "summary": "In recent years, characterized by the innovation of technology and the\ndigital revolution, the field of media has become important. The transfer and\nexchange of multimedia data and duplication have become major concerns of\nresearchers. Consequently, protecting copyrights and ensuring service safety is\nneeded. Cryptography has a specific role, is to protect secret files against\nunauthorized access. In this paper, a hierarchical cryptosystem algorithm based\non Logistic Map chaotic systems is proposed. The results show that the proposed\nmethod improves the security of the image. Experimental results on a database\nof 200 medical images show that the proposed method significantly gives better\nresults.", 
    "link": "http://arxiv.org/pdf/1409.4587v1", 
    "arxiv-id": "1409.4587v1"
},{
    "category": "cs.CY", 
    "author": "Abu Kalam Shamsuddin", 
    "title": "Child Education Through Animation: An Experimental Study", 
    "publish": "2014-11-07T13:08:07Z", 
    "summary": "Teachers have tried to teach their students by introducing text books along\nwith verbal instructions in traditional education system. However, teaching and\nlearning methods could be changed for developing Information and Communication\nTechnology. It's time to adapt students with interactive learning system so\nthat they can improve their learning, catching, and memorizing capabilities. It\nis indispensable to create high quality and realistic leaning environment for\nstudents. Visual learning can be easier to understand and deal with their\nlearning. We developed visual learning materials in the form of video for\nstudents of primary level using different multimedia application tools. The\nobjective of this paper is to examine the impact of students abilities to\nacquire new knowledge or skills through visual learning materials and blended\nleaning that is integration of visual learning materials with teachers\ninstructions. We visited a primary school in Dhaka city for this study and\nconducted teaching with three different groups of students, (i) teacher taught\nstudents by traditional system on same materials and marked level of students\nability to adapt by a set of questions, (ii) another group was taught with only\nvisual learning material and assessment was done with 15 questionnaires, (iii)\nthe third group was taught with the video of solar system combined with\nteachers instructions and assessed with the same questionnaires. This\nintegration of visual materials with verbal instructions is a blended approach\nof learning. The interactive blended approach greatly promoted students ability\nof acquisition of knowledge and skills. Students response and perception were\nvery positive towards the blended technique than the other two methods. This\ninteractive blending leaning system may be an appropriate method especially for\nschool children.", 
    "link": "http://arxiv.org/pdf/1411.1897v1", 
    "arxiv-id": "1411.1897v1"
},{
    "category": "cs.CY", 
    "author": "Abu Kalam Shamsuddin", 
    "title": "Interactive Digital Learning Materials for Kindergarten Students in   Bangladesh", 
    "publish": "2014-11-08T02:21:43Z", 
    "summary": "The pedagogy of teaching and learning has changed with the proliferation of\ncommunication technology and it is necessary to develop interactive learning\nmaterials for children that may improve their learning, catching, and\nmemorizing capabilities. Perhaps, one of the most important innovations in the\nage of technology is multimedia and its application. It is imperative to create\nhigh quality and realistic learning environment for children. Interactive\nlearning materials can be easier to understand and deal with their first\nlearning. We developed some interactive learning materials in the form of a\nvideo for Playgroup using multimedia application tools. This study investigated\nthe impact of students' abilities to acquire new knowledge or skills through\ninteractive learning materials. We visited one kindergartens (Nursery schools),\ninterviewed class teachers about their teaching methods and level of students'\nability of recognizing English alphabets, pictures, etc. The course teachers\nwere provided interactive learning materials to show their playgroups for a\nnumber of sessions. The video included English alphabets with related words and\npictures, and motivational fun. We noticed that almost all children were very\ninterested to interact with their leaning video. The students were assessed\nindividually and asked to recognize the alphabets, and pictures. The students\nadapted with their first alphabets very quickly. However, there were individual\ndifferences in their cognitive development. This interactive multimedia can be\nan alternative to traditional pedagogy for teaching playgroups.", 
    "link": "http://arxiv.org/pdf/1411.2075v1", 
    "arxiv-id": "1411.2075v1"
},{
    "category": "cs.MM", 
    "author": "Yiannis Andreopoulos", 
    "title": "Precision-Energy-Throughput Scaling Of Generic Matrix Multiplication and   Convolution Kernels Via Linear Projections", 
    "publish": "2014-11-11T15:59:35Z", 
    "summary": "Generic matrix multiplication (GEMM) and one-dimensional\nconvolution/cross-correlation (CONV) kernels often constitute the bulk of the\ncompute- and memory-intensive processing within image/audio recognition and\nmatching systems. We propose a novel method to scale the energy and processing\nthroughput of GEMM and CONV kernels for such error-tolerant multimedia\napplications by adjusting the precision of computation. Our technique employs\nlinear projections to the input matrix or signal data during the top-level GEMM\nand CONV blocking and reordering. The GEMM and CONV kernel processing then uses\nthe projected inputs and the results are accumulated to form the final outputs.\nThroughput and energy scaling takes place by changing the number of projections\ncomputed by each kernel, which in turn produces approximate results, i.e.\nchanges the precision of the performed computation. Results derived from a\nvoltage- and frequency-scaled ARM Cortex A15 processor running face recognition\nand music matching algorithms demonstrate that the proposed approach allows for\n280%~440% increase of processing throughput and 75%~80% decrease of energy\nconsumption against optimized GEMM and CONV kernels without any impact in the\nobtained recognition or matching accuracy. Even higher gains can be obtained if\none is willing to tolerate some reduction in the accuracy of the recognition\nand matching applications.", 
    "link": "http://arxiv.org/pdf/1411.2860v1", 
    "arxiv-id": "1411.2860v1"
},{
    "category": "cs.CR", 
    "author": "Hafiz Malik", 
    "title": "Audio Splicing Detection and Localization Using Environmental Signature", 
    "publish": "2014-11-26T02:10:26Z", 
    "summary": "Audio splicing is one of the most common manipulation techniques in the area\nof audio forensics. In this paper, the magnitudes of acoustic channel impulse\nresponse and ambient noise are proposed as the environmental signature.\nSpecifically, the spliced audio segments are detected according to the\nmagnitude correlation between the query frames and reference frames via a\nstatically optimal threshold. The detection accuracy is further refined by\ncomparing the adjacent frames. The effectiveness of the proposed method is\ntested on two data sets. One is generated from TIMIT database, and the other\none is made in four acoustic environments using a commercial grade microphones.\nExperimental results show that the proposed method not only detects the\npresence of spliced frames, but also localizes the forgery segments with near\nperfect accuracy. Comparison results illustrate that the identification\naccuracy of the proposed scheme is higher than the previous schemes. In\naddition, experimental results also show that the proposed scheme is robust to\nMP3 compression attack, which is also superior to the previous works.", 
    "link": "http://arxiv.org/pdf/1411.7084v1", 
    "arxiv-id": "1411.7084v1"
},{
    "category": "cs.CV", 
    "author": "Ram Nevatia", 
    "title": "Temporal Localization of Fine-Grained Actions in Videos by Domain   Transfer from Web Images", 
    "publish": "2015-04-04T05:40:55Z", 
    "summary": "We address the problem of fine-grained action localization from temporally\nuntrimmed web videos. We assume that only weak video-level annotations are\navailable for training. The goal is to use these weak labels to identify\ntemporal segments corresponding to the actions, and learn models that\ngeneralize to unconstrained web videos. We find that web images queried by\naction names serve as well-localized highlights for many actions, but are\nnoisily labeled. To solve this problem, we propose a simple yet effective\nmethod that takes weak video labels and noisy image labels as input, and\ngenerates localized action frames as output. This is achieved by cross-domain\ntransfer between video frames and web images, using pre-trained deep\nconvolutional neural networks. We then use the localized action frames to train\naction recognition models with long short-term memory networks. We collect a\nfine-grained sports action data set FGA-240 of more than 130,000 YouTube\nvideos. It has 240 fine-grained actions under 85 sports activities. Convincing\nresults are shown on the FGA-240 data set, as well as the THUMOS 2014\nlocalization data set with untrimmed training videos.", 
    "link": "http://arxiv.org/pdf/1504.00983v2", 
    "arxiv-id": "1504.00983v2"
},{
    "category": "cs.HC", 
    "author": "Pablo Gagliardo", 
    "title": "Preprint A Game Based Assistive Tool for Rehabilitation of Dysphonic   Patients", 
    "publish": "2015-04-04T17:43:54Z", 
    "summary": "This is the preprint version of our paper on 3rd International Workshop on\nVirtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015\n(VR2015). An assistive training tool for rehabilitation of dysphonic patients\nis designed and developed according to the practical clinical needs. The\nassistive tool employs a space flight game as the attractive logic part, and\nmicrophone arrays as input device, which is getting rid of ambient noise by\nsetting a specific orientation. The therapist can guide the patient to play the\ngame as well as the voice training simultaneously side by side, while not\ninterfere the patient voice. The voice information can be recorded and\nextracted for evaluating the long-time rehabilitation progress. This paper\noutlines a design science approach for the development of an initial useful\nsoftware prototype of such a tool, considering 'Intuitive', 'Entertainment',\n'Incentive' as main design factors.", 
    "link": "http://arxiv.org/pdf/1504.01030v2", 
    "arxiv-id": "1504.01030v2"
},{
    "category": "physics.optics", 
    "author": "Tomoyoshi Ito", 
    "title": "Improvement of the image quality of random phase--free holography using   an iterative method", 
    "publish": "2015-04-06T21:51:09Z", 
    "summary": "Our proposed method of random phase-free holography using virtual convergence\nlight can obtain large reconstructed images exceeding the size of the hologram,\nwithout the assistance of random phase. The reconstructed images have\nlow-speckle noise in the amplitude and phase-only holograms (kinoforms);\nhowever, in low-resolution holograms, we obtain a degraded image quality\ncompared to the original image. We propose an iterative random phase-free\nmethod with virtual convergence light to address this problem.", 
    "link": "http://arxiv.org/pdf/1504.01424v1", 
    "arxiv-id": "1504.01424v1"
},{
    "category": "cs.CV", 
    "author": "Xiangyang Xue", 
    "title": "Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for   Video Classification", 
    "publish": "2015-04-07T11:53:46Z", 
    "summary": "Classifying videos according to content semantics is an important problem\nwith a wide range of applications. In this paper, we propose a hybrid deep\nlearning framework for video classification, which is able to model static\nspatial information, short-term motion, as well as long-term temporal clues in\nthe videos. Specifically, the spatial and the short-term motion features are\nextracted separately by two Convolutional Neural Networks (CNN). These two\ntypes of CNN-based features are then combined in a regularized feature fusion\nnetwork for classification, which is able to learn and utilize feature\nrelationships for improved performance. In addition, Long Short Term Memory\n(LSTM) networks are applied on top of the two features to further model\nlonger-term temporal clues. The main contribution of this work is the hybrid\nlearning framework that can model several important aspects of the video data.\nWe also show that (1) combining the spatial and the short-term motion features\nin the regularized fusion network is better than direct classification and\nfusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is\nhighly complementary to the traditional classification strategy without\nconsidering the temporal frame orders. Extensive experiments are conducted on\ntwo popular and challenging benchmarks, the UCF-101 Human Actions and the\nColumbia Consumer Videos (CCV). On both benchmarks, our framework achieves\nto-date the best reported performance: $91.3\\%$ on the UCF-101 and $83.5\\%$ on\nthe CCV.", 
    "link": "http://arxiv.org/pdf/1504.01561v1", 
    "arxiv-id": "1504.01561v1"
},{
    "category": "cs.NI", 
    "author": "Tony Tsang", 
    "title": "Video Contents Prior Storing Server for Optical Access Network", 
    "publish": "2015-04-08T13:29:12Z", 
    "summary": "One of the most important multimedia applications is Internet protocol TV\n(IPTV) for next-generation networks. IPTV provides triple-play services that\nrequire high-speed access networks with the functions of multicasting and\nquality of service (QoS) guarantees. Among optical access networks, Ethernet\npassive optical networks (EPONs) are regarded as among the best solutions to\nmeet higher bandwidth demands. In this paper, we propose a new architecture for\nmulticasting live IPTV traffic in optical access network. The proposed\nmechanism involves assigning a unique logical link identifier to each IPTV\nchannel. To manage multicasting, a prior storing server in the optical line\nterminal (OLT) and in each optical network unit (ONU) is constructed. In this\nwork, we propose a partial prior storing strategy that considers the changes in\nthe popularity of the video content segments over time and the access patterns\nof the users to compute the utility of the objects in the prior storage. We\nalso propose to partition the prior storage to avoid the eviction of the\npopular objects (those not accessed frequently) by the unpopular ones which are\naccessed with higher frequency. The popularity distribution and ageing of\npopularity are measured from two online datasets and use the parameters in\nsimulations. Simulation results show that our proposed architecture can improve\nthe system performance and QoS parameters in terms of packet delay, jitter and\npacket loss.", 
    "link": "http://arxiv.org/pdf/1504.01957v1", 
    "arxiv-id": "1504.01957v1"
},{
    "category": "cs.MM", 
    "author": "Daniel Caragata", 
    "title": "On the Security of a Revised Fragile Watermarking Scheme", 
    "publish": "2015-04-20T20:53:29Z", 
    "summary": "This paper analyzes a revised fragile watermarking scheme proposed by Botta\net al. which was developed as a revision of the watermarking scheme previously\nproposed by Rawat et al. A new attack is presented that allows an attacker to\napply a valid watermark on tampered images, therefore circumventing the\nprotection that the watermarking scheme under study was supposed to offer.\nFurthermore, the presented attack has very low computational and memory\nrequirements.", 
    "link": "http://arxiv.org/pdf/1504.05226v1", 
    "arxiv-id": "1504.05226v1"
},{
    "category": "cs.MM", 
    "author": "Mohamed Cheriet", 
    "title": "Deviation Based Pooling Strategies For Full Reference Image Quality   Assessment", 
    "publish": "2015-04-26T05:06:33Z", 
    "summary": "The state-of-the-art pooling strategies for perceptual image quality\nassessment (IQA) are based on the mean and the weighted mean. They are robust\npooling strategies which usually provide a moderate to high performance for\ndifferent IQAs. Recently, standard deviation (SD) pooling was also proposed.\nAlthough, this deviation pooling provides a very high performance for a few\nIQAs, its performance is lower than mean poolings for many other IQAs. In this\npaper, we propose to use the mean absolute deviation (MAD) and show that it is\na more robust and accurate pooling strategy for a wider range of IQAs. In fact,\nMAD pooling has the advantages of both mean pooling and SD pooling. The joint\ncomputation and use of the MAD and SD pooling strategies is also considered in\nthis paper. Experimental results provide useful information on the choice of\nthe proper deviation pooling strategy for different IQA models.", 
    "link": "http://arxiv.org/pdf/1504.06786v2", 
    "arxiv-id": "1504.06786v2"
},{
    "category": "cs.NI", 
    "author": "John Cosmas", 
    "title": "Evaluating the Performance of Multicast Resource Allocation Policies   over LTE Systems", 
    "publish": "2015-04-26T12:39:33Z", 
    "summary": "This paper addresses a multi-criteria decision method properly designed to\neffectively evaluate the most performing strategy for multicast content\ndelivery in Long Term Evolution (LTE) and beyond systems. We compared the\nlegacy conservative-based approach with other promising strategies in\nliterature, i.e., opportunistic multicasting and subgroup-based policies\ntailored to exploit different cost functions, such as maximum throughput,\nproportional fairness and the multicast dissatisfaction index (MDI). We provide\na comparison among above schemes in terms of aggregate data rate (ADR),\nfairness and spectral efficiency. We further design a multi-criteria decision\nmaking method, namely TOPSIS, to evaluate through a single mark the overall\nperformance of considered strategies. The obtained results show that the MDI\nsubgrouping strategy represents the most suitable approach for multicast\ncontent delivery as it provides the most promising trade-off between the\nfairness and the throughput achieved by the multicast members.", 
    "link": "http://arxiv.org/pdf/1504.06812v2", 
    "arxiv-id": "1504.06812v2"
},{
    "category": "cs.NI", 
    "author": "Li Wang", 
    "title": "Efficient Spectrum Management Exploiting D2D Communication in 5G Systems", 
    "publish": "2015-04-26T12:45:40Z", 
    "summary": "In the future standardization of the 5G networks, in Long Term Evolution\n(LTE) Release 13 and beyond, Device-to-Device communications (D2D) is\nrecognized as one of the key technologies that will support the 5G\narchitecture. In fact, D2D can be exploited for different proximity-based\nservices (ProSe) where the users discover their neighbors and benefit form\ndifferent services like social applications, advertisement, public safety, and\nwarning messages. In such a scenario, the aim is to manage in a proper way the\nradio spectrum and the energy consumption to provide high Quality of Experience\n(QoE) and better Quality of Services (QoS). To reach this goal, in this paper\nwe propose a novel D2D-based uploading scheme in order to decrease the amount\nof radio resources needed to upload to the eNodeB a certain multimedia content.\nAs a further improvement, the proposed scheme enhances the energy consumption\nof the users in the network, without affects the content uploading time. The\nobtained results show that our scheme achieves a gain of about 35\\% in term of\nmean radio resources used with respect to the standard LTE cellular approach.\nIn addition, it is also 40 times more efficient in terms of energy consumption\nneeded to upload the multimedia content.", 
    "link": "http://arxiv.org/pdf/1504.06813v2", 
    "arxiv-id": "1504.06813v2"
},{
    "category": "cs.CV", 
    "author": "Alireza Avanaki", 
    "title": "Exact Histogram Specification Optimized for Structural Similarity", 
    "publish": "2008-12-31T06:13:39Z", 
    "summary": "An exact histogram specification (EHS) method modifies its input image to\nhave a specified histogram. Applications of EHS include image (contrast)\nenhancement (e.g., by histogram equalization) and histogram watermarking.\nPerforming EHS on an image, however, reduces its visual quality. Starting from\nthe output of a generic EHS method, we maximize the structural similarity index\n(SSIM) between the original image (before EHS) and the result of EHS\niteratively. Essential in this process is the computationally simple and\naccurate formula we derive for SSIM gradient. As it is based on gradient\nascent, the proposed EHS always converges. Experimental results confirm that\nwhile obtaining the histogram exactly as specified, the proposed method\ninvariably outperforms the existing methods in terms of visual quality of the\nresult. The computational complexity of the proposed method is shown to be of\nthe same order as that of the existing methods.\n  Index terms: histogram modification, histogram equalization, optimization for\nperceptual visual quality, structural similarity gradient ascent, histogram\nwatermarking, contrast enhancement.", 
    "link": "http://arxiv.org/pdf/0901.0065v1", 
    "arxiv-id": "0901.0065v1"
},{
    "category": "cs.MM", 
    "author": "Yinjing Guo", 
    "title": "Condition for Energy Efficient Watermarking with Random Vector Model   without WSS Assumption", 
    "publish": "2009-01-11T02:25:17Z", 
    "summary": "Energy efficient watermarking preserves the watermark energy after linear\nattack as much as possible. We consider in this letter non-stationary signal\nmodels and derive conditions for energy efficient watermarking under random\nvector model without WSS assumption. We find that the covariance matrix of the\nenergy efficient watermark should be proportional to host covariance matrix to\nbest resist the optimal linear removal attacks. In WSS process our result\nreduces to the well known power spectrum condition. Intuitive geometric\ninterpretation of the results are also discussed which in turn also provide\nmore simpler proof of the main results.", 
    "link": "http://arxiv.org/pdf/0901.1407v1", 
    "arxiv-id": "0901.1407v1"
},{
    "category": "cs.MM", 
    "author": "Yann Ga\u00ebstel", 
    "title": "Human Daily Activities Indexing in Videos from Wearable Cameras for   Monitoring of Patients with Dementia Diseases", 
    "publish": "2010-07-23T13:59:42Z", 
    "summary": "Our research focuses on analysing human activities according to a known\nbehaviorist scenario, in case of noisy and high dimensional collected data. The\ndata come from the monitoring of patients with dementia diseases by wearable\ncameras. We define a structural model of video recordings based on a Hidden\nMarkov Model. New spatio-temporal features, color features and localization\nfeatures are proposed as observations. First results in recognition of\nactivities are promising.", 
    "link": "http://arxiv.org/pdf/1007.4134v1", 
    "arxiv-id": "1007.4134v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Multi-Level Steganography: Improving Hidden Communication in Networks", 
    "publish": "2011-01-25T12:26:00Z", 
    "summary": "The paper presents Multi-Level Steganography (MLS), which defines a new\nconcept for hidden communication in telecommunication networks. In MLS, at\nleast two steganographic methods are utilised simultaneously, in such a way\nthat one method (called the upper-level) serves as a carrier for the second one\n(called the lower-level). Such a relationship between two (or more) information\nhiding solutions has several potential benefits. The most important is that the\nlower-level method steganographic bandwidth can be utilised to make the\nsteganogram unreadable even after the detection of the upper-level method:\ne.g., it can carry a cryptographic key that deciphers the steganogram carried\nby the upper-level one. It can also be used to provide the steganogram with\nintegrity. Another important benefit is that the lower-layer method may be used\nas a signalling channel in which to exchange information that affects the way\nthat the upper-level method functions, thus possibly making the steganographic\ncommunication harder to detect. The prototype of MLS for IP networks was also\ndeveloped, and the experimental results are included in this paper.", 
    "link": "http://arxiv.org/pdf/1101.4789v3", 
    "arxiv-id": "1101.4789v3"
},{
    "category": "cs.MM", 
    "author": "S. Immanuel Alex Pandian", 
    "title": "A Color Image Digital Watermarking Scheme Based on SOFM", 
    "publish": "2011-01-13T15:01:12Z", 
    "summary": "Digital watermarking technique has been presented and widely researched to\nsolve some important issues in the digital world, such as copyright protection,\ncopy protection and content authentication. Several robust watermarking schemes\nbased on vector quantization (VQ) have been presented. In this paper, we\npresent a new digital image watermarking method based on SOFM vector quantizer\nfor color images. This method utilizes the codebook partition technique in\nwhich the watermark bit is embedded into the selected VQ encoded block. The\nmain feature of this scheme is that the watermark exists both in VQ compressed\nimage and in the reconstructed image. The watermark extraction can be performed\nwithout the original image. The watermark is hidden inside the compressed\nimage, so much transmission time and storage space can be saved when the\ncompressed data are transmitted over the Internet. Simulation results\ndemonstrate that the proposed method has robustness against various image\nprocessing operations without sacrificing compression performance and the\ncomputational speed.", 
    "link": "http://arxiv.org/pdf/1101.5127v1", 
    "arxiv-id": "1101.5127v1"
},{
    "category": "cs.DC", 
    "author": "Haller Piroska", 
    "title": "Using Planetlab to Implement Multicast at the Application Level", 
    "publish": "2011-01-30T17:49:58Z", 
    "summary": "Application-layer multicast implements the multicast functionality at the\napplication layer. The main goal of application-layer multicast is to construct\nand maintain efficient distribution structures between endhosts. In this paper\nwe focus on the implementation of an application-layer multicast network using\nPlanetLab. We observe that the total time required to measure network latency\nover TCP is influenced dramatically by the TCP connection time. We argue that\nend-host distribution is not only influenced by the quality of network links\nbut also by the time required to make connections between nodes. We provide\nseveral solutions to decrease the total end-host distribution time.", 
    "link": "http://arxiv.org/pdf/1101.5791v1", 
    "arxiv-id": "1101.5791v1"
},{
    "category": "cs.CR", 
    "author": "S. R. Bhadra Chaudhuri", 
    "title": "A Session based Multiple Image Hiding Technique using DWT and DCT", 
    "publish": "2012-08-04T18:10:35Z", 
    "summary": "This work proposes Steganographic technique for hiding multiple images in a\ncolor image based on DWT and DCT. The cover image is decomposed into three\nseparate color planes namely R, G and B. Individual planes are decomposed into\nsubbands using DWT. DCT is applied in HH component of each plane. Secret images\nare dispersed among the selected DCT coefficients using a pseudo random\nsequence and a Session key. Secret images are extracted using the session key\nand the size of the images from the planer decomposed stego image. In this\napproach the stego image generated is of acceptable level of imperceptibility\nand distortion compared to the cover image and the overall security is high.", 
    "link": "http://arxiv.org/pdf/1208.0950v1", 
    "arxiv-id": "1208.0950v1"
},{
    "category": "cs.HC", 
    "author": "Tanvi Sethi", 
    "title": "Anthropomorphic User Interface Feedback in a Sewing Context and   Affordances", 
    "publish": "2012-08-16T09:27:49Z", 
    "summary": "The aim of the authors' research is to gain better insights into the\neffectiveness and user satisfaction of anthropomorphism at the user interface.\nTherefore, this paper presents a between users experiment and the results in\nthe context of anthropomorphism at the user interface and the giving of\ninstruction for learning sewing stitches. Two experimental conditions were\nused, where the information for learning sewing stitches was the same. However\nthe manner of presentation was varied. Therefore one condition was\nanthropomorphic and the other was non-anthropomorphic. Also the work is closely\nlinked with Hartson's theory of affordances applied to user interfaces. The\nresults suggest that facilitation of the affordances in an anthropomorphic user\ninterface lead to statistically significant results in terms of effectiveness\nand user satisfaction in the sewing context. Further some violation of the\naffordances leads to an interface being less usable in terms of effectiveness\nand user satisfaction.", 
    "link": "http://arxiv.org/pdf/1208.3323v1", 
    "arxiv-id": "1208.3323v1"
},{
    "category": "cs.CV", 
    "author": "Zhang Yi", 
    "title": "Constructing the L2-Graph for Robust Subspace Learning and Subspace   Clustering", 
    "publish": "2012-09-05T01:36:11Z", 
    "summary": "Under the framework of graph-based learning, the key to robust subspace\nclustering and subspace learning is to obtain a good similarity graph that\neliminates the effects of errors and retains only connections between the data\npoints from the same subspace (i.e., intra-subspace data points). Recent works\nachieve good performance by modeling errors into their objective functions to\nremove the errors from the inputs. However, these approaches face the\nlimitations that the structure of errors should be known prior and a complex\nconvex problem must be solved. In this paper, we present a novel method to\neliminate the effects of the errors from the projection space (representation)\nrather than from the input space. We first prove that $\\ell_1$-, $\\ell_2$-,\n$\\ell_{\\infty}$-, and nuclear-norm based linear projection spaces share the\nproperty of Intra-subspace Projection Dominance (IPD), i.e., the coefficients\nover intra-subspace data points are larger than those over inter-subspace data\npoints. Based on this property, we introduce a method to construct a sparse\nsimilarity graph, called L2-Graph. The subspace clustering and subspace\nlearning algorithms are developed upon L2-Graph. Experiments show that L2-Graph\nalgorithms outperform the state-of-the-art methods for feature extraction,\nimage clustering, and motion segmentation in terms of accuracy, robustness, and\ntime efficiency.", 
    "link": "http://arxiv.org/pdf/1209.0841v7", 
    "arxiv-id": "1209.0841v7"
},{
    "category": "cs.MM", 
    "author": "John MacCormick", 
    "title": "Video Chat with Multiple Cameras", 
    "publish": "2012-09-06T20:10:02Z", 
    "summary": "The dominant paradigm for video chat employs a single camera at each end of\nthe conversation, but some conversations can be greatly enhanced by using\nmultiple cameras at one or both ends. This paper provides the first rigorous\ninvestigation of multi-camera video chat, concentrating especially on the\nability of users to switch between views at either end of the conversation. A\nuser study of 23 individuals analyzes the advantages and disadvantages of\npermitting a user to switch between views at a remote location. Benchmark\nexperiments employing up to four webcams simultaneously demonstrate that\nmulti-camera video chat is feasible on consumer hardware. The paper also\npresents the design of MultiCam, a software package permitting multi-camera\nvideo chat. Some important trade-offs in the design of MultiCam are discussed,\nand typical usage scenarios are analyzed.", 
    "link": "http://arxiv.org/pdf/1209.1399v1", 
    "arxiv-id": "1209.1399v1"
},{
    "category": "cs.MM", 
    "author": "Yi Wang", 
    "title": "Content-based Multi-media Retrieval Technology", 
    "publish": "2012-08-15T16:00:06Z", 
    "summary": "This paper gives a summary of the content-based Image Retrieval and\nContent-based Audio Retrieval, which are two parts of the Content-based\nRetrieval. Content-based Retrieval is the retrieval based on the features of\nthe content. Generally, it is a way to extract features of the media data and\nfind other data with the similar features from the database automatically.\nContent-based Retrieval can not only work on discrete media like texts, but\nalso can be used on continuous media, such as video and audio.", 
    "link": "http://arxiv.org/pdf/1209.2070v1", 
    "arxiv-id": "1209.2070v1"
},{
    "category": "cs.MM", 
    "author": "Mika Aalto", 
    "title": "Investigating Streaming Techniques and Energy Efficiency of Mobile Video   Services", 
    "publish": "2012-09-13T11:25:19Z", 
    "summary": "We report results from a measurement study of three video streaming services,\nYouTube, Dailymotion and Vimeo on six different smartphones. We measure and\nanalyze the traffic and energy consumption when streaming different quality\nvideos over Wi-Fi and 3G. We identify five different techniques to deliver the\nvideo and show that the use of a particular technique depends on the device,\nplayer, quality, and service. The energy consumption varies dramatically\nbetween devices, services, and video qualities depending on the streaming\ntechnique used. As a consequence, we come up with suggestions on how to improve\nthe energy efficiency of mobile video streaming services.", 
    "link": "http://arxiv.org/pdf/1209.2855v1", 
    "arxiv-id": "1209.2855v1"
},{
    "category": "cs.MM", 
    "author": "Goutam Saha", 
    "title": "Comparison of Speech Activity Detection Techniques for Speaker   Recognition", 
    "publish": "2012-10-01T07:10:12Z", 
    "summary": "Speech activity detection (SAD) is an essential component for a variety of\nspeech processing applications. It has been observed that performances of\nvarious speech based tasks are very much dependent on the efficiency of the\nSAD. In this paper, we have systematically reviewed some popular SAD techniques\nand their applications in speaker recognition. Speaker verification system\nusing different SAD technique are experimentally evaluated on NIST speech\ncorpora using Gaussian mixture model- universal background model (GMM-UBM)\nbased classifier for clean and noisy conditions. It has been found that two\nGaussian modeling based SAD is comparatively better than other SAD techniques\nfor different types of noises.", 
    "link": "http://arxiv.org/pdf/1210.0297v2", 
    "arxiv-id": "1210.0297v2"
},{
    "category": "cs.SI", 
    "author": "John R Smith", 
    "title": "Tracking Large-Scale Video Remix in Real-World Events", 
    "publish": "2012-10-02T00:43:36Z", 
    "summary": "Social information networks, such as YouTube, contains traces of both\nexplicit online interaction (such as \"like\", leaving a comment, or subscribing\nto video feed), and latent interactions (such as quoting, or remixing parts of\na video). We propose visual memes, or frequently re-posted short video\nsegments, for tracking such latent video interactions at scale. Visual memes\nare extracted by scalable detection algorithms that we develop, with high\naccuracy. We further augment visual memes with text, via a statistical model of\nlatent topics. We model content interactions on YouTube with visual memes,\ndefining several measures of influence and building predictive models for meme\npopularity. Experiments are carried out on with over 2 million video shots from\nmore than 40,000 videos on two prominent news events in 2009: the election in\nIran and the swine flu epidemic. In these two events, a high percentage of\nvideos contain remixed content, and it is apparent that traditional news media\nand citizen journalists have different roles in disseminating remixed content.\nWe perform two quantitative evaluations for annotating visual memes and\npredicting their popularity. The joint statistical model of visual memes and\nwords outperform a concurrence model, and the average error is ~2% for\npredicting meme volume and ~17% for their lifespan.", 
    "link": "http://arxiv.org/pdf/1210.0623v3", 
    "arxiv-id": "1210.0623v3"
},{
    "category": "cs.GR", 
    "author": "Sukhpal Singh", 
    "title": "Reduction of Blocking Artifacts In JPEG Compressed Image", 
    "publish": "2012-10-03T18:53:36Z", 
    "summary": "In JPEG (DCT based) compresses image data by representing the original image\nwith a small number of transform coefficients. It exploits the fact that for\ntypical images a large amount of signal energy is concentrated in a small\nnumber of coefficients. The goal of DCT transform coding is to minimize the\nnumber of retained transform coefficients while keeping distortion at an\nacceptable level.In JPEG, it is done in 8X8 non overlapping blocks. It divides\nan image into blocks of equal size and processes each block independently.\nBlock processing allows the coder to adapt to the local image statistics,\nexploit the correlation present among neighboring image pixels, and to reduce\ncomputational and storage requirements. One of the most degradation of the\nblock transform coding is the blocking artifact. These artifacts appear as a\nregular pattern of visible block boundaries. This degradation is a direct\nresult of the coarse quantization of the coefficients and the independent\nprocessing of the blocks which does not take into account the existing\ncorrelations among adjacent block pixels. In this paper attempt is being made\nto reduce the blocking artifact introduced by the Block DCT Transform in JPEG.", 
    "link": "http://arxiv.org/pdf/1210.1192v3", 
    "arxiv-id": "1210.1192v3"
},{
    "category": "cs.CV", 
    "author": "Shuicheng Yan", 
    "title": "Video De-fencing", 
    "publish": "2012-10-08T19:58:59Z", 
    "summary": "This paper describes and provides an initial solution to a novel video\nediting task, i.e., video de-fencing. It targets automatic restoration of the\nvideo clips that are corrupted by fence-like occlusions during capture. Our key\nobservation lies in the visual parallax between fences and background scenes,\nwhich is caused by the fact that the former are typically closer to the camera.\nUnlike in traditional image inpainting, fence-occluded pixels in the videos\ntend to appear later in the temporal dimension and are therefore recoverable\nvia optimized pixel selection from relevant frames. To eventually produce\nfence-free videos, major challenges include cross-frame sub-pixel image\nalignment under diverse scene depth, and \"correct\" pixel selection that is\nrobust to dominating fence pixels. Several novel tools are developed in this\npaper, including soft fence detection, weighted truncated optical flow method\nand robust temporal median filter. The proposed algorithm is validated on\nseveral real-world video clips with fences.", 
    "link": "http://arxiv.org/pdf/1210.2388v1", 
    "arxiv-id": "1210.2388v1"
},{
    "category": "cs.MM", 
    "author": "Pascal Frossard", 
    "title": "Navigation domain representation for interactive multiview imaging", 
    "publish": "2012-10-18T07:41:17Z", 
    "summary": "Enabling users to interactively navigate through different viewpoints of a\nstatic scene is a new interesting functionality in 3D streaming systems. While\nit opens exciting perspectives towards rich multimedia applications, it\nrequires the design of novel representations and coding techniques in order to\nsolve the new challenges imposed by interactive navigation. Interactivity\nclearly brings new design constraints: the encoder is unaware of the exact\ndecoding process, while the decoder has to reconstruct information from\nincomplete subsets of data since the server can generally not transmit images\nfor all possible viewpoints due to resource constrains. In this paper, we\npropose a novel multiview data representation that permits to satisfy bandwidth\nand storage constraints in an interactive multiview streaming system. In\nparticular, we partition the multiview navigation domain into segments, each of\nwhich is described by a reference image and some auxiliary information. The\nauxiliary information enables the client to recreate any viewpoint in the\nnavigation segment via view synthesis. The decoder is then able to navigate\nfreely in the segment without further data request to the server; it requests\nadditional data only when it moves to a different segment. We discuss the\nbenefits of this novel representation in interactive navigation systems and\nfurther propose a method to optimize the partitioning of the navigation domain\ninto independent segments, under bandwidth and storage constraints.\nExperimental results confirm the potential of the proposed representation;\nnamely, our system leads to similar compression performance as classical\ninter-view coding, while it provides the high level of flexibility that is\nrequired for interactive streaming. Hence, our new framework represents a\npromising solution for 3D data representation in novel interactive multimedia\nservices.", 
    "link": "http://arxiv.org/pdf/1210.5041v2", 
    "arxiv-id": "1210.5041v2"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "Steganalysis of Transcoding Steganography", 
    "publish": "2012-10-22T13:18:19Z", 
    "summary": "TranSteg (Trancoding Steganography) is a fairly new IP telephony\nsteganographic method that functions by compressing overt (voice) data to make\nspace for the steganogram by means of transcoding. It offers high\nsteganographic bandwidth, retains good voice quality and is generally harder to\ndetect than other existing VoIP steganographic methods. In TranSteg, after the\nsteganogram reaches the receiver, the hidden information is extracted and the\nspeech data is practically restored to what was originally sent. This is a huge\nadvantage compared with other existing VoIP steganographic methods, where the\nhidden data can be extracted and removed but the original data cannot be\nrestored because it was previously erased due to a hidden data insertion\nprocess. In this paper we address the issue of steganalysis of TranSteg.\nVarious TranSteg scenarios and possibilities of warden(s) localization are\nanalyzed with regards to the TranSteg detection. A steganalysis method based on\nMFCC (Mel-Frequency Cepstral Coefficients) parameters and GMMs (Gaussian\nMixture Models) was developed and tested for various overt/covert codec pairs\nin a single warden scenario with double transcoding. The proposed method\nallowed for efficient detection of some codec pairs (e.g., G.711/G.729), whilst\nsome others remained more resistant to detection (e.g., iLBC/AMR).", 
    "link": "http://arxiv.org/pdf/1210.5888v1", 
    "arxiv-id": "1210.5888v1"
},{
    "category": "cs.CR", 
    "author": "G. Pitchammal", 
    "title": "Multilayer image watermarking scheme for providing high security", 
    "publish": "2012-10-22T15:55:03Z", 
    "summary": "The main theme of this application is to provide an algorithm color image\nwatermark to manage the attacks such as rotation, scaling and translation. In\nthe existing watermarking algorithms, those exploited robust features are more\nor less related to the pixel position, so they cannot be more robust against\nthe attacks. In order to solve this problem this application focus on certain\nparameters rather than the pixel position for watermarking. Two statistical\nfeatures such as the histogram shape and the mean of Gaussian filtered\nlow-frequency component of images are taken for this proposed application to\nmake the watermarking algorithm robust to attacks and also AES technique is\nused to provide higher security.", 
    "link": "http://arxiv.org/pdf/1210.5941v1", 
    "arxiv-id": "1210.5941v1"
},{
    "category": "cs.CV", 
    "author": "Lalitha Rangarajan", 
    "title": "Mugshot Identification from Manipulated Facial Images", 
    "publish": "2012-10-31T12:55:57Z", 
    "summary": "Editing on digital images is ubiquitous. Identification of deliberately\nmodified facial images is a new challenge for face identification system. In\nthis paper, we address the problem of identification of a face or person from\nheavily altered facial images. In this face identification problem, the input\nto the system is a manipulated or transformed face image and the system reports\nback the determined identity from a database of known individuals. Such a\nsystem can be useful in mugshot identification in which mugshot database\ncontains two views (frontal and profile) of each criminal. We considered only\nfrontal view from the available database for face identification and the query\nimage is a manipulated face generated by face transformation software tool\navailable online. We propose SIFT features for efficient face identification in\nthis scenario. Further comparative analysis has been given with well known\neigenface approach. Experiments have been conducted with real case images to\nevaluate the performance of both methods.", 
    "link": "http://arxiv.org/pdf/1210.8318v1", 
    "arxiv-id": "1210.8318v1"
},{
    "category": "cs.MM", 
    "author": "S. Varadarajan", 
    "title": "The Robust Digital Image Watermarking using Quantization and Fuzzy Logic   Approach in DWT Domain", 
    "publish": "2013-02-18T11:41:09Z", 
    "summary": "In this paper a novel approach to embed watermark into the host image using\nquantization with the help of Dynamic Fuzzy Inference System (DFIS) is\nproposed. The cover image is decomposed up to 3- levels using quantization and\nDiscrete Wavelet Transform (DWT). A bitmap of size 64x64 pixels is embedded\ninto the host image using DFIS rule base. The DFIS is utilized to generate the\nwatermark weighting function to embed the imperceptible watermark. The\nimplemented watermarking algorithm is imperceptible and robust to some normal\nattacks such as JPEG Compression, salt&pepper noise, median filtering, rotation\nand cropping.\n  Keywords: Watermark, Quantization, Dynamic Fuzzy Inference System,\nImperceptible, Robust, JPEG Compression, Cropping.", 
    "link": "http://arxiv.org/pdf/1302.4233v1", 
    "arxiv-id": "1302.4233v1"
},{
    "category": "cs.CR", 
    "author": "Antonio Ortega", 
    "title": "P3: Toward Privacy-Preserving Photo Sharing", 
    "publish": "2013-02-20T18:17:05Z", 
    "summary": "With increasing use of mobile devices, photo sharing services are\nexperiencing greater popularity. Aside from providing storage, photo sharing\nservices enable bandwidth-efficient downloads to mobile devices by performing\nserver-side image transformations (resizing, cropping). On the flip side, photo\nsharing services have raised privacy concerns such as leakage of photos to\nunauthorized viewers and the use of algorithmic recognition technologies by\nproviders. To address these concerns, we propose a privacy-preserving photo\nencoding algorithm that extracts and encrypts a small, but significant,\ncomponent of the photo, while preserving the remainder in a public,\nstandards-compatible, part. These two components can be separately stored. This\ntechnique significantly reduces the signal-to-noise ratio and the accuracy of\nautomated detection and recognition on the public part, while preserving the\nability of the provider to perform server-side transformations to conserve\ndownload bandwidth usage. Our prototype privacy-preserving photo sharing\nsystem, P3, works with Facebook, and can be extended to other services as well.\nP3 requires no changes to existing services or mobile application software, and\nadds minimal photo storage overhead.", 
    "link": "http://arxiv.org/pdf/1302.5062v1", 
    "arxiv-id": "1302.5062v1"
},{
    "category": "cs.CY", 
    "author": "Mohareb A Alsmadi", 
    "title": "The impact of teaching two courses (electronic curriculum design,   multimedia) on the acquisition of electronic content design skills", 
    "publish": "2013-02-22T19:13:13Z", 
    "summary": "The use of Multimedia applications in Learning provides useful concepts for\nInstructional Content Design. This study aimed to investigate the effect of\ndesign electronic curriculum and multimedia applications on acquiring e-content\ndesign skills, and improving their attitudes towards e-learning. To achieve the\nobjective of the study, the researchers developed a test to measure the\nefficiencies of designing electronic content and the measure of attitudes\ntowards e-learning, The results showed that study of both courses contributed\npositively to the acquisition of design skills of e-content, The results\nrevealed that there are statistical significant differences between the scores\nof the students in the two applications (pre and post) on the total score of\nthe attitude measure and three areas of it.", 
    "link": "http://arxiv.org/pdf/1302.5677v1", 
    "arxiv-id": "1302.5677v1"
},{
    "category": "cs.MM", 
    "author": "Enrico Magli", 
    "title": "Band Codes for Energy-Efficient Network Coding with Application to P2P   Mobile Streaming", 
    "publish": "2013-09-02T07:58:45Z", 
    "summary": "A key problem in random network coding (NC) lies in the complexity and energy\nconsumption associated with the packet decoding processes, which hinder its\napplication in mobile environments. Controlling and hence limiting such factors\nhas always been an important but elusive research goal, since the packet degree\ndistribution, which is the main factor driving the complexity, is altered in a\nnon-deterministic way by the random recombinations at the network nodes. In\nthis paper we tackle this problem proposing Band Codes (BC), a novel class of\nnetwork codes specifically designed to preserve the packet degree distribution\nduring packet encoding, ecombination and decoding. BC are random codes over\nGF(2) that exhibit low decoding complexity, feature limited and controlled\ndegree distribution by construction, and hence allow to effectively apply NC\neven in energy-constrained scenarios. In particular, in this paper we motivate\nand describe our new design and provide a thorough analysis of its performance.\nWe provide numerical simulations of the performance of BC in order to validate\nthe analysis and assess the overhead of BC with respect to a onventional NC\nscheme. Moreover, peer-to-peer media streaming experiments with a random-push\nprotocol show that BC reduce the decoding complexity by a factor of two, to a\npoint where NC-based mobile streaming to mobile devices becomes practically\nfeasible.", 
    "link": "http://arxiv.org/pdf/1309.0316v1", 
    "arxiv-id": "1309.0316v1"
},{
    "category": "cs.MM", 
    "author": "T. Kishore Kumar", 
    "title": "Speech Enhancement using Kernel and Normalized Kernel Affine Projection   Algorithm", 
    "publish": "2013-09-10T01:57:14Z", 
    "summary": "The goal of this paper is to investigate the speech signal enhancement using\nKernel Affine Projection Algorithm (KAPA) and Normalized KAPA. The removal of\nbackground noise is very important in many applications like speech\nrecognition, telephone conversations, hearing aids, forensic, etc. Kernel\nadaptive filters shown good performance for removal of noise. If the evaluation\nof background noise is more slowly than the speech, i.e., noise signal is more\nstationary than the speech, we can easily estimate the noise during the pauses\nin speech. Otherwise it is more difficult to estimate the noise which results\nin degradation of speech. In order to improve the quality and intelligibility\nof speech, unlike time and frequency domains, we can process the signal in new\ndomain like Reproducing Kernel Hilbert Space (RKHS) for high dimensional to\nyield more powerful nonlinear extensions. For experiments, we have used the\ndatabase of noisy speech corpus (NOIZEUS). From the results, we observed the\nremoval noise in RKHS has great performance in signal to noise ratio values in\ncomparison with conventional adaptive filters.", 
    "link": "http://arxiv.org/pdf/1309.2359v1", 
    "arxiv-id": "1309.2359v1"
},{
    "category": "cs.MM", 
    "author": "K. Anusudha", 
    "title": "Robust watermarking based on DWT SVD", 
    "publish": "2013-09-10T09:18:27Z", 
    "summary": "Digital information revolution has brought about many advantages and new\nissues. The protection of ownership and the prevention of unauthorized\nmanipulation of digital audio, image, and video materials has become an\nimportant concern due to the ease of editing and perfect reproduction.\nWatermarking is identified as a major means to achieve copyright protection. It\nis a branch of information hiding which is used to hide proprietary information\nin digital media like photographs, digital music, digital video etc. In this\npaper, a new image watermarking algorithm that is robust against various\nattacks is presented. DWT (Discrete Wavelet Transform) and SVD (Singular Value\nDecomposition) have been used to embed two watermarks in the HL and LH bands of\nthe host image. Simulation evaluation demonstrates that the proposed technique\nwithstand various attacks.", 
    "link": "http://arxiv.org/pdf/1309.2423v7", 
    "arxiv-id": "1309.2423v7"
},{
    "category": "cs.AR", 
    "author": "Djamel Benazzouz", 
    "title": "Evaluation of the Performance/Energy Overhead in DSP Video Decoding and   its Implications", 
    "publish": "2013-09-10T14:50:29Z", 
    "summary": "Video decoding is considered as one of the most compute and energy intensive\napplication in energy constrained mobile devices. Some specific processing\nunits, such as DSPs, are added to those devices in order to optimize the\nperformance and the energy consumption. However, in DSP video decoding, the\ninter-processor communication overhead may have a considerable impact on the\nperformance and the energy consumption. In this paper, we propose to evaluate\nthis overhead and analyse its impact on the performance and the energy\nconsumption as compared to the GPP decoding. Our work revealed that the GPP can\nbe the best choice in many cases due to the a significant overhead in DSP\ndecoding which may represents 30% of the total decoding energy.", 
    "link": "http://arxiv.org/pdf/1309.2533v1", 
    "arxiv-id": "1309.2533v1"
},{
    "category": "cs.NI", 
    "author": "Holger Karl", 
    "title": "Anticipatory Buffer Control and Quality Selection for Wireless Video   Streaming", 
    "publish": "2013-09-21T15:40:29Z", 
    "summary": "Video streaming is in high demand by mobile users, as recent studies\nindicate. In cellular networks, however, the unreliable wireless channel leads\nto two major problems. Poor channel states degrade video quality and interrupt\nthe playback when a user cannot sufficiently fill its local playout buffer:\nbuffer underruns occur. In contrast to that, good channel conditions cause\ncommon greedy buffering schemes to pile up very long buffers. Such\nover-buffering wastes expensive wireless channel capacity.\n  To keep buffering in balance, we employ a novel approach. Assuming that we\ncan predict data rates, we plan the quality and download time of the video\nsegments ahead. This anticipatory scheduling avoids buffer underruns by\ndownloading a large number of segments before a channel outage occurs, without\nwasting wireless capacity by excessive buffering. We formalize this approach as\nan optimization problem and derive practical heuristics for segmented video\nstreaming protocols (e.g., HLS or MPEG DASH). Simulation results and testbed\nmeasurements show that our solution essentially eliminates playback\ninterruptions without significantly decreasing video quality.", 
    "link": "http://arxiv.org/pdf/1309.5491v2", 
    "arxiv-id": "1309.5491v2"
},{
    "category": "cs.MM", 
    "author": "Svein J. Knapskog", 
    "title": "An Efficient Authorship Protection Scheme for Shared Multimedia Content", 
    "publish": "2013-09-29T19:17:18Z", 
    "summary": "Many electronic content providers today like Flickr and Google, offer space\nto users to publish their electronic media (e.g. photos and videos) in their\ncloud infrastructures, so that they can be publicly accessed. Features like\nincluding other information, such as keywords or owner information into the\ndigital material is already offered by existing providers. Despite the useful\nfeatures made available to users by such infrastructures, the authorship of the\npublished content is not protected against various attacks such as compression.\nIn this paper we propose a robust scheme that uses digital invisible\nwatermarking and hashing to protect the authorship of the digital content and\nprovide resistance against malicious manipulation of multimedia content. The\nscheme is enhanced by an algorithm called MMBEC, that is an extension of an\nestablished scheme MBEC, towards higher resistance.", 
    "link": "http://arxiv.org/pdf/1309.7640v1", 
    "arxiv-id": "1309.7640v1"
},{
    "category": "cs.MM", 
    "author": "Julio Hernandez-Castro", 
    "title": "Steganography using the Extensible Messaging and Presence Protocol   (XMPP)", 
    "publish": "2013-09-27T12:00:29Z", 
    "summary": "We present here the first work to propose different mechanisms for hiding\ndata in the Extensible Messaging and Presence Protocol (XMPP). This is a very\npopular instant messaging protocol used by many messaging platforms such as\nGoogle Talk, Cisco, LiveJournal and many others. Our paper describes how to\nsend a secret message from one XMPP client to another, without raising the\nsuspicion of any intermediaries. The methods described primarily focus on using\nthe underlying protocol as a means for steganography, unlike other related\nworks that try to hide data in the content of instant messages. In doing so, we\nprovide a more robust means of data hiding and additionally offer some\npreliminary analysis of its general security, in particular against\nentropic-based steganalysis.", 
    "link": "http://arxiv.org/pdf/1310.0524v1", 
    "arxiv-id": "1310.0524v1"
},{
    "category": "cs.NI", 
    "author": "Lars Eggert", 
    "title": "Congestion Control using FEC for Conversational Multimedia Communication", 
    "publish": "2013-10-06T13:04:16Z", 
    "summary": "In this paper, we propose a new rate control algorithm for conversational\nmultimedia flows. In our approach, along with Real-time Transport Protocol\n(RTP) media packets, we propose sending redundant packets to probe for\navailable bandwidth. These redundant packets are Forward Error Correction (FEC)\nencoded RTP packets. A straightforward interpretation is that if no losses\noccur, the sender can increase the sending rate to include the FEC bit rate,\nand in the case of losses due to congestion the redundant packets help in\nrecovering the lost packets. We also show that by varying the FEC bit rate, the\nsender is able to conservatively or aggressively probe for available bandwidth.\nWe evaluate our FEC-based Rate Adaptation (FBRA) algorithm in a network\nsimulator and in the real-world and compare it to other congestion control\nalgorithms.", 
    "link": "http://arxiv.org/pdf/1310.1582v1", 
    "arxiv-id": "1310.1582v1"
},{
    "category": "cs.CV", 
    "author": "Yong You", 
    "title": "Early Fire Detection Using HEP and Space-time Analysis", 
    "publish": "2013-10-07T16:41:23Z", 
    "summary": "In this article, a video base early fire alarm system is developed by\nmonitoring the smoke in the scene. There are two major contributions in this\nwork. First, to find the best texture feature for smoke detection, a general\nframework, named Histograms of Equivalent Patterns (HEP), is adopted to achieve\nan extensive evaluation of various kinds of texture features. Second, the\n\\emph{Block based Inter-Frame Difference} (BIFD) and a improved version of\nLBP-TOP are proposed and ensembled to describe the space-time characteristics\nof the smoke. In order to reduce the false alarms, the Smoke History Image\n(SHI) is utilized to register the recent classification results of candidate\nsmoke blocks. Experimental results using SVM show that the proposed method can\nachieve better accuracy and less false alarm compared with the state-of-the-art\ntechnologies.", 
    "link": "http://arxiv.org/pdf/1310.1855v1", 
    "arxiv-id": "1310.1855v1"
},{
    "category": "cs.NI", 
    "author": "Dimitris Dimopoulos", 
    "title": "Improving Mobile Video Streaming with Mobility Prediction and   Prefetching in Integrated Cellular-WiFi Networks", 
    "publish": "2013-10-23T10:25:56Z", 
    "summary": "We present and evaluate a procedure that utilizes mobility and throughput\nprediction to prefetch video streaming data in integrated cellular and WiFi\nnetworks. The effective integration of such heterogeneous wireless technologies\nwill be significant for supporting high performance and energy efficient video\nstreaming in ubiquitous networking environments. Our evaluation is based on\ntrace-driven simulation considering empirical measurements and shows how\nvarious system parameters influence the performance, in terms of the number of\npaused video frames and the energy consumption; these parameters include the\nnumber of video streams, the mobile, WiFi, and ADSL backhaul throughput, and\nthe number of WiFi hotspots. Also, we assess the procedure's robustness to time\nand throughput variability. Finally, we present our initial prototype that\nimplements the proposed approach.", 
    "link": "http://arxiv.org/pdf/1310.6171v1", 
    "arxiv-id": "1310.6171v1"
},{
    "category": "cs.MM", 
    "author": "Dah Ming Chiu", 
    "title": "Fake View Analytics in Online Video Services", 
    "publish": "2013-12-18T06:41:12Z", 
    "summary": "Online video-on-demand(VoD) services invariably maintain a view count for\neach video they serve, and it has become an important currency for various\nstakeholders, from viewers, to content owners, advertizers, and the online\nservice providers themselves. There is often significant financial incentive to\nuse a robot (or a botnet) to artificially create fake views. How can we detect\nthe fake views? Can we detect them (and stop them) using online algorithms as\nthey occur? What is the extent of fake views with current VoD service\nproviders? These are the questions we study in the paper. We develop some\nalgorithms and show that they are quite effective for this problem.", 
    "link": "http://arxiv.org/pdf/1312.5050v1", 
    "arxiv-id": "1312.5050v1"
},{
    "category": "cs.HC", 
    "author": "Lassi A Liikkanen", 
    "title": "Three Metrics for Measuring User Engagement with Online Media and a   YouTube Case Study", 
    "publish": "2013-12-19T13:45:11Z", 
    "summary": "This technical report discusses three metrics of user engagement with online\nmedia. They are Commenting frequency, Voting frequency, and Voting balance.\nThese relative figures can be derived from established, basic statistics\navailable for many services, prominently YouTube. The paper includes case a\nstudy of popular YouTube videos to illustrate the characteristics and\nusefulness of the measures. The study documents the range of observed values\nand their relationships. The empirical sample shows the three measures to be\nonly moderately correlated with the original statistics despite the common\nnumerators and denominators. The paper concludes by discussing future\napplications and the needs of the quantification of user interaction with new\nmedia services.", 
    "link": "http://arxiv.org/pdf/1312.5547v2", 
    "arxiv-id": "1312.5547v2"
},{
    "category": "cs.LG", 
    "author": "K. Lu", 
    "title": "Manifold regularized kernel logistic regression for web image annotation", 
    "publish": "2013-12-21T00:32:24Z", 
    "summary": "With the rapid advance of Internet technology and smart devices, users often\nneed to manage large amounts of multimedia information using smart devices,\nsuch as personal image and video accessing and browsing. These requirements\nheavily rely on the success of image (video) annotation, and thus large scale\nimage annotation through innovative machine learning methods has attracted\nintensive attention in recent years. One representative work is support vector\nmachine (SVM). Although it works well in binary classification, SVM has a\nnon-smooth loss function and can not naturally cover multi-class case. In this\npaper, we propose manifold regularized kernel logistic regression (KLR) for web\nimage annotation. Compared to SVM, KLR has the following advantages: (1) the\nKLR has a smooth loss function; (2) the KLR produces an explicit estimate of\nthe probability instead of class label; and (3) the KLR can naturally be\ngeneralized to the multi-class case. We carefully conduct experiments on MIR\nFLICKR dataset and demonstrate the effectiveness of manifold regularized kernel\nlogistic regression for image annotation.", 
    "link": "http://arxiv.org/pdf/1312.6180v1", 
    "arxiv-id": "1312.6180v1"
},{
    "category": "cs.IR", 
    "author": "Xiangjie Kong", 
    "title": "Mobile Multimedia Recommendation in Smart Communities: A Survey", 
    "publish": "2013-12-23T15:01:35Z", 
    "summary": "Due to the rapid growth of internet broadband access and proliferation of\nmodern mobile devices, various types of multimedia (e.g. text, images, audios\nand videos) have become ubiquitously available anytime. Mobile device users\nusually store and use multimedia contents based on their personal interests and\npreferences. Mobile device challenges such as storage limitation have however\nintroduced the problem of mobile multimedia overload to users. In order to\ntackle this problem, researchers have developed various techniques that\nrecommend multimedia for mobile users. In this survey paper, we examine the\nimportance of mobile multimedia recommendation systems from the perspective of\nthree smart communities, namely, mobile social learning, mobile event guide and\ncontext-aware services. A cautious analysis of existing research reveals that\nthe implementation of proactive, sensor-based and hybrid recommender systems\ncan improve mobile multimedia recommendations. Nevertheless, there are still\nchallenges and open issues such as the incorporation of context and social\nproperties, which need to be tackled in order to generate accurate and\ntrustworthy mobile multimedia recommendations.", 
    "link": "http://arxiv.org/pdf/1312.6565v1", 
    "arxiv-id": "1312.6565v1"
},{
    "category": "cs.SI", 
    "author": "Philip S. Yu", 
    "title": "Deriving Latent Social Impulses to Determine Longevous Videos", 
    "publish": "2013-12-26T00:24:30Z", 
    "summary": "Online video websites receive huge amount of videos daily from users all\naround the world. How to provide valuable recommendations to viewers is an\nimportant task for both video websites and related third parties, such as\nsearch engines. Previous work conducted numerous analysis on the view counts of\nvideos, which measure a video's value in terms of popularity. However, the\nlong-lasting value of an online video, namely longevity, is hidden behind the\nhistory that a video accumulates its \"popularity\" through time. Generally\nspeaking, a longevous video tends to constantly draw society's attention. With\nfocus on one of the leading video websites, Youtube, this paper proposes a\nscoring mechanism quantifying a video's longevity. Evaluating a video's\nlongevity can not only improve a video recommender system, but also help us to\ndiscover videos having greater advertising value, as well as adjust a video\nwebsite's strategy of storing videos to shorten its responding time. In order\nto accurately quantify longevity, we introduce the concept of latent social\nimpulses and how to use them measure a video's longevity. In order to derive\nlatent social impulses, we view the video website as a digital signal filter\nand formulate the task as a convex minimization problem. The proposed longevity\ncomputation is based on the derived social impulses. Unfortunately, the\nrequired information to derive social impulses are not always public, which\nmakes a third party unable to directly evaluate every video's longevity. To\nsolve this problem, we formulate a semi-supervised learning task by using part\nof videos having known longevity scores to predict the unknown longevity\nscores. We propose a Gaussian Random Markov model with Loopy Belief Propagation\nto solve this problem. The conducted experiments on Youtube demonstrate that\nthe proposed method significantly improves the prediction results comparing to\nbaselines.", 
    "link": "http://arxiv.org/pdf/1312.7036v1", 
    "arxiv-id": "1312.7036v1"
},{
    "category": "cs.MM", 
    "author": "Ravindra Thool", 
    "title": "Evaluating the Performance of IPTV over Fixed WiMAX", 
    "publish": "2013-12-28T15:19:09Z", 
    "summary": "IEEE specifies different modulation techniques for WiMAX; namely, BPSK, QPSK,\n16 QAM and 64 QAM. This paper studies the performance of Internet Protocol\nTelevision (IPTV) over Fixed WiMAX system considering different combinations of\ndigital modulation. The performance is studied taking into account a number of\nkey system parameters which include the variation in the video coding,\npath-loss, scheduling service classes different rated codes in FEC channel\ncoding. The performance study was conducted using OPNET simulation. The\nperformance is studied in terms of packet lost, packet jitter delay, end-to-end\ndelay, and network throughput. Simulation results show that higher order\nmodulation and coding schemes (namely, 16 QAM and 64 QAM) yield better\nperformance than that of QPSK.", 
    "link": "http://arxiv.org/pdf/1312.7442v1", 
    "arxiv-id": "1312.7442v1"
},{
    "category": "cs.NI", 
    "author": "Mihaela van der Schaar", 
    "title": "Non-stationary Resource Allocation Policies for Delay-constrained Video   Streaming: Application to Video over Internet-of-Things-enabled Networks", 
    "publish": "2014-01-05T00:49:22Z", 
    "summary": "Due to the high bandwidth requirements and stringent delay constraints of\nmulti-user wireless video transmission applications, ensuring that all video\nsenders have sufficient transmission opportunities to use before their delay\ndeadlines expire is a longstanding research problem. We propose a novel\nsolution that addresses this problem without assuming detailed packet-level\nknowledge, which is unavailable at resource allocation time. Instead, we\ntranslate the transmission delay deadlines of each sender's video packets into\na monotonically-decreasing weight distribution within the considered time\nhorizon. Higher weights are assigned to the slots that have higher probability\nfor deadline-abiding delivery. Given the sets of weights of the senders' video\nstreams, we propose the low-complexity Delay-Aware Resource Allocation (DARA)\napproach to compute the optimal slot allocation policy that maximizes the\ndeadline-abiding delivery of all senders. A unique characteristic of the DARA\napproach is that it yields a non-stationary slot allocation policy that depends\non the allocation of previous slots. We prove that the DARA approach is optimal\nfor weight distributions that are exponentially decreasing in time. We further\nimplement our framework for real-time video streaming in wireless personal area\nnetworks that are gaining significant traction within the new\nInternet-of-Things (IoT) paradigm. For multiple surveillance videos encoded\nwith H.264/AVC and streamed via the 6tisch framework that simulates the\nIoT-oriented IEEE 802.15.4e TSCH medium access control, our solution is shown\nto be the only one that ensures all video bitstreams are delivered with\nacceptable quality in a deadline-abiding manner.", 
    "link": "http://arxiv.org/pdf/1401.0855v1", 
    "arxiv-id": "1401.0855v1"
},{
    "category": "cs.MM", 
    "author": "Kre\u0161imir \u0106osi\u0107", 
    "title": "STIMONT: A core ontology for multimedia stimuli description", 
    "publish": "2014-01-10T23:36:51Z", 
    "summary": "Affective multimedia documents such as images, sounds or videos elicit\nemotional responses in exposed human subjects. These stimuli are stored in\naffective multimedia databases and successfully used for a wide variety of\nresearch in psychology and neuroscience in areas related to attention and\nemotion processing. Although important all affective multimedia databases have\nnumerous deficiencies which impair their applicability. These problems, which\nare brought forward in the paper, result in low recall and precision of\nmultimedia stimuli retrieval which makes creating emotion elicitation\nprocedures difficult and labor-intensive. To address these issues a new core\nontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and\nextends W3C EmotionML format with an expressive and formal representation of\naffective concepts, high-level semantics, stimuli document metadata and the\nelicited physiology. The advantages of ontology in description of affective\nmultimedia stimuli are demonstrated in a document retrieval experiment and\ncompared against contemporary keyword-based querying methods. Also, a software\ntool Intelligent Stimulus Generator for retrieval of affective multimedia and\nconstruction of stimuli sequences is presented.", 
    "link": "http://arxiv.org/pdf/1401.2482v1", 
    "arxiv-id": "1401.2482v1"
},{
    "category": "cs.NI", 
    "author": "David Oran", 
    "title": "Streaming Video over HTTP with Consistent Quality", 
    "publish": "2014-01-21T04:35:08Z", 
    "summary": "In conventional HTTP-based adaptive streaming (HAS), a video source is\nencoded at multiple levels of constant bitrate representations, and a client\nmakes its representation selections according to the measured network\nbandwidth. While greatly simplifying adaptation to the varying network\nconditions, this strategy is not the best for optimizing the video quality\nexperienced by end users. Quality fluctuation can be reduced if the natural\nvariability of video content is taken into consideration. In this work, we\nstudy the design of a client rate adaptation algorithm to yield consistent\nvideo quality. We assume that clients have visibility into incoming video\nwithin a finite horizon. We also take advantage of the client-side video\nbuffer, by using it as a breathing room for not only network bandwidth\nvariability, but also video bitrate variability. The challenge, however, lies\nin how to balance these two variabilities to yield consistent video quality\nwithout risking a buffer underrun. We propose an optimization solution that\nuses an online algorithm to adapt the video bitrate step-by-step, while\napplying dynamic programming at each step. We incorporate our solution into\nPANDA -- a practical rate adaptation algorithm designed for HAS deployment at\nscale.", 
    "link": "http://arxiv.org/pdf/1401.5174v1", 
    "arxiv-id": "1401.5174v1"
},{
    "category": "cs.MM", 
    "author": "Farokh Marvasti", 
    "title": "Image Block Loss Restoration Using Sparsity Pattern as Side Information", 
    "publish": "2014-01-23T13:23:27Z", 
    "summary": "In this paper, we propose a method for image block loss restoration based on\nthe notion of sparse representation. We use the sparsity pattern as side\ninformation to efficiently restore block losses by iteratively imposing the\nconstraints of spatial and transform domains on the corrupted image. Two novel\nfeatures, including a pre-interpolation and a criterion for stopping the\niterations, are proposed to improve the performance. Also, to deal with\npractical applications, we develop a technique to transmit the side information\nalong with the image. In this technique, we first compress the side information\nand then embed its LDPC coded version in the least significant bits of the\nimage pixels. This technique ensures the error-free transmission of the side\ninformation, while causing only a small perturbation on the transmitted image.\nMathematical analysis and extensive simulations are performed to validate the\nmethod and investigate the efficiency of the proposed techniques. The results\nverify that the proposed method outperforms its counterparts for image block\nloss restoration.", 
    "link": "http://arxiv.org/pdf/1401.5966v2", 
    "arxiv-id": "1401.5966v2"
},{
    "category": "cs.NI", 
    "author": "Toufik Ahmed", 
    "title": "Differenciated Bandwidth Allocation in P2P Layered Streaming", 
    "publish": "2013-10-21T14:21:25Z", 
    "summary": "There is an increasing demand for P2P streaming in particular for layered\nvideo. In this category of applications, the stream is composed of\nhierarchically encoded sub-streams layers namely the base layer and\nenhancements layers. We consider a scenario where the receiver peer uses the\npull-based approach to adjust the video quality level to their capability by\nsubscribing to different number of layers. We note that higher layers received\nwithout their corresponding lower layers are considered as useless and cannot\nbe played, consequently the throughput of the system will drastically degrade.\nTo avoid this situation, we propose an economical model based on auction\nmechanisms to optimize the allocation of sender peers' upload bandwidth. The\nupstream peers organize auctions to \"sell\" theirs items (links' bandwidth)\naccording to bids submitted by the downstream peers taking into consideration\nthe peers priorities and the requested layers importance. The ultimate goal is\nto satisfy the quality level requirement for each peer, while reducing the\noverall streaming cost. Through theoretical study and performance evaluation we\nshow the effectiveness of our model in terms of users and network's utility.", 
    "link": "http://arxiv.org/pdf/1401.6132v1", 
    "arxiv-id": "1401.6132v1"
},{
    "category": "cs.MM", 
    "author": "K. Satya Prasad", 
    "title": "Robust Video Watermarking Schemes in Phase domain Using Binary Phase   Shift Keying", 
    "publish": "2014-03-21T06:10:27Z", 
    "summary": "This paper presents a robust video watermarking scheme in Discrete Fourier\nTransform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT\nand SCHT coefficients are complex and consist of both magnitude and phase and\nare well suited to adopt phase shift keying techniques to embed the watermark.\nIn the proposed schemes, the phases of DFT and SCHT coefficients are modified\nto convey watermark information using binary phase shift keying in cover video.\nLow amplitude block selection (LABS) is used to improve transparency, amplitude\nboost to improve the resistance of watermark from signal processing and\ncompression attacks and spread spectrum technique is used for encrypting\nwatermark in order to protect it from third party. It is observed that both\nalgorithms showing more or less same robustness but SCHT offers high\ntransparency, simple implementation and less computational cost than DFT.", 
    "link": "http://arxiv.org/pdf/1404.1314v1", 
    "arxiv-id": "1404.1314v1"
},{
    "category": "cs.MM", 
    "author": "Dr. Timur Mirzoev", 
    "title": "Reduction of Field Loss by a Video Processing System", 
    "publish": "2014-04-09T01:35:26Z", 
    "summary": "Streaming of 60 de-interlaced fields per second digital uncompressed video\nwith 720x480 resolution without a loss of video fields is one of the desired\ntechnologies by scientists in biomechanics. If it is possible to stream digital\nuncompressed video without dropped video fields, then a sophisticated computer\nanalysis of the transmitted via IEEE 1394a connection video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it using specific software such as Arial Performance Analysis\nSystems.", 
    "link": "http://arxiv.org/pdf/1404.2592v1", 
    "arxiv-id": "1404.2592v1"
},{
    "category": "cs.MM", 
    "author": "Noui Lemnouar", 
    "title": "A blind robust watermarking scheme based on svd and circulant matrices", 
    "publish": "2014-04-10T20:56:21Z", 
    "summary": "Multimedia security has been the aim point of considerable research activity\nbecause of its wide application area. The major technology to achieve copyright\nprotection, content authentication, access control and multimedia security is\nwatermarking which is the process of embedding data into a multimedia element\nsuch as image or audio, this embedded data can later be extracted from, or\ndetected in the embedded element for different purposes. In this work, a blind\nwatermarking algorithm based on SVD and circulant matrices has been presented.\nEvery circulant matrix is associated with a matrix for which the SVD\ndecomposition coincides with the spectral decomposition. This leads to improve\nthe Chandra algorithm [1], our presentation will include a discussion on the\ndata hiding capacity, watermark transparency and robustness against a wide\nrange of common image processing attacks.", 
    "link": "http://arxiv.org/pdf/1404.2952v1", 
    "arxiv-id": "1404.2952v1"
},{
    "category": "cs.MM", 
    "author": "Alfred M. Bruckstein", 
    "title": "Motion-Compensated Coding and Frame-Rate Up-Conversion: Models and   Analysis", 
    "publish": "2014-04-12T14:21:21Z", 
    "summary": "Block-based motion estimation (ME) and compensation (MC) techniques are\nwidely used in modern video processing algorithms and compression systems. The\ngreat variety of video applications and devices results in numerous compression\nspecifications. Specifically, there is a diversity of frame-rates and\nbit-rates. In this paper, we study the effect of frame-rate and compression\nbit-rate on block-based ME and MC as commonly utilized in inter-frame coding\nand frame-rate up conversion (FRUC). This joint examination yields a\ncomprehensive foundation for comparing MC procedures in coding and FRUC. First,\nthe video signal is modeled as a noisy translational motion of an image. Then,\nwe theoretically model the motion-compensated prediction of an available and\nabsent frames as in coding and FRUC applications, respectively. The theoretic\nMC-prediction error is further analyzed and its autocorrelation function is\ncalculated for coding and FRUC applications. We show a linear relation between\nthe variance of the MC-prediction error and temporal-distance. While the\naffecting distance in MC-coding is between the predicted and reference frames,\nMC-FRUC is affected by the distance between the available frames used for the\ninterpolation. Moreover, the dependency in temporal-distance implies an inverse\neffect of the frame-rate. FRUC performance analysis considers the prediction\nerror variance, since it equals to the mean-squared-error of the interpolation.\nHowever, MC-coding analysis requires the entire autocorrelation function of the\nerror; hence, analytic simplicity is beneficial. Therefore, we propose two\nconstructions of a separable autocorrelation function for prediction error in\nMC-coding. We conclude by comparing our estimations with experimental results.", 
    "link": "http://arxiv.org/pdf/1404.3290v1", 
    "arxiv-id": "1404.3290v1"
},{
    "category": "cs.MM", 
    "author": "Cl\u00e9ment Poncelet Sanchez", 
    "title": "Antescofo Intermediate Representation", 
    "publish": "2014-04-29T12:30:36Z", 
    "summary": "We describe an intermediate language designed as a medium-level internal\nrepresentation of programs of the interactive music system Antescofo. This\nrepresentation is independent both of the Antescofo source language and of the\narchitecture of the execution platform. It is used in tasks such as\nverification of timings, model-based conformance testing, static control-flow\nanalysis or simulation. This language is essentially a flat representation of\nAntescofo's code, as a finite state machine extended with local and global\nvariables, with delays and with concurrent threads creation. It features a\nsmall number of simple instructions which are either blocking (wait for\nexternal event, signal or duration) or not (variable assignment, message\nemission and control).", 
    "link": "http://arxiv.org/pdf/1404.7335v1", 
    "arxiv-id": "1404.7335v1"
},{
    "category": "cs.CR", 
    "author": "Sunit Kumar", 
    "title": "High Security Image Steganography with Modified Arnold cat map", 
    "publish": "2014-08-17T17:07:29Z", 
    "summary": "Information security is concerned with maintaining the secrecy, reliability\nand accessibility of data. The main objective of information security is to\nprotect information and information system from unauthorized access,\nrevelation, disruption, alteration, annihilation and use. This paper uses\nspatial domain LSB substitution method for information embedding and modified\nforms of Arnold transform are applied twice in two different phases to ensure\nsecurity. The system is tested and validated against a series of standard\nimages and the results show that the method is highly secure and provides high\ndata hiding capacity.", 
    "link": "http://arxiv.org/pdf/1408.3838v1", 
    "arxiv-id": "1408.3838v1"
},{
    "category": "cs.CV", 
    "author": "Alan F. Smeaton", 
    "title": "Object Segmentation in Images using EEG Signals", 
    "publish": "2014-08-19T15:24:44Z", 
    "summary": "This paper explores the potential of brain-computer interfaces in segmenting\nobjects from images. Our approach is centered around designing an effective\nmethod for displaying the image parts to the users such that they generate\nmeasurable brain reactions. When an image region, specifically a block of\npixels, is displayed we estimate the probability of the block containing the\nobject of interest using a score based on EEG activity. After several such\nblocks are displayed, the resulting probability map is binarized and combined\nwith the GrabCut algorithm to segment the image into object and background\nregions. This study shows that BCI and simple EEG analysis are useful in\nlocating object boundaries in images.", 
    "link": "http://arxiv.org/pdf/1408.4363v1", 
    "arxiv-id": "1408.4363v1"
},{
    "category": "cs.MM", 
    "author": "J\u00f6rg Ott", 
    "title": "Characterizing Internet Video for Large-scale Active Measurements", 
    "publish": "2014-08-07T16:38:25Z", 
    "summary": "The availability of high definition video content on the web has brought\nabout a significant change in the characteristics of Internet video, but not\nmany studies on characterizing video have been done after this change. Video\ncharacteristics such as video length, format, target bit rate, and resolution\nprovide valuable input to design Adaptive Bit Rate (ABR) algorithms, sizing\nplayout buffers in Dynamic Adaptive HTTP streaming (DASH) players, model the\nvariability in video frame sizes, etc. This paper presents datasets collected\nin 2013 and 2014 that contains over 130,000 videos from YouTube's most viewed\n(or most popular) video charts in 58 countries. We describe the basic\ncharacteristics of the videos on YouTube for each category, format, video\nlength, file size, and data rate variation, observing that video length and\nfile size fit a log normal distribution. We show that three minutes of a video\nsuffice to represent its instant data rate fluctuation and that we can infer\ndata rate characteristics of different video resolutions from a single given\none. Based on our findings, we design active measurements for measuring the\nperformance of Internet video.", 
    "link": "http://arxiv.org/pdf/1408.5777v1", 
    "arxiv-id": "1408.5777v1"
},{
    "category": "cs.MM", 
    "author": "Mohammad Saiedur Rahaman", 
    "title": "An adaptive quasi harmonic broadcasting scheme with optimal bandwidth   requirement", 
    "publish": "2014-10-06T17:48:39Z", 
    "summary": "The aim of Harmonic Broadcasting protocol is to reduce the bandwidth usage in\nvideo-on-demand service where a video is divided into some equal sized segments\nand every segment is repeatedly transmitted over a number of channels that\nfollows harmonic series for channel bandwidth assignment. As the bandwidth of\nchannels differs from each other and users can join at any time to these\nmulticast channels, they may experience a synchronization problem between\ndownload and playback. To deal with this issue, some schemes have been\nproposed, however, at the cost of additional or wastage of bandwidth or sudden\nextreme bandwidth requirement. In this paper we present an adaptive quasi\nharmonic broadcasting scheme (AQHB) which delivers all data segment on time\nthat is the download and playback synchronization problem is eliminated while\nkeeping the bandwidth consumption as same as traditional harmonic broadcasting\nscheme without cost of any additional or wastage of bandwidth. It also ensures\nthe video server not to increase the channel bandwidth suddenly that is, also\neliminates the sudden buffer requirement at the client side. We present several\nanalytical results to exhibit the efficiency of our proposed broadcasting\nscheme over the existing ones.", 
    "link": "http://arxiv.org/pdf/1410.1474v1", 
    "arxiv-id": "1410.1474v1"
},{
    "category": "cs.GR", 
    "author": "Martin Prantl", 
    "title": "Image compression overview", 
    "publish": "2014-09-14T11:10:06Z", 
    "summary": "Compression plays a significant role in a data storage and a transmission. If\nwe speak about a generall data compression, it has to be a lossless one. It\nmeans, we are able to recover the original data 1:1 from the compressed file.\nMultimedia data (images, video, sound...), are a special case. In this area, we\ncan use something called a lossy compression. Our main goal is not to recover\ndata 1:1, but only keep them visually similar. This article is about an image\ncompression, so we will be interested only in image compression. For a human\neye, it is not a huge difference, if we recover RGB color with values\n[150,140,138] instead of original [151,140,137]. The magnitude of a difference\ndetermines the loss rate of the compression. The bigger difference usually\nmeans a smaller file, but also worse image quality and noticable differences\nfrom the original image. We want to cover compression techniques mainly from\nthe last decade. Many of them are variations of existing ones, only some of\nthem uses new principes.", 
    "link": "http://arxiv.org/pdf/1410.2259v1", 
    "arxiv-id": "1410.2259v1"
},{
    "category": "cs.MM", 
    "author": "Xiaolong Fu", 
    "title": "Recommendation Scheme Based on Converging Properties for Contents   Broadcasting", 
    "publish": "2014-10-09T00:53:11Z", 
    "summary": "Popular videos are often clicked by a mount of users in a short period. With\ncontent recommendation, the popular contents could be broadcast to the\npotential users in wireless network, to save huge transmitting resource. In\nthis paper, the contents propagation model is analyzed due to users' historical\nbehavior, location, and the converging properties in wireless data\ntransmission, with the users' communication log in the Chinese commercial\ncellular network. And a recommendation scheme is proposed to achieve high\nenergy efficiency.", 
    "link": "http://arxiv.org/pdf/1410.2324v1", 
    "arxiv-id": "1410.2324v1"
},{
    "category": "cs.MM", 
    "author": "Ankit Chaudhary", 
    "title": "Hiding Sound in Image by K-LSB Mutation", 
    "publish": "2014-10-24T06:36:48Z", 
    "summary": "In this paper a novel approach to hide sound files in a digital image is\nproposed and implemented such that it becomes difficult to conclude about the\nexistence of the hidden data inside the image. In this approach, we utilize the\nrightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The\npixels are so chosen that the distortion in image would be minimized due to\nembedding. This requires comparing all the possible permutations of pixel\nvalues, which may would lead to exponential time computation. To speed up this,\nCuckoo Search (CS) could be used to find the most optimal solution. The\nadvantage of using proposed CS is that it is easy to implement and is very\neffective at converging in relatively less iterations/generations.", 
    "link": "http://arxiv.org/pdf/1410.6592v1", 
    "arxiv-id": "1410.6592v1"
},{
    "category": "cs.MM", 
    "author": "Benedikt Boehm", 
    "title": "StegExpose - A Tool for Detecting LSB Steganography", 
    "publish": "2014-10-24T11:52:00Z", 
    "summary": "Steganalysis tools play an important part in saving time and providing new\nangles of attack for forensic analysts. StegExpose is a solution designed for\nuse in the real world, and is able to analyse images for LSB steganography in\nbulk using proven attacks in a time efficient manner. When steganalytic methods\nare combined intelligently, they are able generate even more accurate results.\nThis is the prime focus of StegExpose.", 
    "link": "http://arxiv.org/pdf/1410.6656v1", 
    "arxiv-id": "1410.6656v1"
},{
    "category": "cs.MM", 
    "author": "Luca Caviglione", 
    "title": "Steganography in Modern Smartphones and Mitigation Techniques", 
    "publish": "2014-08-27T08:46:05Z", 
    "summary": "By offering sophisticated services and centralizing a huge volume of personal\ndata, modern smartphones changed the way we socialize, entertain and work. To\nthis aim, they rely upon complex hardware/software frameworks leading to a\nnumber of vulnerabilities, attacks and hazards to profile individuals or gather\nsensitive information. However, the majority of works evaluating the security\ndegree of smartphones neglects steganography, which can be mainly used to: i)\nexfiltrate confidential data via camouflage methods, and ii) conceal valuable\nor personal information into innocent looking carriers.\n  Therefore, this paper surveys the state of the art of steganographic\ntechniques for smartphones, with emphasis on methods developed over the period\n2005 to the second quarter of 2014. The different approaches are grouped\naccording to the portion of the device used to hide information, leading to\nthree different covert channels, i.e., local, object and network. Also, it\nreviews the relevant approaches used to detect and mitigate steganographic\nattacks or threats. Lastly, it showcases the most popular software applications\nto embed secret data into carriers, as well as possible future directions.", 
    "link": "http://arxiv.org/pdf/1410.6796v1", 
    "arxiv-id": "1410.6796v1"
},{
    "category": "cs.CR", 
    "author": "Musheer Ahmad", 
    "title": "Fisher-Yates Chaotic Shuffling Based Image Encryption", 
    "publish": "2014-10-28T07:48:03Z", 
    "summary": "In Present era, information security is of utmost concern and encryption is\none of the alternatives to ensure security. Chaos based cryptography has\nbrought a secure and efficient way to meet the challenges of secure multimedia\ntransmission over the networks. In this paper, we have proposed a secure\nGrayscale image encryption methodology in wavelet domain. The proposed\nalgorithm performs shuffling followed by encryption using states of chaotic map\nin a secure manner. Firstly, the image is transformed from spatial domain to\nwavelet domain by the Haar wavelet. Subsequently, Fisher Yates chaotic\nshuffling technique is employed to shuffle the image in wavelet domain to\nconfuse the relationship between plain image and cipher image. A key dependent\npiece-wise linear chaotic map is used to generate chaos for the chaotic\nshuffling. Further, the resultant shuffled approximate coefficients are\nchaotically modulated. To enhance the statistical characteristics from\ncryptographic point of view, the shuffled image is self keyed diffused and\nmixing operation is carried out using keystream extracted from one-dimensional\nchaotic map and the plain-image. The proposed algorithm is tested over some\nstandard image dataset. The results of several experimental, statistical and\nsensitivity analyses proved that the algorithm provides an efficient and secure\nmethod to achieve trusted gray scale image encryption.", 
    "link": "http://arxiv.org/pdf/1410.7540v1", 
    "arxiv-id": "1410.7540v1"
},{
    "category": "cs.CY", 
    "author": "Joscha Jaeger", 
    "title": "Annotating Video with Open Educational Resources in a Flipped Classroom   Scenario", 
    "publish": "2014-12-04T19:26:07Z", 
    "summary": "A wealth of Open Educational Resources is now available, and beyond the first\nand evident problem of finding them, the issue of articulating a set of\nresources is arising. When using audiovisual resources, among different\npossibilities, annotating a video resource with additional resources linked to\nspecific fragments can constitute one of the articulation modalities.\nAnnotating a video is a complex task, and in a pedagogical context,\nintermediary activities should be proposed in order to mitigate this\ncomplexity. In this paper, we describe a tool dedicated to supporting video\nannotation activities. It aims at improving learner engagement, by having\nstudents be more active when watching videos by offering a progressive\nannotation process, first guided by providing predefined resources, then more\nfreely, to accompany users in the practice of annotating videos.", 
    "link": "http://arxiv.org/pdf/1412.1780v1", 
    "arxiv-id": "1412.1780v1"
},{
    "category": "cs.CV", 
    "author": "Shmuel Peleg", 
    "title": "EgoSampling: Fast-Forward and Stereo for Egocentric Videos", 
    "publish": "2014-12-11T10:37:55Z", 
    "summary": "While egocentric cameras like GoPro are gaining popularity, the videos they\ncapture are long, boring, and difficult to watch from start to end. Fast\nforwarding (i.e. frame sampling) is a natural choice for faster video browsing.\nHowever, this accentuates the shake caused by natural head motion, making the\nfast forwarded video useless.\n  We propose EgoSampling, an adaptive frame sampling that gives more stable\nfast forwarded videos. Adaptive frame sampling is formulated as energy\nminimization, whose optimal solution can be found in polynomial time.\n  In addition, egocentric video taken while walking suffers from the left-right\nmovement of the head as the body weight shifts from one leg to another. We turn\nthis drawback into a feature: Stereo video can be created by sampling the\nframes from the left most and right most head positions of each step, forming\napproximate stereo-pairs.", 
    "link": "http://arxiv.org/pdf/1412.3596v2", 
    "arxiv-id": "1412.3596v2"
},{
    "category": "cs.MM", 
    "author": "Yeong Min Jang", 
    "title": "Radio Resource Allocation for Scalable Video Services over Wireless   Cellular Networks", 
    "publish": "2014-12-11T12:35:11Z", 
    "summary": "Good quality video services always require higher bandwidth. Hence, to\nprovide the video services e.g., multicast/broadcast services (MBS) and unicast\nservices along with the existing voice, internet, and other background traffic\nservices over the wireless cellular networks, it is required to efficiently\nmanage the wireless resources in order to reduce the overall forced call\ntermination probability, to maximize the overall service quality, and to\nmaximize the revenue. Fixed bandwidth allocation for the MBS sessions either\nreduces the quality of the MBS videos and bandwidth utilization or increases\nthe overall forced call termination probability and of course the handover call\ndropping probability as well. Scalable Video Coding (SVC) technique allows the\nvariable bit rate allocation for the video services. In this paper, we propose\na bandwidth allocation scheme that efficiently allocates bandwidth among the\nMBS sessions and the non-MBS traffic calls (e.g., voice, unicast, internet, and\nother background traffic). The proposed scheme reduces the bandwidth allocation\nfor the MBS sessions during the congested traffic condition only to accommodate\nmore calls in the system. Instead of allocating fixed bandwidths for the BMS\nsessions and the non-MBS traffic, our scheme allocates variable bandwidths for\nthem. However, the minimum quality of the videos is guaranteed by allocating\nminimum bandwidth for them. Using the mathematical and numerical analyses, we\nshow that the proposed scheme maximizes the bandwidth utilization and\nsignificantly reduces the overall forced call termination probability as well\nas the handover call dropping probability.", 
    "link": "http://arxiv.org/pdf/1412.3628v1", 
    "arxiv-id": "1412.3628v1"
},{
    "category": "cs.MM", 
    "author": "Zygmunt J. Haas", 
    "title": "Call Admission Control based on Adaptive Bandwidth Allocation for   Wireless Networks", 
    "publish": "2014-12-11T12:38:28Z", 
    "summary": "Provisioning of Quality of Service (QoS) is a key issue in any multi-media\nsystem. However, in wireless systems, supporting QoS requirements of different\ntraffic types is more challenging due to the need to minimize two performance\nmetrics - the probability of dropping a handover call and the probability of\nblocking a new call. Since QoS requirements are not as stringent for\nnon-real-time traffic types, as opposed to real-time traffic, more calls can be\naccommodated by releasing some bandwidth from the already admitted\nnon-real-time traffic calls. If we require that such a released bandwidth to\naccept a handover call ought to be larger than the bandwidth to accept a new\ncall, then the resulting probability of dropping a handover call will be\nsmaller than the probability of blocking a new call. In this paper we propose\nan efficient Call Admission Control (CAC) that relies on adaptive multi-level\nbandwidth-allocation scheme for non-real-time calls. The scheme allows\nreduction of the call dropping probability along with increase of the bandwidth\nutilization. The numerical results show that the proposed scheme is capable of\nattaining negligible handover call dropping probability without sacrificing\nbandwidth utilization.", 
    "link": "http://arxiv.org/pdf/1412.3630v1", 
    "arxiv-id": "1412.3630v1"
},{
    "category": "cs.NI", 
    "author": "Mostafa Zaman Chowdhury", 
    "title": "Adaptive Resource Management for Multimedia Applications in   Femtocellular and Macrocellular Networks", 
    "publish": "2014-12-14T06:03:57Z", 
    "summary": "The increasing demands of various high data rate wireless applications have\nbeen seen in the recent years and it will continue in the future. To fulfill\nthese demands, the limited existing wireless resources should be utilized\nproperly or new wireless technology should be developed. Therefore, we propose\nsome novel idea to manage the wireless resources and deployment of\nfemtocellular network technology. The study was mainly divided into two parts:\n(a) femtocellular network deployment and resource allocation and (b) resource\nmanagement for macrocellular networks. The femtocellular network deployment\nscenarios, integrated femtocell/macrocell network architectures, cost-effective\nfrequency planning, and mobility management schemes are presented in first\npart. In the second part, we provide a CAC based on adaptive bandwidth\nallocation for the wireless network in. The proposed CAC relies on adaptive\nmulti-level bandwidth-allocation scheme for non-real-time calls. We propose\nvideo service provisioning over wireless networks. We provide a QoS adaptive\nradio resource allocation as well as popularity based bandwidth allocation\nschemes for scalable videos over wireless cellular networks. All the proposed\nschemes are verified through several numerical and simulation results. The\nresearch results presented in this dissertation clearly imply the advantages of\nour proposed schemes.", 
    "link": "http://arxiv.org/pdf/1412.4318v1", 
    "arxiv-id": "1412.4318v1"
},{
    "category": "cs.MM", 
    "author": "Sreenivasa Reddy Edara", 
    "title": "Block Based Medical Image Watermarking Technique for Tamper Detection   and Recovery", 
    "publish": "2014-11-17T16:58:54Z", 
    "summary": "In this paper, we propose a novel fragile block based medical image\nwatermarking technique for embedding data of patient into medical image,\nverifying the integrity of ROI (Region of Interest), detecting the tampered\nblocks inside ROI and recovering original ROI with less size authentication and\nrecovery data and with simple mathematical calculations. In the proposed\nmethod, the medical image is divided into three regions called ROI, RONI\n(Region of Non Interest) and border pixels. Later, authentication data of ROI\nand Electronic Patient Record (EPR) are compressed using Run Length Encoding\n(RLE) technique and then embedded into ROI. Recovery information of ROI is\nembedded inside RONI and information of ROI is embedded inside border pixels.\nResults of experiments conducted on several medical images reveal that proposed\nmethod produces high quality watermarked medical images, identifies tampered\nareas inside ROI of watermarked medical images and recovers the original ROI.", 
    "link": "http://arxiv.org/pdf/1412.6143v1", 
    "arxiv-id": "1412.6143v1"
},{
    "category": "cs.NI", 
    "author": "Kun Hua", 
    "title": "Compression of Video Tracking and Bandwidth Balancing Routing in   Wireless Multimedia Sensor Networks", 
    "publish": "2014-12-20T03:57:47Z", 
    "summary": "There has been a tremendous growth in multimedia applications over wireless\nnetworks. Wireless Multimedia Sensor Networks(WMSNs) have become the premier\nchoice in many research communities and industry. Many state-of-art\napplications, such as surveillance, traffic monitoring, and remote heath care\nare essentially video tracking and transmission in WMSNs. The transmission\nspeed is constrained by big size of video data and fixed bandwidth allocation\nin constant routing path. In this paper, we present a CamShift based algorithm\nto compress the tracking of videos. Then we propose a bandwidth balancing\nstrategy in which each sensor node is able to dynamically select the node for\nnext hop with the highest potential bandwidth capacity to resume communication.\nKey to the strategy is that each node merely maintains two parameters that\ncontains its historical bandwidth varying trend and then predicts its near\nfuture bandwidth capacity. Then forwarding node selects the next hop with the\nhighest potential bandwidth capacity. Simulations demonstrate that our approach\nsignificantly increases the data received by sink node and decreases the delay\non video transmission in Wireless Multimedia Sensor Network environment.", 
    "link": "http://arxiv.org/pdf/1412.6593v1", 
    "arxiv-id": "1412.6593v1"
},{
    "category": "cs.MM", 
    "author": "Seyed Rasoul Mousavi", 
    "title": "Improving image watermarking based on Tabu search by Chaos", 
    "publish": "2015-01-07T17:59:15Z", 
    "summary": "With the fast development of communication and multimedia technology, the\nrights of the owners of multimedia products is vulnerable to the unauthorized\ncopies and watermarking is one of the best known methods for proving the\nownership of a product. In this paper we prosper the previous watermarking\nmethod which was based on Tabu search by Chaos. The modification applied in the\npermutation step of watermarking and the initial population generation of the\nTabu search. We analyze our method on some well known images and experimental\nresults shows the improvement in the quality and speed of the proposed\nwatermarking method.", 
    "link": "http://arxiv.org/pdf/1501.01576v3", 
    "arxiv-id": "1501.01576v3"
},{
    "category": "cs.MM", 
    "author": "Mohammed Ghanbari", 
    "title": "Minimization of image watermarking side effects through subjective   optimization", 
    "publish": "2015-01-08T08:02:01Z", 
    "summary": "This paper investigates the use of Structural Similaritys (SSIM) index on the\nminimized side effect to image watermarking. For fast implementation and more\ncompatibility with the standard DCT based codecs, watermark insertion is\ncarried out on the DCT coefficients and hence a SSIM model for DCT based\nwatermarking is developed. For faster implementation, the SSIM index is\nmaximized over independent 4x4 non-overlapped blocks but the disparity between\nthe adjacent blocks reduces the overall image quality. This problem is resolved\nthrough optimization of overlapped blocks, but, the higher image quality is\nachieved at a cost of high computational complexity. To reduce the\ncomputational complexity while preserving the good quality, optimization of\nsemi-overlapped blocks is introduced. We show that while SSIM-based\noptimization over overlapped blocks has as high as 64 times the complexity of\nthe 4x4 non-overlapped method, with semi-overlapped optimization the high\nquality of overlapped method is preserved only at a cost of less than 8 times\nthe non-overlapped method.", 
    "link": "http://arxiv.org/pdf/1501.01755v1", 
    "arxiv-id": "1501.01755v1"
},{
    "category": "cs.MM", 
    "author": "Shahrokh Ghaemmaghami", 
    "title": "Enhance Robustness of Image-in-Image Watermarking through Data   Partitioning", 
    "publish": "2015-01-08T08:08:50Z", 
    "summary": "Vulnerability of watermarking schemes against intense signal processing\nattacks is generally a major concern, particularly when there are techniques to\nreproduce an acceptable copy of the original signal with no chance for\ndetecting the watermark. In this paper, we propose a two-layer, data\npartitioning (DP) based, image in image watermarking method in the DCT domain\nto improve the watermark detection performance. Truncated singular value\ndecomposition, binary wavelet decomposition and spatial scalability idea in\nH.264/SVC are analyzed and employed as partitioning methods. It is shown that\nthe proposed scheme outperforms its two recent competitors in terms of both\ndata payload and robustness to intense attacks.", 
    "link": "http://arxiv.org/pdf/1501.01758v1", 
    "arxiv-id": "1501.01758v1"
},{
    "category": "cs.CV", 
    "author": "Jalil Rasekhi", 
    "title": "A Modified No Search Algorithm for Fractal Image Compression", 
    "publish": "2015-01-13T07:19:17Z", 
    "summary": "Fractal image compression has some desirable properties like high quality at\nhigh compression ratio, fast decoding, and resolution independence. Therefore\nit can be used for many applications such as texture mapping and pattern\nrecognition and image watermarking. But it suffers from long encoding time due\nto its need to find the best match between sub blocks. This time is related to\nthe approach that is used. In this paper we present a fast encoding Algorithm\nbased on no search method. Our goal is that more blocks are covered in initial\nstep of quad tree algorithm. Experimental result has been compared with other\nnew fast fractal coding methods, showing it is better in term of bit rate in\nsame condition while the other parameters are fixed.", 
    "link": "http://arxiv.org/pdf/1501.02894v2", 
    "arxiv-id": "1501.02894v2"
},{
    "category": "cs.NI", 
    "author": "Luigia Micciullo", 
    "title": "LTE enhancements for Public Safety and Security communications to   support Group Multimedia Communications", 
    "publish": "2015-01-15T09:46:22Z", 
    "summary": "Currently Public Safety and Security communication systems rely on reliable\nand secure Professional Mobile Radio (PMR) Networks that are mainly devoted to\nprovide voice services. However, the evolution trend for PMR networks is\ntowards the provision of new value-added multimedia services such as video\nstreaming, in order to improve the situational awareness and enhance the\nlife-saving operations. The challenge here is to exploit the future commercial\nbroadband networks to deliver voice and multimedia services satisfying the PMR\nservice requirements. In particular, a viable solution till now seems that of\nadapting the new Long Term Evolution technology to provide IP-based broadband\nservices with the security and reliability typical of PMR networks. This paper\noutlines different alternatives to achieve this goal and, in particular,\nproposes a proper solution for providing multimedia services with PMR standards\nover commercial LTE networks.", 
    "link": "http://arxiv.org/pdf/1501.03613v1", 
    "arxiv-id": "1501.03613v1"
},{
    "category": "cs.NI", 
    "author": "Chang Wen Chen", 
    "title": "Service Provisioning and Profit Maximization in Network-assisted   Adaptive HTTP Streaming", 
    "publish": "2015-01-18T02:16:02Z", 
    "summary": "Adaptive HTTP streaming with centralized consideration of multiple streams\nhas gained increasing interest. It poses a special challenge that the interests\nof both content provider and network operator need to be deliberately balanced.\nMore importantly, the adaptation strategy is required to be flexible enough to\nbe ported to various systems that work under different network environments,\nQoE levels, and economic objectives. To address these challenges, we propose a\nMarkov Decision Process (MDP) based network-assisted adaptation framework,\nwherein cost of buffering, significant playback variation, bandwidth management\nand income of playback are jointly investigated. We then demonstrate its\npromising service provisioning and maximal profit for a mobile network in which\nfair or differentiated service is required.", 
    "link": "http://arxiv.org/pdf/1501.04254v1", 
    "arxiv-id": "1501.04254v1"
},{
    "category": "cs.NI", 
    "author": "Hossam Afifi", 
    "title": "Optimization of Quality of Experience through File Duplication in Video   Sharing Servers", 
    "publish": "2015-02-04T08:26:53Z", 
    "summary": "Consumers of short videos on Internet can have a bad Quality of Experience\nQoE due to the long distance between the consumers and the servers that hosting\nthe videos. We propose an optimization of the file allocation in\ntelecommunication operators content sharing servers to improve the QoE through\nfiles duplication, thus bringing the files closer to the consumers. This\noptimization allows the network operator to set the level of QoE and to have\ncontrol over the users access cost by setting a number of parameters. Two\noptimization methods are given and are followed by a comparison of their\nefficiency. Also, the hosting costs versus the gain of optimization are\nanalytically discussed.", 
    "link": "http://arxiv.org/pdf/1502.01120v1", 
    "arxiv-id": "1502.01120v1"
},{
    "category": "cs.SD", 
    "author": "Radoje Albijanic", 
    "title": "CS reconstruction of the speech and musical signals", 
    "publish": "2015-02-05T20:40:41Z", 
    "summary": "The application of Compressive sensing approach to the speech and musical\nsignals is considered in this paper. Compressive sensing (CS) is a new approach\nto the signal sampling that allows signal reconstruction from a small set of\nrandomly acquired samples. This method is developed for the signals that\nexhibit the sparsity in a certain domain. Here we have observed two sparsity\ndomains: discrete Fourier and discrete cosine transform domain. Furthermore,\ntwo different types of audio signals are analyzed in terms of sparsity and CS\nperformance - musical and speech signals. Comparative analysis of the CS\nreconstruction using different number of signal samples is performed in the two\ndomains of sparsity. It is shown that the CS can be successfully applied to\nboth, musical and speech signals, but the speech signals are more demanding in\nterms of the number of observations. Also, our results show that discrete\ncosine transform domain allows better reconstruction using lower number of\nobservations, compared to the Fourier transform domain, for both types of\nsignals.", 
    "link": "http://arxiv.org/pdf/1502.01707v1", 
    "arxiv-id": "1502.01707v1"
},{
    "category": "cs.CR", 
    "author": "Hu Chen", 
    "title": "An SVD-based Fragile Watermarking Scheme With Grouped Blocks", 
    "publish": "2015-02-10T08:18:10Z", 
    "summary": "This paper proposes a novel fragile watermarking scheme for digital image\nauthentication which is based on Singular Value Decomposition(SVD) and grouped\nblocks. The watermark bits which include two types of bits are inserted into\nthe least significant bit(LSB) plane of the host image using the adaptive\nchaotic map to determine the positions. The groped blocks break the block-wise\nindependence and therefore can withstand the Vector Quantization attack(VQ\nattack). The inserting positions are related to the statistical information of\nimage block data, in order to increase the security and provide an auxiliary\nway to authenticate the image data. The effectiveness of the proposed scheme is\nchecked by a variety of attacks, and the experimental results prove that it has\na remarkable tamper detection ability and also has a precise locating ability.", 
    "link": "http://arxiv.org/pdf/1502.02809v2", 
    "arxiv-id": "1502.02809v2"
},{
    "category": "cs.NI", 
    "author": "Adam Wolisz", 
    "title": "A Control-Theoretic Approach to Adaptive Video Streaming in Dense   Wireless Networks", 
    "publish": "2015-02-10T15:18:21Z", 
    "summary": "Recently, the way people consume video content has been undergoing a dramatic\nchange. Plain TV sets, that have been the center of home entertainment for a\nlong time, are losing grounds to Hybrid TV's, PC's, game consoles, and, more\nrecently, mobile devices such as tablets and smartphones. The new predominant\nparadigm is: watch what I want, when I want, and where I want.\n  The challenges of this shift are manifold. On the one hand, broadcast\ntechnologies such as DVB-T/C/S need to be extended or replaced by mechanisms\nsupporting asynchronous viewing, such as IPTV and video streaming over\nbest-effort networks, while remaining scalable to millions of users. On the\nother hand, the dramatic increase of wireless data traffic begins to stretch\nthe capabilities of the existing wireless infrastructure to its limits.\nFinally, there is a challenge to video streaming technologies to cope with a\nhigh heterogeneity of end-user devices and dynamically changing network\nconditions, in particular in wireless and mobile networks.\n  In the present work, our goal is to design an efficient system that supports\na high number of unicast streaming sessions in a dense wireless access network.\nWe address this goal by jointly considering the two problems of wireless\ntransmission scheduling and video quality adaptation, using techniques inspired\nby the robustness and simplicity of Proportional-Integral-Derivative (PID)\ncontrollers. We show that the control-theoretic approach allows to efficiently\nutilize available wireless resources, providing high Quality of Experience\n(QoE) to a large number of users.", 
    "link": "http://arxiv.org/pdf/1502.02943v1", 
    "arxiv-id": "1502.02943v1"
},{
    "category": "cs.SD", 
    "author": "Julius O. Smith III", 
    "title": "Efficient Synthesis of Room Acoustics via Scattering Delay Networks", 
    "publish": "2015-02-19T23:58:36Z", 
    "summary": "An acoustic reverberator consisting of a network of delay lines connected via\nscattering junctions is proposed. All parameters of the reverberator are\nderived from physical properties of the enclosure it simulates. It allows for\nsimulation of unequal and frequency-dependent wall absorption, as well as\ndirectional sources and microphones. The reverberator renders the first-order\nreflections exactly, while making progressively coarser approximations of\nhigher-order reflections. The rate of energy decay is close to that obtained\nwith the image method (IM) and consistent with the predictions of Sabine and\nEyring equations. The time evolution of the normalized echo density, which was\npreviously shown to be correlated with the perceived texture of reverberation,\nis also close to that of IM. However, its computational complexity is one to\ntwo orders of magnitude lower, comparable to the computational complexity of a\nfeedback delay network (FDN), and its memory requirements are negligible.", 
    "link": "http://arxiv.org/pdf/1502.05751v2", 
    "arxiv-id": "1502.05751v2"
},{
    "category": "cs.CV", 
    "author": "Jun Xie", 
    "title": "Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise   Tone Mapping", 
    "publish": "2015-02-21T07:36:26Z", 
    "summary": "Video enhancement plays an important role in various video applications. In\nthis paper, we propose a new intra-and-inter-constraint-based video enhancement\napproach aiming to 1) achieve high intra-frame quality of the entire picture\nwhere multiple region-of-interests (ROIs) can be adaptively and simultaneously\nenhanced, and 2) guarantee the inter-frame quality consistencies among video\nframes. We first analyze features from different ROIs and create a piecewise\ntone mapping curve for the entire frame such that the intra-frame quality of a\nframe can be enhanced. We further introduce new inter-frame constraints to\nimprove the temporal quality consistency. Experimental results show that the\nproposed algorithm obviously outperforms the state-of-the-art algorithms.", 
    "link": "http://arxiv.org/pdf/1502.06080v1", 
    "arxiv-id": "1502.06080v1"
},{
    "category": "cs.MM", 
    "author": "Jiangchuan Liu", 
    "title": "Crowdsourced Live Streaming over the Cloud", 
    "publish": "2015-02-23T04:44:02Z", 
    "summary": "Empowered by today's rich tools for media generation and distribution, and\nthe convenient Internet access, crowdsourced streaming generalizes the\nsingle-source streaming paradigm by including massive contributors for a video\nchannel. It calls a joint optimization along the path from crowdsourcers,\nthrough streaming servers, to the end-users to minimize the overall latency.\nThe dynamics of the video sources, together with the globalized request demands\nand the high computation demand from each sourcer, make crowdsourced live\nstreaming challenging even with powerful support from modern cloud computing.\nIn this paper, we present a generic framework that facilitates a cost-effective\ncloud service for crowdsourced live streaming. Through adaptively leasing, the\ncloud servers can be provisioned in a fine granularity to accommodate\ngeo-distributed video crowdsourcers. We present an optimal solution to deal\nwith service migration among cloud instances of diverse lease prices. It also\naddresses the location impact to the streaming quality. To understand the\nperformance of the proposed strategies in the realworld, we have built a\nprototype system running over the planetlab and the Amazon/Microsoft Cloud. Our\nextensive experiments demonstrate that the effectiveness of our solution in\nterms of deployment cost and streaming quality.", 
    "link": "http://arxiv.org/pdf/1502.06314v1", 
    "arxiv-id": "1502.06314v1"
},{
    "category": "cs.CV", 
    "author": "Shih-Fu Chang", 
    "title": "Exploiting Feature and Class Relationships in Video Categorization with   Regularized Deep Neural Networks", 
    "publish": "2015-02-25T15:41:48Z", 
    "summary": "In this paper, we study the challenging problem of categorizing videos\naccording to high-level semantics such as the existence of a particular human\naction or a complex event. Although extensive efforts have been devoted in\nrecent years, most existing works combined multiple video features using simple\nfusion strategies and neglected the utilization of inter-class semantic\nrelationships. This paper proposes a novel unified framework that jointly\nexploits the feature relationships and the class relationships for improved\ncategorization performance. Specifically, these two types of relationships are\nestimated and utilized by rigorously imposing regularizations in the learning\nprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can be\nefficiently realized using a GPU-based implementation with an affordable\ntraining cost. Through arming the DNN with better capability of harnessing both\nthe feature and the class relationships, the proposed rDNN is more suitable for\nmodeling video semantics. With extensive experimental evaluations, we show that\nrDNN produces superior performance over several state-of-the-art approaches. On\nthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain\nvery competitive results: 66.9\\% and 73.5\\% respectively in terms of mean\naverage precision. In addition, to substantially evaluate our rDNN and\nstimulate future research on large scale video categorization, we collect and\nrelease a new benchmark dataset, called FCVID, which contains 91,223 Internet\nvideos and 239 manually annotated categories.", 
    "link": "http://arxiv.org/pdf/1502.07209v1", 
    "arxiv-id": "1502.07209v1"
},{
    "category": "cs.MM", 
    "author": "Rashid Jalal Qureshi", 
    "title": "A Secure Cyclic Steganographic Technique for Color Images using   Randomization", 
    "publish": "2015-02-27T03:05:43Z", 
    "summary": "Information Security is a major concern in today's modern era. Almost all the\ncommunicating bodies want the security, confidentiality and integrity of their\npersonal data. But this security goal cannot be achieved easily when we are\nusing an open network like Internet. Steganography provides one of the best\nsolutions to this problem. This paper represents a new Cyclic Steganographic T\nechnique (CST) based on Least Significant Bit (LSB) for true color (RGB)\nimages. The proposed method hides the secret data in the LSBs of cover image\npixels in a randomized cyclic manner. The proposed technique is evaluated using\nboth subjective and objective analysis using histograms changeability, Peak\nSignal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is\nfound that the proposed method gives promising results in terms of security,\nimperceptibility and robustness as compared to some existent methods and\nvindicates this new algorithm.", 
    "link": "http://arxiv.org/pdf/1502.07808v1", 
    "arxiv-id": "1502.07808v1"
},{
    "category": "cs.MM", 
    "author": "Stefano Tubaro", 
    "title": "Hybrid coding of visual content and local image features", 
    "publish": "2015-02-27T08:44:21Z", 
    "summary": "Distributed visual analysis applications, such as mobile visual search or\nVisual Sensor Networks (VSNs) require the transmission of visual content on a\nbandwidth-limited network, from a peripheral node to a processing unit.\nTraditionally, a Compress-Then-Analyze approach has been pursued, in which\nsensing nodes acquire and encode the pixel-level representation of the visual\ncontent, that is subsequently transmitted to a sink node in order to be\nprocessed. This approach might not represent the most effective solution, since\nseveral analysis applications leverage a compact representation of the content,\nthus resulting in an inefficient usage of network resources. Furthermore,\ncoding artifacts might significantly impact the accuracy of the visual task at\nhand. To tackle such limitations, an orthogonal approach named\nAnalyze-Then-Compress has been proposed. According to such a paradigm, sensing\nnodes are responsible for the extraction of visual features, that are encoded\nand transmitted to a sink node for further processing. In spite of improved\ntask efficiency, such paradigm implies the central processing node not being\nable to reconstruct a pixel-level representation of the visual content. In this\npaper we propose an effective compromise between the two paradigms, namely\nHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual\ncontent and local image features. Furthermore, we show how a target tradeoff\nbetween image quality and task accuracy might be achieved by accurately\nallocating the bitrate to either visual content or local features.", 
    "link": "http://arxiv.org/pdf/1502.07828v1", 
    "arxiv-id": "1502.07828v1"
},{
    "category": "cs.MM", 
    "author": "Stefano Tubaro", 
    "title": "Coding local and global binary visual features extracted from video   sequences", 
    "publish": "2015-02-26T14:23:39Z", 
    "summary": "Binary local features represent an effective alternative to real-valued\ndescriptors, leading to comparable results for many visual analysis tasks,\nwhile being characterized by significantly lower computational complexity and\nmemory requirements. When dealing with large collections, a more compact\nrepresentation based on global features is often preferred, which can be\nobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)\nmodel. Several applications, including for example visual sensor networks and\nmobile augmented reality, require visual features to be transmitted over a\nbandwidth-limited network, thus calling for coding techniques that aim at\nreducing the required bit budget, while attaining a target level of efficiency.\nIn this paper we investigate a coding scheme tailored to both local and global\nbinary features, which aims at exploiting both spatial and temporal redundancy\nby means of intra- and inter-frame coding. In this respect, the proposed coding\nscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)\nparadigm. That is, visual features are extracted from the acquired content,\nencoded at remote nodes, and finally transmitted to a central controller that\nperforms visual analysis. This is in contrast with the traditional approach, in\nwhich visual content is acquired at a node, compressed and then sent to a\ncentral unit for further processing, according to the Compress-Then-Analyze\n(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of\nrate-efficiency curves in the context of two different visual analysis tasks:\nhomography estimation and content-based retrieval. Our results show that the\nnovel ATC paradigm based on the proposed coding primitives can be competitive\nwith CTA, especially in bandwidth limited scenarios.", 
    "link": "http://arxiv.org/pdf/1502.07939v1", 
    "arxiv-id": "1502.07939v1"
},{
    "category": "cs.CV", 
    "author": "Zhengyou Zhang", 
    "title": "Activity Recognition Using A Combination of Category Components And   Local Models for Video Surveillance", 
    "publish": "2015-02-28T06:49:33Z", 
    "summary": "This paper presents a novel approach for automatic recognition of human\nactivities for video surveillance applications. We propose to represent an\nactivity by a combination of category components, and demonstrate that this\napproach offers flexibility to add new activities to the system and an ability\nto deal with the problem of building models for activities lacking training\ndata. For improving the recognition accuracy, a Confident-Frame- based\nRecognition algorithm is also proposed, where the video frames with high\nconfidence for recognizing an activity are used as a specialized local model to\nhelp classify the remainder of the video frames. Experimental results show the\neffectiveness of the proposed approach.", 
    "link": "http://arxiv.org/pdf/1503.00081v1", 
    "arxiv-id": "1503.00081v1"
},{
    "category": "cs.MM", 
    "author": "Bing Zhou", 
    "title": "Macroblock Classification Method for Video Applications Involving   Motions", 
    "publish": "2015-02-28T07:14:01Z", 
    "summary": "In this paper, a macroblock classification method is proposed for various\nvideo processing applications involving motions. Based on the analysis of the\nMotion Vector field in the compressed video, we propose to classify Macroblocks\nof each video frame into different classes and use this class information to\ndescribe the frame content. We demonstrate that this low-computation-complexity\nmethod can efficiently catch the characteristics of the frame. Based on the\nproposed macroblock classification, we further propose algorithms for different\nvideo processing applications, including shot change detection, motion\ndiscontinuity detection, and outlier rejection for global motion estimation.\nExperimental results demonstrate that the methods based on the proposed\napproach can work effectively on these applications.", 
    "link": "http://arxiv.org/pdf/1503.00087v1", 
    "arxiv-id": "1503.00087v1"
},{
    "category": "cs.GR", 
    "author": "Wenjun Zhang", 
    "title": "Facial Expression Cloning with Elastic and Muscle Models", 
    "publish": "2015-02-28T07:23:08Z", 
    "summary": "Expression cloning plays an important role in facial expression synthesis. In\nthis paper, a novel algorithm is proposed for facial expression cloning. The\nproposed algorithm first introduces a new elastic model to balance the global\nand local warping effects, such that the impacts from facial feature diversity\namong people can be minimized, and thus more effective geometric warping\nresults can be achieved. Furthermore, a muscle-distribution-based (MD) model is\nproposed, which utilizes the muscle distribution of the human face and results\nin more accurate facial illumination details. In addition, we also propose a\nnew distance-based metric to automatically select the optimal parameters such\nthat the global and local warping effects in the elastic model can be suitably\nbalanced. Experimental results show that our proposed algorithm outperforms the\nexisting methods.", 
    "link": "http://arxiv.org/pdf/1503.00088v1", 
    "arxiv-id": "1503.00088v1"
},{
    "category": "cs.MM", 
    "author": "H. R. Chennamma", 
    "title": "A Survey On Video Forgery Detection", 
    "publish": "2015-03-03T07:17:46Z", 
    "summary": "The Digital Forgeries though not visibly identifiable to human perception it\nmay alter or meddle with underlying natural statistics of digital content.\nTampering involves fiddling with video content in order to cause damage or make\nunauthorized alteration/modification. Tampering detection in video is\ncumbersome compared to image when considering the properties of the video.\nTampering impacts need to be studied and the applied technique/method is used\nto establish the factual information for legal course in judiciary. In this\npaper we give an overview of the prior literature and challenges involved in\nvideo forgery detection where passive approach is found.", 
    "link": "http://arxiv.org/pdf/1503.00843v1", 
    "arxiv-id": "1503.00843v1"
},{
    "category": "cs.MM", 
    "author": "Li-Jia Li", 
    "title": "YFCC100M: The New Data in Multimedia Research", 
    "publish": "2015-03-05T23:43:42Z", 
    "summary": "We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),\nthe largest public multimedia collection that has ever been released. The\ndataset contains a total of 100 million media objects, of which approximately\n99.2 million are photos and 0.8 million are videos, all of which carry a\nCreative Commons license. Each media object in the dataset is represented by\nseveral pieces of metadata, e.g. Flickr identifier, owner name, camera, title,\ntags, geo, media source. The collection provides a comprehensive snapshot of\nhow photos and videos were taken, described, and shared over the years, from\nthe inception of Flickr in 2004 until early 2014. In this article we explain\nthe rationale behind its creation, as well as the implications the dataset has\nfor science, research, engineering, and development. We further present several\nnew challenges in multimedia research that can now be expanded upon with our\ndataset.", 
    "link": "http://arxiv.org/pdf/1503.01817v2", 
    "arxiv-id": "1503.01817v2"
},{
    "category": "cs.NI", 
    "author": "Adam Wolisz", 
    "title": "Low-Delay Adaptive Video Streaming Based on Short-Term TCP Throughput   Prediction", 
    "publish": "2015-03-10T15:40:41Z", 
    "summary": "Recently, HTTP-Based Adaptive Streaming has become the de facto standard for\nvideo streaming over the Internet. It allows the client to adapt media\ncharacteristics to varying network conditions in order to maximize Quality of\nExperience (QoE). In the case of live streaming this task becomes particularly\nchallenging. An important factor than might help improving performance is the\ncapability to correctly predict network throughput dynamics on short to medium\ntimescales. It becomes notably difficult in wireless networks that are often\nsubject to continuous throughput fluctuations.\n  In the present work, we develop an adaptation algorithm for HTTP-Based\nAdaptive Live Streaming that, for each adaptation decision, maximizes a\nQoE-based utility function depending on the probability of playback\ninterruptions, average video quality, and the amount of video quality\nfluctuations. To compute the utility function the algorithm leverages\nthroughput predictions, and dynamically estimated prediction accuracy.\n  We are trying to close the gap created by the lack of studies analyzing TCP\nthroughput on short to medium timescales. We study several time series\nprediction methods and their error distributions. We observe that Simple Moving\nAverage performs best in most cases. We also observe that the relative\nunderestimation error is best represented by a truncated normal distribution,\nwhile the relative overestimation error is best represented by a Lomax\ndistribution. Moreover, underestimations and overestimations exhibit a temporal\ncorrelation that we use to further improve prediction accuracy.\n  We compare the proposed algorithm with a baseline approach that uses a fixed\nmargin between past throughput and selected media bit rate, and an oracle-based\napproach that has perfect knowledge over future throughput for a certain time\nhorizon.", 
    "link": "http://arxiv.org/pdf/1503.02955v3", 
    "arxiv-id": "1503.02955v3"
},{
    "category": "cs.IR", 
    "author": "Brian Regan", 
    "title": "Fusing Text and Image for Event Detection in Twitter", 
    "publish": "2015-03-13T01:11:08Z", 
    "summary": "In this contribution, we develop an accurate and effective event detection\nmethod to detect events from a Twitter stream, which uses visual and textual\ninformation to improve the performance of the mining process. The method\nmonitors a Twitter stream to pick up tweets having texts and images and stores\nthem into a database. This is followed by applying a mining algorithm to detect\nan event. The procedure starts with detecting events based on text only by\nusing the feature of the bag-of-words which is calculated using the term\nfrequency-inverse document frequency (TF-IDF) method. Then it detects the event\nbased on image only by using visual features including histogram of oriented\ngradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color\nhistogram. K nearest neighbours (Knn) classification is used in the detection.\nThe final decision of the event detection is made based on the reliabilities of\ntext only detection and image only detection. The experiment result showed that\nthe proposed method achieved high accuracy of 0.94, comparing with 0.89 with\ntexts only, and 0.86 with images only.", 
    "link": "http://arxiv.org/pdf/1503.03920v1", 
    "arxiv-id": "1503.03920v1"
},{
    "category": "cs.MM", 
    "author": "Jennifer Won", 
    "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans", 
    "publish": "2015-03-13T23:36:42Z", 
    "summary": "The YLI Multimedia Event Detection corpus is a public-domain index of videos\nwith annotations and computed features, specialized for research in multimedia\nevent detection (MED), i.e., automatically identifying what's happening in a\nvideo by analyzing the audio and visual content. The videos indexed in the\nYLI-MED corpus are a subset of the larger YLI feature corpus, which is being\ndeveloped by the International Computer Science Institute and Lawrence\nLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100\nMillion (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting\none of ten target events, or no target event, and are annotated for additional\nattributes like language spoken and whether the video has a musical score. The\nannotations also include degree of annotator agreement and average annotator\nconfidence scores for the event categorization of each video. Version 1.0 of\nYLI-MED includes 1823 \"positive\" videos that depict the target events and\n48,138 \"negative\" videos, as well as 177 supplementary videos that are similar\nto event videos but are not positive examples. Our goal in producing YLI-MED is\nto be as open about our data and procedures as possible. This report describes\nthe procedures used to collect the corpus; gives detailed descriptive\nstatistics about the corpus makeup (and how video attributes affected\nannotators' judgments); discusses possible biases in the corpus introduced by\nour procedural choices and compares it with the most similar existing dataset,\nTRECVID MED's HAVIC corpus; and gives an overview of our future plans for\nexpanding the annotation effort.", 
    "link": "http://arxiv.org/pdf/1503.04250v1", 
    "arxiv-id": "1503.04250v1"
},{
    "category": "cs.CR", 
    "author": "Christophe Rosenberger", 
    "title": "Image Watermaking With Biometric Data For Copyright Protection", 
    "publish": "2015-03-16T12:31:45Z", 
    "summary": "In this paper, we deal with the proof of ownership or legitimate usage of a\ndigital content, such as an image, in order to tackle the illegitimate copy.\nThe proposed scheme based on the combination of the watermark-ing and\ncancelable biometrics does not require a trusted third party, all the exchanges\nare between the provider and the customer. The use of cancelable biometrics\npermits to provide a privacy compliant proof of identity. We illustrate the\nrobustness of this method against intentional and unintentional attacks of the\nwatermarked content.", 
    "link": "http://arxiv.org/pdf/1503.04624v1", 
    "arxiv-id": "1503.04624v1"
},{
    "category": "cs.CV", 
    "author": "Marco Tagliasacchi", 
    "title": "Fast keypoint detection in video sequences", 
    "publish": "2015-03-24T09:28:28Z", 
    "summary": "A number of computer vision tasks exploit a succinct representation of the\nvisual content in the form of sets of local features. Given an input image,\nfeature extraction algorithms identify a set of keypoints and assign to each of\nthem a description vector, based on the characteristics of the visual content\nsurrounding the interest point. Several tasks might require local features to\nbe extracted from a video sequence, on a frame-by-frame basis. Although\ntemporal downsampling has been proven to be an effective solution for mobile\naugmented reality and visual search, high temporal resolution is a key\nrequirement for time-critical applications such as object tracking, event\nrecognition, pedestrian detection, surveillance. In recent years, more and more\ncomputationally efficient visual feature detectors and decriptors have been\nproposed. Nonetheless, such approaches are tailored to still images. In this\npaper we propose a fast keypoint detection algorithm for video sequences, that\nexploits the temporal coherence of the sequence of keypoints. According to the\nproposed method, each frame is preprocessed so as to identify the parts of the\ninput frame for which keypoint detection and description need to be performed.\nOur experiments show that it is possible to achieve a reduction in\ncomputational time of up to 40%, without significantly affecting the task\naccuracy.", 
    "link": "http://arxiv.org/pdf/1503.06959v1", 
    "arxiv-id": "1503.06959v1"
},{
    "category": "cs.MM", 
    "author": "R. M. Campello de Souza", 
    "title": "A Low-throughput Wavelet-based Steganography Audio Scheme", 
    "publish": "2015-02-05T03:15:25Z", 
    "summary": "This paper presents the preliminary of a novel scheme of steganography, and\nintroduces the idea of combining two secret keys in the operation. The first\nsecret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,\nSAFER+, etc.) prior to the wavelet audio decomposition. The way in which the\ncipher text is embedded in the file requires another key, namely a stego-key,\nwhich is associated with features of the audio wavelet analysis.", 
    "link": "http://arxiv.org/pdf/1503.07551v1", 
    "arxiv-id": "1503.07551v1"
},{
    "category": "cs.IR", 
    "author": "Phillip Popp", 
    "title": "Large Scale Discovery of Seasonal Music From User Data", 
    "publish": "2015-05-04T03:38:04Z", 
    "summary": "The consumption history of online media content such as music and video\noffers a rich source of data from which to mine information. Trends in this\ndata are of particular interest because they reflect user preferences as well\nas associated cultural contexts that can be exploited in systems such as\nrecommendation or search. This paper classifies songs as seasonal using a\nlarge, real-world dataset of user listening data. Results show strong\nperformance of classification of Christmas music with Gaussian Mixture Models.", 
    "link": "http://arxiv.org/pdf/1505.00519v1", 
    "arxiv-id": "1505.00519v1"
},{
    "category": "cs.CV", 
    "author": "Yong Rui", 
    "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", 
    "publish": "2015-05-07T20:13:33Z", 
    "summary": "Automatically describing video content with natural language is a fundamental\nchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequence\ndynamics, has attracted increasing attention on visual interpretation. However,\nmost existing approaches generate a word locally with given previous words and\nthe visual content, while the relationship between sentence semantics and\nvisual content is not holistically exploited. As a result, the generated\nsentences may be contextually correct but the semantics (e.g., subjects, verbs\nor objects) are not true.\n  This paper presents a novel unified framework, named Long Short-Term Memory\nwith visual-semantic Embedding (LSTM-E), which can simultaneously explore the\nlearning of LSTM and visual-semantic embedding. The former aims to locally\nmaximize the probability of generating the next word given previous words and\nvisual content, while the latter is to create a visual-semantic embedding space\nfor enforcing the relationship between the semantics of the entire sentence and\nvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or\n3-D deep convolutional neural networks for learning powerful video\nrepresentation, a deep RNN for generating sentences, and a joint embedding\nmodel for exploring the relationships between visual content and sentence\nsemantics. The experiments on YouTube2Text dataset show that our proposed\nLSTM-E achieves to-date the best reported performance in generating natural\nsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also\ndemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)\ntriplets to several state-of-the-art techniques.", 
    "link": "http://arxiv.org/pdf/1505.01861v3", 
    "arxiv-id": "1505.01861v3"
},{
    "category": "cs.NI", 
    "author": "Wei Tsang Ooi", 
    "title": "Wireless Multicast for Zoomable Video Streaming", 
    "publish": "2015-05-08T06:03:15Z", 
    "summary": "Zoomable video streaming refers to a new class of interactive video\napplications, where users can zoom into a video stream to view a selected\nregion of interest in higher resolutions and pan around to move the region of\ninterest. The zoom and pan effects are typically achieved by breaking the\nsource video into a grid of independently decodable tiles. Streaming the tiles\nto a set of heterogeneous users using broadcast is challenging, as users have\ndifferent link rates and different regions of interest at different resolution\nlevels. In this paper, we consider the following problem: given the subset of\ntiles that each user requested, the link rate of each user, and the available\ntime slots, at which resolution should each tile be sent, to maximize the\noverall video quality received by all users. We design an efficient algorithm\nto solve the problem above, and evaluate the solution on a testbed using 10\nmobile devices. Our method is able to achieve up to 12dB improvements over\nother heuristic methods.", 
    "link": "http://arxiv.org/pdf/1505.01933v1", 
    "arxiv-id": "1505.01933v1"
},{
    "category": "cs.DC", 
    "author": "Moreno Marzolla", 
    "title": "Cloud for Gaming", 
    "publish": "2015-05-10T21:04:48Z", 
    "summary": "Cloud for Gaming refers to the use of cloud computing technologies to build\nlarge-scale gaming infrastructures, with the goal of improving scalability and\nresponsiveness, improve the user's experience and enable new business models.", 
    "link": "http://arxiv.org/pdf/1505.02435v2", 
    "arxiv-id": "1505.02435v2"
},{
    "category": "cs.CV", 
    "author": "Li Fudi", 
    "title": "Image aesthetic evaluation using paralleled deep convolution neural   network", 
    "publish": "2015-05-20T02:03:23Z", 
    "summary": "Image aesthetic evaluation has attracted much attention in recent years.\nImage aesthetic evaluation methods heavily depend on the effective aesthetic\nfeature. Traditional meth-ods always extract hand-crafted features. However,\nthese hand-crafted features are always designed to adapt particu-lar datasets,\nand extraction of them needs special design. Rather than extracting\nhand-crafted features, an automati-cally learn of aesthetic features based on\ndeep convolutional neural network (DCNN) is first adopt in this paper. As we\nall know, when the training dataset is given, the DCNN architecture with high\ncomplexity may meet the over-fitting problem. On the other side, the DCNN\narchitecture with low complexity would not efficiently extract effective\nfeatures. For these reasons, we further propose a paralleled convolutional\nneural network (PDCNN) with multi-level structures to automatically adapt to\nthe training dataset. Experimental results show that our proposed PDCNN\narchitecture achieves better performance than other traditional methods.", 
    "link": "http://arxiv.org/pdf/1505.05225v1", 
    "arxiv-id": "1505.05225v1"
},{
    "category": "cs.HC", 
    "author": "Davor Jug", 
    "title": "GWAT: The Geneva Affective Picture Database WordNet Annotation Tool", 
    "publish": "2015-05-27T16:27:23Z", 
    "summary": "The Geneva Affective Picture Database WordNet Annotation Tool (GWAT) is a\nuser-friendly web application for manual annotation of pictures in Geneva\nAffective Picture Database (GAPED) with WordNet. The annotation tool has an\nintuitive interface which can be efficiently used with very little technical\ntraining. A single picture may be labeled with many synsets allowing experts to\ndescribe semantics with different levels of detail. Noun, verb, adjective and\nadverb synsets can be keyword-searched and attached to a specific GAPED picture\nwith their unique identification numbers. Changes are saved automatically in\nthe tool's relational database. The attached synsets can be reviewed, changed\nor deleted later. Additionally, GAPED pictures may be browsed in the tool's\nuser interface using simple commands where previously attached WordNet synsets\nare displayed alongside the pictures. Stored annotations can be exported from\nthe tool's database to different data formats and used in 3rd party\napplications if needed. Since GAPED does not define keywords of individual\npictures but only a general category of picture groups, GWAT represents a\nsignificant improvement towards development of comprehensive picture semantics.\nThe tool was developed with open technologies WordNet API, Apache, PHP5 and\nMySQL. It is freely available for scientific and non-commercial use.", 
    "link": "http://arxiv.org/pdf/1505.07395v1", 
    "arxiv-id": "1505.07395v1"
},{
    "category": "cs.HC", 
    "author": "Dragutin Ivanec", 
    "title": "Comparing affective responses to standardized pictures and videos: A   study report", 
    "publish": "2015-05-27T16:39:05Z", 
    "summary": "Multimedia documents such as text, images, sounds or videos elicit emotional\nresponses of different polarity and intensity in exposed human subjects. These\nstimuli are stored in affective multimedia databases. The problem of emotion\nprocessing is an important issue in Human-Computer Interaction and different\ninterdisciplinary studies particularly those related to psychology and\nneuroscience. Accurate prediction of users' attention and emotion has many\npractical applications such as the development of affective computer\ninterfaces, multifaceted search engines, video-on-demand, Internet\ncommunication and video games. To this regard we present results of a study\nwith N=10 participants to investigate the capability of standardized affective\nmultimedia databases in stimulation of emotion. Each participant was exposed to\npicture and video stimuli with previously determined semantics and emotion.\nDuring exposure participants' physiological signals were recorded and estimated\nfor emotion in an off-line analysis. Participants reported their emotion states\nafter each exposure session. The a posteriori and a priori emotion values were\ncompared. The experiment showed, among other reported results, that carefully\ndesigned video sequences induce a stronger and more accurate emotional reaction\nthan pictures. Individual participants' differences greatly influence the\nintensity and polarity of experienced emotion.", 
    "link": "http://arxiv.org/pdf/1505.07398v1", 
    "arxiv-id": "1505.07398v1"
},{
    "category": "cs.MM", 
    "author": "J\u00f6rg Keller", 
    "title": "Micro protocol engineering for unstructured carriers: On the embedding   of steganographic control protocols into audio transmissions", 
    "publish": "2015-05-28T16:54:13Z", 
    "summary": "Network steganography conceals the transfer of sensitive information within\nunobtrusive data in computer networks. So-called micro protocols are\ncommunication protocols placed within the payload of a network steganographic\ntransfer. They enrich this transfer with features such as reliability, dynamic\noverlay routing, or performance optimization --- just to mention a few. We\npresent different design approaches for the embedding of hidden channels with\nmicro protocols in digitized audio signals under consideration of different\nrequirements. On the basis of experimental results, our design approaches are\ncompared, and introduced into a protocol engineering approach for micro\nprotocols.", 
    "link": "http://arxiv.org/pdf/1505.07757v1", 
    "arxiv-id": "1505.07757v1"
},{
    "category": "math.NA", 
    "author": "A. Latif", 
    "title": "Digital image watermarking using normal matrices", 
    "publish": "2015-05-23T12:53:38Z", 
    "summary": "This paper presents techniques for digital image watermarking based on\neigenvalue decomposition of normal matrices. The introduced methods are\nconvenient and self-explanatory, achieve satisfactory results, as well as\nrequire less and easy computations compared to some current methods. Through\nthe proposed methods, host images and watermarks are transformed to the space\nof normal matrices, and the properties of spectral decompositions are dealt\nwith to obtain watermarked images. Watermark extraction is carried out via a\nprocedure similar to embedding. Experimental results are provided to illustrate\nthe reliability and robustness of the methods.", 
    "link": "http://arxiv.org/pdf/1506.01952v1", 
    "arxiv-id": "1506.01952v1"
},{
    "category": "cs.MM", 
    "author": "Kris Gaj", 
    "title": "Using Facebook for Image Steganography", 
    "publish": "2015-06-05T21:16:22Z", 
    "summary": "Because Facebook is available on hundreds of millions of desktop and mobile\ncomputing platforms around the world and because it is available on many\ndifferent kinds of platforms (from desktops and laptops running Windows, Unix,\nor OS X to hand held devices running iOS, Android, or Windows Phone), it would\nseem to be the perfect place to conduct steganography. On Facebook, information\nhidden in image files will be further obscured within the millions of pictures\nand other images posted and transmitted daily. Facebook is known to alter and\ncompress uploaded images so they use minimum space and bandwidth when displayed\non Facebook pages. The compression process generally disrupts attempts to use\nFacebook for image steganography. This paper explores a method to minimize the\ndisruption so JPEG images can be used as steganography carriers on Facebook.", 
    "link": "http://arxiv.org/pdf/1506.02071v1", 
    "arxiv-id": "1506.02071v1"
},{
    "category": "cs.MM", 
    "author": "Krzysztof Szczypiorski", 
    "title": "StegBlocks: ensuring perfect undetectability of network steganography", 
    "publish": "2015-06-07T20:46:27Z", 
    "summary": "The paper presents StegBlocks, which defines a new concept for performing\nundetectable hidden communication. StegBlocks is a general approach for\nconstructing methods of network steganography. In StegBlocks, one has to\ndetermine objects with defined properties which will be used to transfer hidden\nmessages. The objects are dependent on a specific network protocol (or\napplication) used as a carrier for a given network steganography method.\nMoreover, the paper presents the approach to perfect undetectability of network\nsteganography, which was developed based on the rules of undetectability for\ngeneral steganography. The approach to undetectability of network steganography\nwas used to show the possibility of developing perfectly undetectable network\nsteganography methods using the StegBlocks concept.", 
    "link": "http://arxiv.org/pdf/1506.02311v1", 
    "arxiv-id": "1506.02311v1"
},{
    "category": "cs.MM", 
    "author": "Fatema Akhter", 
    "title": "A Novel Approach for Image Steganography in Spatial Domain", 
    "publish": "2015-06-11T14:05:45Z", 
    "summary": "This paper presents a new approach for hiding information in digital image in\nspatial domain. In this approach three bits of message is embedded in a pixel\nusing Lucas number system but only one bit plane is allowed for alternation.\nThe experimental results show that the proposed method has the larger capacity\nof embedding data, high peak signal to noise ratio compared to existing methods\nand is hardly detectable for steganolysis algorithm.", 
    "link": "http://arxiv.org/pdf/1506.03681v1", 
    "arxiv-id": "1506.03681v1"
},{
    "category": "cs.RO", 
    "author": "Davide Scaramuzza", 
    "title": "LightPanel: Active Mobile Platform for Dense 3D Modelling", 
    "publish": "2015-06-16T10:30:34Z", 
    "summary": "In this paper we introduce a novel platform for dense 3D modelling. This\nplatform is an active image acquisition setup assisted with a set of light\nsources and a distance sensor. The hardware setup is designed for being mounted\non a mobile robot which is remotely driven to create accurate dense 3D models\nfrom out-of-reach objects. For this reason, the object is actively illuminated\nby the imaging setup and Photometric Stereo is used to recover the dense 3D\nmodel. The proposed image acquisition setup, called LightPanel, is described\nfrom design to calibration and discusses the practical challenges of using\nPhotometric Stereo under uncontrolled lighting conditions.", 
    "link": "http://arxiv.org/pdf/1506.04904v1", 
    "arxiv-id": "1506.04904v1"
},{
    "category": "cs.MM", 
    "author": "Guochu Shou", 
    "title": "A QoS Guarantee Strategy for Multimedia Conferencing based on Bayesian   Networks", 
    "publish": "2015-06-21T02:18:34Z", 
    "summary": "Service Oriented Architecture (SOA) is commonly employed in the design and\nimplementation of web service systems. The key technology to enable media\ncommunications in the context of SOA is the Service Oriented Communication. To\nexploit the advantage of SOA, we design and implement a web-based multimedia\nconferencing system that provides users with a hybrid orchestration of web and\ncommunication services. As the current SOA lacks effective QoS guarantee\nsolutions for multimedia services, the user satisfaction is greatly challenged\nwith QoS violations, e.g., low video PSNR (Peak Signal-to-Noise Ratio) and long\nplayback delay. Motivated by addressing the critical problem, we firstly employ\nthe Business Process Execution Language (BPEL) service engine for the hybrid\nservices orchestration and execution. Secondly, we propose a novel\ncontext-aware approach to quantify and leverage the causal relationships\nbetween QoS metrics and available contexts based on Bayesian networks (CABIN).\nThis approach includes three phases: (1) information discretization, (2) causal\nrelationship profiling, and (3) optimal context tuning. We implement CABIN in a\nreal-life multimedia conferencing system and compare its performance with\nexisting delay and throughput oriented schemes. Experimental results show that\nCABIN outperforms the competing approaches in improving the video quality in\nterms of PSNR. It also provides a one-stop shop controls both the web and\ncommunication services.", 
    "link": "http://arxiv.org/pdf/1506.06312v1", 
    "arxiv-id": "1506.06312v1"
},{
    "category": "cs.MM", 
    "author": "Zhi Wang", 
    "title": "Data-driven Approaches for Social Video Distribution", 
    "publish": "2015-06-26T15:46:24Z", 
    "summary": "The Internet has recently witnessed the convergence of online social network\nservices and online video services: users import videos from content sharing\nsites, and propagate them along the social connections by re-sharing them. Such\nsocial behaviors have dramatically reshaped how videos are disseminated, and\nthe users are now actively engaged to be part of the social ecosystem, rather\nthan being passively consumers. Despite the increasingly abundant bandwidth and\ncomputation resources, the ever increasing data volume of user generated video\ncontent and the boundless coverage of socialized sharing have presented\nunprecedented challenges. In this paper, we first presents the challenges in\nsocial-aware video delivery. Then, we present a principal framework for\ndata-driven social video delivery approaches. Moreover, we identify the unique\ncharacteristics of social-aware video access and the social content\npropagation, and closely reveal the design of individual modules and their\nintegration towards enhancing users' experience in the social network context.", 
    "link": "http://arxiv.org/pdf/1506.08125v1", 
    "arxiv-id": "1506.08125v1"
},{
    "category": "cs.NI", 
    "author": "Roch Glitho", 
    "title": "Social Network Analysis Inspired Content Placement with QoS in   Cloud-based Content Delivery Networks", 
    "publish": "2015-06-28T03:25:27Z", 
    "summary": "Content Placement (CP) problem in Cloud-based Content Delivery Networks\n(CCDNs) leverage resource elasticity to build cost effective CDNs that\nguarantee QoS. In this paper, we present our novel CP model, which optimally\nplaces content on surrogates in the cloud, to achieve (a) minimum cost of\nleasing storage and bandwidth resources for data coming into and going out of\nthe cloud zones and regions, (b) guarantee Service Level Agreement (SLA), and\n(c) minimize degree of QoS violations. The CP problem is NP-Hard, hence we\ndesign a unique push-based heuristic, called Weighted Social Network Analysis\n(W-SNA) for CCDN providers. W-SNA is based on Betweeness Centrality (BC) from\nSNA and prioritizes surrogates based on their relationship to the other\nvertices in the network graph. To achieve our unique objectives, we further\nprioritize surrogates based on weights derived from storage cost and content\nrequests. We compare our heuristic to current state of the art Greedy Site (GS)\nand purely Social Network Analysis (SNA) heuristics, which are relevant to our\nwork. We show that W-SNA outperforms GS and SNA in minimizing cost and QoS.\nMoreover, W-SNA guarantees SLA but also minimizes the degree of QoS violations.\nTo the best of our knowledge, this is the first model and heuristic of its\nkind, which is timely and gives a fundamental pre-allocation scheme for future\nonline and dynamic resource provision for CCDNs.", 
    "link": "http://arxiv.org/pdf/1506.08348v3", 
    "arxiv-id": "1506.08348v3"
},{
    "category": "cs.MM", 
    "author": "A. Latif", 
    "title": "A new approach for image compression using normal matrices", 
    "publish": "2015-05-23T21:43:59Z", 
    "summary": "In this paper, we present methods for image compression on the basis of\neigenvalue decomposition of normal matrices. The proposed methods are\nconvenient and self-explanatory, requiring fewer and easier computations as\ncompared to some existing methods. Through the proposed techniques, the image\nis transformed to the space of normal matrices. Then, the properties of\nspectral decomposition are dealt with to obtain compressed images. Experimental\nresults are provided to illustrate the validity of the methods.", 
    "link": "http://arxiv.org/pdf/1506.08811v1", 
    "arxiv-id": "1506.08811v1"
},{
    "category": "cs.CV", 
    "author": "Prithwijit Guha", 
    "title": "TV News Commercials Detection using Success based Locally Weighted   Kernel Combination", 
    "publish": "2015-07-05T12:01:34Z", 
    "summary": "Commercial detection in news broadcast videos involves judicious selection of\nmeaningful audio-visual feature combinations and efficient classifiers. And,\nthis problem becomes much simpler if these combinations can be learned from the\ndata. To this end, we propose an Multiple Kernel Learning based method for\nboosting successful kernel functions while ignoring the irrelevant ones. We\nadopt a intermediate fusion approach where, a SVM is trained with a weighted\nlinear combination of different kernel functions instead of single kernel\nfunction. Each kernel function is characterized by a feature set and kernel\ntype. We identify the feature sub-space locations of the prediction success of\na particular classifier trained only with particular kernel function. We\npropose to estimate a weighing function using support vector regression (with\nRBF kernel) for each kernel function which has high values (near 1.0) where the\nclassifier learned on kernel function succeeded and lower values (nearly 0.0)\notherwise. Second contribution of this work is TV News Commercials Dataset of\n150 Hours of News videos. Classifier trained with our proposed scheme has\noutperformed the baseline methods on 6 of 8 benchmark dataset and our own TV\ncommercials dataset.", 
    "link": "http://arxiv.org/pdf/1507.01209v1", 
    "arxiv-id": "1507.01209v1"
},{
    "category": "cs.MM", 
    "author": "Yan Gao", 
    "title": "Joint Data Scheduling and FEC Coding for Multihomed Wireless Video   Delivery", 
    "publish": "2015-07-18T11:45:44Z", 
    "summary": "This paper studies the problem of mobile video delivery in heterogenous\nwireless networks from a server to multihomed device. Most existing works only\nconsider delivering video streaming on single path which bandwidth is limited\ncausing ultimate video transmission rate. To solve this live video streaming\ntransmission bottleneck problem, we propose a novel solution named Joint Data\nAllocation and Fountain Coding (JDAFC) method that contain below characters:\n(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We\nevaluate the performance of JDAFC by simulation experiments using Exata and\nJVSM and compare it with some reference solutions. Experimental results\nrepresent that JDAFC outperforms the competing solutions in improving the video\npeak signal-to-noise ratio as well as reducing the end-to-end delay.", 
    "link": "http://arxiv.org/pdf/1507.05174v1", 
    "arxiv-id": "1507.05174v1"
},{
    "category": "cs.MM", 
    "author": "Muhammet Ba\u015ftan", 
    "title": "Mobile Multi-View Object Image Search", 
    "publish": "2015-07-31T13:02:23Z", 
    "summary": "High user interaction capability of mobile devices can help improve the\naccuracy of mobile visual search systems. At query time, it is possible to\ncapture multiple views of an object from different viewing angles and at\ndifferent scales with the mobile device camera to obtain richer information\nabout the object compared to a single view and hence return more accurate\nresults. Motivated by this, we developed a mobile multi-view object image\nsearch system, using a client-server architecture. Multi-view images of objects\nacquired by the mobile clients are processed and local features are sent to the\nserver, which combines the query image representations with early/late fusion\nmethods based on bag-of-visual-words and sends back the query results. We\nperformed a comprehensive analysis of early and late fusion approaches using\nvarious similarity functions, on an existing single view and a new multi-view\nobject image database. The experimental results show that multi-view search\nprovides significantly better retrieval accuracy compared to single view\nsearch.", 
    "link": "http://arxiv.org/pdf/1507.08861v1", 
    "arxiv-id": "1507.08861v1"
},{
    "category": "cs.MM", 
    "author": "Marco Tagliasacchi", 
    "title": "Estimating snow cover from publicly available images", 
    "publish": "2015-08-05T12:46:26Z", 
    "summary": "In this paper we study the problem of estimating snow cover in mountainous\nregions, that is, the spatial extent of the earth surface covered by snow. We\nargue that publicly available visual content, in the form of user generated\nphotographs and image feeds from outdoor webcams, can both be leveraged as\nadditional measurement sources, complementing existing ground, satellite and\nairborne sensor data. To this end, we describe two content acquisition and\nprocessing pipelines that are tailored to such sources, addressing the specific\nchallenges posed by each of them, e.g., identifying the mountain peaks,\nfiltering out images taken in bad weather conditions, handling varying\nillumination conditions. The final outcome is summarized in a snow cover index,\nwhich indicates for a specific mountain and day of the year, the fraction of\nvisible area covered by snow, possibly at different elevations. We created a\nmanually labelled dataset to assess the accuracy of the image snow covered area\nestimation, achieving 90.0% precision at 91.1% recall. In addition, we show\nthat seasonal trends related to air temperature are captured by the snow cover\nindex.", 
    "link": "http://arxiv.org/pdf/1508.01055v1", 
    "arxiv-id": "1508.01055v1"
},{
    "category": "cs.CV", 
    "author": "Roman Fedorov", 
    "title": "Mountain Peak Detection in Online Social Media", 
    "publish": "2015-08-12T15:43:16Z", 
    "summary": "We present a system for the classification of mountain panoramas from\nuser-generated photographs followed by identification and extraction of\nmountain peaks from those panoramas. We have developed an automatic technique\nthat, given as input a geo-tagged photograph, estimates its FOV (Field Of View)\nand the direction of the camera using a matching algorithm on the photograph\nedge maps and a rendered view of the mountain silhouettes that should be seen\nfrom the observer's point of view. The extraction algorithm then identifies the\nmountain peaks present in the photograph and their profiles. We discuss\npossible applications in social fields such as photograph peak tagging on\nsocial portals, augmented reality on mobile devices when viewing a mountain\npanorama, and generation of collective intelligence systems (such as\nenvironmental models) from massive social media collections (e.g. snow water\navailability maps based on mountain peak states extracted from photograph\nhosting services).", 
    "link": "http://arxiv.org/pdf/1508.02959v1", 
    "arxiv-id": "1508.02959v1"
},{
    "category": "cs.GR", 
    "author": "Biola Oyediran", 
    "title": "3D-Computer Animation for a Yoruba Native Folktale", 
    "publish": "2015-08-16T15:58:11Z", 
    "summary": "Computer graphics has wide range of applications which are implemented into\ncomputer animation, computer modeling among others. Since the invention of\ncomputer graphics researchers have not paid much of attentions toward the\npossibility of converting oral tales otherwise known as folktales into possible\ncartoon animated videos. This paper is based on how to develop cartoons of\nlocal folktales that will be of huge benefits to Nigerians. The activities were\ndivided into 5 stages; analysis, design, development, implementation and\nevaluation which involved various processes and use of various specialized\nsoftware and hardware. After the implementation of this project, the video\ncharacteristics were evaluated using likert scale. Analysis of 30 user\nresponses indicated that 17 users (56.7 percent) rated the image quality as\nexcellent, the video and image synchronization was rated as excellent by 9\nusers (30 percent), the Background noise was rated excellent by 18 users (60\npercent), the Character Impression was rated Excellent by 11 users (36.67\npercent), the general assessment of the storyline was rated excellent by 17\nusers (56.7 percent), the video Impression was rated excellent by 11 users\n(36.67 percent) and the voice quality was rated by 10 users (33.33 percent) as\nexcellent.", 
    "link": "http://arxiv.org/pdf/1508.03840v1", 
    "arxiv-id": "1508.03840v1"
},{
    "category": "cs.MM", 
    "author": "Steffen Wendzel", 
    "title": "\"The Good, The Bad And The Ugly\": Evaluation of Wi-Fi Steganography", 
    "publish": "2015-08-20T13:35:43Z", 
    "summary": "In this paper we propose a new method for the evaluation of network\nsteganography algorithms based on the new concept of \"the moving observer\". We\nconsidered three levels of undetectability named: \"good\", \"bad\", and \"ugly\". To\nillustrate this method we chose Wi-Fi steganography as a solid family of\ninformation hiding protocols. We present the state of the art in this area\ncovering well-known hiding techniques for 802.11 networks. \"The moving\nobserver\" approach could help not only in the evaluation of steganographic\nalgorithms, but also might be a starting point for a new detection system of\nnetwork steganography. The concept of a new detection system, called MoveSteg,\nis explained in detail.", 
    "link": "http://arxiv.org/pdf/1508.04978v2", 
    "arxiv-id": "1508.04978v2"
},{
    "category": "cs.MM", 
    "author": "Xavier Gir\u00f3-i-Nieto", 
    "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual   Sentiment Prediction", 
    "publish": "2015-08-20T17:36:48Z", 
    "summary": "Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction.", 
    "link": "http://arxiv.org/pdf/1508.05056v2", 
    "arxiv-id": "1508.05056v2"
},{
    "category": "cs.SI", 
    "author": "Jiebo Luo", 
    "title": "Pinterest Board Recommendation for Twitter Users", 
    "publish": "2015-09-01T21:42:27Z", 
    "summary": "Pinboard on Pinterest is an emerging media to engage online social media\nusers, on which users post online images for specific topics. Regardless of its\nsignificance, there is little previous work specifically to facilitate\ninformation discovery based on pinboards. This paper proposes a novel pinboard\nrecommendation system for Twitter users. In order to associate contents from\nthe two social media platforms, we propose to use MultiLabel classification to\nmap Twitter user followees to pinboard topics and visual diversification to\nrecommend pinboards given user interested topics. A preliminary experiment on a\ndataset with 2000 users validated our proposed system.", 
    "link": "http://arxiv.org/pdf/1509.00511v1", 
    "arxiv-id": "1509.00511v1"
},{
    "category": "cs.CR", 
    "author": "M. Mani Roja", 
    "title": "Dual-Layer Video Encryption using RSA Algorithm", 
    "publish": "2015-09-14T06:52:20Z", 
    "summary": "This paper proposes a video encryption algorithm using RSA and Pseudo Noise\n(PN) sequence, aimed at applications requiring sensitive video information\ntransfers. The system is primarily designed to work with files encoded using\nthe Audio Video Interleaved (AVI) codec, although it can be easily ported for\nuse with Moving Picture Experts Group (MPEG) encoded files. The audio and video\ncomponents of the source separately undergo two layers of encryption to ensure\na reasonable level of security. Encryption of the video component involves\napplying the RSA algorithm followed by the PN-based encryption. Similarly, the\naudio component is first encrypted using PN and further subjected to encryption\nusing the Discrete Cosine Transform. Combining these techniques, an efficient\nsystem, invulnerable to security breaches and attacks with favorable values of\nparameters such as encryption/decryption speed, encryption/decryption ratio and\nvisual degradation; has been put forth. For applications requiring encryption\nof sensitive data wherein stringent security requirements are of prime concern,\nthe system is found to yield negligible similarities in visual perception\nbetween the original and the encrypted video sequence. For applications wherein\nvisual similarity is not of major concern, we limit the encryption task to a\nsingle level of encryption which is accomplished by using RSA, thereby\nquickening the encryption process. Although some similarity between the\noriginal and encrypted video is observed in this case, it is not enough to\ncomprehend the happenings in the video.", 
    "link": "http://arxiv.org/pdf/1509.04387v1", 
    "arxiv-id": "1509.04387v1"
},{
    "category": "cs.MM", 
    "author": "Jiebo Luo", 
    "title": "User-Curated Image Collections: Modeling and Recommendation", 
    "publish": "2015-09-18T15:45:41Z", 
    "summary": "Most state-of-the-art image retrieval and recommendation systems\npredominantly focus on individual images. In contrast, socially curated image\ncollections, condensing distinctive yet coherent images into one set, are\nlargely overlooked by the research communities. In this paper, we aim to design\na novel recommendation system that can provide users with image collections\nrelevant to individual personal preferences and interests. To this end, two key\nissues need to be addressed, i.e., image collection modeling and similarity\nmeasurement. For image collection modeling, we consider each image collection\nas a whole in a group sparse reconstruction framework and extract concise\ncollection descriptors given the pretrained dictionaries. We then consider\nimage collection recommendation as a dynamic similarity measurement problem in\nresponse to user's clicked image set, and employ a metric learner to measure\nthe similarity between the image collection and the clicked image set. As there\nis no previous work directly comparable to this study, we implement several\ncompetitive baselines and related methods for comparison. The evaluations on a\nlarge scale Pinterest data set have validated the effectiveness of our proposed\nmethods for modeling and recommending image collections.", 
    "link": "http://arxiv.org/pdf/1509.05671v1", 
    "arxiv-id": "1509.05671v1"
},{
    "category": "cs.CV", 
    "author": "Jun Wang", 
    "title": "Fusing Multi-Stream Deep Networks for Video Classification", 
    "publish": "2015-09-21T00:38:54Z", 
    "summary": "This paper studies deep network architectures to address the problem of video\nclassification. A multi-stream framework is proposed to fully utilize the rich\nmultimodal information in videos. Specifically, we first train three\nConvolutional Neural Networks to model spatial, short-term motion and audio\nclues respectively. Long Short Term Memory networks are then adopted to explore\nlong-term temporal dynamics. With the outputs of the individual streams, we\npropose a simple and effective fusion method to generate the final predictions,\nwhere the optimal fusion weights are learned adaptively for each class, and the\nlearning process is regularized by automatically estimated class relationships.\nOur contributions are two-fold. First, the proposed multi-stream framework is\nable to exploit multimodal features that are more comprehensive than those\npreviously attempted. Second, we demonstrate that the adaptive fusion method\nusing the class relationship as a regularizer outperforms traditional\nalternatives that estimate the weights in a \"free\" fashion. Our framework\nproduces significantly better results than the state of the arts on two popular\nbenchmarks, 92.2\\% on UCF-101 (without using audio) and 84.9\\% on Columbia\nConsumer Videos.", 
    "link": "http://arxiv.org/pdf/1509.06086v2", 
    "arxiv-id": "1509.06086v2"
},{
    "category": "cs.MM", 
    "author": "Sung Wook Baik", 
    "title": "Ontology-based Secure Retrieval of Semantically Significant Visual   Contents", 
    "publish": "2015-10-08T01:09:32Z", 
    "summary": "Image classification is an enthusiastic research field where large amount of\nimage data is classified into various classes based on their visual contents.\nResearchers have presented various low-level features-based techniques for\nclassifying images into different categories. However, efficient and effective\nclassification and retrieval is still a challenging problem due to complex\nnature of visual contents. In addition, the traditional information retrieval\ntechniques are vulnerable to security risks, making it easy for attackers to\nretrieve personal visual contents such as patients records and law enforcement\nagencies databases. Therefore, we propose a novel ontology-based framework\nusing image steganography for secure image classification and information\nretrieval. The proposed framework uses domain-specific ontology for mapping the\nlow-level image features to high-level concepts of ontologies which\nconsequently results in efficient classification. Furthermore, the proposed\nmethod utilizes image steganography for hiding the image semantics as a secret\nmessage inside them, making the information retrieval process secure from third\nparties. The proposed framework minimizes the computational complexity of\ntraditional techniques, increasing its suitability for secure and real-time\nvisual contents retrieval from personalized image databases. Experimental\nresults confirm the efficiency, effectiveness, and security of the proposed\nframework as compared with other state-of-the-art systems.", 
    "link": "http://arxiv.org/pdf/1510.02177v1", 
    "arxiv-id": "1510.02177v1"
},{
    "category": "cs.LO", 
    "author": "G\u00e9rard Assayag", 
    "title": "NTCCRT: A concurrent constraint framework for real-time interaction   (extended version)", 
    "publish": "2015-10-09T21:53:12Z", 
    "summary": "Writing multimedia interaction systems is not easy. Their concurrent\nprocesses usually access shared resources in a non-deterministic order, often\nleading to unpredictable behavior. Using Pure Data (Pd) and Max/MSP is possible\nto program concurrency, however, it is difficult to synchronize processes based\non multiple criteria. Process calculi such as the Non-deterministic Timed\nConcurrent Constraint (ntcc) calculus, overcome that problem by representing\nmultiple criteria as constraints. We propose using our framework Ntccrt to\nmanage concurrency in Pd and Max. Ntccrt is a real-time capable inter- preter\nfor ntcc. Using Ntccrt externals (binary plugins) in Pd we ran models for\nmachine improvisation and signal processing.", 
    "link": "http://arxiv.org/pdf/1510.02834v1", 
    "arxiv-id": "1510.02834v1"
},{
    "category": "cs.CV", 
    "author": "Cees G. M. Snoek", 
    "title": "TagBook: A Semantic Video Representation without Supervision for Event   Detection", 
    "publish": "2015-10-10T09:28:56Z", 
    "summary": "We consider the problem of event detection in video for scenarios where only\nfew, or even zero examples are available for training. For this challenging\nsetting, the prevailing solutions in the literature rely on a semantic video\nrepresentation obtained from thousands of pre-trained concept detectors.\nDifferent from existing work, we propose a new semantic video representation\nthat is based on freely available social tagged videos only, without the need\nfor training any intermediate concept detectors. We introduce a simple\nalgorithm that propagates tags from a video's nearest neighbors, similar in\nspirit to the ones used for image retrieval, but redesign it for video event\ndetection by including video source set refinement and varying the video tag\nassignment. We call our approach TagBook and study its construction,\ndescriptiveness and detection performance on the TRECVID 2013 and 2014\nmultimedia event detection datasets and the Columbia Consumer Video dataset.\nDespite its simple nature, the proposed TagBook video representation is\nremarkably effective for few-example and zero-example event detection, even\noutperforming very recent state-of-the-art alternatives building on supervised\nrepresentations.", 
    "link": "http://arxiv.org/pdf/1510.02899v2", 
    "arxiv-id": "1510.02899v2"
},{
    "category": "cs.NI", 
    "author": "Spyros Denazis", 
    "title": "Congestion Control for P2P Live Streaming", 
    "publish": "2015-10-11T13:26:53Z", 
    "summary": "In recent years, research efforts tried to exploit peer-to-peer (P2P) systems\nin order to provide Live Streaming (LS) and Video-on-Demand (VoD) services.\nMost of these research efforts focus on the development of distributed P2P\nblock schedulers for content exchange among the participating peers and on the\ncharacteristics of the overlay graph (P2P overlay) that interconnects the set\nof these peers. Currently, researchers try to combine peer-to-peer systems with\ncloud infrastructures. They developed monitoring and control architectures that\nuse resources from the cloud in order to enhance QoS and achieve an attractive\ntrade-off between stability and low cost operation. However, there is a lack of\nresearch effort on the congestion control of these systems and the existing\ncongestion control architectures are not suitable for P2P live streaming\ntraffic (small sequential non persistent traffic towards multiple network\nlocations). This paper proposes a P2P live streaming traffic aware congestion\ncontrol protocol that: i) is capable to manage sequential traffic heading to\nmultiple network destinations , ii) efficiently exploits the available\nbandwidth, iii) accurately measures the idle peer resources, iv) avoids network\ncongestion, and v) is friendly to traditional TCP generated traffic. The\nproposed P2P congestion control has been implemented, tested and evaluated\nthrough a series of real experiments powered across the BonFIRE infrastructure.", 
    "link": "http://arxiv.org/pdf/1510.03050v1", 
    "arxiv-id": "1510.03050v1"
},{
    "category": "cs.MM", 
    "author": "Mauricio Toro", 
    "title": "Towards non-threaded Concurrent Constraint Programming for implementing   multimedia interaction systems", 
    "publish": "2015-10-11T14:15:47Z", 
    "summary": "In this work we explain the implementation of event-driven real-time\ninterpreters for the Concurrent Constraint Programming (CCP) and\nNon-deterministic Timed Concurrent Constraint (NTCC) for- malisms. The CCP\ninterpreter was tested with a program to find, concurrently, paths in a graph\nand it will be used in the future to find musical sequences in the music\nimprovisation software Omax, developed by the French Acoustics/Music Research\nInstitute (IRCAM). In the other hand, the NTCC interpreter was tested with a\nmusic improvisation system based on NTCC (CCFOMI), developed by the AVISPA\nresearch group and IRCAM. Additionally, we present GECOL 2, a wrapper for the\nGeneric Constraints Development Environment (GECODE) to Common LISP, de-\nveloped to port the interpreters to Common LISP in the future. We concluded\nthat using GECODE for the concurrency control avoids the need of having threads\nand synchronizing them, leading to a simple and efficient implementation of CCP\nand NTCC. We also noticed that the time units in NTCC interpreter do not\nrepresent discrete time units, because when we simulate the NTCC specifications\nin the interpreter, the time units have different durations. In the future, we\npropose forcing the duration of each time unit to a fix time, that way we would\nbe able to reason about NTCC time units as we do with discrete time units.", 
    "link": "http://arxiv.org/pdf/1510.03057v1", 
    "arxiv-id": "1510.03057v1"
},{
    "category": "cs.MM", 
    "author": "Julien Castet", 
    "title": "An Extension of Interactive Scores for Multimedia Scenarios with   Temporal Relations for Micro and Macro Controls", 
    "publish": "2015-10-11T19:15:56Z", 
    "summary": "Software to design multimedia scenarios is usually based either on a fixed\ntimeline or on cue lists, but both models are unrelated temporally. On the\ncontrary, the formalism of interactive scores can describe multimedia scenarios\nwith flexible and fixed temporal relations among the objects of the scenario,\nbut cannot express neither temporal relations for micro controls nor signal\nprocessing. We extend interactive scores with such relations and with sound\nprocessing. We show some applications and we describe how they can be\nimplemented in Pure Data. Our implementation has low average relative jitter\neven under high cpu load.", 
    "link": "http://arxiv.org/pdf/1510.03090v1", 
    "arxiv-id": "1510.03090v1"
},{
    "category": "cs.NI", 
    "author": "Yeong Min Jang", 
    "title": "Quality-Aware Popularity Based Bandwidth Allocation for Scalable Video   Broadcast over Wireless Access Networks", 
    "publish": "2015-10-14T06:01:53Z", 
    "summary": "Video broadcast/multicast over wireless access networks is an attractive\nresearch issue in the field of wireless communication. With the rapid\nimprovement of various wireless network technologies, it is now possible to\nprovide high quality video transmission over wireless networks. The high\nquality video streams need higher bandwidth. Hence, during the video\ntransmission through wireless networks, it is very important to make the best\nutilization of the limited bandwidth. Therefore, when many broadcasting video\nsessions are active, the bandwidth per video session can be allocated based on\npopularity of the video sessions (programs). Instead of allocating equal\nbandwidth to each of them, our proposed scheme allocates bandwidth per\nbroadcasting video session based on popularity of the video program. When the\nsystem bandwidth is not sufficient to allocate the demanded bandwidth for all\nthe active video sessions, our proposed scheme efficiently allocates the total\nsystem bandwidth among all the scalable active video sessions in such a way\nthat higher bandwidth is allocated to higher popularity one. Using the\nmathematical and simulation analyses, we show that the proposed scheme\nmaximizes the average user satisfaction level and achieves the best utilization\nof bandwidth. The simulation results indicate that a large number of\nsubscribers can receive a significantly improved quality of video. To improve\nthe video quality for large number of subscribers, the only tradeoff is that a\nvery few subscribers receive slightly degraded video quality.", 
    "link": "http://arxiv.org/pdf/1510.03971v1", 
    "arxiv-id": "1510.03971v1"
},{
    "category": "cs.NI", 
    "author": "Yeong Min Jang", 
    "title": "Radio Resource Management Based on Reused Frequency Allocation for   Dynamic Channel Borrowing Scheme in Wireless Networks", 
    "publish": "2015-10-14T06:08:51Z", 
    "summary": "In the modern era, cellular communication consumers are exponentially\nincreasing as they find the system more user-friendly. Due to enormous users\nand their numerous demands, it has become a mandate to make the best use of the\nlimited radio resources that assures the highest standard of Quality of Service\n(QoS). To reach the guaranteed level of QoS for the maximum number of users,\nmaximum utilization of bandwidth is not only the key issue to be considered,\nrather some other factors like interference, call blocking probability etc. are\nalso needed to keep under deliberation. The lower performances of these factors\nmay retrograde the overall cellular networks performances. Keeping these\ndifficulties under consideration, we propose an effective dynamic channel\nborrowing model that safeguards better QoS, other factors as well. The proposed\nscheme reduces the excessive overall call blocking probability and does\ninterference mitigation without sacrificing bandwidth utilization. The proposed\nscheme is modeled in such a way that the cells are bifurcated after the channel\nborrowing process if the borrowed channels have the same type of frequency band\n(i.e. reused frequency). We also propose that the unoccupied interfering\nchannels of adjacent cells can also be inactivated, instead of cell bifurcation\nfor interference mitigation. The simulation endings show satisfactory\nperformances in terms of overall call blocking probability and bandwidth\nutilization that are compared to the conventional scheme without channel\nborrowing. Furthermore, signal to interference plus noise ratio (SINR) level,\ncapacity, and outage probability are compared to the conventional scheme\nwithout interference mitigation after channel borrowing that may attract the\nconsiderable concentration to the operators.", 
    "link": "http://arxiv.org/pdf/1510.03973v1", 
    "arxiv-id": "1510.03973v1"
},{
    "category": "cs.MM", 
    "author": "Muhammad Zubair", 
    "title": "Secure Image Steganography using Cryptography and Image Transposition", 
    "publish": "2015-10-15T06:27:29Z", 
    "summary": "Information security is one of the most challenging problems in today's\ntechnological world. In order to secure the transmission of secret data over\nthe public network (Internet), various schemes have been presented over the\nlast decade. Steganography combined with cryptography, can be one of the best\nchoices for solving this problem. This paper proposes a new steganographic\nmethod based on gray-level modification for true colour images using image\ntransposition, secret key and cryptography. Both the secret key and secret\ninformation are initially encrypted using multiple encryption algorithms\n(bitxor operation, bits shuffling, and stego key-based encryption); these are,\nsubsequently, hidden in the host image pixels. In addition, the input image is\ntransposed before data hiding. Image transposition, bits shuffling, bitxoring,\nstego key-based encryption, and gray-level modification introduce five\ndifferent security levels to the proposed scheme, making the data recovery\nextremely difficult for attackers. The proposed technique is evaluated by\nobjective analysis using various image quality assessment metrics, producing\npromising results in terms of imperceptibility and security. Moreover, the high\nquality stego images and its minimal histogram changeability, also validate the\neffectiveness of the proposed approach.", 
    "link": "http://arxiv.org/pdf/1510.04413v1", 
    "arxiv-id": "1510.04413v1"
},{
    "category": "cs.HC", 
    "author": "Jean-Claude Dufourd", 
    "title": "MSoS: A Multi-Screen-Oriented Web Page Segmentation Approach", 
    "publish": "2015-10-16T09:43:12Z", 
    "summary": "In this paper we describe a multiscreen-oriented approach for segmenting web\npages. The segmentation is an automatic and hybrid visual and structural\nmethod. It aims at creating coherent blocks which have different functions\ndetermined by the multiscreen environment. It is also characterized by a\ndynamic adaptation to the page content. Experiments are conducted on a set of\nexisting applications that contain multimedia elements, in particular YouTube\nand video player pages. Results are compared with one seg-mentation method from\nthe literature and with a ground truth manually created. With a 75% precision,\nthe MSoS is a promising method that is capable of producing good segmentation\nresults.", 
    "link": "http://arxiv.org/pdf/1510.04825v1", 
    "arxiv-id": "1510.04825v1"
},{
    "category": "cs.CV", 
    "author": "Tomislav Hrka\u0107", 
    "title": "Towards Reversible De-Identification in Video Sequences Using 3D Avatars   and Steganography", 
    "publish": "2015-10-16T12:31:29Z", 
    "summary": "We propose a de-identification pipeline that protects the privacy of humans\nin video sequences by replacing them with rendered 3D human models, hence\nconcealing their identity while retaining the naturalness of the scene. The\noriginal images of humans are steganographically encoded in the carrier image,\ni.e. the image containing the original scene and the rendered 3D human models.\nWe qualitatively explore the feasibility of our approach, utilizing the Kinect\nsensor and its libraries to detect and localize human joints. A 3D avatar is\nrendered into the scene using the obtained joint positions, and the original\nhuman image is steganographically encoded in the new scene. Our qualitative\nevaluation shows reasonably good results that merit further exploration.", 
    "link": "http://arxiv.org/pdf/1510.04861v1", 
    "arxiv-id": "1510.04861v1"
},{
    "category": "cs.NI", 
    "author": "Torsten Braun", 
    "title": "Content-Aware Delivery of Scalable Video in Network Coding Enabled Named   Data Networks", 
    "publish": "2015-10-22T15:36:27Z", 
    "summary": "In this paper, we propose a novel network coding enabled NDN architecture for\nthe delivery of scalable video. Our scheme utilizes network coding in order to\naddress the problem that arises in the original NDN protocol, where optimal use\nof the bandwidth and caching resources necessitates the coordination of the\nforwarding decisions. To optimize the performance of the proposed network\ncoding based NDN protocol and render it appropriate for transmission of\nscalable video, we devise a novel rate allocation algorithm that decides on the\noptimal rates of Interest messages sent by clients and intermediate nodes. This\nalgorithm guarantees that the achieved flow of Data objects will maximize the\naverage quality of the video delivered to the client population. To support the\nhandling of Interest messages and Data objects when intermediate nodes perform\nnetwork coding, we modify the standard NDN protocol and introduce the use of\nBloom filters, which store efficiently additional information about the\nInterest messages and Data objects. The proposed architecture is evaluated for\ntransmission of scalable video over PlanetLab topologies. The evaluation shows\nthat the proposed scheme performs very close to the optimal performance.", 
    "link": "http://arxiv.org/pdf/1510.06659v1", 
    "arxiv-id": "1510.06659v1"
},{
    "category": "cs.CV", 
    "author": "Rita Cucchiara", 
    "title": "A Deep Siamese Network for Scene Detection in Broadcast Videos", 
    "publish": "2015-10-29T20:34:15Z", 
    "summary": "We present a model that automatically divides broadcast videos into coherent\nscenes by learning a distance measure between shots. Experiments are performed\nto demonstrate the effectiveness of our approach by comparing our algorithm\nagainst recent proposals for automatic scene segmentation. We also propose an\nimproved performance measure that aims to reduce the gap between numerical\nevaluation and expected results, and propose and release a new benchmark\ndataset.", 
    "link": "http://arxiv.org/pdf/1510.08893v1", 
    "arxiv-id": "1510.08893v1"
},{
    "category": "cs.CV", 
    "author": "Cees G. M. Snoek", 
    "title": "VideoStory Embeddings Recognize Events when Examples are Scarce", 
    "publish": "2015-11-08T14:59:14Z", 
    "summary": "This paper aims for event recognition when video examples are scarce or even\ncompletely absent. The key in such a challenging setting is a semantic video\nrepresentation. Rather than building the representation from individual\nattribute detectors and their annotations, we propose to learn the entire\nrepresentation from freely available web videos and their descriptions using an\nembedding between video features and term vectors. In our proposed embedding,\nwhich we call VideoStory, the correlations between the terms are utilized to\nlearn a more effective representation by optimizing a joint objective balancing\ndescriptiveness and predictability.We show how learning the VideoStory using a\nmultimodal predictability loss, including appearance, motion and audio\nfeatures, results in a better predictable representation. We also propose a\nvariant of VideoStory to recognize an event in video from just the important\nterms in a text query by introducing a term sensitive descriptiveness loss. Our\nexperiments on three challenging collections of web videos from the NIST\nTRECVID Multimedia Event Detection and Columbia Consumer Videos datasets\ndemonstrate: i) the advantages of VideoStory over representations using\nattributes or alternative embeddings, ii) the benefit of fusing video\nmodalities by an embedding over common strategies, iii) the complementarity of\nterm sensitive descriptiveness and multimodal predictability for event\nrecognition without examples. By it abilities to improve predictability upon\nany underlying video feature while at the same time maximizing semantic\ndescriptiveness, VideoStory leads to state-of-the-art accuracy for both few-\nand zero-example recognition of events in video.", 
    "link": "http://arxiv.org/pdf/1511.02492v1", 
    "arxiv-id": "1511.02492v1"
},{
    "category": "cs.MM", 
    "author": "Chang Wen Chen", 
    "title": "Attribute-Based Multi-Dimensional Scalable Access Control For Social   Media Sharing", 
    "publish": "2015-11-11T01:32:47Z", 
    "summary": "Media sharing is an extremely popular paradigm of social interaction in\nonline social networks (OSNs) nowadays. The scalable media access control is\nessential to perform information sharing among users with various access\nprivileges. In this paper, we present a multi-dimensional scalable media access\ncontrol (MD-SMAC) system based on the proposed scalable ciphertext policy\nattribute-based encryption (SCP-ABE) algorithm. In the proposed MD-SMAC system,\nfine-grained access control can be performed on the media contents encoded in a\nmulti-dimensional scalable manner based on data consumers' diverse attributes.\nThrough security analysis, we show that the proposed MC-SMAC system is able to\nresist collusion attacks. Additionally, we conduct experiments to evaluate the\nefficiency performance of the proposed system, especially on mobile devices.", 
    "link": "http://arxiv.org/pdf/1511.03351v1", 
    "arxiv-id": "1511.03351v1"
},{
    "category": "cs.MM", 
    "author": "C. -C. Jay Kuo", 
    "title": "A GMM-Based Stair Quality Model for Human Perceived JPEG Images", 
    "publish": "2015-11-11T06:44:31Z", 
    "summary": "Based on the notion of just noticeable differences (JND), a stair quality\nfunction (SQF) was recently proposed to model human perception on JPEG images.\nFurthermore, a k-means clustering algorithm was adopted to aggregate JND data\ncollected from multiple subjects to generate a single SQF. In this work, we\npropose a new method to derive the SQF using the Gaussian Mixture Model (GMM).\nThe newly derived SQF can be interpreted as a way to characterize the mean\nviewer experience. Furthermore, it has a lower information criterion (BIC)\nvalue than the previous one, indicating that it offers a better model. A\nspecific example is given to demonstrate the advantages of the new approach.", 
    "link": "http://arxiv.org/pdf/1511.03398v1", 
    "arxiv-id": "1511.03398v1"
},{
    "category": "cs.MM", 
    "author": "Mark Sandler", 
    "title": "Understanding Music Playlists", 
    "publish": "2015-11-22T12:33:08Z", 
    "summary": "As music streaming services dominate the music industry, the playlist is\nbecoming an increasingly crucial element of music consumption. Con- sequently,\nthe music recommendation problem is often casted as a playlist generation prob-\nlem. Better understanding of the playlist is there- fore necessary for\ndeveloping better playlist gen- eration algorithms. In this work, we analyse\ntwo playlist datasets to investigate some com- monly assumed hypotheses about\nplaylists. Our findings indicate that deeper understanding of playlists is\nneeded to provide better prior infor- mation and improve machine learning\nalgorithms in the design of recommendation systems.", 
    "link": "http://arxiv.org/pdf/1511.07004v1", 
    "arxiv-id": "1511.07004v1"
},{
    "category": "cs.MM", 
    "author": "St\u00e9fane Paris", 
    "title": "A proposal project for a blind image quality assessment by learning   distortions from the full reference image quality assessments", 
    "publish": "2015-11-04T12:21:04Z", 
    "summary": "This short paper presents a perspective plan to build a null reference image\nquality assessment. Its main goal is to deliver both the objective score and\nthe distortion map for a given distorted image without the knowledge of its\nreference image.", 
    "link": "http://arxiv.org/pdf/1512.04354v1", 
    "arxiv-id": "1512.04354v1"
},{
    "category": "cs.CV", 
    "author": "Adrian Popescu", 
    "title": "On Deep Representation Learning from Noisy Web Images", 
    "publish": "2015-12-15T13:57:39Z", 
    "summary": "The keep-growing content of Web images may be the next important data source\nto scale up deep neural networks, which recently obtained a great success in\nthe ImageNet classification challenge and related tasks. This prospect,\nhowever, has not been validated on convolutional networks (convnet) -- one of\nbest performing deep models -- because of their supervised regime. While\nunsupervised alternatives are not so good as convnet in generalizing the\nlearned model to new domains, we use convnet to leverage semi-supervised\nrepresentation learning. Our approach is to use massive amounts of unlabeled\nand noisy Web images to train convnets as general feature detectors despite\nchallenges coming from data such as high level of mislabeled data, outliers,\nand data biases. Extensive experiments are conducted at several data scales,\ndifferent network architectures, and data reranking techniques. The learned\nrepresentations are evaluated on nine public datasets of various topics. The\nbest results obtained by our convnets, trained on 3.14 million Web images,\noutperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is\nclosing the gap with VGG-16. These prominent results suggest a budget solution\nto use deep learning in practice and motivate more research in semi-supervised\nrepresentation learning.", 
    "link": "http://arxiv.org/pdf/1512.04785v2", 
    "arxiv-id": "1512.04785v2"
},{
    "category": "cs.MM", 
    "author": "Majed Haddad", 
    "title": "NEWCAST: Anticipating Resource Management and QoE Provisioning for   Mobile Video Streaming", 
    "publish": "2015-12-17T18:16:25Z", 
    "summary": "The knowledge of future throughput variation in wireless networks using\nsmartphone becomes more and more possible by exploiting the rich contextual\ninformation from smartphone sensors through mobile applications and services.\nContextual information may include the traffic, mobility and radio conditions.\nInspired by the attractive features and potential advantages of this agile\nresource management, several approaches have been proposed during the last\nperiod. However, agile resource management also comes with its own challenges,\nand there are significant technical issues that still need to be addressed for\nsuccessful rollout and operation of this technique. In this paper, we propose\nan approach for anticipating throughput variation for mobile video streaming\nservices. The solution of the optimization problem realizes a fundamental\ntrade-offs among critical metrics that impact the user's perceptual quality of\nthe experience (QoE) and system utilization. Both simulated and real-world\ntraces are carried out to evaluate the performance of the proposed approach. It\nis shown that our approach provides the accuracy, efficiency and robustness\nthat the new 5G architectures require.", 
    "link": "http://arxiv.org/pdf/1512.05705v3", 
    "arxiv-id": "1512.05705v3"
},{
    "category": "cs.MM", 
    "author": "Alan Hanjalic", 
    "title": "Learning Subclass Representations for Visually-varied Image   Classification", 
    "publish": "2016-01-12T15:30:58Z", 
    "summary": "In this paper, we present a subclass-representation approach that predicts\nthe probability of a social image belonging to one particular class. We explore\nthe co-occurrence of user-contributed tags to find subclasses with a strong\nconnection to the top level class. We then project each image on to the\nresulting subclass space to generate a subclass representation for the image.\nThe novelty of the approach is that subclass representations make use of not\nonly the content of the photos themselves, but also information on the\nco-occurrence of their tags, which determines membership in both subclasses and\ntop-level classes. The novelty is also that the images are classified into\nsmaller classes, which have a chance of being more visually stable and easier\nto model. These subclasses are used as a latent space and images are\nrepresented in this space by their probability of relatedness to all of the\nsubclasses. In contrast to approaches directly modeling each top-level class\nbased on the image content, the proposed method can exploit more information\nfor visually diverse classes. The approach is evaluated on a set of $2$ million\nphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale\nFlickr-tag Image Classification Grand Challenge. Experiments show that the\nproposed system delivers sound performance for visually diverse classes\ncompared with methods that directly model top classes.", 
    "link": "http://arxiv.org/pdf/1601.02913v1", 
    "arxiv-id": "1601.02913v1"
},{
    "category": "cs.CV", 
    "author": "Manuel M. Oliveira", 
    "title": "Digital Image Forensics vs. Image Composition: An Indirect Arms Race", 
    "publish": "2016-01-13T13:38:36Z", 
    "summary": "The field of image composition is constantly trying to improve the ways in\nwhich an image can be altered and enhanced. While this is usually done in the\nname of aesthetics and practicality, it also provides tools that can be used to\nmaliciously alter images. In this sense, the field of digital image forensics\nhas to be prepared to deal with the influx of new technology, in a constant\narms-race. In this paper, the current state of this arms-race is analyzed,\nsurveying the state-of-the-art and providing means to compare both sides. A\nnovel scale to classify image forensics assessments is proposed, and\nexperiments are performed to test composition techniques in regards to\ndifferent forensics traces. We show that even though research in forensics\nseems unaware of the advanced forms of image composition, it possesses the\nbasic tools to detect it.", 
    "link": "http://arxiv.org/pdf/1601.03239v1", 
    "arxiv-id": "1601.03239v1"
},{
    "category": "cs.MM", 
    "author": "Jie Lin", 
    "title": "Egocentric Activity Recognition with Multimodal Fisher Vector", 
    "publish": "2016-01-25T13:57:07Z", 
    "summary": "With the increasing availability of wearable devices, research on egocentric\nactivity recognition has received much attention recently. In this paper, we\nbuild a Multimodal Egocentric Activity dataset which includes egocentric videos\nand sensor data of 20 fine-grained and diverse activity categories. We present\na novel strategy to extract temporal trajectory-like features from sensor data.\nWe propose to apply the Fisher Kernel framework to fuse video and temporal\nenhanced sensor features. Experiment results show that with careful design of\nfeature extraction and fusion algorithm, sensor data can enhance\ninformation-rich video data. We make publicly available the Multimodal\nEgocentric Activity dataset to facilitate future research.", 
    "link": "http://arxiv.org/pdf/1601.06603v1", 
    "arxiv-id": "1601.06603v1"
},{
    "category": "cs.MM", 
    "author": "T. V. Cooklev", 
    "title": "Robust Image Watermarking Using Non-Regular Wavelets", 
    "publish": "2016-01-27T00:18:26Z", 
    "summary": "An approach to watermarking digital images using non-regular wavelets is\nadvanced. Non-regular transforms spread the energy in the transform domain. The\nproposed method leads at the same time to increased image quality and increased\nrobustness with respect to lossy compression. The approach provides robust\nwatermarking by suitably creating watermarked messages that have energy\ncompaction and frequency spreading. Our experimental results show that the\napplication of non-regular wavelets, instead of regular ones, can furnish a\nsuperior robust watermarking scheme. The generated watermarked data is more\nimmune against non-intentional JPEG and JPEG2000 attacks.", 
    "link": "http://arxiv.org/pdf/1601.07232v1", 
    "arxiv-id": "1601.07232v1"
},{
    "category": "cs.MM", 
    "author": "Alan Hanjalic", 
    "title": "Geo-distinctive Visual Element Matching for Location Estimation of   Images", 
    "publish": "2016-01-28T20:13:01Z", 
    "summary": "We propose an image representation and matching approach that substantially\nimproves visual-based location estimation for images. The main novelty of the\napproach, called distinctive visual element matching (DVEM), is its use of\nrepresentations that are specific to the query image whose location is being\npredicted. These representations are based on visual element clouds, which\nrobustly capture the connection between the query and visual evidence from\ncandidate locations. We then maximize the influence of visual elements that are\ngeo-distinctive because they do not occur in images taken at many other\nlocations. We carry out experiments and analysis for both geo-constrained and\ngeo-unconstrained location estimation cases using two large-scale,\npublicly-available datasets: the San Francisco Landmark dataset with $1.06$\nmillion street-view images and the MediaEval '15 Placing Task dataset with\n$5.6$ million geo-tagged images from Flickr. We present examples that\nillustrate the highly-transparent mechanics of the approach, which are based on\ncommon sense observations about the visual patterns in image collections. Our\nresults show that the proposed method delivers a considerable performance\nimprovement compared to the state of the art.", 
    "link": "http://arxiv.org/pdf/1601.07884v1", 
    "arxiv-id": "1601.07884v1"
},{
    "category": "cs.HC", 
    "author": "Mar Gonzalez-Franco", 
    "title": "Assessing 3D scan quality in Virtual Reality through paired-comparisons   psychophysics test", 
    "publish": "2016-01-31T12:43:34Z", 
    "summary": "Consumer 3D scanners and depth cameras are increasingly being used to\ngenerate content and avatars for Virtual Reality (VR) environments and avoid\nthe inconveniences of hand modeling; however, it is sometimes difficult to\nevaluate quantitatively the mesh quality at which 3D scans should be exported,\nand whether the object perception might be affected by its shading. We propose\nusing a paired-comparisons test based on psychophysics of perception to do that\nevaluation. As psychophysics is not subject to opinion, skill level, mental\nstate, or economic situation it can be considered a quantitative way to measure\nhow people perceive the mesh quality. In particular, we propose using the\npsychophysical measure for the comparison of four different levels of mesh\nquality (1K, 5K, 10K and 20K triangles). We present two studies within\nsubjects: in one we investigate the quality perception variations of seeing an\nobject in a regular screen monitor against an stereoscopic Head Mounted Display\n(HMD); while in the second experiment we aim at detecting the effects of\nshading into quality perception. At each iteration of the pair-test comparisons\nparticipants pick the mesh that they think had higher quality; by the end of\nthe experiment we compile a preference matrix. The matrix evidences the\ncorrelation between real quality and assessed quality. Regarding the shading\nmode, we find an interaction with quality and shading when the model has high\ndefinition. Furthermore, we assess the subjective realism of the most/least\npreferred scans using an Immersive Augmented Reality (IAR) video-see-through\nsetup. Results show higher levels of realism were perceived through the HMD\nthan when using a monitor, although the quality was similarly perceived in both\nsystems.", 
    "link": "http://arxiv.org/pdf/1602.00238v2", 
    "arxiv-id": "1602.00238v2"
},{
    "category": "cs.CV", 
    "author": "B. S. Manjunath", 
    "title": "Search Tracker: Human-derived object tracking in-the-wild through   large-scale search and retrieval", 
    "publish": "2016-02-05T00:01:13Z", 
    "summary": "Humans use context and scene knowledge to easily localize moving objects in\nconditions of complex illumination changes, scene clutter and occlusions. In\nthis paper, we present a method to leverage human knowledge in the form of\nannotated video libraries in a novel search and retrieval based setting to\ntrack objects in unseen video sequences. For every video sequence, a document\nthat represents motion information is generated. Documents of the unseen video\nare queried against the library at multiple scales to find videos with similar\nmotion characteristics. This provides us with coarse localization of objects in\nthe unseen video. We further adapt these retrieved object locations to the new\nvideo using an efficient warping scheme. The proposed method is validated on\nin-the-wild video surveillance datasets where we outperform state-of-the-art\nappearance-based trackers. We also introduce a new challenging dataset with\ncomplex object appearance changes.", 
    "link": "http://arxiv.org/pdf/1602.01890v1", 
    "arxiv-id": "1602.01890v1"
},{
    "category": "cs.CY", 
    "author": "Joachim Allgaier", 
    "title": "Science on YouTube: What users find when they search for climate science   and climate manipulation", 
    "publish": "2016-02-08T18:43:30Z", 
    "summary": "Online video-sharing sites such as YouTube are very popular and also used by\na lot of people to obtain knowledge and information, also on science, health\nand technology. Technically they could be valuable tools for the public\ncommunication of science and technology, but the users of YouTube are also\nconfronted with conspiracy theories and erroneous and misleading information\nthat deviates from scientific consensus views. This contribution details the\nresults of a study that investigates what kind of information users find when\nthey are searching for climate science and climate manipulation topics on\nYouTube and whether this information corresponds with or challenges scientific\nconsensus views. An innovative methodological approach using the anonymization\nnetwork Tor is introduced for drawing randomized samples of YouTube videos.\nThis approach was used to select and examine a sample of 140 YouTube videos on\nclimate topics.", 
    "link": "http://arxiv.org/pdf/1602.02692v2", 
    "arxiv-id": "1602.02692v2"
},{
    "category": "cs.NI", 
    "author": "Zurina Mohd Hanapi", 
    "title": "Feasible HCCA Polling Mechanism for Video Transmission in IEEE 802.11e   WLANs", 
    "publish": "2016-02-11T13:09:05Z", 
    "summary": "IEEE 802.11e standard defines two Medium Access Control (MAC) functions to\nsupport Quality of Service (QoS) for wireless local area networks: Enhanced\nDistributed Channel Access (EDCA) and HCF Controlled Channel Access (HCCA).\nEDCA provides fair prioritized QoS support while HCCA guarantees parameterized\nQoS for the traffics with rigid QoS requirements. The latter shows higher QoS\nprovisioning with Constant Bit Rate (CBR) traffics. However, it does not\nefficiently cope with the fluctuation of the Variable Bit Rate (VBR) video\nstreams since its reference scheduler generates a schedule based on the mean\ncharacteristics of the traffic. Scheduling based on theses characteristics is\nnot always accurate as these tra_cs show high irregularity over the time. In\nthis paper, we propose an enhancement on the HCCA polling mechanism to address\nthe problem of scheduling pre-recorded VBR video streams. Our approach enhances\nthe polling mechanism by feed-backing the arrival time of the subsequent video\nframe of the uplink traffic obtained through cross-layering approach.\nSimulation experiments have been conducted on several publicly available video\ntraces in order to show the efficiency of our mechanism. The simulation results\nreveal the efficiency of the proposed mechanism in providing less delay and\nhigh throughput with conserving medium channel through minimizing the number of\nNull-Frames caused by wasted polls", 
    "link": "http://arxiv.org/pdf/1602.03716v1", 
    "arxiv-id": "1602.03716v1"
},{
    "category": "cs.NI", 
    "author": "Zurina Mohd Hanapi", 
    "title": "Providing Dynamic TXOP for QoS Support of Video Transmission in IEEE   802.11e WLANs", 
    "publish": "2016-02-11T13:34:14Z", 
    "summary": "The IEEE 802.11e standard introduced by IEEE 802.11 Task Group E (TGe)\nenhances the Quality of Service (QoS) by means of HCF Controlled Channel Access\n(HCCA). The scheduler of HCCA allocates Transmission Opportunities (TXOPs) to\nQoS-enabled Station (QSTA) based on their TS Specifications (TSPECs) negotiated\nat the traffic setup time so that it is only efficient for Constant Bit Rate\n(CBR) applications. However, Variable Bit Rate (VBR) traffics are not\nefficiently supported as they exhibit nondeterministic profile during the time.\nIn this paper, we present a dynamic TXOP assignment Scheduling Algorithm for\nsupporting the video traffics transmission over IEEE 802.11e wireless networks.\nThis algorithm uses a piggybacked information about the size of the subsequent\nvideo frames of the uplink traffic to assist the Hybrid Coordinator accurately\nassign the TXOP according to the fast changes in the VBR profile. The proposed\nscheduling algorithm has been evaluated using simulation with different\nvariability level video streams. The simulation results show that the proposed\nalgorithm reduces the delay experienced by VBR traffic streams comparable to\nHCCA scheduler due to the accurate assignment of the TXOP which preserve the\nchannel time for transmission.", 
    "link": "http://arxiv.org/pdf/1602.03886v1", 
    "arxiv-id": "1602.03886v1"
},{
    "category": "cs.MM", 
    "author": "Koen Vos", 
    "title": "High-Quality, Low-Delay Music Coding in the Opus Codec", 
    "publish": "2016-02-15T21:30:54Z", 
    "summary": "The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide\nrange of real-time Internet applications by combining a linear prediction coder\nwith a transform coder. We describe the transform coder, with particular\nattention to the psychoacoustic knowledge built into the format. The result\nout-performs existing audio codecs that do not operate under real-time\nconstraints.", 
    "link": "http://arxiv.org/pdf/1602.04845v1", 
    "arxiv-id": "1602.04845v1"
},{
    "category": "cs.MM", 
    "author": "Gregory Maxwell", 
    "title": "A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay", 
    "publish": "2016-02-17T05:50:50Z", 
    "summary": "We propose an audio codec that addresses the low-delay requirements of some\napplications such as network music performance. The codec is based on the\nmodified discrete cosine transform (MDCT) with very short frames and uses\ngain-shape quantization to preserve the spectral envelope. The short frame\nsizes required for low delay typically hinder the performance of transform\ncodecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the\nproposed codec out-performs the ULD codec operating at the same rate. The total\ncomplexity of the codec is small, at only 17 WMOPS for real-time operation at\n48 kHz.", 
    "link": "http://arxiv.org/pdf/1602.05311v1", 
    "arxiv-id": "1602.05311v1"
},{
    "category": "cs.SD", 
    "author": "Gregory Maxwell", 
    "title": "A High-Quality Speech and Audio Codec With Less Than 10 ms Delay", 
    "publish": "2016-02-17T18:41:16Z", 
    "summary": "With increasing quality requirements for multimedia communications, audio\ncodecs must maintain both high quality and low delay. Typically, audio codecs\noffer either low delay or high quality, but rarely both. We propose a codec\nthat simultaneously addresses both these requirements, with a delay of only 8.7\nms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the\nfrequency domain with time-domain pitch prediction. We demonstrate that the\nproposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C\nand MP3 and has quality comparable to AAC-LD, despite having less than one\nfourth of the algorithmic delay of these codecs.", 
    "link": "http://arxiv.org/pdf/1602.05526v1", 
    "arxiv-id": "1602.05526v1"
},{
    "category": "cs.SI", 
    "author": "Georges Linar\u00e8s", 
    "title": "Narrative Smoothing: Dynamic Conversational Network for the Analysis of   TV Series Plots", 
    "publish": "2016-02-25T06:06:04Z", 
    "summary": "-Modern popular TV series often develop complex storylines spanning several\nseasons, but are usually watched in quite a discontinuous way. As a result, the\nviewer generally needs a comprehensive summary of the previous season plot\nbefore the new one starts. The generation of such summaries requires first to\nidentify and characterize the dynamics of the series subplots. One way of doing\nso is to study the underlying social network of interactions between the\ncharacters involved in the narrative. The standard tools used in the Social\nNetworks Analysis field to extract such a network rely on an integration of\ntime, either over the whole considered period, or as a sequence of several\ntime-slices. However, they turn out to be inappropriate in the case of TV\nseries, due to the fact the scenes showed onscreen alternatively focus on\nparallel storylines, and do not necessarily respect a traditional chronology.\nThis makes existing extraction methods inefficient to describe the dynamics of\nrelationships between characters, or to get a relevant instantaneous view of\nthe current social state in the plot. This is especially true for characters\nshown as interacting with each other at some previous point in the plot but\ntemporarily neglected by the narrative. In this article, we introduce narrative\nsmoothing, a novel, still exploratory, network extraction method. It smooths\nthe relationship dynamics based on the plot properties, aiming at solving some\nof the limitations present in the standard approaches. In order to assess our\nmethod, we apply it to a new corpus of 3 popular TV series, and compare it to\nboth standard approaches. Our results are promising, showing narrative\nsmoothing leads to more relevant observations when it comes to the\ncharacterization of the protagonists and their relationships. It could be used\nas a basis for further modeling the intertwined storylines constituting TV\nseries plots.", 
    "link": "http://arxiv.org/pdf/1602.07811v3", 
    "arxiv-id": "1602.07811v3"
},{
    "category": "cs.SD", 
    "author": "Jean-Marc Valin", 
    "title": "Extension spectrale d'un signal de parole de la bande t\u00e9l\u00e9phonique   \u00e0 la bande AM", 
    "publish": "2016-02-26T03:16:37Z", 
    "summary": "This document proposes a bandwidth extension system producing a wideband\nsignal from a narrowband speech signal. The extension is performed\nindependently for high and low frequencies. High-frequency extension uses the\nexcitation-filter model. Extension of the excitation is performed in the time\ndomain using a non-linear function, while the spectral envelope is extended in\nthe cepstral domain using a multi-layer perceptron. Low-band extension is based\non the sinusoidal model. The amplitude of sinusoids is also estimated using a\nmulti-layer perceptron.\n  The results show that the sound quality after extension is higher than that\nof narrowband speech, with a significant variation across listeners. Some of\nthe techniques, including excitation extension, are of interest in the field of\nspeech coding.\n  -----\n  Le pr\\'esent m\\'emoire propose un syst\\`eme d'extension de la bande\npermettant de produire un signal en bande AM \\`a partir d'un signal de parole\nen bande t\\'el\\'ephonique. L'extension est effectu\\'ee de fa\\c{c}on\nind\\'ependante pour les hautes fr\\'equences et les basses fr\\'equences.\nL'extension des hautes fr\\'equences utilise le mod\\`ele filtre-excitation.\nL'extension de l'excitation est r\\'ealis\\'ee dans le domaine temporel par une\nfonction non lin\\'eaire, alors que l'extension de l'enveloppe spectrale\ns'effectue dans le domaine cepstral par un perceptron multi-couches.\nL'extension de la bande basse utilise le mod\\`ele sinuso\\\"idal. L'amplitude des\nsinuso\\\"ides est aussi estim\\'ee par un perceptron multi-couches.\n  Les r\\'esultats obtenus montrent que la qualit\\'e sonore apr\\`es extension\nest sup\\'erieure \\`a celle de la bande t\\'el\\'ephonique, avec une importante\ndiff\\'erence entre les auditeurs. Certaines techniques d\\'evelopp\\'ees, dont\nl'extension de l'excitation, pr\\'esentent un certain int\\'er\\^et pour le\ndomaine du codage de la parole.", 
    "link": "http://arxiv.org/pdf/1602.08185v1", 
    "arxiv-id": "1602.08185v1"
},{
    "category": "cs.NI", 
    "author": "Adam Wolisz", 
    "title": "QoE-Based Low-Delay Live Streaming Using Throughput Predictions", 
    "publish": "2016-03-02T20:38:01Z", 
    "summary": "Recently, HTTP-based adaptive streaming has become the de facto standard for\nvideo streaming over the Internet. It allows clients to dynamically adapt media\ncharacteristics to network conditions in order to ensure a high quality of\nexperience, that is, minimize playback interruptions, while maximizing video\nquality at a reasonable level of quality changes. In the case of live\nstreaming, this task becomes particularly challenging due to the latency\nconstraints. The challenge further increases if a client uses a wireless\nnetwork, where the throughput is subject to considerable fluctuations.\nConsequently, live streams often exhibit latencies of up to 30 seconds. In the\npresent work, we introduce an adaptation algorithm for HTTP-based live\nstreaming called LOLYPOP (Low-Latency Prediction-Based Adaptation) that is\ndesigned to operate with a transport latency of few seconds. To reach this\ngoal, LOLYPOP leverages TCP throughput predictions on multiple time scales,\nfrom 1 to 10 seconds, along with an estimate of the prediction error\ndistribution. In addition to satisfying the latency constraint, the algorithm\nheuristically maximizes the quality of experience by maximizing the average\nvideo quality as a function of the number of skipped segments and quality\ntransitions. In order to select an efficient prediction method, we studied the\nperformance of several time series prediction methods in IEEE 802.11 wireless\naccess networks. We evaluated LOLYPOP under a large set of experimental\nconditions limiting the transport latency to 3 seconds, against a\nstate-of-the-art adaptation algorithm from the literature, called FESTIVE. We\nobserved that the average video quality is by up to a factor of 3 higher than\nwith FESTIVE. We also observed that LOLYPOP is able to reach a broader region\nin the quality of experience space, and thus it is better adjustable to the\nuser profile or service provider requirements.", 
    "link": "http://arxiv.org/pdf/1603.00859v2", 
    "arxiv-id": "1603.00859v2"
},{
    "category": "cs.CV", 
    "author": "Stefano Tubaro", 
    "title": "Camera identification with deep convolutional networks", 
    "publish": "2016-03-03T12:10:47Z", 
    "summary": "The possibility of detecting which camera has been used to shoot a specific\npicture is of paramount importance for many forensics tasks. This is extremely\nuseful for copyright infringement cases, ownership attribution, as well as for\ndetecting the authors of distributed illicit material (e.g., pedo-pornographic\nshots). Due to its importance, the forensics community has developed a series\nof robust detectors that exploit characteristic traces left by each camera on\nthe acquired images during the acquisition pipeline. These traces are\nreverse-engineered in order to attribute a picture to a camera. In this paper,\nwe investigate an alternative approach to solve camera identification problem.\nIndeed, we propose a data-driven algorithm based on convolutional neural\nnetworks, which learns features characterizing each camera directly from the\nacquired pictures. The proposed approach is tested on both instance-attribution\nand model-attribution, providing an accuracy greater than 94% in discriminating\n27 camera models.", 
    "link": "http://arxiv.org/pdf/1603.01068v1", 
    "arxiv-id": "1603.01068v1"
},{
    "category": "cs.NI", 
    "author": "Stefan Valentin", 
    "title": "Anticipatory Radio Resource Management for Mobile Video Streaming with   Linear Programming", 
    "publish": "2016-03-08T10:45:05Z", 
    "summary": "In anticipatory networking, channel prediction is used to improve\ncommunication performance. This paper describes a new approach for allocating\nresources to video streaming traffic while accounting for quality of service.\nThe proposed method is based on integrating a model of the user's local\nplay-out buffer into the radio access network. The linearity of this model\nallows to formulate a Linear Programming problem that optimizes the trade-off\nbetween the allocated resources and the stalling time of the media stream. Our\nsimulation results demonstrate the full power of anticipatory optimization in a\nsimple, yet representative, scenario. Compared to instantaneous adaptation, our\nanticipatory solution shows impressive gains in spectral efficiency and\nstalling duration at feasible computation time while being robust against\nprediction errors.", 
    "link": "http://arxiv.org/pdf/1603.02472v1", 
    "arxiv-id": "1603.02472v1"
},{
    "category": "cs.MM", 
    "author": "Jean-Marc Valin", 
    "title": "Predicting Chroma from Luma with Frequency Domain Intra Prediction", 
    "publish": "2016-03-10T22:55:36Z", 
    "summary": "This paper describes a technique for performing intra prediction of the\nchroma planes based on the reconstructed luma plane in the frequency domain.\nThis prediction exploits the fact that while RGB to YUV color conversion has\nthe property that it decorrelates the color planes globally across an image,\nthere is still some correlation locally at the block level. Previous proposals\ncompute a linear model of the spatial relationship between the luma plane (Y)\nand the two chroma planes (U and V). In codecs that use lapped transforms this\nis not possible since transform support extends across the block boundaries and\nthus neighboring blocks are unavailable during intra-prediction. We design a\nfrequency domain intra predictor for chroma that exploits the same local\ncorrelation with lower complexity than the spatial predictor and which works\nwith lapped transforms. We then describe a low-complexity algorithm that\ndirectly uses luma coefficients as a chroma predictor based on gain-shape\nquantization and band partitioning. An experiment is performed that compares\nthese two techniques inside the experimental Daala video codec and shows the\nlower complexity algorithm to be a better chroma predictor.", 
    "link": "http://arxiv.org/pdf/1603.03482v1", 
    "arxiv-id": "1603.03482v1"
},{
    "category": "cs.NI", 
    "author": "Chakchai So-In", 
    "title": "Modeling and Resource Allocation for HD Videos over WiMAX Broadband   Wireless Networks", 
    "publish": "2016-03-25T19:35:59Z", 
    "summary": "Mobile video is considered a major upcoming application and revenue generator\nfor broadband wireless networks like WiMAX and LTE. Therefore, it is important\nto design a proper resource allocation scheme for mobile video, since video\ntraffic is both throughput consuming and delay sensitive.", 
    "link": "http://arxiv.org/pdf/1603.07990v1", 
    "arxiv-id": "1603.07990v1"
},{
    "category": "cs.SD", 
    "author": "Kazuyoshi Yoshii", 
    "title": "Singing Voice Separation and Vocal F0 Estimation based on Mutual   Combination of Robust Principal Component Analysis and Subharmonic Summation", 
    "publish": "2016-04-01T10:28:51Z", 
    "summary": "This paper presents a new method of singing voice analysis that performs\nmutually-dependent singing voice separation and vocal fundamental frequency\n(F0) estimation. Vocal F0 estimation is considered to become easier if singing\nvoices can be separated from a music audio signal, and vocal F0 contours are\nuseful for singing voice separation. This calls for an approach that improves\nthe performance of each of these tasks by using the results of the other. The\nproposed method first performs robust principal component analysis (RPCA) for\nroughly extracting singing voices from a target music audio signal. The F0\ncontour of the main melody is then estimated from the separated singing voices\nby finding the optimal temporal path over an F0 saliency spectrogram. Finally,\nthe singing voices are separated again more accurately by combining a\nconventional time-frequency mask given by RPCA with another mask that passes\nonly the harmonic structures of the estimated F0s. Experimental results showed\nthat the proposed method significantly improved the performances of both\nsinging voice separation and vocal F0 estimation. The proposed method also\noutperformed all the other methods of singing voice separation submitted to an\ninternational music analysis competition called MIREX 2014.", 
    "link": "http://arxiv.org/pdf/1604.00192v1", 
    "arxiv-id": "1604.00192v1"
},{
    "category": "cs.MM", 
    "author": "Minati Mishra", 
    "title": "Steganography -- A Game of Hide and Seek in Information Communication", 
    "publish": "2016-04-02T12:21:52Z", 
    "summary": "With the growth of communication over computer networks, how to maintain the\nconfidentiality and security of transmitted information have become some of the\nimportant issues. In order to transfer data securely to the destination without\nunwanted disclosure or damage, nature inspired hide and seek tricks such as,\ncryptography and Steganography are heavily in use. Just like the Chameleon and\nmany other bio-species those change their body color and hide themselves in the\nbackground in order to protect them from external attacks, Cryptography and\nSteganography are techniques those are used to encrypt and hide the secret data\ninside other media to ensure data security. This paper discusses the concept of\na simple spatial domain LSB Steganography that encrypts the secrets using\nFibonacci- Lucas transformation, before hiding, for better security.", 
    "link": "http://arxiv.org/pdf/1604.00493v1", 
    "arxiv-id": "1604.00493v1"
},{
    "category": "cs.CR", 
    "author": "Meisam Sadeghi", 
    "title": "Image Encryption Based On Gradient Haar Wavelet and Rational Order   Chaotic Maps", 
    "publish": "2016-04-08T06:20:18Z", 
    "summary": "Haar wavelet is one of the best mathematical tools in image cryptography and\nanalysis. Because of the specific structure, this wavelet has the ability which\nis combined with other mathematical tools such as chaotic maps. The rational\norder chaotic maps are one of clusters of chaotic maps which their\ndeterministic behaviors have high sensitivity. In this paper, we propose a\nnovel method of gradient Haar wavelet transform for image encryption. This\nmethod use linearity properties of the scaling function of the gradient Haar\nwavelet and deterministic behaviors of rational order chaotic maps in order to\ngenerate encrypted images with high security factor. The security of the\nencrypted images is evaluated by the key space analysis, the correlation\ncoefficient analysis, and differential attack. The method could be used in\nother fields such as image and signal processing.", 
    "link": "http://arxiv.org/pdf/1604.02235v1", 
    "arxiv-id": "1604.02235v1"
},{
    "category": "cs.MM", 
    "author": "Sos Agaian", 
    "title": "Trends toward real-time network data steganography", 
    "publish": "2016-04-11T02:58:06Z", 
    "summary": "Network steganography has been a well-known covert data channeling method for\nover three decades. The basic set of techniques and implementation tools have\nnot changed significantly since their introduction in the early 1980's. In this\npaper, we review the predominant methods of classical network steganography,\ndescribing the detailed operations and resultant challenges involved in\nembedding data in the network transport domain. We also consider the various\ncyber threat vectors of network steganography and point out the major\ndifferences between classical network steganography and the widely known\nend-point multimedia embedding techniques, which focus exclusively on static\ndata modification for data hiding. We then challenge the security community by\nintroducing an entirely new network dat hiding methodology, which we refer to\nas real-time network data steganography. Finally we provide the groundwork for\nthis fundamental change of covert network data embedding by forming a basic\nframework for real-time network data operations that will open the path for\neven further advances in computer network security.", 
    "link": "http://arxiv.org/pdf/1604.02778v1", 
    "arxiv-id": "1604.02778v1"
},{
    "category": "cs.CR", 
    "author": "S. Lakshmanan", 
    "title": "An Integrated Method of Data Hiding and Compression of Medical Images", 
    "publish": "2016-04-11T05:51:22Z", 
    "summary": "A new technique for embedding data into an image coupled with compression has\nbeen proposed in this paper. A fast and efficient coding algorithms are needed\nfor effective storage and transmission, due to the popularity of telemedicine\nand the use of digital medical images. Medical images are produced and\ntransferred between hospitals for review by physicians who are geographically\napart. Such image data need to be stored for future reference of patients as\nwell. This necessitates compact storage of medical images before being\ntransmitted over Internet. Moreover, as the patient information is also\nembedded within the medical images, it is very important to maintain the\nconfidentiality of patient data. Hence, this article aims at hiding patient\ninformation as well, within the medical image followed by joint compression.\nThe hidden data and the host image are absolutely recoverable from the embedded\nimage without any loss.", 
    "link": "http://arxiv.org/pdf/1604.02797v1", 
    "arxiv-id": "1604.02797v1"
},{
    "category": "cs.SD", 
    "author": "Haizhou Li", 
    "title": "Noise Robust Speech Recognition Using Multi-Channel Based Channel   Selection And ChannelWeighting", 
    "publish": "2016-04-12T07:52:27Z", 
    "summary": "In this paper, we study several microphone channel selection and weighting\nmethods for robust automatic speech recognition (ASR) in noisy conditions. For\nchannel selection, we investigate two methods based on the maximum likelihood\n(ML) criterion and minimum autoencoder reconstruction criterion, respectively.\nFor channel weighting, we produce enhanced log Mel filterbank coefficients as a\nweighted sum of the coefficients of all channels. The weights of the channels\nare estimated by using the ML criterion with constraints. We evaluate the\nproposed methods on the CHiME-3 noisy ASR task. Experiments show that channel\nweighting significantly outperforms channel selection due to its higher\nflexibility. Furthermore, on real test data in which different channels have\ndifferent gains of the target signal, the channel weighting method performs\nequally well or better than the MVDR beamforming, despite the fact that the\nchannel weighting does not make use of the phase delay information which is\nnormally used in beamforming.", 
    "link": "http://arxiv.org/pdf/1604.03276v1", 
    "arxiv-id": "1604.03276v1"
},{
    "category": "cs.CV", 
    "author": "Xavier Giro-i-Nieto", 
    "title": "From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment   Prediction", 
    "publish": "2016-04-12T17:24:39Z", 
    "summary": "Visual multimedia have become an inseparable part of our digital social\nlives, and they often capture moments tied with deep affections. Automated\nvisual sentiment analysis tools can provide a means of extracting the rich\nfeelings and latent dispositions embedded in these media. In this work, we\nexplore how Convolutional Neural Networks (CNNs), a now de facto computational\nmachine learning tool particularly in the area of Computer Vision, can be\nspecifically applied to the task of visual sentiment prediction. We accomplish\nthis through fine-tuning experiments using a state-of-the-art CNN and via\nrigorous architecture analysis, we present several modifications that lead to\naccuracy improvements over prior art on a dataset of images from a popular\nsocial media platform. We additionally present visualizations of local patterns\nthat the network learned to associate with image sentiment for insight into how\nvisual positivity (or negativity) is perceived by the model.", 
    "link": "http://arxiv.org/pdf/1604.03489v2", 
    "arxiv-id": "1604.03489v2"
},{
    "category": "cs.CV", 
    "author": "Xavier Giro-i-Nieto", 
    "title": "Bags of Local Convolutional Features for Scalable Instance Search", 
    "publish": "2016-04-15T22:02:22Z", 
    "summary": "This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark.", 
    "link": "http://arxiv.org/pdf/1604.04653v1", 
    "arxiv-id": "1604.04653v1"
},{
    "category": "cs.AI", 
    "author": "Mark Sandler", 
    "title": "Text-based LSTM networks for Automatic Music Composition", 
    "publish": "2016-04-18T21:43:44Z", 
    "summary": "In this paper, we introduce new methods and discuss results of text-based\nLSTM (Long Short-Term Memory) networks for automatic music composition. The\nproposed network is designed to learn relationships within text documents that\nrepresent chord progressions and drum tracks in two case studies. In the\nexperiments, word-RNNs (Recurrent Neural Networks) show good results for both\ncases, while character-based RNNs (char-RNNs) only succeed to learn chord\nprogressions. The proposed system can be used for fully automatic composition\nor as semi-automatic systems that help humans to compose music by controlling a\ndiversity parameter of the model.", 
    "link": "http://arxiv.org/pdf/1604.05358v1", 
    "arxiv-id": "1604.05358v1"
},{
    "category": "cs.MM", 
    "author": "Andy M. Connor", 
    "title": "Mainstreaming video annotation software for critical video analysis", 
    "publish": "2016-04-20T02:58:36Z", 
    "summary": "The range of video annotation software currently available is set within\ncommercially specialized professions, distributed via outdated sources or\nthrough online video hosting services. As video content becomes an increasingly\nsignificant tool for analysis, there is a demand for appropriate digital\nannotation techniques that offer equivalent functionality to tools used for\nannotation of text based literature sources. This paper argues for the\nimportance of video annotating as an effective method for research that is as\naccessible as literature annotation is. Video annotation has been shown to\ntrigger higher learning and engagement but research struggles to explain the\nabsence of video annotation in contemporary structures of education practice.\nIn both academic and informal settings the use of video playback as a\nmeaningful tool of analysis is apparent, yet the availability of supplementary\nannotation software is not within obvious grasp or even prevalent in\nstandardized computer software. Practical software tools produced by the\nresearcher have demonstrated effective video annotation in a short development\ntime. With software design programs available for rapid application creation,\nthis paper also highlights the absence of a development community. This paper\nargues that video annotation is an accessible tool, not just for academic\ncontexts, but also for wider practical video analysis applications, potentially\nbecoming a mainstream learning tool. This paper thus presents a practical\nmultimodal public approach to video research that potentially affords a deeper\nanalysis of media content. This is supported by an in-depth consideration of\nthe motivation for undertaking video annotation and a critical analysis of\ncurrently available tools.", 
    "link": "http://arxiv.org/pdf/1604.05799v1", 
    "arxiv-id": "1604.05799v1"
},{
    "category": "cs.SD", 
    "author": "Luc Van Gool", 
    "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic   Event Detection", 
    "publish": "2016-04-25T08:25:03Z", 
    "summary": "We propose a novel method for Acoustic Event Detection (AED). In contrast to\nspeech, sounds coming from acoustic events may be produced by a wide variety of\nsources. Furthermore, distinguishing them often requires analyzing an extended\ntime period due to the lack of a clear sub-word unit. In order to incorporate\nthe long-time frequency structure for AED, we introduce a convolutional neural\nnetwork (CNN) with a large input field. In contrast to previous works, this\nenables to train audio event detection end-to-end. Our architecture is inspired\nby the success of VGGNet and uses small, 3x3 convolutions, but more depth than\nprevious methods in AED. In order to prevent over-fitting and to take full\nadvantage of the modeling capabilities of our network, we further propose a\nnovel data augmentation method to introduce data variation. Experimental\nresults show that our CNN significantly outperforms state of the art methods\nincluding Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%\nabsolute improvement.", 
    "link": "http://arxiv.org/pdf/1604.07160v2", 
    "arxiv-id": "1604.07160v2"
},{
    "category": "cs.MM", 
    "author": "Jean-Charles Gr\u00e9goire", 
    "title": "Towards Reduced Reference Parametric Models for Estimating Audiovisual   Quality in Multimedia Services", 
    "publish": "2016-04-25T11:43:09Z", 
    "summary": "We have developed reduced reference parametric models for estimating\nperceived quality in audiovisual multimedia services. We have created 144\nunique configurations for audiovisual content including various application and\nnetwork parameters such as bitrates and distortions in terms of bandwidth,\npacket loss rate and jitter. To generate the data needed for model training and\nvalidation we have tasked 24 subjects, in a controlled environment, to rate the\noverall audiovisual quality on the absolute category rating (ACR) 5-level\nquality scale. We have developed models using Random Forest and Neural Network\nbased machine learning methods in order to estimate Mean Opinion Scores (MOS)\nvalues. We have used information retrieved from the packet headers and side\ninformation provided as network parameters for model training. Random Forest\nbased models have performed better in terms of Root Mean Square Error (RMSE)\nand Pearson correlation coefficient. The side information proved to be very\neffective in developing the model. We have found that, while the model\nperformance might be improved by replacing the side information with more\naccurate bit stream level measurements, they are performing well in estimating\nperceived quality in audiovisual multimedia services.", 
    "link": "http://arxiv.org/pdf/1604.07211v1", 
    "arxiv-id": "1604.07211v1"
},{
    "category": "cs.MM", 
    "author": "Antonio Liotta", 
    "title": "Predictive No-Reference Assessment of Video Quality", 
    "publish": "2016-04-25T16:34:17Z", 
    "summary": "Among the various means to evaluate the quality of video streams,\nNo-Reference (NR) methods have low computation and may be executed on thin\nclients. Thus, NR algorithms would be perfect candidates in cases of real-time\nquality assessment, automated quality control and, particularly, in adaptive\nmobile streaming. Yet, existing NR approaches are often inaccurate, in\ncomparison to Full-Reference (FR) algorithms, especially under lossy network\nconditions. In this work, we present an NR method that combines machine\nlearning with simple NR metrics to achieve a quality index comparably as\naccurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method\nis tested in an extensive dataset (960 videos), under lossy network conditions\nand considering nine different machine learning algorithms. Overall, we achieve\nan over 97% correlation with VQM, while allowing real-time assessment of video\nquality of experience in realistic streaming scenarios.", 
    "link": "http://arxiv.org/pdf/1604.07322v2", 
    "arxiv-id": "1604.07322v2"
},{
    "category": "cs.CV", 
    "author": "Shmuel Peleg", 
    "title": "EgoSampling: Wide View Hyperlapse from Egocentric Videos", 
    "publish": "2016-04-26T16:25:24Z", 
    "summary": "The possibility of sharing one's point of view makes use of wearable cameras\ncompelling. These videos are often long, boring and coupled with extreme shake,\nas the camera is worn on a moving person. Fast forwarding (i.e. frame sampling)\nis a natural choice for quick video browsing. However, this accentuates the\nshake caused by natural head motion in an egocentric video, making the fast\nforwarded video useless. We propose EgoSampling, an adaptive frame sampling\nthat gives stable, fast forwarded, hyperlapse videos. Adaptive frame sampling\nis formulated as an energy minimization problem, whose optimal solution can be\nfound in polynomial time. We further turn the camera shake from a drawback into\na feature, enabling the increase in field-of-view of the output video. This is\nobtained when each output frame is mosaiced from several input frames. The\nproposed technique also enables the generation of a single hyperlapse video\nfrom multiple egocentric videos, allowing even faster video consumption.", 
    "link": "http://arxiv.org/pdf/1604.07741v2", 
    "arxiv-id": "1604.07741v2"
},{
    "category": "cs.MM", 
    "author": "Qin Jin", 
    "title": "Detecting Violence in Video using Subclasses", 
    "publish": "2016-04-27T14:32:16Z", 
    "summary": "This paper attacks the challenging problem of violence detection in videos.\nDifferent from existing works focusing on combining multi-modal features, we go\none step further by adding and exploiting subclasses visually related to\nviolence. We enrich the MediaEval 2015 violence dataset by \\emph{manually}\nlabeling violence videos with respect to the subclasses. Such fine-grained\nannotations not only help understand what have impeded previous efforts on\nlearning to fuse the multi-modal features, but also enhance the generalization\nability of the learned fusion to novel test data. The new subclass based\nsolution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,\noutperforms several state-of-the-art alternatives. Notice that our solution\ndoes not require fine-grained annotations on the test set, so it can be\ndirectly applied on novel and fully unlabeled videos. Interestingly, our study\nshows that motion related features, though being essential part in previous\nsystems, are dispensable.", 
    "link": "http://arxiv.org/pdf/1604.08088v1", 
    "arxiv-id": "1604.08088v1"
},{
    "category": "cs.MM", 
    "author": "Alberto Del Bimbo", 
    "title": "Bloom Filters and Compact Hash Codes for Efficient and Distributed Image   Retrieval", 
    "publish": "2016-05-03T15:50:54Z", 
    "summary": "This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a $2\\times$ speedup.", 
    "link": "http://arxiv.org/pdf/1605.00957v1", 
    "arxiv-id": "1605.00957v1"
},{
    "category": "cs.CV", 
    "author": "Zongming Guo", 
    "title": "MARLow: A Joint Multiplanar Autoregressive and Low-Rank Approach for   Image Completion", 
    "publish": "2016-05-03T23:41:57Z", 
    "summary": "In this paper, we propose a novel multiplanar autoregressive (AR) model to\nexploit the correlation in cross-dimensional planes of a similar patch group\ncollected in an image, which has long been neglected by previous AR models. On\nthat basis, we then present a joint multiplanar AR and low-rank based approach\n(MARLow) for image completion from random sampling, which exploits the nonlocal\nself-similarity within natural images more effectively. Specifically, the\nmultiplanar AR model constraints the local stationarity in different\ncross-sections of the patch group, while the low-rank minimization captures the\nintrinsic coherence of nonlocal patches. The proposed approach can be readily\nextended to multichannel images (e.g. color images), by simultaneously\nconsidering the correlation in different channels. Experimental results\ndemonstrate that the proposed approach significantly outperforms\nstate-of-the-art methods, even if the pixel missing rate is as high as 90%.", 
    "link": "http://arxiv.org/pdf/1605.01115v2", 
    "arxiv-id": "1605.01115v2"
},{
    "category": "cs.MM", 
    "author": "Teemu K\u00e4m\u00e4r\u00e4inen", 
    "title": "A First Look at Quality of Mobile Live Streaming Experience: the Case of   Periscope", 
    "publish": "2016-05-13T17:44:52Z", 
    "summary": "Live multimedia streaming from mobile devices is rapidly gaining popularity\nbut little is known about the QoE they provide. In this paper, we examine the\nPeriscope service. We first crawl the service in order to understand its usage\npatterns. Then, we study the protocols used, the typical quality of experience\nindicators, such as playback smoothness and latency, video quality, and the\nenergy consumption of the Android application.", 
    "link": "http://arxiv.org/pdf/1605.04270v2", 
    "arxiv-id": "1605.04270v2"
},{
    "category": "cs.CV", 
    "author": "Liangliang Cao", 
    "title": "Video2GIF: Automatic Generation of Animated GIFs from Video", 
    "publish": "2016-05-16T17:44:31Z", 
    "summary": "We introduce the novel problem of automatically generating animated GIFs from\nvideo. GIFs are short looping video with no sound, and a perfect combination\nbetween image and video that really capture our attention. GIFs tell a story,\nexpress emotion, turn events into humorous moments, and are the new wave of\nphotojournalism. We pose the question: Can we automate the entirely manual and\nelaborate process of GIF creation by leveraging the plethora of user generated\nGIF content? We propose a Robust Deep RankNet that, given a video, generates a\nranked list of its segments according to their suitability as GIF. We train our\nmodel to learn what visual content is often selected for GIFs by using over\n100K user generated GIFs and their corresponding video sources. We effectively\ndeal with the noisy web data by proposing a novel adaptive Huber loss in the\nranking formulation. We show that our approach is robust to outliers and picks\nup several patterns that are frequently present in popular animated GIFs. On\nour new large-scale benchmark dataset, we show the advantage of our approach\nover several state-of-the-art methods.", 
    "link": "http://arxiv.org/pdf/1605.04850v1", 
    "arxiv-id": "1605.04850v1"
},{
    "category": "cs.MM", 
    "author": "Cedric Westphal", 
    "title": "Resource Provisioning and Profit Maximization for Transcoding in   Information Centric Networking", 
    "publish": "2016-05-18T21:10:16Z", 
    "summary": "Adaptive bitrate streaming (ABR) has been widely adopted to support video\nstreaming services over heterogeneous devices and varying network conditions.\nWith ABR, each video content is transcoded into multiple representations in\ndifferent bitrates and resolutions. However, video transcoding is computing\nintensive, which requires the transcoding service providers to deploy a large\nnumber of servers for transcoding the video contents published by the content\nproducers. As such, a natural question for the transcoding service provider is\nhow to provision the computing resource for transcoding the video contents\nwhile maximizing service profit. To address this problem, we design a cloud\nvideo transcoding system by taking the advantage of cloud computing technology\nto elastically allocate computing resource. We propose a method for jointly\nconsidering the task scheduling and resource provisioning problem in two\ntimescales, and formulate the service profit maximization as a two-timescale\nstochastic optimization problem. We derive some approximate policies for the\ntask scheduling and resource provisioning. Based on our proposed methods, we\nimplement our open source cloud video transcoding system Morph and evaluate its\nperformance in a real environment. The experiment results demonstrate that our\nproposed method can reduce the resource consumption and achieve a higher profit\ncompared with the baseline schemes.", 
    "link": "http://arxiv.org/pdf/1605.05758v1", 
    "arxiv-id": "1605.05758v1"
},{
    "category": "cs.MM", 
    "author": "Lifeng Sun", 
    "title": "Understanding the Smartrouter-based Peer CDN for Video Streaming", 
    "publish": "2016-05-25T01:50:17Z", 
    "summary": "Recent years have witnessed a new video delivery paradigm: smartrouter-based\nvideo delivery network, which is enabled by smartrouters deployed at users'\nhomes, together with the conventional video servers deployed in the\ndatacenters. Recently, ChinaCache, a large content delivery network (CDN)\nprovider, and Youku, a video service provider using smartrouters to assist\nvideo delivery, announced their cooperation to create a new paradigm of content\ndelivery based on householders' network resources. This new paradigm is\ndifferent from the conventional peer-to-peer (P2P) approach, because such\ndedicated smartrouters are inherently operated by the centralized video service\nproviders in a coordinative manner. It is intriguing to study the strategies,\nperformance and potential impact on the content delivery ecosystem of such peer\nCDN systems. In this paper, we study the Youku peer CDN, which has deployed\nover 300K smartrouter devices for its video streaming. In our measurement, 78K\nvideos were investigated and 3TB traffic has been analyzed, over controlled\nrouters and players. Our contributions are the following measurement insights.\nFirst, a global replication and caching strategy is essential for the peer CDN\nsystems, and proactively scheduling replication and caching on a daily basis\ncan guarantee their performance. Second, such peer CDN deployment can itself\nform an effective Quality of Service (QoS) monitoring sub-system, which can be\nused for fine-grained user request redirection. We also provide our analysis on\nthe performance issues and potential improvements to the peer CDN systems.", 
    "link": "http://arxiv.org/pdf/1605.07704v1", 
    "arxiv-id": "1605.07704v1"
},{
    "category": "cs.MM", 
    "author": "Lifeng Sun", 
    "title": "Understanding Content Placement Strategies in Smartrouter-based Peer CDN   for Video Streaming", 
    "publish": "2016-05-25T01:51:25Z", 
    "summary": "Recent years have witnessed a new video delivery paradigm: smartrouter-based\npeer video content delivery network, which is enabled by smartrouters deployed\nat users' homes. ChinaCache (one of the largest CDN providers in China) and\nYouku (a video provider using smartrouters to assist video delivery) announced\ntheir cooperation in 2015, to create a new paradigm of content delivery based\non householders' network resources. This new paradigm is different from the\nconventional peer-to-peer (P2P) approach, because millions of dedicated\nsmartrouters are operated by the centralized video service providers in a\ncoordinative manner. Thus it is intriguing to study the content placement\nstrategies used in a smartrouter-based content delivery system, as well as its\npotential impact on the content delivery ecosystem. In this paper, we carry out\nmeasurement studies of Youku's peer video CDN, who has deployed over 300K\nsmartrouter devices for its video delivery. In our measurement studies, 104K\nvideos were investigated and 4TB traffic has been analyzed, over controlled\nsmartrouter nodes and players. Our measurement insights are as follows. First,\na global content replication strategy is essential for the peer CDN systems.\nSecond, such peer CDN deployment itself can form an effective sub-system for\nend-to-end QoS monitoring, which can be used for fine-grained request\nredirection (e.g., user-level) and content replication. We also show our\nanalysis on the performance limitations and propose potential improvements to\nthe peer CDN systems.", 
    "link": "http://arxiv.org/pdf/1605.07705v2", 
    "arxiv-id": "1605.07705v2"
},{
    "category": "cs.CV", 
    "author": "Rajer Sindhu", 
    "title": "A Feature based Approach for Video Compression", 
    "publish": "2016-05-26T23:04:24Z", 
    "summary": "It is a high cost problem for panoramic image stitching via image matching\nalgorithm and not practical for real-time performance. In this paper, we take\nfull advantage ofHarris corner invariant characterization method light\nintensity parallel meaning, translation and rotation, and made a realtime\npanoramic image stitching algorithm. According to the basic characteristics and\nperformance FPGA classical algorithm, several modules such as the feature point\nextraction, and matching description is to optimize the feature-based logic.\nReal-time optimization system to achieve high precision match. The new\nalgorithm process the image from pixel domain and obtained from CCD camera\nXilinx Spartan-6 hardware platform. After the image stitching algorithm, will\neventually form a portable interface to output high-definition content on the\ndisplay. The results showed that, the proposed algorithm has higher precision\nwith good real-time performance and robustness.", 
    "link": "http://arxiv.org/pdf/1605.08470v1", 
    "arxiv-id": "1605.08470v1"
},{
    "category": "cs.MM", 
    "author": "Manuel Torres", 
    "title": "Models and Algorithms for Graph Watermarking", 
    "publish": "2016-05-30T21:46:31Z", 
    "summary": "We introduce models and algorithmic foundations for graph watermarking. Our\nframeworks include security definitions and proofs, as well as\ncharacterizations when graph watermarking is algorithmically feasible, in spite\nof the fact that the general problem is NP-complete by simple reductions from\nthe subgraph isomorphism or graph edit distance problems. In the digital\nwatermarking of many types of files, an implicit step in the recovery of a\nwatermark is the mapping of individual pieces of data, such as image pixels or\nmovie frames, from one object to another. In graphs, this step corresponds to\napproximately matching vertices of one graph to another based on graph\ninvariants such as vertex degree. Our approach is based on characterizing the\nfeasibility of graph watermarking in terms of keygen, marking, and\nidentification functions defined over graph families with known distributions.\nWe demonstrate the strength of this approach with exemplary watermarking\nschemes for two random graph models, the classic Erd\\H{o}s-R\\'{e}nyi model and\na random power-law graph model, both of which are used to model real-world\nnetworks.", 
    "link": "http://arxiv.org/pdf/1605.09425v1", 
    "arxiv-id": "1605.09425v1"
},{
    "category": "cs.MM", 
    "author": "Alan Bertoni", 
    "title": "Advanced Transport Options for the Dynamic Adaptive Streaming over HTTP", 
    "publish": "2016-06-01T12:49:09Z", 
    "summary": "Multimedia streaming over HTTP is no longer a niche research topic as it has\nentered our daily live. The common assumption is that it is deployed on top of\nthe existing infrastructure utilizing application (HTTP) and transport (TCP)\nlayer protocols as is. Interestingly, standards like MPEG's Dynamic Adaptive\nStreaming over HTTP (DASH) do not mandate the usage of any specific transport\nprotocol allowing for sufficient deployment flexibility which is further\nsupported by emerging developments within both protocol layers. This paper\ninvestigates and evaluates the usage of advanced transport options for the\ndynamic adaptive streaming over HTTP. We utilize a common test setup to\nevaluate HTTP/2.0 and Google's Quick UDP Internet Connections (QUIC) protocol\nin the context of DASH-based services.", 
    "link": "http://arxiv.org/pdf/1606.00264v1", 
    "arxiv-id": "1606.00264v1"
},{
    "category": "cs.CV", 
    "author": "Oge Marques", 
    "title": "Automatic Separation of Compound Figures in Scientific Articles", 
    "publish": "2016-06-03T09:53:01Z", 
    "summary": "Content-based analysis and retrieval of digital images found in scientific\narticles is often hindered by images consisting of multiple subfigures\n(compound figures). We address this problem by proposing a method to\nautomatically classify and separate compound figures, which consists of two\nmain steps: (i) a supervised compound figure classifier (CFC) discriminates\nbetween compound and non-compound figures using task-specific image features;\nand (ii) an image processing algorithm is applied to predicted compound images\nto perform compound figure separation (CFS). Our CFC approach is shown to\nachieve state-of-the-art classification performance on a published dataset. Our\nCFS algorithm shows superior separation accuracy on two different datasets\ncompared to other known automatic approaches. Finally, we propose a method to\nevaluate the effectiveness of the CFC-CFS process chain and use it to optimize\nthe misclassification loss of CFC for maximal effectiveness in the process\nchain.", 
    "link": "http://arxiv.org/pdf/1606.01021v2", 
    "arxiv-id": "1606.01021v2"
},{
    "category": "cs.MM", 
    "author": "Sos Agaian", 
    "title": "High Capacity Image Steganography using Adjunctive Numerical   Representations with Multiple Bit-Plane Decomposition Methods", 
    "publish": "2016-06-07T20:00:41Z", 
    "summary": "LSB steganography is a one of the most widely used methods for implementing\ncovert data channels in image file exchanges [1][2]. The low computational\ncomplexity and implementation simplicity of the algorithm are significant\nfactors for its popularity with the primary reason being low image distortion.\nMany attempts have been made to increase the embedding capacity of LSB\nalgorithms by expanding into the second or third binary layers of the image\nwhile maintaining a low probability of detection with minimal distortive\neffects [2][3][4]. In this paper, we introduce an advanced technique for\ncovertly embedding data within images using redundant number system\ndecomposition over non-standard digital bit planes. Both grayscale and\nbit-mapped images are equally effective as cover files. It will be shown that\nthis unique steganography method has minimal visual distortive affects while\nalso preserving the cover file statistics, making it less susceptible to most\ngeneral steganography detection algorithms.", 
    "link": "http://arxiv.org/pdf/1606.02312v1", 
    "arxiv-id": "1606.02312v1"
},{
    "category": "cs.SD", 
    "author": "Bhiksha Raj", 
    "title": "Audio Content based Geotagging in Multimedia", 
    "publish": "2016-06-09T04:01:36Z", 
    "summary": "In this paper we propose methods to extract geographically relevant\ninformation in a multimedia recording using its audio. Our method primarily is\nbased on the fact that urban acoustic environment consists of a variety of\nsounds. Hence, location information can be inferred from the composition of\nsound events/classes present in the audio. More specifically, we adopt matrix\nfactorization techniques to obtain semantic content of recording in terms of\ndifferent sound classes. These semantic information are then combined to\nidentify the location of recording.", 
    "link": "http://arxiv.org/pdf/1606.02816v2", 
    "arxiv-id": "1606.02816v2"
},{
    "category": "cs.MM", 
    "author": "Heng Tao Shen", 
    "title": "Bidirectional Long-Short Term Memory for Video Description", 
    "publish": "2016-06-15T03:26:53Z", 
    "summary": "Video captioning has been attracting broad research attention in multimedia\ncommunity. However, most existing approaches either ignore temporal information\namong video frames or just employ local contextual temporal knowledge. In this\nwork, we propose a novel video captioning framework, termed as\n\\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures\nbidirectional global temporal structure in video. Specifically, we first devise\na joint visual modelling approach to encode video data by combining a forward\nLSTM pass, a backward LSTM pass, together with visual features from\nConvolutional Neural Networks (CNNs). Then, we inject the derived video\nrepresentation into the subsequent language model for initialization. The\nbenefits are in two folds: 1) comprehensively preserving sequential and visual\ninformation; and 2) adaptively learning dense visual features and sparse\nsemantic representations for videos and sentences, respectively. We verify the\neffectiveness of our proposed video captioning framework on a commonly-used\nbenchmark, i.e., Microsoft Video Description (MSVD) corpus, and the\nexperimental results demonstrate that the superiority of the proposed approach\nas compared to several state-of-the-art methods.", 
    "link": "http://arxiv.org/pdf/1606.04631v1", 
    "arxiv-id": "1606.04631v1"
},{
    "category": "cs.CL", 
    "author": "Louis-Philippe Morency", 
    "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis   in Online Opinion Videos", 
    "publish": "2016-06-20T19:23:53Z", 
    "summary": "People are sharing their opinions, stories and reviews through online video\nsharing websites every day. Studying sentiment and subjectivity in these\nopinion videos is experiencing a growing attention from academia and industry.\nWhile sentiment analysis has been successful for text, it is an understudied\nresearch question for videos and multimedia content. The biggest setbacks for\nstudies in this direction are lack of a proper dataset, methodology, baselines\nand statistical analysis of how information from different modality sources\nrelate to each other. This paper introduces to the scientific community the\nfirst opinion-level annotated corpus of sentiment and subjectivity analysis in\nonline videos called Multimodal Opinion-level Sentiment Intensity dataset\n(MOSI). The dataset is rigorously annotated with labels for subjectivity,\nsentiment intensity, per-frame and per-opinion annotated visual features, and\nper-milliseconds annotated audio features. Furthermore, we present baselines\nfor future studies in this direction as well as a new multimodal fusion\napproach that jointly models spoken words and visual gestures.", 
    "link": "http://arxiv.org/pdf/1606.06259v2", 
    "arxiv-id": "1606.06259v2"
},{
    "category": "cs.MM", 
    "author": "Weisi Lin", 
    "title": "Personality, Culture, and System Factors - Impact on Affective Response   to Multimedia", 
    "publish": "2016-06-22T10:02:39Z", 
    "summary": "Whilst affective responses to various forms and genres of multimedia content\nhave been well researched, precious few studies have investigated the combined\nimpact that multimedia system parameters and human factors have on affect.\nConsequently, in this paper we explore the role that two primordial dimensions\nof human factors - personality and culture - in conjunction with system factors\n- frame rate, resolution, and bit rate - have on user affect and enjoyment of\nmultimedia presentations. To this end, a two-site, cross-cultural study was\nundertaken, the results of which produced three predictve models. Personality\nand Culture traits were shown statistically to represent 5.6% of the variance\nin positive affect, 13.6% in negative affect and 9.3% in enjoyment. The\ncorrelation between affect and enjoyment, was significant. Predictive modeling\nincorporating human factors showed about 8%, 7% and 9% improvement in\npredicting positive affect, negative affect and enjoyment respectively when\ncompared to models trained only on system factors. Results and analysis\nindicate the significant role played by human factors in influencing affect\nthat users experience while watching multimedia.", 
    "link": "http://arxiv.org/pdf/1606.06873v2", 
    "arxiv-id": "1606.06873v2"
},{
    "category": "cs.MM", 
    "author": "Vladimir Trajkovik", 
    "title": "N-queens-based algorithm for moving object detection in distributed   wireless sensor networks", 
    "publish": "2016-06-24T07:18:42Z", 
    "summary": "The main constraint of wireless sensor networks (WSN) in enabling wireless\nimage communication is the high energy requirement, which may exceed even the\nfuture capabilities of battery technologies. In this paper we have shown that\nthis bottleneck can be overcome by developing local in-network image processing\nalgorithm that offers optimal energy consumption. Our algorithm is very\nsuitable for intruder detection applications. Each node is responsible for\nprocessing the image captured by the video sensor, which consists of NxN\nblocks. If an intruder is detected in the monitoring region, the node will\ntransmit the image for further processing. Otherwise, the node takes no action.\nResults provided from our experiments show that our algorithm is better than\nthe traditional moving object detection techniques by a factor of (N/2) in\nterms of energy savings.", 
    "link": "http://arxiv.org/pdf/1606.07583v1", 
    "arxiv-id": "1606.07583v1"
},{
    "category": "cs.CV", 
    "author": "Gonzalo Vaca-Castano", 
    "title": "Finding the Topic of a Set of Images", 
    "publish": "2016-06-25T15:06:27Z", 
    "summary": "In this paper we introduce the problem of determining the topic that a set of\nimages is describing, where every topic is represented as a set of words.\nDifferent from other problems like tag assignment or similar, a) we assume\nmultiple images are used as input instead of single image, b) Input images are\ntypically not visually related, c) Input images are not necessarily\nsemantically close, and d) Output word space is unconstrained. In our proposed\nsolution, visual information of each query image is used to retrieve similar\nimages with text labels (tags) from an image database. We consider a scenario\nwhere the tags are very noisy and diverse, given that they were obtained by\nimplicit crowd-sourcing in a database of 1 million images and over seventy\nseven thousand tags. The words or tags associated to each query are processed\njointly in a word selection algorithm using random walks that allows to refine\nthe search topic, rejecting words that are not part of the topic and produce a\nset of words that fairly describe the topic. Experiments on a dataset of 300\ntopics, with up to twenty images per topic, show that our algorithm performs\nbetter than the proposed baseline for any number of query images. We also\npresent a new Conditional Random Field (CRF) word mapping algorithm that\npreserves the semantic similarity of the mapped words, increasing the\nperformance of the results over the baseline.", 
    "link": "http://arxiv.org/pdf/1606.07921v1", 
    "arxiv-id": "1606.07921v1"
},{
    "category": "cs.MM", 
    "author": "Winston H. Hsu", 
    "title": "De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile   Visual Search", 
    "publish": "2016-06-29T08:33:28Z", 
    "summary": "Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.", 
    "link": "http://arxiv.org/pdf/1606.08999v1", 
    "arxiv-id": "1606.08999v1"
},{
    "category": "cs.MM", 
    "author": "Lifeng Sun", 
    "title": "Towards Network-Failure-Tolerant Content Delivery for Web Content", 
    "publish": "2016-07-05T09:08:56Z", 
    "summary": "Popularly used to distribute a variety of multimedia content items in today\nInternet, HTTP-based web content delivery still suffers from various content\ndelivery failures. Hindered by the expensive deployment cost, the conventional\nCDN can not deploy as many edge servers as possible to successfully deliver\ncontent items to all users under these delivery failures. In this paper, we\npropose a joint CDN and peer-assisted web content delivery framework to address\nthe delivery failure problem. Different from conventional peer-assisted\napproaches for web content delivery, which mainly focus on alleviating the CDN\nservers bandwidth load, we study how to use a browser-based peer-assisted\nscheme, namely WebRTC, to resolve content delivery failures. To this end, we\ncarry out large-scale measurement studies on how users access and view\nwebpages. Our measurement results demonstrate the challenges (e.g., peers stay\non a webpage extremely short) that can not be directly solved by conventional\nP2P strategies, and some important webpage viewing patterns. Due to these\nunique characteristics, WebRTC peers open up new possibilities for helping the\nweb content delivery, coming with the problem of how to utilize the dynamic\nresources efficiently. We formulate the peer selection that is the critical\nstrategy in our framework, as an optimization problem, and design a heuristic\nalgorithm based on the measurement insights to solve it. Our simulation\nexperiments driven by the traces from Tencent QZone demonstrate the\neffectiveness of our design: compared with non-peer-assisted strategy and\nrandom peer selection strategy, our design significantly improves the\nsuccessful relay ratio of web content items under network failures, e.g., our\ndesign improves the content download ratio up to 60% even when users located in\na particular region (e.g., city) where none can connect to the regional CDN\nserver.", 
    "link": "http://arxiv.org/pdf/1607.01159v1", 
    "arxiv-id": "1607.01159v1"
},{
    "category": "cs.MM", 
    "author": "Lifeng Sun", 
    "title": "A Measurement Study of TCP Performance for Chunk Delivery in DASH", 
    "publish": "2016-07-05T09:51:25Z", 
    "summary": "Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly\npopular paradigm for video streaming [13], in which a video is segmented into\nmany chunks delivered to users by HTTP request/response over Transmission\nControl Protocol (TCP) con- nections. Therefore, it is intriguing to study the\nperformance of strategies implemented in conventional TCPs, which are not\ndedicated for video streaming, e.g., whether chunks are efficiently delivered\nwhen users per- form interactions with the video players. In this paper, we\nconduct mea- surement studies on users chunk requesting traces in DASH from a\nrep- resentative video streaming provider, to investigate users behaviors in\nDASH, and TCP-connection-level traces from CDN servers, to investi- gate the\nperformance of TCP for DASH. By studying how video chunks are delivered in both\nthe slow start and congestion avoidance phases, our observations have revealed\nthe performance characteristics of TCP for DASH as follows: (1) Request\npatterns in DASH have a great impact on the performance of TCP variations\nincluding cubic; (2) Strategies in conventional TCPs may cause user perceived\nquality degradation in DASH streaming; (3) Potential improvement to TCP\nstrategies for better delivery in DASH can be further explored.", 
    "link": "http://arxiv.org/pdf/1607.01172v1", 
    "arxiv-id": "1607.01172v1"
},{
    "category": "cs.SD", 
    "author": "Gerald Friedland", 
    "title": "DCAR: A Discriminative and Compact Audio Representation to Improve Event   Detection", 
    "publish": "2016-07-15T04:28:14Z", 
    "summary": "This paper presents a novel two-phase method for audio representation,\nDiscriminative and Compact Audio Representation (DCAR), and evaluates its\nperformance at detecting events in consumer-produced videos. In the first phase\nof DCAR, each audio track is modeled using a Gaussian mixture model (GMM) that\nincludes several components to capture the variability within that track. The\nsecond phase takes into account both global structure and local structure. In\nthis phase, the components are rendered more discriminative and compact by\nformulating an optimization problem on Grassmannian manifolds, which we found\nrepresents the structure of audio effectively.\n  Our experiments used the YLI-MED dataset (an open TRECVID-style video corpus\nbased on YFCC100M), which includes ten events. The results show that the\nproposed DCAR representation consistently outperforms state-of-the-art audio\nrepresentations. DCAR's advantage over i-vector, mv-vector, and GMM\nrepresentations is significant for both easier and harder discrimination tasks.\nWe discuss how these performance differences across easy and hard cases follow\nfrom how each type of model leverages (or doesn't leverage) the intrinsic\nstructure of the data. Furthermore, DCAR shows a particularly notable accuracy\nadvantage on events where humans have more difficulty classifying the videos,\ni.e., events with lower mean annotator confidence.", 
    "link": "http://arxiv.org/pdf/1607.04378v1", 
    "arxiv-id": "1607.04378v1"
},{
    "category": "cs.SD", 
    "author": "Bhiksha Raj", 
    "title": "Features and Kernels for Audio Event Recognition", 
    "publish": "2016-07-19T21:29:03Z", 
    "summary": "One of the most important problems in audio event detection research is\nabsence of benchmark results for comparison with any proposed method. Different\nworks consider different sets of events and datasets which makes it difficult\nto comprehensively analyze any novel method with an existing one. In this paper\nwe propose to establish results for audio event recognition on two recent\npublicly-available datasets. In particular we use Gaussian Mixture model based\nfeature representation and combine them with linear as well as non-linear\nkernel Support Vector Machines.", 
    "link": "http://arxiv.org/pdf/1607.05765v1", 
    "arxiv-id": "1607.05765v1"
},{
    "category": "cs.MM", 
    "author": "Patrick Bas", 
    "title": "Natural Steganography: cover-source switching for better steganography", 
    "publish": "2016-07-26T18:02:44Z", 
    "summary": "This paper proposes a new steganographic scheme relying on the principle of\ncover-source switching, the key idea being that the embedding should switch\nfrom one cover-source to another. The proposed implementation, called Natural\nSteganography, considers the sensor noise naturally present in the raw images\nand uses the principle that, by the addition of a specific noise the\nsteganographic embedding tries to mimic a change of ISO sensitivity. The\nembedding methodology consists in 1) perturbing the image in the raw domain, 2)\nmodeling the perturbation in the processed domain, 3) embedding the payload in\nthe processed domain. We show that this methodology is easily tractable\nwhenever the processes are known and enables to embed large and undetectable\npayloads. We also show that already used heuristics such as synchronization of\nembedding changes or detectability after rescaling can be respectively\nexplained by operations such as color demosaicing and down-scaling kernels.", 
    "link": "http://arxiv.org/pdf/1607.07824v1", 
    "arxiv-id": "1607.07824v1"
},{
    "category": "cs.CR", 
    "author": "Juan C. Quiroz", 
    "title": "A New Approach to SMS Steganography using Mathematical Equations", 
    "publish": "2016-07-27T03:42:49Z", 
    "summary": "In the era of Information Technology, cyber-crime has always been a worrying\nissue for online users. Phishing, social engineering, and third party attacks\nhave made people reluctant to share their personal information, even with\ntrusted entities. Messages that are sent via Short Message Service (SMS) are\neasily copied and hacked by using special software. To enforce the security of\nsending messages through mobile phones, one solution is SMS steganography. SMS\nSteganography is a technique that hides a secret message in the SMS. We propose\na new approach for SMS steganography that uses a mathematical equation as the\nstego media in order to transmit the data. With this approach, we can hide up\nto 35 characters (25%) of a secret message on a single SMS with maximum of 140\ncharacters.", 
    "link": "http://arxiv.org/pdf/1607.07947v1", 
    "arxiv-id": "1607.07947v1"
},{
    "category": "cs.CV", 
    "author": "Peter Corke", 
    "title": "Exploiting Temporal Information for DCNN-based Fine-Grained Object   Classification", 
    "publish": "2016-08-01T16:34:16Z", 
    "summary": "Fine-grained classification is a relatively new field that has concentrated\non using information from a single image, while ignoring the enormous potential\nof using video data to improve classification. In this work we present the\nnovel task of video-based fine-grained object classification, propose a\ncorresponding new video dataset, and perform a systematic study of several\nrecent deep convolutional neural network (DCNN) based approaches, which we\nspecifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream\nDCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where\nspatial and temporal data from two independent DCNNs are fused either via early\nfusion (combination of the fully-connected layers) and late fusion\n(concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs,\ninformation from the convolutional layers of the spatial and temporal DCNNs is\ncombined via local co-occurrences. We then fuse the bilinear DCNN and early\nfusion of the two-stream approach to combine the spatial and temporal\ninformation at the local and global level (Spatio-Temporal Co-occurrence).\nUsing the new and challenging video dataset of birds, classification\nperformance is improved from 23.1% (using single images) to 41.1% when using\nthe Spatio-Temporal Co-occurrence system. Incorporating automatically detected\nbounding box location further improves the classification accuracy to 53.6%.", 
    "link": "http://arxiv.org/pdf/1608.00486v3", 
    "arxiv-id": "1608.00486v3"
},{
    "category": "cs.MM", 
    "author": "Divam Gupta", 
    "title": "PicHunt: Social Media Image Retrieval for Improved Law Enforcement", 
    "publish": "2016-08-02T17:09:19Z", 
    "summary": "First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%.", 
    "link": "http://arxiv.org/pdf/1608.00905v2", 
    "arxiv-id": "1608.00905v2"
},{
    "category": "cs.MM", 
    "author": "Yiannis Andreopoulos", 
    "title": "Media Query Processing For The Internet-of-Things: Coupling Of Device   Energy Consumption And Cloud Infrastructure Billing", 
    "publish": "2016-08-02T18:40:03Z", 
    "summary": "Audio/visual recognition and retrieval applications have recently garnered\nsignificant attention within Internet-of-Things (IoT) oriented services, given\nthat video cameras and audio processing chipsets are now ubiquitous even in\nlow-end embedded systems. In the most typical scenario for such services, each\ndevice extracts audio/visual features and compacts them into feature\ndescriptors, which comprise media queries. These queries are uploaded to a\nremote cloud computing service that performs content matching for\nclassification or retrieval applications. Two of the most crucial aspects for\nsuch services are: (i) controlling the device energy consumption when using the\nservice; (ii) reducing the billing cost incurred from the cloud infrastructure\nprovider. In this paper we derive analytic conditions for the optimal coupling\nbetween the device energy consumption and the incurred cloud infrastructure\nbilling. Our framework encapsulates: the energy consumption to produce and\ntransmit audio/visual queries, the billing rates of the cloud infrastructure,\nthe number of devices concurrently connected to the same cloud server, {the\nquery volume constraint of each cluster of devices,} and the statistics of the\nquery data production volume per device. Our analytic results are validated via\na deployment with: (i) the device side comprising compact image descriptors\n(queries) computed on Beaglebone Linux embedded platforms and transmitted to\nAmazon Web Services (AWS) Simple Storage Service; (ii) the cloud side carrying\nout image similarity detection via AWS Elastic Compute Cloud (EC2) instances,\nwith the AWS Auto Scaling being used to control the number of instances\naccording to the demand.", 
    "link": "http://arxiv.org/pdf/1608.00925v1", 
    "arxiv-id": "1608.00925v1"
},{
    "category": "cs.CR", 
    "author": "Krzysztof Szczypiorski", 
    "title": "StegIbiza: New Method for Information Hiding in Club Music", 
    "publish": "2016-08-09T22:04:30Z", 
    "summary": "In this paper a new method for information hiding in club music is\nintroduced. The method called StegIbiza is based on using the music tempo as a\ncarrier. The tempo is modulated by hidden messages with a 3-value coding\nscheme, which is an adoption of Morse code for StegIbiza. The evaluation of the\nsystem was performed for several music samples (with and without StegIbiza\nenabled) on a selected group of testers who had a music background. Finally,\nfor the worst case scenario, none of them could identify any differences in the\naudio with a 1% margin of changed tempo.", 
    "link": "http://arxiv.org/pdf/1608.02988v1", 
    "arxiv-id": "1608.02988v1"
},{
    "category": "cs.MM", 
    "author": "Jiebo Luo", 
    "title": "Mining Fashion Outfit Composition Using An End-to-End Deep Learning   Approach on Set Data", 
    "publish": "2016-08-10T01:11:32Z", 
    "summary": "Fashion composition involves deep understanding of fashion standards while\nincorporating creativity for choosing multiple fashion items (e.g., Jewelry,\nBag, Pants, Dress). In fashion websites, popular or high-quality fashion\ncompositions are usually designed by fashion experts and followed by large\naudiences. In this paper, we aim to employ a machine learning strategy to\ncompose fashion compositions by learning directly from the fashion websites. We\npropose an end-to-end system to learn a fashion item embedding that helps\ndisentangle the factors contributing to fashion popularity, such as instance\naesthetics and set compatibility. Our learning system consists of 1) deep\nconvolutional network embedding of fashion images, 2) title embedding, and 3)\ncategory embedding. To leverage the multimodal information, we develop a\nmultiple-layer perceptron module with different pooling strategies to predict\nthe set popularity. For our experiments, we have collected a large-scale\nfashion set from the fashion website Polyvore. Although fashion composition is\na rather challenging task, the performance of our system is quite encouraging:\nwe have achieved an AUC of 85\\% for the fashion set popularity prediction task\non the Polyvore fashion set.", 
    "link": "http://arxiv.org/pdf/1608.03016v1", 
    "arxiv-id": "1608.03016v1"
},{
    "category": "cs.CV", 
    "author": "Ozgur Yilmaz", 
    "title": "Multi-View Product Image Search Using ConvNets Features", 
    "publish": "2016-08-11T13:50:07Z", 
    "summary": "Multi-view queries on a multi-view product image database with bag-of-visual\nwords (BoWs) have been shown to improve the average precision significantly\ncompared to traditional single view queries on single view databases. In this\npaper, we investigate the performance of deep convolutional neural networks\n(ConvNets) on multi-view product image search and compare to the classical\nBoWs. We used a simplified version of VGG network to train and extract global\nConvNets image representations for retrieval. We performed experiments on the\npublicly available Multi-View Object Image Dataset (MVOD 5K) and concluded that\n(1) multi-view queries with ConvNets representations perform significantly\nbetter than single view queries, (2) ConvNets perform much better than BoWs and\nhave room for further improvement.", 
    "link": "http://arxiv.org/pdf/1608.03462v1", 
    "arxiv-id": "1608.03462v1"
},{
    "category": "cs.CV", 
    "author": "James Z. Wang", 
    "title": "Detecting Vanishing Points in Natural Scenes with Application in Photo   Composition Analysis", 
    "publish": "2016-08-15T13:48:22Z", 
    "summary": "Linear perspective is widely used in landscape photography to create the\nimpression of depth on a 2D photo. Automated understanding of the use of linear\nperspective in landscape photography has a number of real-world applications,\nincluding aesthetics assessment, image retrieval, and on-site feedback for\nphoto composition. We address this problem by detecting vanishing points and\nthe associated line structures in photos. However, natural landscape scenes\npose great technical challenges because there are often inadequate number of\nstrong edges converging to the vanishing points. To overcome this difficulty,\nwe propose a novel vanishing point detection method that exploits global\nstructures in the scene via contour detection. We show that our method\nsignificantly outperforms state-of-the-art methods on a public ground truth\nlandscape image dataset that we have created. Based on the detection results,\nwe further demonstrate how our approach to linear perspective understanding can\nbe used to provide on-site guidance to amateur photographers on their work\nthrough a novel viewpoint-specific image retrieval system.", 
    "link": "http://arxiv.org/pdf/1608.04267v1", 
    "arxiv-id": "1608.04267v1"
},{
    "category": "cs.CV", 
    "author": "Wen Gao", 
    "title": "Globally Variance-Constrained Sparse Representation for Image Set   Compression", 
    "publish": "2016-08-17T09:34:51Z", 
    "summary": "Sparse representation presents an efficient approach to approximately recover\na signal by the linear composition of a few bases from a learnt dictionary,\nbased on which various successful applications have been observed. However, in\nthe scenario of data compression, its efficiency and popularity are hindered\ndue to the extra overhead for encoding the sparse coefficients. Therefore, how\nto establish an accurate rate model in sparse coding and dictionary learning\nbecomes meaningful, which has been not fully exploited in the context of sparse\nrepresentation. According to the Shannon entropy inequality, the variance of\ndata source bounds its entropy, which can reflect the actual coding bits.\nHence, in this work a Globally Variance-Constrained Sparse Representation\n(GVCSR) model is proposed, where a variance-constrained rate model is\nintroduced in the optimization process. Specifically, we employ the Alternating\nDirection Method of Multipliers (ADMM) to solve the non-convex optimization\nproblem for sparse coding and dictionary learning, both of which have shown\nstate-of-the-art performance in image representation. Furthermore, we\ninvestigate the potential of GVCSR in practical image set compression, where a\ncommon dictionary is trained by several key images to represent the whole image\nset. Experimental results have demonstrated significant performance\nimprovements against the most popular image codecs including JPEG and JPEG2000.", 
    "link": "http://arxiv.org/pdf/1608.04902v1", 
    "arxiv-id": "1608.04902v1"
},{
    "category": "cs.MM", 
    "author": "Christophe Guyeux", 
    "title": "Steganalyzer performances in operational contexts", 
    "publish": "2016-08-20T17:26:15Z", 
    "summary": "Steganography and steganalysis are two important branches of the information\nhiding field of research. Steganography methods consist in hiding information\nin such a way that the secret message is undetectable for the uninitiated.\nSteganalyzis encompasses all the techniques that attempt to detect the presence\nof such hidden information. This latter is usually designed by making\nclassifiers able to separate innocent images from steganographied ones\naccording to their differences on well-selected features. We wonder, in this\narticle whether it is possible to construct a kind of universal steganalyzer\nwithout any knowledge regarding the steganographier side. The effects on the\nclassification score of a modification of either parameters or methods between\nthe learning and testing stages are then evaluated, while the possibility to\nimprove the separation score by merging many methods during learning stage is\ndeeper investigated.", 
    "link": "http://arxiv.org/pdf/1608.05850v1", 
    "arxiv-id": "1608.05850v1"
},{
    "category": "cs.MM", 
    "author": "F. G. B. De Natale", 
    "title": "Automatic Synchronization of Multi-User Photo Galleries", 
    "publish": "2016-08-24T10:17:16Z", 
    "summary": "In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization.", 
    "link": "http://arxiv.org/pdf/1608.06770v2", 
    "arxiv-id": "1608.06770v2"
},{
    "category": "cs.CR", 
    "author": "Artur Janicki", 
    "title": "YouSkyde: Information Hiding for Skype Video Traffic", 
    "publish": "2016-08-25T23:28:33Z", 
    "summary": "In this paper a new information hiding method for Skype videoconference calls\n- YouSkyde - is introduced. A Skype traffic analysis revealed that introducing\nintentional losses into the Skype video traffic stream to provide the means for\nclandestine communication is the most favourable solution. A YouSkyde\nproof-of-concept implementation was carried out and its experimental evaluation\nis presented. The results obtained prove that the proposed method is feasible\nand offer a steganographic bandwidth as high as 0.93 kbps, while introducing\nnegligible distortions into transmission quality and providing high\nundetectability.", 
    "link": "http://arxiv.org/pdf/1608.07337v1", 
    "arxiv-id": "1608.07337v1"
},{
    "category": "cs.CV", 
    "author": "Hirokatsu Kataoka", 
    "title": "Human Action Recognition without Human", 
    "publish": "2016-08-29T01:22:38Z", 
    "summary": "The objective of this paper is to evaluate \"human action recognition without\nhuman\". Motion representation is frequently discussed in human action\nrecognition. We have examined several sophisticated options, such as dense\ntrajectories (DT) and the two-stream convolutional neural network (CNN).\nHowever, some features from the background could be too strong, as shown in\nsome recent studies on human action recognition. Therefore, we considered\nwhether a background sequence alone can classify human actions in current\nlarge-scale action datasets (e.g., UCF101).\n  In this paper, we propose a novel concept for human action analysis that is\nnamed \"human action recognition without human\". An experiment clearly shows the\neffect of a background sequence for understanding an action label.", 
    "link": "http://arxiv.org/pdf/1608.07876v1", 
    "arxiv-id": "1608.07876v1"
},{
    "category": "cs.CV", 
    "author": "Thomas M. Breuel", 
    "title": "Active Canny: Edge Detection and Recovery with Open Active Contour   Models", 
    "publish": "2016-09-12T14:13:26Z", 
    "summary": "We introduce an edge detection and recovery framework based on open active\ncontour models (snakelets). This is motivated by the noisy or broken edges\noutput by standard edge detection algorithms, like Canny. The idea is to\nutilize the local continuity and smoothness cues provided by strong edges and\ngrow them to recover the missing edges. This way, the strong edges are used to\nrecover weak or missing edges by considering the local edge structures, instead\nof blindly linking them if gradient magnitudes are above some threshold. We\ninitialize short snakelets on the gradient magnitudes or binary edges\nautomatically and then deform and grow them under the influence of gradient\nvector flow. The output snakelets are able to recover most of the breaks or\nweak edges, and they provide a smooth edge representation of the image; they\ncan also be used for higher level analysis, like contour segmentation.", 
    "link": "http://arxiv.org/pdf/1609.03415v1", 
    "arxiv-id": "1609.03415v1"
},{
    "category": "cs.MM", 
    "author": "Hongtao Lu", 
    "title": "Generalized residual vector quantization for large scale data", 
    "publish": "2016-09-17T14:50:06Z", 
    "summary": "Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.", 
    "link": "http://arxiv.org/pdf/1609.05345v1", 
    "arxiv-id": "1609.05345v1"
},{
    "category": "cs.CV", 
    "author": "Xian-Sheng Hua", 
    "title": "Deep CTR Prediction in Display Advertising", 
    "publish": "2016-09-20T04:50:03Z", 
    "summary": "Click through rate (CTR) prediction of image ads is the core task of online\ndisplay advertising systems, and logistic regression (LR) has been frequently\napplied as the prediction model. However, LR model lacks the ability of\nextracting complex and intrinsic nonlinear features from handcrafted\nhigh-dimensional image features, which limits its effectiveness. To solve this\nissue, in this paper, we introduce a novel deep neural network (DNN) based\nmodel that directly predicts the CTR of an image ad based on raw image pixels\nand other basic features in one step. The DNN model employs convolution layers\nto automatically extract representative visual features from images, and\nnonlinear CTR features are then learned from visual features and other\ncontextual features by using fully-connected layers. Empirical evaluations on a\nreal world dataset with over 50 million records demonstrate the effectiveness\nand efficiency of this method.", 
    "link": "http://arxiv.org/pdf/1609.06018v1", 
    "arxiv-id": "1609.06018v1"
},{
    "category": "cs.MM", 
    "author": "Kazimierz Wiatr", 
    "title": "FPGA implementation of the procedures for video quality assessment", 
    "publish": "2016-09-20T11:27:16Z", 
    "summary": "Video resolutions used in variety of media are constantly rising. While\nmanufacturers struggle to perfect their screens it also important to ensure\nhigh quality of displayed image. Overall quality can be measured using Mean\nOpinion Score (MOS). Video quality can be affected by miscellaneous artifacts,\nappearing at every stage of video creation and transmission. In this paper we\npresent a solution to calculate four distinct video quality metrics that can be\napplied to a real time video quality assessment system. Our assessment module\nis capable of processing 8K resolution in real time manner set at the level of\n30 frames per second. Throughput of 2.19 GB/s surpasses performance of\npuresoftware solutions. To concentrate on architectural optimization module was\ncreated using high level language.", 
    "link": "http://arxiv.org/pdf/1609.06109v1", 
    "arxiv-id": "1609.06109v1"
},{
    "category": "cs.CV", 
    "author": "Yu-Gang Jiang", 
    "title": "Deep Learning for Video Classification and Captioning", 
    "publish": "2016-09-22T00:08:59Z", 
    "summary": "Accelerated by the tremendous increase in Internet bandwidth and storage\nspace, video data has been generated, published and spread explosively,\nbecoming an indispensable part of today's big data. In this paper, we focus on\nreviewing two lines of research aiming to stimulate the comprehension of videos\nwith deep learning: video classification and video captioning. While video\nclassification concentrates on automatically labeling video clips based on\ntheir semantic contents like human actions or complex events, video captioning\nattempts to generate a complete and natural sentence, enriching the single\nlabel as in video classification, to capture the most informative dynamics in\nvideos. In addition, we also provide a review of popular benchmarks and\ncompetitions, which are critical for evaluating the technical progress of this\nvibrant field.", 
    "link": "http://arxiv.org/pdf/1609.06782v1", 
    "arxiv-id": "1609.06782v1"
},{
    "category": "cs.MM", 
    "author": "Alexander Wong", 
    "title": "Deep Quality: A Deep No-reference Quality Assessment System", 
    "publish": "2016-09-22T21:26:21Z", 
    "summary": "Image quality assessment (IQA) continues to garner great interest in the\nresearch community, particularly given the tremendous rise in consumer video\ncapture and streaming. Despite significant research effort in IQA in the past\nfew decades, the area of no-reference image quality assessment remains a great\nchallenge and is largely unsolved. In this paper, we propose a novel\nno-reference image quality assessment system called Deep Quality, which\nleverages the power of deep learning to model the complex relationship between\nvisual content and the perceived quality. Deep Quality consists of a novel\nmulti-scale deep convolutional neural network, trained to learn to assess image\nquality based on training samples consisting of different distortions and\ndegradations such as blur, Gaussian noise, and compression artifacts.\nPreliminary results using the CSIQ benchmark image quality dataset showed that\nDeep Quality was able to achieve strong quality prediction performance (89%\npatch-level and 98% image-level prediction accuracy), being able to achieve\nsimilar performance as full-reference IQA methods.", 
    "link": "http://arxiv.org/pdf/1609.07170v1", 
    "arxiv-id": "1609.07170v1"
},{
    "category": "cs.NI", 
    "author": "Subhasis Chaudhuri", 
    "title": "Congestion Control for Network-Aware Telehaptic Communication", 
    "publish": "2016-10-03T16:06:20Z", 
    "summary": "Telehaptic applications involve delay-sensitive multimedia communication\nbetween remote locations with distinct Quality of Service (QoS) requirements\nfor different media components. These QoS constraints pose a variety of\nchallenges, especially when the communication occurs over a shared network,\nwith unknown and time-varying cross-traffic. In this work, we propose a\ntransport layer congestion control protocol for telehaptic applications\noperating over shared networks, termed as dynamic packetization module (DPM).\nDPM is a lossless, network-aware protocol which tunes the telehaptic\npacketization rate based on the level of congestion in the network. To monitor\nthe network congestion, we devise a novel network feedback module, which\ncommunicates the end-to-end delays encountered by the telehaptic packets to the\nrespective transmitters with negligible overhead. Via extensive simulations, we\nshow that DPM meets the QoS requirements of telehaptic applications over a wide\nrange of network cross-traffic conditions. We also report qualitative results\nof a real-time telepottery experiment with several human subjects, which reveal\nthat DPM preserves the quality of telehaptic activity even under heavily\ncongested network scenarios. Finally, we compare the performance of DPM with\nseveral previously proposed telehaptic communication protocols and demonstrate\nthat DPM outperforms these protocols.", 
    "link": "http://arxiv.org/pdf/1610.00609v2", 
    "arxiv-id": "1610.00609v2"
},{
    "category": "cs.MM", 
    "author": "Tomasz Tyl", 
    "title": "MoveSteg: A Method of Network Steganography Detection", 
    "publish": "2016-10-06T17:12:37Z", 
    "summary": "This article presents a new method for detecting a source point of time based\nnetwork steganography - MoveSteg. A steganography carrier could be an example\nof multimedia stream made with packets. These packets are then delayed\nintentionally to send hidden information using time based steganography\nmethods. The presented analysis describes a method that allows finding the\nsource of steganography stream in network that is under our management.", 
    "link": "http://arxiv.org/pdf/1610.01955v1", 
    "arxiv-id": "1610.01955v1"
},{
    "category": "cs.CV", 
    "author": "Xiaodong Li", 
    "title": "Deep Image Aesthetics Classification using Inception Modules and   Fine-tuning Connected Layer", 
    "publish": "2016-10-07T12:46:45Z", 
    "summary": "In this paper we investigate the image aesthetics classification problem,\naka, automatically classifying an image into low or high aesthetic quality,\nwhich is quite a challenging problem beyond image recognition. Deep\nconvolutional neural network (DCNN) methods have recently shown promising\nresults for image aesthetics assessment. Currently, a powerful inception module\nis proposed which shows very high performance in object classification.\nHowever, the inception module has not been taken into consideration for the\nimage aesthetics assessment problem. In this paper, we propose a novel DCNN\nstructure codenamed ILGNet for image aesthetics classification, which\nintroduces the Inception module and connects intermediate Local layers to the\nGlobal layer for the output. Besides, we use a pre-trained image classification\nCNN called GoogLeNet on the ImageNet dataset and fine tune our connected local\nand global layer on the large scale aesthetics assessment AVA dataset. The\nexperimental results show that the proposed ILGNet outperforms the state of the\nart results in image aesthetics assessment in the AVA benchmark.", 
    "link": "http://arxiv.org/pdf/1610.02256v1", 
    "arxiv-id": "1610.02256v1"
},{
    "category": "cs.MM", 
    "author": "Saira Beg", 
    "title": "Steganography between Silence Intervals of Audio in Video Content Using   Chaotic Maps", 
    "publish": "2016-10-14T07:27:46Z", 
    "summary": "Steganography is the art of hiding data, in such a way that it is\nundetectable under traffic-pattern analysis and the data hidden is only known\nto the receiver and the sender. In this paper new method of text steganography\nover the silence interval of audio in a video file, is presented. In the\nproposed method first the audio signal is extracted from the video. After doing\naudio enhancement, the data on the audio signal is steganographed using new\ntechnique and then audio signal is rewritten in video file again.\nhttp://www.learnrnd.com/All_latest_research_findings.php\n  To enhance the security level we apply chaotic maps on arbitrary text.\nFurthermore, the algorithm in this paper, gives a technique which states that\nundetectable stegotext and cover-text has same probability distribution and no\nstatistical test can detect the presence of the hidden message.\nhttp://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research\n  Moreover, hidden message does not affect the transmission rate of video file\nat all.", 
    "link": "http://arxiv.org/pdf/1610.04346v1", 
    "arxiv-id": "1610.04346v1"
},{
    "category": "cs.MM", 
    "author": "Shohreh Kasaei", 
    "title": "A Novel Boundary Matching Algorithm for Video Temporal Error Concealment", 
    "publish": "2016-10-25T07:11:41Z", 
    "summary": "With the fast growth of communication networks, the video data transmission\nfrom these networks is extremely vulnerable. Error concealment is a technique\nto estimate the damaged data by employing the correctly received data at the\ndecoder. In this paper, an efficient boundary matching algorithm for estimating\ndamaged motion vectors (MVs) is proposed. The proposed algorithm performs error\nconcealment for each damaged macro block (MB) according to the list of\nidentified priority of each frame. It then uses a classic boundary matching\ncriterion or the proposed boundary matching criterion adaptively to identify\nmatching distortion in each boundary of candidate MB. Finally, the candidate MV\nwith minimum distortion is selected as an MV of damaged MB and the list of\npriorities is updated. Experimental results show that the proposed algorithm\nimproves both objective and subjective qualities of reconstructed frames\nwithout any significant increase in computational cost. The PSNR for test\nsequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to\nthe classic boundary matching, directional boundary matching, and directional\ntemporal boundary matching algorithm, respectively.", 
    "link": "http://arxiv.org/pdf/1610.07753v1", 
    "arxiv-id": "1610.07753v1"
},{
    "category": "cs.MM", 
    "author": "Yongdong Zhang", 
    "title": "Image Credibility Analysis with Effective Domain Transferred Deep   Networks", 
    "publish": "2016-11-16T15:45:19Z", 
    "summary": "Numerous fake images spread on social media today and can severely jeopardize\nthe credibility of online content to public. In this paper, we employ deep\nnetworks to learn distinct fake image related features. In contrast to\nauthentic images, fake images tend to be eye-catching and visually striking.\nCompared with traditional visual recognition tasks, it is extremely challenging\nto understand these psychologically triggered visual patterns in fake images.\nTraditional general image classification datasets, such as ImageNet set, are\ndesigned for feature learning at the object level but are not suitable for\nlearning the hyper-features that would be required by image credibility\nanalysis. In order to overcome the scarcity of training samples of fake images,\nwe first construct a large-scale auxiliary dataset indirectly related to this\ntask. This auxiliary dataset contains 0.6 million weakly-labeled fake and real\nimages collected automatically from social media. Through an AdaBoost-like\ntransfer learning algorithm, we train a CNN model with a few instances in the\ntarget training set and 0.6 million images in the collected auxiliary set. This\nlearning algorithm is able to leverage knowledge from the auxiliary set and\ngradually transfer it to the target task. Experiments on a real-world testing\nset show that our proposed domain transferred CNN model outperforms several\ncompeting baselines. It obtains superiror results over transfer learning\nmethods based on the general ImageNet set. Moreover, case studies show that our\nproposed method reveals some interesting patterns for distinguishing fake and\nauthentic images.", 
    "link": "http://arxiv.org/pdf/1611.05328v1", 
    "arxiv-id": "1611.05328v1"
},{
    "category": "cs.CV", 
    "author": "Zhenmin Tang", 
    "title": "Exploiting Web Images for Dataset Construction: A Domain Robust Approach", 
    "publish": "2016-11-22T06:22:19Z", 
    "summary": "Labelled image datasets have played a critical role in high-level image\nunderstanding. However, the process of manual labelling is both time-consuming\nand labor intensive. To reduce the cost of manual labelling, there has been\nincreased research interest in automatically constructing image datasets by\nexploiting web images. Datasets constructed by existing methods tend to have a\nweak domain adaptation ability, which is known as the \"dataset bias problem\".\nTo address this issue, we present a novel image dataset construction framework\nthat can be generalized well to unseen target domains. Specifically, the given\nqueries are first expanded by searching the Google Books Ngrams Corpus to\nobtain a rich semantic description, from which the visually non-salient and\nless relevant expansions are filtered out. By treating each selected expansion\nas a \"bag\" and the retrieved images as \"instances\", image selection can be\nformulated as a multi-instance learning problem with constrained positive bags.\nWe propose to solve the employed problems by the cutting-plane and\nconcave-convex procedure (CCCP) algorithm. By using this approach, images from\ndifferent distributions can be kept while noisy images are filtered out. To\nverify the effectiveness of our proposed approach, we build an image dataset\nwith 20 categories. Extensive experiments on image classification,\ncross-dataset generalization, diversity comparison and object detection\ndemonstrate the domain robustness of our dataset.", 
    "link": "http://arxiv.org/pdf/1611.07156v2", 
    "arxiv-id": "1611.07156v2"
},{
    "category": "cs.SD", 
    "author": "Leonard Johard", 
    "title": "MOMOS-MT: Mobile Monophonic System for Music Transcription", 
    "publish": "2016-11-22T15:18:31Z", 
    "summary": "Music holds a significant cultural role in social identity and in the\nencouragement of socialization. Technology, by the destruction of physical and\ncultural distance, has lead to many changes in musical themes and the complete\nloss of forms. Yet, it also allows for the preservation and distribution of\nmusic from societies without a history of written sheet music. This paper\npresents early work on a tool for musicians and ethnomusicologists to\ntranscribe sheet music from monophonic voiced pieces for preservation and\ndistribution. Using FFT, the system detects the pitch frequencies, also other\nmethods detect note durations, tempo, time signatures and generates sheet\nmusic. The final system is able to be used in mobile platforms allowing the\nuser to take recordings and produce sheet music in situ to a performance.", 
    "link": "http://arxiv.org/pdf/1611.07351v1", 
    "arxiv-id": "1611.07351v1"
},{
    "category": "cs.MM", 
    "author": "Heng Tao Shen", 
    "title": "Binary Subspace Coding for Query-by-Image Video Retrieval", 
    "publish": "2016-12-06T04:01:17Z", 
    "summary": "The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.", 
    "link": "http://arxiv.org/pdf/1612.01657v1", 
    "arxiv-id": "1612.01657v1"
},{
    "category": "cs.SD", 
    "author": "Gerhard Widmer", 
    "title": "Towards computer-assisted understanding of dynamics in symphonic music", 
    "publish": "2016-12-07T11:18:21Z", 
    "summary": "Many people enjoy classical symphonic music. Its diverse instrumentation\nmakes for a rich listening experience. This diversity adds to the conductor's\nexpressive freedom to shape the sound according to their imagination. As a\nresult, the same piece may sound quite differently from one conductor to\nanother. Differences in interpretation may be noticeable subjectively to\nlisteners, but they are sometimes hard to pinpoint, presumably because of the\nacoustic complexity of the sound. We describe a computational model that\ninterprets dynamics---expressive loudness variations in performances---in terms\nof the musical score, highlighting differences between performances of the same\npiece. We demonstrate experimentally that the model has predictive power, and\ngive examples of conductor ideosyncrasies found by using the model as an\nexplanatory tool. Although the present model is still in active development, it\nmay pave the road for a consumer-oriented companion to interactive classical\nmusic understanding.", 
    "link": "http://arxiv.org/pdf/1612.02198v2", 
    "arxiv-id": "1612.02198v2"
},{
    "category": "cs.MM", 
    "author": "Poorna Banerjee Dasgupta", 
    "title": "Algorithmic Analysis of Invisible Video Watermarking using LSB Encoding   Over a Client-Server Framework", 
    "publish": "2016-07-03T12:39:53Z", 
    "summary": "Video watermarking is extensively used in many media-oriented applications\nfor embedding watermarks, i.e. hidden digital data, in a video sequence to\nprotect the video from illegal copying and to identify manipulations made in\nthe video. In case of an invisible watermark, the human eye can not perceive\nany difference in the video, but a watermark extraction application can read\nthe watermark and obtain the embedded information. Although numerous\nmethodologies exist for embedding watermarks, many of them have shortcomings\nwith respect to performance efficiency, especially over a distributed network.\nThis paper proposes and analyses a 2-bit Least Significant Bit (LSB) parallel\nalgorithmic approach for achieving performance efficiency to watermark and\ndistribute videos over a client-server framework.", 
    "link": "http://arxiv.org/pdf/1612.04688v1", 
    "arxiv-id": "1612.04688v1"
},{
    "category": "cs.IR", 
    "author": "Cees G. M. Snoek", 
    "title": "Video Stream Retrieval of Unseen Queries using Semantic Memory", 
    "publish": "2016-12-20T16:59:24Z", 
    "summary": "Retrieval of live, user-broadcast video streams is an under-addressed and\nincreasingly relevant challenge. The on-line nature of the problem requires\ntemporal evaluation and the unforeseeable scope of potential queries motivates\nan approach which can accommodate arbitrary search queries. To account for the\nbreadth of possible queries, we adopt a no-example approach to query retrieval,\nwhich uses a query's semantic relatedness to pre-trained concept classifiers.\nTo adapt to shifting video content, we propose memory pooling and memory\nwelling methods that favor recent information over long past content. We\nidentify two stream retrieval tasks, instantaneous retrieval at any particular\ntime and continuous retrieval over a prolonged duration, and propose means for\nevaluating them. Three large scale video datasets are adapted to the challenge\nof stream retrieval. We report results for our search methods on the new stream\nretrieval tasks, as well as demonstrate their efficacy in a traditional,\nnon-streaming video task.", 
    "link": "http://arxiv.org/pdf/1612.06753v1", 
    "arxiv-id": "1612.06753v1"
},{
    "category": "cs.CV", 
    "author": "Shawn Newsam", 
    "title": "Efficient Action Detection in Untrimmed Videos via Multi-Task Learning", 
    "publish": "2016-12-22T00:37:42Z", 
    "summary": "This paper studies the joint learning of action recognition and temporal\nlocalization in long, untrimmed videos. We employ a multi-task learning\nframework that performs the three highly related steps of action proposal,\naction recognition, and action localization refinement in parallel instead of\nthe standard sequential pipeline that performs the steps in order. We develop a\nnovel temporal actionness regression module that estimates what proportion of a\nclip contains action. We use it for temporal localization but it could have\nother applications like video retrieval, surveillance, summarization, etc. We\nalso introduce random shear augmentation during training to simulate viewpoint\nchange. We evaluate our framework on three popular video benchmarks. Results\ndemonstrate that our joint model is efficient in terms of storage and\ncomputation in that we do not need to compute and cache dense trajectory\nfeatures, and that it is several times faster than its sequential ConvNets\ncounterpart. Yet, despite being more efficient, it outperforms state-of-the-art\nmethods with respect to accuracy.", 
    "link": "http://arxiv.org/pdf/1612.07403v1", 
    "arxiv-id": "1612.07403v1"
},{
    "category": "cs.MM", 
    "author": "James Storer", 
    "title": "Semantic Perceptual Image Compression using Deep Convolution Networks", 
    "publish": "2016-12-27T19:21:18Z", 
    "summary": "It has long been considered a significant problem to improve the visual\nquality of lossy image and video compression. Recent advances in computing\npower together with the availability of large training data sets has increased\ninterest in the application of deep learning cnns to address image recognition\nand image processing tasks. Here, we present a powerful cnn tailored to the\nspecific task of semantic image understanding to achieve higher visual quality\nin lossy compression. A modest increase in complexity is incorporated to the\nencoder which allows a standard, off-the-shelf jpeg decoder to be used. While\njpeg encoding may be optimized for generic images, the process is ultimately\nunaware of the specific content of the image to be compressed. Our technique\nmakes jpeg content-aware by designing and training a model to identify multiple\nsemantic regions in a given image. Unlike object detection techniques, our\nmodel does not require labeling of object positions and is able to identify\nobjects in a single pass. We present a new cnn architecture directed\nspecifically to image compression, which generates a map that highlights\nsemantically-salient regions so that they can be encoded at higher quality as\ncompared to background regions. By adding a complete set of features for every\nclass, and then taking a threshold over the sum of all feature activations, we\ngenerate a map that highlights semantically-salient regions so that they can be\nencoded at a better quality compared to background regions. Experiments are\npresented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset,\nin which our algorithm achieves higher visual quality for the same compressed\nsize.", 
    "link": "http://arxiv.org/pdf/1612.08712v1", 
    "arxiv-id": "1612.08712v1"
},{
    "category": "cs.MM", 
    "author": "Gaurav Sharma", 
    "title": "Creating A Musical Performance Dataset for Multimodal Music Analysis:   Challenges, Insights, and Applications", 
    "publish": "2016-12-27T20:27:24Z", 
    "summary": "We introduce a dataset for facilitating audio-visual analysis of musical\nperformances. The dataset comprises a number of simple multi-instrument musical\npieces assembled from coordinated but separately recorded performances of\nindividual tracks. For each piece, we provide the musical score in MIDI format,\nthe audio recordings of the individual tracks, the audio and video recording of\nthe assembled mixture, and ground-truth annotation files including frame-level\nand note-level transcriptions. We anticipate that the dataset will be useful\nfor developing and evaluating multi-modal techniques for music source\nseparation, transcription, score following, and performance analysis. We\ndescribe our methodology for the creation of this dataset, particularly\nhighlighting our approaches for addressing the challenges involved in\nmaintaining synchronization and naturalness. We briefly discuss the research\nquestions that can be investigated with this dataset.", 
    "link": "http://arxiv.org/pdf/1612.08727v1", 
    "arxiv-id": "1612.08727v1"
},{
    "category": "cs.NI", 
    "author": "Pascal Frossard", 
    "title": "Price-based Controller for Quality-Fair HTTP Adaptive Streaming   (Extended Version)", 
    "publish": "2017-01-05T17:16:58Z", 
    "summary": "HTTP adaptive streaming (HAS) has become the universal technology for video\nstreaming over the Internet. Many HAS system designs aim at sharing the network\nbandwidth in a rate-fair manner. However, rate fairness is in general not\nequivalent to quality fairness as different video sequences might have\ndifferent characteristics and resource requirements. In this work, we focus on\nthis limitation and propose a novel controller for HAS clients that is able to\nreach quality fairness while preserving the main characteristics of HAS systems\nand with a limited support from the network devices. In particular, we adopt a\nprice-based mechanism in order to build a controller that maximizes the\naggregate video quality for a set of HAS clients that share a common\nbottleneck. When network resources are scarce, the clients with simple video\nsequences reduce the requested bitrate in favor of users that subscribe to more\ncomplex video sequences, leading to a more efficient network usage. The\nproposed controller has been implemented in a network simulator, and the\nsimulation results demonstrate its ability to share the available bandwidth\namong the HAS users in a quality-fair manner.", 
    "link": "http://arxiv.org/pdf/1701.01392v1", 
    "arxiv-id": "1701.01392v1"
},{
    "category": "cs.CV", 
    "author": "Pascal Frossard", 
    "title": "Light Field Super-Resolution Via Graph-Based Regularization", 
    "publish": "2017-01-09T11:32:30Z", 
    "summary": "Light field cameras can capture the 3D information in a scene with a single\nshot. This special feature makes light field cameras very appealing for a\nvariety of applications: from the popular post-capture refocus, to depth\nestimation and image-based rendering. However, light field cameras suffer by\ndesign from strong limitations in their spatial resolution, which should\ntherefore be augmented by computational methods. On the one hand, off-the-shelf\nsingle-frame and multi-frame super-resolution algorithms are not ideal for\nlight field data, as they do not consider its particular structure. On the\nother hand, the few super-resolution algorithms explicitly tailored for light\nfield data exhibit significant limitations, such as the need to estimate an\nexplicit disparity map at each view. In this work we propose a new light field\nsuper-resolution algorithm meant to address these limitations. We adopt a\nmulti-frame alike super-resolution approach, where the complementary\ninformation in the different light field views is used to augment the spatial\nresolution of the whole light field. We show that coupling the multi-frame\napproach with a graph regularizer, that enforces the light field structure via\nnon local self similarities, permits to avoid the costly and challenging\ndisparity estimation step for all the views. Extensive experiments show that\nthe proposed algorithm compares favorably to the other state-of-the-art methods\nfor light field super-resolution, both in terms of PSNR and in terms of visual\nquality. Moreover, differently from the other light field super-resolution\nmethods, the new algorithm provides reconstructed light field views with\nuniform quality, which happens to be an important feature for any light field\napplication.", 
    "link": "http://arxiv.org/pdf/1701.02141v1", 
    "arxiv-id": "1701.02141v1"
},{
    "category": "cs.MM", 
    "author": "Shivendra Panwar", 
    "title": "WiLiTV: A Low-Cost Wireless Framework for Live TV Services", 
    "publish": "2017-01-10T16:31:37Z", 
    "summary": "With the evolution of HDTV and Ultra HDTV, the bandwidth requirement for\nIP-based TV content is rapidly increasing. Consumers demand uninterrupted\nservice with a high Quality of Experience (QoE). Service providers are\nconstantly trying to differentiate themselves by innovating new ways of\ndistributing content more efficiently with lower cost and higher penetration.\nIn this work, we propose a cost-efficient wireless framework (WiLiTV) for\ndelivering live TV services, consisting of a mix of wireless access\ntechnologies (e.g. Satellite, WiFi and LTE overlay links). In the proposed\narchitecture, live TV content is injected into the network at a few residential\nlocations using satellite dishes. The content is then further distributed to\nother homes using a house-to-house WiFi network or via an overlay LTE network.\nOur problem is to construct an optimal TV distribution network with the minimum\nnumber of satellite injection points, while preserving the highest QoE, for\ndifferent neighborhood densities. We evaluate the framework using realistic\ntime-varying demand patterns and a diverse set of home location data. Our study\ndemonstrates that the architecture requires 75 - 90% fewer satellite injection\npoints, compared to traditional architectures. Furthermore, we show that most\ncost savings can be obtained using simple and practical relay routing\nsolutions.", 
    "link": "http://arxiv.org/pdf/1701.02669v1", 
    "arxiv-id": "1701.02669v1"
},{
    "category": "cs.MM", 
    "author": "Chaokun Wang", 
    "title": "Investigating the role of musical genre in human perception of music   stretching resistance", 
    "publish": "2017-01-12T09:26:22Z", 
    "summary": "To stretch a music piece to a given length is a common demand in people's\ndaily lives, e.g., in audio-video synchronization and animation production.\nHowever, it is not always guaranteed that the stretched music piece is\nacceptable for general audience since music stretching suffers from people's\nperceptual artefacts. Over-stretching a music piece will make it uncomfortable\nfor human psychoacoustic hearing. The research on music stretching resistance\nattempts to estimate the maximum stretchability of music pieces to further\navoid over-stretch. It has been observed that musical genres can significantly\nimprove the accuracy of automatic estimation of music stretching resistance,\nbut how musical genres are related to music stretching resistance has never\nbeen explained or studied in detail in the literature. In this paper, the\ncharacteristics of music stretching resistance are compared across different\nmusical genres. It is found that music stretching resistance has strong\nintra-genre cohesiveness and inter-genre discrepancies in the experiments.\nMoreover, the ambiguity and the symmetry of music stretching resistance are\nalso observed in the experimental analysis. These findings lead to a new\nmeasurement on the similarity between different musical genres based on their\nmusic stretching resistance. In addition, the analysis of variance (ANOVA) also\nsupports the findings in this paper by verifying the significance of musical\ngenre in shaping music stretching resistance.", 
    "link": "http://arxiv.org/pdf/1701.03274v1", 
    "arxiv-id": "1701.03274v1"
},{
    "category": "cs.HC", 
    "author": "Miguel P. Eckstein", 
    "title": "Attention Allocation Aid for Visual Search", 
    "publish": "2017-01-14T21:58:28Z", 
    "summary": "This paper outlines the development and testing of a novel, feedback-enabled\nattention allocation aid (AAAD), which uses real-time physiological data to\nimprove human performance in a realistic sequential visual search task. Indeed,\nby optimizing over search duration, the aid improves efficiency, while\npreserving decision accuracy, as the operator identifies and classifies targets\nwithin simulated aerial imagery. Specifically, using experimental eye-tracking\ndata and measurements about target detectability across the human visual field,\nwe develop functional models of detection accuracy as a function of search\ntime, number of eye movements, scan path, and image clutter. These models are\nthen used by the AAAD in conjunction with real time eye position data to make\nprobabilistic estimations of attained search accuracy and to recommend that the\nobserver either move on to the next image or continue exploring the present\nimage. An experimental evaluation in a scenario motivated from human\nsupervisory control in surveillance missions confirms the benefits of the AAAD.", 
    "link": "http://arxiv.org/pdf/1701.03968v1", 
    "arxiv-id": "1701.03968v1"
},{
    "category": "cs.MM", 
    "author": "Tomoyoshi Ito", 
    "title": "Inkjet printing-based volumetric display projecting multiple full-colour   2D patterns", 
    "publish": "2017-02-01T10:01:44Z", 
    "summary": "In this study, a method to construct a full-colour volumetric display is\npresented using a commercially available inkjet printer. Photoreactive\nluminescence materials are minutely and automatically printed as the volume\nelements, and volumetric displays are constructed with high resolution using\neasy-to-fabricate means that exploit inkjet printing technologies. The results\nexperimentally demonstrate the first prototype of an inkjet printing-based\nvolumetric display composed of multiple layers of transparent films that yield\na full-colour three-dimensional (3D) image. Moreover, we propose a design\nalgorithm with 3D structures that provide multiple different 2D full-colour\npatterns when viewed from different directions and experimentally demonstrates\nprototypes. It is considered that these types of 3D volumetric structures and\ntheir fabrication methods based on widely deployed existing printing\ntechnologies can be utilised as novel information display devices and systems,\nincluding digital signage, media art, entertainment and security.", 
    "link": "http://arxiv.org/pdf/1702.00182v1", 
    "arxiv-id": "1702.00182v1"
},{
    "category": "cs.CV", 
    "author": "In So Kweon", 
    "title": "Textually Customized Video Summaries", 
    "publish": "2017-02-06T08:31:44Z", 
    "summary": "The best summary of a long video differs among different people due to its\nhighly subjective nature. Even for the same person, the best summary may change\nwith time or mood. In this paper, we introduce the task of generating\ncustomized video summaries through simple text. First, we train a deep\narchitecture to effectively learn semantic embeddings of video frames by\nleveraging the abundance of image-caption data via a progressive and residual\nmanner. Given a user-specific text description, our algorithm is able to select\nsemantically relevant video segments and produce a temporally aligned video\nsummary. In order to evaluate our textually customized video summaries, we\nconduct experimental comparison with baseline methods that utilize ground-truth\ninformation. Despite the challenging baselines, our method still manages to\nshow comparable or even exceeding performance. We also show that our method is\nable to generate semantically diverse video summaries by only utilizing the\nlearned visual embeddings.", 
    "link": "http://arxiv.org/pdf/1702.01528v1", 
    "arxiv-id": "1702.01528v1"
},{
    "category": "cs.DL", 
    "author": "Bernd Freisleben", 
    "title": "Content-Based Video Retrieval in Historical Collections of the German   Broadcasting Archive", 
    "publish": "2017-02-13T14:42:31Z", 
    "summary": "The German Broadcasting Archive (DRA) maintains the cultural heritage of\nradio and television broadcasts of the former German Democratic Republic (GDR).\nThe uniqueness and importance of the video material stimulates a large\nscientific interest in the video content. In this paper, we present an\nautomatic video analysis and retrieval system for searching in historical\ncollections of GDR television recordings. It consists of video analysis\nalgorithms for shot boundary detection, concept classification, person\nrecognition, text recognition and similarity search. The performance of the\nsystem is evaluated from a technical and an archival perspective on 2,500 hours\nof GDR television recordings.", 
    "link": "http://arxiv.org/pdf/1702.03790v1", 
    "arxiv-id": "1702.03790v1"
},{
    "category": "cs.MM", 
    "author": "Ramesh Jain", 
    "title": "From Photo Streams to Evolving Situations", 
    "publish": "2017-02-20T06:53:21Z", 
    "summary": "Photos are becoming spontaneous, objective, and universal sources of\ninformation. This paper develops evolving situation recognition using photo\nstreams coming from disparate sources combined with the advances of deep\nlearning. Using visual concepts in photos together with space and time\ninformation, we formulate the situation detection into a semi-supervised\nlearning framework and propose new graph-based models to solve the problem. To\nextend the method for unknown situations, we introduce a soft label method\nwhich enables the traditional semi-supervised learning framework to accurately\npredict predefined labels as well as effectively form new clusters. To overcome\nthe noisy data which degrades graph quality, leading to poor recognition\nresults, we take advantage of two kinds of noise-robust norms which can\neliminate the adverse effects of outliers in visual concepts and improve the\naccuracy of situation recognition. Finally, we demonstrate the idea and the\neffectiveness of the proposed model on Yahoo Flickr Creative Commons 100\nMillion.", 
    "link": "http://arxiv.org/pdf/1702.05878v1", 
    "arxiv-id": "1702.05878v1"
},{
    "category": "cs.HC", 
    "author": "Zhenguang Liu", 
    "title": "I Ate This: A Photo-based Food Journaling System with Expert Feedback", 
    "publish": "2017-02-20T13:13:53Z", 
    "summary": "What we eat is one of the most frequent and important health decisions we\nmake in daily life, yet it remains notoriously difficult to capture and\nunderstand. Effective food journaling is thus a grand challenge in personal\nhealth informatics. In this paper we describe a system for food journaling\ncalled I Ate This, which is inspired by the Remote Food Photography Method\n(RFPM). I Ate This is simple: you use a smartphone app to take a photo and give\na very basic description of any food or beverage you are about to consume.\nLater, a qualified dietitian will evaluate your photo, giving you feedback on\nhow you did and where you can improve. The aim of I Ate This is to provide a\nconvenient, visual and reliable way to help users learn from their eating\nhabits and nudge them towards better choices each and every day. Ultimately,\nthis incremental approach can lead to long-term behaviour change. Our goal is\nto bring RFPM to a wider audience, through APIs that can be incorporated into\nother apps.", 
    "link": "http://arxiv.org/pdf/1702.05957v2", 
    "arxiv-id": "1702.05957v2"
},{
    "category": "cs.MM", 
    "author": "Houqiang Li", 
    "title": "Projection based advanced motion model for cubic mapping for 360-degree   video", 
    "publish": "2017-02-21T06:35:01Z", 
    "summary": "This paper proposes a novel advanced motion model to handle the irregular\nmotion for the cubic map projection of 360-degree video. Since the irregular\nmotion is mainly caused by the projection from the sphere to the cube map, we\nfirst try to project the pixels in both the current picture and reference\npicture from unfolding cube back to the sphere. Then through utilizing the\ncharacteristic that most of the motions in the sphere are uniform, we can\nderive the relationship between the motion vectors of various pixels in the\nunfold cube. The proposed advanced motion model is implemented in the High\nEfficiency Video Coding reference software. Experimental results demonstrate\nthat quite obvious performance improvement can be achieved for the sequences\nwith obvious motions.", 
    "link": "http://arxiv.org/pdf/1702.06277v1", 
    "arxiv-id": "1702.06277v1"
},{
    "category": "cs.CR", 
    "author": "Adrian G. Bors", 
    "title": "Steganalysis of 3D Objects Using Statistics of Local Feature Sets", 
    "publish": "2017-02-23T11:24:03Z", 
    "summary": "3D steganalysis aims to identify subtle invisible changes produced in\ngraphical objects through digital watermarking or steganography. Sets of\nstatistical representations of 3D features, extracted from both cover and stego\n3D mesh objects, are used as inputs into machine learning classifiers in order\nto decide whether any information was hidden in the given graphical object.\nAccording to previous studies, sets of local geometry features can be used to\ndefine the differences between stego and cover-objects. The features proposed\nin this paper include those representing the local object curvature, vertex\nnormals, the local geometry representation in the spherical coordinate system\nand are considered in various combinations with others. We also analyze the\neffectiveness of various 3D feature sets applied for steganalysis based on the\nPearson correlation coefficient. The classifiers proposed in this study for\ndiscriminating the 3D stego and cover-objects include Support Vector Machine\nand the Fisher Linear Discriminant ensemble. Three different watermarking and\nsteganographic methods are used for hiding information in the 3D objects used\nfor testing the performance of the proposed steganalysis methodology.", 
    "link": "http://arxiv.org/pdf/1702.07178v1", 
    "arxiv-id": "1702.07178v1"
},{
    "category": "cs.MM", 
    "author": "Hiroshi Esaki", 
    "title": "Software Defined Media: Virtualization of Audio-Visual Services", 
    "publish": "2017-02-24T03:09:25Z", 
    "summary": "Internet-native audio-visual services are witnessing rapid development. Among\nthese services, object-based audio-visual services are gaining importance. In\n2014, we established the Software Defined Media (SDM) consortium to target new\nresearch areas and markets involving object-based digital media and\nInternet-by-design audio-visual environments. In this paper, we introduce the\nSDM architecture that virtualizes networked audio-visual services along with\nthe development of smart buildings and smart cities using Internet of Things\n(IoT) devices and smart building facilities. Moreover, we design the SDM\narchitecture as a layered architecture to promote the development of innovative\napplications on the basis of rapid advancements in software-defined networking\n(SDN). Then, we implement a prototype system based on the architecture, present\nthe system at an exhibition, and provide it as an SDM API to application\ndevelopers at hackathons. Various types of applications are developed using the\nAPI at these events. An evaluation of SDM API access shows that the prototype\nSDM platform effectively provides 3D audio reproducibility and interactiveness\nfor SDM applications.", 
    "link": "http://arxiv.org/pdf/1702.07452v1", 
    "arxiv-id": "1702.07452v1"
},{
    "category": "cs.SD", 
    "author": "Elizabeth Wiebel", 
    "title": "Data sonification and sound visualization", 
    "publish": "2000-07-05T21:26:48Z", 
    "summary": "This article describes a collaborative project between researchers in the\nMathematics and Computer Science Division at Argonne National Laboratory and\nthe Computer Music Project of the University of Illinois at Urbana-Champaign.\nThe project focuses on the use of sound for the exploration and analysis of\ncomplex data sets in scientific computing. The article addresses digital sound\nsynthesis in the context of DIASS (Digital Instrument for Additive Sound\nSynthesis) and sound visualization in a virtual-reality environment by means of\nM4CAVE. It describes the procedures and preliminary results of some experiments\nin scientific sonification and sound visualization.", 
    "link": "http://arxiv.org/pdf/cs/0007007v1", 
    "arxiv-id": "cs/0007007v1"
},{
    "category": "cs.CR", 
    "author": "Dan Zhang", 
    "title": "On the security of the Yen-Guo's domino signal encryption algorithm   (DSEA)", 
    "publish": "2005-01-08T15:35:13Z", 
    "summary": "Recently, a new domino signal encryption algorithm (DSEA) was proposed for\ndigital signal transmission, especially for digital images and videos. This\npaper analyzes the security of DSEA, and points out the following weaknesses:\n1) its security against the brute-force attack was overestimated; 2) it is not\nsufficiently secure against ciphertext-only attacks, and only one ciphertext is\nenough to get some information about the plaintext and to break the value of a\nsub-key; 3) it is insecure against known/chosen-plaintext attacks, in the sense\nthat the secret key can be recovered from a number of continuous bytes of only\none known/chosen plaintext and the corresponding ciphertext. Experimental\nresults are given to show the performance of the proposed attacks, and some\ncountermeasures are discussed to improve DSEA.", 
    "link": "http://arxiv.org/pdf/cs/0501013v2", 
    "arxiv-id": "cs/0501013v2"
},{
    "category": "cs.IT", 
    "author": "Brian Chen", 
    "title": "Authentication with Distortion Criteria", 
    "publish": "2005-03-12T21:13:21Z", 
    "summary": "In a variety of applications, there is a need to authenticate content that\nhas experienced legitimate editing in addition to potential tampering attacks.\nWe develop one formulation of this problem based on a strict notion of\nsecurity, and characterize and interpret the associated information-theoretic\nperformance limits. The results can be viewed as a natural generalization of\nclassical approaches to traditional authentication. Additional insights into\nthe structure of such systems and their behavior are obtained by further\nspecializing the results to Bernoulli and Gaussian cases. The associated\nsystems are shown to be substantially better in terms of performance and/or\nsecurity than commonly advocated approaches based on data hiding and digital\nwatermarking. Finally, the formulation is extended to obtain efficient layered\nauthentication system constructions.", 
    "link": "http://arxiv.org/pdf/cs/0503027v2", 
    "arxiv-id": "cs/0503027v2"
},{
    "category": "cs.MM", 
    "author": "Jonathan P. Bowen", 
    "title": "Can Small Museums Develop Compelling, Educational and Accessible Web   Resources? The Case of Accademia Carrara", 
    "publish": "2005-08-13T14:46:16Z", 
    "summary": "Due to the lack of budget, competence, personnel and time, small museums are\noften unable to develop compelling, educational and accessible web resources\nfor their permanent collections or temporary exhibitions. In an attempt to\nprove that investing in these types of resources can be very fruitful even for\nsmall institutions, we will illustrate the case of Accademia Carrara, a museum\nin Bergamo, northern Italy, which, for a current temporary exhibition on\nCezanne and Renoir's masterpieces from the Paul Guillaume collection, developed\na series of multimedia applications, including an accessible website, rich in\ncontent and educational material [www.cezannerenoir.it].", 
    "link": "http://arxiv.org/pdf/cs/0508066v1", 
    "arxiv-id": "cs/0508066v1"
},{
    "category": "cs.IT", 
    "author": "Ender Ayanoglu", 
    "title": "Multimedia Capacity Analysis of the IEEE 802.11e Contention-based   Infrastructure Basic Service Set", 
    "publish": "2007-07-19T05:16:53Z", 
    "summary": "We first propose a simple mathematical analysis framework for the Enhanced\nDistributed Channel Access (EDCA) function of the recently ratified IEEE\n802.11e standard. Our analysis considers the fact that the distributed random\naccess systems exhibit cyclic behavior. The proposed model is valid for\narbitrary assignments of AC-specific Arbitration Interframe Space (AIFS) values\nand Contention Window (CW) sizes and is the first that considers an arbitrary\ndistribution of active Access Categories (ACs) at the stations. Validating the\ntheoretical results via extensive simulations, we show that the proposed\nanalysis accurately captures the EDCA saturation performance. Next, we propose\na framework for multimedia capacity analysis of the EDCA function. We calculate\nan accurate station- and AC-specific queue utilization ratio by appropriately\nweighing the service time predictions of the cycle time model for different\nnumber of active stations. Based on the calculated queue utilization ratio, we\ndesign a simple model-based admission control scheme. We show that the proposed\ncall admission control algorithm maintains satisfactory user-perceived quality\nfor coexisting voice and video connections in an infrastructure BSS and does\nnot present over- or under-admission problems of previously proposed models in\nthe literature.", 
    "link": "http://arxiv.org/pdf/0707.2836v3", 
    "arxiv-id": "0707.2836v3"
},{
    "category": "nlin.CD", 
    "author": "Wolfgang A. Halang", 
    "title": "Cryptanalysis of an image encryption scheme based on a new total   shuffling algorithm", 
    "publish": "2007-10-29T16:00:56Z", 
    "summary": "Chaotic systems have been broadly exploited through the last two decades to\nbuild encryption methods. Recently, two new image encryption schemes have been\nproposed, where the encryption process involves a permutation operation and an\nXOR-like transformation of the shuffled pixels, which are controlled by three\nchaotic systems. This paper discusses some defects of the schemes and how to\nbreak them with a chosen-plaintext attack.", 
    "link": "http://arxiv.org/pdf/0710.5465v1", 
    "arxiv-id": "0710.5465v1"
},{
    "category": "cs.IT", 
    "author": "David J. Love", 
    "title": "On the Capacity and Design of Limited Feedback Multiuser MIMO Uplinks", 
    "publish": "2008-02-22T04:56:57Z", 
    "summary": "The theory of multiple-input multiple-output (MIMO) technology has been\nwell-developed to increase fading channel capacity over single-input\nsingle-output (SISO) systems. This capacity gain can often be leveraged by\nutilizing channel state information at the transmitter and the receiver. Users\nmake use of this channel state information for transmit signal adaptation. In\nthis correspondence, we derive the capacity region for the MIMO multiple access\nchannel (MIMO MAC) when partial channel state information is available at the\ntransmitters, where we assume a synchronous MIMO multiuser uplink. The partial\nchannel state information feedback has a cardinality constraint and is fed back\nfrom the basestation to the users using a limited rate feedback channel. Using\nthis feedback information, we propose a finite codebook design method to\nmaximize sum-rate. In this correspondence, the codebook is a set of transmit\nsignal covariance matrices. We also derive the capacity region and codebook\ndesign methods in the case that the covariance matrix is rank-one (i.e.,\nbeamforming). This is motivated by the fact that beamforming is optimal in\ncertain conditions. The simulation results show that when the number of\nfeedback bits increases, the capacity also increases. Even with a small number\nof feedback bits, the performance of the proposed system is close to an optimal\nsolution with the full feedback.", 
    "link": "http://arxiv.org/pdf/0802.3253v1", 
    "arxiv-id": "0802.3253v1"
},{
    "category": "cs.MM", 
    "author": "Keith Ross", 
    "title": "Characterizing Video Responses in Social Networks", 
    "publish": "2008-04-30T16:39:32Z", 
    "summary": "Video sharing sites, such as YouTube, use video responses to enhance the\nsocial interactions among their users. The video response feature allows users\nto interact and converse through video, by creating a video sequence that\nbegins with an opening video and followed by video responses from other users.\nOur characterization is over 3.4 million videos and 400,000 video responses\ncollected from YouTube during a 7-day period. We first analyze the\ncharacteristics of the video responses, such as popularity, duration, and\ngeography. We then examine the social networks that emerge from the video\nresponse interactions.", 
    "link": "http://arxiv.org/pdf/0804.4865v1", 
    "arxiv-id": "0804.4865v1"
},{
    "category": "cs.MM", 
    "author": "Andreas U. Schmidt", 
    "title": "On the Superdistribution of Digital Goods", 
    "publish": "2008-06-09T22:03:40Z", 
    "summary": "Business models involving buyers of digital goods in the distribution process\nare called superdistribution schemes. We review the state-of-the art of\nresearch and application of superdistribution and propose systematic approach\nto market mechanisms using super-distribution and technical system\narchitectures supporting it. The limiting conditions on such markets are of\neconomic, legal, technical, and psychological nature.", 
    "link": "http://arxiv.org/pdf/0806.1543v1", 
    "arxiv-id": "0806.1543v1"
},{
    "category": "cs.MM", 
    "author": "Eunmi L. Oh", 
    "title": "Scalar Quantization for Audio Data Coding", 
    "publish": "2008-06-26T12:19:27Z", 
    "summary": "This paper is concerned with scalar quantization of transform coefficients in\nan audio codec. The generalized Gaussian distribution (GGD) is used as an\napproximation of one-dimensional probability density function for transform\ncoefficients obtained by modulated lapped transform (MLT) or modified cosine\ntransform (MDCT) filterbank. The rationale of the model is provided in\ncomparison with theoretically achievable rate-distortion function. The\nrate-distortion function computed for the random sequence obtained from a real\nsequence of samples from a large database is compared with that computed for\nrandom sequence obtained by a GGD random generator. A simple algorithm of\nconstructing the Extended Zero Zone (EZZ) quantizer is proposed. Simulation\nresults show that the EZZ quantizer yields a negligible loss in terms of coding\nefficiency compared to optimal scalar quantizers. Furthermore, we describe an\nadaptive version of the EZZ quantizer which works efficiently with low bitrate\nrequirements for transmitting side information", 
    "link": "http://arxiv.org/pdf/0806.4293v1", 
    "arxiv-id": "0806.4293v1"
},{
    "category": "cs.MM", 
    "author": "Christian Jutten", 
    "title": "Approximate Sparse Decomposition Based on Smoothed L0-Norm", 
    "publish": "2008-11-18T09:14:18Z", 
    "summary": "In this paper, we propose a method to address the problem of source\nestimation for Sparse Component Analysis (SCA) in the presence of additive\nnoise. Our method is a generalization of a recently proposed method (SL0),\nwhich has the advantage of directly minimizing the L0-norm instead of L1-norm,\nwhile being very fast. SL0 is based on minimization of the smoothed L0-norm\nsubject to As=x. In order to better estimate the source vector for noisy\nmixtures, we suggest then to remove the constraint As=x, by relaxing exact\nequality to an approximation (we call our method Smoothed L0-norm Denoising or\nSL0DN). The final result can then be obtained by minimization of a proper\nlinear combination of the smoothed L0-norm and a cost function for the\napproximation. Experimental results emphasize on the significant enhancement of\nthe modified method in noisy cases.", 
    "link": "http://arxiv.org/pdf/0811.2868v1", 
    "arxiv-id": "0811.2868v1"
},{
    "category": "cs.MM", 
    "author": "St\u00e9phane Pateux", 
    "title": "Wide spread spectrum watermarking with side information and interference   cancellation", 
    "publish": "2008-11-28T16:28:59Z", 
    "summary": "Nowadays, a popular method used for additive watermarking is wide spread\nspectrum. It consists in adding a spread signal into the host document. This\nsignal is obtained by the sum of a set of carrier vectors, which are modulated\nby the bits to be embedded. To extract these embedded bits, weighted\ncorrelations between the watermarked document and the carriers are computed.\nUnfortunately, even without any attack, the obtained set of bits can be\ncorrupted due to the interference with the host signal (host interference) and\nalso due to the interference with the others carriers (inter-symbols\ninterference (ISI) due to the non-orthogonality of the carriers). Some recent\nwatermarking algorithms deal with host interference using side informed\nmethods, but inter-symbols interference problem is still open. In this paper,\nwe deal with interference cancellation methods, and we propose to consider ISI\nas side information and to integrate it into the host signal. This leads to a\ngreat improvement of extraction performance in term of signal-to-noise ratio\nand/or watermark robustness.", 
    "link": "http://arxiv.org/pdf/0811.4483v1", 
    "arxiv-id": "0811.4483v1"
},{
    "category": "cs.IT", 
    "author": "Ga\u00ebtan Le Guelvouit", 
    "title": "Informed stego-systems in active warden context: statistical   undetectability and capacity", 
    "publish": "2008-11-28T12:04:51Z", 
    "summary": "Several authors have studied stego-systems based on Costa scheme, but just a\nfew ones gave both theoretical and experimental justifications of these schemes\nperformance in an active warden context. We provide in this paper a\nsteganographic and comparative study of three informed stego-systems in active\nwarden context: scalar Costa scheme, trellis-coded quantization and spread\ntransform scalar Costa scheme. By leading on analytical formulations and on\nexperimental evaluations, we show the advantages and limits of each scheme in\nterm of statistical undetectability and capacity in the case of active warden.\nSuch as the undetectability is given by the distance between the stego-signal\nand the cover distance. It is measured by the Kullback-Leibler distance.", 
    "link": "http://arxiv.org/pdf/0811.4697v1", 
    "arxiv-id": "0811.4697v1"
},{
    "category": "cs.MM", 
    "author": "Ga\u00ebtan Le Guelvouit", 
    "title": "Trellis-coded quantization for public-key steganography", 
    "publish": "2008-11-28T16:07:33Z", 
    "summary": "This paper deals with public-key steganography in the presence of a passive\nwarden. The aim is to hide secret messages within cover-documents without\nmaking the warden suspicious, and without any preliminar secret key sharing.\nWhereas a practical attempt has been already done to provide a solution to this\nproblem, it suffers of poor flexibility (since embedding and decoding steps\nhighly depend on cover-signals statistics) and of little capacity compared to\nrecent data hiding techniques. Using the same framework, this paper explores\nthe use of trellis-coded quantization techniques (TCQ and turbo TCQ) to design\na more efficient public-key scheme. Experiments on audio signals show great\nimprovements considering Cachin's security criterion.", 
    "link": "http://arxiv.org/pdf/0811.4700v1", 
    "arxiv-id": "0811.4700v1"
},{
    "category": "cs.IT", 
    "author": "Christine Guillemot", 
    "title": "Information-theoretic resolution of perceptual WSS watermarking of non   i.i.d. Gaussian signals", 
    "publish": "2008-11-28T12:41:23Z", 
    "summary": "The theoretical foundations of data hiding have been revealed by formulating\nthe problem as message communication over a noisy channel. We revisit the\nproblem in light of a more general characterization of the watermark channel\nand of weighted distortion measures. Considering spread spectrum based\ninformation hiding, we release the usual assumption of an i.i.d. cover signal.\nThe game-theoretic resolution of the problem reveals a generalized\ncharacterization of optimum attacks. The paper then derives closed-form\nexpressions for the different parameters exhibiting a practical embedding and\nextraction technique.", 
    "link": "http://arxiv.org/pdf/0811.4702v1", 
    "arxiv-id": "0811.4702v1"
},{
    "category": "cs.IT", 
    "author": "Gaurav Bhatnagar", 
    "title": "A new Contrast Based Image Fusion using Wavelet Packets", 
    "publish": "2008-12-03T17:29:19Z", 
    "summary": "Image Fusion, a technique which combines complimentary information from\ndifferent images of the same scene so that the fused image is more suitable for\nsegmentation, feature extraction, object recognition and Human Visual System.\nIn this paper, a simple yet efficient algorithm is presented based on contrast\nusing wavelet packet decomposition. First, all the source images are decomposed\ninto low and high frequency sub-bands and then fusion of high frequency\nsub-bands is done by the means of Directive Contrast. Now, inverse wavelet\npacket transform is performed to reconstruct the fused image. The performance\nof the algorithm is carried out by the comparison made between proposed and\nexisting algorithm.", 
    "link": "http://arxiv.org/pdf/0812.0759v1", 
    "arxiv-id": "0812.0759v1"
},{
    "category": "cs.CV", 
    "author": "Judith Kelner", 
    "title": "A Standalone Markerless 3D Tracker for Handheld Augmented Reality", 
    "publish": "2009-02-12T18:25:13Z", 
    "summary": "This paper presents an implementation of a markerless tracking technique\ntargeted to the Windows Mobile Pocket PC platform. The primary aim of this work\nis to allow the development of standalone augmented reality applications for\nhandheld devices based on natural feature tracking. In order to achieve this\ngoal, a subset of two computer vision libraries was ported to the Pocket PC\nplatform. They were also adapted to use fixed point math, with the purpose of\nimproving the overall performance of the routines. The port of these libraries\nopens up the possibility of having other computer vision tasks being executed\non mobile platforms. A model based tracking approach that relies on edge\ninformation was adopted. Since it does not require a high processing power, it\nis suitable for constrained devices such as handhelds. The OpenGL ES graphics\nlibrary was used to perform computer vision tasks, taking advantage of existing\ngraphics hardware acceleration. An augmented reality application was created\nusing the implemented technique and evaluations were done regarding tracking\nperformance and accuracy", 
    "link": "http://arxiv.org/pdf/0902.2187v1", 
    "arxiv-id": "0902.2187v1"
},{
    "category": "cs.DL", 
    "author": "Farshad Fotouhi", 
    "title": "ImageSpace: An Environment for Image Ontology Management", 
    "publish": "2009-02-17T17:28:25Z", 
    "summary": "More and more researchers have realized that ontologies will play a critical\nrole in the development of the Semantic Web, the next generation Web in which\ncontent is not only consumable by humans, but also by software agents. The\ndevelopment of tools to support ontology management including creation,\nvisualization, annotation, database storage, and retrieval is thus extremely\nimportant. We have developed ImageSpace, an image ontology creation and\nannotation tool that features (1) full support for the standard web ontology\nlanguage DAML+OIL; (2) image ontology creation, visualization, image annotation\nand display in one integrated framework; (3) ontology consistency assurance;\nand (4) storing ontologies and annotations in relational databases. It is\nexpected that the availability of such a tool will greatly facilitate the\ncreation of image repositories as islands of the Semantic Web.", 
    "link": "http://arxiv.org/pdf/0902.2953v1", 
    "arxiv-id": "0902.2953v1"
},{
    "category": "cs.DL", 
    "author": "Peter Wittenburg", 
    "title": "OntoELAN: An Ontology-based Linguistic Multimedia Annotator", 
    "publish": "2009-02-18T00:40:37Z", 
    "summary": "Despite its scientific, political, and practical value, comprehensive\ninformation about human languages, in all their variety and complexity, is not\nreadily obtainable and searchable. One reason is that many language data are\ncollected as audio and video recordings which imposes a challenge to document\nindexing and retrieval. Annotation of multimedia data provides an opportunity\nfor making the semantics explicit and facilitates the searching of multimedia\ndocuments. We have developed OntoELAN, an ontology-based linguistic multimedia\nannotator that features: (1) support for loading and displaying ontologies\nspecified in OWL; (2) creation of a language profile, which allows a user to\nchoose a subset of terms from an ontology and conveniently rename them if\nneeded; (3) creation of ontological tiers, which can be annotated with profile\nterms and, therefore, corresponding ontological terms; and (4) saving\nannotations in the XML format as Multimedia Ontology class instances and,\nlinked to them, class instances of other ontologies used in ontological tiers.\nTo our best knowledge, OntoELAN is the first audio/video annotation tool in\nlinguistic domain that provides support for ontology-based annotation.", 
    "link": "http://arxiv.org/pdf/0902.3026v1", 
    "arxiv-id": "0902.3026v1"
},{
    "category": "cs.DL", 
    "author": "Anthony Aristar", 
    "title": "Ontology-Based Annotation of Multimedia Language Data for the Semantic   Web", 
    "publish": "2009-02-18T01:16:50Z", 
    "summary": "There is an increasing interest and effort in preserving and documenting\nendangered languages. Language data are valuable only when they are\nwell-cataloged, indexed and searchable. Many language data, particularly those\nof lesser-spoken languages, are collected as audio and video recordings. While\nmultimedia data provide more channels and dimensions to describe a language's\nfunction, and gives a better presentation of the cultural system associated\nwith the language of that community, they are not text-based or structured (in\nbinary format), and their semantics is implicit in their content. The content\nis thus easy for a human being to understand, but difficult for computers to\ninterpret. Hence, there is a great need for a powerful and user-friendly system\nto annotate multimedia data with text-based, well-structured and searchable\nmetadata. This chapter describes an ontology-based multimedia annotation tool,\nOntoELAN, that enables annotation of language multimedia data with a linguistic\nontology.", 
    "link": "http://arxiv.org/pdf/0902.3027v1", 
    "arxiv-id": "0902.3027v1"
},{
    "category": "cs.MM", 
    "author": "Jian Zhang", 
    "title": "Efficiently Learning a Detection Cascade with Sparse Eigenvectors", 
    "publish": "2009-03-18T08:17:05Z", 
    "summary": "In this work, we first show that feature selection methods other than\nboosting can also be used for training an efficient object detector. In\nparticular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)\n\\cite{Moghaddam2007Fast} for its conceptual simplicity and computational\nefficiency; and slightly better detection performance is achieved compared with\n\\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted\nGreedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a\ndetection cascade. BGSLDA exploits the sample re-weighting property of boosting\nand the class-separability criterion of GSLDA.", 
    "link": "http://arxiv.org/pdf/0903.3103v1", 
    "arxiv-id": "0903.3103v1"
},{
    "category": "cs.NI", 
    "author": "Gregor v. Bochmann", 
    "title": "Overlay Structure for Large Scale Content Sharing: Leveraging Geography   as the Basis for Routing Locality", 
    "publish": "2009-03-24T16:16:26Z", 
    "summary": "In this paper we place our arguments on two related issues in the design of\ngeneralized structured peer-to-peer overlays. First, we argue that for the\nlarge-scale content-sharing applications, lookup and content transport\nfunctions need to be treated separately. Second, to create a location-based\nrouting overlay suitable for content sharing and other applications, we argue\nthat off-the-shelf geographic coordinates of Internet-connected hosts can be\nused as a basis. We then outline the design principles and present a design for\nthe generalized routing overlay based on adaptive hierarchical partitioning of\nthe geographical space.", 
    "link": "http://arxiv.org/pdf/0903.4113v1", 
    "arxiv-id": "0903.4113v1"
},{
    "category": "cs.NI", 
    "author": "Gregor v. Bochmann", 
    "title": "CliqueStream: an efficient and fault-resilient live streaming network on   a clustered peer-to-peer overlay", 
    "publish": "2009-03-25T15:58:47Z", 
    "summary": "Several overlay-based live multimedia streaming platforms have been proposed\nin the recent peer-to-peer streaming literature. In most of the cases, the\noverlay neighbors are chosen randomly for robustness of the overlay. However,\nthis causes nodes that are distant in terms of proximity in the underlying\nphysical network to become neighbors, and thus data travels unnecessary\ndistances before reaching the destination. For efficiency of bulk data\ntransmission like multimedia streaming, the overlay neighborhood should\nresemble the proximity in the underlying network. In this paper, we exploit the\nproximity and redundancy properties of a recently proposed clique-based\nclustered overlay network, named eQuus, to build efficient as well as robust\noverlays for multimedia stream dissemination. To combine the efficiency of\ncontent pushing over tree structured overlays and the robustness of data-driven\nmesh overlays, higher capacity stable nodes are organized in tree structure to\ncarry the long haul traffic and less stable nodes with intermittent presence\nare organized in localized meshes. The overlay construction and fault-recovery\nprocedures are explained in details. Simulation study demonstrates the good\nlocality properties of the platform. The outage time and control overhead\ninduced by the failure recovery mechanism are minimal as demonstrated by the\nanalysis.", 
    "link": "http://arxiv.org/pdf/0903.4365v2", 
    "arxiv-id": "0903.4365v2"
},{
    "category": "cs.HC", 
    "author": "Malik Mallem", 
    "title": "A Distributed Software Architecture for Collaborative Teleoperation   based on a VR Platform and Web Application Interoperability", 
    "publish": "2009-04-14T11:21:47Z", 
    "summary": "Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a\nreal help to complete complex tasks, such as robot teleoperation and\ncooperative teleassistance. Using appropriate augmentations, the HO can\ninteract faster, safer and easier with the remote real world. In this paper, we\npresent an extension of an existing distributed software and network\narchitecture for collaborative teleoperation based on networked human-scaled\nmixed reality and mobile platform. The first teleoperation system was composed\nby a VR application and a Web application. However the 2 systems cannot be used\ntogether and it is impossible to control a distant robot simultaneously. Our\ngoal is to update the teleoperation system to permit a heterogeneous\ncollaborative teleoperation between the 2 platforms. An important feature of\nthis interface is based on different Mobile platforms to control one or many\nrobots.", 
    "link": "http://arxiv.org/pdf/0904.2096v1", 
    "arxiv-id": "0904.2096v1"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Structural Solutions for Cross-Layer Optimization of Wireless Multimedia   Transmission", 
    "publish": "2009-05-25T22:09:58Z", 
    "summary": "In this paper, we propose a systematic solution to the problem of cross-layer\noptimization for delay-sensitive media transmission over time-varying wireless\nchannels as well as investigate the structures and properties of this solution,\nsuch that it can be easily implemented in various multimedia systems and\napplications. Specifically, we formulate this problem as a finite-horizon\nMarkov decision process (MDP) by explicitly considering the users'\nheterogeneous multimedia traffic characteristics (e.g. delay deadlines,\ndistortion impacts and dependencies etc.), time-varying network conditions as\nwell as, importantly, their ability to adapt their cross-layer transmission\nstrategies in response to these dynamics. Based on the heterogeneous\ncharacteristics of the media packets, we are able to express the transmission\npriorities between packets as a new type of directed acyclic graph (DAG). This\nDAG provides the necessary structure for determining the optimal cross-layer\nactions in each time slot: the root packet in the DAG will always be selected\nfor transmission since it has the highest positive marginal utility; and the\ncomplexity of the proposed cross-layer solution is demonstrated to linearly\nincrease w.r.t. the number of disconnected packet pairs in the DAG and\nexponentially increase w.r.t. the number of packets on which the current\npackets depend on. The simulation results demonstrate that the proposed\nsolution significantly outperforms existing state-of-the-art cross-layer\nsolutions. Moreover, we show that our solution provides the upper bound\nperformance for the cross-layer optimization solutions with delayed feedback\nsuch as the well-known RaDiO framework.", 
    "link": "http://arxiv.org/pdf/0905.4087v1", 
    "arxiv-id": "0905.4087v1"
},{
    "category": "cs.CR", 
    "author": "Balasubramanian Raman", 
    "title": "Robust Watermarking in Multiresolution Walsh-Hadamard Transform", 
    "publish": "2009-06-24T07:23:51Z", 
    "summary": "In this paper, a newer version of Walsh-Hadamard Transform namely\nmultiresolution Walsh-Hadamard Transform (MR-WHT) is proposed for images.\nFurther, a robust watermarking scheme is proposed for copyright protection\nusing MRWHT and singular value decomposition. The core idea of the proposed\nscheme is to decompose an image using MR-WHT and then middle singular values of\nhigh frequency sub-band at the coarsest and the finest level are modified with\nthe singular values of the watermark. Finally, a reliable watermark extraction\nscheme is developed for the extraction of the watermark from the distorted\nimage. The experimental results show better visual imperceptibility and\nresiliency of the proposed scheme against intentional or un-intentional variety\nof attacks.", 
    "link": "http://arxiv.org/pdf/0906.4415v1", 
    "arxiv-id": "0906.4415v1"
},{
    "category": "cs.DB", 
    "author": "Jakub Lokoc", 
    "title": "On Metric Skyline Processing by PM-tree", 
    "publish": "2009-10-06T12:09:52Z", 
    "summary": "The task of similarity search in multimedia databases is usually accomplished\nby range or k nearest neighbor queries. However, the expressing power of these\n\"single-example\" queries fails when the user's delicate query intent is not\navailable as a single example. Recently, the well-known skyline operator was\nreused in metric similarity search as a \"multi-example\" query type. When\napplied on a multi-dimensional database (i.e., on a multi-attribute table), the\ntraditional skyline operator selects all database objects that are not\ndominated by other objects. The metric skyline query adopts the skyline\noperator such that the multiple attributes are represented by distances\n(similarities) to multiple query examples. Hence, we can view the metric\nskyline as a set of representative database objects which are as similar to all\nthe examples as possible and, simultaneously, are semantically distinct. In\nthis paper we propose a technique of processing the metric skyline query by use\nof PM-tree, while we show that our technique significantly outperforms the\noriginal M-tree based implementation in both time and space costs. In\nexperiments we also evaluate the partial metric skyline processing, where only\na controlled number of skyline objects is retrieved.", 
    "link": "http://arxiv.org/pdf/0910.0983v1", 
    "arxiv-id": "0910.0983v1"
},{
    "category": "cs.CV", 
    "author": "Jagannathan Ramaswamy", 
    "title": "An Innovative Scheme For Effectual Fingerprint Data Compression Using   Bezier Curve Representations", 
    "publish": "2009-11-03T05:11:40Z", 
    "summary": "Naturally, with the mounting application of biometric systems, there arises a\ndifficulty in storing and handling those acquired biometric data. Fingerprint\nrecognition has been recognized as one of the most mature and established\ntechnique among all the biometrics systems. In recent times, with fingerprint\nrecognition receiving increasingly more attention the amount of fingerprints\ncollected has been constantly creating enormous problems in storage and\ntransmission. Henceforth, the compression of fingerprints has emerged as an\nindispensable step in automated fingerprint recognition systems. Several\nresearchers have presented approaches for fingerprint image compression. In\nthis paper, we propose a novel and efficient scheme for fingerprint image\ncompression. The presented scheme utilizes the Bezier curve representations for\neffective compression of fingerprint images. Initially, the ridges present in\nthe fingerprint image are extracted along with their coordinate values using\nthe approach presented. Subsequently, the control points are determined for all\nthe ridges by visualizing each ridge as a Bezier curve. The control points of\nall the ridges determined are stored and are used to represent the fingerprint\nimage. When needed, the fingerprint image is reconstructed from the stored\ncontrol points using Bezier curves. The quality of the reconstructed\nfingerprint is determined by a formal evaluation. The proposed scheme achieves\nconsiderable memory reduction in storing the fingerprint.", 
    "link": "http://arxiv.org/pdf/0911.0499v1", 
    "arxiv-id": "0911.0499v1"
},{
    "category": "cs.SD", 
    "author": "Olivier Michel Tache", 
    "title": "G3 : GENESIS software envrionment update", 
    "publish": "2009-11-24T15:07:37Z", 
    "summary": "GENESIS3 is the new version of the GENESIS software environment for musical\ncreation by means of mass-interaction physics network modeling. It was\ndesigned, and developed from scratch, in hindsight of more than 10 years\nworking on and using the previous version. We take the opportunity of this\nbirth to provide in this article (1) an analysis of the peculiarities in\nGENESIS, aiming at highlighting its core ?software paradigm?; and (2) an update\non the features of the new version as compared to the last.", 
    "link": "http://arxiv.org/pdf/0911.4642v1", 
    "arxiv-id": "0911.4642v1"
},{
    "category": "cs.MM", 
    "author": "Suresh Kumar Thakur", 
    "title": "Performance analysis of Non Linear Filtering Algorithms for underwater   images", 
    "publish": "2009-12-05T12:33:09Z", 
    "summary": "Image filtering algorithms are applied on images to remove the different\ntypes of noise that are either present in the image during capturing or\ninjected in to the image during transmission. Underwater images when captured\nusually have Gaussian noise, speckle noise and salt and pepper noise. In this\nwork, five different image filtering algorithms are compared for the three\ndifferent noise types. The performances of the filters are compared using the\nPeak Signal to Noise Ratio (PSNR) and Mean Square Error (MSE). The modified\nspatial median filter gives desirable results in terms of the above two\nparameters for the three different noise. Forty underwater images are taken for\nstudy.", 
    "link": "http://arxiv.org/pdf/0912.1005v1", 
    "arxiv-id": "0912.1005v1"
},{
    "category": "cs.CR", 
    "author": "Mohammed M. Nasef", 
    "title": "Genetic Programming Framework for Fingerprint Matching", 
    "publish": "2009-12-05T13:27:10Z", 
    "summary": "A fingerprint matching is a very difficult problem. Minutiae based matching\nis the most popular and widely used technique for fingerprint matching. The\nminutiae points considered in automatic identification systems are based\nnormally on termination and bifurcation points. In this paper we propose a new\ntechnique for fingerprint matching using minutiae points and genetic\nprogramming. The goal of this paper is extracting the mathematical formula that\ndefines the minutiae points.", 
    "link": "http://arxiv.org/pdf/0912.1017v1", 
    "arxiv-id": "0912.1017v1"
},{
    "category": "cs.HC", 
    "author": "M. Uma Maheswari", 
    "title": "Vision Based Game Development Using Human Computer Interaction", 
    "publish": "2010-02-10T19:46:07Z", 
    "summary": "A Human Computer Interface (HCI) System for playing games is designed here\nfor more natural communication with the machines. The system presented here is\na vision-based system for detection of long voluntary eye blinks and\ninterpretation of blink patterns for communication between man and machine.\nThis system replaces the mouse with the human face as a new way to interact\nwith the computer. Facial features (nose tip and eyes) are detected and tracked\nin realtime to use their actions as mouse events. The coordinates and movement\nof the nose tip in the live video feed are translated to become the coordinates\nand movement of the mouse pointer on the application. The left or right eye\nblinks fire left or right mouse click events. The system works with inexpensive\nUSB cameras and runs at a frame rate of 30 frames per second.", 
    "link": "http://arxiv.org/pdf/1002.2191v1", 
    "arxiv-id": "1002.2191v1"
},{
    "category": "cs.LG", 
    "author": "Mihaela van der Schaar", 
    "title": "Structure-Aware Stochastic Control for Transmission Scheduling", 
    "publish": "2010-03-12T04:07:41Z", 
    "summary": "In this paper, we consider the problem of real-time transmission scheduling\nover time-varying channels. We first formulate the transmission scheduling\nproblem as a Markov decision process (MDP) and systematically unravel the\nstructural properties (e.g. concavity in the state-value function and\nmonotonicity in the optimal scheduling policy) exhibited by the optimal\nsolutions. We then propose an online learning algorithm which preserves these\nstructural properties and achieves -optimal solutions for an arbitrarily small\n. The advantages of the proposed online method are that: (i) it does not\nrequire a priori knowledge of the traffic arrival and channel statistics and\n(ii) it adaptively approximates the state-value functions using piece-wise\nlinear functions and has low storage and computation complexity. We also extend\nthe proposed low-complexity online learning solution to the prioritized data\ntransmission. The simulation results demonstrate that the proposed method\nachieves significantly better utility (or delay)-energy trade-offs when\ncomparing to existing state-of-art online optimization methods.", 
    "link": "http://arxiv.org/pdf/1003.2471v1", 
    "arxiv-id": "1003.2471v1"
},{
    "category": "cs.CV", 
    "author": "Kunio Kashino", 
    "title": "A stochastic model of human visual attention with a dynamic Bayesian   network", 
    "publish": "2010-04-01T08:51:32Z", 
    "summary": "Recent studies in the field of human vision science suggest that the human\nresponses to the stimuli on a visual display are non-deterministic. People may\nattend to different locations on the same visual input at the same time. Based\non this knowledge, we propose a new stochastic model of visual attention by\nintroducing a dynamic Bayesian network to predict the likelihood of where\nhumans typically focus on a video scene. The proposed model is composed of a\ndynamic Bayesian network with 4 layers. Our model provides a framework that\nsimulates and combines the visual saliency response and the cognitive state of\na person to estimate the most probable attended regions. Sample-based inference\nwith Markov chain Monte-Carlo based particle filter and stream processing with\nmulti-core processors enable us to estimate human visual attention in near real\ntime. Experimental results have demonstrated that our model performs\nsignificantly better in predicting human visual attention compared to the\nprevious deterministic models.", 
    "link": "http://arxiv.org/pdf/1004.0085v1", 
    "arxiv-id": "1004.0085v1"
},{
    "category": "cs.HC", 
    "author": "Craig A. Lindley", 
    "title": "Trends and Techniques in Visual Gaze Analysis", 
    "publish": "2010-04-01T23:48:23Z", 
    "summary": "Visualizing gaze data is an effective way for the quick interpretation of eye\ntracking results. This paper presents a study investigation benefits and\nlimitations of visual gaze analysis among eye tracking professionals and\nresearchers. The results were used to create a tool for visual gaze analysis\nwithin a Master's project.", 
    "link": "http://arxiv.org/pdf/1004.0258v1", 
    "arxiv-id": "1004.0258v1"
},{
    "category": "cs.IR", 
    "author": "Suresh Reddy. S", 
    "title": "Audio enabled information extraction system for cricket and hockey   domains", 
    "publish": "2010-04-26T10:11:00Z", 
    "summary": "The proposed system aims at the retrieval of the summarized information from\nthe documents collected from web based search engine as per the user query\nrelated to cricket and hockey domain. The system is designed in a manner that\nit takes the voice commands as keywords for search. The parts of speech in the\nquery are extracted using the natural language extractor for English. Based on\nthe keywords the search is categorized into 2 types: - 1.Concept wise -\ninformation retrieved to the query is retrieved based on the keywords and the\nconcept words related to it. The retrieved information is summarized using the\nprobabilistic approach and weighted means algorithm.2.Keyword search - extracts\nthe result relevant to the query from the highly ranked document retrieved from\nthe search by the search engine. The relevant search results are retrieved and\nthen keywords are used for summarizing part. During summarization it follows\nthe weighted and probabilistic approaches in order to identify the data\ncomparable to the keywords extracted. The extracted information is then refined\nrepeatedly through the aggregation process to reduce redundancy. Finally the\nresultant data is submitted to the user in the form of audio output.", 
    "link": "http://arxiv.org/pdf/1004.4464v1", 
    "arxiv-id": "1004.4464v1"
},{
    "category": "cs.LO", 
    "author": "Freek Wiedijk", 
    "title": "Proviola: A Tool for Proof Re-animation", 
    "publish": "2010-05-15T12:54:28Z", 
    "summary": "To improve on existing models of interaction with a proof assistant (PA), in\nparticular for storage and replay of proofs, we in- troduce three related\nconcepts, those of: a proof movie, consisting of frames which record both user\ninput and the corresponding PA response; a camera, which films a user's\ninteractive session with a PA as a movie; and a proviola, which replays a movie\nframe-by-frame to a third party. In this paper we describe the movie data\nstructure and we discuss a proto- type implementation of the camera and\nproviola based on the ProofWeb system. ProofWeb uncouples the interaction with\na PA via a web- interface (the client) from the actual PA that resides on the\nserver. Our camera films a movie by \"listening\" to the ProofWeb communication.\nThe first reason for developing movies is to uncouple the reviewing of a formal\nproof from the PA used to develop it: the movie concept enables users to\ndiscuss small code fragments without the need to install the PA or to load a\nwhole library into it. Other advantages include the possibility to develop a\nseparate com- mentary track to discuss or explain the PA interaction. We assert\nthat a combined camera+proviola provides a generic layer between a client\n(user) and a server (PA). Finally we claim that movies are the right type of\ndata to be stored in an encyclopedia of formalized mathematics, based on our\nexperience in filming the Coq standard library.", 
    "link": "http://arxiv.org/pdf/1005.2672v1", 
    "arxiv-id": "1005.2672v1"
},{
    "category": "cs.HC", 
    "author": "Jean-Loup Florens", 
    "title": "A basic gesture and motion format for virtual reality multisensory   applications", 
    "publish": "2010-05-25T13:16:29Z", 
    "summary": "The question of encoding movements such as those produced by human gestures\nmay become central in the coming years, given the growing importance of\nmovement data exchanges between heterogeneous systems and applications (musical\napplications, 3D motion control, virtual reality interaction, etc.). For the\npast 20 years, various formats have been proposed for encoding movement,\nespecially gestures. Though, these formats, at different degrees, were designed\nin the context of quite specific applications (character animation, motion\ncapture, musical gesture, biomechanical concerns...). The article introduce a\nnew file format, called GMS (for 'Gesture and Motion Signal'), with the aim of\nbeing more low-level and generic, by defining the minimal features a format\ncarrying movement/gesture information needs, rather than by gathering all the\ninformation generally given by the existing formats. The article argues that,\ngiven its growing presence in virtual reality situations, the \"gesture signal\"\nitself must be encoded, and that a specific format is needed. The proposed\nformat features the inner properties of such signals: dimensionality,\nstructural features, types of variables, and spatial and temporal properties.\nThe article first reviews the various situations with multisensory virtual\nobjects in which gesture controls intervene. The proposed format is then\ndeduced, as a mean to encode such versatile and variable \"gestural and animated\nscene\".", 
    "link": "http://arxiv.org/pdf/1005.4564v1", 
    "arxiv-id": "1005.4564v1"
},{
    "category": "cs.CV", 
    "author": "Shigeru Takagi", 
    "title": "Fully automatic extraction of salient objects from videos in near   real-time", 
    "publish": "2010-08-03T10:00:07Z", 
    "summary": "Automatic video segmentation plays an important role in a wide range of\ncomputer vision and image processing applications. Recently, various methods\nhave been proposed for this purpose. The problem is that most of these methods\nare far from real-time processing even for low-resolution videos due to the\ncomplex procedures. To this end, we propose a new and quite fast method for\nautomatic video segmentation with the help of 1) efficient optimization of\nMarkov random fields with polynomial time of number of pixels by introducing\ngraph cuts, 2) automatic, computationally efficient but stable derivation of\nsegmentation priors using visual saliency and sequential update mechanism, and\n3) an implementation strategy in the principle of stream processing with\ngraphics processor units (GPUs). Test results indicates that our method\nextracts appropriate regions from videos as precisely as and much faster than\nprevious semi-automatic methods even though any supervisions have not been\nincorporated.", 
    "link": "http://arxiv.org/pdf/1008.0502v2", 
    "arxiv-id": "1008.0502v2"
},{
    "category": "cs.MM", 
    "author": "Mihaela van der Schaar", 
    "title": "Structural Solutions to Dynamic Scheduling for Multimedia Transmission   in Unknown Wireless Environments", 
    "publish": "2010-08-25T23:06:39Z", 
    "summary": "In this paper, we propose a systematic solution to the problem of scheduling\ndelay-sensitive media data for transmission over time-varying wireless\nchannels. We first formulate the dynamic scheduling problem as a Markov\ndecision process (MDP) that explicitly considers the users' heterogeneous\nmultimedia data characteristics (e.g. delay deadlines, distortion impacts and\ndependencies etc.) and time-varying channel conditions, which are not\nsimultaneously considered in state-of-the-art packet scheduling algorithms.\nThis formulation allows us to perform foresighted decisions to schedule\nmultiple data units for transmission at each time in order to optimize the\nlong-term utilities of the multimedia applications. The heterogeneity of the\nmedia data enables us to express the transmission priorities between the\ndifferent data units as a priority graph, which is a directed acyclic graph\n(DAG). This priority graph provides us with an elegant structure to decompose\nthe multi-data unit foresighted decision at each time into multiple single-data\nunit foresighted decisions which can be performed sequentially, from the high\npriority data units to the low priority data units, thereby significantly\nreducing the computation complexity. When the statistical knowledge of the\nmultimedia data characteristics and channel conditions is unknown a priori, we\ndevelop a low-complexity online learning algorithm to update the value\nfunctions which capture the impact of the current decision on the future\nutility. The simulation results show that the proposed solution significantly\noutperforms existing state-of-the-art scheduling solutions.", 
    "link": "http://arxiv.org/pdf/1008.4406v1", 
    "arxiv-id": "1008.4406v1"
},{
    "category": "cs.MM", 
    "author": "Shakirah Mohd Taib", 
    "title": "M-Learning: A New Paradigm of Learning Mathematics in Malaysia", 
    "publish": "2010-09-06T22:33:38Z", 
    "summary": "M-Learning is a new learning paradigm of the new social structure with mobile\nand wireless technologies.Smart school is one of the four flagship applications\nfor Multimedia Super Corridor (MSC) under Malaysian government initiative to\nimprove education standard in the country. With the advances of mobile devices\ntechnologies, mobile learning could help the government in realizing the\ninitiative. This paper discusses the prospect of implementing mobile learning\nfor primary school students. It indicates significant and challenges and\nanalysis of user perceptions on potential mobile applications through a survey\ndone in primary school context. The authors propose the m-Learning for\nmathematics by allowing the extension of technology in the traditional\nclassroom in term of learning and teaching.", 
    "link": "http://arxiv.org/pdf/1009.1170v1", 
    "arxiv-id": "1009.1170v1"
},{
    "category": "cs.IT", 
    "author": "Guangming Shi", 
    "title": "Morphological dilation image coding with context weights prediction", 
    "publish": "2010-09-29T03:22:34Z", 
    "summary": "This paper proposes an adaptive morphological dilation image coding with\ncontext weights prediction. The new dilation method is not to use fixed models,\nbut to decide whether a coefficient needs to be dilated or not according to the\ncoefficient's predicted significance degree. It includes two key dilation\ntechnologies: 1) controlling dilation process with context weights to reduce\nthe output of insignificant coefficients, and 2) using variable-length group\ntest coding with context weights to adjust the coding order and cost as few\nbits as possible to present the events with large probability. Moreover, we\nalso propose a novel context weight strategy to predict coefficient's\nsignificance degree more accurately, which serves for two dilation\ntechnologies. Experimental results show that our proposed method outperforms\nthe state of the art image coding algorithms available today.", 
    "link": "http://arxiv.org/pdf/1009.5762v1", 
    "arxiv-id": "1009.5762v1"
},{
    "category": "cs.CV", 
    "author": "Ambuj K. Singh", 
    "title": "Profile Based Sub-Image Search in Image Databases", 
    "publish": "2010-10-07T17:42:09Z", 
    "summary": "Sub-image search with high accuracy in natural images still remains a\nchallenging problem. This paper proposes a new feature vector called profile\nfor a keypoint in a bag of visual words model of an image. The profile of a\nkeypoint captures the spatial geometry of all the other keypoints in an image\nwith respect to itself, and is very effective in discriminating true matches\nfrom false matches. Sub-image search using profiles is a single-phase process\nrequiring no geometric validation, yields high precision on natural images, and\nworks well on small visual codebook. The proposed search technique differs from\ntraditional methods that first generate a set of candidates disregarding\nspatial information and then verify them geometrically. Conventional methods\nalso use large codebooks. We achieve a precision of 81% on a combined data set\nof synthetic and real natural images using a codebook size of 500 for top-10\nqueries; that is 31% higher than the conventional candidate generation\napproach.", 
    "link": "http://arxiv.org/pdf/1010.1496v1", 
    "arxiv-id": "1010.1496v1"
},{
    "category": "cs.CV", 
    "author": "Rokia Missaoui", 
    "title": "Texture feature extraction in the spatial-frequency domain for   content-based image retrieval", 
    "publish": "2010-12-23T14:10:25Z", 
    "summary": "The advent of large scale multimedia databases has led to great challenges in\ncontent-based image retrieval (CBIR). Even though CBIR is considered an\nemerging field of research, however it constitutes a strong background for new\nmethodologies and systems implementations. Therefore, many research\ncontributions are focusing on techniques enabling higher image retrieval\naccuracy while preserving low level of computational complexity. Image\nretrieval based on texture features is receiving special attention because of\nthe omnipresence of this visual feature in most real-world images. This paper\nhighlights the state-of-the-art and current progress relevant to texture-based\nimage retrieval and spatial-frequency image representations. In particular, it\ngives an overview of statistical methodologies and techniques employed for\ntexture feature extraction using most popular spatial-frequency image\ntransforms, namely discrete wavelets, Gabor wavelets, dual-tree complex wavelet\nand contourlets. Indications are also given about used similarity measurement\nfunctions and most important achieved results.", 
    "link": "http://arxiv.org/pdf/1012.5208v1", 
    "arxiv-id": "1012.5208v1"
},{
    "category": "cs.IR", 
    "author": "Yong Rui", 
    "title": "Sparse Transfer Learning for Interactive Video Search Reranking", 
    "publish": "2011-03-14T19:48:20Z", 
    "summary": "Visual reranking is effective to improve the performance of the text-based\nvideo search. However, existing reranking algorithms can only achieve limited\nimprovement because of the well-known semantic gap between low level visual\nfeatures and high level semantic concepts. In this paper, we adopt interactive\nvideo search reranking to bridge the semantic gap by introducing user's\nlabeling effort. We propose a novel dimension reduction tool, termed sparse\ntransfer learning (STL), to effectively and efficiently encode user's labeling\ninformation. STL is particularly designed for interactive video search\nreranking. Technically, it a) considers the pair-wise discriminative\ninformation to maximally separate labeled query relevant samples from labeled\nquery irrelevant ones, b) achieves a sparse representation for the subspace to\nencodes user's intention by applying the elastic net penalty, and c) propagates\nuser's labeling information from labeled samples to unlabeled samples by using\nthe data distribution knowledge. We conducted extensive experiments on the\nTRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular\ndimension reduction algorithms. We report superior performance by using the\nproposed STL based interactive video search reranking.", 
    "link": "http://arxiv.org/pdf/1103.2756v3", 
    "arxiv-id": "1103.2756v3"
},{
    "category": "cs.PL", 
    "author": "Samuel Mimram", 
    "title": "Liquidsoap: a High-Level Programming Language for Multimedia Streaming", 
    "publish": "2011-04-14T07:01:54Z", 
    "summary": "Generating multimedia streams, such as in a netradio, is a task which is\ncomplex and difficult to adapt to every users' needs. We introduce a novel\napproach in order to achieve it, based on a dedicated high-level functional\nprogramming language, called Liquidsoap, for generating, manipulating and\nbroadcasting multimedia streams. Unlike traditional approaches, which are based\non configuration files or static graphical interfaces, it also allows the user\nto build complex and highly customized systems. This language is based on a\nmodel for streams and contains operators and constructions, which make it\nadapted to the generation of streams. The interpreter of the language also\nensures many properties concerning the good execution of the stream generation.", 
    "link": "http://arxiv.org/pdf/1104.2681v1", 
    "arxiv-id": "1104.2681v1"
},{
    "category": "cs.SD", 
    "author": "Deva O'Neil", 
    "title": "Simulating the Electroweak Phase Transition: Sonification of Bubble   Nucleation", 
    "publish": "2011-06-03T20:22:03Z", 
    "summary": "As an applicaton of sonification, a simulation of the early universe was\ndeveloped to portray a phase transition that occurred shortly after the Big\nBang. The Standard Model of particle physics postulates that a hypothetical\nparticle, the Higgs boson, is responsible for the breaking of the symmetry\nbetween the electromagnetic force and the weak force. This phase transition may\nhave been responsible for triggering Baryogenesis, the generation of an\nabundance of matter over anti-matter. This hypothesis is known as Electroweak\nBaryogenesis. In this simulation, aspects of bubble nucleation in Standard\nModel Electroweak Baryogenesis were examined and modeled using Mathematica, and\nsonified using SuperCollider3. The resulting simulation, which has been used\nfor pedagogical purposes by one of the authors, suggests interesting\npossibilities for the integration of science and aesthetics as well as auditory\nperception. The sonification component in particular also had the unexpected\nbenefit of being useful in debugging the Mathematica code.", 
    "link": "http://arxiv.org/pdf/1106.0760v1", 
    "arxiv-id": "1106.0760v1"
},{
    "category": "cs.IT", 
    "author": "Martin Vetterli", 
    "title": "Bits from Photons: Oversampled Image Acquisition Using Binary Poisson   Statistics", 
    "publish": "2011-06-06T03:52:31Z", 
    "summary": "We study a new image sensor that is reminiscent of traditional photographic\nfilm. Each pixel in the sensor has a binary response, giving only a one-bit\nquantized measurement of the local light intensity. To analyze its performance,\nwe formulate the oversampled binary sensing scheme as a parameter estimation\nproblem based on quantized Poisson statistics. We show that, with a\nsingle-photon quantization threshold and large oversampling factors, the\nCram\\'er-Rao lower bound (CRLB) of the estimation variance approaches that of\nan ideal unquantized sensor, that is, as if there were no quantization in the\nsensor measurements. Furthermore, the CRLB is shown to be asymptotically\nachievable by the maximum likelihood estimator (MLE). By showing that the\nlog-likelihood function of our problem is concave, we guarantee the global\noptimality of iterative algorithms in finding the MLE. Numerical results on\nboth synthetic data and images taken by a prototype sensor verify our\ntheoretical analysis and demonstrate the effectiveness of our image\nreconstruction algorithm. They also suggest the potential application of the\noversampled binary sensing scheme in high dynamic range photography.", 
    "link": "http://arxiv.org/pdf/1106.0954v2", 
    "arxiv-id": "1106.0954v2"
},{
    "category": "cs.NI", 
    "author": "Ralf Klamma", 
    "title": "Mobile Cloud Computing: A Comparison of Application Models", 
    "publish": "2011-07-25T13:17:13Z", 
    "summary": "Cloud computing is an emerging concept combining many fields of computing.\nThe foundation of cloud computing is the delivery of services, software and\nprocessing capacity over the Internet, reducing cost, increasing storage,\nautomating systems, decoupling of service delivery from underlying technology,\nand providing flexibility and mobility of information. However, the actual\nrealization of these benefits is far from being achieved for mobile\napplications and open many new research questions. In order to better\nunderstand how to facilitate the building of mobile cloud-based applications,\nwe have surveyed existing work in mobile computing through the prism of cloud\ncomputing principles. We give a definition of mobile cloud coputing and provide\nan overview of the results from this review, in particular, models of mobile\ncloud applications. We also highlight research challenges in the area of mobile\ncloud computing. We conclude with recommendations for how this better\nunderstanding of mobile cloud computing can help building more powerful mobile\napplications.", 
    "link": "http://arxiv.org/pdf/1107.4940v1", 
    "arxiv-id": "1107.4940v1"
},{
    "category": "cs.SD", 
    "author": "Tijl De Bie", 
    "title": "An end-to-end machine learning system for harmonic analysis of music", 
    "publish": "2011-07-25T15:09:37Z", 
    "summary": "We present a new system for simultaneous estimation of keys, chords, and bass\nnotes from music audio. It makes use of a novel chromagram representation of\naudio that takes perception of loudness into account. Furthermore, it is fully\nbased on machine learning (instead of expert knowledge), such that it is\npotentially applicable to a wider range of genres as long as training data is\navailable. As compared to other models, the proposed system is fast and memory\nefficient, while achieving state-of-the-art performance.", 
    "link": "http://arxiv.org/pdf/1107.4969v1", 
    "arxiv-id": "1107.4969v1"
},{
    "category": "cs.PF", 
    "author": "Tania Jimenez", 
    "title": "Analysis of Buffer Starvation with Application to Objective QoE   Optimization of Streaming Services", 
    "publish": "2011-07-31T16:10:57Z", 
    "summary": "Our purpose in this paper is to characterize buffer starvations for streaming\nservices. The buffer is modeled as an M/M/1 queue, plus the consideration of\nbursty arrivals. When the buffer is empty, the service restarts after a certain\namount of packets are \\emph{prefetched}. With this goal, we propose two\napproaches to obtain the \\emph{exact distribution} of the number of buffer\nstarvations, one of which is based on \\emph{Ballot theorem}, and the other uses\nrecursive equations. The Ballot theorem approach gives an explicit result. We\nextend this approach to the scenario with a constant playback rate using\nT\\`{a}kacs Ballot theorem. The recursive approach, though not offering an\nexplicit result, can obtain the distribution of starvations with\nnon-independent and identically distributed (i.i.d.) arrival process in which\nan ON/OFF bursty arrival process is considered in this work. We further compute\nthe starvation probability as a function of the amount of prefetched packets\nfor a large number of files via a fluid analysis. Among many potential\napplications of starvation analysis, we show how to apply it to optimize the\nobjective quality of experience (QoE) of media streaming, by exploiting the\ntradeoff between startup/rebuffering delay and starvations.", 
    "link": "http://arxiv.org/pdf/1108.0187v5", 
    "arxiv-id": "1108.0187v5"
},{
    "category": "cs.CV", 
    "author": "Ankit Chaudhary", 
    "title": "An Efficient Real Time Method of Fingertip Detection", 
    "publish": "2011-08-02T07:46:16Z", 
    "summary": "Fingertips detection has been used in many applications, and it is very\npopular and commonly used in the area of Human Computer Interaction these days.\nThis paper presents a novel time efficient method that will lead to fingertip\ndetection after cropping the irrelevant parts of input image. Binary silhouette\nof the input image is generated using HSV color space based skin filter and\nhand cropping done based on histogram of the hand image. The cropped image will\nbe used to figure out the fingertips.", 
    "link": "http://arxiv.org/pdf/1108.0502v1", 
    "arxiv-id": "1108.0502v1"
},{
    "category": "cs.MM", 
    "author": "DahMing Chiu", 
    "title": "Compression and Quantitative Analysis of Buffer Map Message in P2P   Streaming System", 
    "publish": "2011-08-31T16:39:09Z", 
    "summary": "BM compression is a straightforward and operable way to reduce buffer message\nlength as well as to improve system performance. In this paper, we thoroughly\ndiscuss the principles and protocol progress of different compression schemes,\nand for the first time present an original compression scheme which can nearly\nremove all redundant information from buffer message. Theoretical limit of\ncompression rates are deduced in the theory of information. Through the\nanalysis of information content and simulation with our measured BM trace of\nUUSee, the validity and superiority of our compression scheme are validated in\nterm of compression ratio.", 
    "link": "http://arxiv.org/pdf/1108.6290v2", 
    "arxiv-id": "1108.6290v2"
},{
    "category": "cs.MM", 
    "author": "DahMing Chiu", 
    "title": "Buffer Map Message Compression Based on Relevant Window in P2P Streaming   Media System", 
    "publish": "2011-08-31T17:22:23Z", 
    "summary": "Popular peer to peer streaming media systems such as PPLive and UUSee rely on\nperiodic buffer-map exchange between peers for proper operation. The buffer-map\nexchange contains redundant information which causes non-negligible overhead.\nIn this paper we present a theoretical framework to study how the overhead can\nbe lowered. Differentiating from the traditional data compression approach, we\ndo not treat each buffer-map as an isolated data block, but consider the\ncorrelations between the sequentially exchanged buffer-maps. Under this\nframework, two buffer-map compression schemes are proposed and the correctness\nof the schemes is proved mathematically. Moreover, we derive the theoretical\nlimit of compression gain based on probability theory and information theory.\nBased on the system parameters of UUSee (a popular P2P streaming platform), our\nsimulations show that the buffer-map sizes are reduced by 86% and 90% (from 456\nbits down to only 66 bits and 46 bits) respectively after applying our schemes.\nFurthermore, by combining with the traditional compression methods (on\nindividual blocks), the sizes are decreased by 91% and 95% (to 42 bits and 24\nbits) respectively. Our study provides a guideline for developing practical\ncompression algorithms.", 
    "link": "http://arxiv.org/pdf/1108.6293v3", 
    "arxiv-id": "1108.6293v3"
},{
    "category": "cs.IT", 
    "author": "Mostafa Ayoubi Mobarhan", 
    "title": "Evaluation of Huffman and Arithmetic Algorithms for Multimedia   Compression Standards", 
    "publish": "2011-08-31T15:34:23Z", 
    "summary": "Compression is a technique to reduce the quantity of data without excessively\nreducing the quality of the multimedia data. The transition and storing of\ncompressed multimedia data is much faster and more efficient than original\nuncompressed multimedia data. There are various techniques and standards for\nmultimedia data compression, especially for image compression such as the JPEG\nand JPEG2000 standards. These standards consist of different functions such as\ncolor space conversion and entropy coding. Arithmetic and Huffman coding are\nnormally used in the entropy coding phase. In this paper we try to answer the\nfollowing question. Which entropy coding, arithmetic or Huffman, is more\nsuitable compared to other from the compression ratio, performance, and\nimplementation points of view? We have implemented and tested Huffman and\narithmetic algorithms. Our implemented results show that compression ratio of\narithmetic coding is better than Huffman coding, while the performance of the\nHuffman coding is higher than Arithmetic coding. In addition, implementation of\nHuffman coding is much easier than the Arithmetic coding.", 
    "link": "http://arxiv.org/pdf/1109.0216v1", 
    "arxiv-id": "1109.0216v1"
},{
    "category": "cs.PL", 
    "author": "Jacques Carette", 
    "title": "SAGA: A DSL for Story Management", 
    "publish": "2011-09-05T01:56:19Z", 
    "summary": "Video game development is currently a very labour-intensive endeavour.\nFurthermore it involves multi-disciplinary teams of artistic content creators\nand programmers, whose typical working patterns are not easily meshed. SAGA is\nour first effort at augmenting the productivity of such teams.\n  Already convinced of the benefits of DSLs, we set out to analyze the domains\npresent in games in order to find out which would be most amenable to the DSL\napproach. Based on previous work, we thus sought those sub-parts that already\nhad a partially established vocabulary and at the same time could be well\nmodeled using classical computer science structures. We settled on the 'story'\naspect of video games as the best candidate domain, which can be modeled using\nstate transition systems.\n  As we are working with a specific company as the ultimate customer for this\nwork, an additional requirement was that our DSL should produce code that can\nbe used within a pre-existing framework. We developed a full system (SAGA)\ncomprised of a parser for a human-friendly language for 'story events', an\ninternal representation of design patterns for implementing object-oriented\nstate-transitions systems, an instantiator for these patterns for a specific\n'story', and three renderers (for C++, C# and Java) for the instantiated\nabstract code.", 
    "link": "http://arxiv.org/pdf/1109.0776v1", 
    "arxiv-id": "1109.0776v1"
},{
    "category": "cs.IT", 
    "author": "Jo\u00e3o Barros", 
    "title": "FEBER: Feedback Based Erasure Recovery for Real-Time Multicast over   802.11 Networks", 
    "publish": "2011-09-06T18:53:27Z", 
    "summary": "We consider the problem of broadcasting data streams over a wireless network\nfor multiple receivers with reliability and timely delivery guarantees. In our\nframework, we consider packets that need to be delivered within a given time\ninterval, after which the packet is no longer useful at the application layer.\nWe set the notion of critical packet and, based on periodic feedback from the\nreceivers, we propose a retransmission scheme that will guarantee timely\ndelivery of such packets, as well as packets that are innovative for other\nreceivers. Our solution provides a trade-off between packet delivery ratio and\nbandwidth use, which contrasts with existing approaches such as FEC and ARQ,\nwhere the focus is on ensuring reliability first, offering no guarantees of\ntimely delivery of data. We evaluate the performance of our proposal in a\n802.11 wireless network testbed.", 
    "link": "http://arxiv.org/pdf/1109.1265v3", 
    "arxiv-id": "1109.1265v3"
},{
    "category": "cs.MM", 
    "author": "Yuxin Peng", 
    "title": "Latent Semantic Learning with Structured Sparse Representation for Human   Action Recognition", 
    "publish": "2011-09-23T00:39:51Z", 
    "summary": "This paper proposes a novel latent semantic learning method for extracting\nhigh-level features (i.e. latent semantics) from a large vocabulary of abundant\nmid-level features (i.e. visual keywords) with structured sparse\nrepresentation, which can help to bridge the semantic gap in the challenging\ntask of human action recognition. To discover the manifold structure of\nmidlevel features, we develop a spectral embedding approach to latent semantic\nlearning based on L1-graph, without the need to tune any parameter for graph\nconstruction as a key step of manifold learning. More importantly, we construct\nthe L1-graph with structured sparse representation, which can be obtained by\nstructured sparse coding with its structured sparsity ensured by novel L1-norm\nhypergraph regularization over mid-level features. In the new embedding space,\nwe learn latent semantics automatically from abundant mid-level features\nthrough spectral clustering. The learnt latent semantics can be readily used\nfor human action recognition with SVM by defining a histogram intersection\nkernel. Different from the traditional latent semantic analysis based on topic\nmodels, our latent semantic learning method can explore the manifold structure\nof mid-level features in both L1-graph construction and spectral embedding,\nwhich results in compact but discriminative high-level features. The\nexperimental results on the commonly used KTH action dataset and unconstrained\nYouTube action dataset show the superior performance of our method.", 
    "link": "http://arxiv.org/pdf/1109.4979v1", 
    "arxiv-id": "1109.4979v1"
},{
    "category": "cs.IT", 
    "author": "Guillermo Sapiro", 
    "title": "Low-rank data modeling via the Minimum Description Length principle", 
    "publish": "2011-09-28T18:56:22Z", 
    "summary": "Robust low-rank matrix estimation is a topic of increasing interest, with\npromising applications in a variety of fields, from computer vision to data\nmining and recommender systems. Recent theoretical results establish the\nability of such data models to recover the true underlying low-rank matrix when\na large portion of the measured matrix is either missing or arbitrarily\ncorrupted. However, if low rank is not a hypothesis about the true nature of\nthe data, but a device for extracting regularity from it, no current guidelines\nexist for choosing the rank of the estimated matrix. In this work we address\nthis problem by means of the Minimum Description Length (MDL) principle -- a\nwell established information-theoretic approach to statistical inference -- as\na guideline for selecting a model for the data at hand. We demonstrate the\npractical usefulness of our formal approach with results for complex background\nextraction in video sequences.", 
    "link": "http://arxiv.org/pdf/1109.6297v1", 
    "arxiv-id": "1109.6297v1"
},{
    "category": "cs.MM", 
    "author": "Laszlo B\u00f6sz\u00f6rmenyi", 
    "title": "Storage Balancing in Self-organizing Multimedia Delivery Systems", 
    "publish": "2011-11-01T16:52:10Z", 
    "summary": "Many of the current bio-inspired delivery networks set their focus on search,\ne.g., by using artificial ants. If the network size and, therefore, the search\nspace gets too large, the users experience high delays until the requested\ncontent can be consumed. In previous work, we proposed different replication\nstrategies to reduce the search space. In this report we further evaluate\nmeasures for storage load balancing, because peers are most likely limited in\nspace. We periodically apply clean-ups if a certain storage level is reached.\nFor our evaluations we combine the already introduced replication measures with\nleast recently used (LRU), least frequently used (LFU) and a hormone-based\nclean-up. The goal is to elaborate a combination that leads to low delays while\nthe replica utilization is high.", 
    "link": "http://arxiv.org/pdf/1111.0242v1", 
    "arxiv-id": "1111.0242v1"
},{
    "category": "cs.NI", 
    "author": "Elbrak Said", 
    "title": "The UWB Solution for Multimedia Traffic in Wireless Sensor Networks", 
    "publish": "2011-11-08T15:03:26Z", 
    "summary": "Several researches are focused on the QoS (Quality of Service) and Energy\nconsumption in wireless Multimedia Sensor Networks. Those research projects\ninvest in theory and practice in order to extend the spectrum of use of norms,\nstandards and technologies which are emerged in wireless communications. The\nperformance of these technologies is strongly related to domains of use and\nlimitations of their characteristics. In this paper, we give a comparison of\nZigBee technology, most widely used in sensor networks, and UWB (Ultra Wide\nBand) which presents itself as competitor that present in these work better\nresults for audiovisual applications with medium-range and high throughput.", 
    "link": "http://arxiv.org/pdf/1111.1930v1", 
    "arxiv-id": "1111.1930v1"
},{
    "category": "cs.IT", 
    "author": "Matthew Fickus", 
    "title": "Fingerprinting with Equiangular Tight Frames", 
    "publish": "2011-11-14T21:44:55Z", 
    "summary": "Digital fingerprinting is a framework for marking media files, such as\nimages, music, or movies, with user-specific signatures to deter illegal\ndistribution. Multiple users can collude to produce a forgery that can\npotentially overcome a fingerprinting system. This paper proposes an\nequiangular tight frame fingerprint design which is robust to such collusion\nattacks. We motivate this design by considering digital fingerprinting in terms\nof compressed sensing. The attack is modeled as linear averaging of multiple\nmarked copies before adding a Gaussian noise vector. The content owner can then\ndetermine guilt by exploiting correlation between each user's fingerprint and\nthe forged copy. The worst-case error probability of this detection scheme is\nanalyzed and bounded. Simulation results demonstrate the average-case\nperformance is similar to the performance of orthogonal and simplex fingerprint\ndesigns, while accommodating several times as many users.", 
    "link": "http://arxiv.org/pdf/1111.3376v1", 
    "arxiv-id": "1111.3376v1"
},{
    "category": "cs.CR", 
    "author": "Jacques M. Bahi", 
    "title": "Chaotic iterations for steganography: Stego-security and   topological-security", 
    "publish": "2011-12-16T16:12:07Z", 
    "summary": "In this paper is proposed a novel steganographic scheme based on chaotic\niterations. This research work takes place into the information hiding security\nfields. We show that the proposed scheme is stego-secure, which is the highest\nlevel of security in a well defined and studied category of attack called\n\"watermark-only attack\". Additionally, we prove that this scheme presents\ntopological properties so that it is one of the firsts able to face, at least\npartially, an adversary when considering the others categories of attacks\ndefined in the literature.", 
    "link": "http://arxiv.org/pdf/1112.3873v1", 
    "arxiv-id": "1112.3873v1"
},{
    "category": "cs.CR", 
    "author": "Jacques M. Bahi", 
    "title": "Chaotic iterations versus Spread-spectrum: topological-security and   stego-security", 
    "publish": "2011-12-16T16:12:49Z", 
    "summary": "A new framework for information hiding security, called topological-security,\nhas been proposed in a previous study. It is based on the evaluation of\nunpredictability of the scheme, whereas existing notions of security, as\nstego-security, are more linked to information leaks. It has been proven that\nspread-spectrum techniques, a well-known stego-secure scheme, are\ntopologically-secure too. In this paper, the links between the two notions of\nsecurity is deepened and the usability of topological-security is clarified, by\npresenting a novel data hiding scheme that is twice stego and\ntopological-secure. This last scheme has better scores than spread-spectrum\nwhen evaluating qualitative and quantitative topological-security properties.\nIncidentally, this result shows that the new framework for security tends to\nimprove the ability to compare data hiding scheme.", 
    "link": "http://arxiv.org/pdf/1112.3874v1", 
    "arxiv-id": "1112.3874v1"
},{
    "category": "cs.IT", 
    "author": "John Scoville", 
    "title": "Critical Data Compression", 
    "publish": "2011-12-23T00:17:46Z", 
    "summary": "A new approach to data compression is developed and applied to multimedia\ncontent. This method separates messages into components suitable for both\nlossless coding and 'lossy' or statistical coding techniques, compressing\ncomplex objects by separately encoding signals and noise. This is demonstrated\nby compressing the most significant bits of data exactly, since they are\ntypically redundant and compressible, and either fitting a maximally likely\nnoise function to the residual bits or compressing them using lossy methods.\nUpon decompression, the significant bits are decoded and added to a noise\nfunction, whether sampled from a noise model or decompressed from a lossy code.\nThis results in compressed data similar to the original. For many test images,\na two-part image code using JPEG2000 for lossy coding and PAQ8l for lossless\ncoding produces less mean-squared error than an equal length of JPEG2000.\nComputer-generated images typically compress better using this method than\nthrough direct lossy coding, as do many black and white photographs and most\ncolor photographs at sufficiently high quality levels. Examples applying the\nmethod to audio and video coding are also demonstrated. Since two-part codes\nare efficient for both periodic and chaotic data, concatenations of roughly\nsimilar objects may be encoded efficiently, which leads to improved inference.\nApplications to artificial intelligence are demonstrated, showing that signals\nusing an economical lossless code have a critical level of redundancy which\nleads to better description-based inference than signals which encode either\ninsufficient data or too much detail.", 
    "link": "http://arxiv.org/pdf/1112.5493v1", 
    "arxiv-id": "1112.5493v1"
},{
    "category": "cs.MM", 
    "author": "Surekha Mariam Varghese", 
    "title": "Personalised product design using virtual interactive techniques", 
    "publish": "2012-02-08T20:26:26Z", 
    "summary": "Use of Virtual Interactive Techniques for personalized product design is\ndescribed in this paper. Usually products are designed and built by considering\ngeneral usage patterns and Prototyping is used to mimic the static or working\nbehaviour of an actual product before manufacturing the product. The user does\nnot have any control on the design of the product. Personalized design\npostpones design to a later stage. It allows for personalized selection of\nindividual components by the user. This is implemented by displaying the\nindividual components over a physical model constructed using Cardboard or\nThermocol in the actual size and shape of the original product. The components\nof the equipment or product such as screen, buttons etc. are then projected\nusing a projector connected to the computer into the physical model. Users can\ninteract with the prototype like the original working equipment and they can\nselect, shape, position the individual components displayed on the interaction\npanel using simple hand gestures. Computer Vision techniques as well as sound\nprocessing techniques are used to detect and recognize the user gestures\ncaptured using a web camera and microphone.", 
    "link": "http://arxiv.org/pdf/1202.1808v1", 
    "arxiv-id": "1202.1808v1"
},{
    "category": "cs.CR", 
    "author": "Christophe Guyeux", 
    "title": "Lyapunov exponent evaluation of a digital watermarking scheme proven to   be secure", 
    "publish": "2012-03-03T22:48:22Z", 
    "summary": "In our previous researches, a new digital watermarking scheme based on\nchaotic iterations has been introduced. This scheme was both stego-secure and\ntopologically secure. The stego-security is to face an attacker in the\n\"watermark only attack\" category, whereas the topological security concerns\nother categories of attacks. Its Lyapunov exponent is evaluated here, to\nquantify the chaos generated by this scheme.\n  Keywords : Lyapunov exponent; Information hiding; Security; Chaotic\niterations; Digital Watermarking.", 
    "link": "http://arxiv.org/pdf/1203.0692v1", 
    "arxiv-id": "1203.0692v1"
},{
    "category": "cs.MM", 
    "author": "B. V. S. Renuka Devi", 
    "title": "A Hybrid Image Cryptosystem Based On OMFLIP Permutation Cipher", 
    "publish": "2012-03-09T17:43:28Z", 
    "summary": "The protection of confidential image data from unauthorized access is an\nimportant area of research in network communication. This paper presents a\nhigh-level security encryption scheme for gray scale images. The gray level\nimage is first decomposed into binary images using bit scale decomposition.\nEach binary image is then compressed by selecting a good scanning path that\nminimizes the total number of bits needed to encode the bit sequence along the\nscanning path using two dimensional run encoding. The compressed bit string is\nthen scrambled iteratively using a pseudo-random number generator and finally\nencrypted using a bit level permutation OMFLIP. The performance is tested,\nillustrated and discussed.", 
    "link": "http://arxiv.org/pdf/1203.2147v1", 
    "arxiv-id": "1203.2147v1"
},{
    "category": "cs.SY", 
    "author": "Muriel Medard", 
    "title": "QoE-aware Media Streaming in Technology and Cost Heterogeneous Networks", 
    "publish": "2012-03-15T03:27:51Z", 
    "summary": "We present a framework for studying the problem of media streaming in\ntechnology and cost heterogeneous environments. We first address the problem of\nefficient streaming in a technology-heterogeneous setting. We employ random\nlinear network coding to simplify the packet selection strategies and alleviate\nissues such as duplicate packet reception. Then, we study the problem of media\nstreaming from multiple cost-heterogeneous access networks. Our objective is to\ncharacterize analytically the trade-off between access cost and user\nexperience. We model the Quality of user Experience (QoE) as the probability of\ninterruption in playback as well as the initial waiting time. We design and\ncharacterize various control policies, and formulate the optimal control\nproblem using a Markov Decision Process (MDP) with a probabilistic constraint.\nWe present a characterization of the optimal policy using the\nHamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we\nprovide an exact and explicit characterization of a threshold policy and prove\nits optimality using the HJB equation.\n  Our simulation results show that under properly designed control policy, the\nexistence of alternative access technology as a complement for a primary access\nnetwork can significantly improve the user experience without any bandwidth\nover-provisioning.", 
    "link": "http://arxiv.org/pdf/1203.3258v1", 
    "arxiv-id": "1203.3258v1"
},{
    "category": "cs.MM", 
    "author": "Ivan Selesnick", 
    "title": "Compressed Sensing for Moving Imagery in Medical Imaging", 
    "publish": "2012-03-26T19:44:26Z", 
    "summary": "Numerous applications in signal processing have benefited from the theory of\ncompressed sensing which shows that it is possible to reconstruct signals\nsampled below the Nyquist rate when certain conditions are satisfied. One of\nthese conditions is that there exists a known transform that represents the\nsignal with a sufficiently small number of non-zero coefficients. However when\nthe signal to be reconstructed is composed of moving images or volumes, it is\nchallenging to form such regularization constraints with traditional transforms\nsuch as wavelets. In this paper, we present a motion compensating prior for\nsuch signals that is derived directly from the optical flow constraint and can\nutilize the motion information during compressed sensing reconstruction.\nProposed regularization method can be used in a wide variety of applications\ninvolving compressed sensing and images or volumes of moving and deforming\nobjects. It is also shown that it is possible to estimate the signal and the\nmotion jointly or separately. Practical examples from magnetic resonance\nimaging has been presented to demonstrate the benefit of the proposed method.", 
    "link": "http://arxiv.org/pdf/1203.5772v1", 
    "arxiv-id": "1203.5772v1"
},{
    "category": "cs.DB", 
    "author": "Pavel Zezula", 
    "title": "Query Language for Complex Similarity Queries", 
    "publish": "2012-04-05T11:30:49Z", 
    "summary": "For complex data types such as multimedia, traditional data management\nmethods are not suitable. Instead of attribute matching approaches, access\nmethods based on object similarity are becoming popular. Recently, this\nresulted in an intensive research of indexing and searching methods for the\nsimilarity-based retrieval. Nowadays, many efficient methods are already\navailable, but using them to build an actual search system still requires\nspecialists that tune the methods and build the system manually. Several\nattempts have already been made to provide a more convenient high-level\ninterface in a form of query languages for such systems, but these are limited\nto support only basic similarity queries. In this paper, we propose a new\nlanguage that allows to formulate content-based queries in a flexible way,\ntaking into account the functionality offered by a particular search engine in\nuse. To ensure this, the language is based on a general data model with an\nabstract set of operations. Consequently, the language supports various\nadvanced query operations such as similarity joins, reverse nearest neighbor\nqueries, or distinct kNN queries, as well as multi-object and multi-modal\nqueries. The language is primarily designed to be used with the MESSIF\nframework for content-based searching but can be employed by other retrieval\nsystems as well.", 
    "link": "http://arxiv.org/pdf/1204.1185v1", 
    "arxiv-id": "1204.1185v1"
},{
    "category": "cs.MM", 
    "author": "Konstantinos Chorianopoulos", 
    "title": "User-based key frame detection in social web video", 
    "publish": "2012-04-09T12:23:36Z", 
    "summary": "Video search results and suggested videos on web sites are represented with a\nvideo thumbnail, which is manually selected by the video up-loader among three\nrandomly generated ones (e.g., YouTube). In contrast, we present a grounded\nuser-based approach for automatically detecting interesting key-frames within a\nvideo through aggregated users' replay interactions with the video player.\nPrevious research has focused on content-based systems that have the benefit of\nanalyzing a video without user interactions, but they are monolithic, because\nthe resulting video thumbnails are the same regardless of the user preferences.\nWe constructed a user interest function, which is based on aggregate video\nreplays, and analyzed hundreds of user interactions. We found that the local\nmaximum of the replaying activity stands for the semantics of information rich\nvideos, such as lecture, and how-to. The concept of user-based key-frame\ndetection could be applied to any video on the web, in order to generate a\nuser-based and dynamic video thumbnail in search results.", 
    "link": "http://arxiv.org/pdf/1204.1868v1", 
    "arxiv-id": "1204.1868v1"
},{
    "category": "cs.MM", 
    "author": "Bane Vasic", 
    "title": "Simplification Resilient LDPC-Coded Sparse-QIM Watermarking for   3D-Meshes", 
    "publish": "2012-04-10T16:41:33Z", 
    "summary": "We propose a blind watermarking scheme for 3-D meshes which combines sparse\nquantization index modulation (QIM) with deletion correction codes. The QIM\noperates on the vertices in rough concave regions of the surface thus ensuring\nimpeccability, while the deletion correction code recovers the data hidden in\nthe vertices which is removed by mesh optimization and/or simplification. The\nproposed scheme offers two orders of magnitude better performance in terms of\nrecovered watermark bit error rate compared to the existing schemes of similar\npayloads and fidelity constraints.", 
    "link": "http://arxiv.org/pdf/1204.2214v3", 
    "arxiv-id": "1204.2214v3"
},{
    "category": "cs.IT", 
    "author": "Jianhua Lu", 
    "title": "Rateless Codes with Progressive Recovery for Layered Multimedia Delivery", 
    "publish": "2012-04-16T07:52:00Z", 
    "summary": "This paper proposes a novel approach, based on unequal error protection, to\nenhance rateless codes with progressive recovery for layered multimedia\ndelivery. With a parallel encoding structure, the proposed Progressive Rateless\ncodes (PRC) assign unequal redundancy to each layer in accordance with their\nimportance. Each output symbol contains information from all layers, and thus\nthe stream layers can be recovered progressively at the expected received\nratios of output symbols. Furthermore, the dependency between layers is\nnaturally considered. The performance of the PRC is evaluated and compared with\nsome related UEP approaches. Results show that our PRC approach provides better\nrecovery performance with lower overhead both theoretically and numerically.", 
    "link": "http://arxiv.org/pdf/1204.3391v2", 
    "arxiv-id": "1204.3391v2"
},{
    "category": "cs.MM", 
    "author": "Konstantinos Chorianopoulos", 
    "title": "Efficient Video Indexing on the Web: A System that Leverages User   Interactions with a Video Player", 
    "publish": "2012-04-27T20:00:20Z", 
    "summary": "In this paper, we propose a user-based video indexing method, that\nautomatically generates thumbnails of the most important scenes of an online\nvideo stream, by analyzing users' interactions with a web video player. As a\ntest bench to verify our idea we have extended the YouTube video player into\nthe VideoSkip system. In addition, VideoSkip uses a web-database (Google\nApplication Engine) to keep a record of some important parameters, such as the\ntiming of basic user actions (play, pause, skip). Moreover, we implemented an\nalgorithm that selects representative thumbnails. Finally, we populated the\nsystem with data from an experiment with nine users. We found that the\nVideoSkip system indexes video content by leveraging implicit users\ninteractions, such as pause and thirty seconds skip. Our early findings point\ntoward improvements of the web video player and its thumbnail generation\ntechnique. The VideSkip system could compliment content-based algorithms, in\norder to achieve efficient video-indexing in difficult videos, such as lectures\nor sports.", 
    "link": "http://arxiv.org/pdf/1204.6321v1", 
    "arxiv-id": "1204.6321v1"
},{
    "category": "cs.NI", 
    "author": "Saira Beg", 
    "title": "Transference & Retrieval of Pulse-code modulation Audio over Short   Messaging Service", 
    "publish": "2012-05-19T21:25:17Z", 
    "summary": "The paper presents the method of transferring PCM (Pulse-Code Modulation)\nbased audio messages through SMS (Short Message Service) over GSM (Global\nSystem for Mobile Communications) network. As SMS is text based service, and\ncould not send voice. Our method enables voice transferring through SMS, by\nconverting PCM audio into characters. Than Huffman coding compression technique\nis applied in order to reduce numbers of characters which will latterly set as\npayload text of SMS. Testing the said method we develop an application using\nJ2me platform", 
    "link": "http://arxiv.org/pdf/1205.4361v1", 
    "arxiv-id": "1205.4361v1"
},{
    "category": "cs.CR", 
    "author": "Saraju P. Mohanty", 
    "title": "ISWAR: An Imaging System with Watermarking and Attack Resilience", 
    "publish": "2012-05-21T05:25:48Z", 
    "summary": "With the explosive growth of internet technology, easy transfer of digital\nmultimedia is feasible. However, this kind of convenience with which authorized\nusers can access information, turns out to be a mixed blessing due to\ninformation piracy. The emerging field of Digital Rights Management (DRM)\nsystems addresses issues related to the intellectual property rights of digital\ncontent. In this paper, an object-oriented (OO) DRM system, called \"Imaging\nSystem with Watermarking and Attack Resilience\" (ISWAR), is presented that\ngenerates and authenticates color images with embedded mechanisms for\nprotection against infringement of ownership rights as well as security\nattacks. In addition to the methods, in the object-oriented sense, for\nperforming traditional encryption and decryption, the system implements methods\nfor visible and invisible watermarking. This paper presents one visible and one\ninvisible watermarking algorithm that have been integrated in the system. The\nqualitative and quantitative results obtained for these two watermarking\nalgorithms with several benchmark images indicate that high-quality watermarked\nimages are produced by the algorithms. With the help of experimental results it\nis demonstrated that the presented invisible watermarking techniques are\nresilient to the well known benchmark attacks and hence a fail-safe method for\nproviding constant protection to ownership rights.", 
    "link": "http://arxiv.org/pdf/1205.4489v1", 
    "arxiv-id": "1205.4489v1"
},{
    "category": "cs.MM", 
    "author": "Alexander Y. Davydov", 
    "title": "Signal and Image Processing with Sinlets", 
    "publish": "2012-06-04T18:02:03Z", 
    "summary": "This paper presents a new family of localized orthonormal bases - sinlets -\nwhich are well suited for both signal and image processing and analysis.\nOne-dimensional sinlets are related to specific solutions of the time-dependent\nharmonic oscillator equation. By construction, each sinlet is infinitely\ndifferentiable and has a well-defined and smooth instantaneous frequency known\nin analytical form. For square-integrable transient signals with infinite\nsupport, one-dimensional sinlet basis provides an advantageous alternative to\nthe Fourier transform by rendering accurate signal representation via a\ncountable set of real-valued coefficients. The properties of sinlets make them\nsuitable for analyzing many real-world signals whose frequency content changes\nwith time including radar and sonar waveforms, music, speech, biological\necholocation sounds, biomedical signals, seismic acoustic waves, and signals\nemployed in wireless communication systems. One-dimensional sinlet bases can be\nused to construct two- and higher-dimensional bases with variety of potential\napplications including image analysis and representation.", 
    "link": "http://arxiv.org/pdf/1206.0692v3", 
    "arxiv-id": "1206.0692v3"
},{
    "category": "cs.DC", 
    "author": "Xiangmin Zhou", 
    "title": "MediaWise - Designing a Smart Media Cloud", 
    "publish": "2012-06-09T14:07:47Z", 
    "summary": "The MediaWise project aims to expand the scope of existing media delivery\nsystems with novel cloud, personalization and collaboration capabilities that\ncan serve the needs of more users, communities, and businesses. The project\ndevelops a MediaWise Cloud platform that supports do-it-yourself creation,\nsearch, management, and consumption of multimedia content. The MediaWise Cloud\nsupports pay-as-you-go models and elasticity that are similar to those offered\nby commercially available cloud services. However, unlike existing commercial\nCDN services providers such as Limelight Networks and Akamai the MediaWise\nCloud require no ownerships of computing infrastructure and instead rely on the\npublic Internet and public cloud services (e.g., commercial cloud storage to\nstore its content). In addition to integrating such public cloud services into\na public cloud-based Content Delivery Network, the MediaWise Cloud also\nprovides advanced Quality of Service (QoS) management as required for the\ndelivery of streamed and interactive high resolution multimedia content. In\nthis paper, we give a brief overview of MediaWise Cloud architecture and\npresent a comprehensive discussion on research objectives related to its\nservice components. Finally, we also compare the features supported by the\nexisting CDN services against the envisioned objectives of MediaWise Cloud.", 
    "link": "http://arxiv.org/pdf/1206.1943v2", 
    "arxiv-id": "1206.1943v2"
},{
    "category": "cs.MM", 
    "author": "Pavel Zezula", 
    "title": "Generic Subsequence Matching Framework: Modularity, Flexibility,   Efficiency", 
    "publish": "2012-06-12T12:39:04Z", 
    "summary": "Subsequence matching has appeared to be an ideal approach for solving many\nproblems related to the fields of data mining and similarity retrieval. It has\nbeen shown that almost any data class (audio, image, biometrics, signals) is or\ncan be represented by some kind of time series or string of symbols, which can\nbe seen as an input for various subsequence matching approaches. The variety of\ndata types, specific tasks and their partial or full solutions is so wide that\nthe choice, implementation and parametrization of a suitable solution for a\ngiven task might be complicated and time-consuming; a possibly fruitful\ncombination of fragments from different research areas may not be obvious nor\neasy to realize. The leading authors of this field also mention the\nimplementation bias that makes difficult a proper comparison of competing\napproaches. Therefore we present a new generic Subsequence Matching Framework\n(SMF) that tries to overcome the aforementioned problems by a uniform frame\nthat simplifies and speeds up the design, development and evaluation of\nsubsequence matching related systems. We identify several relatively separate\nsubtasks solved differently over the literature and SMF enables to combine them\nin straightforward manner achieving new quality and efficiency. This framework\ncan be used in many application domains and its components can be reused\neffectively. Its strictly modular architecture and openness enables also\ninvolvement of efficient solutions from different fields, for instance\nefficient metric-based indexes. This is an extended version of a paper\npublished on DEXA 2012.", 
    "link": "http://arxiv.org/pdf/1206.2510v1", 
    "arxiv-id": "1206.2510v1"
},{
    "category": "cs.CR", 
    "author": "Jacques M. Bahi", 
    "title": "Topological study and Lyapunov exponent of a secure steganographic   scheme", 
    "publish": "2012-06-13T15:44:05Z", 
    "summary": "CIS2 is a steganographic scheme proposed in the information hiding\nliterature, belonging into the small category of algorithms being both stego\nand topologically secure. Due to its stego-security, this scheme is able to\nface attacks that take place into the \"watermark only attack\" framework. Its\ntopological security reinforce its capability to face attacks in other\nframeworks as \"known message attack\" or \"known original attack\", in the\nSimmons' prisoner problem. In this research work, the study of topological\nproperties of C I S 2 is enlarged by describing this scheme as iterations over\nthe real line, and investigating other security properties of topological\nnature as the Lyapunov exponent. Results show that this scheme is able to\nwithdraw a malicious attacker in the \"estimated original attack\" context too.", 
    "link": "http://arxiv.org/pdf/1206.2847v2", 
    "arxiv-id": "1206.2847v2"
},{
    "category": "cs.CR", 
    "author": "David Dunson", 
    "title": "Bayesian Watermark Attacks", 
    "publish": "2012-06-18T15:30:35Z", 
    "summary": "This paper presents an application of statistical machine learning to the\nfield of watermarking. We propose a new attack model on additive\nspread-spectrum watermarking systems. The proposed attack is based on Bayesian\nstatistics. We consider the scenario in which a watermark signal is repeatedly\nembedded in specific, possibly chosen based on a secret message bitstream,\nsegments (signals) of the host data. The host signal can represent a patch of\npixels from an image or a video frame. We propose a probabilistic model that\ninfers the embedded message bitstream and watermark signal, directly from the\nwatermarked data, without access to the decoder. We develop an efficient Markov\nchain Monte Carlo sampler for updating the model parameters from their\nconjugate full conditional posteriors. We also provide a variational Bayesian\nsolution, which further increases the convergence speed of the algorithm.\nExperiments with synthetic and real image signals demonstrate that the attack\nmodel is able to correctly infer a large part of the message bitstream and\nobtain a very accurate estimate of the watermark signal.", 
    "link": "http://arxiv.org/pdf/1206.4662v1", 
    "arxiv-id": "1206.4662v1"
},{
    "category": "stat.ML", 
    "author": "St\u00e9phane Ayache", 
    "title": "PAC-Bayesian Majority Vote for Late Classifier Fusion", 
    "publish": "2012-07-04T15:09:05Z", 
    "summary": "A lot of attention has been devoted to multimedia indexing over the past few\nyears. In the literature, we often consider two kinds of fusion schemes: The\nearly fusion and the late fusion. In this paper we focus on late classifier\nfusion, where one combines the scores of each modality at the decision level.\nTo tackle this problem, we investigate a recent and elegant well-founded\nquadratic program named MinCq coming from the Machine Learning PAC-Bayes\ntheory. MinCq looks for the weighted combination, over a set of real-valued\nfunctions seen as voters, leading to the lowest misclassification rate, while\nmaking use of the voters' diversity. We provide evidence that this method is\nnaturally adapted to late fusion procedure. We propose an extension of MinCq by\nadding an order- preserving pairwise loss for ranking, helping to improve Mean\nAveraged Precision measure. We confirm the good behavior of the MinCq-based\nfusion approaches with experiments on a real image benchmark.", 
    "link": "http://arxiv.org/pdf/1207.1019v1", 
    "arxiv-id": "1207.1019v1"
},{
    "category": "cs.NI", 
    "author": "R. Srikant", 
    "title": "Real-Time Peer-to-Peer Streaming Over Multiple Random Hamiltonian Cycles", 
    "publish": "2012-07-12T22:12:52Z", 
    "summary": "We are motivated by the problem of designing a simple distributed algorithm\nfor Peer-to-Peer streaming applications that can achieve high throughput and\nlow delay, while allowing the neighbor set maintained by each peer to be small.\nWhile previous works have mostly used tree structures, our algorithm constructs\nmultiple random directed Hamiltonian cycles and disseminates content over the\nsuperposed graph of the cycles. We show that it is possible to achieve the\nmaximum streaming capacity even when each peer only transmits to and receives\nfrom Theta(1) neighbors. Further, we show that the proposed algorithm achieves\nthe streaming delay of Theta(log N) when the streaming rate is less than\n(1-1/K) of the maximum capacity for any fixed integer K>1, where N denotes the\nnumber of peers in the network. The key theoretical contribution is to\ncharacterize the distance between peers in a graph formed by the superposition\nof directed random Hamiltonian cycles, in which edges from one of the cycles\nmay be dropped at random. We use Doob martingales and graph expansion ideas to\ncharacterize this distance as a function of N, with high probability.", 
    "link": "http://arxiv.org/pdf/1207.3110v5", 
    "arxiv-id": "1207.3110v5"
},{
    "category": "cs.HC", 
    "author": "Giuliano Gaia", 
    "title": "Usability, Design and Content Issues of Mobile Apps for Cultural   Heritage Promotion: The Malta Culture Guide Experience", 
    "publish": "2012-07-14T13:10:10Z", 
    "summary": "The paper discusses the experience of producing and distributing an iPhone\napp for promotion of the Maltese Cultural Heritage on behalf of the Malta\nTourism Authority. Thanks to its position at the heart of the Mediterranean\nSea, Malta has been a crossroads of civilisations whose traces are still\nvisible today, leaving a particularly rich and varied cultural heritage, from\nmegalithic temples to baroque palaces and Caravaggio masterpieces. Conveying\nall these different aspects within a single application, using textual, visual,\nand audio means, has raised many different issues about the planning and\nproduction of cultural content for mobile usage, together with usability\naspects regarding design and distribution of a mobile app. In this paper, we\noutline all of these aspects, focusing on the design and planning strategies\nfor a long-term user commitment and how to evaluate results for cultural mobile\napplications. We include experience of all the steps of developing a mobile\napp, information that is of possible benefit to other app developers in the\ncultural sector.", 
    "link": "http://arxiv.org/pdf/1207.3422v1", 
    "arxiv-id": "1207.3422v1"
},{
    "category": "cs.CR", 
    "author": "Yaser Sadra", 
    "title": "Public key Steganography Using Discrete Cross-Coupled Chaotic Maps", 
    "publish": "2012-11-01T04:09:10Z", 
    "summary": "By cross-coupling two logistic maps a novel method is proposed for the public\nkey steganography in JPEG image. Chaotic maps entail high complexity in the\nused algorithm for embedding secret data in a medium. In this paper, discrete\ncross- coupled chaotic maps are used to specifying the location of the\ndifferent parts of the secret data in the image. Modifying JPEG format during\ncompressing and decompressing, and also using public key enhanced difficulty of\nthe algorithm. Simulation results show that in addition to excessive capacity,\nthis method has high robustness and resistance against hackers and can be\napplicable in secret communication. Also the PSNR value is high compared to the\nother works.", 
    "link": "http://arxiv.org/pdf/1211.0086v1", 
    "arxiv-id": "1211.0086v1"
},{
    "category": "cs.MM", 
    "author": "Martin Vetterli", 
    "title": "Sampling and Reconstruction of Spatial Fields using Mobile Sensors", 
    "publish": "2012-11-01T10:05:46Z", 
    "summary": "Spatial sampling is traditionally studied in a static setting where static\nsensors scattered around space take measurements of the spatial field at their\nlocations. In this paper we study the emerging paradigm of sampling and\nreconstructing spatial fields using sensors that move through space. We show\nthat mobile sensing offers some unique advantages over static sensing in\nsensing time-invariant bandlimited spatial fields. Since a moving sensor\nencounters such a spatial field along its path as a time-domain signal, a\ntime-domain anti-aliasing filter can be employed prior to sampling the signal\nreceived at the sensor. Such a filtering procedure, when used by a\nconfiguration of sensors moving at constant speeds along equispaced parallel\nlines, leads to a complete suppression of spatial aliasing in the direction of\nmotion of the sensors. We analytically quantify the advantage of using such a\nsampling scheme over a static sampling scheme by computing the reduction in\nsampling noise due to the filter. We also analyze the effects of non-uniform\nsensor speeds on the reconstruction accuracy. Using simulation examples we\ndemonstrate the advantages of mobile sampling over static sampling in practical\nproblems.\n  We extend our analysis to sampling and reconstruction schemes for monitoring\ntime-varying bandlimited fields using mobile sensors. We demonstrate that in\nsome situations we require a lower density of sensors when using a mobile\nsensing scheme instead of the conventional static sensing scheme. The exact\nadvantage is quantified for a problem of sampling and reconstructing an audio\nfield.", 
    "link": "http://arxiv.org/pdf/1211.0135v1", 
    "arxiv-id": "1211.0135v1"
},{
    "category": "cs.IT", 
    "author": "Jarek Duda", 
    "title": "Embedding grayscale halftone pictures in QR Codes using Correction Trees", 
    "publish": "2012-11-07T15:19:23Z", 
    "summary": "Barcodes like QR Codes have made that encoded messages have entered our\neveryday life, what suggests to attach them a second layer of information:\ndirectly available to human receiver for informational or marketing purposes.\nWe will discuss a general problem of using codes with chosen statistical\nconstrains, for example reproducing given grayscale picture using halftone\ntechnique. If both sender and receiver know these constrains, the optimal\ncapacity can be easily approached by entropy coder. The problem is that this\ntime only the sender knows them - we will refer to these scenarios as\nconstrained coding. Kuznetsov and Tsybakov problem in which only the sender\nknows which bits are fixed can be seen as a special case, surprisingly\napproaching the same capacity as if both sides would know the constrains. We\nwill analyze Correction Trees to approach analogous capacity in the general\ncase - use weaker: statistical constrains, what allows to apply them to all\nbits. Finding satisfying coding is similar to finding the proper correction in\nerror correction problem, but instead of single ensured possibility, there are\nnow statistically expected some. While in standard steganography we hide\ninformation in the least important bits, this time we create codes resembling\ngiven picture - hide information in the freedom of realizing grayness by black\nand white pixels using halftone technique. We will also discuss combining with\nerror correction and application to rate distortion problem.", 
    "link": "http://arxiv.org/pdf/1211.1572v3", 
    "arxiv-id": "1211.1572v3"
},{
    "category": "cs.CG", 
    "author": "Xianfeng Gu", 
    "title": "Teichm\u00fcller extremal mapping and its applications to landmark matching   registration", 
    "publish": "2012-11-12T11:16:31Z", 
    "summary": "Registration, which aims to find an optimal 1-1 correspondence between\nshapes, is an important process in different research areas. Conformal mappings\nhave been widely used to obtain a diffeomorphism between shapes that minimizes\nangular distortion. Conformal registrations are beneficial since it preserves\nthe local geometry well. However, when landmark constraints are enforced,\nconformal mappings generally do not exist. This motivates us to look for a\nunique landmark matching quasi-conformal registration, which minimizes the\nconformality distortion. Under suitable condition on the landmark constraints,\na unique diffeomporphism, called the Teichm\\\"uller extremal mapping between two\nsurfaces can be obtained, which minimizes the maximal conformality distortion.\nIn this paper, we propose an efficient iterative algorithm, called the\nQuasi-conformal (QC) iterations, to compute the Teichm\\\"uller mapping. The\nbasic idea is to represent the set of diffeomorphisms using Beltrami\ncoefficients (BCs), and look for an optimal BC associated to the desired\nTeichm\\\"uller mapping. The associated diffeomorphism can be efficiently\nreconstructed from the optimal BC using the Linear Beltrami Solver(LBS). Using\nBCs to represent diffeomorphisms guarantees the diffeomorphic property of the\nregistration. Using our proposed method, the Teichm\\\"uller mapping can be\naccurately and efficiently computed within 10 seconds. The obtained\nregistration is guaranteed to be bijective. The proposed algorithm can also be\nextended to compute Teichm\\\"uller mapping with soft landmark constraints. We\napplied the proposed algorithm to real applications, such as brain landmark\nmatching registration, constrained texture mapping and human face registration.\nExperimental results shows that our method is both effective and efficient in\ncomputing a non-overlap landmark matching registration with least amount of\nconformality distortion.", 
    "link": "http://arxiv.org/pdf/1211.2569v1", 
    "arxiv-id": "1211.2569v1"
},{
    "category": "cs.IT", 
    "author": "Pascal Frossard", 
    "title": "Intermediate Performance Analysis of Growth Codes", 
    "publish": "2012-11-16T20:32:57Z", 
    "summary": "Growth codes are a subclass of Rateless codes that have found interesting\napplications in data dissemination problems. Compared to other Rateless and\nconventional channel codes, Growth codes show improved intermediate performance\nwhich is particularly useful in applications where performance increases with\nthe number of decoded data units. In this paper, we provide a generic\nanalytical framework for studying the asymptotic performance of Growth codes in\ndifferent settings. Our analysis based on Wormald method applies to any class\nof Rateless codes that does not include a precoding step. We evaluate the\ndecoding probability model for short codeblocks and validate our findings by\nexperiments. We then exploit the decoding probability model in an illustrative\napplication of Growth codes to error resilient video transmission. The video\ntransmission problem is cast as a joint source and channel rate allocation\nproblem that is shown to be convex with respect to the channel rate. This\napplication permits to highlight the main advantage of Growth codes that is\nimproved performance (hence distortion in video) in the intermediate loss\nregion.", 
    "link": "http://arxiv.org/pdf/1211.4014v1", 
    "arxiv-id": "1211.4014v1"
},{
    "category": "cs.MM", 
    "author": "Alan Hanjalic", 
    "title": "Corpus Development for Affective Video Indexing", 
    "publish": "2012-11-23T13:06:25Z", 
    "summary": "Affective video indexing is the area of research that develops techniques to\nautomatically generate descriptions of video content that encode the emotional\nreactions which the video content evokes in viewers. This paper provides a set\nof corpus development guidelines based on state-of-the-art practice intended to\nsupport researchers in this field. Affective descriptions can be used for video\nsearch and browsing systems offering users affective perspectives. The paper is\nmotivated by the observation that affective video indexing has yet to fully\nprofit from the standard corpora (data sets) that have benefited conventional\nforms of video indexing. Affective video indexing faces unique challenges,\nsince viewer-reported affective reactions are difficult to assess. Moreover\naffect assessment efforts must be carefully designed in order to both cover the\ntypes of affective responses that video content evokes in viewers and also\ncapture the stable and consistent aspects of these responses. We first present\nbackground information on affect and multimedia and related work on affective\nmultimedia indexing, including existing corpora. Three dimensions emerge as\ncritical for affective video corpora, and form the basis for our proposed\nguidelines: the context of viewer response, personal variation among viewers,\nand the effectiveness and efficiency of corpus creation. Finally, we present\nexamples of three recent corpora and discuss how these corpora make progressive\nsteps towards fulfilling the guidelines.", 
    "link": "http://arxiv.org/pdf/1211.5492v3", 
    "arxiv-id": "1211.5492v3"
},{
    "category": "cs.CR", 
    "author": "M. Sharma", 
    "title": "A Hash based Approach for Secure Keyless Steganography in Lossless RGB   Images", 
    "publish": "2012-11-23T21:34:53Z", 
    "summary": "This paper proposes an improved steganography approach for hiding text\nmessages in lossless RGB images. The objective of this work is to increase the\nsecurity level and to improve the storage capacity with compression techniques.\nThe security level is increased by randomly distributing the text message over\nthe entire image instead of clustering within specific image portions. Storage\ncapacity is increased by utilizing all the color channels for storing\ninformation and providing the source text message compression. The degradation\nof the images can be minimized by changing only one least significant bit per\ncolor channel for hiding the message, incurring a very little change in the\noriginal image. Using steganography alone with simple LSB has a potential\nproblem that the secret message is easily detectable from the histogram\nanalysis method. To improve the security as well as the image embedding\ncapacity indirectly, a compression based scheme is introduced. Various tests\nhave been done to check the storage capacity and message distribution. These\ntestes show the superiority of the proposed approach with respect to other\nexisting approaches.", 
    "link": "http://arxiv.org/pdf/1211.5614v5", 
    "arxiv-id": "1211.5614v5"
},{
    "category": "math.DG", 
    "author": "Xianfeng Gu", 
    "title": "A Conformal Approach for Surface Inpainting", 
    "publish": "2012-12-05T10:01:23Z", 
    "summary": "We address the problem of surface inpainting, which aims to fill in holes or\nmissing regions on a Riemann surface based on its surface geometry. In\npractical situation, surfaces obtained from range scanners often have holes\nwhere the 3D models are incomplete. In order to analyze the 3D shapes\neffectively, restoring the incomplete shape by filling in the surface holes is\nnecessary. In this paper, we propose a novel conformal approach to inpaint\nsurface holes on a Riemann surface based on its surface geometry. The basic\nidea is to represent the Riemann surface using its conformal factor and mean\ncurvature. According to Riemann surface theory, a Riemann surface can be\nuniquely determined by its conformal factor and mean curvature up to a rigid\nmotion. Given a Riemann surface $S$, its mean curvature $H$ and conformal\nfactor $\\lambda$ can be computed easily through its conformal parameterization.\nConversely, given $\\lambda$ and $H$, a Riemann surface can be uniquely\nreconstructed by solving the Gauss-Codazzi equation on the conformal parameter\ndomain. Hence, the conformal factor and the mean curvature are two geometric\nquantities fully describing the surface. With this $\\lambda$-$H$ representation\nof the surface, the problem of surface inpainting can be reduced to the problem\nof image inpainting of $\\lambda$ and $H$ on the conformal parameter domain.\nOnce $\\lambda$ and $H$ are inpainted, a Riemann surface can be reconstructed\nwhich effectively restores the 3D surface with missing holes. Since the\ninpainting model is based on the geometric quantities $\\lambda$ and $H$, the\nrestored surface follows the surface geometric pattern. We test the proposed\nalgorithm on synthetic data as well as real surface data. Experimental results\nshow that our proposed method is an effective surface inpainting algorithm to\nfill in surface holes on an incomplete 3D models based their surface geometry.", 
    "link": "http://arxiv.org/pdf/1212.0981v1", 
    "arxiv-id": "1212.0981v1"
},{
    "category": "cs.IT", 
    "author": "Youssef Nasser", 
    "title": "Enhanced Mobile Digital Video Broadcasting with Distributed Space-Time   Coding", 
    "publish": "2012-12-11T09:18:43Z", 
    "summary": "This paper investigates the distributed space-time (ST) coding proposals for\nthe future Digital Video Broadcasting--Next Generation Handheld (DVB-NGH)\nstandard. We first theoretically show that the distributed MIMO scheme is the\nbest broadcasting scenario in terms of channel capacity. Consequently we\nevaluate the performance of several ST coding proposals for DVB-NGH with\npractical system specifications and channel conditions. Simulation results\ndemonstrate that the 3D code is the best ST coding solution for broadcasting in\nthe distributed MIMO scenario.", 
    "link": "http://arxiv.org/pdf/1212.2345v1", 
    "arxiv-id": "1212.2345v1"
},{
    "category": "cs.CV", 
    "author": "Svetlana Lazebnik", 
    "title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and   their Semantics", 
    "publish": "2012-12-18T22:02:43Z", 
    "summary": "This paper investigates the problem of modeling Internet images and\nassociated text or tags for tasks such as image-to-image search, tag-to-image\nsearch, and image-to-tag search (image annotation). We start with canonical\ncorrelation analysis (CCA), a popular and successful approach for mapping\nvisual and textual features to the same latent space, and incorporate a third\nview capturing high-level image semantics, represented either by a single\ncategory or multiple non-mutually-exclusive concepts. We present two ways to\ntrain the three-view embedding: supervised, with the third view coming from\nground-truth labels or search keywords; and unsupervised, with semantic themes\nautomatically obtained by clustering the tags. To ensure high accuracy for\nretrieval tasks while keeping the learning process scalable, we combine\nmultiple strong visual features and use explicit nonlinear kernel mappings to\nefficiently approximate kernel CCA. To perform retrieval, we use a specially\ndesigned similarity function in the embedded space, which substantially\noutperforms the Euclidean distance. The resulting system produces compelling\nqualitative results and outperforms a number of two-view baselines on retrieval\ntasks on three large-scale Internet image datasets.", 
    "link": "http://arxiv.org/pdf/1212.4522v2", 
    "arxiv-id": "1212.4522v2"
},{
    "category": "cs.MM", 
    "author": "Miao Song", 
    "title": "Computer-Assisted Interactive Documentary and Performance Arts in   Illimitable Space", 
    "publish": "2012-12-26T20:49:45Z", 
    "summary": "This major component of the research described in this thesis is 3D computer\ngraphics, specifically the realistic physics-based softbody simulation and\nhaptic responsive environments. Minor components include advanced\nhuman-computer interaction environments, non-linear documentary storytelling,\nand theatre performance. The journey of this research has been unusual because\nit requires a researcher with solid knowledge and background in multiple\ndisciplines; who also has to be creative and sensitive in order to combine the\npossible areas into a new research direction. [...] It focuses on the advanced\ncomputer graphics and emerges from experimental cinematic works and theatrical\nartistic practices. Some development content and installations are completed to\nprove and evaluate the described concepts and to be convincing. [...] To\nsummarize, the resulting work involves not only artistic creativity, but\nsolving or combining technological hurdles in motion tracking, pattern\nrecognition, force feedback control, etc., with the available documentary\nfootage on film, video, or images, and text via a variety of devices [....] and\nprogramming, and installing all the needed interfaces such that it all works in\nreal-time. Thus, the contribution to the knowledge advancement is in solving\nthese interfacing problems and the real-time aspects of the interaction that\nhave uses in film industry, fashion industry, new age interactive theatre,\ncomputer games, and web-based technologies and services for entertainment and\neducation. It also includes building up on this experience to integrate Kinect-\nand haptic-based interaction, artistic scenery rendering, and other forms of\ncontrol. This research work connects all the research disciplines, seemingly\ndisjoint fields of research, such as computer graphics, documentary film,\ninteractive media, and theatre performance together.", 
    "link": "http://arxiv.org/pdf/1212.6250v1", 
    "arxiv-id": "1212.6250v1"
},{
    "category": "cs.MM", 
    "author": "John Scoville", 
    "title": "Bounding Lossy Compression using Lossless Codes at Reduced Precision", 
    "publish": "2012-12-31T22:42:02Z", 
    "summary": "An alternative approach to two-part 'critical compression' is presented.\nWhereas previous results were based on summing a lossless code at reduced\nprecision with a lossy-compressed error or noise term, the present approach\nuses a similar lossless code at reduced precision to establish absolute bounds\nwhich constrain an arbitrary lossy data compression algorithm applied to the\noriginal data.", 
    "link": "http://arxiv.org/pdf/1301.0026v1", 
    "arxiv-id": "1301.0026v1"
},{
    "category": "cs.MM", 
    "author": "Nagappa U. Bhajantri", 
    "title": "An Extensive Analysis of Query by Singing/Humming System Through Query   Proportion", 
    "publish": "2013-01-09T15:36:40Z", 
    "summary": "Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system\nwith small audio excerpt as query. The rising availability of digital music\nstipulates effective music retrieval methods. Further, MIR systems support\ncontent based searching for music and requires no musical acquaintance. Current\nwork on QBSH focuses mainly on melody features such as pitch, rhythm, note\netc., size of databases, response time, score matching and search algorithms.\nEven though a variety of QBSH techniques are proposed, there is a dearth of\nwork to analyze QBSH through query excerption. Here, we present an analysis\nthat works on QBSH through query excerpt. To substantiate a series of\nexperiments are conducted with the help of Mel-Frequency Cepstral Coefficients\n(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral\nCoefficients (LPCC) to portray the robustness of the knowledge representation.\nProposed experiments attempt to reveal that retrieval performance as well as\nprecision diminishes in the snail phase with the growing database size.", 
    "link": "http://arxiv.org/pdf/1301.1894v1", 
    "arxiv-id": "1301.1894v1"
},{
    "category": "cs.IT", 
    "author": "Robert W. Heath Jr", 
    "title": "Loss Visibility Optimized Real-time Video Transmission over MIMO Systems", 
    "publish": "2013-01-14T22:21:36Z", 
    "summary": "The structured nature of video data motivates introducing video-aware\ndecisions that make use of this structure for improved video transmission over\nwireless networks. In this paper, we introduce an architecture for real-time\nvideo transmission over multiple-input multiple-output (MIMO) wireless\ncommunication systems using loss visibility side information. We quantify the\nperceptual importance of a packet through the packet loss visibility and use\nthe loss visibility distribution to provide a notion of relative packet\nimportance. To jointly achieve video quality and low latency, we define the\noptimization objective function as the throughput weighted by the loss\nvisibility of each packet, a proxy for the total perceptual value of successful\npackets per unit time. We solve the problem of mapping video packets to MIMO\nsubchannels and adapting per-stream rates to maximize the proposed objective.\nWe show that the solution enables jointly reaping gains in terms of improved\nvideo quality and lower latency. Optimized packet-stream mapping enables\ntransmission of more relevant packets over more reliable streams while unequal\nmodulation opportunistically increases the transmission rate on the stronger\nstreams to enable low latency delivery of high priority packets. We extend the\nsolution to capture codebook-based limited feedback and MIMO mode adaptation.\nResults show that the composite quality and throughput gains are significant\nunder full channel state information as well as limited feedback. Tested on\nH.264-encoded video sequences, for a 4x4 MIMO with 3 spatial streams, the\nproposed architecture achieves 8 dB power reduction for the same video quality\nand supports 2.4x higher throughput due to unequal modulation. Furthermore, the\ngains are achieved at the expense of few bits of cross-layer overhead rather\nthan a complex cross-layer design.", 
    "link": "http://arxiv.org/pdf/1301.3174v2", 
    "arxiv-id": "1301.3174v2"
},{
    "category": "cs.IT", 
    "author": "Michelle Effros", 
    "title": "Multi-Resolution Video Streaming in Peer-to-peer Networks", 
    "publish": "2013-04-06T05:50:09Z", 
    "summary": "We consider multi-resolution streaming in fully-connected peer-to-peer\nnetworks, where transmission rates are constrained by arbitrarily specified\nupload capacities of the source and peers. We fully characterize the capacity\nregion of rate vectors achievable with arbitrary coding, where an achievable\nrate vector describes a vector of throughputs of the different resolutions that\ncan be supported by the network. We then prove that all rate vectors in the\ncapacity region can be achieved using pure routing strategies. This shows that\ncoding has no capacity advantage over routing in this scenario.", 
    "link": "http://arxiv.org/pdf/1304.1858v2", 
    "arxiv-id": "1304.1858v2"
},{
    "category": "cs.MM", 
    "author": "Stefan Valentin", 
    "title": "Anticipatory Buffer Control and Resource Allocation for Wireless Video   Streaming", 
    "publish": "2013-04-10T19:12:38Z", 
    "summary": "This paper describes a new approach for allocating resources to video\nstreaming traffic. Assuming that the future channel state can be predicted for\na certain time, we minimize the fraction of the bandwidth consumed for smooth\nstreaming by jointly allocating wireless channel resources and play-out buffer\nsize. To formalize this idea, we introduce a new model to capture the dynamic\nof a video streaming buffer and the allocated spectrum in an optimization\nproblem. The result is a Linear Program that allows to trade off buffer size\nand allocated bandwidth. Based on this tractable model, our simulation results\nshow that anticipating poor channel states and pre-loading the buffer\naccordingly allows to serve more users at perfect video quality.", 
    "link": "http://arxiv.org/pdf/1304.3056v1", 
    "arxiv-id": "1304.3056v1"
},{
    "category": "cs.CV", 
    "author": "C\u00e9line Hudelot", 
    "title": "Combinaison d'information visuelle, conceptuelle, et contextuelle pour   la construction automatique de hierarchies semantiques adaptees a   l'annotation d'images", 
    "publish": "2013-04-18T09:40:12Z", 
    "summary": "This paper proposes a new methodology to automatically build semantic\nhierarchies suitable for image annotation and classification. The building of\nthe hierarchy is based on a new measure of semantic similarity. The proposed\nmeasure incorporates several sources of information: visual, conceptual and\ncontextual as we defined in this paper. The aim is to provide a measure that\nbest represents image semantics. We then propose rules based on this measure,\nfor the building of the final hierarchy, and which explicitly encode\nhierarchical relationships between different concepts. Therefore, the built\nhierarchy is used in a semantic hierarchical classification framework for image\nannotation. Our experiments and results show that the hierarchy built improves\nclassification results.\n  Ce papier propose une nouvelle methode pour la construction automatique de\nhierarchies semantiques adaptees a la classification et a l'annotation\nd'images. La construction de la hierarchie est basee sur une nouvelle mesure de\nsimilarite semantique qui integre plusieurs sources d'informations: visuelle,\nconceptuelle et contextuelle que nous definissons dans ce papier. L'objectif\nest de fournir une mesure qui est plus proche de la semantique des images. Nous\nproposons ensuite des regles, basees sur cette mesure, pour la construction de\nla hierarchie finale qui encode explicitement les relations hierarchiques entre\nles differents concepts. La hierarchie construite est ensuite utilisee dans un\ncadre de classification semantique hierarchique d'images en concepts visuels.\nNos experiences et resultats montrent que la hierarchie construite permet\nd'ameliorer les resultats de la classification.", 
    "link": "http://arxiv.org/pdf/1304.5063v2", 
    "arxiv-id": "1304.5063v2"
},{
    "category": "cs.DB", 
    "author": "A. Fr. Zung", 
    "title": "A new Watermarking Technique for Secure Database", 
    "publish": "2013-04-26T08:46:42Z", 
    "summary": "Digital multimedia watermarking technology was suggested in the last decade\nto embed copyright information in digital objects such images, audio and video.\nHowever, the increasing use of relational database systems in many real-life\napplications created an ever increasing need for watermarking database systems.\nAs a result, watermarking relational database systems is now merging as a\nresearch area that deals with the legal issue of copyright protection of\ndatabase systems. Approach: In this study, we proposed an efficient database\nwatermarking algorithm based on inserting binary image watermarks in\nnon-numeric mutli-word attributes of selected database tuples. Results: The\nalgorithm is robust as it resists attempts to remove or degrade the embedded\nwatermark and it is blind as it does not require the original database in order\nto extract the embedded watermark. Conclusion: Experimental results\ndemonstrated blindness and the robustness of the algorithm against common\ndatabase attacks.", 
    "link": "http://arxiv.org/pdf/1304.7094v1", 
    "arxiv-id": "1304.7094v1"
},{
    "category": "cs.DB", 
    "author": "Rajesh Kumar Tiwari", 
    "title": "A Novel approach for Hybrid Database", 
    "publish": "2013-04-26T08:50:54Z", 
    "summary": "In the current world of economic crises, the cost control is one of the chief\nconcerns for all types of industries, especially for the small venders. The\nsmall vendors are suppose to minimize their budget on Information Technology by\nreducing the initial investment in hardware and costly database servers like\nORACLE, SQL Server, SYBASE, etc. for the purpose of data processing and\nstoring. In other divisions, the electronic devices manufacturing companies\nwant to increase the demand and reduce the manufacturing cost by introducing\nthe low cost technologies. The new small devices like ipods, iphones, palm top\netc. are now-a-days used as data computation and storing tools. For both the\ncases mentioned above, instead of going for the costly database servers which\nadditionally requires extra hardware as well as the extra expenses in training\nand handling, the flat file may be considered as a candidate due to its easy\nhandling nature, fast accessing, and of course free of cost. But the main\nhurdle is the security aspects which are not up to the optimum level. In this\npaper, we propose a methodology that combines all the merit of the flat file\nand with the help of a novel steganographic technique we can maintain the\nutmost security fence. The new proposed methodology will undoubtedly be highly\nbeneficial for small vendors as well as for the above said electronic devices\nmanufacturer", 
    "link": "http://arxiv.org/pdf/1304.7096v1", 
    "arxiv-id": "1304.7096v1"
},{
    "category": "cs.DS", 
    "author": "Kannan Ramchandran", 
    "title": "Computing a k-sparse n-length Discrete Fourier Transform using at most   4k samples and O(k log k) complexity", 
    "publish": "2013-05-04T02:54:59Z", 
    "summary": "Given an $n$-length input signal $\\mbf{x}$, it is well known that its\nDiscrete Fourier Transform (DFT), $\\mbf{X}$, can be computed in $O(n \\log n)$\ncomplexity using a Fast Fourier Transform (FFT). If the spectrum $\\mbf{X}$ is\nexactly $k$-sparse (where $k<<n$), can we do better? We show that\nasymptotically in $k$ and $n$, when $k$ is sub-linear in $n$ (precisely, $k\n\\propto n^{\\delta}$ where $0 < \\delta <1$), and the support of the non-zero DFT\ncoefficients is uniformly random, we can exploit this sparsity in two\nfundamental ways (i) {\\bf {sample complexity}}: we need only $M=rk$\ndeterministically chosen samples of the input signal $\\mbf{x}$ (where $r < 4$\nwhen $0 < \\delta < 0.99$); and (ii) {\\bf {computational complexity}}: we can\nreliably compute the DFT $\\mbf{X}$ using $O(k \\log k)$ operations, where the\nconstants in the big Oh are small and are related to the constants involved in\ncomputing a small number of DFTs of length approximately equal to the sparsity\nparameter $k$. Our algorithm succeeds with high probability, with the\nprobability of failure vanishing to zero asymptotically in the number of\nsamples acquired, $M$.", 
    "link": "http://arxiv.org/pdf/1305.0870v2", 
    "arxiv-id": "1305.0870v2"
},{
    "category": "cs.IT", 
    "author": "Michael J. Neely", 
    "title": "Utility Optimal Scheduling and Admission Control for Adaptive Video   Streaming in Small Cell Networks", 
    "publish": "2013-05-15T18:56:03Z", 
    "summary": "We consider the jointly optimal design of a transmission scheduling and\nadmission control policy for adaptive video streaming over small cell networks.\nWe formulate the problem as a dynamic network utility maximization and observe\nthat it naturally decomposes into two subproblems: admission control and\ntransmission scheduling. The resulting algorithms are simple and suitable for\ndistributed implementation. The admission control decisions involve each user\nchoosing the quality of the video chunk asked for download, based on the\nnetwork congestion in its neighborhood. This form of admission control is\ncompatible with the current video streaming technology based on the DASH\nprotocol over TCP connections. Through simulations, we evaluate the performance\nof the proposed algorithm under realistic assumptions for a small-cell network.", 
    "link": "http://arxiv.org/pdf/1305.3586v1", 
    "arxiv-id": "1305.3586v1"
},{
    "category": "cs.GR", 
    "author": "Min Wu", 
    "title": "Sparse Norm Filtering", 
    "publish": "2013-05-17T03:13:28Z", 
    "summary": "Optimization-based filtering smoothes an image by minimizing a fidelity\nfunction and simultaneously preserves edges by exploiting a sparse norm penalty\nover gradients. It has obtained promising performance in practical problems,\nsuch as detail manipulation, HDR compression and deblurring, and thus has\nreceived increasing attentions in fields of graphics, computer vision and image\nprocessing. This paper derives a new type of image filter called sparse norm\nfilter (SNF) from optimization-based filtering. SNF has a very simple form,\nintroduces a general class of filtering techniques, and explains several\nclassic filters as special implementations of SNF, e.g. the averaging filter\nand the median filter. It has advantages of being halo free, easy to implement,\nand low time and memory costs (comparable to those of the bilateral filter).\nThus, it is more generic than a smoothing operator and can better adapt to\ndifferent tasks. We validate the proposed SNF by a wide variety of applications\nincluding edge-preserving smoothing, outlier tolerant filtering, detail\nmanipulation, HDR compression, non-blind deconvolution, image segmentation, and\ncolorization.", 
    "link": "http://arxiv.org/pdf/1305.3971v1", 
    "arxiv-id": "1305.3971v1"
},{
    "category": "cs.IT", 
    "author": "Andreas F. Molisch", 
    "title": "Wireless Device-to-Device Caching Networks: Basic Principles and System   Performance", 
    "publish": "2013-05-22T18:03:46Z", 
    "summary": "As wireless video transmission is the fastest-growing form of data traffic,\nmethods for spectrally efficient video on-demand wireless streaming are\nessential to service providers and users alike. A key property of video\non-demand is the asynchronous content reuse, such that a few dominant videos\naccount for a large part of the traffic, but are viewed by users at different\ntimes. Caching of content on devices in conjunction with D2D communications\nallows to exploit this property, and provide a network throughput that is\nsignificantly in excess of both the conventional approach of unicasting from\nthe base station and the traditional D2D networks for regular data traffic.\nThis paper presents in a semi-tutorial concise form some recent results on the\nthroughput scaling laws of wireless networks with caching and asynchronous\ncontent reuse, contrasting the D2D approach with a competing approach based on\ncombinatorial cache design and network coded transmission from the base station\n(BS) only, referred to as coded multicasting. Interestingly, the spatial reuse\ngain of the former and the coded multicasting gain of the latter yield, somehow\nsurprisingly, the same near-optimal throughput behavior in the relevant regime\nwhere the number of video files in the library is smaller than the number of\nstreaming users. Based on our recent theoretical results, we propose a holistic\nD2D system design that incorporates traditional microwave (2 GHz) as well as\nmillimeter-wave D2D links; the direct connections to the base station can be\nused to provide those rare video requests that cannot be found in local caches.\nWe provide extensive simulations under a variety of system settings, and\ncompare our scheme with other existing schemes by the BS. We show that, despite\nthe similar behavior of the scaling laws, the proposed D2D approach offers very\nsignificant throughput gains with respect to the BS-only schemes.", 
    "link": "http://arxiv.org/pdf/1305.5216v2", 
    "arxiv-id": "1305.5216v2"
},{
    "category": "cs.IT", 
    "author": "John Scoville", 
    "title": "Fast Autocorrelated Context Models for Data Compression", 
    "publish": "2013-05-23T17:11:39Z", 
    "summary": "A method is presented to automatically generate context models of data by\ncalculating the data's autocorrelation function. The largest values of the\nautocorrelation function occur at the offsets or lags in the bitstream which\ntend to be the most highly correlated to any particular location. These offsets\nare ideal for use in predictive coding, such as predictive partial match (PPM)\nor context-mixing algorithms for data compression, making such algorithms more\nefficient and more general by reducing or eliminating the need for ad-hoc\nmodels based on particular types of data. Instead of using the definition of\nthe autocorrelation function, which considers the pairwise correlations of data\nrequiring O(n^2) time, the Weiner-Khinchin theorem is applied, quickly\nobtaining the autocorrelation as the inverse Fast Fourier transform of the\ndata's power spectrum in O(n log n) time, making the technique practical for\nthe compression of large data objects. The method is shown to produce the\nhighest levels of performance obtained to date on a lossless image compression\nbenchmark.", 
    "link": "http://arxiv.org/pdf/1305.5486v2", 
    "arxiv-id": "1305.5486v2"
},{
    "category": "cs.IT", 
    "author": "Syed Mahfuzul Aziz", 
    "title": "Resource Efficient LDPC Decoders for Multimedia Communication", 
    "publish": "2013-05-27T13:38:22Z", 
    "summary": "Achieving high image quality is an important aspect in an increasing number\nof wireless multimedia applications. These applications require resource\nefficient error correction hardware to detect and correct errors introduced by\nthe communication channel. This paper presents an innovative flexible\narchitecture for error correction using Low-Density Parity-Check (LDPC) codes.\nThe proposed partially-parallel decoder architecture utilizes a novel code\nconstruction technique based on multi-level Hierarchical Quasi-Cyclic (HQC)\nmatrix with innovative layering of random sub-matrices. Simulation of a\nhigh-level MATLAB model shows that the proposed HQC matrices have bit error\nrate (BER) performance close to that of unstructured random matrices. The\nproposed decoder has been implemented on FPGA. It is very resource efficient\nand provides very high throughput compared to other decoders reported to date.\nPerformance evaluation of the decoder has been carried out by transmitting JPEG\nimages over an AWGN channel and comparing the quality of the reconstructed\nimages with those from other decoders.", 
    "link": "http://arxiv.org/pdf/1305.6216v2", 
    "arxiv-id": "1305.6216v2"
},{
    "category": "cs.HC", 
    "author": "Kristin Grace Erickson", 
    "title": "The Story of Telebrain: A multi-performer telematic platform for   performatization", 
    "publish": "2013-05-27T21:28:04Z", 
    "summary": "This paper presents Telebrain, a browser-based performatization platform\ninvented for organizing real-time telematic performances. Performatization is\nthe human performance of algorithms. When computers and humans performatize\ncooperatively, the human-computer interaction (HCI) becomes the location of\ncomputation. Novel modes of machine-human communication are necessary for\norganizing performatizations. Telebrain is designed to facilitate machine-human\nlanguages. Capitalizing on the ubiquity and cross-platform compatibility of the\nInternet, Telebrain is an open-source web application supporting PerPL\n(Performer Programming Language), a human-interpreted configurable language of\nmulti-media instructions used to program performers. Telebrain facilitates a\nvariety of performance disciplines such as music, theater, dance, computational\nperformance, networked scoring (image and audio), prompted improvisation,\nreal-space multi-player gaming, collaborative transdisciplinary karaoke and\nquantum square-dancing. (http://telebrain.org)", 
    "link": "http://arxiv.org/pdf/1305.6332v1", 
    "arxiv-id": "1305.6332v1"
},{
    "category": "cs.IT", 
    "author": "Vincent Lau", 
    "title": "Cache-Enabled Opportunistic Cooperative MIMO for Video Streaming in   Wireless Systems", 
    "publish": "2013-06-12T03:17:21Z", 
    "summary": "We propose a cache-enabled opportunistic cooperative MIMO (CoMP) framework\nfor wireless video streaming. By caching a portion of the video files at the\nrelays (RS) using a novel MDS-coded random cache scheme, the base station (BS)\nand RSs opportunistically employ CoMP to achieve spatial multiplexing gain\nwithout expensive payload backhaul. We study a two timescale joint optimization\nof power and cache control to support real-time video streaming. The cache\ncontrol is to create more CoMP opportunities and is adaptive to the long-term\npopularity of the video files. The power control is to guarantee the QoS\nrequirements and is adaptive to the channel state information (CSI), the cache\nstate at the RS and the queue state information (QSI) at the users. The joint\nproblem is decomposed into an inner power control problem and an outer cache\ncontrol problem. We first derive a closed-form power control policy from an\napproximated Bellman equation. Based on this, we transform the outer problem\ninto a convex stochastic optimization problem and propose a stochastic\nsubgradient algorithm to solve it. Finally, the proposed solution is shown to\nbe asymptotically optimal for high SNR and small timeslot duration. Its\nsuperior performance over various baselines is verified by simulations.", 
    "link": "http://arxiv.org/pdf/1306.2701v3", 
    "arxiv-id": "1306.2701v3"
},{
    "category": "cs.MM", 
    "author": "Yuanyan Tang", 
    "title": "Multiview Hessian Discriminative Sparse Coding for Image Annotation", 
    "publish": "2013-07-15T03:14:05Z", 
    "summary": "Sparse coding represents a signal sparsely by using an overcomplete\ndictionary, and obtains promising performance in practical computer vision\napplications, especially for signal restoration tasks such as image denoising\nand image inpainting. In recent years, many discriminative sparse coding\nalgorithms have been developed for classification problems, but they cannot\nnaturally handle visual data represented by multiview features. In addition,\nexisting sparse coding algorithms use graph Laplacian to model the local\ngeometry of the data distribution. It has been identified that Laplacian\nregularization biases the solution towards a constant function which possibly\nleads to poor extrapolating power. In this paper, we present multiview Hessian\ndiscriminative sparse coding (mHDSC) which seamlessly integrates Hessian\nregularization with discriminative sparse coding for multiview learning\nproblems. In particular, mHDSC exploits Hessian regularization to steer the\nsolution which varies smoothly along geodesics in the manifold, and treats the\nlabel information as an additional view of feature for incorporating the\ndiscriminative power for image annotation. We conduct extensive experiments on\nPASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image\nannotation.", 
    "link": "http://arxiv.org/pdf/1307.3811v1", 
    "arxiv-id": "1307.3811v1"
},{
    "category": "cs.SI", 
    "author": "Tsuhan Chen", 
    "title": "A Latent Social Approach to YouTube Popularity Prediction", 
    "publish": "2013-08-06T20:40:52Z", 
    "summary": "Current works on Information Centric Networking assume the spectrum of\ncaching strategies under the Least Recently/ Frequently Used (LRFU) scheme as\nthe de-facto standard, due to the ease of implementation and easier analysis of\nsuch strategies. In this paper we predict the popularity distribution of\nYouTube videos within a campus network. We explore two broad approaches in\npredicting the popularity of videos in the network: consensus approaches based\non aggregate behavior in the network, and social approaches based on the\ninformation diffusion over an implicit network. We measure the performance of\nour approaches under a simple caching framework by picking the k most popular\nvideos according to our predicted distribution and calculating the hit rate on\nthe cache. We develop our approach by first incorporating video inter-arrival\ntime (based on the power-law distribution governing the transmission time\nbetween two receivers of the same message in scale-free networks) to the\nbaseline (LRFU), then combining with an information diffusion model over the\ninferred latent social graph that governs diffusion of videos in the network.\nWe apply techniques from latent social network inference to learn the sharing\nprobabilities between users in the network and apply a virus propagation model\nborrowed from mathematical epidemiology to estimate the number of times a video\nwill be accessed in the future. Our approach gives rise to a 14% hit rate\nimprovement over the baseline.", 
    "link": "http://arxiv.org/pdf/1308.1418v1", 
    "arxiv-id": "1308.1418v1"
},{
    "category": "cs.MM", 
    "author": "Tuomas Eerola", 
    "title": "Semantic Computing of Moods Based on Tags in Social Media of Music", 
    "publish": "2013-08-08T11:29:24Z", 
    "summary": "Social tags inherent in online music services such as Last.fm provide a rich\nsource of information on musical moods. The abundance of social tags makes this\ndata highly beneficial for developing techniques to manage and retrieve mood\ninformation, and enables study of the relationships between music content and\nmood representations with data substantially larger than that available for\nconventional emotion research. However, no systematic assessment has been done\non the accuracy of social tags and derived semantic models at capturing mood\ninformation in music. We propose a novel technique called Affective Circumplex\nTransformation (ACT) for representing the moods of music tracks in an\ninterpretable and robust fashion based on semantic computing of social tags and\nresearch in emotion modeling. We validate the technique by predicting listener\nratings of moods in music tracks, and compare the results to prediction with\nthe Vector Space Model (VSM), Singular Value Decomposition (SVD), Nonnegative\nMatrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA).\nThe results show that ACT consistently outperforms the baseline techniques, and\nits performance is robust against a low number of track-level mood tags. The\nresults give validity and analytical insights for harnessing millions of music\ntracks and associated mood data available through social tags in application\ndevelopment.", 
    "link": "http://arxiv.org/pdf/1308.1817v1", 
    "arxiv-id": "1308.1817v1"
},{
    "category": "cs.NI", 
    "author": "Amine Berqia", 
    "title": "An Enhanced Time Space Priority Scheme to Manage QoS for Multimedia   Flows transmitted to an end user in HSDPA Network", 
    "publish": "2013-08-12T15:48:04Z", 
    "summary": "When different type of packets with different needs of Quality of Service\n(QoS) requirements share the same network resources, it became important to use\nqueue management and scheduling schemes in order to maintain perceived quality\nat the end users at an acceptable level. Many schemes have been studied in the\nliterature, these schemes use time priority (to maintain QoS for Real Time (RT)\npackets) and/or space priority (to maintain QoS for Non Real Time (NRT)\npackets). In this paper, we study and show the drawback of a combined time and\nspace priority (TSP) scheme used to manage QoS for RT and NRT packets intended\nfor an end user in High Speed Downlink Packet Access (HSDPA) cell, and we\npropose an enhanced scheme (Enhanced Basic-TSP scheme) to improve QoS\nrelatively to the RT packets, and to exploit efficiently the network resources.\nA mathematical model for the EB-TSP scheme is done, and numerical results show\nthe positive impact of this scheme.", 
    "link": "http://arxiv.org/pdf/1308.2600v1", 
    "arxiv-id": "1308.2600v1"
},{
    "category": "cs.MM", 
    "author": "A. M. Alimi", 
    "title": "An interactive engine for multilingual video browsing using semantic   content", 
    "publish": "2013-08-14T19:54:11Z", 
    "summary": "The amount of audio-visual information has increased dramatically with the\nadvent of High Speed Internet. Furthermore, technological advances in recent\nyears in the field of information technology, have simplified the use of video\ndata in various fields by the general public. This made it possible to store\nlarge collections of video documents into computer systems. To enable efficient\nuse of these collections, it is necessary to develop tools to facilitate access\nto these documents and handling them. In this paper we propose a method for\nindexing and retrieval of video sequences in a video database of large\ndimension, based on a weighting technique to calculate the degree of membership\nof a concept in a video also a structuring of the data of the audio-visual\n(context / concept / video) and a relevance feedback mechanism.", 
    "link": "http://arxiv.org/pdf/1308.3225v1", 
    "arxiv-id": "1308.3225v1"
},{
    "category": "cs.MM", 
    "author": "A. M. Alimi", 
    "title": "Arabic Text Recognition in Video Sequences", 
    "publish": "2013-08-14T20:15:44Z", 
    "summary": "In this paper, we propose a robust approach for text extraction and\nrecognition from Arabic news video sequence. The text included in video\nsequences is an important needful for indexing and searching system. However,\nthis text is difficult to detect and recognize because of the variability of\nits size, their low resolution characters and the complexity of the\nbackgrounds. To solve these problems, we propose a system performing in two\nmain tasks: extraction and recognition of text. Our system is tested on a\nvaried database composed of different Arabic news programs and the obtained\nresults are encouraging and show the merits of our approach.", 
    "link": "http://arxiv.org/pdf/1308.3243v1", 
    "arxiv-id": "1308.3243v1"
},{
    "category": "cs.CV", 
    "author": "Jonas Unger", 
    "title": "A Unified Framework for Multi-Sensor HDR Video Reconstruction", 
    "publish": "2013-08-22T15:58:01Z", 
    "summary": "One of the most successful approaches to modern high quality HDR-video\ncapture is to use camera setups with multiple sensors imaging the scene through\na common optical system. However, such systems pose several challenges for HDR\nreconstruction algorithms. Previous reconstruction techniques have considered\ndebayering, denoising, resampling (align- ment) and exposure fusion as separate\nproblems. In contrast, in this paper we present a unifying approach, performing\nHDR assembly directly from raw sensor data. Our framework includes a camera\nnoise model adapted to HDR video and an algorithm for spatially adaptive HDR\nreconstruction based on fitting of local polynomial approximations to observed\nsensor data. The method is easy to implement and allows reconstruction to an\narbitrary resolution and output mapping. We present an implementation in CUDA\nand show real-time performance for an experimental 4 Mpixel multi-sensor HDR\nvideo system. We further show that our algorithm has clear advantages over\nexisting methods, both in terms of flexibility and reconstruction quality.", 
    "link": "http://arxiv.org/pdf/1308.4908v1", 
    "arxiv-id": "1308.4908v1"
}]