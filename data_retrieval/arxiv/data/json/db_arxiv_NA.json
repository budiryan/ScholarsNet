[{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/9907005v1", 
    "title": "Alternative Local Discriminant Bases Using Empirical Expectation and   Variance Estimation", 
    "arxiv-id": "cs/9907005v1", 
    "author": "Eirik Fossgaard", 
    "publish": "1999-07-06T11:48:33Z", 
    "summary": "We propose alternative discriminant measures for selecting the best basis\namong a large collection of orthonormal bases for classification purposes. A\ngeneralization of the Local Discriminant Basis Algorithm of Saito and Coifman\nis constructed. The success of these new methods is evaluated and compared to\nearlier methods in experiments."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0102023v1", 
    "title": "Factored Notation for Interval I/O", 
    "arxiv-id": "cs/0102023v1", 
    "author": "M. H. van Emden", 
    "publish": "2001-02-23T14:24:08Z", 
    "summary": "This note addresses the input and output of intervals in the sense of\ninterval arithmetic and interval constraints. The most obvious, and so far most\nwidely used notation, for intervals has drawbacks that we remedy with a new\nnotation that we propose to call factored notation. It is more compact and\nallows one to find a good trade-off between interval width and ease of reading.\nWe describe how such a trade-off can be based on the information yield (in the\nsense of information theory) of the last decimal shown."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0201015v1", 
    "title": "On the Significance of Digits in Interval Notation", 
    "arxiv-id": "cs/0201015v1", 
    "author": "M. H. van Emden", 
    "publish": "2002-01-17T22:31:21Z", 
    "summary": "To analyse the significance of the digits used for interval bounds, we\nclarify the philosophical presuppositions of various interval notations. We use\ninformation theory to determine the information content of the last digit of\nthe numeral used to denote the interval's bounds. This leads to the notion of\nefficiency of a decimal digit: the actual value as percentage of the maximal\nvalue of its information content. By taking this efficiency into account, many\npresentations of intervals can be made more readable at the expense of\nnegligible loss of information."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0206031v1", 
    "title": "A sufficient condition for global invertibility of Lipschitz mapping", 
    "arxiv-id": "cs/0206031v1", 
    "author": "S. Tarasov", 
    "publish": "2002-06-20T18:44:11Z", 
    "summary": "We show that S.Vavasis' sufficient condition for global invertibility of a\npolynomial mapping can be easily generalized to the case of a general Lipschitz\nmapping. Keywords: Invertibility conditions, generalized Jacobian, nonsmooth\nanalysis."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0210015v2", 
    "title": "New Developments in Interval Arithmetic and Their Implications for   Floating-Point Standardization", 
    "arxiv-id": "cs/0210015v2", 
    "author": "M. H. van Emden", 
    "publish": "2002-10-16T23:22:13Z", 
    "summary": "We consider the prospect of a processor that can perform interval arithmetic\nat the same speed as conventional floating-point arithmetic. This makes it\npossible for all arithmetic to be performed with the superior security of\ninterval methods without any penalty in speed. In such a situation the IEEE\nfloating-point standard needs to be compared with a version of floating-point\narithmetic that is ideal for the purpose of interval arithmetic. Such a\ncomparison requires a succinct and complete exposition of interval arithmetic\naccording to its recent developments. We present such an exposition in this\npaper. We conclude that the directed roundings toward the infinities and the\ndefinition of division by the signed zeros are valuable features of the\nstandard. Because the operations of interval arithmetic are always defined,\nexceptions do not arise. As a result neither Nans nor exceptions are needed. Of\nthe status flags, only the inexact flag may be useful. Denormalized numbers\nseem to have no use for interval arithmetic; in the use of interval\nconstraints, they are a handicap."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0306015v1", 
    "title": "Computing sharp and scalable bounds on errors in approximate zeros of   univariate polynomials", 
    "arxiv-id": "cs/0306015v1", 
    "author": "Sudhir Kumar Singh", 
    "publish": "2003-06-02T12:26:18Z", 
    "summary": "There are several numerical methods for computing approximate zeros of a\ngiven univariate polynomial. In this paper, we develop a simple and novel\nmethod for determining sharp upper bounds on errors in approximate zeros of a\ngiven polynomial using Rouche's theorem from complex analysis. We compute the\nerror bounds using non-linear optimization. Our bounds are scalable in the\nsense that we compute sharper error bounds for better approximations of zeros.\nWe use high precision computations using the LEDA/real floating-point filter\nfor computing our bounds robustly."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0404034v1", 
    "title": "Propagation by Selective Initialization and Its Application to Numerical   Constraint Satisfaction Problems", 
    "arxiv-id": "cs/0404034v1", 
    "author": "B. Moa", 
    "publish": "2004-04-16T00:23:32Z", 
    "summary": "Numerical analysis has no satisfactory method for the more realistic\noptimization models. However, with constraint programming one can compute a\ncover for the solution set to arbitrarily close approximation. Because the use\nof constraint propagation for composite arithmetic expressions is\ncomputationally expensive, consistency is computed with interval arithmetic. In\nthis paper we present theorems that support, selective initialization, a simple\nmodification of constraint propagation that allows composite arithmetic\nexpressions to be handled efficiently."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0965-9978(03)00113-3", 
    "link": "http://arxiv.org/pdf/cs/0407022v4", 
    "title": "Solving Elliptic Finite Element Systems in Near-Linear Time with Support   Preconditioners", 
    "arxiv-id": "cs/0407022v4", 
    "author": "Stephen Vavasis", 
    "publish": "2004-07-09T20:08:41Z", 
    "summary": "We consider linear systems arising from the use of the finite element method\nfor solving scalar linear elliptic problems. Our main result is that these\nlinear systems, which are symmetric and positive semidefinite, are well\napproximated by symmetric diagonally dominant matrices. Our framework for\ndefining matrix approximation is support theory. Significant graph theoretic\nwork has already been developed in the support framework for preconditioners in\nthe diagonally dominant case, and in particular it is known that such systems\ncan be solved with iterative methods in nearly linear time. Thus, our\napproximation result implies that these graph theoretic techniques can also\nsolve a class of finite element problems in nearly linear time. We show that\nthe support number bounds, which control the number of iterations in the\npreconditioned iterative solver, depend on mesh quality measures but not on the\nproblem size or shape of the domain."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10543-010-0283-3", 
    "link": "http://arxiv.org/pdf/cs/0410045v4", 
    "title": "Analysis of and workarounds for element reversal for a finite   element-based algorithm for warping triangular and tetrahedral meshes", 
    "arxiv-id": "cs/0410045v4", 
    "author": "Stephen A. Vavasis", 
    "publish": "2004-10-18T22:33:59Z", 
    "summary": "We consider an algorithm called FEMWARP for warping triangular and\ntetrahedral finite element meshes that computes the warping using the finite\nelement method itself. The algorithm takes as input a two- or three-dimensional\ndomain defined by a boundary mesh (segments in one dimension or triangles in\ntwo dimensions) that has a volume mesh (triangles in two dimensions or\ntetrahedra in three dimensions) in its interior. It also takes as input a\nprescribed movement of the boundary mesh. It computes as output updated\npositions of the vertices of the volume mesh. The first step of the algorithm\nis to determine from the initial mesh a set of local weights for each interior\nvertex that describes each interior vertex in terms of the positions of its\nneighbors. These weights are computed using a finite element stiffness matrix.\nAfter a boundary transformation is applied, a linear system of equations based\nupon the weights is solved to determine the final positions of the interior\nvertices. The FEMWARP algorithm has been considered in the previous literature\n(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing\ndeformed meshes for certain applications. However, sometimes FEMWARP reverses\nelements; this is our main concern in this paper. We analyze the causes for\nthis undesirable behavior and propose several techniques to make the method\nmore robust against reversals. The most successful of the proposed methods\nincludes combining FEMWARP with an optimization-based untangler."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10543-010-0283-3", 
    "link": "http://arxiv.org/pdf/cs/0412009v1", 
    "title": "A Fully Sparse Implementation of a Primal-Dual Interior-Point Potential   Reduction Method for Semidefinite Programming", 
    "arxiv-id": "cs/0412009v1", 
    "author": "Stephen A. Vavasis", 
    "publish": "2004-12-02T21:28:13Z", 
    "summary": "In this paper, we show a way to exploit sparsity in the problem data in a\nprimal-dual potential reduction method for solving a class of semidefinite\nprograms. When the problem data is sparse, the dual variable is also sparse,\nbut the primal one is not. To avoid working with the dense primal variable, we\napply Fukuda et al.'s theory of partial matrix completion and work with partial\nmatrices instead. The other place in the algorithm where sparsity should be\nexploited is in the computation of the search direction, where the gradient and\nthe Hessian-matrix product of the primal and dual barrier functions must be\ncomputed in every iteration. By using an idea from automatic differentiation in\nbackward mode, both the gradient and the Hessian-matrix product can be computed\nin time proportional to the time needed to compute the barrier functions of\nsparse variables itself. Moreover, the high space complexity that is normally\nassociated with the use of automatic differentiation in backward mode can be\navoided in this case. In addition, we suggest a technique to efficiently\ncompute the determinant of the positive definite matrix completion that is\nrequired to compute primal search directions. The method of obtaining one of\nthe primal search directions that minimizes the number of the evaluations of\nthe determinant of the positive definite completion is also proposed. We then\nimplement the algorithm and test it on the problem of finding the maximum cut\nof a graph."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10543-010-0283-3", 
    "link": "http://arxiv.org/pdf/cs/0501041v1", 
    "title": "Two Iterative Algorithms for Solving Systems of Simultaneous Linear   Algebraic Equations with Real Matrices of Coefficients", 
    "arxiv-id": "cs/0501041v1", 
    "author": "N. P. Polishchuk", 
    "publish": "2005-01-20T13:20:27Z", 
    "summary": "The paper describes two iterative algorithms for solving general systems of M\nsimultaneous linear algebraic equations (SLAE) with real matrices of\ncoefficients. The system can be determined, underdetermined, and\noverdetermined. Linearly dependent equations are also allowed. Both algorithms\nuse the method of Lagrange multipliers to transform the original SLAE into a\npositively determined function F of real original variables X(i) (i=1,...,N)\nand Lagrange multipliers Lambda(i) (i=1,...,M). Function F is differentiated\nwith respect to variables X(i) and the obtained relationships are used to\nexpress F in terms of Lagrange multipliers Lambda(i). The obtained function is\nminimized with respect to variables Lambda(i) with the help of one of two the\nfollowing minimization techniques: (1) relaxation method or (2) method of\nconjugate gradients by Fletcher and Reeves. Numerical examples are given."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10543-010-0283-3", 
    "link": "http://arxiv.org/pdf/cs/0502059v1", 
    "title": "New approach for Finite Difference Method for Thermal Analysis of   Passive Solar Systems", 
    "arxiv-id": "cs/0502059v1", 
    "author": "Anton Stoilov", 
    "publish": "2005-02-13T15:29:37Z", 
    "summary": "Mathematical treatment of massive wall systems is a useful tool for\ninvestigation of these solar applications. The objectives of this work are to\ndevelop (and validate) a numerical solution model for predication the thermal\nbehaviour of passive solar systems with massive wall, to improve knowledge of\nusing indirect passive solar systems and assess its energy efficiency according\nto climatic conditions in Bulgaria. The problem of passive solar systems with\nmassive walls is modelled by thermal and mass transfer equations. As a boundary\nconditions for the mathematical problem are used equations, which describe\ninfluence of weather data and constructive parameters of building on the\nthermal performance of the passive system. The mathematical model is solved by\nmeans of finite differences method and improved solution procedure. In article\nare presented results of theoretical and experimental study for developing and\nvalidating a numerical solution model for predication the thermal behaviour of\npassive solar systems with massive wall."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0502092v1", 
    "title": "Divergence-free Wavelets for Navier-Stokes", 
    "arxiv-id": "cs/0502092v1", 
    "author": "Val\u00e9rie Perrier", 
    "publish": "2005-02-25T10:57:32Z", 
    "summary": "In this paper, we investigate the use of compactly supported divergence-free\nwavelets for the representation of the Navier-Stokes solution. After reminding\nthe theoretical construction of divergence-free wavelet vectors, we present in\ndetail the bases and corresponding fast algorithms for 2D and 3D incompressible\nflows. In order to compute the nonlinear term, we propose a new method which\nprovides in practice with the Hodge decomposition of any flow: this\ndecomposition enables us to separate the incompressible part of the flow from\nits orthogonal complement, which corresponds to the gradient component of the\nflow. Finally we show numerical tests to validate our approach."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0503086v2", 
    "title": "Segmentation of the Homogeneity of a Signal Using a Piecewise Linear   Recognition Tool", 
    "arxiv-id": "cs/0503086v2", 
    "author": "Joseph Morlier", 
    "publish": "2005-03-30T13:53:19Z", 
    "summary": "In this paper a new method of detection of homogeneous zones and singularity\nparts of a 1D signal is proposed. The entropy function is used to transform\nsignal in piecewise linear one. The multiple regression permits to detect lines\nand project them in the Hough parameters space in order to easily recognise\nhomogeneous zone and abrupt changes of the signal. Two application examples are\nanalysed, the first is a classical fractal signal and the other is issued from\na dynamic mechanical study."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0609114v1", 
    "title": "A VFRoe scheme for 1D shallow water flows : wetting and drying   simulation", 
    "arxiv-id": "cs/0609114v1", 
    "author": "Abdou Wahidi Bello", 
    "publish": "2006-09-20T09:22:13Z", 
    "summary": "A finite-volume method for the one-dimensional shallow-water equations\nincluding topographic source terms is presented. Exploiting an original idea by\nLeroux, the system of partial-differential equations is completed by a trivial\nequation for the bathymetry. By applying a change of variable, the system is\ngiven a celerity-speed formulation, and linearized. As a result, an approximate\nRiemann solver preserving the positivity of the celerity can be constructed,\npermitting wetting and drying flow simulations to be performed. Finally, the\nsimulation of numerical test cases is presented."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0611143v2", 
    "title": "An informational approach to the global optimization of   expensive-to-evaluate functions", 
    "arxiv-id": "cs/0611143v2", 
    "author": "Eric Walter", 
    "publish": "2006-11-28T14:25:07Z", 
    "summary": "In many global optimization problems motivated by engineering applications,\nthe number of function evaluations is severely limited by time or cost. To\nensure that each evaluation contributes to the localization of good candidates\nfor the role of global minimizer, a sequential choice of evaluation points is\nusually carried out. In particular, when Kriging is used to interpolate past\nevaluations, the uncertainty associated with the lack of information on the\nfunction can be expressed and used to compute a number of criteria accounting\nfor the interest of an additional evaluation at any given point. This paper\nintroduces minimizer entropy as a new Kriging-based criterion for the\nsequential choice of points at which the function should be evaluated. Based on\n\\emph{stepwise uncertainty reduction}, it accounts for the informational gain\non the minimizer expected from a new evaluation. The criterion is approximated\nusing conditional simulations of the Gaussian process model behind Kriging, and\nthen inserted into an algorithm similar in spirit to the \\emph{Efficient Global\nOptimization} (EGO) algorithm. An empirical comparison is carried out between\nour criterion and \\emph{expected improvement}, one of the reference criteria in\nthe literature. Experimental results indicate major evaluation savings over\nEGO. Finally, the method, which we call IAGO (for Informational Approach to\nGlobal Optimization) is extended to robust optimization problems, where both\nthe factors to be tuned and the function evaluations are corrupted by noise."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0701141v2", 
    "title": "The Fundamental Theorems of Interval Analysis", 
    "arxiv-id": "cs/0701141v2", 
    "author": "B. Moa", 
    "publish": "2007-01-22T23:11:15Z", 
    "summary": "Expressions are not functions. Confusing the two concepts or failing to\ndefine the function that is computed by an expression weakens the rigour of\ninterval arithmetic. We give such a definition and continue with the required\nre-statements and proofs of the fundamental theorems of interval arithmetic and\ninterval analysis.\n  Revision Feb. 10, 2009: added reference to and acknowledgement of P. Taylor."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0703003v1", 
    "title": "Functions to Support Input and Output of Intervals", 
    "arxiv-id": "cs/0703003v1", 
    "author": "S. C. Somosan", 
    "publish": "2007-02-28T23:13:23Z", 
    "summary": "Interval arithmetic is hardly feasible without directed rounding as provided,\nfor example, by the IEEE floating-point standard. Equally essential for\ninterval methods is directed rounding for conversion between the external\ndecimal and internal binary numerals. This is not provided by the standard I/O\nlibraries. Conversion algorithms exist that guarantee identity upon conversion\nfollowed by its inverse. Although it may be possible to adapt these algorithms\nfor use in decimal interval I/O, we argue that outward rounding in radix\nconversion is computationally a simpler problem than guaranteeing identity.\nHence it is preferable to develop decimal interval I/O ab initio, which is what\nwe do in this paper."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0703082v1", 
    "title": "Remarks on the O(N) Implementation of the Fast Marching Method", 
    "arxiv-id": "cs/0703082v1", 
    "author": "Thomas Satzger", 
    "publish": "2007-03-15T13:41:11Z", 
    "summary": "The fast marching algorithm computes an approximate solution to the eikonal\nequation in O(N log N) time, where the factor log N is due to the\nadministration of a priority queue. Recently, Yatziv, Bartesaghi and Sapiro\nhave suggested to use an untidy priority queue, reducing the overall complexity\nto O(N) at the price of a small error in the computed solution. In this paper,\nwe give an explicit estimate of the error introduced, which is based on a\ndiscrete comparison principle. This estimates implies in particular that the\nchoice of an accuracy level that is independent of the speed function F results\nin the complexity bound O(Fmax /Fmin N). A numerical experiment illustrates\nthis robustness problem for large ratios Fmax /Fmin ."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/cs/0703119v2", 
    "title": "Support-Graph Preconditioners for 2-Dimensional Trusses", 
    "arxiv-id": "cs/0703119v2", 
    "author": "Daniel A. Spielman", 
    "publish": "2007-03-23T15:03:34Z", 
    "summary": "We use support theory, in particular the fretsaw extensions of Shklarski and\nToledo, to design preconditioners for the stiffness matrices of 2-dimensional\ntruss structures that are stiffly connected. Provided that all the lengths of\nthe trusses are within constant factors of each other, that the angles at the\ncorners of the triangles are bounded away from 0 and $\\pi$, and that the\nelastic moduli and cross-sectional areas of all the truss elements are within\nconstant factors of each other, our preconditioners allow us to solve linear\nequations in the stiffness matrices to accuracy $\\epsilon$ in time $O (n^{5/4}\n(\\log^{2}n \\log \\log n)^{3/4} \\log (1/\\epsilon))$."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/0706.0903v1", 
    "title": "Families of traveling impulses and fronts in some models with   cross-diffusion", 
    "arxiv-id": "0706.0903v1", 
    "author": "Georgy Karev", 
    "publish": "2007-06-06T20:19:20Z", 
    "summary": "An analysis of traveling wave solutions of partial differential equation\n(PDE) systems with cross-diffusion is presented. The systems under study fall\nin a general class of the classical Keller-Segel models to describe chemotaxis.\nThe analysis is conducted using the theory of the phase plane analysis of the\ncorresponding wave systems without a priory restrictions on the boundary\nconditions of the initial PDE. Special attention is paid to families of\ntraveling wave solutions. Conditions for existence of front-impulse,\nimpulse-front, and front-front traveling wave solutions are formulated. In\nparticular, the simplest mathematical model is presented that has an\nimpulse-impulse solution; we also show that a non-isolated singular point in\nthe ordinary differential equation (ODE) wave system implies existence of\nfree-boundary fronts. The results can be used for construction and analysis of\ndifferent mathematical models describing systems with chemotaxis."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/0710.3170v2", 
    "title": "Fast Intrinsic Mode Decomposition of Time Series Data with Sawtooth   Transform", 
    "arxiv-id": "0710.3170v2", 
    "author": "Louis Yu Lu", 
    "publish": "2007-10-16T20:19:27Z", 
    "summary": "An efficient method is introduced in this paper to find the intrinsic mode\nfunction (IMF) components of time series data. This method is faster and more\npredictable than the Empirical Mode Decomposition (EMD) method devised by the\nauthor of Hilbert Huang Transform (HHT). The approach is to transforms the\noriginal data function into a piecewise linear sawtooth function (or triangle\nwave function), then directly constructs the upper envelope by connecting the\nmaxima and construct lower envelope by connecting minima with straight line\nsegments in the sawtooth space, the IMF is calculated as the difference between\nthe sawtooth function and the mean of the upper and lower envelopes. The\nresults found in the sawtooth space are reversely transformed into the original\ndata space as the required IMF and envelopes mean. This decomposition method\nprocess the data in one pass to obtain a unique IMF component without the time\nconsuming repetitive sifting process of EMD method. An alternative\ndecomposition method with sawtooth function expansion is also presented."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/0804.0581v2", 
    "title": "Computing a Finite Size Representation of the Set of Approximate   Solutions of an MOP", 
    "arxiv-id": "0804.0581v2", 
    "author": "El-Ghazali Talbi", 
    "publish": "2008-04-03T15:32:17Z", 
    "summary": "Recently, a framework for the approximation of the entire set of\n$\\epsilon$-efficient solutions (denote by $E_\\epsilon$) of a multi-objective\noptimization problem with stochastic search algorithms has been proposed. It\nwas proven that such an algorithm produces -- under mild assumptions on the\nprocess to generate new candidate solutions --a sequence of archives which\nconverges to $E_{\\epsilon}$ in the limit and in the probabilistic sense. The\nresult, though satisfactory for most discrete MOPs, is at least from the\npractical viewpoint not sufficient for continuous models: in this case, the set\nof approximate solutions typically forms an $n$-dimensional object, where $n$\ndenotes the dimension of the parameter space, and thus, it may come to\nperfomance problems since in practise one has to cope with a finite archive.\nHere we focus on obtaining finite and tight approximations of $E_\\epsilon$, the\nlatter measured by the Hausdorff distance. We propose and investigate a novel\narchiving strategy theoretically and empirically. For this, we analyze the\nconvergence behavior of the algorithm, yielding bounds on the obtained\napproximation quality as well as on the cardinality of the resulting\napproximation, and present some numerical results."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/0806.2159v3", 
    "title": "Communication-optimal parallel and sequential QR and LU factorizations:   theory and practice", 
    "arxiv-id": "0806.2159v3", 
    "author": "Julien Langou", 
    "publish": "2008-06-12T21:05:37Z", 
    "summary": "We present parallel and sequential dense QR factorization algorithms that are\nboth optimal (up to polylogarithmic factors) in the amount of communication\nthey perform, and just as stable as Householder QR. Our first algorithm, Tall\nSkinny QR (TSQR), factors m-by-n matrices in a one-dimensional (1-D) block\ncyclic row layout, and is optimized for m >> n. Our second algorithm, CAQR\n(Communication-Avoiding QR), factors general rectangular matrices distributed\nin a two-dimensional block cyclic layout. It invokes TSQR for each block column\nfactorization."
},{
    "category": "cs.NA", 
    "doi": "10.1080/14685240500260547", 
    "link": "http://arxiv.org/pdf/0806.2548v1", 
    "title": "A Data-Parallel Algorithm to Reliably Solve Systems of Nonlinear   Equations", 
    "arxiv-id": "0806.2548v1", 
    "author": "Alexandre Goldsztejn", 
    "publish": "2008-06-16T11:40:07Z", 
    "summary": "Numerical methods based on interval arithmetic are efficient means to\nreliably solve nonlinear systems of equations. Algorithm bc3revise is an\ninterval method that tightens variables' domains by enforcing a property called\nbox consistency. It has been successfully used on difficult problems whose\nsolving eluded traditional numerical methods. We present a new algorithm to\nenforce box consistency that is simpler than bc3revise, faster, and easily data\nparallelizable. A parallel implementation with Intel SSE2 SIMD instructions\nshows that an increase in performance of up to an order of magnitude and more\nis achievable."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0806.3099v2", 
    "title": "On the stability of bubble functions and a stabilized mixed finite   element formulation for the Stokes problem", 
    "arxiv-id": "0806.3099v2", 
    "author": "K. D. Hjelmstad", 
    "publish": "2008-06-18T22:29:27Z", 
    "summary": "In this paper we investigate the relationship between stabilized and enriched\nfinite element formulations for the Stokes problem. We also present a new\nstabilized mixed formulation for which the stability parameter is derived\npurely by the method of weighted residuals. This new formulation allows equal\norder interpolation for the velocity and pressure fields. Finally, we show by\ncounterexample that a direct equivalence between subgrid-based stabilized\nfinite element methods and Galerkin methods enriched by bubble functions cannot\nbe constructed for quadrilateral and hexahedral elements using standard bubble\nfunctions."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0806.3514v1", 
    "title": "Consistent Newton-Raphson vs. fixed-point for variational multiscale   formulations for incompressible Navier-Stokes", 
    "arxiv-id": "0806.3514v1", 
    "author": "K. D. Hjelmstad", 
    "publish": "2008-06-21T14:43:31Z", 
    "summary": "The following paper compares a consistent Newton-Raphson and fixed-point\niteration based solution strategy for a variational multiscale finite element\nformulation for incompressible Navier-Stokes. The main contributions of this\nwork include a consistent linearization of the Navier-Stokes equations, which\nprovides an avenue for advanced algorithms that require origins in a consistent\nmethod. We also present a comparison between formulations that differ only in\ntheir linearization, but maintain all other equivalences. Using the variational\nmultiscale concept, we construct a stabilized formulation (that may be\nconsidered an extension of the MINI element to nonlinear Navier-Stokes). We\nthen linearize the problem using fixed-point iteration and by deriving a\nconsistent tangent matrix for the update equation to obtain the solution via\nNewton-Raphson iterations. We show that the consistent formulation converges in\nfewer iterations, as expected, for several test problems. We also show that the\nconsistent formulation converges for problems for which fixed-point iteration\ndiverges. We present the results of both methods for problems of Reynold's\nnumber up to 5000."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0806.3963v1", 
    "title": "A stabilized finite element formulation for advection-diffusion using   the generalized finite element framework", 
    "arxiv-id": "0806.3963v1", 
    "author": "K. D. Hjelmstad", 
    "publish": "2008-06-24T19:56:38Z", 
    "summary": "The following work presents a generalized (extended) finite element\nformulation for the advection-diffusion equation. Using enrichment functions\nthat represent the exponential nature of the exact solution, smooth numerical\nsolutions are obtained for problems with steep gradients and high Peclet\nnumbers (up to Pe = 25) in one and two-dimensions. As opposed to traditional\nstabilized methods that require the construction of stability parameters and\nstabilization terms, the present work avoids numerical instabilities by\nimproving the classical Galerkin solution with an enrichment function. To\ncontextualize this method among other stabilized methods, we show by\ndecomposition of the solution (in a multiscale manner) an equivalence to both\nGalerkin/least-squares type methods and those that use bubble functions. This\nwork also presents a strategy for constructing the enrichment function for\nproblems with complex geometries by employing a global-local approach."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0806.4286v1", 
    "title": "Implementation for blow up of tornado-type solutions for complex version   of 3D Navier-Stokes system", 
    "arxiv-id": "0806.4286v1", 
    "author": "A. V. Khokhlov", 
    "publish": "2008-06-26T11:03:49Z", 
    "summary": "We consider Cauchy problem for Fourier transformation of 3-dimensional\nNavier-Stokes system with zero external force. Using initial data purposed by\nDong Li and Ya.G.Sinai we implement self-similar regime producing fast growing\nbehavior of the energy of solution while time tends to critical value."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0808.2827v3", 
    "title": "Fast Intrinsic Mode Decomposition and Filtering of Time Series Data", 
    "arxiv-id": "0808.2827v3", 
    "author": "Louis Yu Lu", 
    "publish": "2008-08-20T21:42:41Z", 
    "summary": "The intrinsic mode function (IMF) provides adaptive function bases for\nnonlinear and non-stationary time series data. A fast convergent iterative\nmethod is introduced in this paper to find the IMF components of the data, the\nmethod is faster and more predictable than the Empirical Mode Decomposition\nmethod devised by the author of Hilbert Huang Transform. The approach is to\niteratively adjust the control points on the data function corresponding to the\nextrema of the refining IMF, the control points of the residue function are\ncalculated as the median of the straight line segments passing through the data\ncontrol points, the residue function is then constructed as the cubic spline\nfunction of the median points. The initial residue function is simply\nconstructed as the straight line segments passing through the extrema of the\nfirst derivative of the data function. The refining IMF is the difference\nbetween the data function and the improved residue function. The IMF found\nreveals all the riding waves in the whole data set. A new data filtering method\non frequency and amplitude of IMF is also presented with the similar approach\nof finding the residue on the part to be filtered out. The program to\ndemonstrate the method is distributed under BSD open source license."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0809.0062v1", 
    "title": "The Stochastic Logarithmic Norm for Stability Analysis of Stochastic   Differential Equations", 
    "arxiv-id": "0809.0062v1", 
    "author": "Soumyendu Raha", 
    "publish": "2008-08-30T14:07:41Z", 
    "summary": "To analyze the stability of It\\^o stochastic differential equations with\nmultiplicative noise, we introduce the stochastic logarithmic norm. The\nlogarithmic norm was originally introduced by G. Dahlquist in 1958 as a tool to\nstudy the growth of solutions to ordinary differential equations and for\nestimating the error growth in discretization methods for their approximate\nsolutions. We extend the concept to the stability analysis of It\\^o stochastic\ndifferential equations with multiplicative noise. Stability estimates for\nlinear It\\^o SDEs using the one, two and $\\infty$-norms in the $l$-th mean,\nwhere $1 \\leq l < \\infty $, are derived and the application of the stochastic\nlogarithmic norm is illustrated with examples."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1936", 
    "link": "http://arxiv.org/pdf/0809.5173v2", 
    "title": "An algebraic approach to the set of intervals (a new approach of   arithmetic of intervals)", 
    "arxiv-id": "0809.5173v2", 
    "author": "Elisabeth Remm", 
    "publish": "2008-09-30T11:45:01Z", 
    "summary": "In this paper we present the set of intervals as a normed vector space. We\ndefine also a four-dimensional associative algebra whose product gives the\nproduct of intervals in any cases. This approach allows to give a notion of\ndivisibility and in some cases an euclidian division. We introduce differential\ncalculus and give some applications."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2009.05.039", 
    "link": "http://arxiv.org/pdf/0810.0322v4", 
    "title": "Non-negative mixed finite element formulations for a tensorial diffusion   equation", 
    "arxiv-id": "0810.0322v4", 
    "author": "A. J. Valocchi", 
    "publish": "2008-10-02T01:39:57Z", 
    "summary": "We consider the tensorial diffusion equation, and address the discrete\nmaximum-minimum principle of mixed finite element formulations. In particular,\nwe address non-negative solutions (which is a special case of the\nmaximum-minimum principle) of mixed finite element formulations. The discrete\nmaximum-minimum principle is the discrete version of the maximum-minimum\nprinciple.\n  In this paper we present two non-negative mixed finite element formulations\nfor tensorial diffusion equations based on constrained optimization techniques\n(in particular, quadratic programming). These proposed mixed formulations\nproduce non-negative numerical solutions on arbitrary meshes for low-order\n(i.e., linear, bilinear and trilinear) finite elements. The first formulation\nis based on the Raviart-Thomas spaces, and is obtained by adding a non-negative\nconstraint to the variational statement of the Raviart-Thomas formulation. The\nsecond non-negative formulation based on the variational multiscale\nformulation.\n  For the former formulation we comment on the affect of adding the\nnon-negative constraint on the local mass balance property of the\nRaviart-Thomas formulation. We also study the performance of the active set\nstrategy for solving the resulting constrained optimization problems. The\noverall performance of the proposed formulation is illustrated on three\ncanonical test problems."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2009.05.039", 
    "link": "http://arxiv.org/pdf/0810.0800v1", 
    "title": "Condition Numbers of Gaussian Random Matrices", 
    "arxiv-id": "0810.0800v1", 
    "author": "Jack Dongarra", 
    "publish": "2008-10-05T04:18:54Z", 
    "summary": "Let $G_{m \\times n}$ be an $m \\times n$ real random matrix whose elements are\nindependent and identically distributed standard normal random variables, and\nlet $\\kappa_2(G_{m \\times n})$ be the 2-norm condition number of $G_{m \\times\nn}$. We prove that, for any $m \\geq 2$, $n \\geq 2$ and $x \\geq |n-m|+1$,\n$\\kappa_2(G_{m \\times n})$ satisfies $\n  \\frac{1}{\\sqrt{2\\pi}} ({c}/{x})^{|n-m|+1} < P(\\frac{\\kappa_2(G_{m \\times n})}\n{{n}/{(|n-m|+1)}}> x) <\n  \\frac{1}{\\sqrt{2\\pi}} ({C}/{x})^{|n-m|+1}, $ where $0.245 \\leq c \\leq 2.000$\nand $ 5.013 \\leq C \\leq 6.414$ are universal positive constants independent of\n$m$, $n$ and $x$. Moreover, for any $m \\geq 2$ and $n \\geq 2$, $\nE(\\log\\kappa_2(G_{m \\times n})) < \\log \\frac{n}{|n-m|+1} + 2.258. $ A similar\npair of results for complex Gaussian random matrices is also established."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2009.05.039", 
    "link": "http://arxiv.org/pdf/0811.2984v1", 
    "title": "Sensitivity Analysis Using a Fixed Point Interval Iteration", 
    "arxiv-id": "0811.2984v1", 
    "author": "Alexandre Goldsztejn", 
    "publish": "2008-11-18T20:27:35Z", 
    "summary": "Proving the existence of a solution to a system of real equations is a\ncentral issue in numerical analysis. In many situations, the system of\nequations depend on parameters which are not exactly known. It is then natural\nto aim proving the existence of a solution for all values of these parameters\nin some given domains. This is the aim of the parametrization of existence\ntests. A new parametric existence test based on the Hansen-Sengupta operator is\npresented and compared to a similar one based on the Krawczyk operator. It is\nused as a basis of a fixed point iteration dedicated to rigorous sensibility\nanalysis of parametric systems of equations."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0901.1906v1", 
    "title": "How to improve the accuracy of the discrete gradient method in the   one-dimensional case", 
    "arxiv-id": "0901.1906v1", 
    "author": "Boguslaw Ratkiewicz", 
    "publish": "2009-01-13T23:16:12Z", 
    "summary": "We present a new numerical scheme for one dimensional dynamical systems. This\nis a modification of the discrete gradient method and keeps its advantages,\nincluding the stability and the conservation of the energy integral. However,\nits accuracy is higher by several orders of magnitude."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0904.3638v1", 
    "title": "On one method of boundary value problem regularization by passage to the   limit", 
    "arxiv-id": "0904.3638v1", 
    "author": "Lyudmila Gaponova", 
    "publish": "2009-04-23T09:51:51Z", 
    "summary": "For one class of boundary value problem depending on small parameter for\nwhich numerical methods for their solution are actually inapplicable, procedure\nof limiting problem acquisition which is much easier and which solution as much\nas close to the initial solution is described."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0905.3564v1", 
    "title": "Two hierarchies of spline interpolations. Practical algorithms for   multivariate higher order splines", 
    "arxiv-id": "0905.3564v1", 
    "author": "Cristian Constantin Lalescu", 
    "publish": "2009-05-21T20:58:46Z", 
    "summary": "A systematic construction of higher order splines using two hierarchies of\npolynomials is presented. Explicit instructions on how to implement one of\nthese hierarchies are given. The results are limited to interpolations on\nregular, rectangular grids, but an approach to other types of grids is also\ndiscussed."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0905.4745v2", 
    "title": "A fast algorithm for computing minimal-norm solutions to underdetermined   systems of linear equations", 
    "arxiv-id": "0905.4745v2", 
    "author": "Mark Tygert", 
    "publish": "2009-05-28T20:40:11Z", 
    "summary": "We introduce a randomized algorithm for computing the minimal-norm solution\nto an underdetermined system of linear equations. Given an arbitrary full-rank\nm x n matrix A with m<n, any m x 1 vector b, and any positive real number\nepsilon less than 1, the procedure computes an n x 1 vector x approximating to\nrelative precision epsilon or better the n x 1 vector p of minimal Euclidean\nnorm satisfying Ap=b. The algorithm typically requires O(mn\nlog(sqrt(n)/epsilon) + m**3) floating-point operations, generally less than the\nO(m**2 n) required by the classical schemes based on QR-decompositions or\nbidiagonalization. We present several numerical examples illustrating the\nperformance of the algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0905.4909v1", 
    "title": "On the Convex Feasibility Problem", 
    "arxiv-id": "0905.4909v1", 
    "author": "Stefan Maruster", 
    "publish": "2009-05-29T15:53:02Z", 
    "summary": "The convergence of the projection algorithm for solving the convex\nfeasibility problem for a family of closed convex sets, is in connection with\nthe regularity properties of the family. In the paper [18] are pointed out four\ncases of such a family depending of the two characteristics: the emptiness and\nboudedness of the intersection of the family. The case four (the interior of\nthe intersection is empty and the intersection itself is bounded) is unsolved.\nIn this paper we give a (partial) answer for the case four: in the case of two\nclosed convex sets in R3 the regularity property holds."
},{
    "category": "cs.NA", 
    "doi": "10.1103/PhysRevE.81.016704", 
    "link": "http://arxiv.org/pdf/0906.0504v3", 
    "title": "A stabilized finite element formulation of non-smooth contact", 
    "arxiv-id": "0906.0504v3", 
    "author": "G. Haikal", 
    "publish": "2009-06-02T01:49:04Z", 
    "summary": "The computational modeling of many engineering problems using the Finite\nElement method involves the modeling of two or more bodies that meet through an\ninterface. The interface can be physical, as in multi-physics and contact\nproblems, or purely numerical, as in the coupling of non-conforming meshes. The\nmost critical part of the modeling process is to ensure geometric compatibility\nand a complete transfer of surface tractions between the different components\nat the connecting interfaces. Popular contact modeling techniques rely on\ngeometric projections to detect and resolve overlapping or mass\ninterpenetration between two or more contacting bodies. Such approaches have\nbeen shown to have two major drawbacks: they are not suitable for contact at\nhighly nonlinear surfaces and sharp corners where smooth normal projections are\nnot feasible, and they fail to guarantee a complete and accurate transfer of\npressure across the interface. This dissertation presents a novel formulation\nfor the modeling of contact problems that possesses the ability to resolve\ncomplicated contact scenarios effectively, while being simpler to implement and\nmore widely applicable than currently available methods. We show that the\nformulation boils down to a node-to-surface gap function that works effectively\nfor non-smooth contact. The numerical implementation using the midpoint rule\nshows the need to guarantee the conservation of the total energy during impact,\nfor which a Lagrange multiplier method is used. We propose a local enrichment\nof the interface and a simple stabilization procedure based on the\ndiscontinuous Galerkin method to guarantee an accurate transfer of the pressure\nfield. The result is a robust interface formulation for contact problems and\nthe coupling of non-conforming meshes."
},{
    "category": "cs.NA", 
    "doi": "10.1137/080730822", 
    "link": "http://arxiv.org/pdf/0907.2083v1", 
    "title": "Simultaneously Sparse Solutions to Linear Inverse Problems with Multiple   System Matrices and a Single Observation Vector", 
    "arxiv-id": "0907.2083v1", 
    "author": "Elfar Adalsteinsson", 
    "publish": "2009-07-13T01:38:06Z", 
    "summary": "A linear inverse problem is proposed that requires the determination of\nmultiple unknown signal vectors. Each unknown vector passes through a different\nsystem matrix and the results are added to yield a single observation vector.\nGiven the matrices and lone observation, the objective is to find a\nsimultaneously sparse set of unknown vectors that solves the system. We will\nrefer to this as the multiple-system single-output (MSSO) simultaneous sparsity\nproblem. This manuscript contrasts the MSSO problem with other simultaneous\nsparsity problems and conducts a thorough initial exploration of algorithms\nwith which to solve it. Seven algorithms are formulated that approximately\nsolve this NP-Hard problem. Three greedy techniques are developed (matching\npursuit, orthogonal matching pursuit, and least squares matching pursuit) along\nwith four methods based on a convex relaxation (iteratively reweighted least\nsquares, two forms of iterative shrinkage, and formulation as a second-order\ncone program). The algorithms are evaluated across three experiments: the first\nand second involve sparsity profile recovery in noiseless and noisy scenarios,\nrespectively, while the third deals with magnetic resonance imaging\nradio-frequency excitation pulse design."
},{
    "category": "cs.NA", 
    "doi": "10.1137/080730822", 
    "link": "http://arxiv.org/pdf/0907.5234v2", 
    "title": "A numerical study of fluids with pressure dependent viscosity flowing   through a rigid porous medium", 
    "arxiv-id": "0907.5234v2", 
    "author": "K. R. Rajagopal", 
    "publish": "2009-07-29T23:22:24Z", 
    "summary": "In this paper we consider modifications to Darcy's equation wherein the drag\ncoefficient is a function of pressure, which is a realistic model for\ntechnological applications like enhanced oil recovery and geological carbon\nsequestration. We first outline the approximations behind Darcy's equation and\nthe modifications that we propose to Darcy's equation, and derive the governing\nequations through a systematic approach using mixture theory. We then propose a\nstabilized mixed finite element formulation for the modified Darcy's equation.\nTo solve the resulting nonlinear equations we present a solution procedure\nbased on the consistent Newton-Raphson method. We solve representative test\nproblems to illustrate the performance of the proposed stabilized formulation.\nOne of the objectives of this paper is also to show that the dependence of\nviscosity on the pressure can have a significant effect both on the qualitative\nand quantitative nature of the solution."
},{
    "category": "cs.NA", 
    "doi": "10.1137/080730822", 
    "link": "http://arxiv.org/pdf/0909.2793v2", 
    "title": "Enhanced sampling schemes for MCMC based blind Bernoulli-Gaussian   deconvolution", 
    "arxiv-id": "0909.2793v2", 
    "author": "E. Le Carpentier", 
    "publish": "2009-09-15T12:29:26Z", 
    "summary": "This paper proposes and compares two new sampling schemes for sparse\ndeconvolution using a Bernoulli-Gaussian model. To tackle such a deconvolution\nproblem in a blind and unsupervised context, the Markov Chain Monte Carlo\n(MCMC) framework is usually adopted, and the chosen sampling scheme is most\noften the Gibbs sampler. However, such a sampling scheme fails to explore the\nstate space efficiently. Our first alternative, the $K$-tuple Gibbs sampler, is\nsimply a grouped Gibbs sampler. The second one, called partially marginalized\nsampler, is obtained by integrating the Gaussian amplitudes out of the target\ndistribution. While the mathematical validity of the first scheme is obvious as\na particular instance of the Gibbs sampler, a more detailed analysis is\nprovided to prove the validity of the second scheme.\n  For both methods, optimized implementations are proposed in terms of\ncomputation and storage cost. Finally, simulation results validate both schemes\nas more efficient in terms of convergence time compared with the plain Gibbs\nsampler. Benchmark sequence simulations show that the partially marginalized\nsampler takes fewer iterations to converge than the $K$-tuple Gibbs sampler.\nHowever, its computation load per iteration grows almost quadratically with\nrespect to the data length, while it only grows linearly for the $K$-tuple\nGibbs sampler."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11784-009-0127-4", 
    "link": "http://arxiv.org/pdf/0909.4101v1", 
    "title": "A Numerical Algorithm for Zero Counting. II: Distance to Ill-posedness   and Smoothed Analysis", 
    "arxiv-id": "0909.4101v1", 
    "author": "Mario Wschebor", 
    "publish": "2009-09-22T21:42:51Z", 
    "summary": "We show a Condition Number Theorem for the condition number of zero counting\nfor real polynomial systems. That is, we show that this condition number equals\nthe inverse of the normalized distance to the set of ill-posed systems (i.e.,\nthose having multiple real zeros). As a consequence, a smoothed analysis of\nthis condition number follows."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11784-009-0127-4", 
    "link": "http://arxiv.org/pdf/0910.4307v1", 
    "title": "A Geometric Approach to Solve Fuzzy Linear Systems of Differential   Equations", 
    "arxiv-id": "0910.4307v1", 
    "author": "A. Golayoglu Fatullayev", 
    "publish": "2009-10-22T12:24:33Z", 
    "summary": "In this paper, systems of linear differential equations with crisp real\ncoefficients and with initial condition described by a vector of fuzzy numbers\nare studied. A new method based on the geometric representations of linear\ntransformations is proposed to find a solution. The most important difference\nbetween this method and methods offered in previous papers is that the solution\nis considered to be a fuzzy set of real vector-functions rather than a fuzzy\nvector-function. Each member of the set satisfies the given system with a\ncertain possibility. It is shown that at any time the solution constitutes a\nfuzzy region in the coordinate space, alfa-cuts of which are nested\nparallelepipeds. Proposed method is illustrated on examples."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11784-009-0127-4", 
    "link": "http://arxiv.org/pdf/0910.5434v1", 
    "title": "An Example of Symmetry Exploitation for Energy-related Eigencomputations", 
    "arxiv-id": "0910.5434v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2009-10-28T17:02:17Z", 
    "summary": "One of the most used approaches in simulating materials is the tight-binding\napproximation. When using this method in a material simulation, it is necessary\nto compute the eigenvalues and eigenvectors of the Hamiltonian describing the\nsystem. In general, the system possesses few explicit symmetries. Due to them,\nthe problem has many degenerate eigenvalues. The ambiguity in choosing a\northonormal basis of the invariant subspaces, associated with degenerate\neigenvalues, will result in eigenvectors which are not invariant under the\naction of the symmetry operators in matrix form. A meaningful computation of\nthe eigenvectors needs to take those symmetries into account. A natural choice\nis a set of eigenvectors, which simultaneously diagonalizes the Hamiltonian and\nthe symmetry matrices. This is possible because all the matrices commute with\neach other. The simultaneous eigenvectors and the corresponding eigenvalues\nwill be in a parametrized form in terms of the lattice momentum components.\nThis functional dependence of the eigenvalues is the dispersion relation and\ndescribes the band structure of a material. Therefore it is important to find\nthis functional dependence in any numerical computation related to material\nproperties."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.05.004", 
    "link": "http://arxiv.org/pdf/0910.5435v6", 
    "title": "Fast algorithms for spherical harmonic expansions, III", 
    "arxiv-id": "0910.5435v6", 
    "author": "Mark Tygert", 
    "publish": "2009-10-28T17:03:05Z", 
    "summary": "We accelerate the computation of spherical harmonic transforms, using what is\nknown as the butterfly scheme. This provides a convenient alternative to the\napproach taken in the second paper from this series on \"Fast algorithms for\nspherical harmonic expansions.\" The requisite precomputations become manageable\nwhen organized as a \"depth-first traversal\" of the program's control-flow\ngraph, rather than as the perhaps more natural \"breadth-first traversal\" that\nprocesses one-by-one each level of the multilevel procedure. We illustrate the\nresults via several numerical examples."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.05.004", 
    "link": "http://arxiv.org/pdf/0911.0547v1", 
    "title": "On Element SDD Approximability", 
    "arxiv-id": "0911.0547v1", 
    "author": "Sivan Toledo", 
    "publish": "2009-11-03T10:55:16Z", 
    "summary": "This short communication shows that in some cases scalar elliptic finite\nelement matrices cannot be approximated well by an SDD matrix. We also give a\ntheoretical analysis of a simple heuristic method for approximating an element\nby an SDD matrix."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.05.004", 
    "link": "http://arxiv.org/pdf/0911.0790v2", 
    "title": "Solution of Non-Square Fuzzy Linear Systems", 
    "arxiv-id": "0911.0790v2", 
    "author": "\u015eahin Emrah Amrahov", 
    "publish": "2009-11-04T11:44:39Z", 
    "summary": "In this paper, a linear system of equations with crisp coefficients and fuzzy\nright-hand sides is investigated. All possible cases pertaining to the number\nof variables, n, and the number of equations, m, are dealt with. A solution is\nsought not as a fuzzy vector, as usual, but as a fuzzy set of vectors. Each\nvector in the solution set solves the given fuzzy linear system with a certain\npossibility. Assuming that the coefficient matrix is a full rank matrix, three\ncases are considered: For m = n (square system), the solution set is shown to\nbe a parallelepiped in coordinate space and is expressed by an explicit\nformula. For m > n (overdetermined system), the solution set is proved to be a\nconvex polyhedron and a novel geometric method is proposed to compute it. For m\n< n (underdetermined system), by determining the contribution of free\nvariables, general solution is computed. From the results of three cases\nmentioned above, a method is proposed to handle the general case, in which the\ncoefficient matrix is not necessarily a full rank matrix. Comprehensive\nexamples are provided and investigated in depth to illustrate each case and\nsuggested method."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.05.004", 
    "link": "http://arxiv.org/pdf/0911.2270v2", 
    "title": "An Iteratively Reweighted Algorithm for Sparse Reconstruction of   Subsurface Flow Properties from Nonlinear Dynamic Data", 
    "arxiv-id": "0911.2270v2", 
    "author": "B. Jafarpour", 
    "publish": "2009-11-11T23:27:03Z", 
    "summary": "In this paper, we present a practical algorithm based on sparsity\nregularization to effectively solve nonlinear dynamic inverse problems that are\nencountered in subsurface model calibration. We use an iteratively reweighted\nalgorithm that is widely used to solve linear inverse problems with sparsity\nconstraint known as compressed sensing to estimate permeability fields from\nnonlinear dynamic flow data."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.05.004", 
    "link": "http://arxiv.org/pdf/0911.4498v2", 
    "title": "Computation- and Space-Efficient Implementation of SSA", 
    "arxiv-id": "0911.4498v2", 
    "author": "Anton Korobeynikov", 
    "publish": "2009-11-23T21:41:17Z", 
    "summary": "The computational complexity of different steps of the basic SSA is\ndiscussed. It is shown that the use of the general-purpose \"blackbox\" routines\n(e.g. found in packages like LAPACK) leads to huge waste of time resources\nsince the special Hankel structure of the trajectory matrix is not taken into\naccount. We outline several state-of-the-art algorithms (for example,\nLanczos-based truncated SVD) which can be modified to exploit the structure of\nthe trajectory matrix. The key components here are hankel matrix-vector\nmultiplication and hankelization operator. We show that both can be computed\nefficiently by the means of Fast Fourier Transform. The use of these methods\nyields the reduction of the worst-case computational complexity from O(N^3) to\nO(k N log(N)), where N is series length and k is the number of eigentriples\ndesired."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/0912.0180v2", 
    "title": "On the indefinite Helmholtz equation: complex stretched absorbing   boundary layers, iterative analysis, and preconditioning", 
    "arxiv-id": "0912.0180v2", 
    "author": "Hisham bin Zubair", 
    "publish": "2009-12-01T15:15:44Z", 
    "summary": "This paper studies and analyzes a preconditioned Krylov solver for Helmholtz\nproblems that are formulated with absorbing boundary layers based on complex\ncoordinate stretching. The preconditioner problem is a Helmholtz problem where\nnot only the coordinates in the absorbing layer have an imaginary part, but\nalso the coordinates in the interior region. This results into a preconditioner\nproblem that is invertible with a multigrid cycle. We give a numerical analysis\nbased on the eigenvalues and evaluate the performance with several numerical\nexperiments. The method is an alternative to the complex shifted Laplacian and\nit gives a comparable performance for the studied model problems."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/0912.1135v2", 
    "title": "A fast randomized algorithm for orthogonal projection", 
    "arxiv-id": "0912.1135v2", 
    "author": "Mark Tygert", 
    "publish": "2009-12-06T21:02:14Z", 
    "summary": "We describe an algorithm that, given any full-rank matrix A having fewer rows\nthan columns, can rapidly compute the orthogonal projection of any vector onto\nthe null space of A, as well as the orthogonal projection onto the row space of\nA, provided that both A and its adjoint can be applied rapidly to arbitrary\nvectors. As an intermediate step, the algorithm solves the overdetermined\nlinear least-squares regression involving the adjoint of A (and so can be used\nfor this, too). The basis of the algorithm is an obvious but numerically\nunstable scheme; suitable use of a preconditioner yields numerical stability.\nWe generate the preconditioner rapidly via a randomized procedure that succeeds\nwith extremely high probability. In many circumstances, the method can\naccelerate interior-point methods for convex optimization, such as linear\nprogramming (Ming Gu, personal communication)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1001.0829v2", 
    "title": "Least-Squares on the Real Symplectic Group", 
    "arxiv-id": "1001.0829v2", 
    "author": "Simone Fiori", 
    "publish": "2010-01-06T07:32:37Z", 
    "summary": "The present paper discusses the problem of least-squares over the real\nsymplectic group of matrices Sp(2n,R)$. The least-squares problem may be\nextended from flat spaces to curved spaces by the notion of geodesic distance.\nThe resulting non-linear minimization problem on manifold may be tackled by\nmeans of a gradient-descent algorithm tailored to the geometry of the space at\nhand. In turn, gradient steepest descent on manifold may be implemented through\na geodesic-based stepping method. As the space Sp(2n,R) is a non-compact Lie\ngroup, it is convenient to endow it with a pseudo-Riemannian geometry. Indeed,\na pseudo-Riemannian metric allows the computation of geodesic arcs and geodesic\ndistances in closed form on Sp(2n,R)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1001.1185v1", 
    "title": "Some Architectures for Chebyshev Interpolation", 
    "arxiv-id": "1001.1185v1", 
    "author": "Theja Tulabandhula", 
    "publish": "2010-01-08T01:34:47Z", 
    "summary": "Digital architectures for Chebyshev interpolation are explored and a\nvariation which is word-serial in nature is proposed. These architectures are\ncontrasted with equispaced system structures. Further, Chebyshev interpolation\nscheme is compared to the conventional equispaced interpolation vis-a-vis\nreconstruction error and relative number of samples. It is also shown that the\nuse of a hybrid (or dual) Analog to Digital converter unit can reduce system\npower consumption by as much as 1/3rd of the original."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1001.5460v1", 
    "title": "Solving Tensor Structured Problems with Computational Tensor Algebra", 
    "arxiv-id": "1001.5460v1", 
    "author": "Patrick Hunziker", 
    "publish": "2010-01-29T19:24:29Z", 
    "summary": "Since its introduction by Gauss, Matrix Algebra has facilitated understanding\nof scientific problems, hiding distracting details and finding more elegant and\nefficient ways of computational solving. Today's largest problems, which often\noriginate from multidimensional data, might profit from even higher levels of\nabstraction. We developed a framework for solving tensor structured problems\nwith tensor algebra that unifies concepts from tensor analysis, multilinear\nalgebra and multidimensional signal processing. In contrast to the conventional\nmatrix approach, it allows the formulation of multidimensional problems, in a\nmultidimensional way, preserving structure and data coherence; and the\nimplementation of automated optimizations of solving algorithms, based on the\ncommutativity of all tensor operations. Its ability to handle large scientific\ntasks is showcased by a real-world, 4D medical imaging problem, with more than\n30 million unknown parameters solved on a current, inexpensive hardware. This\nsignificantly surpassed the best published matrix-based approach."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1002.1167v1", 
    "title": "Geometric Programming Problem with Co-Efficients and Exponents   Associated with Binary Numbers", 
    "arxiv-id": "1002.1167v1", 
    "author": "A. K. Das", 
    "publish": "2010-02-05T09:29:23Z", 
    "summary": "Geometric programming (GP) provides a power tool for solving a variety of\noptimization problems. In the real world, many applications of geometric\nprogramming (GP) are engineering design problems in which some of the problem\nparameters are estimating of actual values. This paper develops a solution\nprocedure to solve nonlinear programming problems using GP technique by\nsplitting the cost coefficients, constraint coefficients and exponents with the\nhelp of binary numbers. The equivalent mathematical programming problems are\nformulated to find their corresponding value of the objective function based on\nthe duality theorem. The ability of calculating the cost coefficients,\nconstraint coefficients and exponents developed in this paper might help lead\nto more realistic modeling efforts in engineering design areas. Standard\nnonlinear programming software has been used to solve the proposed optimization\nproblem. Two numerical examples are presented to illustrate the method."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1003.1473v1", 
    "title": "Comments on \"Routh Stability Criterion\"", 
    "arxiv-id": "1003.1473v1", 
    "author": "S. K. Katti", 
    "publish": "2010-03-07T14:09:11Z", 
    "summary": "In this note, we have shown special case on Routh stability criterion, which\nis not discussed, in previous literature. This idea can be useful in computer\nscience applications."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1003.1477v1", 
    "title": "Multi-objective Geometric Programming Problem With Weighted Mean Method", 
    "arxiv-id": "1003.1477v1", 
    "author": "K. K. Biswal", 
    "publish": "2010-03-07T14:19:58Z", 
    "summary": "Geometric programming is an important class of optimization problems that\nenable practitioners to model a large variety of real-world applications,\nmostly in the field of engineering design. In many real life optimization\nproblem multi-objective programming plays a vital role in socio-economical and\nindustrial optimizing problems. In this paper we have discussed the basic\nconcepts and principle of multiple objective optimization problems and\ndeveloped geometric programming (GP) technique to solve this optimization\nproblem using weighted method to obtain the non-inferior solutions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2010.07.022", 
    "link": "http://arxiv.org/pdf/1003.1794v1", 
    "title": "New Approach to Identify Common Eigenvalues of real matrices using   Gerschgorin Theorem and Bisection method", 
    "arxiv-id": "1003.1794v1", 
    "author": "S. K. Katti", 
    "publish": "2010-03-09T06:38:08Z", 
    "summary": "In this paper, a new approach is presented to determine common eigenvalues of\ntwo matrices. It is based on Gerschgorin theorem and Bisection method. The\nproposed approach is simple and can be useful in image processing and noise\nestimation."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1003.5257v3", 
    "title": "Enforcing the non-negativity constraint and maximum principles for   diffusion with decay on general computational grids", 
    "arxiv-id": "1003.5257v3", 
    "author": "K. B. Nakshatrala", 
    "publish": "2010-03-27T02:43:52Z", 
    "summary": "In this paper, we consider anisotropic diffusion with decay, and the\ndiffusivity coefficient to be a second-order symmetric and positive definite\ntensor. It is well-known that this particular equation is a second-order\nelliptic equation, and satisfies a maximum principle under certain regularity\nassumptions. However, the finite element implementation of the classical\nGalerkin formulation for both anisotropic and isotropic diffusion with decay\ndoes not respect the maximum principle.\n  We first show that the numerical accuracy of the classical Galerkin\nformulation deteriorates dramatically with increase in the decay coefficient\nfor isotropic medium and violates the discrete maximum principle. However, in\nthe case of isotropic medium, the extent of violation decreases with mesh\nrefinement. We then show that, in the case of anisotropic medium, the classical\nGalerkin formulation for anisotropic diffusion with decay violates the discrete\nmaximum principle even at lower values of decay coefficient and does not vanish\nwith mesh refinement. We then present a methodology for enforcing maximum\nprinciples under the classical Galerkin formulation for anisotropic diffusion\nwith decay on general computational grids using optimization techniques.\nRepresentative numerical results (which take into account anisotropy and\nheterogeneity) are presented to illustrate the performance of the proposed\nformulation."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1004.4769v1", 
    "title": "VAGO method for the solution of elliptic second-order boundary value   problems", 
    "arxiv-id": "1004.4769v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2010-04-27T11:55:56Z", 
    "summary": "Mathematical physics problems are often formulated using differential\noprators of vector analysis - invariant operators of first order, namely,\ndivergence, gradient and rotor operators. In approximate solution of such\nproblems it is natural to employ similar operator formulations for grid\nproblems, too. The VAGO (Vector Analysis Grid Operators) method is based on\nsuch a methodology. In this paper the vector analysis difference operators are\nconstructed using the Delaunay triangulation and the Voronoi diagrams. Further\nthe VAGO method is used to solve approximately boundary value problems for the\ngeneral elliptic equation of second order. In the convection-diffusion-reaction\nequation the diffusion coefficient is a symmetric tensor of second order."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1005.3097v1", 
    "title": "Effective Resistances, Statistical Leverage, and Applications to Linear   Equation Solving", 
    "arxiv-id": "1005.3097v1", 
    "author": "Michael W. Mahoney", 
    "publish": "2010-05-18T04:24:36Z", 
    "summary": "Recent work in theoretical computer science and scientific computing has\nfocused on nearly-linear-time algorithms for solving systems of linear\nequations. While introducing several novel theoretical perspectives, this work\nhas yet to lead to practical algorithms. In an effort to bridge this gap, we\ndescribe in this paper two related results. Our first and main result is a\nsimple algorithm to approximate the solution to a set of linear equations\ndefined by a Laplacian (for a graph $G$ with $n$ nodes and $m \\le n^2$ edges)\nconstraint matrix. The algorithm is a non-recursive algorithm; even though it\nruns in $O(n^2 \\cdot \\polylog(n))$ time rather than $O(m \\cdot polylog(n))$\ntime (given an oracle for the so-called statistical leverage scores), it is\nextremely simple; and it can be used to compute an approximate solution with a\ndirect solver. In light of this result, our second result is a straightforward\nconnection between the concept of graph resistance (which has proven useful in\nrecent algorithms for linear equation solvers) and the concept of statistical\nleverage (which has proven useful in numerically-implementable randomized\nalgorithms for large matrix problems and which has a natural data-analytic\ninterpretation)."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1005.5631v1", 
    "title": "Experimental Comparisons of Derivative Free Optimization Algorithms", 
    "arxiv-id": "1005.5631v1", 
    "author": "Marc Schoenauer", 
    "publish": "2010-05-31T09:10:30Z", 
    "summary": "In this paper, the performances of the quasi-Newton BFGS algorithm, the\nNEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES), the Differential Evolution (DE) algorithm and Particle Swarm\nOptimizers (PSO) are compared experimentally on benchmark functions reflecting\nimportant challenges encountered in real-world optimization problems.\nDependence of the performances in the conditioning of the problem and\nrotational invariance of the algorithms are in particular investigated."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1005.5712v1", 
    "title": "SM stability for time-dependent problems", 
    "arxiv-id": "1005.5712v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2010-05-31T17:55:44Z", 
    "summary": "Various classes of stable finite difference schemes can be constructed to\nobtain a numerical solution. It is important to select among all stable schemes\nsuch a scheme that is optimal in terms of certain additional criteria. In this\nstudy, we use a simple boundary value problem for a one-dimensional parabolic\nequation to discuss the selection of an approximation with respect to time. We\nconsider the pure diffusion equation, the pure convective transport equation\nand combined convection-diffusion phenomena. Requirements for the\nunconditionally stable finite difference schemes are formulated that are\nrelated to retaining the main features of the differential problem. The concept\nof SM stable finite difference scheme is introduced. The starting point are\ndifference schemes constructed on the basis of the various Pad$\\acute{e}$\napproximations."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1008.1596v1", 
    "title": "Bootstrap Markov chain Monte Carlo and optimal solutions for the Law of   Categorical Judgment (Corrected)", 
    "arxiv-id": "1008.1596v1", 
    "author": "Burton S. Rosner", 
    "publish": "2010-08-09T21:19:36Z", 
    "summary": "A novel procedure is described for accelerating the convergence of Markov\nchain Monte Carlo computations. The algorithm uses an adaptive bootstrap\ntechnique to generate candidate steps in the Markov Chain. It is efficient for\nsymmetric, convex probability distributions, similar to multivariate Gaussians,\nand it can be used for Bayesian estimation or for obtaining maximum likelihood\nsolutions with confidence limits. As a test case, the Law of Categorical\nJudgment (Corrected) was fitted with the algorithm to data sets from simulated\nrating scale experiments. The correct parameters were recovered from\npractical-sized data sets simulated for Full Signal Detection Theory and its\nspecial cases of standard Signal Detection Theory and Complementary Signal\nDetection Theory."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1010.2412v1", 
    "title": "Splitting schemes for hyperbolic heat conduction equation", 
    "arxiv-id": "1010.2412v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2010-10-12T15:18:47Z", 
    "summary": "Rapid processes of heat transfer are not described by the standard heat\nconduction equation. To take into account a finite velocity of heat transfer,\nwe use the hyperbolic model of heat conduction, which is connected with the\nrelaxation of heat fluxes. In this case, the mathematical model is based on a\nhyperbolic equation of second order or a system of equations for the\ntemperature and heat fluxes. In this paper we construct for the hyperbolic heat\nconduction equation the additive schemes of splitting with respect to\ndirections. Unconditional stability of locally one-dimensional splitting\nschemes is established. New splitting schemes are proposed and studied for a\nsystem of equations written in terms of the temperature and heat fluxes."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1011.1599v2", 
    "title": "A Stable Explicit Scheme for Solving Non-Homogeneous Constant   Coefficients Equation using Green's Function", 
    "arxiv-id": "1011.1599v2", 
    "author": "Hiroshi Abe", 
    "publish": "2010-11-07T00:47:11Z", 
    "summary": "A numerical explicit method to evaluates transient solutions of linear\npartial differential non-homogeneous equation with constant coefficients is\nproposed."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1011.2531v1", 
    "title": "A Stable Explicit Scheme for Solving Inhomogeneous Constant Coefficients   Differential Equation using Green's Function", 
    "arxiv-id": "1011.2531v1", 
    "author": "Hiroshi Abe", 
    "publish": "2010-11-10T23:35:48Z", 
    "summary": "A numerical explicit method to evaluates transient solutions of linear\npartial differential inhomogeneous equation with constant coefficients is\nproposed. A general form of the scheme for a specific linear inhomogeneous\nequation is shown. The method is applied to the wave equation and the diffuse\nequation and is investigated by simulating simple models. The numerical\nsolutions of the proposed method show good agreement to the exact solutions.\nComparing with explicit FDM, FDM shows the instability by the violation of CFL\ncondition whereas the proposed method is always stable irrespective of any time\nstep width."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1011.4580v1", 
    "title": "A New Algorithm for General Cyclic Heptadiagonal Linear Systems Using   Sherman-Morrison-Woodbury formula", 
    "arxiv-id": "1011.4580v1", 
    "author": "A. A. Karawia", 
    "publish": "2010-11-20T12:37:45Z", 
    "summary": "In this paper, a new efficient computational algorithm is presented for\nsolving cyclic heptadiagonal linear systems based on using of heptadiagonal\nlinear solver and Sherman-Morrison-Woodbury formula. The implementation of the\nalgorithm using computer algebra systems (CAS) such as MAPLE and MATLAB is\nstraightforward. Numerical example is presented for the sake of illustration."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1012.4307v2", 
    "title": "A preconditioned iterative solver for the scattering solutions of the   Schr\u00f6dinger equation", 
    "arxiv-id": "1012.4307v2", 
    "author": "Wim Vanroose", 
    "publish": "2010-12-20T12:48:09Z", 
    "summary": "The Schr\\\"odinger equation defines the dynamics of quantum particles which\nhas been an area of unabated interest in physics. We demonstrate how simple\ntransformations of the Schr\\\"odinger equation leads to a coupled linear system,\nwhereby each diagonal block is a high frequency Helmholtz problem. Based on\nthis model, we derive indefinite Helmholtz model problems with strongly varying\nwavenumbers. We employ the iterative approach for their solution. In\nparticular, we develop a preconditioner that has its spectrum restricted to a\nquadrant (of the complex plane) thereby making it easily invertible by\nmultigrid methods with standard components. This multigrid preconditioner is\nused in conjuction with suitable Krylov-subspace methods for solving the\nindefinite Helmholtz model problems. The aim of this study is to report the\nfeasbility of this preconditioner for the model problems. We compare this idea\nwith the other prevalent preconditioning ideas, and discuss its merits. Results\nof numerical experiments are presented, which complement the proposed ideas,\nand show that this preconditioner may be used in an automatic setting."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1012.5379v2", 
    "title": "GMRES-based multigrid for the complex scaled preconditoner for the   indefinite Helmholtz equation", 
    "arxiv-id": "1012.5379v2", 
    "author": "Hisham bin Zubair", 
    "publish": "2010-12-24T10:13:47Z", 
    "summary": "Multigrid preconditioners and solvers for the indefinite Helmholtz equation\nsuffer from non-stability of the stationary smoothers due to the indefinite\nspectrum of the operator. In this paper we explore GMRES as a replacement for\nthe stationary smoothers of the standard multigrid method. This results in a\nrobust and efficient solver for a complex shifted or stretched Helmholtz\nproblem that can be used as a preconditioner. Very few GMRES iterations are\nrequired on each level to build a good multigrid method. The convergence\nbehavior is compared to a theoretically derived stable polynomial smoother. We\ntest this method on some benchmark problems and report on the observed\nconvergence behavior."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1012.5484v1", 
    "title": "A contribution to the conditioning of the total least squares problem", 
    "arxiv-id": "1012.5484v1", 
    "author": "Serge Gratton", 
    "publish": "2010-12-25T16:42:30Z", 
    "summary": "We derive closed formulas for the condition number of a linear function of\nthe total least squares solution. Given an over determined linear system Ax=b,\nwe show that this condition number can be computed using the singular values\nand the right singular vectors of [A,b] and A. We also provide an upper bound\nthat requires the computation of the largest and the smallest singular value of\n[A,b] and the smallest singular value of A. In numerical examples, we compare\nthese values and the resulting forward error bounds with existing error\nestimates."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1101.2173v1", 
    "title": "The power and Arnoldi methods in an algebra of circulants", 
    "arxiv-id": "1101.2173v1", 
    "author": "James M. Varah", 
    "publish": "2011-01-11T18:05:00Z", 
    "summary": "Circulant matrices play a central role in a recently proposed formulation of\nthree-way data computations. In this setting, a three-way table corresponds to\na matrix where each \"scalar\" is a vector of parameters defining a circulant.\nThis interpretation provides many generalizations of results from matrix or\nvector-space algebra. We derive the power and Arnoldi methods in this algebra.\nIn the course of our derivation, we define inner products, norms, and other\nnotions. These extensions are straightforward in an algebraic sense, but the\nimplications are dramatically different from the standard matrix case. For\nexample, a matrix of circulants has a polynomial number of eigenvalues in its\ndimension; although, these can all be represented by a carefully chosen\ncanonical set of eigenvalues and vectors. These results and algorithms are\nclosely related to standard decoupling techniques on block-circulant matrices\nusing the fast Fourier transform."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1101.2395v1", 
    "title": "Domain decomposition schemes for evolutionary equations of first order   with not self-adjoint operators", 
    "arxiv-id": "1101.2395v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2011-01-12T16:12:27Z", 
    "summary": "Domain decomposition methods are essential in solving applied problems on\nparallel computer systems. For boundary value problems for evolutionary\nequations the implicit schemes are in common use to solve problems at a new\ntime level employing iterative methods of domain decomposition. An alternative\napproach is based on constructing iteration-free methods based on special\nschemes of splitting into subdomains. Such regionally-additive schemes are\nconstructed using the general theory of additive operator-difference schemes.\nThere are employed the analogues of classical schemes of alternating direction\nmethod, locally one-dimensional schemes, factorization methods, vector and\nregularized additive schemes. The main results were obtained here for\ntime-dependent problems with self-adjoint elliptic operators of second order.\n  The paper discusses the Cauchy problem for the first order evolutionary\nequations with a nonnegative not self-adjoint operator in a finite-dimensional\nHilbert space. Based on the partition of unit, we have constructed the\noperators of decomposition which preserve nonnegativity for the individual\noperator terms of splitting. Unconditionally stable additive schemes of domain\ndecomposition were constructed using the regularization principle for\noperator-difference schemes. Vector additive schemes were considered, too. The\nresults of our work are illustrated by a model problem for the two-dimensional\nparabolic equation."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1102.0642v1", 
    "title": "Domain decomposition schemes for the Stokes equation", 
    "arxiv-id": "1102.0642v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2011-02-03T10:51:42Z", 
    "summary": "Numerical algorithms for solving problems of mathematical physics on modern\nparallel computers employ various domain decomposition techniques. Domain\ndecomposition schemes are developed here to solve numerically initial/boundary\nvalue problems for the Stokes system of equations in the primitive variables\npressure-velocity. Unconditionally stable schemes of domain decomposition are\nbased on the partition of unit for a computational domain and the corresponding\nHilbert spaces of grid functions."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1102.4821v1", 
    "title": "Rank Aggregation via Nuclear Norm Minimization", 
    "arxiv-id": "1102.4821v1", 
    "author": "Lek-Heng Lim", 
    "publish": "2011-02-23T19:14:23Z", 
    "summary": "The process of rank aggregation is intimately intertwined with the structure\nof skew-symmetric matrices. We apply recent advances in the theory and\nalgorithms of matrix completion to skew-symmetric matrices. This combination of\nideas produces a new method for ranking a set of items. The essence of our idea\nis that a rank aggregation describes a partially filled skew-symmetric matrix.\nWe extend an algorithm for matrix completion to handle skew-symmetric data and\nuse that to extract ranks for each item. Our algorithm applies to both pairwise\ncomparison and rating data. Because it is based on matrix completion, it is\nrobust to both noise and incomplete data. We show a formal recovery result for\nthe noiseless case and present a detailed study of the algorithm on synthetic\ndata and Netflix ratings."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1103.3026v1", 
    "title": "Generalized Filtering Decomposition", 
    "arxiv-id": "1103.3026v1", 
    "author": "Fr\u00e9d\u00e9ric Nataf", 
    "publish": "2011-03-15T20:35:13Z", 
    "summary": "This paper introduces a new preconditioning technique that is suitable for\nmatrices arising from the discretization of a system of PDEs on unstructured\ngrids. The preconditioner satisfies a so-called filtering property, which\nensures that the input matrix is identical with the preconditioner on a given\nfiltering vector. This vector is chosen to alleviate the effect of low\nfrequency modes on convergence and so decrease or eliminate the plateau which\nis often observed in the convergence of iterative methods. In particular, the\npaper presents a general approach that allows to ensure that the filtering\ncondition is satisfied in a matrix decomposition. The input matrix can have an\narbitrary sparse structure. Hence, it can be reordered using nested dissection,\nto allow a parallel computation of the preconditioner and of the iterative\nprocess."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1105.3448v1", 
    "title": "Substructuring domain decomposition scheme for unsteady problems", 
    "arxiv-id": "1105.3448v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2011-05-17T18:40:18Z", 
    "summary": "Domain decomposition methods are used for approximate solving boundary\nproblems for partial differential equations on parallel computing systems.\nSpecific features of unsteady problems are taken into account in the most\ncomplete way in iteration-free schemes of domain decomposition.\nRegionally-additive schemes are based on different classes of splitting\nschemes. In this paper we highlight a class of domain decomposition schemes\nwhich is based on the partition of the initial domain into subdomains with\ncommon boundary nodes. Using the partition of unit we have constructed and\nstudied unconditionally stable schemes of domain decomposition based on\ntwo-component splitting: the problem within subdomain and the problem at their\nboundaries. As an example there is considered the Cauchy problem for\nevolutionary equations of first and second order with non-negative self-adjoint\noperator in a finite Hilbert space. The theoretical consideration is\nsupplemented with numerical solving a model problem for the two-dimensional\nparabolic equation."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.2389", 
    "link": "http://arxiv.org/pdf/1105.4337v1", 
    "title": "Equivalent Effect Function and Fast Intrinsic Mode Decomposition", 
    "arxiv-id": "1105.4337v1", 
    "author": "Louis Yu Lu", 
    "publish": "2011-05-22T13:14:06Z", 
    "summary": "The Equivalent Effect Function (EEF) is defined as having the identical\nintegral values on the control points of the original time series data; the EEF\ncan be obtained from the derivative of the spline function passing through the\nintegral values on the control points. By choosing control points with\ndifferent criteria, the EEF can be used to find the intrinsic mode\nfunction(IMF, fluctuation) and the residue (trend); to fit the curve of the\noriginal data function; and to take samples on original data with equivalent\neffect. As examples of application, results of trend and fluctuation on real\nstock historical data are calculated on different time scales. A new approach\nto extend the EEF to 2D intrinsic mode decomposition is introduced to resolve\nthe inter slice non continuity problem, some photo image decomposition examples\nare presented."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSPCT.2009.5164207", 
    "link": "http://arxiv.org/pdf/1107.4189v1", 
    "title": "Development and Modelling of High-Efficiency Computing Structure for   Digital Signal Processing", 
    "arxiv-id": "1107.4189v1", 
    "author": "Hoon Jae Lee", 
    "publish": "2011-07-21T07:44:17Z", 
    "summary": "The paper is devoted to problem of spline approximation. A new method of\nnodes location for curves and surfaces computer construction by means of\nB-splines and results of simulink-modeling is presented. The advantages of this\npaper is that we comprise the basic spline with classical polynomials both on\naccuracy, as well as degree of paralleling calculations are also shown."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSPCT.2009.5164207", 
    "link": "http://arxiv.org/pdf/1107.4810v4", 
    "title": "Numerical Stability of Explicit Runge-Kutta Finite-Difference Schemes   for the Nonlinear Schr\u00f6dinger Equation", 
    "arxiv-id": "1107.4810v4", 
    "author": "Ricardo Carretero-Gonz\u00e1lez", 
    "publish": "2011-07-24T22:08:52Z", 
    "summary": "Linearized numerical stability bounds for solving the nonlinear\ntime-dependent Schr\\\"odinger equation (NLSE) using explicit finite-differencing\nare shown. The bounds are computed for the fourth-order Runge-Kutta scheme in\ntime and both second-order and fourth-order central differencing in space.\nResults are given for Dirichlet, modulus-squared Dirichlet, Laplacian-zero, and\nperiodic boundary conditions for one, two, and three dimensions. Our approach\nis to use standard Runge-Kutta linear stability theory, treating the\nnonlinearity of the NLSE as a constant. The required bounds on the eigenvalues\nof the scheme matrices are found analytically when possible, and otherwise\nestimated using the Gershgorin circle theorem."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSPCT.2009.5164207", 
    "link": "http://arxiv.org/pdf/1107.5087v2", 
    "title": "Image representation by blob and its application in CT reconstruction   from few projections", 
    "arxiv-id": "1107.5087v2", 
    "author": "Samuel Legoupil", 
    "publish": "2011-07-25T22:22:16Z", 
    "summary": "The localized radial symmetric function, or blob, is an ideal alternative to\nthe pixel basis for X-ray computed tomography (CT) image reconstruction. In\nthis paper we develop image representation models using blob, and propose\nreconstruction methods for few projections data. The image is represented in a\nshift invariant space generated by a Gaussian blob or a multiscale blob system\nof different frequency selectivity, and the reconstruction is done through\nminimizing the Total Variation or the 1 norm of blob coefficients. Some 2D\nnumerical results are presented, where we use GPU platform for accelerating the\nX-ray projection and back-projection, the interpolation and the gradient\ncomputations."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSPCT.2009.5164207", 
    "link": "http://arxiv.org/pdf/1107.5479v1", 
    "title": "Iterative methods for solving the pressure problem at multiphase   filtration", 
    "arxiv-id": "1107.5479v1", 
    "author": "M. Vasil'eva", 
    "publish": "2011-07-27T14:08:45Z", 
    "summary": "Applied problems of oil and gas recovery are studied numerically using the\nmathematical models of multiphase fluid flows in porous media. The basic model\nincludes the continuity equations and the Darcy laws for each phase, as well as\nthe algebraic expression for the sum of saturations. Primary computational\nalgorithms are implemented for such problems using the pressure equation. In\nthis paper, we highlight the basic properties of the pressure problem and\ndiscuss the necessity of their fulfillment at the discrete level. The resulting\nelliptic problem for the pressure equation is characterized by a\nnon-selfadjoint operator. Possibilities of approximate solving the elliptic\nproblem are considered using the iterative methods. Special attention is given\nto the numerical algorithms for calculating the pressure on parallel computers."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1107.5790v1", 
    "title": "Image Deblurring Using Derivative Compressed Sensing for Optical Imaging   Application", 
    "arxiv-id": "1107.5790v1", 
    "author": "Zhou Wang", 
    "publish": "2011-07-28T18:52:28Z", 
    "summary": "Reconstruction of multidimensional signals from the samples of their partial\nderivatives is known to be a standard problem in inverse theory. Such and\nsimilar problems routinely arise in numerous areas of applied sciences,\nincluding optical imaging, laser interferometry, computer vision, remote\nsensing and control. Though being ill-posed in nature, the above problem can be\nsolved in a unique and stable manner, provided proper regularization and\nrelevant boundary conditions. In this paper, however, a more challenging setup\nis addressed, in which one has to recover an image of interest from its noisy\nand blurry version, while the only information available about the imaging\nsystem at hand is the amplitude of the generalized pupil function (GPF) along\nwith partial observations of the gradient of GPF's phase. In this case, the\nphase-related information is collected using a simplified version of the\nShack-Hartmann interferometer, followed by recovering the entire phase by means\nof derivative compressed sensing. Subsequently, the estimated phase can be\ncombined with the amplitude of the GPF to produce an estimate of the point\nspread function (PSF), whose knowledge is essential for subsequent image\ndeconvolution. In summary, the principal contribution of this work is twofold.\nFirst, we demonstrate how to simplify the construction of the Shack-Hartmann\ninterferometer so as to make it less expensive and hence more accessible.\nSecond, it is shown by means of numerical experiments that the above\nsimplification and its associated solution scheme produce image reconstructions\nof the quality comparable to those obtained using dense sampling of the GPF\nphase."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1108.5822v3", 
    "title": "Banded Householder representation of linear subspaces", 
    "arxiv-id": "1108.5822v3", 
    "author": "Geoffrey Irving", 
    "publish": "2011-08-30T03:53:31Z", 
    "summary": "We show how to compactly represent any $n$-dimensional subspace of $R^m$ as a\nbanded product of Householder reflections using $n(m - n)$ floating point\nnumbers. This is optimal since these subspaces form a Grassmannian space\n$Gr_n(m)$ of dimension $n(m - n)$. The representation is stable and easy to\ncompute: any matrix can be factored into the product of a banded Householder\nmatrix and a square matrix using two to three QR decompositions."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1110.5989v1", 
    "title": "A Heuristic Description of Fast Fourier Transform", 
    "arxiv-id": "1110.5989v1", 
    "author": "Xiao Fan", 
    "publish": "2011-10-27T05:51:23Z", 
    "summary": "Fast Fourier Transform (FFT) is an efficient algorithm to compute the\nDiscrete Fourier Transform (DFT) and its inverse. In this paper, we pay special\nattention to the description of complex-data FFT. We analyze two common\ndescriptions of FFT and propose a new presentation. Our heuristic description\nis helpful for students and programmers to grasp the algorithm entirely and\ndeeply."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1111.3971v1", 
    "title": "The Numerical Generalized Least-Squares Estimator of an Unknown Constant   Mean of Random Field", 
    "arxiv-id": "1111.3971v1", 
    "author": "Tomasz Suslo", 
    "publish": "2011-11-15T12:45:54Z", 
    "summary": "We constraint on computer the best linear unbiased generalized statistics of\nrandom field for the best linear unbiased generalized statistics of an unknown\nconstant mean of random field and derive the numerical generalized\nleast-squares estimator of an unknown constant mean of random field. We derive\nthe third constraint of spatial statistics and show that the classic\ngeneralized least-squares estimator of an unknown constant mean of the field is\nonly an asymptotic disjunction of the numerical one."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1112.1294v1", 
    "title": "Additive schemes (splitting schemes) for some systems of evolutionary   equations", 
    "arxiv-id": "1112.1294v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2011-12-06T14:42:14Z", 
    "summary": "On the basis of additive schemes (splitting schemes) we construct efficient\nnumerical algorithms to solve approximately the initial-boundary value problems\nfor systems of time-dependent partial differential equations (PDEs). In many\napplied problems the individual components of the vector of unknowns are\ncoupled together and then splitting schemes are applied in order to get a\nsimple problem for evaluating components at a new time level. Typically, the\nadditive operator-difference schemes for systems of evolutionary equations are\nconstructed for operators coupled in space. In this paper we investigate more\ngeneral problems where coupling of derivatives in time for components of the\nsolution vector takes place. Splitting schemes are developed using an additive\nrepresentation for both the primary operator of the problem and the operator at\nthe time derivative. Splitting schemes are based on a triangular two-component\nrepresentation of the operators."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1201.3914v1", 
    "title": "Floating-Point Arithmetic on Round-to-Nearest Representations", 
    "arxiv-id": "1201.3914v1", 
    "author": "Adrien Panhaleux", 
    "publish": "2012-01-18T10:32:38Z", 
    "summary": "Recently we introduced a class of number representations denoted\nRN-representations, allowing an un-biased rounding-to-nearest to take place by\na simple truncation. In this paper we briefly review the binary fixed-point\nrepresentation in an encoding which is essentially an ordinary 2's complement\nrepresentation with an appended round-bit. Not only is this rounding a constant\ntime operation, so is also sign inversion, both of which are at best log-time\noperations on ordinary 2's complement representations. Addition, multiplication\nand division is defined in such a way that rounding information can be carried\nalong in a meaningful way, at minimal cost. Based on the fixed-point encoding\nwe here define a floating point representation, and describe to some detail a\npossible implementation of a floating point arithmetic unit employing this\nrepresentation, including also the directed roundings."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1201.5975v1", 
    "title": "Floating-Point Numbers with Error Estimates (revised)", 
    "arxiv-id": "1201.5975v1", 
    "author": "Glauco Masotti", 
    "publish": "2012-01-28T17:22:28Z", 
    "summary": "The study addresses the problem of precision in floating-point (FP)\ncomputations. A method for estimating the errors which affect intermediate and\nfinal results is proposed and a summary of many software simulations is\ndiscussed. The basic idea consists of representing FP numbers by means of a\ndata structure collecting value and estimated error information. Under certain\nconstraints, the estimate of the absolute error is accurate and has a compact\nstatistical distribution. By monitoring the estimated relative error during a\ncomputation (an ad-hoc definition of relative error has been used), the\nvalidity of results can be ensured. The error estimate enables the\nimplementation of robust algorithms, and the detection of ill-conditioned\nproblems. A dynamic extension of number precision, under the control of error\nestimates, is advocated, in order to compute results within given error bounds.\nA reduced time penalty could be achieved by a specialized FP processor. The\nrealization of a hardwired processor incorporating the method, with current\ntechnology, should not be anymore a problem and would make the practical\nadoption of the method feasible for most applications."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2012.2190610", 
    "link": "http://arxiv.org/pdf/1203.0889v1", 
    "title": "Data-Driven Execution of Fast Multipole Methods", 
    "arxiv-id": "1203.0889v1", 
    "author": "Rio Yokota", 
    "publish": "2012-03-05T12:40:45Z", 
    "summary": "Fast multipole methods have O(N) complexity, are compute bound, and require\nvery little synchronization, which makes them a favorable algorithm on\nnext-generation supercomputers. Their most common application is to accelerate\nN-body problems, but they can also be used to solve boundary integral\nequations. When the particle distribution is irregular and the tree structure\nis adaptive, load-balancing becomes a non-trivial question. A common strategy\nfor load-balancing FMMs is to use the work load from the previous step as\nweights to statically repartition the next step. The authors discuss in the\npaper another approach based on data-driven execution to efficiently tackle\nthis challenging load-balancing problem. The core idea consists of breaking the\nmost time-consuming stages of the FMMs into smaller tasks. The algorithm can\nthen be represented as a Directed Acyclic Graph (DAG) where nodes represent\ntasks, and edges represent dependencies among them. The execution of the\nalgorithm is performed by asynchronously scheduling the tasks using the QUARK\nruntime environment, in a way such that data dependencies are not violated for\nnumerical correctness purposes. This asynchronous scheduling results in an\nout-of-order execution. The performance results of the data-driven FMM\nexecution outperform the previous strategy and show linear speedup on a\nquad-socket quad-core Intel Xeon system."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1203.2739v1", 
    "title": "Analyzing and enhancing OSKI for sparse matrix-vector multiplication", 
    "arxiv-id": "1203.2739v1", 
    "author": "Cevdet Aykanat", 
    "publish": "2012-03-13T08:50:14Z", 
    "summary": "Sparse matrix-vector multiplication (SpMxV) is a kernel operation widely used\nin iterative linear solvers. The same sparse matrix is multiplied by a dense\nvector repeatedly in these solvers. Matrices with irregular sparsity patterns\nmake it difficult to utilize cache locality effectively in SpMxV computations.\nIn this work, we investigate single- and multiple-SpMxV frameworks for\nexploiting cache locality in SpMxV computations. For the single-SpMxV\nframework, we propose two cache-size-aware top-down row/column-reordering\nmethods based on 1D and 2D sparse matrix partitioning by utilizing the\ncolumn-net and enhancing the row-column-net hypergraph models of sparse\nmatrices. The multiple-SpMxV framework depends on splitting a given matrix into\na sum of multiple nonzero-disjoint matrices so that the SpMxV operation is\nperformed as a sequence of multiple input- and output-dependent SpMxV\noperations. For an effective matrix splitting required in this framework, we\npropose a cache-size-aware top-down approach based on 2D sparse matrix\npartitioning by utilizing the row-column-net hypergraph model. The primary\nobjective in all of the three methods is to maximize the exploitation of\ntemporal locality. We evaluate the validity of our models and methods on a wide\nrange of sparse matrices by performing actual runs through using OSKI.\nExperimental results show that proposed methods and models outperform\nstate-of-the-art schemes."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1203.4481v2", 
    "title": "Matrix Recipes for Hard Thresholding Methods", 
    "arxiv-id": "1203.4481v2", 
    "author": "Volkan Cevher", 
    "publish": "2012-03-20T15:56:58Z", 
    "summary": "In this paper, we present and analyze a new set of low-rank recovery\nalgorithms for linear inverse problems within the class of hard thresholding\nmethods. We provide strategies on how to set up these algorithms via basic\ningredients for different configurations to achieve complexity vs. accuracy\ntradeoffs. Moreover, we study acceleration schemes via memory-based techniques\nand randomized, $\\epsilon$-approximate matrix projections to decrease the\ncomputational costs in the recovery process. For most of the configurations, we\npresent theoretical analysis that guarantees convergence under mild problem\nconditions. Simulation results demonstrate notable performance improvements as\ncompared to state-of-the-art algorithms both in terms of reconstruction\naccuracy and computational complexity."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1205.3597v1", 
    "title": "The Correct Classic Generalized Least-Squares Estimator of an Unknown   Constant Mean of Randon Field", 
    "arxiv-id": "1205.3597v1", 
    "author": "Tomasz Suslo", 
    "publish": "2012-05-16T08:44:58Z", 
    "summary": "The aim of the paper is to derive for the negative correlation function with\na time parameter an asymptotic disjunction of the numerical generalized\nleast-squares estimator of an unknown constant mean of random field in fact the\ncorrect classic generalized least-squares estimator of an unknown constant mean\nof the field."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1206.0701v3", 
    "title": "A numerical methodology for enforcing maximum principles and the   non-negative constraint for transient diffusion equations", 
    "arxiv-id": "1206.0701v3", 
    "author": "M. Shabouei", 
    "publish": "2012-06-04T18:50:29Z", 
    "summary": "Transient diffusion equations arise in many branches of engineering and\napplied sciences (e.g., heat transfer and mass transfer), and are parabolic\npartial differential equations. It is well-known that, under certain\nassumptions on the input data, these equations satisfy important mathematical\nproperties like maximum principles and the non-negative constraint, which have\nimplications in mathematical modeling. However, existing numerical formulations\nfor these types of equations do not, in general, satisfy maximum principles and\nthe non-negative constraint. In this paper, we present a methodology for\nenforcing maximum principles and the non-negative constraint for transient\nanisotropic diffusion equation. The method of horizontal lines (also known as\nthe Rothe method) is applied in which the time is discretized first. This\nresults in solving steady anisotropic diffusion equation with decay equation at\nevery discrete time level. The proposed methodology for transient anisotropic\ndiffusion equation will satisfy maximum principles and the non-negative\nconstraint on general computational grids, and with no additional restrictions\non the time step. We illustrate the performance and accuracy of the proposed\nformulation using representative numerical examples. We also perform numerical\nconvergence of the proposed methodology. For comparison, we also present the\nresults from the standard single-field semi-discrete formulation and the\nresults from a popular software package, which all will violate maximum\nprinciples and the non-negative constraint."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1208.0701v10", 
    "title": "Numerical Computations For Operator Axiom", 
    "arxiv-id": "1208.0701v10", 
    "author": "Pith Xie", 
    "publish": "2012-08-03T10:31:20Z", 
    "summary": "Operator Axiom produces new real numbers with new operators. New operators\nnaturally produce new equations and thus extend the traditional mathematical\nmodels which are selected to describe various scientific rules. So new\noperators help to describe complex scientific rules which are difficult\ndescribed by traditional equations and have an enormous application potential.\nAs to the equations including new operators, engineering computation often need\nthe approximate solutions reflecting an intuitive order relation and\nequivalence relation. However, the order relation and equivalence relation of\nreal numbers are not as intuitive as those of base-b expansions. Thus, this\npaper introduces numerical computations to approximate all real numbers with\nbase-b expansions."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1208.2292v2", 
    "title": "L1 Splines for Robust, Simple, and Fast Smoothing of Grid Data", 
    "arxiv-id": "1208.2292v2", 
    "author": "Guillermo Sapiro", 
    "publish": "2012-08-10T22:09:15Z", 
    "summary": "Splines are a popular and attractive way of smoothing noisy data. Computing\nsplines involves minimizing a functional which is a linear combination of a\nfitting term and a regularization term. The former is classically computed\nusing a (weighted) L2 norm while the latter ensures smoothness. Thus, when\ndealing with grid data, the optimization can be solved very efficiently using\nthe DCT. In this work we propose to replace the L2 norm in the fitting term\nwith an L1 norm, leading to automatic robustness to outliers. To solve the\nresulting minimization problem we propose an extremely simple and efficient\nnumerical scheme based on split-Bregman iteration combined with DCT.\nExperimental validation shows the high-quality results obtained in short\nprocessing times."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1208.2451v1", 
    "title": "LU factorization with panel rank revealing pivoting and its   communication avoiding version", 
    "arxiv-id": "1208.2451v1", 
    "author": "Ming Gu", 
    "publish": "2012-08-12T19:19:23Z", 
    "summary": "We present the LU decomposition with panel rank revealing pivoting (LU_PRRP),\nan LU factorization algorithm based on strong rank revealing QR panel\nfactorization. LU_PRRP is more stable than Gaussian elimination with partial\npivoting (GEPP). Our extensive numerical experiments show that the new\nfactorization scheme is as numerically stable as GEPP in practice, but it is\nmore resistant to pathological cases and easily solves the Wilkinson matrix and\nthe Foster matrix. We also present CALU_PRRP, a communication avoiding version\nof LU_PRRP that minimizes communication. CALU_PRRP is based on tournament\npivoting, with the selection of the pivots at each step of the tournament being\nperformed via strong rank revealing QR factorization. CALU_PRRP is more stable\nthan CALU, the communication avoiding version of GEPP. CALU_PRRP is also more\nstable in practice and is resistant to pathological cases on which GEPP and\nCALU fail."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1209.3516v3", 
    "title": "An FMM Based on Dual Tree Traversal for Many-core Architectures", 
    "arxiv-id": "1209.3516v3", 
    "author": "Rio Yokota", 
    "publish": "2012-09-16T19:27:02Z", 
    "summary": "The present work attempts to integrate the independent efforts in the fast\nN-body community to create the fastest N-body library for many-core and\nheterogenous architectures. Focus is placed on low accuracy optimizations, in\nresponse to the recent interest to use FMM as a preconditioner for sparse\nlinear solvers. A direct comparison with other state-of-the-art fast N-body\ncodes demonstrates that orders of magnitude increase in performance can be\nachieved by careful selection of the optimal algorithm and low-level\noptimization of the code. The current N-body solver uses a fast multipole\nmethod with an efficient strategy for finding the list of cell-cell\ninteractions by a dual tree traversal. A task-based threading model is used to\nmaximize thread-level parallelism and intra-node load-balancing. In order to\nextract the full potential of the SIMD units on the latest CPUs, the inner\nkernels are optimized using AVX instructions. Our code -- exaFMM -- is an order\nof magnitude faster than the current state-of-the-art FMM codes, which are\nthemselves an order of magnitude faster than the average FMM code."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1211.4332v1", 
    "title": "Real root refinements for univariate polynomial equations", 
    "arxiv-id": "1211.4332v1", 
    "author": "Ye Liang", 
    "publish": "2012-11-19T08:30:30Z", 
    "summary": "Real root finding of polynomial equations is a basic problem in computer\nalgebra. This task is usually divided into two parts: isolation and refinement.\nIn this paper, we propose two algorithms LZ1 and LZ2 to refine real roots of\nunivariate polynomial equations. Our algorithms combine Newton's method and the\nsecant method to bound the unique solution in an interval of a monotonic convex\nisolation (MCI) of a polynomial, and have quadratic and cubic convergence\nrates, respectively. To avoid the swell of coefficients and speed up the\ncomputation, we implement the two algorithms by using the floating-point\ninterval method in Maple15 with the package intpakX. Experiments show that our\nmethods are effective and much faster than the function RefineBox in the\nsoftware Maple15 on benchmark polynomials."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1211.6822v2", 
    "title": "Calculation of orthant probabilities by the holonomic gradient method", 
    "arxiv-id": "1211.6822v2", 
    "author": "Akimichi Takemura", 
    "publish": "2012-11-29T07:03:24Z", 
    "summary": "We apply the holonomic gradient method (HGM) introduced by [9] to the\ncalculation of orthant probabilities of multivariate normal distribution. The\nholonomic gradient method applied to orthant probabilities is found to be a\nvariant of Plackett's recurrence relation ([14]). However an implementation of\nthe method yields recurrence relations more suitable for numerical computation\nthan Plackett's recurrence relation. We derive some theoretical results on the\nholonomic system for the orthant probabilities. These results show that\nmultivariate normal orthant probabilities possess some remarkable properties\nfrom the viewpoint of holonomic systems. Finally we show that numerical\nperformance of our method is comparable or superior compared to existing\nmethods."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1302.3876v2", 
    "title": "An Efficient Implementation of the Ensemble Kalman Filter Based on an   Iterative Sherman-Morrison Formula", 
    "arxiv-id": "1302.3876v2", 
    "author": "Jeffrey Anderson", 
    "publish": "2013-02-15T20:46:05Z", 
    "summary": "We present a practical implementation of the ensemble Kalman (EnKF) filter\nbased on an iterative Sherman-Morrison formula. The new direct method exploits\nthe special structure of the ensemble-estimated error covariance matrices in\norder to efficiently solve the linear systems involved in the analysis step of\nthe EnKF. The computational complexity of the proposed implementation is\nequivalent to that of the best EnKF implementations available in the literature\nwhen the number of observations is much larger than the number of ensemble\nmembers. Even when this conditions is not fulfilled, the proposed method is\nexpected to perform well since it does not employ matrix decompositions.\nComputational experiments using the Lorenz 96 and the oceanic quasi-geostrophic\nmodels are performed in order to compare the proposed algorithm with EnKF\nimplementations that use matrix decompositions. In terms of accuracy, the\nresults of all implementations are similar. The proposed method is considerably\nfaster than other EnKF variants, even when the number of observations is large\nrelative to the number of ensemble members."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1308.4275v2", 
    "title": "Efficient estimation of eigenvalue counts in an interval", 
    "arxiv-id": "1308.4275v2", 
    "author": "Yousef Saad", 
    "publish": "2013-08-20T10:16:16Z", 
    "summary": "Estimating the number of eigenvalues located in a given interval of a large\nsparse Hermitian matrix is an important problem in certain applications and it\nis a prerequisite of eigensolvers based on a divide-and-conquer paradigm. Often\nan exact count is not necessary and methods based on stochastic estimates can\nbe utilized to yield rough approximations. This paper examines a number of\ntechniques tailored to this specific task. It reviews standard approaches and\nexplores new ones based on polynomial and rational approximation filtering\ncombined with a stochastic procedure."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1308.5625v1", 
    "title": "Shape identification and classification in echolocation", 
    "arxiv-id": "1308.5625v1", 
    "author": "Han Wang", 
    "publish": "2013-08-26T16:16:07Z", 
    "summary": "The paper aims at proposing the first shape identification and classification\nalgorithm in echolocation. The approach is based on first extracting geometric\nfeatures from the reflected waves and then matching them with precomputed ones\nassociated with a dictionary of targets. The construction of such\nfrequency-dependent shape descriptors is based on some important properties of\nthe scattering coefficients and new invariants. The stability and resolution of\nthe proposed identification algorithm with respect to measurement noise and the\nlimited-view aspect are analytically and numerically quantified."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1308.5915v1", 
    "title": "Generalized Perron--Frobenius Theorem for Nonsquare Matrices", 
    "arxiv-id": "1308.5915v1", 
    "author": "David Peleg", 
    "publish": "2013-08-27T16:26:57Z", 
    "summary": "The celebrated Perron--Frobenius (PF) theorem is stated for irreducible\nnonnegative square matrices, and provides a simple characterization of their\neigenvectors and eigenvalues. The importance of this theorem stems from the\nfact that eigenvalue problems on such matrices arise in many fields of science\nand engineering, including dynamical systems theory, economics, statistics and\noptimization. However, many real-life scenarios give rise to nonsquare\nmatrices. A natural question is whether the PF Theorem (along with its\napplications) can be generalized to a nonsquare setting. Our paper provides a\ngeneralization of the PF Theorem to nonsquare matrices. The extension can be\ninterpreted as representing client-server systems with additional degrees of\nfreedom, where each client may choose between multiple servers that can\ncooperate in serving it (while potentially interfering with other clients).\nThis formulation is motivated by applications to power control in wireless\nnetworks, economics and others, all of which extend known examples for the use\nof the original PF Theorem.\n  We show that the option of cooperation between servers does not improve the\nsituation, in the sense that in the optimal solution no cooperation is needed,\nand only one server needs to serve each client. Hence, the additional power of\nhaving several potential servers per client translates into \\emph{choosing} the\nbest single server and not into \\emph{sharing} the load between the servers in\nsome way, as one might have expected.\n  The two main contributions of the paper are (i) a generalized PF Theorem that\ncharacterizes the optimal solution for a non-convex nonsquare problem, and (ii)\nan algorithm for finding the optimal solution in polynomial time."
},{
    "category": "cs.NA", 
    "doi": "10.1137/100813956", 
    "link": "http://arxiv.org/pdf/1309.3719v6", 
    "title": "A New Method for the Analysis of Signals: The Square Wave Transform   (SWT)", 
    "arxiv-id": "1309.3719v6", 
    "author": "Sherry Gapper", 
    "publish": "2013-09-15T03:02:07Z", 
    "summary": "The results obtained by analyzing signals with the Square Wave Method (SWM)\nintroduced previously can be presented in the frequency domain clearly and\nprecisely by using the Square Wave Transform (SWT) described here. As an\nexample, the SWT is used to analyze a sequence of samples (that is, of measured\nvalues) taken from an electroencephalographic recording."
},{
    "category": "cs.NA", 
    "doi": "10.1109/HPCSim.2013.6641458", 
    "link": "http://arxiv.org/pdf/1309.4616v1", 
    "title": "Exponential Integrators on Graphic Processing Units", 
    "arxiv-id": "1309.4616v1", 
    "author": "Alexander Ostermann", 
    "publish": "2013-09-18T11:21:05Z", 
    "summary": "In this paper we revisit stencil methods on GPUs in the context of\nexponential integrators. We further discuss boundary conditions, in the same\ncontext, and show that simple boundary conditions (for example, homogeneous\nDirichlet or homogeneous Neumann boundary conditions) do not affect the\nperformance if implemented directly into the CUDA kernel. In addition, we show\nthat stencil methods with position-dependent coefficients can be implemented\nefficiently as well.\n  As an application, we discuss the implementation of exponential integrators\nfor different classes of problems in a single and multi GPU setup (up to 4\nGPUs). We further show that for stencil based methods such parallelization can\nbe done very efficiently, while for some unstructured matrices the\nparallelization to multiple GPUs is severely limited by the throughput of the\nPCIe bus."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2014.2309076", 
    "link": "http://arxiv.org/pdf/1311.0156v3", 
    "title": "$L_{1/2}$ Regularization: Convergence of Iterative Half Thresholding   Algorithm", 
    "arxiv-id": "1311.0156v3", 
    "author": "Zongben Xu", 
    "publish": "2013-11-01T12:03:25Z", 
    "summary": "In recent studies on sparse modeling, the nonconvex regularization approaches\n(particularly, $L_{q}$ regularization with $q\\in(0,1)$) have been demonstrated\nto possess capability of gaining much benefit in sparsity-inducing and\nefficiency. As compared with the convex regularization approaches (say, $L_{1}$\nregularization), however, the convergence issue of the corresponding algorithms\nare more difficult to tackle. In this paper, we deal with this difficult issue\nfor a specific but typical nonconvex regularization scheme, the $L_{1/2}$\nregularization, which has been successfully used to many applications. More\nspecifically, we study the convergence of the iterative \\textit{half}\nthresholding algorithm (the \\textit{half} algorithm for short), one of the most\nefficient and important algorithms for solution to the $L_{1/2}$\nregularization. As the main result, we show that under certain conditions, the\n\\textit{half} algorithm converges to a local minimizer of the $L_{1/2}$\nregularization, with an eventually linear convergence rate. The established\nresult provides a theoretical guarantee for a wide range of applications of the\n\\textit{half} algorithm. We provide also a set of simulations to support the\ncorrectness of theoretical assertions and compare the time efficiency of the\n\\textit{half} algorithm with other known typical algorithms for $L_{1/2}$\nregularization like the iteratively reweighted least squares (IRLS) algorithm\nand the iteratively reweighted $l_{1}$ minimization (IRL1) algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2014.2309076", 
    "link": "http://arxiv.org/pdf/1311.2661v2", 
    "title": "An Approximate, Efficient Solver for LP Rounding", 
    "arxiv-id": "1311.2661v2", 
    "author": "Stephen J. Wright", 
    "publish": "2013-11-12T02:23:43Z", 
    "summary": "Many problems in machine learning can be solved by rounding the solution of\nan appropriate linear program (LP). This paper shows that we can recover\nsolutions of comparable quality by rounding an approximate LP solution instead\nof the ex- act one. These approximate LP solutions can be computed efficiently\nby applying a parallel stochastic-coordinate-descent method to a\nquadratic-penalty formulation of the LP. We derive worst-case runtime and\nsolution quality guarantees of this scheme using novel perturbation and\nconvergence analysis. Our experiments demonstrate that on such combinatorial\nproblems as vertex cover, independent set and multiway-cut, our approximate\nrounding scheme is up to an order of magnitude faster than Cplex (a commercial\nLP solver) while producing solutions of similar quality."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1311.5202v2", 
    "title": "A fast directional BEM for large-scale acoustic problems based on the   Burton-Miller formulation", 
    "arxiv-id": "1311.5202v2", 
    "author": "Yijun Liu", 
    "publish": "2013-11-17T09:09:32Z", 
    "summary": "In this paper, a highly efficient fast boundary element method (BEM) for\nsolving large-scale engineering acoustic problems in a broad frequency range is\ndeveloped and implemented. The acoustic problems are modeled by the\nBurton-Miller boundary integral equation (BIE), thus the fictitious frequency\nissue is completely avoided. The BIE is discretized by using the Nystr\\\"om\nmethod based on the curved quadratic elements, leading to simple numerical\nimplementation (no edge or corner problems) and high accuracy in the BEM\nanalysis. The linear systems are solved iteratively and accelerated by using a\nnewly developed kernel-independent wideband fast directional algorithm (FDA)\nfor fast summation of oscillatory kernels. In addition, the computational\nefficiency of the FDA is further promoted by exploiting the low-rank features\nof the translation matrices, resulting in two- to three-fold reduction in the\ncomputational time of the multipole-to-local translations. The high accuracy\nand nearly linear computational complexity of the present method are clearly\ndemonstrated by typical examples. An acoustic scattering problem with\ndimensionless wave number $kD$ (where $k$ is the wave number and $D$ is the\ntypical length of the obstacle) up to 1000 and the degrees of freedom up to 4\nmillion is successfully solved within 10 hours on a computer with one core and\nthe memory usage is 24 GB."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1404.6979v1", 
    "title": "An adjustable-width window with good dynamic range", 
    "arxiv-id": "1404.6979v1", 
    "author": "I M Stewart", 
    "publish": "2014-04-28T13:51:35Z", 
    "summary": "A new variable-width window is presented and compared with several other\nwindows, both of variable and fixed widths. The comparison focuses on\nsensitivity and dynamic range. The equivalent noise bandwidth or ENBW (or\nrather, its reciprocal) is used as a proxy for the first; maximum sidelobe\nlevel and high-frequency roll-off in the Fourier transform, for the second. The\nnew window can access any value of ENBW by appropriate choice of the width\nparameter. At any given value of ENBW below about 3, a setting can be found at\nwhich the sidelobes of the window are lower than those of any other in the\nmoderate frequency regime below about 100 cycles."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1406.0080v2", 
    "title": "On the equivalence between low rank matrix completion and tensor rank", 
    "arxiv-id": "1406.0080v2", 
    "author": "Harm Derksen", 
    "publish": "2014-05-31T14:18:34Z", 
    "summary": "The Rank Minimization Problem asks to find a matrix of lowest rank inside a\nlinear variety of the space of n x n matrices. The Low Rank Matrix Completion\nproblem asks to complete a partially filled matrix such that the resulting\nmatrix has smallest possible rank.\n  The Tensor Rank Problem asks to determine the rank of a tensor. We show that\nthese three problems are equivalent: each one of the problems can be reduced to\nthe other two."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1406.1572v1", 
    "title": "Consensus-based In-Network Computation of the PARAFAC Decomposition", 
    "arxiv-id": "1406.1572v1", 
    "author": "Andr\u00e9 L. F. de Almeida", 
    "publish": "2014-06-06T02:23:11Z", 
    "summary": "In this work, we present a new approach for the distributed computation of\nthe PARAFAC decomposition of a third-order tensor across a network of\ncollaborating nodes. We are interested in the case where the overall data\ngathered across the network can be modeled as a data tensor admitting an\nessentially unique PARAFAC decomposition, while each node only observes a\nsub-tensor with not necessarily enough diversity so that identifiability\nconditions are not locally fulfilled at each node. In this situation,\nconventional (centralized) tensor based methods cannot be applied individually\nat each node. By allowing collaboration between neighboring nodes of the\nnetwork, we propose distributed versions of the alternating least squares (ALS)\nand Levenberg-Marquardt (LM) algorithms for the in-network estimation of the\nfactor matrices of a third-order tensor. We assume that one of the factor\nmatrices contains parameters that are local to each node, while the two\nremaining factor matrices contain global parameters that are common to the\nwhole network. The proposed algorithms combine the estimation of the local\nfactors with an in-network computation of the global factors of the PARAFAC\ndecomposition using average consensus over graphs. They emulate their\ncentralized counterparts in the case of ideal data exchange and ideal consensus\ncomputations. The performance of the proposed algorithms are evaluated in both\nideal and imperfect cases."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1406.3461v1", 
    "title": "Clifford Type Algebra Characteristics Investigation", 
    "arxiv-id": "1406.3461v1", 
    "author": "Alina S. Turenko", 
    "publish": "2014-06-13T09:40:39Z", 
    "summary": "The main properties of hypercomplex generalization of quaternion system as\nantiquaternion are presented in this article. Definitions and studied of\nantiquaternions conjugation are introduced, their norm and zero divisor, and\nhow to perform operations on them."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1406.4812v2", 
    "title": "A benchmark generator for boolean quadratic programming", 
    "arxiv-id": "1406.4812v2", 
    "author": "Michael X. Zhou", 
    "publish": "2014-05-27T04:43:25Z", 
    "summary": "For boolean quadratic programming (BQP), we will show that there is no\nduality gap between the primal and dual problems under some conditions by using\nthe classical Lagrangian duality. A benchmark generator is given to create\nrandom BQP problems which can be solved in polynomial time. Several numerical\nexamples are generated to demonstrate the effectiveness of the proposed method."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1408.1237v1", 
    "title": "Preconditioned Krylov solvers for kernel regression", 
    "arxiv-id": "1408.1237v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-08-06T10:39:59Z", 
    "summary": "A primary computational problem in kernel regression is solution of a dense\nlinear system with the $N\\times N$ kernel matrix. Because a direct solution has\nan O($N^3$) cost, iterative Krylov methods are often used with fast\nmatrix-vector products. For poorly conditioned problems, convergence of the\niteration is slow and preconditioning becomes necessary. We investigate\npreconditioning from the viewpoint of scalability and efficiency. The problems\nthat conventional preconditioners face when applied to kernel methods are\ndemonstrated. A \\emph{novel flexible preconditioner }that not only improves\nconvergence but also allows utilization of fast kernel matrix-vector products\nis introduced. The performance of this preconditioner is first illustrated on\nsynthetic data, and subsequently on a suite of test problems in kernel\nregression and geostatistical kriging."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1409.2030v1", 
    "title": "Algorithms and Polynomiography for Solving Quaternion Quadratic   Equations", 
    "arxiv-id": "1409.2030v1", 
    "author": "Bahman Kalantari", 
    "publish": "2014-09-06T16:32:00Z", 
    "summary": "Solving a quadratic equation $P(x)=ax^2+bx+c=0$ with real coefficients is\nknown to middle school students. Solving the equation over the quaternions is\nnot straightforward. Huang and So \\cite{Huang} give a complete set of formulas,\nbreaking it into several cases depending on the coefficients. From a result of\nthe second author in \\cite{kalQ}, zeros of $P(x)$ can be expressed in terms of\nthe zeros of a real quartic equation. This drastically simplifies solving a\nquadratic equation. Here we also consider solving $P(x)=0$ iteratively via\nNewton and Halley methods developed in \\cite{kalQ}. We prove a property of the\nJacobian of Newton and Halley methods and describe several 2D polynomiography\nbased on these methods. The images not only encode the outcome of the iterative\nprocess, but by measuring the time taken to render them we find the relative\nspeed of convergence for the methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1409.2455v1", 
    "title": "Degree reduction of disk rational B\u00e9zier curves", 
    "arxiv-id": "1409.2455v1", 
    "author": "Mao Shi", 
    "publish": "2014-09-08T18:31:29Z", 
    "summary": "This paper presents an algorithm for optimal multi-degree reduction of\nrational disk B\\'ezier curve in the $L^{2}$ norm. We start by introducing a\nnovel disk rational B\\'ezier based on parallel projection, whose properties are\nalso discussed. Then we transform multi-degree reduction of the error radius\ncurve and the weight curve of the disk rational B\\'ezier curve into solving a\nconstrained quadratic programming (QP) problem. Finally, applying weighted\nleast squares, we provide the optimal multi-degree reduced polynomial\napproximation of the center curve of the original disk rational B\\'ezier curve.\nAlso this paper gives error estimation for this algorithm, and shows some\nnumerical examples to illustrate the correctness and the validity of\ntheoretical reasoning."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1501.06318v2", 
    "title": "Simultaneous diagonalization: the asymmetric, low-rank, and noisy   settings", 
    "arxiv-id": "1501.06318v2", 
    "author": "Percy Liang", 
    "publish": "2015-01-26T10:36:03Z", 
    "summary": "Simultaneous matrix diagonalization is used as a subroutine in many machine\nlearning problems, including blind source separation and paramater estimation\nin latent variable models. Here, we extend algorithms for performing joint\ndiagonalization to low-rank and asymmetric matrices, and we also provide\nextensions to the perturbation analysis of these methods. Our results allow\njoint diagonalization to be applied in several new settings."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1007.4518v1", 
    "title": "Piecewise Convex-Concave Approximation in the $\\ell_{\\infty}$ Norm", 
    "arxiv-id": "1007.4518v1", 
    "author": "M. P. Cullinan", 
    "publish": "2010-07-26T17:50:16Z", 
    "summary": "Suppose that $\\ff \\in \\reals^{n}$ is a vector of $n$ error-contaminated\nmeasurements of $n$ smooth values measured at distinct and strictly ascending\nabscissae. The following projective technique is proposed for obtaining a\nvector of smooth approximations to these values. Find \\yy\\ minimizing $\\| \\yy -\n\\ff \\|_{\\infty}$ subject to the constraints that the second order consecutive\ndivided differences of the components of \\yy\\ change sign at most $q$ times.\nThis optimization problem (which is also of general geometrical interest) does\nnot suffer from the disadvantage of the existence of purely local minima and\nallows a solution to be constructed in $O(nq)$ operations. A new algorithm for\ndoing this is developed and its effectiveness is proved. Some of the results of\napplying it to undulating and peaky data are presented, showing that it is\neconomical and can give very good results, particularly for large\ndensely-packed data, even when the errors are quite large."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2014.07.006", 
    "link": "http://arxiv.org/pdf/1109.1027v2", 
    "title": "A Two-Step High-Order Compact Scheme for the Laplacian Operator and its   Implementation in an Explicit Method for Integrating the Nonlinear   Schr\u007f\u00f6dinger Equation", 
    "arxiv-id": "1109.1027v2", 
    "author": "R. Carretero", 
    "publish": "2011-09-05T22:38:05Z", 
    "summary": "We describe and test an easy-to-implement two-step high-order compact (2SHOC)\nscheme for the Laplacian operator and its implementation into an explicit\nfinite-difference scheme for simulating the nonlinear Schr\u007f\\\"odinger equation\n(NLSE). Our method relies on a compact `double-differencing' which is shown to\nbe computationally equivalent to standard fourth-order non-compact schemes.\nThrough numerical simulations of the NLSE using fourth-order Runge-Kutta, we\nconfirm that our scheme shows the desired fourth-order accuracy. A computation\nand storage requirement comparison is made between the 2SHOC scheme and the\nnon-compact equivalent scheme for both the Laplacian operator alone, as well as\nwhen implemented in the NLSE simulations. Stability bounds are also shown in\norder to get maximum efficiency out of the method. We conclude that the modest\nincrease in storage and computation of the 2SHOC schemes are well worth the\nadvantages of having the schemes compact, and their ease of implementation\nmakes their use very useful for practical implementations."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.2884", 
    "link": "http://arxiv.org/pdf/1109.5913v1", 
    "title": "On the control of the load increments for a proper description of   multiple delamination in a domain decomposition framework", 
    "arxiv-id": "1109.5913v1", 
    "author": "Pierre Gosselet", 
    "publish": "2011-09-27T14:47:03Z", 
    "summary": "In quasi-static nonlinear time-dependent analysis, the choice of the time\ndiscretization is a complex issue. The most basic strategy consists in\ndetermining a value of the load increment that ensures the convergence of the\nsolution with respect to time on the base of preliminary simulations. In more\nadvanced applications, the load increments can be controlled for instance by\nprescribing the number of iterations of the nonlinear resolution procedure, or\nby using an arc-length algorithm. These techniques usually introduce a\nparameter whose correct value is not easy to obtain. In this paper, an\nalternative procedure is proposed. It is based on the continuous control of the\nresidual of the reference problem over time, whose measure is easy to\ninterpret. This idea is applied in the framework of a multiscale domain\ndecomposition strategy in order to perform 3D delamination analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.2884", 
    "link": "http://arxiv.org/pdf/1207.1773v1", 
    "title": "A hybrid Hermitian general eigenvalue solver", 
    "arxiv-id": "1207.1773v1", 
    "author": "Jack Dongarra", 
    "publish": "2012-07-07T07:52:48Z", 
    "summary": "The adoption of hybrid GPU-CPU nodes in traditional supercomputing platforms\nopens acceleration opportunities for electronic structure calculations in\nmaterials science and chemistry applications, where medium sized Hermitian\ngeneralized eigenvalue problems must be solved many times. The small size of\nthe problems limits the scalability on a distributed memory system, hence they\ncan benefit from the massive computational performance concentrated on a single\nnode, hybrid GPU-CPU system. However, new algorithms that efficiently exploit\nheterogeneity and massive parallelism of not just GPUs, but of multi/many-core\nCPUs as well are required. Addressing these demands, we implemented a novel\nHermitian general eigensolver algorithm. This algorithm is based on a standard\neigenvalue solver, and existing algorithms can be used. The resulting\neigensolvers are state-of-the-art in HPC, significantly outperforming existing\nlibraries. We analyze their performance impact on applications of interest,\nwhen different fractions of eigenvectors are needed by the host electronic\nstructure code."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.2884", 
    "link": "http://arxiv.org/pdf/1301.0581v1", 
    "title": "General Lower Bounds based on Computer Generated Higher Order Expansions", 
    "arxiv-id": "1301.0581v1", 
    "author": "Hilbert Kappen", 
    "publish": "2012-12-12T15:57:07Z", 
    "summary": "In this article we show the rough outline of a computer algorithm to generate\nlower bounds on the exponential function of (in principle) arbitrary precision.\nWe implemented this to generate all necessary analytic terms for the Boltzmann\nmachine partition function thus leading to lower bounds of any order. It turns\nout that the extra variational parameters can be optimized analytically. We\nshow that bounds upto nineth order are still reasonably calculable in practical\nsituations. The generated terms can also be used as extra correction terms\n(beyond TAP) in mean field expansions."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.2884", 
    "link": "http://arxiv.org/pdf/1301.2481v1", 
    "title": "Zur iterativen Loesosung von linearen Gleichungssystemen", 
    "arxiv-id": "1301.2481v1", 
    "author": "Sebstian Karl", 
    "publish": "2013-01-11T12:53:14Z", 
    "summary": "It is well known that a fixed point iteration for solving a linear equation\nsystem converges if and only if the spectral radius of the iteration matrix is\nless than one. A method is presented which guarantees the Fixed Point, even if\nthis condition is not (\"spectral radius <1\") fulfilled and demonstrated through\ncalculation examples."
},{
    "category": "cs.NA", 
    "doi": "10.1137/110842685", 
    "link": "http://arxiv.org/pdf/1301.5412v2", 
    "title": "A2ILU: Auto-accelerated ILU Preconditioner for Sparse Linear Systems", 
    "arxiv-id": "1301.5412v2", 
    "author": "Teruyoshi Washizawa", 
    "publish": "2013-01-23T06:34:44Z", 
    "summary": "The ILU-based preconditioning methods in previous work have their own\nparameters to improve their performances. Although the parameters may degrade\nthe performance, their determination is left to users. Thus, these previous\nmethods are not reliable in practical computer-aided engineering use. This\npaper proposes a novel ILU-based preconditioner called the auto-accelerated\nILU, or A2ILU. In order to improve the convergence, A2ILU introduces\nacceleration parameters which modify the ILU factorized preconditioning matrix.\nA$^2$ILU needs no more operations than the original ILU because the\nacceleration parameters are optimized automatically by A2ILU itself. Numerical\ntests reveal the performance of A2ILU is superior to previous ILU-based methods\nwith manually optimized parameters. The numerical tests also demonstrate the\nability to apply auto-acceleration to ILU-based methods to improve their\nperformances and robustness of parameter sensitivities."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1301.5435v3", 
    "title": "On the $\\mathbb{F}_2$-linear relations of Mersenne Twister pseudorandom   number generators", 
    "arxiv-id": "1301.5435v3", 
    "author": "Shin Harase", 
    "publish": "2013-01-23T08:53:13Z", 
    "summary": "Sequence generators obtained by linear recursions over the two-element field\n$\\mathbb{F}_2$, i.e., $\\mathbb{F}_2$-linear generators, are widely used as\npseudorandom number generators. For example, the Mersenne Twister MT19937 is\none of the most successful applications. An advantage of such generators is\nthat we can assess them quickly by using theoretical criteria, such as the\ndimension of equidistribution with $v$-bit accuracy. To compute these\ndimensions, several polynomial-time lattice reduction algorithms have been\nproposed in the case of $\\mathbb{F}_2$-linear generators.\n  In this paper, in order to assess non-random bit patterns in dimensions that\nare higher than the dimension of equidistribution with $v$-bit accuracy,we\nfocus on the relationship between points in the Couture--L'Ecuyer dual lattices\nand $\\mathbb{F}_2$-linear relations on the most significant $v$ bits of output\nsequences, and consider a new figure of merit $N_v$ based on the minimum weight\nof $\\mathbb{F}_2$-linear relations whose degrees are minimal for $v$. Next, we\nnumerically show that MT19937 has low-weight $\\mathbb{F}_2$-linear relations in\ndimensions higher than 623, and show that some output vectors with specific\nlags are rejected or have small $p$-values in the birthday spacings tests. We\nalso report that some variants of Mersenne Twister, such as WELL generators,\nare significantly improved from the perspective of $N_v$."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1304.4292v1", 
    "title": "Relative error due to a single bit-flip in floating-point arithmetic", 
    "arxiv-id": "1304.4292v1", 
    "author": "Bradley R. Lowery", 
    "publish": "2013-04-15T23:24:46Z", 
    "summary": "We consider the error due to a single bit-flip in a floating point number. We\nassume IEEE 754 double precision arithmetic, which encodes binary floating\npoint numbers in a 64-bit word. We assume that the bit-flip happens randomly so\nit has equi-probability (1/64) to hit any of the 64 bits. Since we want to\nmitigate the assumption on our initial floating-point number, we assume that it\nis uniformly picked among all normalized number. With this framework, we can\nsummarize our findings as follows. The probability for a single bit flip to\ncause a relative error less than 10^-11 in a normalized floating-point number\nis above 25%; The probability for a single bit flip to cause a relative error\nless than 10^-6 in a normalized floating-point number is above 50%; Etc."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1304.4738v1", 
    "title": "Computing Enclosures of Overdetermined Interval Linear Systems", 
    "arxiv-id": "1304.4738v1", 
    "author": "Milan Hlad\u00edk", 
    "publish": "2013-04-17T09:04:39Z", 
    "summary": "This work considers special types of interval linear systems - overdetermined\nsystems. Simply said these systems have more equations than variables. The\nsolution set of an interval linear system is a collection of all solutions of\nall instances of an interval system. By the instance we mean a point real\nsystem that emerges when we independently choose a real number from each\ninterval coefficient of the interval system. Enclosing the solution set of\nthese systems is in some ways more difficult than for square systems. The main\ngoal of this work is to present various methods for solving overdetermined\ninterval linear systems. We would like to present them in an understandable way\neven for nonspecialists in a field of linear systems. The second goal is a\nnumerical comparison of all the methods on random interval linear systems\nregarding widths of enclosures, computation times and other special properties\nof methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1305.0203v1", 
    "title": "Matrix Compression using the Nystro\u00f6m Method", 
    "arxiv-id": "1305.0203v1", 
    "author": "Alon Schclar", 
    "publish": "2013-05-01T15:24:26Z", 
    "summary": "The Nystr\\\"{o}m method is routinely used for out-of-sample extension of\nkernel matrices. We describe how this method can be applied to find the\nsingular value decomposition (SVD) of general matrices and the eigenvalue\ndecomposition (EVD) of square matrices. We take as an input a matrix $M\\in\n\\mathbb{R}^{m\\times n}$, a user defined integer $s\\leq min(m,n)$ and $A_M \\in\n\\mathbb{R}^{s\\times s}$, a matrix sampled from the columns and rows of $M$.\nThese are used to construct an approximate rank-$s$ SVD of $M$ in\n$O\\left(s^2\\left(m+n\\right)\\right)$ operations. If $M$ is square, the rank-$s$\nEVD can be similarly constructed in $O\\left(s^2 n\\right)$ operations. Thus, the\nmatrix $A_M$ is a compressed version of $M$. We discuss the choice of $A_M$ and\npropose an algorithm that selects a good initial sample for a pivoted version\nof $M$. The proposed algorithm performs well for general matrices and kernel\nmatrices whose spectra exhibit fast decay."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1305.1059v1", 
    "title": "Subsquares Approach - Simple Scheme for Solving Overdetermined Interval   Linear Systems", 
    "arxiv-id": "1305.1059v1", 
    "author": "Milan Hlad\u00edk", 
    "publish": "2013-05-05T22:13:57Z", 
    "summary": "In this work we present a new simple but efficient scheme - Subsquares\napproach - for development of algorithms for enclosing the solution set of\noverdetermined interval linear systems. We are going to show two algorithms\nbased on this scheme and discuss their features. We start with a simple\nalgorithm as a motivation, then we continue with a sequential algorithm. Both\nalgorithms can be easily parallelized. The features of both algorithms will be\ndiscussed and numerically tested."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1305.2135v2", 
    "title": "Almost-commuting matrices are almost jointly diagonalizable", 
    "arxiv-id": "1305.2135v2", 
    "author": "Michael M. Bronstein", 
    "publish": "2013-05-07T11:09:11Z", 
    "summary": "We study the relation between approximate joint diagonalization of\nself-adjoint matrices and the norm of their commutator, and show that almost\ncommuting self-adjoint matrices are almost jointly diagonalizable by a unitary\nmatrix."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1306.0103v1", 
    "title": "Dominant Frequency Extraction", 
    "arxiv-id": "1306.0103v1", 
    "author": "Rastislav Telgarsky", 
    "publish": "2013-06-01T12:55:26Z", 
    "summary": "Time series are collected and studied extensively for the knowledge about the\ndata source characteristics such as the trend or the spectral landscape. Some\npeaks in the spectral landscape correspond to dominant frequencies. The\napproach here is empirical: all time series are discrete and finite. Contents:\nIntroduction. 1 Examples of periodic phenomena. 2 Algorithms and libraries. 3\nTime series analysis. 4 Dominant frequency in ladar data. Conclusion.\nReferences."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1306.5216v1", 
    "title": "Modification to Darcy model for high pressure and high velocity   applications and associated mixed finite element formulations", 
    "arxiv-id": "1306.5216v1", 
    "author": "K. B. Nakshatrala", 
    "publish": "2013-06-21T19:15:57Z", 
    "summary": "The Darcy model is based on a plethora of assumptions. One of the most\nimportant assumptions is that the Darcy model assumes the drag coefficient to\nbe constant. However, there is irrefutable experimental evidence that\nviscosities of organic liquids and carbon-dioxide depend on the pressure.\nExperiments have also shown that the drag varies nonlinearly with respect to\nthe velocity at high flow rates. In important technological applications like\nenhanced oil recovery and geological carbon-dioxide sequestration, one\nencounters both high pressures and high flow rates. It should be emphasized\nthat flow characteristics and pressure variation under varying drag are both\nquantitatively and qualitatively different from that of constant drag.\nMotivated by experimental evidence, we consider the drag coefficient to depend\non both the pressure and velocity. We consider two major modifications to the\nDarcy model based on the Barus formula and Forchheimer approximation. The\nproposed modifications to the Darcy model result in nonlinear partial\ndifferential equations, which are not amenable to analytical solutions. To this\nend, we present mixed finite element formulations based on least-squares\nformalism and variational multiscale formalism for the resulting governing\nequations. The proposed modifications to the Darcy model and its associated\nfinite element formulations are used to solve realistic problems with relevance\nto enhanced oil recovery. We also study the competition between the nonlinear\ndependence of drag on the velocity and the dependence of viscosity on the\npressure. To the best of the authors' knowledge such a systematic study has not\nbeen performed."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.3564v1", 
    "title": "Pursuit Fractal Analysis of Time-Series Data", 
    "arxiv-id": "1310.3564v1", 
    "author": "Shuya Kanagawa", 
    "publish": "2013-10-14T04:57:34Z", 
    "summary": "In this study, we present a method to measure changes over time of fractal\ndimension. We confirmed that our method can calculate the fractal dimension\nwith the same precision as conventional methods, and tracking performance of\nour method is higher than that of the conventional methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.4046v1", 
    "title": "Explicit schemes for parabolic and hyperbolic equations", 
    "arxiv-id": "1310.4046v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2013-10-15T13:20:52Z", 
    "summary": "Standard explicit schemes for parabolic equations are not very convenient for\ncomputing practice due to the fact that they have strong restrictions on a time\nstep. More promising explicit schemes are associated with explicit-implicit\nsplitting of the problem operator (Saul'yev asymmetric schemes, explicit\nalternating direction (ADE) schemes, group explicit method). These schemes\nbelong to the class of unconditionally stable schemes, but they demonstrate bad\napproximation properties. These explicit schemes are treated as schemes of the\nalternating triangle method and can be considered as factorized schemes where\nthe problem operator is splitted into the sum of two operators that are adjoint\nto each other. Here we propose a multilevel modification of the alternating\ntriangle method, which demonstrates better properties in terms of accuracy. We\nalso consider explicit schemes of the alternating triangle method for the\nnumerical solution of boundary value problems for hyperbolic equations of\nsecond order. The study is based on the general theory of stability\n(well-posedness) for operator-difference schemes."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.7697v6", 
    "title": "Linear Convergence of Comparison-based Step-size Adaptive Randomized   Search via Stability of Markov Chains", 
    "arxiv-id": "1310.7697v6", 
    "author": "Nikolaus Hansen", 
    "publish": "2013-10-29T06:54:29Z", 
    "summary": "In this paper, we consider comparison-based adaptive stochastic algorithms\nfor solving numerical optimisation problems. We consider a specific subclass of\nalgorithms that we call comparison-based step-size adaptive randomized search\n(CB-SARS), where the state variables at a given iteration are a vector of the\nsearch space and a positive parameter, the step-size, typically controlling the\noverall standard deviation of the underlying search distribution.We investigate\nthe linear convergence of CB-SARS on\\emph{scaling-invariant} objective\nfunctions. Scaling-invariantfunctions preserve the ordering of points with\nrespect to their functionvalue when the points are scaled with the same\npositive parameter (thescaling is done w.r.t. a fixed reference point). This\nclass offunctions includes norms composed with strictly increasing functions\naswell as many non quasi-convex and non-continuousfunctions. On\nscaling-invariant functions, we show the existence of ahomogeneous Markov\nchain, as a consequence of natural invarianceproperties of CB-SARS (essentially\nscale-invariance and invariance tostrictly increasing transformation of the\nobjective function). We thenderive sufficient conditions for \\emph{global\nlinear convergence} ofCB-SARS, expressed in terms of different stability\nconditions of thenormalised homogeneous Markov chain (irreducibility,\npositivity, Harrisrecurrence, geometric ergodicity) and thus define a general\nmethodologyfor proving global linear convergence of CB-SARS algorithms\nonscaling-invariant functions. As a by-product we provide aconnexion between\ncomparison-based adaptive stochasticalgorithms and Markov chain Monte Carlo\nalgorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.8121v6", 
    "title": "Easy Accurate Reading and Writing of Floating-Point Numbers", 
    "arxiv-id": "1310.8121v6", 
    "author": "Aubrey Jaffer", 
    "publish": "2013-10-28T00:01:26Z", 
    "summary": "Presented here are algorithms for converting between (decimal)\nscientific-notation and (binary) IEEE-754 double-precision floating-point\nnumbers. By employing a rounding integer quotient operation these algorithms\nare much simpler than those previously published. The values are stable under\nrepeated conversions between the formats. Unlike Java-1.6, the scientific\nrepresentations generated use only the minimum number of mantissa digits needed\nto convert back to the original binary values.\n  Implemented in Java these algorithms execute as fast or faster than Java's\nnative conversions over nearly all of the IEEE-754 double-precision range."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.8397v1", 
    "title": "Linear Convergence on Positively Homogeneous Functions of a Comparison   Based Step-Size Adaptive Randomized Search: the (1+1) ES with Generalized   One-fifth Success Rule", 
    "arxiv-id": "1310.8397v1", 
    "author": "Nikolaus Hansen", 
    "publish": "2013-10-31T06:38:56Z", 
    "summary": "In the context of unconstraint numerical optimization, this paper\ninvestigates the global linear convergence of a simple probabilistic\nderivative-free optimization algorithm (DFO). The algorithm samples a candidate\nsolution from a standard multivariate normal distribution scaled by a step-size\nand centered in the current solution. This solution is accepted if it has a\nbetter objective function value than the current one. Crucial to the algorithm\nis the adaptation of the step-size that is done in order to maintain a certain\nprobability of success. The algorithm, already proposed in the 60's, is a\ngeneralization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (ES)\nwith one-fifth success rule which was also proposed by Devroye under the name\ncompound random search or by Schumer and Steiglitz under the name step-size\nadaptive random search. In addition to be derivative-free, the algorithm is\nfunction-value-free: it exploits the objective function only through\ncomparisons. It belongs to the class of comparison-based step-size adaptive\nrandomized search (CB-SARS). For the convergence analysis, we follow the\nmethodology developed in a companion paper for investigating linear convergence\nof CB-SARS: by exploiting invariance properties of the algorithm, we turn the\nstudy of global linear convergence on scaling-invariant functions into the\nstudy of the stability of an underlying normalized Markov chain (MC). We hence\nprove global linear convergence by studying the stability (irreducibility,\nrecurrence, positivity, geometric ergodicity) of the normalized MC associated\nto the $(1+1)$-ES. More precisely, we prove that starting from any initial\nsolution and any step-size, linear convergence with probability one and in\nexpectation occurs. Our proof holds on unimodal functions that are the\ncomposite of strictly increasing functions by positively homogeneous functions\nwith degree $\\alpha$ (assumed also to be continuously differentiable). This\nfunction class includes composite of norm functions but also non-quasi convex\nfunctions. Because of the composition by a strictly increasing function, it\nincludes non continuous functions. We find that a sufficient condition for\nglobal linear convergence is the step-size increase on linear functions, a\ncondition typically satisfied for standard parameter choices. While introduced\nmore than 40 years ago, we provide here the first proof of global linear\nconvergence for the $(1+1)$-ES with generalized one-fifth success rule and the\nfirst proof of linear convergence for a CB-SARS on such a class of functions\nthat includes non-quasi convex and non-continuous functions. Our proof also\nholds on functions where linear convergence of some CB-SARS was previously\nproven, namely convex-quadratic functions (including the well-know sphere\nfunction)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1310.8573v2", 
    "title": "An optimally concentrated Gabor transform for localized time-frequency   components", 
    "arxiv-id": "1310.8573v2", 
    "author": "Darian Onchis", 
    "publish": "2013-10-31T16:25:25Z", 
    "summary": "Gabor analysis is one of the most common instances of time-frequency signal\nanalysis. Choosing a suitable window for the Gabor transform of a signal is\noften a challenge for practical applications, in particular in audio signal\nprocessing. Many time-frequency (TF) patterns of different shapes may be\npresent in a signal and they can not all be sparsely represented in the same\nspectrogram. We propose several algorithms, which provide optimal windows for a\nuser-selected TF pattern with respect to different concentration criteria. We\nbase our optimization algorithm on $l^p$-norms as measure of TF spreading. For\na given number of sampling points in the TF plane we also propose optimal\nlattices to be used with the obtained windows. We illustrate the potentiality\nof the method on selected numerical examples."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1312.3618v1", 
    "title": "Randomness of D Sequences via Diehard Testing", 
    "arxiv-id": "1312.3618v1", 
    "author": "James Bellamy", 
    "publish": "2013-12-12T20:40:13Z", 
    "summary": "This paper presents a comparison of the quality of randomness of D sequences\nbased on diehard tests. Since D sequences can model any random sequence, this\ncomparison is of value beyond this specific class."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1312.7637v1", 
    "title": "Insight into Primal Augmented Lagrangian Multilplier Method", 
    "arxiv-id": "1312.7637v1", 
    "author": "K P Soman", 
    "publish": "2013-12-30T06:09:28Z", 
    "summary": "We provide a simplified form of Primal Augmented Lagrange Multiplier\nalgorithm. We intend to fill the gap in the steps involved in the mathematical\nderivations of the algorithm so that an insight into the algorithm is made. The\nexperiment is focused to show the reconstruction done using this algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.0190v1", 
    "title": "The DFLU flux for systems of conservation laws", 
    "arxiv-id": "1401.0190v1", 
    "author": "J\u00e9r\u00f4me Jaffr\u00e9", 
    "publish": "2013-12-31T17:09:32Z", 
    "summary": "The DFLU numerical flux was introduced in order to solve hyperbolic scalar\nconservation laws with a flux function discontinuous in space. We show how this\nflux can be used to solve certain class of systems of conservation laws such as\nsystems modeling polymer flooding in oil reservoir engineering. Furthermore,\nthese results are extended to the case where the flux function is discontinuous\nin the space variable. Such a situation arises for example while dealing with\noil reservoirs which are heterogeneous. Numerical experiments are presented to\nillustrate the efficiency of this new scheme compared to other standard schemes\nlike upstream mobility, Lax-Friedrichs and Force schemes."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.0193v1", 
    "title": "Mathematical analysis of a discrete fracture model coupling Darcy flow   in the matrix with Darcy-Forchheimer flow in the fracture", 
    "arxiv-id": "1401.0193v1", 
    "author": "Jean Roberts", 
    "publish": "2013-12-31T17:13:44Z", 
    "summary": "We consider a model for flow in a porous medium with a fracture in which the\nflow in the fracture is governed by the Darcy-Forchheimer law while that in the\nsurrounding matrix is governed by Darcy's law. We give an appropriate mixed,\nvariational formulation and show existence and uniqueness of the solution. To\nshow existence we give an analogous formulation for the model in which the\nDarcy-Forchheimer law is the governing equation throughout the domain. We show\nexistence and uniqueness of the solution and show that the solution for the\nmodel with Darcy's law in the matrix is the weak limit of solutions of the\nmodel with the Darcy-Forchheimer law in the entire domain when the Forchheimer\ncoefficient in the matrix tends toward zero."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.0248v2", 
    "title": "Twofold fast summation", 
    "arxiv-id": "1401.0248v2", 
    "author": "Evgeny Latkin", 
    "publish": "2014-01-01T04:25:16Z", 
    "summary": "Debugging accumulation of floating-point errors is hard; ideally, computer\nshould track it automatically. Here we consider twofold approximation of an\nexact real with value + error pair of floating-point numbers. Normally, value +\nerror sum is more accurate than value alone, so error can estimate deviation\nbetween value and its exact target. Fast summation algorithm, that provides\ntwofold sum of x[1]+...+x[N] or dot product x[1]*y[1]+...+x[N]*y[N], can be\nsame fast as direct summation sometimes if leveraging processor underused\npotential. This way, we can hit three goals: improve precision, track\ninaccuracy, and do this with little if any loss in performance."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.0798v2", 
    "title": "Domain decomposition methods with overlapping subdomains for   time-dependent problems", 
    "arxiv-id": "1401.0798v2", 
    "author": "Petr Zakharov", 
    "publish": "2014-01-04T11:06:43Z", 
    "summary": "Domain decomposition (DD) methods for solving time-dependent problems can be\nclassified by (i) the method of domain decomposition used, (ii) the choice of\ndecomposition operators (exchange of boundary conditions), and (iii) the\nsplitting scheme employed. To construct homogeneous numerical algorithms,\noverlapping subdomain methods are preferable. Domain decomposition is\nassociated with the corresponding additive representation of the problem\noperator. To solve time-dependent problems with the DD splitting, different\noperator-splitting schemes are used. Various variants of decomposition\noperators differ by distinct types of data exchanges on interfaces. They ensure\nthe convergence of the approximate solution in various spaces of grid\nfunctions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.6235v6", 
    "title": "Twofold fast arithmetic", 
    "arxiv-id": "1401.6235v6", 
    "author": "Evgeny Latkin", 
    "publish": "2014-01-24T01:33:54Z", 
    "summary": "Can we assure math computations by automatic verifying floating-point\naccuracy? We define fast arithmetic (based on Dekker [1971]) over twofold\napproximations $z\\approx z_0+z_1$, such that $z_0$ is standard result and $z_1$\nassesses inaccuracy $\\Delta z_0=z-z_0$. We propose on-fly tracking $z_1$,\ndetecting if $\\Delta z_0$ appears too high. We believe permanent tracking is\nworth its cost. C++ test code for Intel AVX available via web."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1401.7129v1", 
    "title": "Towards a Resolution of P = NP Conjecture", 
    "arxiv-id": "1401.7129v1", 
    "author": "Garimella Rama Murthy", 
    "publish": "2014-01-28T10:36:04Z", 
    "summary": "In this research paper, the problem of optimization of a quadratic form over\nthe convex hull generated by the corners of hypercube is attempted and solved.\nIt is reasoned that under some conditions, the optimum occurs at the corners of\nhypercube. Results related to the computation of global optimum stable state\n(an NP hard problem) are discussed. An algorithm is proposed. It is hoped that\nthe results shed light on resolving the P not equal to NP problem."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2014.02.002", 
    "link": "http://arxiv.org/pdf/1403.2273v1", 
    "title": "Some isomorphic classes for noncanonical hypercomplex number systems of   dimension 2", 
    "arxiv-id": "1403.2273v1", 
    "author": "Iana V. Khitsko", 
    "publish": "2014-03-07T19:13:06Z", 
    "summary": "Building of some isomorphic classes for noncanonical hypercomplex number\nsystems o dimension 2 is described. In general case, such systems with specific\nconstraints to structural constants can be isomorphic to complex, dual or\ndouble number system. Isomorphic transition between noncanonical hypercomplex\nnumber systems of the general form and diagonal form is built."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSP.2013.2297439", 
    "link": "http://arxiv.org/pdf/1403.4462v1", 
    "title": "Tensor Decompositions for Signal Processing Applications From Two-way to   Multiway Component Analysis", 
    "arxiv-id": "1403.4462v1", 
    "author": "L. De Lathauwer", 
    "publish": "2014-03-17T11:03:58Z", 
    "summary": "The widespread use of multi-sensor technology and the emergence of big\ndatasets has highlighted the limitations of standard flat-view matrix models\nand the necessity to move towards more versatile data analysis tools. We show\nthat higher-order tensors (i.e., multiway arrays) enable such a fundamental\nparadigm shift towards models that are essentially polynomial and whose\nuniqueness, unlike the matrix methods, is guaranteed under verymild and natural\nconditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical\nbackbone, data analysis techniques using tensor decompositions are shown to\nhave great flexibility in the choice of constraints that match data properties,\nand to find more general latent components in the data than matrix-based\nmethods. A comprehensive introduction to tensor decompositions is provided from\na signal processing perspective, starting from the algebraic foundations, via\nbasic Canonical Polyadic and Tucker models, through to advanced cause-effect\nand multi-view data analysis schemes. We show that tensor decompositions enable\nnatural generalizations of some commonly used signal processing paradigms, such\nas canonical correlation and subspace techniques, signal separation, linear\nregression, feature extraction and classification. We also cover computational\naspects, and point out how ideas from compressed sensing and scientific\ncomputing may be used for addressing the otherwise unmanageable storage and\nmanipulation problems associated with big datasets. The concepts are supported\nby illustrative real world case studies illuminating the benefits of the tensor\nframework, as efficient and promising tools for modern signal processing, data\nanalysis and machine learning applications; these benefits also extend to\nvector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker\ndecomposition, HOSVD, tensor networks, Tensor Train."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSP.2013.2297439", 
    "link": "http://arxiv.org/pdf/1403.4747v1", 
    "title": "A fast directional boundary element method for high frequency acoustic   problems in three dimensions", 
    "arxiv-id": "1403.4747v1", 
    "author": "Jinyou Xiao", 
    "publish": "2014-03-19T09:37:21Z", 
    "summary": "A highly efficient fast boundary element method (BEM) for solving large-scale\nengineering acoustic problems in a broad frequency range is developed and\nimplemented. The acoustic problems are modeled by the Burton-Miller boundary\nintegral equation (BIE), thus the fictitious frequency issue is completely\navoided. The BIE is discretized by using the collocation method with piecewise\nconstant elements. The linear systems are solved iteratively and accelerated by\nusing a newly developed kernel-independent wideband fast directional algorithm\n(FDA) for fast summation of oscillatory kernels. In addition, the computational\nefficiency of the FDA is further promoted by exploiting the low-rank features\nof the translation matrices. The high accuracy and nearly linear computational\ncomplexity of the present method are clearly demonstrated by typical examples.\nAn acoustic scattering problem with dimensionless wave number $kD$ (where $k$\nis the wave number and $D$ is the typical length of the obstacle) up to 1000\nand the degrees of freedom up to 4 million is successfully solved within 4\nhours on a computer with one core and the memory usage is 24.7 GB."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSP.2013.2297439", 
    "link": "http://arxiv.org/pdf/1403.7458v7", 
    "title": "Solvers for $\\mathcal{O} (N)$ Electronic Structure in the Strong Scaling   Limit", 
    "arxiv-id": "1403.7458v7", 
    "author": "Laxmikant V. Kal\u00e9", 
    "publish": "2014-03-28T17:37:13Z", 
    "summary": "We present a hybrid OpenMP/Charm++ framework for solving the $\\mathcal{O}\n(N)$ Self-Consistent-Field eigenvalue problem with parallelism in the strong\nscaling regime, $P\\gg{N}$, where $P$ is the number of cores, and $N$ a measure\nof system size, i.e. the number of matrix rows/columns, basis functions, atoms,\nmolecules, etc. This result is achieved with a nested approach to Spectral\nProjection and the Sparse Approximate Matrix Multiply [Bock and Challacombe,\nSIAM J.~Sci.~Comput. 35 C72, 2013], and involves a recursive, task-parallel\nalgorithm, often employed by generalized $N$-Body solvers, to occlusion and\nculling of negligible products in the case of matrices with decay. Employing\nclassic technologies associated with generalized $N$-Body solvers, including\nover-decomposition, recursive task parallelism, orderings that preserve\nlocality, and persistence-based load balancing, we obtain scaling beyond\nhundreds of cores per molecule for small water clusters ([H${}_2$O]${}_N$, $N\n\\in \\{ 30, 90, 150 \\}$, $P/N \\approx \\{ 819, 273, 164 \\}$) and find support for\nan increasingly strong scalability with increasing system size $N$."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSP.2013.2297439", 
    "link": "http://arxiv.org/pdf/1403.7692v2", 
    "title": "A Derivative-Free Trust Region Framework for Variational Data   Assimilation", 
    "arxiv-id": "1403.7692v2", 
    "author": "Adrian Sandu", 
    "publish": "2014-03-30T02:24:49Z", 
    "summary": "This study develops a hybrid ensemble-variational approach for solving data\nassimilation problems. The method, called TR-4D-EnKF, is based on a trust\nregion framework and consists of three computational steps. First an ensemble\nof model runs is propagated forward in time and snapshots of the state are\nstored. Next, a sequence of basis vectors is built and a low-dimensional\nrepresentation of the data assimilation system is obtained by projecting the\nmodel state onto the space spanned by the ensemble deviations from the mean.\nFinally, the low-dimensional optimization problem is solved in the\nreduced-space using a trust region approach; the size of the trust region is\nupdated according to the relative decrease of the reduced order surrogate cost\nfunction. The analysis state is projected back onto the full space, and the\nprocess is repeated with the current analysis serving as a new background. A\nheuristic approach based on the trust region size is proposed in order to\nadjust the background error statistics from one iteration to the next.\nExperimental simulations are carried out using the Lorenz and the\nquasi-geostrophic models. The results show that TR-4D-EnKF is an efficient\ncomputational approach, and is more accurate than the current state of the art\n4D-EnKF implementations such as the POD-4D-EnKF and the Iterative Subspace\nMinimization methods."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MSP.2013.2297439", 
    "link": "http://arxiv.org/pdf/1403.7698v1", 
    "title": "Recursive computation of spherical harmonic rotation coefficients of   large degree", 
    "arxiv-id": "1403.7698v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-03-30T04:13:46Z", 
    "summary": "Computation of the spherical harmonic rotation coefficients or elements of\nWigner's d-matrix is important in a number of quantum mechanics and\nmathematical physics applications. Particularly, this is important for the Fast\nMultipole Methods in three dimensions for the Helmholtz, Laplace and related\nequations, if rotation-based decomposition of translation operators are used.\nIn these and related problems related to representation of functions on a\nsphere via spherical harmonic expansions computation of the rotation\ncoefficients of large degree $n$ (of the order of thousands and more) may be\nnecessary. Existing algorithms for their computation, based on recursions, are\nusually unstable, and do not extend to $n$. We develop a new recursion and\nstudy its behavior for large degrees, via computational and asymptotic\nanalyses. Stability of this recursion was studied based on a novel application\nof the Courant-Friedrichs-Lewy condition and the von Neumann method for\nstability of finite-difference schemes for solution of PDEs. A recursive\nalgorithm of minimal complexity $O\\left(n^{2}\\right)$ for degree $n$ and\nFFT-based algorithms of complexity $O\\left(n^{2}\\log n\\right) $ suitable for\ncomputation of rotation coefficients of large degrees are proposed, studied\nnumerically, and cross-validated. It is shown that the latter algorithm can be\nused for $n\\lesssim 10^{3}$ in double precision, while the former algorithm was\ntested for large $n$ (up to $10^{4}$ in our experiments) and demonstrated\nbetter performance and accuracy compared to the FFT-based algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1186/1687-6180-2014-142", 
    "link": "http://arxiv.org/pdf/1405.7442v2", 
    "title": "Overview of Constrained PARAFAC Models", 
    "arxiv-id": "1405.7442v2", 
    "author": "Andr\u00e9 L. F. de Almeida", 
    "publish": "2014-05-29T02:27:50Z", 
    "summary": "In this paper, we present an overview of constrained PARAFAC models where the\nconstraints model linear dependencies among columns of the factor matrices of\nthe tensor decomposition, or alternatively, the pattern of interactions between\ndifferent modes of the tensor which are captured by the equivalent core tensor.\nSome tensor prerequisites with a particular emphasis on mode combination using\nKronecker products of canonical vectors that makes easier matricization\noperations, are first introduced. This Kronecker product based approach is also\nformulated in terms of the index notation, which provides an original and\nconcise formalism for both matricizing tensors and writing tensor models. Then,\nafter a brief reminder of PARAFAC and Tucker models, two families of\nconstrained tensor models, the co-called PARALIND/CONFAC and PARATUCK models,\nare described in a unified framework, for $N^{th}$ order tensors. New tensor\nmodels, called nested Tucker models and block PARALIND/CONFAC models, are also\nintroduced. A link between PARATUCK models and constrained PARAFAC models is\nthen established. Finally, new uniqueness properties of PARATUCK models are\ndeduced from sufficient conditions for essential uniqueness of their associated\nconstrained PARAFAC models."
},{
    "category": "cs.NA", 
    "doi": "10.4006/0836-1398-27.4.616", 
    "link": "http://arxiv.org/pdf/1407.3374v1", 
    "title": "An improved car-following model considering variable safety headway   distance", 
    "arxiv-id": "1407.3374v1", 
    "author": "Yiman Du", 
    "publish": "2014-07-12T11:26:44Z", 
    "summary": "Considering high speed following on expressway or highway, an improved\ncar-following model is developed in this paper by introducing variable safety\nheadway distance. Stability analysis of the new model is carried out using the\ncontrol theory method. Finally, numerical simulations are implemented and the\nresults show good consistency with theoretical study."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1410.1934v1", 
    "title": "Approximate Exponential Algorithms to Solve the Chemical Master Equation", 
    "arxiv-id": "1410.1934v1", 
    "author": "Adrian Sandu", 
    "publish": "2014-10-07T22:11:53Z", 
    "summary": "This paper discusses new simulation algorithms for stochastic chemical\nkinetics that exploit the linearity of the chemical master equation and its\nmatrix exponential exact solution. These algorithms make use of various\napproximations of the matrix exponential to evolve probability densities in\ntime. A sampling of the approximate solutions of the chemical master equation\nis used to derive accelerated stochastic simulation algorithms. Numerical\nexperiments compare the new methods with the established stochastic simulation\nalgorithm and the tau-leaping method."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1410.2202v1", 
    "title": "Newton-Ellipsoid Method and its Polynomiography", 
    "arxiv-id": "1410.2202v1", 
    "author": "Eric Lee", 
    "publish": "2014-10-08T18:12:26Z", 
    "summary": "We introduce a new iterative root-finding method for complex polynomials,\ndubbed {\\it Newton-Ellipsoid} method. It is inspired by the Ellipsoid method, a\nclassical method in optimization, and a property of Newton's Method derived in\n\\cite{kalFTA}, according to which at each complex number a half-space can be\nfound containing a root. Newton-Ellipsoid method combines this property, bounds\non zeros, together with the plane-cutting properties of the Ellipsoid Method.\nWe present computational results for several examples, as well as corresponding\npolynomiography. Polynomiography refers to algorithmic visualization of\nroot-finding. Newton's method is the first member of the infinite family of\niterations, the {\\it basic family}. We also consider general versions of this\nellipsoid approach where Newton's method is replaced by a higher-order member\nof the family such as Halley's method."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1410.8155v1", 
    "title": "Solving stochastic chemical kinetics by Metropolis Hastings sampling", 
    "arxiv-id": "1410.8155v1", 
    "author": "Adrian Sandu", 
    "publish": "2014-10-29T20:34:59Z", 
    "summary": "This study considers using Metropolis-Hastings algorithm for stochastic\nsimulation of chemical reactions. The proposed method uses SSA (Stochastic\nSimulation Algorithm) distribution which is a standard method for solving\nwell-stirred chemically reacting systems as a desired distribution. A new\nnumerical solvers based on exponential form of exact and approximate solutions\nof CME (Chemical Master Equation) is employed for obtaining target and proposal\ndistributions in Metropolis-Hastings algorithm to accelerate the accuracy of\nthe tau-leap method. Samples generated by this technique have the same\ndistribution as SSA and the histogram of samples show it's convergence to SSA."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1411.3406v6", 
    "title": "A Field Guide to Forward-Backward Splitting with a FASTA Implementation", 
    "arxiv-id": "1411.3406v6", 
    "author": "Richard Baraniuk", 
    "publish": "2014-11-13T00:38:52Z", 
    "summary": "Non-differentiable and constrained optimization play a key role in machine\nlearning, signal and image processing, communications, and beyond. For\nhigh-dimensional minimization problems involving large datasets or many\nunknowns, the forward-backward splitting method provides a simple, practical\nsolver. Despite its apparently simplicity, the performance of the\nforward-backward splitting is highly sensitive to implementation details.\n  This article is an introductory review of forward-backward splitting with a\nspecial emphasis on practical implementation concerns. Issues like stepsize\nselection, acceleration, stopping conditions, and initialization are\nconsidered. Numerical experiments are used to compare the effectiveness of\ndifferent approaches.\n  Many variations of forward-backward splitting are implemented in the solver\nFASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA\nprovides a simple interface for applying forward-backward splitting to a broad\nrange of problems."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1412.2675v3", 
    "title": "Enhanced joint sparsity via Iterative Support Detection", 
    "arxiv-id": "1412.2675v3", 
    "author": "Tingzhu Huang", 
    "publish": "2014-12-08T17:42:58Z", 
    "summary": "Joint sparsity has attracted considerable attention in recent years in many\nfields including sparse signal recovery in compressed sensing (CS), statistics,\nand machine learning. Traditional convex models suffer from the suboptimal\nperformance though enjoying tractable computation. In this paper, we propose a\nnew non-convex joint sparsity model, and develop a corresponding multi-stage\nadaptive convex relaxation algorithm. This method extends the idea of iterative\nsupport detection (ISD) from the single vector estimation to the multi-vector\nestimation by considering the joint sparsity prior. We provide some preliminary\ntheoretical analysis including convergence analysis and a sufficient recovery\ncondition. Numerical experiments from both compressive sensing and feature\nlearning show the better performance of the proposed method in comparison with\nseveral state-of-the-art alternatives. Moreover, we demonstrate that the\nextension of ISD from the single vector to multi-vector estimation is not\ntrivial. In particular, while ISD does not work well for reconstructing the\nsignal channel sparse Bernoulli signal, it does achieve significantly improved\nperformance when recovering the multi-channel sparse Bernoulli signal thanks to\nits ability of natural incorporation of the joint sparsity structure."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1502.04749v1", 
    "title": "Improved Monte Carlo Variance Reduction for Space and Energy   Self-Shielding", 
    "arxiv-id": "1502.04749v1", 
    "author": "R. N. Slaybaugh", 
    "publish": "2015-02-16T23:16:36Z", 
    "summary": "Continued demand for accurate and computationally efficient transport methods\nto solve optically thick, fixed-source transport problems has inspired research\non variance-reduction (VR) techniques for Monte Carlo (MC). Methods that use\ndeterministic results to create VR maps for MC constitute a dominant branch of\nthis research, with Forward Weighted-Consistent Adjoint Driven Importance\nSampling (FW-CADIS) being a particularly successful example. However, locations\nin which energy and spatial self-shielding are combined, such as thin plates\nembedded in concrete, challenge FW-CADIS. In these cases the deterministic flux\ncannot appropriately capture transport behavior, and the associated VR\nparameters result in high variance in and following the plate.\n  This work presents a new method that improves performance in transport\ncalculations that contain regions of combined space and energy self-shielding\nwithout significant impact on the solution quality in other parts of the\nproblem. This method is based on FW-CADIS and applies a Resonance Factor\ncorrection to the adjoint source. The impact of the Resonance Factor method is\ninvestigated in this work through an example problem. It is clear that this new\nmethod dramatically improves performance in terms of lowering the maximum 95%\nconfidence interval relative error and reducing the compute time. Based on this\nwork, we recommend that the Resonance Factor method be used when the accuracy\nof the solution in the presence of combined space and energy self-shielding is\nimportant."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1504.00523v4", 
    "title": "Fast Spectral Low Rank Matrix Approximation", 
    "arxiv-id": "1504.00523v4", 
    "author": "Zhihua Zhang", 
    "publish": "2015-04-02T12:29:34Z", 
    "summary": "First, we extend the results of approximate matrix multiplication from the\nFrobenius norm to the spectral norm. Second, We develop a class of fast\napproximate generalized linear regression algorithms with respect to the\nspectral norm. Finally, We give a fast approximate SVD."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1505.02740v3", 
    "title": "Noise Robustness of a Combined Phase Retrieval and Reconstruction Method   for Phase-Contrast Tomography", 
    "arxiv-id": "1505.02740v3", 
    "author": "Per Christian Hansen", 
    "publish": "2015-05-11T19:17:33Z", 
    "summary": "Classical reconstruction methods for phase-contrast tomography consist of two\nstages: phase retrieval and tomographic reconstruction. A novel algebraic\nmethod combining the two was suggested by Kostenko et al. (Opt. Express, 21,\n12185, 2013) and preliminary results demonstrating improved reconstruction\ncompared to a two-stage method given. Using simulated free-space propagation\nexperiments with a single sample-detector distance, we thoroughly compare the\nnovel method with the two-stage method to address limitations of the\npreliminary results. We demonstrate that the novel method is substantially more\nrobust towards noise; our simulations point to a possible reduction in counting\ntimes by an order of magnitude."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1505.06151v1", 
    "title": "Automatic Detection of the Common and Non-common Frequencies in   Congruent Discrete Spectra. A Theoretical Approach", 
    "arxiv-id": "1505.06151v1", 
    "author": "Constantin Paunoiu", 
    "publish": "2015-05-22T17:10:52Z", 
    "summary": "Both sampling a time-varying signal, and its spectral analysis are activities\nsubjected to theoretically compelling, such as Shannon's theorem and the\nobjectively limiting of the frequency's resolution. Usually, the signals'\nspectra are processed and interpreted by a scientist who, presumably, has\nsufficient prior information about the monitored signals to conclude on the\nsignificant frequencies, for example. On the other hand, processing and\ninterpretation of signals' spectra can be routine tasks that must be automated\nusing suitable software, i.e. PC application. In the above context, the paper\npresents the theoretic bases of an intuitive and practical approach of the\n(automatic) detection of the common and non-common frequencies in two or more\ncongruent spectra."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1506.01701v1", 
    "title": "Reversible Digital Filters Total Parametric Sensitivity Optimization   using Non-canonical Hypercomplex Number Systems", 
    "arxiv-id": "1506.01701v1", 
    "author": "Iana V. Khitsko", 
    "publish": "2015-01-25T18:38:09Z", 
    "summary": "Digital filter construction method, which is optimal by parametric\nsensitivity, based on using of non-canonical hypercomplex number systems is\nproposed and investigated. It is shown that the use of non-canonical\nhypercomplex number system with greater number of non-zero structure constants\nin multiplication table can significantly improve the sensitivity of the\ndigital filter."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1507.00687v2", 
    "title": "Improving the numerical stability of fast matrix multiplication", 
    "arxiv-id": "1507.00687v2", 
    "author": "Oded Schwartz", 
    "publish": "2015-07-02T18:42:28Z", 
    "summary": "Fast algorithms for matrix multiplication, namely those that perform\nasymptotically fewer scalar operations than the classical algorithm, have been\nconsidered primarily of theoretical interest. Apart from Strassen's original\nalgorithm, few fast algorithms have been efficiently implemented or used in\npractical applications. However, there exist many practical alternatives to\nStrassen's algorithm with varying performance and numerical properties. Fast\nalgorithms are known to be numerically stable, but because their error bounds\nare slightly weaker than the classical algorithm, they are not used even in\ncases where they provide a performance benefit.\n  We argue in this paper that the numerical sacrifice of fast algorithms,\nparticularly for the typical use cases of practical algorithms, is not\nprohibitive, and we explore ways to improve the accuracy both theoretically and\nempirically. The numerical accuracy of fast matrix multiplication depends on\nproperties of the algorithm and of the input matrices, and we consider both\ncontributions independently. We generalize and tighten previous error analyses\nof fast algorithms and compare their properties. We discuss algorithmic\ntechniques for improving the error guarantees from two perspectives:\nmanipulating the algorithms, and reducing input anomalies by various forms of\ndiagonal scaling. Finally, we benchmark performance and demonstrate our\nimproved numerical accuracy."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1507.03173v1", 
    "title": "A Gauss-Seidel Iterative Thresholding Algorithm for lq Regularized Least   Squares Regression", 
    "arxiv-id": "1507.03173v1", 
    "author": "Shaobo Lin", 
    "publish": "2015-07-12T00:47:51Z", 
    "summary": "In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularized least\nsquares regression ($l_q$LS) has received considerable attention due to its\nsuperiorities on sparsity-inducing and bias-reduction over the convex\ncounterparts. In this paper, we propose a Gauss-Seidel iterative thresholding\nalgorithm (called GAITA) for solution to this problem. Different from the\nclassical iterative thresholding algorithms using the Jacobi updating rule,\nGAITA takes advantage of the Gauss-Seidel rule to update the coordinate\ncoefficients. Under a mild condition, we can justify that the support set and\nsign of an arbitrary sequence generated by GAITA will converge within finite\niterations. This convergence property together with the Kurdyka-{\\L}ojasiewicz\nproperty of ($l_q$LS) naturally yields the strong convergence of GAITA under\nthe same condition as above, which is generally weaker than the condition for\nthe convergence of the classical iterative thresholding algorithms.\nFurthermore, we demonstrate that GAITA converges to a local minimizer under\ncertain additional conditions. A set of numerical experiments are provided to\nshow the effectiveness, particularly, much faster convergence of GAITA as\ncompared with the classical iterative thresholding algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1507.03331v7", 
    "title": "Certified Roundoff Error Bounds Using Semidefinite Programming", 
    "arxiv-id": "1507.03331v7", 
    "author": "Alastair Donaldson", 
    "publish": "2015-07-13T06:21:01Z", 
    "summary": "Roundoff errors cannot be avoided when implementing numerical programs with\nfinite precision. The ability to reason about rounding is especially important\nif one wants to explore a range of potential representations, for instance for\nFPGAs or custom hardware implementations. This problem becomes challenging when\nthe program does not employ solely linear operations, and non-linearities are\ninherent to many interesting computational problems in real-world applications.\n  Existing solutions to reasoning possibly lead to either inaccurate bounds or\nhigh analysis time in the presence of nonlinear correlations between variables.\nFurthermore, while it is easy to implement a straightforward method such as\ninterval arithmetic, sophisticated techniques are less straightforward to\nimplement in a formal setting. Thus there is a need for methods which output\ncertificates that can be formally validated inside a proof assistant.\n  We present a framework to provide upper bounds on absolute roundoff errors of\nfloating-point nonlinear programs. This framework is based on optimization\ntechniques employing semidefinite programming and sums of squares certificates,\nwhich can be checked inside the Coq theorem prover to provide formal roundoff\nerror bounds for polynomial programs. Our tool covers a wide range of nonlinear\nprograms, including polynomials and transcendental operations as well as\nconditional statements. We illustrate the efficiency and precision of this tool\non non-trivial programs coming from biology, optimization and space control.\nOur tool produces more accurate error bounds for 23% of all programs and yields\nbetter performance in 66% of all programs."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1508.05273v1", 
    "title": "Rank-1 Tensor Approximation Methods and Application to Deflation", 
    "arxiv-id": "1508.05273v1", 
    "author": "Andre Lima Ferrer de Almeida", 
    "publish": "2015-08-21T14:04:02Z", 
    "summary": "Because of the attractiveness of the canonical polyadic (CP) tensor\ndecomposition in various applications, several algorithms have been designed to\ncompute it, but efficient ones are still lacking. Iterative deflation\nalgorithms based on successive rank-1 approximations can be used to perform\nthis task, since the latter are rather easy to compute. We first present an\nalgebraic rank-1 approximation method that performs better than the standard\nhigher-order singular value decomposition (HOSVD) for three-way tensors.\nSecond, we propose a new iterative rank-1 approximation algorithm that improves\nany other rank-1 approximation method. Third, we describe a probabilistic\nframework allowing to study the convergence of deflation CP decomposition\n(DCPD) algorithms based on successive rank-1 approximations. A set of computer\nexperiments then validates theoretical results and demonstrates the efficiency\nof DCPD algorithms compared to other ones."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1509.02157v1", 
    "title": "Detecting Potential Instabilities of Numerical Algorithms", 
    "arxiv-id": "1509.02157v1", 
    "author": "Yao Yang", 
    "publish": "2015-09-07T09:45:34Z", 
    "summary": "It has been the standard teaching of today that backward stability analysis\nis taught as absolute, just as in Newtonian physics time is taught absolute\ntime. We will prove it is not true in general. It depends on algorithms. We\nwill prove that forward and mixed stability anlaysis are absolutely invalid\nstability analysis in the sense that they have absolutely wrong reference\npoints for detecting huge element growth of any algoritms(if any), even an\n\"ideal\" or \"desirable\" backward stability analysis is not so \"ideal\" or\n\"desirable\" in general. Any of forward stable, backward stable and mixed stable\nalgorihms as in Demmel, Kahan , Parlett and other's papers and text books, see\nDemmel(6) and Higham(8)may not be really stable at all because they may fail to\ndetect and expose any potential instabilities of the algorithm in corresponding\nstability analysis. Therefore, it is impossible to prove an algorithm is stable\naccording to the standard teachin of today, just as it is impossible to prove a\nmathematical equuation(Maxwell's) is a law of physics according to the standard\nteaching in Newtonian physics."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1509.05895v2", 
    "title": "Trading Accuracy for Numerical Stability: Orthogonalization,   Biorthogonalization and Regularization", 
    "arxiv-id": "1509.05895v2", 
    "author": "Alan V. Oppenheim", 
    "publish": "2015-09-19T14:10:31Z", 
    "summary": "This paper presents two novel regularization methods motivated in part by the\ngeometric significance of biorthogonal bases in signal processing applications.\nThese methods, in particular, draw upon the structural relevance of\northogonality and biorthogonality principles and are presented from the\nperspectives of signal processing, convex programming, continuation methods and\nnonlinear projection operators. Each method is specifically endowed with either\na homotopy or tuning parameter to facilitate tradeoff analysis between accuracy\nand numerical stability. An example involving a basis comprised of real\nexponential signals illustrates the utility of the proposed methods on an\nill-conditioned inverse problem and the results are compared to standard\nregularization techniques from the signal processing literature."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.01118v1", 
    "title": "Illustration of iterative linear solver behavior on simple 1D and 2D   problems", 
    "arxiv-id": "1510.01118v1", 
    "author": "Sokolov Dmitry", 
    "publish": "2015-10-05T12:11:43Z", 
    "summary": "In geometry processing, numerical optimization methods often involve solving\nsparse linear systems of equations. These linear systems have a structure that\nstrongly resembles to adjacency graphs of the underlying mesh. We observe how\nclassic linear solvers behave on this specific type of problems. For the sake\nof simplicity, we minimise either the squared gradient or the squared\nLaplacian, evaluated by finite differences on a regular 1D or 2D grid. We\nobserved the evolution of the solution for both energies, in 1D and 2D, and\nwith different solvers: Jacobi, Gauss-Seidel, SSOR (Symmetric successive\nover-relaxation) and CG (conjugate gradient [She94]). Plotting results at\ndifferent iterations allows to have an intuition of the behavior of these\nclassic solvers."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.01145v1", 
    "title": "Reduced Precision Checking to Detect Errors in Floating Point Arithmetic", 
    "arxiv-id": "1510.01145v1", 
    "author": "Daniel J. Sorin", 
    "publish": "2015-10-05T13:38:23Z", 
    "summary": "In this paper, we use reduced precision checking (RPC) to detect errors in\nfloating point arithmetic. Prior work explored RPC for addition and\nmultiplication. In this work, we extend RPC to a complete floating point unit\n(FPU), including division and square root, and we present precise analyses of\nthe errors undetectable with RPC that show bounds that are smaller than prior\nwork. We implement RPC for a complete FPU in RTL and experimentally evaluate\nits error coverage and cost."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.04591v1", 
    "title": "New fast divide-and-conquer algorithms for the symmetric tridiagonal   eigenvalue problem", 
    "arxiv-id": "1510.04591v1", 
    "author": "Hao Jiang", 
    "publish": "2015-10-15T15:42:18Z", 
    "summary": "In this paper, two accelerated divide-and-conquer algorithms are proposed for\nthe symmetric tridiagonal eigenvalue problem, which cost $O(N^2r)$ {flops} in\nthe worst case, where $N$ is the dimension of the matrix and $r$ is a modest\nnumber depending on the distribution of eigenvalues. Both of these algorithms\nuse hierarchically semiseparable (HSS) matrices to approximate some\nintermediate eigenvector matrices which are Cauchy-like matrices and are\noff-diagonally low-rank. The difference of these two versions lies in using\ndifferent HSS construction algorithms, one (denoted by {ADC1}) uses a\nstructured low-rank approximation method and the other ({ADC2}) uses a\nrandomized HSS construction algorithm. For the ADC2 algorithm, a method is\nproposed to estimate the off-diagonal rank. Numerous experiments have been done\nto show their stability and efficiency. These algorithms are implemented in\nparallel in a shared memory environment, and some parallel implementation\ndetails are included. Comparing the ADCs with highly optimized multithreaded\nlibraries such as Intel MKL, we find that ADCs could be more than 6x times\nfaster for some large matrices with few deflations."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.04632v1", 
    "title": "Monte Carlo Dynamically Weighted Importance Sampling For Finite Element   Model Updating", 
    "arxiv-id": "1510.04632v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2015-10-15T17:11:12Z", 
    "summary": "The Finite Element Method (FEM) is generally unable to accurately predict\nnatural frequencies and mode shapes of structures (eigenvalues and\neigenvectors). Engineers develop numerical methods and a variety of techniques\nto compensate for this misalignment of modal properties, between experimentally\nmeasured data and the computed result from the FEM of structures. In this paper\nwe compare two indirect methods of updating namely, the Adaptive Metropolis\nHastings and a newly applied algorithm called Monte Carlo Dynamically Weighted\nImportance Sampling (MCDWIS). The approximation of a posterior predictive\ndistribution is based on Bayesian inference of continuous multivariate Gaussian\nprobability density functions, defining the variability of physical properties\naffected by forced vibration. The motivation behind applying MCDWIS is in the\ncomplexity of computing normalizing constants in higher dimensional or\nmultimodal systems. The MCDWIS accounts for this intractability by analytically\ncomputing importance sampling estimates at each time step of the algorithm. In\naddition, a dynamic weighting step with an Adaptive Pruned Enriched Population\nControl Scheme (APEPCS) allows for further control over weighted samples and\npopulation size. The performance of the MCDWIS simulation is graphically\nillustrated for all algorithm dependent parameters and show unbiased, stable\nsample estimates."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.04658v2", 
    "title": "Spectral Partitioning with Blends of Eigenvectors", 
    "arxiv-id": "1510.04658v2", 
    "author": "David A. Bader", 
    "publish": "2015-10-15T18:36:51Z", 
    "summary": "Many common methods for data analysis rely on linear algebra. We provide new\nresults connecting data analysis error to numerical accuracy, which leads to\nthe first meaningful stopping criterion for two way spectral partitioning. More\ngenerally, we provide pointwise convergence guarantees so that blends (linear\ncombinations) of eigenvectors can be employed to solve data analysis problems\nwith confidence in their accuracy. We demonstrate this theory on an accessible\nmodel problem, the Ring of Cliques, by deriving the relevant eigenpairs and\ncomparing the predicted results to numerical solutions. These results bridge\nthe gap between linear algebra based data analysis methods and the convergence\ntheory of iterative approximation methods."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.04853v1", 
    "title": "Efficient Approaches for Enclosing the United Solution Set of the   Interval Generalized Sylvester Matrix Equation", 
    "arxiv-id": "1510.04853v1", 
    "author": "Milan Hlad\u00edk", 
    "publish": "2015-10-16T11:59:50Z", 
    "summary": "In this work, we investigate the interval generalized Sylvester matrix\nequation ${\\bf{A}}X{\\bf{B}}+{\\bf{C}}X{\\bf{D}}={\\bf{F}}$ and develop some\ntechniques for obtaining outer estimations for the so-called united solution\nset of this interval system. First, we propose a modified variant of the\nKrawczyk operator which causes reducing computational complexity to cubic,\ncompared to Kronecker product form. We then propose an iterative technique for\nenclosing the solution set. These approaches are based on spectral\ndecompositions of the midpoints of ${\\bf{A}}$, ${\\bf{B}}$, ${\\bf{C}}$ and\n${\\bf{D}}$ and in both of them we suppose that the midpoints of ${\\bf{A}}$ and\n${\\bf{C}}$ are simultaneously diagonalizable as well as for the midpoints of\nthe matrices ${\\bf{B}}$ and ${\\bf{D}}$. Some numerical experiments are given to\nillustrate the performance of the proposed methods."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1510.05231v1", 
    "title": "Signal Processing Structures for Solving Conservative Constraint   Satisfaction Problems", 
    "arxiv-id": "1510.05231v1", 
    "author": "Thomas A. Baran", 
    "publish": "2015-10-18T12:21:48Z", 
    "summary": "This primary purpose of this paper is to succinctly state a number of\nverifiable and tractable sufficient conditions under which a particular class\nof conservative signal processing structures may be readily used to solve a\ncompanion class of constraint satisfaction problems using both synchronous and\nasynchronous implementation protocols. In particular, the mentioned class of\nstructures is shown to have desirable convergence and robustness properties\nwith respect to various uncertainties involving communication and processing\ndelays. Essential ingredients to the arguments herein involve blending together\nfunctional composition methods, conservation principles, asynchronous signal\nprocessing implementation protocols, and methods of homotopy. Numerical\nexperiments complement the theoretical presentation and connections to\noptimization theory are made."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1511.01306v2", 
    "title": "About Notations in Multiway Array Processing", 
    "arxiv-id": "1511.01306v2", 
    "author": "Jeremy E. Cohen", 
    "publish": "2015-11-04T12:38:56Z", 
    "summary": "This paper gives an overview of notations used in multiway array processing.\nWe redefine the vectorization and matricization operators to comply with some\nproperties of the Kronecker product. The tensor product and Kronecker product\nare also represented with two different symbols, and it is shown how these\nnotations lead to clearer expressions for multiway array operations. Finally,\nthe paper recalls the useful yet widely unknown properties of the array normal\nlaw with suggested notations."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1511.01598v4", 
    "title": "A Simple Approach to Optimal CUR Decomposition", 
    "arxiv-id": "1511.01598v4", 
    "author": "Zhihua Zhang", 
    "publish": "2015-11-05T03:48:28Z", 
    "summary": "Prior optimal CUR decomposition and near optimal column reconstruction\nmethods have been established by combining BSS sampling and adaptive sampling.\nIn this paper, we propose a new approach to the optimal CUR decomposition and\nnear optimal column reconstruction by just using leverage score sampling. In\nour approach, both the BSS sampling and adaptive sampling are not needed.\nMoreover, our approach is the first $O(\\mathrm{nnz}(\\A))$ optimal CUR algorithm\nwhere $\\A$ is a data matrix in question. We also extend our approach to the\nNystr{\\\"o}m method, obtaining a fast algorithm which runs $\\tilde{O}(n^{2})$ or\n$O(\\mathrm{\\nnz}(\\A))$"
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1511.05362v2", 
    "title": "Accelerating Random Kaczmarz Algorithm Based on Clustering Information", 
    "arxiv-id": "1511.05362v2", 
    "author": "Haishan Ye", 
    "publish": "2015-11-17T11:58:24Z", 
    "summary": "Kaczmarz algorithm is an efficient iterative algorithm to solve\noverdetermined consistent system of linear equations. During each updating\nstep, Kaczmarz chooses a hyperplane based on an individual equation and\nprojects the current estimate for the exact solution onto that space to get a\nnew estimate. Many vairants of Kaczmarz algorithms are proposed on how to\nchoose better hyperplanes. Using the property of randomly sampled data in\nhigh-dimensional space, we propose an accelerated algorithm based on clustering\ninformation to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss\nlemma. Additionally, we theoretically demonstrate convergence improvement on\nblock Kaczmarz algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1511.06227v2", 
    "title": "Empirical Research and Automatic Processing Method of Precision-specific   Operation", 
    "arxiv-id": "1511.06227v2", 
    "author": "Xinrui He", 
    "publish": "2015-11-19T16:10:04Z", 
    "summary": "Significant inaccuracy often occurs during the process of mathematical\ncalculation due to the digit limitation of floating point, which may lead to\ncatastrophic loss. Normally, people believe that adjustment of floating-point\nprecision is an effective way to solve this problem, since high-precision\nfloating-point has more digits to store information. Thus, it is a prevalent\nmethod to reduce the inaccuracy in much floating-point related research, that\nperforming all the operations with higher precision. However, we discover that\nsome operations may lead to larger error in higher precision. In this paper, we\ndefine this kind of operation that generates large error due to precision\nadjustment a precision-specific operation. Furthermore, we propose a\nlight-weight searching algorithm for detecting precision-specific operations\nand figure out an automatic processing method to fixing them. In addition, we\nconducted an experiment on the scientific mathematical library of GLIBC. The\nresult shows that there are many precision-specific operations, and our fixing\napproach can significantly reduce the inaccuracy."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1511.08547v1", 
    "title": "Wilkinson's Inertia-Revealing Factorization and Its Application to   Sparse Matrices", 
    "arxiv-id": "1511.08547v1", 
    "author": "Sivan Toledo", 
    "publish": "2015-11-27T03:11:17Z", 
    "summary": "We propose a new inertia-revealing factorization for sparse matrices. The\nfactorization scheme and the method for extracting the inertia from it were\nproposed in the 1960s for dense, banded, or tridiagonal matrices, but they have\nbeen abandoned in favor of faster methods. We show that this scheme can be\napplied to any sparse matrix and that the fill in the factorization is bounded\nby the fill in the sparse QR factorization of the same matrix (but is usually\nmuch smaller). We present experimental results, studying the method's numerical\nstability and performance. Our implementation of the method is somewhat naive,\nbut still demonstrates its potential."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1512.03224v1", 
    "title": "Spectral Compressed Sensing via CANDECOMP/PARAFAC Decomposition of   Incomplete Tensors", 
    "arxiv-id": "1512.03224v1", 
    "author": "Hongbin Li", 
    "publish": "2015-12-10T12:04:56Z", 
    "summary": "We consider the line spectral estimation problem which aims to recover a\nmixture of complex sinusoids from a small number of randomly observed time\ndomain samples. Compressed sensing methods formulates line spectral estimation\nas a sparse signal recovery problem by discretizing the continuous frequency\nparameter space into a finite set of grid points. Discretization, however,\ninevitably incurs errors and leads to deteriorated estimation performance. In\nthis paper, we propose a new method which leverages recent advances in tensor\ndecomposition. Specifically, we organize the observed data into a structured\ntensor and cast line spectral estimation as a CANDECOMP/PARAFAC (CP)\ndecomposition problem with missing entries. The uniqueness of the CP\ndecomposition allows the frequency components to be super-resolved with\ninfinite precision. Simulation results show that the proposed method provides a\ncompetitive estimate accuracy compared with existing state-of-the-art\nalgorithms."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.00112v1", 
    "title": "Stability and bifurcation properties of the algorithms for keeping of   differential equations solutions on the required level", 
    "arxiv-id": "1601.00112v1", 
    "author": "Yu. V. Troshchiev", 
    "publish": "2016-01-01T20:42:09Z", 
    "summary": "Algorithms of control of differential equations solutions are under\ninvestigation in the article. Idealized and real modifications of the\nalgorithms are distinguished. An equation, which can be the base equation for\ninvestigation of the idealized algorithms properties, is constructed. The\ndifference appearing for real systems and real algorithms is for separate\ninvestigation. This difference tends to zero under tending to zero of the time\nstep of control. If the systems of equations satisfy or almost satisfy some\nproperties for which the algorithms are intended, then the results are similar\nnumerically as well. One of the algorithms demonstrates high reliability.\nAnother one is of more complex properties. Bifurcations, periodic solutions and\nstrange attractors are possible in both algorithms in addition to stable steady\nstates."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.01083v1", 
    "title": "Efficient tensor completion: Low-rank tensor train", 
    "arxiv-id": "1601.01083v1", 
    "author": "Minh N. Do", 
    "publish": "2016-01-06T05:48:21Z", 
    "summary": "This paper proposes a novel formulation of the tensor completion problem to\nimpute missing entries of data represented by tensors. The formulation is\nintroduced in terms of tensor train (TT) rank which can effectively capture\nglobal information of tensors thanks to its construction by a well-balanced\nmatricization scheme. Two algorithms are proposed to solve the corresponding\ntensor completion problem. The first one called simple low-rank tensor\ncompletion via tensor train (SiLRTC-TT) is intimately related to minimizing the\nTT nuclear norm. The second one is based on a multilinear matrix factorization\nmodel to approximate the TT rank of the tensor and called tensor completion by\nparallel matrix factorization via tensor train (TMac-TT). These algorithms are\napplied to complete both synthetic and real world data tensors. Simulation\nresults of synthetic data show that the proposed algorithms are efficient in\nestimating missing entries for tensors with either low Tucker rank or TT rank\nwhile Tucker-based algorithms are only comparable in the case of low Tucker\nrank tensors. When applied to recover color images represented by ninth-order\ntensors augmented from third-order ones, the proposed algorithms outperforms\nthe Tucker-based algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.03230v1", 
    "title": "An Optimal Block Diagonal Preconditioner for Heterogeneous Saddle Point   Problems in Phase Separation", 
    "arxiv-id": "1601.03230v1", 
    "author": "Pawan Kumar", 
    "publish": "2016-01-13T13:18:24Z", 
    "summary": "The phase separation processes are typically modeled by Cahn-Hilliard\nequations. This equation was originally introduced to model phase separation in\nbinary alloys, where phase stands for concentration of different components in\nalloy. When the binary alloy under preparation is subjected to a rapid\nreduction in temperature below a critical temperature, it has been\nexperimentally observed that the concentration changes from a mixed state to a\nvisibly distinct spatially separated two phase for binary alloy. This rapid\nreduction in the temperature, the so-called \"deep quench limit\", is modeled\neffectively by obstacle potential. The discretization of Cahn-Hilliard equation\nwith obstacle potential leads to a block $2 \\times 2$ {\\em non-linear} system,\nwhere the $(1,1)$ block has a non-linear and non-smooth term. Recently a\nglobally convergent Newton Schur method was proposed for the non-linear Schur\ncomplement corresponding to this non-linear system. The proposed method is\nsimilar to an inexact active set method in the sense that the active sets are\nfirst approximately identified by solving a quadratic obstacle problem\ncorresponding to the $(1,1)$ block of the block $2 \\times 2$ system, and later\nsolving a reduced linear system by annihilating the rows and columns\ncorresponding to identified active sets. For solving the quadratic obstacle\nproblem, various optimal multigrid like methods have been proposed. In this\npaper, we study a non-standard norm that is equivalent to applying a block\ndiagonal preconditioner to the reduced linear systems. Numerical experiments\nconfirm the optimality of the solver and convergence independent of problem\nparameters on sufficiently fine mesh."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.05695v1", 
    "title": "Fluid Dynamics Modeling : The Numerical Solution Of 2D Navier Hyperbolic   Equations", 
    "arxiv-id": "1601.05695v1", 
    "author": "Narek Jilavyan", 
    "publish": "2016-01-21T16:18:55Z", 
    "summary": "In the following paper we will consider Navier-Stokes problem and it's\ninterpretation by hyperbolic waves, focusing on wave propagation. We will begin\nwith solution for linear waves, then present problem for non-linear waves.\nLater we will derive for numerical solution using PDE's. Also we will design a\nMatlab program to solve and simulate wave propagation."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.05758v2", 
    "title": "A New Pivot Selection Algorithm for Symmetric Indefinite Factorization   Arising in Quadratic Programming with Block Constraint Matrices", 
    "arxiv-id": "1601.05758v2", 
    "author": "Gun Srijuntongsiri", 
    "publish": "2016-01-21T19:12:54Z", 
    "summary": "Quadratic programmingis a class of constrained optimization problem with\nquadratic objective functions and linear constraints. It has applications in\nmany areas and is also used to solve nonlinear optimization problems. This\narticle focuses on the equality constrained quadratic programs whose constraint\nmatrices are block diagonal. Using the direct solution method, we propose a new\npivot selection algorithm for the factorization of the Karush-Kuhn-Tucker(KKT)\nmatrix for this problem that maintains the sparsity and stability of the\nproblem. Our experiments show that our pivot selection algorithm appears to\nproduce no fill-ins in the factorizationof such matrices. In addition, we\ncompare our method with MA57 and find that the factors produced by our\nalgorithm are sparser."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1601.06812v1", 
    "title": "New Pivot Selection for Sparse Symmetric Indefinite Factorization", 
    "arxiv-id": "1601.06812v1", 
    "author": "Gun Srijuntongsiri", 
    "publish": "2016-01-25T21:25:41Z", 
    "summary": "We propose a new pivot selection technique for symmetric indefinite\nfactorization of sparse matrices. Such factorization should maintain both\nsparsity and numerical stability of the factors, both of which depend solely on\nthe choices of the pivots. Our method is based on the minimum degree algorithm\nand also considers the stability of the factors at the same time. Our\nexperiments show that our method produces factors that are sparser than the\nfactors computed by MA57 and are stable."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1602.02244v1", 
    "title": "Fast Multipole Method as a Matrix-Free Hierarchical Low-Rank   Approximation", 
    "arxiv-id": "1602.02244v1", 
    "author": "David Keyes", 
    "publish": "2016-02-06T12:04:14Z", 
    "summary": "There has been a large increase in the amount of work on hierarchical\nlow-rank approximation methods, where the interest is shared by multiple\ncommunities that previously did not intersect. This objective of this article\nis two-fold; to provide a thorough review of the recent advancements in this\nfield from both analytical and algebraic perspectives, and to present a\ncomparative benchmark of two highly optimized implementations of contrasting\nmethods for some simple yet representative test cases. We categorize the recent\nadvances in this field from the perspective of compute-memory tradeoff, which\nhas not been considered in much detail in this area. Benchmark tests reveal\nthat there is a large difference in the memory consumption and performance\nbetween the different methods."
},{
    "category": "cs.NA", 
    "doi": "10.3846/13926292.2015.1048760", 
    "link": "http://arxiv.org/pdf/1602.02740v1", 
    "title": "Toom-Cook Multiplication: Some Theoretical and Practical Aspects", 
    "arxiv-id": "1602.02740v1", 
    "author": "M. J. Kronenburg", 
    "publish": "2016-02-08T20:53:25Z", 
    "summary": "Toom-Cook multiprecision multiplication is a well-known multiprecision\nmultiplication method, which can make use of multiprocessor systems. In this\npaper the Toom-Cook complexity is derived, some explicit proofs of the\nToom-Cook interpolation method are given, the even-odd method for interpolation\nis explained, and certain aspects of a 32-bit C++ and assembler implementation,\nwhich is in development, are discussed. A performance graph of this\nimplementation is provided. The Toom-Cook method can also be used to\nmultithread other types of multiplication, which is demonstrated for 32-bit GMP\nFFT multiplication."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1602.07067v2", 
    "title": "Online Low-Rank Tensor Subspace Tracking from Incomplete Data by CP   Decomposition using Recursive Least Squares", 
    "arxiv-id": "1602.07067v2", 
    "author": "Hiroyuki Kasai", 
    "publish": "2016-02-23T07:40:40Z", 
    "summary": "We propose an online tensor subspace tracking algorithm based on the CP\ndecomposition exploiting the recursive least squares (RLS), dubbed OnLine\nLow-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). Numerical\nevaluations show that the proposed OLSTEC algorithm gives faster convergence\nper iteration comparing with the state-of-the-art online algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1602.07558v2", 
    "title": "The swept rule for breaking the latency barrier in time advancing   two-dimensional PDEs", 
    "arxiv-id": "1602.07558v2", 
    "author": "John Williams", 
    "publish": "2016-02-23T17:42:36Z", 
    "summary": "This article describes a method to accelerate parallel, explicit time\nintegration of two-dimensional unsteady PDEs. The method is motivated by our\nobservation that latency, not bandwidth, often limits how fast PDEs can be\nsolved in parallel. The method is called the swept rule of space-time domain\ndecomposition. Compared to conventional, space-only domain decomposition, it\ncommunicates similar amount of data, but in fewer messages. The swept rule\nachieves this by decomposing space and time among computing nodes in ways that\nexploit the domains of influence and the domain of dependency, making it\npossible to communicate once per many time steps with no redundant computation.\nBy communicating less often, the swept rule effectively breaks the latency\nbarrier, advancing on average more than one time step per ping-pong latency of\nthe network. The article presents simple theoretical analysis to the\nperformance of the swept rule in two spatial dimensions, and supports the\nanalysis with numerical experiments."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1602.08391v1", 
    "title": "Ultrafast a Distributed Arithmetic in multi-row codes", 
    "arxiv-id": "1602.08391v1", 
    "author": "V. I. Shcherbakov", 
    "publish": "2015-11-20T14:44:44Z", 
    "summary": "In this paper we consider the matrix structure of arithmetic processors based\non distributed arithmetic in multi-row codes. Scope - development of\nsupercomputers."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1604.00617v1", 
    "title": "A Direct Elliptic Solver Based on Hierarchically Low-rank Schur   Complements", 
    "arxiv-id": "1604.00617v1", 
    "author": "David Keyes", 
    "publish": "2016-04-03T09:52:40Z", 
    "summary": "A parallel fast direct solver for rank-compressible block tridiagonal linear\nsystems is presented. Algorithmic synergies between Cyclic Reduction and\nHierarchical matrix arithmetic operations result in a solver with $O(N \\log^2\nN)$ arithmetic complexity and $O(N \\log N)$ memory footprint. We provide a\nbaseline for performance and applicability by comparing with well known\nimplementations of the $\\mathcal{H}$-LU factorization and algebraic multigrid\nwith a parallel implementation that leverages the concurrency features of the\nmethod. Numerical experiments reveal that this method is comparable with other\nfast direct solvers based on Hierarchical Matrices such as $\\mathcal{H}$-LU and\nthat it can tackle problems where algebraic multigrid fails to converge."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1604.08476v1", 
    "title": "Unit Consistency, Generalized Inverses, and Effective System Design   Methods", 
    "arxiv-id": "1604.08476v1", 
    "author": "Jeffrey Uhlmann", 
    "publish": "2016-04-24T16:20:08Z", 
    "summary": "This paper examines the potential role of unit consistency as a system design\nprinciple. Unit-consistent generalized matrix inverses and unit-invariant\nmatrix decompositions are derived in support of this principle. Applications of\nthe methods described are illustrated with examples relating to nonlinear\nsystem identification and robustness to multiplicative noise for image database\nretrieval."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.00190v2", 
    "title": "Pixel matrices: An elementary technique for solving nonlinear systems", 
    "arxiv-id": "1605.00190v2", 
    "author": "David I. Spivak", 
    "publish": "2016-05-01T00:39:38Z", 
    "summary": "A new technique for approximating the entire solution set for a nonlinear\nsystem of relations (nonlinear equations, inequalities, etc. involving\nalgebraic, smooth, or even continuous functions) is presented. The technique is\nto first plot each function as a pixel matrix, and to then perform a sequence\nof basic matrix operations, as dictated by how variables are shared by the\nrelations in the system. The result is a pixel matrix graphing the approximated\nsimultaneous solution set for the system."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.00621v1", 
    "title": "A Necessary and Sufficient Condition for Local Maxima of Polynomial   Modulus Over Unit Disc", 
    "arxiv-id": "1605.00621v1", 
    "author": "Bahman Kalantari", 
    "publish": "2016-05-02T19:06:46Z", 
    "summary": "An important quantity associated with a complex polynomial $p(z)$ is $\\Vert p\n\\Vert_\\infty$, the maximum of its modulus over the unit disc $D$. We prove,\n$z_* \\in D$ is a local maximum of $|p(z)|$ if and only if $a_*$ satisfies,\n$z_*=p(z_*)|p'(z_*)|/p'(z_*)|p(z_*)|$, i.e. it is proportional to its\ncorresponding Newton direction. This explicit formula gives rise to novel\niterative algorithms for computing $\\Vert p \\Vert_\\infty$. We describe two such\nalgorithms, including a Newton-like method and present some visualization of\ntheir performance."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.04644v1", 
    "title": "Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation", 
    "arxiv-id": "1605.04644v1", 
    "author": "Bilong Shen", 
    "publish": "2016-05-16T03:55:31Z", 
    "summary": "The main shortage of principle component analysis (PCA) based anomaly\ndetection models is their interpretability. In this paper, our goal is to\npropose an interpretable PCA-based model for anomaly detection and\ninterpretation. The propose ASPCA model constructs principal components with\nsparse and orthogonal loading vectors to represent the abnormal subspace, and\nuses them to interpret detected anomalies. Our experiments on a synthetic\ndataset and two real world datasets showed that the proposed ASPCA models\nachieved comparable detection accuracies as the PCA model, and can provide\ninterpretations for individual anomalies."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.05023v1", 
    "title": "Comments on \"A Square-Root-Free Matrix Decomposition Method for   Energy-Efficient Least Square Computation on Embedded Systems\"", 
    "arxiv-id": "1605.05023v1", 
    "author": "Mohammad M. Mansour", 
    "publish": "2016-05-17T05:51:42Z", 
    "summary": "A square-root-free matrix QR decomposition (QRD) scheme was rederived in [1]\nbased on [2] to simplify computations when solving least-squares (LS) problems\non embedded systems. The scheme of [1] aims at eliminating both the square-root\nand division operations in the QRD normalization and backward substitution\nsteps in the LS computations. It is claimed in [1] that the LS solution only\nrequires finding the directions of the orthogonal basis of the matrix in\nquestion, regardless of the normalization of their Euclidean norms. MIMO\ndetection problems have been named as potential applications that benefit from\nthis. While this is true for unconstrained LS problems, we conversely show here\nthat constrained LS problems such as MIMO detection still require computing the\nnorms of the orthogonal basis to produce the correct result."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.05610v2", 
    "title": "A Short Proof for Gap Independence of Simultaneous Iteration", 
    "arxiv-id": "1605.05610v2", 
    "author": "Edo Liberty", 
    "publish": "2016-05-18T15:09:46Z", 
    "summary": "This note provides a very short proof of a spectral gap independent property\nof the simultaneous iterations algorithm for finding the top singular space of\na matrix. See Rokhlin-Szlam-Tygert-2009, Halko-Martinsson-Tropp-2011 and\nMusco-Musco-2015. The proof is terse but completely self contained and should\nbe accessible to the linear algebra savvy reader."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.06760v1", 
    "title": "Space-Efficient Karatsuba Multiplication for Multi-Precision Integers", 
    "arxiv-id": "1605.06760v1", 
    "author": "Yiping Cheng", 
    "publish": "2016-05-22T09:08:09Z", 
    "summary": "The traditional Karatsuba algorithm for the multiplication of polynomials and\nmulti-precision integers has a time complexity of $O(n^{1.59})$ and a space\ncomplexity of $O(n)$. Roche proposed an improved algorithm with the same\n$O(n^{1.59})$ time complexity but with a much reduced $O(\\log n)$ space\ncomplexity. In Roche's paper details were provided for multiplication of\npolynomials, but not for multi-precision integers. Multi-precision integers\ndiffer from polynomials by the presence of carries, which poses difficulties in\nimplementing Roche's scheme in multi-precision integers. This paper provides a\ndetailed solution to these difficulties. Finally, numerical comparisons between\nthe schoolbook, traditional Karatsuba, and space-efficient Karatsuba algorithms\nare provided."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1605.08134v1", 
    "title": "A Rank Revealing Randomized Singular Value Decomposition (R3SVD)   Algorithm for Low-rank Matrix Approximations", 
    "arxiv-id": "1605.08134v1", 
    "author": "Yaohang Li", 
    "publish": "2016-05-26T03:17:33Z", 
    "summary": "In this paper, we present a Rank Revealing Randomized Singular Value\nDecomposition (R3SVD) algorithm to incrementally construct a low-rank\napproximation of a potentially large matrix while adaptively estimating the\nappropriate rank that can capture most of the actions of the matrix. Starting\nfrom a low-rank approximation with an initial guessed rank, R3SVD adopts an\northogonal Gaussian sampling approach to obtain the dominant subspace within\nthe leftover space, which is used to add up to the existing low-rank\napproximation. Orthogonal Gaussian sampling is repeated until an appropriate\nlow-rank approximation with satisfactory accuracy, measured by the overall\nenergy percentage of the original matrix, is obtained. While being a fast\nalgorithm, R3SVD is also a memory-aware algorithm where the computational\nprocess can be decomposed into a series of sampling tasks that use constant\namount of memory. Numerical examples in image compression and matrix completion\nare used to demonstrate the effectiveness of R3SVD in low-rank approximation."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1606.00803v1", 
    "title": "Locality-Aware Laplacian Mesh Smoothing", 
    "arxiv-id": "1606.00803v1", 
    "author": "Padma Raghavan", 
    "publish": "2016-06-02T18:58:53Z", 
    "summary": "In this paper, we propose a novel reordering scheme to improve the\nperformance of a Laplacian Mesh Smoothing (LMS). While the Laplacian smoothing\nalgorithm is well optimized and studied, we show how a simple reordering of the\nvertices of the mesh can greatly improve the execution time of the smoothing\nalgorithm. The idea of our reordering is based on (i) the postulate that cache\nmisses are a very time consuming part of the execution of LMS, and (ii) the\nstudy of the reuse distance patterns of various executions of the LMS\nalgorithm.\n  Our reordering algorithm is very simple but allows for huge performance\nimprovement. We ran it on a Westmere-EX platform and obtained a speedup of 75\non 32 cores compared to the single core execution without reordering, and a\ngain in execution of 32% on 32 cores compared to state of the art reordering.\nFinally, we show that we leave little room for a better ordering by reducing\nthe L2 and L3 cache misses to a bare minimum."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1606.00807v1", 
    "title": "A Parallel Implementation of the Ensemble Kalman Filter Based on   Modified Cholesky Decomposition", 
    "arxiv-id": "1606.00807v1", 
    "author": "Xinwei Deng", 
    "publish": "2016-05-31T14:33:06Z", 
    "summary": "This paper discusses an efficient parallel implementation of the ensemble\nKalman filter based on the modified Cholesky decomposition. The proposed\nimplementation starts with decomposing the domain into sub-domains. In each\nsub-domain a sparse estimation of the inverse background error covariance\nmatrix is computed via a modified Cholesky decomposition; the estimates are\ncomputed concurrently on separate processors. The sparsity of this estimator is\ndictated by the conditional independence of model components for some radius of\ninfluence. Then, the assimilation step is carried out in parallel without the\nneed of inter-processor communication. Once the local analysis states are\ncomputed, the analysis sub-domains are mapped back onto the global domain to\nobtain the analysis ensemble. Computational experiments are performed using the\nAtmospheric General Circulation Model (SPEEDY) with the T-63 resolution on the\nBlueridge cluster at Virginia Tech. The number of processors used in the\nexperiments ranges from 96 to 2,048. The proposed implementation outperforms in\nterms of accuracy the well-known local ensemble transform Kalman filter (LETKF)\nfor all the model variables. The computational time of the proposed\nimplementation is similar to that of the parallel LETKF method (where no\ncovariance estimation is performed). Finally, for the largest number of\nprocessors, the proposed parallel implementation is 400 times faster than the\nserial version of the proposed method."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1606.01753v1", 
    "title": "Mathematical Modeling of General Inaccurate Adders", 
    "arxiv-id": "1606.01753v1", 
    "author": "Kirthi Krishna Muntimadugu", 
    "publish": "2016-06-06T14:09:30Z", 
    "summary": "Inaccurate circuits make possible the conservation of limited resources, such\nas energy. But effective design of such circuits requires an understanding of\nresulting tradeoffs between accuracy and design parameters, such as voltages\nand speed of execution. Although studies of tradeoffs have been done on\nspecific circuits, the applicability of those studies is narrow. This paper\npresents a comprehensive and mathematically rigorous method for analyzing a\nlarge class of inaccurate circuits for addition. Furthermore, it presents new,\nfast algorithms for the computation of key statistical measures of inaccuracy\nin such adders, thus helping hardware architects explore the design space with\ngreater confidence."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ICASSP.2016.7472131", 
    "link": "http://arxiv.org/pdf/1606.03032v1", 
    "title": "A strategy to implement Dirichlet boundary conditions in the context of   ADER finite volume schemes. One-dimensional conservation laws", 
    "arxiv-id": "1606.03032v1", 
    "author": "Gino I. Montecinos", 
    "publish": "2016-06-09T17:39:43Z", 
    "summary": "ADER schemes are numerical methods, which can reach an arbitrary order of\naccuracy in both space and time. They are based on a reconstruction procedure\nand the solution of generalized Riemann problems. However, for general boundary\nconditions, in particular of Dirichlet type, a lack of accuracy might occur if\na suitable treatment of boundaries conditions is not properly carried out. In\nthis work the treatment of Dirichlet boundary conditions for conservation laws\nin the context of ADER schemes, is concerned. The solution of generalized\nRiemann problems at the extremes of the computational domain, provides the\ncorrect influence of boundaries. The reconstruction procedure, for data near to\nthe boundaries, demands for information outside the computational domain, which\nis carried out in terms of ghost cells, which are provided by using the\nnumerical solution of auxiliary problems. These auxiliary problems are\nhyperbolic and they are constructed from the conservation laws and the\ninformation at boundaries, which may be partially or totally known in terms of\nprescribed functions. The evolution of these problems, unlike to the usual\nmanner, is done in space rather than in time due to that these problems are\nnamed here, {\\it reverse problems}. The methodology can be considered as a\nnumerical counterpart of the inverse Lax-Wendroff procedure for filling ghost\ncells. However, the use of Taylor series expansions, as well as, Lax-Wendroff\nprocedure, are avoided. For the scalar case is shown that the present procedure\npreserve the accuracy of the scheme which is reinforced with some numerical\nresults. Expected orders of accuracy for solving conservation laws by using the\nproposed strategy at boundaries, are obtained up to fifth-order in both space\nand time."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1606.08743v2", 
    "title": "Monotonicity-preserving finite element schemes based on differentiable   nonlinear stabilization", 
    "arxiv-id": "1606.08743v2", 
    "author": "Jes\u00fas Bonilla", 
    "publish": "2016-06-28T14:46:50Z", 
    "summary": "In this work, we propose a nonlinear stabilization technique for scalar\nconservation laws with implicit time stepping. The method relies on an\nartificial diffusion method, based on a graph-Laplacian operator. It is\nnonlinear, since it depends on a shock detector. The same shock detector is\nused to gradually lump the mass matrix. The resulting method is LED, positivity\npreserving, linearity preserving, and also satisfies a global DMP. Lipschitz\ncontinuity has also been proved. However, the resulting scheme is highly\nnonlinear, leading to very poor nonlinear convergence rates. We propose a\nsmooth version of the scheme, which leads to twice differentiable nonlinear\nstabilization schemes. It allows one to straightforwardly use Newton's method\nand obtain quadratic convergence. In the numerical experiments, steady and\ntransient linear transport, and transient Burgers' equation have been\nconsidered in 2D. Using the Newton method with a smooth version of the scheme\nwe can reduce 10 to 20 times the number of iterations of Anderson acceleration\nwith the original non-smooth scheme. In any case, these properties are only\ntrue for the converged solution, but not for iterates. In this sense, we have\nalso proposed the concept of projected nonlinear solvers, where a projection\nstep is performed at the end of every nonlinear iteration onto a FE space of\nadmissible solutions. The space of admissible solutions is the one that\nsatisfies the desired monotonic properties (maximum principle or positivity)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1607.02584v1", 
    "title": "A Unified Alternating Direction Method of Multipliers by Majorization   Minimization", 
    "arxiv-id": "1607.02584v1", 
    "author": "Zhouchen Lin", 
    "publish": "2016-07-09T08:15:50Z", 
    "summary": "Accompanied with the rising popularity of compressed sensing, the Alternating\nDirection Method of Multipliers (ADMM) has become the most widely used solver\nfor linearly constrained convex problems with separable objectives. In this\nwork, we observe that many previous variants of ADMM update the primal variable\nby minimizing different majorant functions with their convergence proofs given\ncase by case. Inspired by the principle of majorization minimization, we\nrespectively present the unified frameworks and convergence analysis for the\nGauss-Seidel ADMMs and Jacobian ADMMs, which use different historical\ninformation for the current updating. Our frameworks further generalize\nprevious ADMMs to the ones capable of solving the problems with non-separable\nobjectives by minimizing their separable majorant surrogates. We also show that\nthe bound which measures the convergence speed of ADMMs depends on the\ntightness of the used majorant function. Then several techniques are introduced\nto improve the efficiency of ADMMs by tightening the majorant functions. In\nparticular, we propose the Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) which\nalleviates the slow convergence issue of Jacobian ADMMs by absorbing merits of\nthe Gauss-Seidel ADMMs. M-ADMM can be further improved by using backtracking,\nwise variable partition and fully exploiting the structure of the constraint.\nBeyond the guarantee in theory, numerical experiments on both synthesized and\nreal-world data further demonstrate the superiority of our new ADMMs in\npractice. Finally, we release a toolbox at https://github.com/canyilu/LibADMM\nthat implements efficient ADMMs for many problems in compressed sensing."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1607.04499v2", 
    "title": "Selecting Algorithms for Black Box Matrices: Checking for Matrix   Properties That Can Simplify Computations", 
    "arxiv-id": "1607.04499v2", 
    "author": "Wayne Eberly", 
    "publish": "2016-07-15T13:17:30Z", 
    "summary": "Processes to automate the selection of appropriate algorithms for various\nmatrix computations are described. In particular, processes to check for, and\ncertify, various matrix properties of black box matrices are presented. These\ninclude sparsity patterns and structural properties that allow \"superfast\"\nalgorithms to be used in place of black-box algorithms. Matrix properties that\nhold generically, and allow the use of matrix preconditioning to be reduced or\neliminated, can also be checked for and certified - notably including in the\nsmall-field case, where this presently has the greatest impact on the\nefficiency of the computation."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1607.04514v1", 
    "title": "Black Box Linear Algebra: Extending Wiedemann's Analysis of a Sparse   Matrix Preconditioner for Computations over Small Fields", 
    "arxiv-id": "1607.04514v1", 
    "author": "Wayne Eberly", 
    "publish": "2016-07-15T14:03:49Z", 
    "summary": "Wiedemann's paper, introducing his algorithm for sparse and structured matrix\ncomputations over arbitrary fields, also presented a pair of matrix\npreconditioners for computations over small fields. The analysis of the second\nof these is extended in order to provide more explicit statements of the\nexpected number of nonzero entries in the matrices obtained as well as bounds\non the probability that such matrices have maximal rank.\n  This is part of ongoing work to establish that this matrix preconditioner can\nalso be used to bound the number of nontrivial nilpotent blocks in the Jordan\nnormal form of a preconditioned matrix, in such a way that one can also sample\nuniformly from the null space of the originally given matrix. If successful\nthis will result in a black box algorithm for the type of matrix computation\nrequired when using the number field sieve for integer factorization that is\nprovably reliable and - by a small factor - asymptotically more efficient than\nalternative techniques that make use of other matrix preconditioners or require\ncomputations over field extensions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1608.00351v1", 
    "title": "Accelerated Kaczmarz Algorithms using History Information", 
    "arxiv-id": "1608.00351v1", 
    "author": "Tengfei Ma", 
    "publish": "2016-08-01T08:38:12Z", 
    "summary": "The Kaczmarz algorithm is a well known iterative method for solving\noverdetermined linear systems. Its randomized version yields provably\nexponential convergence in expectation. In this paper, we propose two new\nmethods to speed up the randomized Kaczmarz algorithm by utilizing the past\nestimates in the iterations. The first one utilize the past estimates to get a\npreconditioner. The second one combines the stochastic average gradient (SAG)\nmethod with the randomized Kaczmarz algorithm. It takes advantage of past\ngradients to improve the convergence speed. Numerical experiments indicate that\nthe new algorithms can dramatically outperform the standard randomized Kaczmarz\nalgorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1608.02166v4", 
    "title": "Analysis of time series and signals using the Square Wave Method", 
    "arxiv-id": "1608.02166v4", 
    "author": "Sherry Gapper", 
    "publish": "2016-08-07T00:48:57Z", 
    "summary": "The Square Wave Method (SWM), previously introduced for the analysis of\nsignals and images, is presented here as a mathematical tool suitable for the\nanalysis of time series and signals. To show the potential that the SWM has to\nanalyze many different types of time series, the results of the analysis of a\ntime series composed of a sequence of 10,000 numerical values are presented\nhere. These values were generated by using the Mathematical Random Number\nGenerator (MRNG)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1608.04571v1", 
    "title": "A simple algorithm to find the L-curve corner in the regularization of   inverse problems", 
    "arxiv-id": "1608.04571v1", 
    "author": "Luca Callegaro", 
    "publish": "2016-08-16T12:32:47Z", 
    "summary": "We propose a simple algorithm devoted to locate the \"corner\" of an L-curve, a\nfunction often used to chose the correct regularization parameter for the\nsolution of ill-posed problems. The algorithm involves the Menger curvature of\na circumcircle and the golden section search method. It efficiently locates the\nregularization parameter value corresponding to the maximum positive curvature\nregion of the L-curve. As an example, the application of the algorithm to the\ndata processing of an electrical resistance tomography experiment on thin\nconductive films is reported."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1609.00893v1", 
    "title": "Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale   Optimization Problems: Perspectives and Challenges PART 1", 
    "arxiv-id": "1609.00893v1", 
    "author": "D. Mandic", 
    "publish": "2016-09-04T05:12:43Z", 
    "summary": "Machine learning and data mining algorithms are becoming increasingly\nimportant in analyzing large volume, multi-relational and multi--modal\ndatasets, which are often conveniently represented as multiway arrays or\ntensors. It is therefore timely and valuable for the multidisciplinary research\ncommunity to review tensor decompositions and tensor networks as emerging tools\nfor large-scale data analysis and data mining. We provide the mathematical and\ngraphical representations and interpretation of tensor networks, with the main\nfocus on the Tucker and Tensor Train (TT) decompositions and their extensions\nor generalizations.\n  Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker\nmodels, tensor train (TT) decompositions, matrix product states (MPS), matrix\nproduct operators (MPO), basic tensor operations, multiway component analysis,\nmultilinear blind source separation, tensor completion, linear/multilinear\ndimensionality reduction, large-scale optimization problems, symmetric\neigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations,\npseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis\n(CCA) (This is Part 1)"
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1609.02258v1", 
    "title": "Tighter bound of Sketched Generalized Matrix Approximation", 
    "arxiv-id": "1609.02258v1", 
    "author": "Zhihua Zhang", 
    "publish": "2016-09-08T04:01:02Z", 
    "summary": "Generalized matrix approximation plays a fundamental role in many machine\nlearning problems, such as CUR decomposition, kernel approximation, and matrix\nlow rank approximation. Especially with today's applications involved in larger\nand larger dataset, more and more efficient generalized matrix approximation\nalgorithems become a crucially important research issue. In this paper, we find\nnew sketching techniques to reduce the size of the original data matrix to\ndevelop new matrix approximation algorithms. Our results derive a much tighter\nbound for the approximation than previous works: we obtain a $(1+\\epsilon)$\napproximation ratio with small sketched dimensions which implies a more\nefficient generalized matrix approximation."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1609.02302v1", 
    "title": "Soft Recovery Through $\\ell_{1,2}$ Minimization with Applications in   Recovery of Simultaneously Sparse and Low-Rank Matrice", 
    "arxiv-id": "1609.02302v1", 
    "author": "Axel Flinth", 
    "publish": "2016-09-08T07:28:41Z", 
    "summary": "This article provides a new type of analysis of a compressed-sensing based\ntechnique for recovering column-sparse matrices, namely minimization of the\n$\\ell_{1,2}$-norm. Rather than providing conditions on the measurement matrix\nwhich guarantees the solution of the program to be exactly equal to the ground\ntruth signal (which already has been thoroughly investigated), it presents a\ncondition which guarantees that the solution is approximately equal to the\nground truth. Soft recovery statements of this kind are to the best knowledge\nof the author a novelty in Compressed Sensing. Apart from the theoretical\nanalysis, we present two heuristic proposes how this property of the\n$\\ell_{1,2}$-program can be utilized to design algorithms for recovery of\nmatrices which are sparse and have low rank at the same time."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1609.04585v2", 
    "title": "On Memory Footprints of Partitioned Sparse Matrices", 
    "arxiv-id": "1609.04585v2", 
    "author": "Daniel Langr", 
    "publish": "2016-09-15T11:54:58Z", 
    "summary": "Runtime characteristics of sparse matrix computations and related processes\nmay be often improved by reducing memory footprints of involved matrices. Such\na reduction can be usually achieved when matrices are processed in a block-wise\nmanner. The presented study analysed memory footprints of 563 representative\nbenchmark sparse matrices with respect to their partitioning into\nuniformly-sized blocks. Different block sizes and different ways of storing\nblocks in memory were considered and statistically evaluated. Memory footprints\nof partitioned matrices were additionally compared with lower bounds and with\nthe CSR storage format. The average measured memory savings against CSR in case\nof single and double precision were 42.3 and 28.7 percents, the corresponding\nworst-case savings 25.5 and 17.1 percents. Moreover, memory footprints of\npartitioned matrices were in average 5 times closer to their lower bounds than\nCSR. Based on the obtained results, generic suggestions for efficient\npartitioning and storage of sparse matrices in a computer memory are provided."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.03764v1", 
    "title": "Technical Report: Improved Fourier Reconstruction using Jump Information   with Applications to MRI", 
    "arxiv-id": "1610.03764v1", 
    "author": "Mark Iwen", 
    "publish": "2016-10-12T15:59:52Z", 
    "summary": "Certain applications such as Magnetic Resonance Imaging (MRI) require the\nreconstruction of functions from Fourier spectral data. When the underlying\nfunctions are piecewise-smooth, standard Fourier approximation methods suffer\nfrom the Gibbs phenomenon - with associated oscillatory artifacts in the\nvicinity of edges and an overall reduced order of convergence in the\napproximation. This paper proposes an edge-augmented Fourier reconstruction\nprocedure which uses only the first few Fourier coefficients of an underlying\npiecewise-smooth function to accurately estimate jump information and then\nincorporate it into a Fourier partial sum approximation. We provide both\ntheoretical and empirical results showing the improved accuracy of the proposed\nmethod, as well as comparisons demonstrating superior performance over existing\nstate-of-the-art sparse optimization-based methods. Extensions of the proposed\ntechniques to functions of several variables are also addressed preliminarily.\nAll code used to generate the results in this report are made publicly\navailable."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.05427v2", 
    "title": "Proximal Algorithms and Temporal Differences for Large Linear Systems:   Extrapolation, Approximation, and Simulation", 
    "arxiv-id": "1610.05427v2", 
    "author": "Dimitri P. Bertsekas", 
    "publish": "2016-10-18T04:16:15Z", 
    "summary": "In this paper we consider large linear fixed point problems and solution with\nproximal algorithms. We show that, under certain assumptions, there is a close\nconnection between proximal iterations, which are prominent in numerical\nanalysis and optimization, and multistep methods of the temporal difference\ntype such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in\nsimulation-based approximate dynamic programming. As an application of this\nconnection, we show that we may accelerate the standard proximal algorithm by\nextrapolation towards the multistep iteration, which generically has a faster\nconvergence rate. We also use the connection with multistep methods to\nintegrate into the proximal algorithmic context several new ideas that have\nemerged in the approximate dynamic programming context. In particular, we\nconsider algorithms that project each proximal iterate onto the subspace\nspanned by a small number of basis functions, using low-dimensional\ncalculations and simulation, and we discuss various algorithmic options from\napproximate dynamic programming."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.07038v2", 
    "title": "Certified Roundoff Error Bounds using Bernstein Expansions and Sparse   Krivine-Stengle Representations", 
    "arxiv-id": "1610.07038v2", 
    "author": "Thao Dang", 
    "publish": "2016-10-22T12:04:12Z", 
    "summary": "Floating point error is an inevitable drawback of embedded systems\nimplementation. Computing rigorous upper bounds of roundoff errors is\nabsolutely necessary to the validation of critical software. This problem is\neven more challenging when addressing non-linear programs. In this paper, we\npropose and compare two new methods based on Bernstein expansions and sparse\nKrivine-Stengle representations, adapted from the field of the global\noptimization to compute upper bounds of roundoff errors for programs\nimplementing polynomial functions. We release two related software package\nFPBern and FPKiSten, and compare them with state of the art tools. We show that\nthese two methods achieve competitive performance, while computing accurate\nupper bounds by comparison with other tools."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.08881v1", 
    "title": "A Revisit of Block Power Methods for Finite State Markov Chain   Applications", 
    "arxiv-id": "1610.08881v1", 
    "author": "Yaohang Li", 
    "publish": "2016-10-27T16:55:27Z", 
    "summary": "In this paper, we revisit the generalized block power methods for\napproximating the eigenvector associated with $\\lambda_1 = 1$ of a Markov chain\ntransition matrix. Our analysis of the block power method shows that when $s$\nlinearly independent probability vectors are used as the initial block, the\nconvergence of the block power method to the stationary distribution depends on\nthe magnitude of the $(s+1)$th dominant eigenvalue $\\lambda_{s+1}$ of $P$\ninstead of that of $\\lambda_2$ in the power method. Therefore, the block power\nmethod with block size $s$ is particularly effective for transition matrices\nwhere $|\\lambda_{s+1}|$ is well separated from $\\lambda_1 = 1$ but\n$|\\lambda_2|$ is not. This approach is particularly useful when visiting the\nelements of a large transition matrix is the main computational bottleneck over\nmatrix--vector multiplications, where the block power method can effectively\nreduce the total number of times to pass over the matrix. To further reduce the\noverall computational cost, we combine the block power method with a sliding\nwindow scheme, taking advantage of the subsequent vectors of the latest $s$\niterations to assemble the block matrix. The sliding window scheme correlates\nvectors in the sliding window to quickly remove the influences from the\neigenvalues whose magnitudes are smaller than $|\\lambda_{s}|$ to reduce the\noverall number of matrix--vector multiplications to reach convergence. Finally,\nwe compare the effectiveness of these methods in a Markov chain model\nrepresenting a stochastic luminal calcium release site."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.09049v1", 
    "title": "Data-driven time parallelism via forecasting", 
    "arxiv-id": "1610.09049v1", 
    "author": "Andrea Barth", 
    "publish": "2016-10-28T01:17:22Z", 
    "summary": "This work proposes a data-driven method for enabling the efficient, stable\ntime-parallel numerical solution of systems of ordinary differential equations\n(ODEs). The method assumes that low-dimensional bases that accurately capture\nthe time evolution of the state are available. The method adopts the parareal\nframework for time parallelism, which is defined by an initialization method, a\ncoarse propagator, and a fine propagator. Rather than employing usual\napproaches for initialization and coarse propagation, we propose novel\ndata-driven techniques that leverage the available time-evolution bases. The\ncoarse propagator is defined by a forecast (proposed in Ref. [1]) applied\nlocally within each coarse time interval, which comprises the following steps:\n(1) apply the fine propagator for a small number of time steps, (2) approximate\nthe state over the entire coarse time interval using gappy POD with the local\ntime-evolution bases, and (3) select the approximation at the end of the time\ninterval as the propagated state. We also propose both local-forecast and\nglobal-forecast initialization. The method is particularly well suited for\nPOD-based reduced-order models (ROMs). In this case, spatial parallelism\nquickly saturates, as the ROM dynamical system is low dimensional; thus, time\nparallelism is needed to enable lower wall times. Further, the time-evolution\nbases can be extracted from the (readily available) right singular vectors\narising during POD computation. In addition to performing analyses related to\nthe method's accuracy, speedup, and stability, we also numerically demonstrate\nthe method's performance. Here, numerical experiments on ROMs for a nonlinear\nconvection-reaction problem demonstrate the method's ability to realize\nnear-ideal speedups; global-forecast initialization with a local-forecast\ncoarse propagator leads to the best performance."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.09079v1", 
    "title": "Stability analysis of the numerical Method of characteristics applied to   energy-preserving systems. Part I: Periodic boundary conditions", 
    "arxiv-id": "1610.09079v1", 
    "author": "Zihao Deng", 
    "publish": "2016-10-28T04:29:38Z", 
    "summary": "We study numerical (in)stability of the Method of characteristics (MoC)\napplied to a system of non-dissipative hyperbolic partial differential\nequations (PDEs) with periodic boundary conditions. We consider three different\nsolvers along the characteristics: simple Euler (SE), modified Euler (ME), and\nLeap-frog (LF). The two former solvers are well known to exhibit a mild, but\nunconditional, numerical instability for non-dissipative ordinary differential\nequations (ODEs). They are found to have a similar (or stronger, for the\nMoC-ME) instability when applied to non-dissipative PDEs. On the other hand,\nthe LF solver is known to be stable when applied to non-dissipative ODEs.\nHowever, when applied to non-dissipative PDEs within the MoC framework, it was\nfound to have by far the strongest instability among all three solvers. We also\ncomment on the use of the fourth-order Runge--Kutta solver within the MoC\nframework."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1610.09080v1", 
    "title": "Stability analysis of the numerical Method of characteristics applied to   energy-preserving systems. Part II: Nonreflecting boundary conditions", 
    "arxiv-id": "1610.09080v1", 
    "author": "Zihao Deng", 
    "publish": "2016-10-28T04:47:59Z", 
    "summary": "We show that imposition of non-periodic, in place of periodic, boundary\nconditions (BC) can alter stability of modes in the Method of characteristics\n(MoC) employing certain ordinary-differential equation (ODE) numerical solvers.\nThus, using non-periodic BC may render some of the MoC schemes stable for most\npractical computations, even though they are unstable for periodic BC. We\nexplain how this fact is related to a statement, found in some literature, that\nan instability detected by the von Neumann analysis for a given numerical\nscheme implies an instability of that scheme with arbitrary (i.e.,\nnon-periodic) BC. We also show that, and explain why, for the MoC employing\nsome other ODE solvers, stability of the modes may be unaffected by the BC."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1611.05800v1", 
    "title": "Splitting schemes for unsteady problems involving the grad-div operator", 
    "arxiv-id": "1611.05800v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2016-11-17T17:41:16Z", 
    "summary": "In this paper we consider various splitting schemes for unsteady problems\ncontaining the grad-div operator. The fully implicit discretization of such\nproblems would yield at each time step a linear problem that couples all\ncomponents of the solution vector. In this paper we discuss various\npossibilities to decouple the equations for the different components that\nresult in unconditionally stable schemes. If the spatial discretization uses\nCartesian grids, the resulting schemes are Locally One Dimensional (LOD). The\nstability analysis of these schemes is based on the general stability theory of\nadditive operator-difference schemes developed by Samarskii and his\ncollaborators. The results of the theoretical analysis are illustrated on a 2D\nnumerical example with a smooth manufactured solution."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1611.09379v1", 
    "title": "Fast Multipole Method based filtering of non-uniformly sampled data", 
    "arxiv-id": "1611.09379v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2016-11-28T21:06:31Z", 
    "summary": "Non-uniform fast Fourier Transform (NUFFT) and inverse NUFFT (INUFFT)\nalgorithms, based on the Fast Multipole Method (FMM) are developed and tested.\nOur algorithms are based on a novel factorization of the FFT kernel, and are\nimplemented with attention to data structures and error analysis.\n  Note: This unpublished manuscript was available on our web pages and has been\nreferred to by others in the literature. To provide a proper archival reference\nwe are placing it on arXiv."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1612.02153v1", 
    "title": "Revisiting Hammel et al. (1987): Does the shadowing property hold for   modern computers?", 
    "arxiv-id": "1612.02153v1", 
    "author": "G. F. V. Amaral", 
    "publish": "2016-12-07T08:53:49Z", 
    "summary": "Computational techniques are extensively applied in nonlinear science.\nHowever, while the use of computers for research has been expressive, the\nevaluation of numerical results does not grow in the same pace. Hammel et al.\n(Journal of Complexity, 1987, 3(2), 136--145) were pioneers in the numerical\nreliability field and have proved a theorem that a pseudo-orbit of a logistic\nmap is shadowed by a true orbit within a distance of $10^{-8}$ for $10^{7}$\niterates. But the simulation of the logistic map with less than 100 iterates\npresents an error greater than $10^{-8}$ in a modern computer, performing a\ntest based on the concept of multiple pseudo-orbits and symbolic computing."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1612.04542v1", 
    "title": "Approximate Fast Fourier Transforms on graphs via multi-layer sparse   approximations", 
    "arxiv-id": "1612.04542v1", 
    "author": "Nicolas Tremblay", 
    "publish": "2016-12-14T09:09:16Z", 
    "summary": "The Fast Fourier Transform (FFT) is an algorithm of paramount importance in\nsignal processing as it allows to apply the Fourier transform in O(n log n)\ninstead of O(n 2) arithmetic operations. Graph Signal Processing (GSP) is a\nrecent research domain that generalizes classical signal processing tools, such\nas the Fourier transform, to situations where the signal domain is given by any\narbitrary graph instead of a regular grid. Today, there is no method to rapidly\napply graph Fourier transforms. We propose in this paper a method to obtain\napproximate graph Fourier transforms that can be applied rapidly. It is based\non a greedy approximate diagonalization of the graph Laplacian matrix, carried\nout using a modified version of the famous Jacobi eigenvalues algorithm. The\nmethod is described and analyzed in details, and then applied to both synthetic\nand real graphs, showing its potential."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1701.01780v1", 
    "title": "Spectral Statistics of Lattice Graph Structured, Non-uniform   Percolations", 
    "arxiv-id": "1701.01780v1", 
    "author": "Jos\u00e9 M. F. Moura", 
    "publish": "2017-01-06T23:57:07Z", 
    "summary": "Design of filters for graph signal processing benefits from knowledge of the\nspectral decomposition of matrices that encode graphs, such as the adjacency\nmatrix and the Laplacian matrix, used to define the shift operator. For shift\nmatrices with real eigenvalues, which arise for symmetric graphs, the empirical\nspectral distribution captures the eigenvalue locations. Under realistic\ncircumstances, stochastic influences often affect the network structure and,\nconsequently, the shift matrix empirical spectral distribution. Nevertheless,\ndeterministic functions may often be found to approximate the asymptotic\nbehavior of empirical spectral distributions of random matrices. This paper\nuses stochastic canonical equation methods developed by Girko to derive such\ndeterministic equivalent distributions for the empirical spectral distributions\nof random graphs formed by structured, non-uniform percolation of a\nD-dimensional lattice supergraph. Included simulations demonstrate the results\nfor sample parameters."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1701.03720v1", 
    "title": "Multivariate predictions of local reduced-order-model errors and   dimensions", 
    "arxiv-id": "1701.03720v1", 
    "author": "Adrian Sandu", 
    "publish": "2017-01-13T16:49:25Z", 
    "summary": "This paper introduces multivariate input-output models to predict the errors\nand bases dimensions of local parametric Proper Orthogonal Decomposition\nreduced-order models. We refer to these multivariate mappings as the MP-LROM\nmodels. We employ Gaussian Processes and Artificial Neural Networks to\nconstruct approximations of these multivariate mappings. Numerical results with\na viscous Burgers model illustrate the performance and potential of the machine\nlearning based regression MP-LROM models to approximate the characteristics of\nparametric local reduced-order models. The predicted reduced-order models\nerrors are compared against the multi-fidelity correction and reduced order\nmodel error surrogates methods predictions, whereas the predicted reduced-order\ndimensions are tested against the standard method based on the spectrum of\nsnapshots matrix. Since the MP-LROM models incorporate more features and\nelements to construct the probabilistic mappings they achieve more accurate\nresults. However, for high-dimensional parametric spaces, the MP-LROM models\nmight suffer from the curse of dimensionality. Scalability challenges of\nMP-LROM models and the feasible ways of addressing them are also discussed in\nthis study."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1701.06528v2", 
    "title": "EPIRK-W and EPIRK-K time discretization methods", 
    "arxiv-id": "1701.06528v2", 
    "author": "Mayya Tokman", 
    "publish": "2017-01-23T17:55:50Z", 
    "summary": "Exponential integrators are special time discretization methods where the\ntraditional linear system solves used by implicit schemes are replaced with\ncomputing the action of matrix exponential-like functions on a vector. A very\ngeneral formulation of exponential integrators is offered by the Exponential\nPropagation Iterative methods of Runge-Kutta type (EPIRK) family of schemes.\nThe use of Jacobian approximations is an important strategy to drastically\nreduce the overall computational costs of implicit schemes while maintaining\nthe quality of their solutions. This paper extends the EPIRK class to allow the\nuse of inexact Jacobians as arguments of the matrix exponential-like functions.\nSpecifically, we develop two new families of methods: EPIRK-W integrators that\ncan accommodate any approximation of the Jacobian, and EPIRK-K integrators that\nrely on a specific Krylov-subspace projection of the exact Jacobian. Classical\norder conditions theories are constructed for these families. A practical\nEPIRK-W method of order three and an EPIRK-K method of order four are\ndeveloped. Numerical experiments indicate that the methods proposed herein are\ncomputationally favorable when compared to existing exponential integrators."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1701.06600v1", 
    "title": "A Practical Randomized CP Tensor Decomposition", 
    "arxiv-id": "1701.06600v1", 
    "author": "Tamara G. Kolda", 
    "publish": "2017-01-23T19:37:35Z", 
    "summary": "The CANDECOMP/PARAFAC (CP) decomposition is a leading method for the analysis\nof multiway data. The standard alternating least squares algorithm for the CP\ndecomposition (CP-ALS) involves a series of highly overdetermined linear least\nsquares problems. We extend randomized least squares methods to tensors and\nshow the workload of CP-ALS can be drastically reduced without a sacrifice in\nquality. We introduce techniques for efficiently preprocessing, sampling, and\ncomputing randomized least squares on a dense tensor of arbitrary order, as\nwell as an efficient sampling-based technique for checking the stopping\ncondition. We also show more generally that the Khatri-Rao product (used within\nthe CP-ALS iteration) produces conditions favorable for direct sampling. In\nnumerical results, we see improvements in speed, reductions in memory\nrequirements, and robustness with respect to initialization."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1702.08124v1", 
    "title": "A Unifying Framework for Convergence Analysis of Approximate Newton   Methods", 
    "arxiv-id": "1702.08124v1", 
    "author": "Zhihua Zhang", 
    "publish": "2017-02-27T02:07:39Z", 
    "summary": "Many machine learning models are reformulated as optimization problems. Thus,\nit is important to solve a large-scale optimization problem in big data\napplications. Recently, subsampled Newton methods have emerged to attract much\nattention for optimization due to their efficiency at each iteration, rectified\na weakness in the ordinary Newton method of suffering a high cost in each\niteration while commanding a high convergence rate. Other efficient stochastic\nsecond order methods are also proposed. However, the convergence properties of\nthese methods are still not well understood. There are also several important\ngaps between the current convergence theory and the performance in real\napplications. In this paper, we aim to fill these gaps. We propose a unifying\nframework to analyze local convergence properties of second order methods.\nBased on this framework, our theoretical analysis matches the performance in\nreal applications."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1703.00279v1", 
    "title": "Systematic Generation of Algorithms for Iterative Methods", 
    "arxiv-id": "1703.00279v1", 
    "author": "Henrik Barthels", 
    "publish": "2017-03-01T13:09:19Z", 
    "summary": "The FLAME methodology makes it possible to derive provably correct algorithms\nfrom a formal description of a linear algebra problem. So far, the methodology\nhas been successfully used to automate the derivation of direct algorithms such\nas the Cholesky decomposition and the solution of Sylvester equations. In this\nthesis, we present an extension of the FLAME methodology to tackle iterative\nmethods such as Conjugate Gradient. As a starting point, we use a formal\ndescription of the iterative method in matrix form. The result is a family of\nprovably correct pseudocode algorithms. We argue that all the intermediate\nsteps are sufficiently systematic to be fully automated."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1703.02283v1", 
    "title": "Using Approximate Computing for the Calculation of Inverse Matrix p-th   Roots", 
    "arxiv-id": "1703.02283v1", 
    "author": "Christian Plessl", 
    "publish": "2017-03-07T08:57:53Z", 
    "summary": "Approximate computing has shown to provide new ways to improve performance\nand power consumption of error-resilient applications. While many of these\napplications can be found in image processing, data classification or machine\nlearning, we demonstrate its suitability to a problem from scientific\ncomputing. Utilizing the self-correcting behavior of iterative algorithms, we\nshow that approximate computing can be applied to the calculation of inverse\nmatrix p-th roots which are required in many applications in scientific\ncomputing. Results show great opportunities to reduce the computational effort\nand bandwidth required for the execution of the discussed algorithm, especially\nwhen targeting special accelerator hardware."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1703.04560v1", 
    "title": "Space-time least-squares Petrov-Galerkin projection for nonlinear model   reduction", 
    "arxiv-id": "1703.04560v1", 
    "author": "Kevin Carlberg", 
    "publish": "2017-03-14T01:27:37Z", 
    "summary": "This work proposes a space-time least-squares Petrov-Galerkin (ST-LSPG)\nprojection method for model reduction of nonlinear dynamical systems. In\ncontrast to typical nonlinear model-reduction methods that first apply\n(Petrov-)Galerkin projection in the spatial dimension and subsequently apply\ntime integration to numerically resolve the resulting low-dimensional dynamical\nsystem, the proposed method applies projection in space and time\nsimultaneously. To accomplish this, the method first introduces a\nlow-dimensional space-time trial subspace, which can be obtained by computing\ntensor decompositions of state-snapshot data. The method then computes\ndiscrete-optimal approximations in this space-time trial subspace by minimizing\nthe residual arising after time discretization over all space and time in a\nweighted $\\ell^2$-norm. This norm can be defined to enable complexity reduction\n(i.e., hyper-reduction) in time, which leads to space-time collocation and\nspace-time GNAT variants of the ST-LSPG method. Advantages of the approach\nrelative to typical spatial-projection-based nonlinear model reduction methods\nsuch as Galerkin projection and least-squares Petrov-Galerkin projection\ninclude: (1) a reduction of both the spatial and temporal dimensions of the\ndynamical system, (2) the removal of spurious temporal modes (e.g., unstable\ngrowth) from the state space, and (2) error bounds that exhibit slower growth\nin time. Numerical examples performed on model problems in fluid dynamics\ndemonstrate the ability of the method to generate orders-of-magnitude\ncomputational savings relative to spatial-projection-based reduced-order models\nwithout sacrificing accuracy."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2016.09.035", 
    "link": "http://arxiv.org/pdf/1703.05487v1", 
    "title": "Accelerated and Inexact Soft-Impute for Large-Scale Matrix and Tensor   Completion", 
    "arxiv-id": "1703.05487v1", 
    "author": "James T. Kwok", 
    "publish": "2017-03-16T06:50:00Z", 
    "summary": "Matrix and tensor completion aim to recover a low-rank matrix / tensor from\nlimited observations and have been commonly used in applications such as\nrecommender systems and multi-relational data mining. A state-of-the-art matrix\ncompletion algorithm is Soft-Impute, which exploits the special \"sparse plus\nlow-rank\" structure of the matrix iterates to allow efficient SVD in each\niteration. Though Soft-Impute is a proximal algorithm, it is generally believed\nthat acceleration destroys the special structure and is thus not useful. In\nthis paper, we show that Soft-Impute can indeed be accelerated without\ncomprising this structure. To further reduce the iteration time complexity, we\npropose an approximate singular value thresholding scheme based on the power\nmethod. Theoretical analysis shows that the proposed algorithm still enjoys the\nfast $O(1/T^2)$ convergence rate of accelerated proximal algorithms. We further\nextend the proposed algorithm to tensor completion with the scaled latent\nnuclear norm regularizer. We show that a similar \"sparse plus low-rank\"\nstructure also exists, leading to low iteration complexity and fast $O(1/T^2)$\nconvergence rate. Extensive experiments demonstrate that the proposed algorithm\nis much faster than Soft-Impute and other state-of-the-art matrix and tensor\ncompletion algorithms."
},{
    "category": "astro-ph", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/astro-ph/0605514v2", 
    "title": "An algorithm for solving the pulsar equation", 
    "arxiv-id": "astro-ph/0605514v2", 
    "author": "Marcin Kolonko", 
    "publish": "2006-05-20T13:23:36Z", 
    "summary": "We present an algorithm of finding numerical solutions of pulsar equation.\nThe problem of finding the solutions was reduced to finding expansion\ncoefficients of the source term of the equation in a base of orthogo- nal\nfunctions defined on the unit interval by minimizing a multi-variable mismatch\nfunction defined on the light cylinder. We applied the algorithm to Scharlemann\n& Wagoner boundary conditions by which a smooth solu- tion is reconstructed\nthat by construction passes success- fully the Gruzinov's test of the source\nfunction exponent."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9904003v1", 
    "title": "The Structure of Weighting Coefficient Matrices of Harmonic Differential   Quadrature and Its Applications", 
    "arxiv-id": "cs/9904003v1", 
    "author": "T. Zhong", 
    "publish": "1999-04-11T08:21:13Z", 
    "summary": "The structure of weighting coefficient matrices of Harmonic Differential\nQuadrature (HDQ) is found to be either centrosymmetric or skew centrosymmetric\ndepending on the order of the corresponding derivatives. The properties of both\nmatrices are briefly discussed in this paper. It is noted that the\ncomputational effort of the harmonic quadrature for some problems can be\nfurther reduced up to 75 per cent by using the properties of the\nabove-mentioned matrices."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9904006v1", 
    "title": "Jacobian matrix: a bridge between linear and nonlinear polynomial-only   problems", 
    "arxiv-id": "cs/9904006v1", 
    "author": "W. Chen", 
    "publish": "1999-04-15T12:31:21Z", 
    "summary": "By using the Hadamard matrix product concept, this paper introduces two\ngeneralized matrix formulation forms of numerical analogue of nonlinear\ndifferential operators. The SJT matrix-vector product approach is found to be a\nsimple, efficient and accurate technique in the calculation of the Jacobian\nmatrix of the nonlinear discretization by finite difference, finite volume,\ncollocation, dual reciprocity BEM or radial functions based numerical methods.\nWe also present and prove simple underlying relationship (theorem (3.1))\nbetween general nonlinear analogue polynomials and their corresponding Jacobian\nmatrices, which forms the basis of this paper. By means of theorem 3.1,\nstability analysis of numerical solutions of nonlinear initial value problems\ncan be easily handled based on the well-known results for linear problems.\nTheorem 3.1 also leads naturally to the straightforward extension of various\nlinear iterative algorithms such as the SOR, Gauss-Seidel and Jacobi methods to\nnonlinear algebraic equations. Since an exact alternative of the quasi-Newton\nequation is established via theorem 3.1, we derive a modified BFGS quasi-Newton\nmethod. A simple formula is also given to examine the deviation between the\napproximate and exact Jacobian matrices. Furthermore, in order to avoid the\nevaluation of the Jacobian matrix and its inverse, the pseudo-Jacobian matrix\nis introduced with a general applicability of any nonlinear systems of\nequations. It should be pointed out that a large class of real-world nonlinear\nproblems can be modeled or numerically discretized polynomial-only algebraic\nsystem of equations. The results presented here are in general applicable for\nall these problems. This paper can be considered as a starting point in the\nresearch of nonlinear computation and analysis from an innovative viewpoint."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9904007v1", 
    "title": "The Study on the Nonlinear Computations of the DQ and DC Methods", 
    "arxiv-id": "cs/9904007v1", 
    "author": "Tingxiu Zhong", 
    "publish": "1999-04-15T13:24:56Z", 
    "summary": "This paper points out that the differential quadrature (DQ) and differential\ncubature (DC) methods due to their global domain property are more efficient\nfor nonlinear problems than the traditional numerical techniques such as finite\nelement and finite difference methods. By introducing the Hadamard product of\nmatrices, we obtain an explicit matrix formulation for the DQ and DC solutions\nof nonlinear differential and integro-differential equations. Due to its\nsimplicity and flexibility, the present Hadamard product approach makes the DQ\nand DC methods much easier to be used. Many studies on the Hadamard product can\nbe fully exploited for the DQ and DC nonlinear computations. Furthermore, we\nfirst present SJT product of matrix and vector to compute accurately and\nefficiently the Frechet derivative matrix in the Newton-Raphson method for the\nsolution of the nonlinear formulations. We also propose a simple approach to\nsimplify the DQ or DC formulations for some nonlinear differential operators\nand thus the computational efficiency of these methods is improved\nsignificantly. We give the matrix multiplication formulas to compute\nefficiently the weighting coefficient matrices of the DC method. The spherical\nharmonics are suggested as the test functions in the DC method to handle the\nnonlinear differential equations occurring in global and hemispheric weather\nforecasting problems. Some examples are analyzed to demonstrate the simplicity\nand efficiency of the presented techniques. It is emphasized that innovations\npresented are applicable to the nonlinear computations of the other numerical\nmethods as well."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9904021v1", 
    "title": "Hadamard product nonlinear formulation of Galerkin and finite element   methods", 
    "arxiv-id": "cs/9904021v1", 
    "author": "W. Chen", 
    "publish": "1999-04-28T13:12:47Z", 
    "summary": "A novel nonlinear formulation of the finite element and Galerkin methods is\npresented here, which leads to the Hadamard product expression of the resultant\nnonlinear algebraic analogue. The presented formulation attains the advantages\nof weak formulation in the standard finite element and Galerkin schemes and\navoids the costly repeated numerical integration of the Jacobian matrix via the\nrecently developed SJT product approach. This also provides possibility of the\nnonlinear decoupling computations."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9906011v1", 
    "title": "A Newton method without evaluation of nonlinear function values", 
    "arxiv-id": "cs/9906011v1", 
    "author": "W. Chen", 
    "publish": "1999-06-09T12:27:03Z", 
    "summary": "The present author recently proposed and proved a relationship theorem\nbetween nonlinear polynomial equations and the corresponding Jacobian matrix.\nBy using this theorem, this paper derives a Newton iterative formula without\nrequiring the evaluation of nonlinear function values in the solution of\nnonlinear polynomial-only problems."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9906012v1", 
    "title": "The application of special matrix product to differential quadrature   solution of geometrically nonlinear bending of orthotropic rectangular plates", 
    "arxiv-id": "cs/9906012v1", 
    "author": "W. He", 
    "publish": "1999-06-09T12:47:13Z", 
    "summary": "The Hadamard and SJT product of matrices are two types of special matrix\nproduct. The latter was first defined by Chen. In this study, they are applied\nto the differential quadrature (DQ) solution of geometrically nonlinear bending\nof isotropic and orthotropic rectangular plates. By using the Hadamard product,\nthe nonlinear formulations are greatly simplified, while the SJT product\napproach minimizes the effort to evaluate the Jacobian derivative matrix in the\nNewton-Raphson method for solving the resultant nonlinear formulations. In\naddition, the coupled nonlinear formulations for the present problems can\neasily be decoupled by means of the Hadamard and SJT product. Therefore, the\nsize of the simultaneous nonlinear algebraic equations is reduced by two-thirds\nand the computing effort and storage requirements are alleviated greatly. Two\nrecent approaches applying the multiple boundary conditions are employed in the\npresent DQ nonlinear computations. The solution accuracies are improved\nobviously in comparison to the previously given by Bert et al. The numerical\nresults and detailed solution procedures are provided to demonstrate the superb\nefficiency, accuracy and simplicity of the new approaches in applying DQ method\nfor nonlinear computations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9907015v2", 
    "title": "Linear-Time Approximation Algorithms for Computing Numerical Summation   with Provably Small Errors", 
    "arxiv-id": "cs/9907015v2", 
    "author": "Jie Wang", 
    "publish": "1999-07-09T18:23:17Z", 
    "summary": "Given a multiset $X=\\{x_1,..., x_n\\}$ of real numbers, the {\\it\nfloating-point set summation} problem asks for $S_n=x_1+...+x_n$. Let $E^*_n$\ndenote the minimum worst-case error over all possible orderings of evaluating\n$S_n$. We prove that if $X$ has both positive and negative numbers, it is\nNP-hard to compute $S_n$ with the worst-case error equal to $E^*_n$. We then\ngive the first known polynomial-time approximation algorithm that has a\nprovably small error for arbitrary $X$. Our algorithm incurs a worst-case error\nat most $2(\\mix)E^*_n$.\\footnote{All logarithms $\\log$ in this paper are base\n2.} After $X$ is sorted, it runs in O(n) time. For the case where $X$ is either\nall positive or all negative, we give another approximation algorithm with a\nworst-case error at most $\\lceil\\log\\log n\\rceil E^*_n$. Even for unsorted $X$,\nthis algorithm runs in O(n) time. Previously, the best linear-time\napproximation algorithm had a worst-case error at most $\\lceil\\log n\\rceil\nE^*_n$, while $E^*_n$ was known to be attainable in $O(n \\log n)$ time using\nHuffman coding."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/9907020v2", 
    "title": "Generalized linearization in nonlinear modeling of data", 
    "arxiv-id": "cs/9907020v2", 
    "author": "W. Chen", 
    "publish": "1999-07-12T11:34:56Z", 
    "summary": "The principal innovative idea in this paper is to transform the original\ncomplex nonlinear modeling problem into a combination of linear problem and\nvery simple nonlinear problems. The key step is the generalized linearization\nof nonlinear terms. This paper only presents the introductory strategy of this\nmethodology. The practical numerical experiments will be provided subsequently."
},{
    "category": "cs.SD", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0006026v2", 
    "title": "Online Correction of Dispersion Error in 2D Waveguide Meshes", 
    "arxiv-id": "cs/0006026v2", 
    "author": "Davide Rocchesso", 
    "publish": "2000-06-12T06:08:57Z", 
    "summary": "An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by\nmodels discretizing the wave equation on the time-space grid (finite difference\nmethods), or locally discretizing the solution of the wave equation (waveguide\nmeshes). The two approaches provide equivalent computational structures, and\nintroduce numerical dispersion that induces a misalignment of the modes from\ntheir theoretical positions. Prior literature shows that dispersion can be\narbitrarily reduced by oversizing and oversampling the mesh, or by adpting\noffline warping techniques. In this paper we propose to reduce numerical\ndispersion by embedding warping elements, i.e., properly tuned allpass filters,\nin the structure. The resulting model exhibits a significant reduction in\ndispersion, and requires less computational resources than a regular mesh\nstructure having comparable accuracy."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0007002v2", 
    "title": "Interval Constraint Solving for Camera Control and Motion Planning", 
    "arxiv-id": "cs/0007002v2", 
    "author": "Marc Christie", 
    "publish": "2000-07-03T17:03:39Z", 
    "summary": "Many problems in robust control and motion planning can be reduced to either\nfind a sound approximation of the solution space determined by a set of\nnonlinear inequalities, or to the ``guaranteed tuning problem'' as defined by\nJaulin and Walter, which amounts to finding a value for some tuning parameter\nsuch that a set of inequalities be verified for all the possible values of some\nperturbation vector. A classical approach to solve these problems, which\nsatisfies the strong soundness requirement, involves some quantifier\nelimination procedure such as Collins' Cylindrical Algebraic Decomposition\nsymbolic method. Sound numerical methods using interval arithmetic and local\nconsistency enforcement to prune the search space are presented in this paper\nas much faster alternatives for both soundly solving systems of nonlinear\ninequalities, and addressing the guaranteed tuning problem whenever the\nperturbation vector has dimension one. The use of these methods in camera\ncontrol is investigated, and experiments with the prototype of a declarative\nmodeller to express camera motion using a cinematic language are reported and\ncommented."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0101003v1", 
    "title": "Signal-Theoretic Characterization of Waveguide Mesh Geometries for   Models of Two--Dimensional Wave Propagation in Elastic Media", 
    "arxiv-id": "cs/0101003v1", 
    "author": "Davide Rocchesso", 
    "publish": "2001-01-04T07:41:59Z", 
    "summary": "Waveguide Meshes are efficient and versatile models of wave propagation along\na multidimensional ideal medium. The choice of the mesh geometry affects both\nthe computational cost and the accuracy of simulations. In this paper, we focus\non 2D geometries and use multidimensional sampling theory to compare the\nsquare, triangular, and hexagonal meshes in terms of sampling efficiency and\ndispersion error under conditions of critical sampling. The analysis shows that\nthe triangular geometry exhibits the most desirable tradeoff between accuracy\nand computational cost."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0104018v1", 
    "title": "Several new domain-type and boundary-type numerical discretization   schemes with radial basis function", 
    "arxiv-id": "cs/0104018v1", 
    "author": "W. Chen", 
    "publish": "2001-04-23T18:50:45Z", 
    "summary": "This paper is concerned with a few novel RBF-based numerical schemes\ndiscretizing partial differential equations. For boundary-type methods, we\nderive the indirect and direct symmetric boundary knot methods (BKM). The\nresulting interpolation matrix of both is always symmetric irrespective of\nboundary geometry and conditions. In particular, the direct BKM applies the\npractical physical variables rather than expansion coefficients and becomes\nvery competitive to the boundary element method. On the other hand, based on\nthe multiple reciprocity principle, we invent the RBF-based boundary particle\nmethod (BPM) for general inhomogeneous problems without a need using inner\nnodes. The direct and symmetric BPM schemes are also developed.\n  For domain-type RBF discretization schemes, by using the Green integral we\ndevelop a new Hermite RBF scheme called as the modified Kansa method (MKM),\nwhich differs from the symmetric Hermite RBF scheme in that the MKM discretizes\nboth governing equation and boundary conditions on the same boundary nodes. The\nlocal spline version of the MKM is named as the finite knot method (FKM). Both\nMKM and FKM significantly reduce calculation errors at nodes adjacent to\nboundary. In addition, the nonsingular high-order fundamental or general\nsolution is strongly recommended as the RBF in the domain-type methods and dual\nreciprocity method approximation of particular solution relating to the BKM.\n  It is stressed that all the above discretization methods of boundary-type and\ndomain-type are symmetric, meshless, and integration-free. The spline-based\nschemes will produce desirable symmetric sparse banded interpolation matrix. In\nappendix, we present a Hermite scheme to eliminate edge effect on the RBF\ngeometric modeling and imaging."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0105014v1", 
    "title": "Errata and supplements to: Orthonormal RBF Wavelet and Ridgelet-like   Series and Transforms for High-Dimensional Problems", 
    "arxiv-id": "cs/0105014v1", 
    "author": "W. Chen", 
    "publish": "2001-05-07T16:53:05Z", 
    "summary": "In recent years some attempts have been done to relate the RBF with wavelets\nin handling high dimensional multiscale problems. To the author's knowledge,\nhowever, the orthonormal and bi-orthogonal RBF wavelets are still missing in\nthe literature. By using the nonsingular general solution and singular\nfundamental solution of differential operator, recently the present author,\nrefer. 3, made some substantial headway to derive the orthonormal RBF wavelets\nseries and transforms. The methodology can be generalized to create the RBF\nwavelets by means of the orthogonal convolution kernel function of various\nintegral operators. In particular, it is stressed that the presented RBF\nwavelets does not apply the tensor product to handle multivariate problems at\nall.\n  This note is to correct some errata in reference 3 and also to supply a few\nlatest advances in the study of orthornormal RBF wavelet transforms."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0110055v1", 
    "title": "Analytical solution of transient scalar wave and diffusion problems of   arbitrary dimensionality and geometry by RBF wavelet series", 
    "arxiv-id": "cs/0110055v1", 
    "author": "W. Chen", 
    "publish": "2001-10-28T10:28:55Z", 
    "summary": "This study applies the RBF wavelet series to the evaluation of analytical\nsolutions of linear time-dependent wave and diffusion problems of any\ndimensionality and geometry. To the best of the author's knowledge, such\nanalytical solutions have never been achieved before. The RBF wavelets can be\nunderstood an alternative for multidimensional problems to the standard Fourier\nseries via fundamental and general solutions of partial differential equation.\nThe present RBF wavelets are infinitely differential, compactly supported,\northogonal over different scales and very simple. The rigorous mathematical\nproof of completeness and convergence is still missing in this study. The\npresent work may open a new window to numerical solution and theoretical\nanalysis of many other high-dimensional time-dependent PDE problems under\narbitrary geometry."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0111063v1", 
    "title": "New RBF collocation methods and kernel RBF with applications", 
    "arxiv-id": "cs/0111063v1", 
    "author": "W. Chen", 
    "publish": "2001-11-30T16:03:00Z", 
    "summary": "A few novel radial basis function (RBF) discretization schemes for partial\ndifferential equations are developed in this study. For boundary-type methods,\nwe derive the indirect and direct symmetric boundary knot methods. Based on the\nmultiple reciprocity principle, the boundary particle method is introduced for\ngeneral inhomogeneous problems without using inner nodes. For domain-type\nschemes, by using the Green integral we develop a novel Hermite RBF scheme\ncalled the modified Kansa method, which significantly reduces calculation\nerrors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the\nleast square RBF collocation scheme. Finally, five types of the kernel RBF are\nalso briefly presented."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0112008v1", 
    "title": "Representation of Uncertainty for Limit Processes", 
    "arxiv-id": "cs/0112008v1", 
    "author": "Mark Burgin", 
    "publish": "2001-12-07T23:56:51Z", 
    "summary": "Many mathematical models utilize limit processes. Continuous functions and\nthe calculus, differential equations and topology, all are based on limits and\ncontinuity. However, when we perform measurements and computations, we can\nachieve only approximate results. In some cases, this discrepancy between\ntheoretical schemes and practical actions changes drastically outcomes of a\nresearch and decision-making resulting in uncertainty of knowledge. In the\npaper, a mathematical approach to such kind of uncertainty, which emerges in\ncomputation and measurement, is suggested on the base of the concept of a fuzzy\nlimit. A mathematical technique is developed for differential models with\nuncertainty. To take into account the intrinsic uncertainty of a model, it is\nsuggested to use fuzzy derivatives instead of conventional derivatives of\nfunctions in this model."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0205019v1", 
    "title": "Distance function wavelets - Part I: Helmholtz and convection-diffusion   transforms and series", 
    "arxiv-id": "cs/0205019v1", 
    "author": "W. Chen", 
    "publish": "2002-05-14T13:43:47Z", 
    "summary": "This report aims to present my research updates on distance function wavelets\n(DFW) based on the fundamental solutions and the general solutions of the\nHelmholtz, modified Helmholtz, and convection-diffusion equations, which\ninclude the isotropic Helmholtz-Fourier (HF) transform and series, the\nHelmholtz-Laplace (HL) transform, and the anisotropic convection-diffusion\nwavelets and ridgelets. The latter is set to handle discontinuous and track\ndata problems. The edge effect of the HF series is addressed. Alternative\nexistence conditions for the DFW transforms are proposed and discussed. To\nsimplify and streamline the expression of the HF and HL transforms, a new\ndimension-dependent function notation is introduced. The HF series is also used\nto evaluate the analytical solutions of linear diffusion problems of arbitrary\ndimensionality and geometry. The weakness of this report is lacking of rigorous\nmathematical analysis due to the author's limited mathematical knowledge."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0206010v2", 
    "title": "Performance Comparison of Function Evaluation Methods", 
    "arxiv-id": "cs/0206010v2", 
    "author": "Leo Liberti", 
    "publish": "2002-06-06T09:10:39Z", 
    "summary": "We perform a comparison of the performance and efficiency of four different\nfunction evaluation methods: black-box functions, binary trees, $n$-ary trees\nand string parsing. The test consists in evaluating 8 different functions of\ntwo variables $x,y$ over 5000 floating point values of the pair $(x,y)$. The\noutcome of the test indicates that the $n$-ary tree representation of algebraic\nexpressions is the fastest method, closely followed by black-box function\nmethod, then by binary trees and lastly by string parsing."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0207062v1", 
    "title": "Some addenda on distance function wavelets", 
    "arxiv-id": "cs/0207062v1", 
    "author": "W. Chen", 
    "publish": "2002-07-15T19:58:27Z", 
    "summary": "This report will add some supplements to the recently finished report series\non the distance function wavelets (DFW). First, we define the general distance\nin terms of the Riesz potential, and then, the distance function Abel wavelets\nare derived via the fractional integral and Laplacian. Second, the DFW Weyl\ntransform is found to be a shifted Laplace potential DFW. The DFW Radon\ntransform is also presented. Third, we present a conjecture on truncation error\nformula of the multiple reciprocity Laplace DFW series and discuss its error\ndistributions in terms of node density distributions. Forth, we point out that\nthe Hermite distance function interpolation can be used to replace overlapping\nin the domain decomposition in order to produce sparse matrix. Fifth, the shape\nparameter is explained as a virtual extra axis contribution in terms of the\nMQ-type Possion kernel. The report is concluded with some remarks on a range of\nother issues."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0209020v1", 
    "title": "A new definition of the fractional Laplacian", 
    "arxiv-id": "cs/0209020v1", 
    "author": "W. Chen", 
    "publish": "2002-09-18T12:45:43Z", 
    "summary": "It is noted that the standard definition of the fractional Laplacian leads to\na hyper-singular convolution integral and is also obscure about how to\nimplement the boundary conditions. This purpose of this note is to introduce a\nnew definition of the fractional Laplacian to overcome these major drawbacks."
},{
    "category": "cs.LO", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0211016v4", 
    "title": "Efficient Solving of Quantified Inequality Constraints over the Real   Numbers", 
    "arxiv-id": "cs/0211016v4", 
    "author": "Stefan Ratschan", 
    "publish": "2002-11-14T11:13:30Z", 
    "summary": "Let a quantified inequality constraint over the reals be a formula in the\nfirst-order predicate language over the structure of the real numbers, where\nthe allowed predicate symbols are $\\leq$ and $<$. Solving such constraints is\nan undecidable problem when allowing function symbols such $\\sin$ or $\\cos$. In\nthe paper we give an algorithm that terminates with a solution for all, except\nfor very special, pathological inputs. We ensure the practical efficiency of\nthis algorithm by employing constraint programming techniques."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0302011v2", 
    "title": "Smoothed Analysis of Interior-Point Algorithms: Condition Number", 
    "arxiv-id": "cs/0302011v2", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-02-10T06:14:26Z", 
    "summary": "We show that the smoothed complexity of the logarithm of Renegar's condition\nnumber is O(log (n/sigma))."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0307033v1", 
    "title": "Excellence in Computer Simulation", 
    "arxiv-id": "cs/0307033v1", 
    "author": "Leo P. Kadanoff", 
    "publish": "2003-07-13T21:15:42Z", 
    "summary": "Excellent computer simulations are done for a purpose. The most valid\npurposes are to explore uncharted territory, to resolve a well-posed scientific\nor technical question, or to make a design choice. Stand-alone modeling can\nserve the first purpose. The other two goals need a full integration of the\nmodeling effort into a scientific or engineering program.\n  Some excellent work, much of it related to the Department of Energy\nLaboratories, is reviewed. Some less happy stories are recounted.\n  In the past, some of the most impressive work has involved complexity and\nchaos. Prediction in a complex world requires a first principles understanding\nbased upon the intersection of theory, experiment and simulation."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0308021v1", 
    "title": "A Bernstein-Bezier Sufficient Condition for Invertibility of Polynomial   Mapping Functions", 
    "arxiv-id": "cs/0308021v1", 
    "author": "Stephen Vavasis", 
    "publish": "2003-08-11T18:43:35Z", 
    "summary": "We propose a sufficient condition for invertibility of a polynomial mapping\nfunction defined on a cube or simplex. This condition is applicable to finite\nelement analysis using curved meshes. The sufficient condition is based on an\nanalysis of the Bernstein-B\\'ezier form of the columns of the derivative."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0310022v4", 
    "title": "Smoothed Analysis of the Condition Numbers and Growth Factors of   Matrices", 
    "arxiv-id": "cs/0310022v4", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-10-12T04:06:09Z", 
    "summary": "Let $\\orig{A}$ be any matrix and let $A$ be a slight random perturbation of\n$\\orig{A}$. We prove that it is unlikely that $A$ has large condition number.\nUsing this result, we prove it is unlikely that $A$ has large growth factor\nunder Gaussian elimination without pivoting. By combining these results, we\nbound the smoothed precision needed by Gaussian elimination without pivoting.\nOur results improve the average-case analysis of Gaussian elimination without\npivoting performed by Yeung and Chan (SIAM J. Matrix Anal. Appl., 1997)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0310036v2", 
    "title": "Solving Sparse, Symmetric, Diagonally-Dominant Linear Systems in Time $O   (m^{1.31})$", 
    "arxiv-id": "cs/0310036v2", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-10-17T15:43:01Z", 
    "summary": "We present a linear-system solver that, given an $n$-by-$n$ symmetric\npositive semi-definite, diagonally dominant matrix $A$ with $m$ non-zero\nentries and an $n$-vector $\\bb $, produces a vector $\\xxt$ within relative\ndistance $\\epsilon$ of the solution to $A \\xx = \\bb$ in time $O (m^{1.31} \\log\n(n \\kappa_{f} (A)/\\epsilon)^{O (1)})$, where $\\kappa_{f} (A)$ is the log of the\nratio of the largest to smallest non-zero eigenvalue of $A$. In particular,\n$\\log (\\kappa_{f} (A)) = O (b \\log n)$, where $b$ is the logarithm of the ratio\nof the largest to smallest non-zero entry of $A$. If the graph of $A$ has genus\n$m^{2\\theta}$ or does not have a $K_{m^{\\theta}} $ minor, then the exponent of\n$m$ can be improved to the minimum of $1 + 5 \\theta $ and $(9/8) (1+\\theta)$.\nThe key contribution of our work is an extension of Vaidya's techniques for\nconstructing and analyzing combinatorial preconditioners."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0310051v10", 
    "title": "Nearly-Linear Time Algorithms for Graph Partitioning, Graph   Sparsification, and Solving Linear Systems", 
    "arxiv-id": "cs/0310051v10", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-10-28T09:23:25Z", 
    "summary": "This paper has been divided into three papers. arXiv:0809.3232,\narXiv:0808.4134, arXiv:cs/0607105"
},{
    "category": "cs.CG", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0407018v2", 
    "title": "An algorithm for two-dimensional mesh generation based on the pinwheel   tiling", 
    "arxiv-id": "cs/0407018v2", 
    "author": "Katerina D. Papoulia", 
    "publish": "2004-07-07T19:06:15Z", 
    "summary": "We propose a new two-dimensional meshing algorithm called PINW able to\ngenerate meshes that accurately approximate the distance between any two domain\npoints by paths composed only of cell edges. This technique is based on an\nextension of pinwheel tilings proposed by Radin and Conway. We prove that the\nalgorithm produces triangles of bounded aspect ratio. This kind of mesh would\nbe useful in cohesive interface finite element modeling when the crack\npropagation pathis an outcome of a simulation process."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0409033v4", 
    "title": "Mean and Variance Estimation by Kriging", 
    "arxiv-id": "cs/0409033v4", 
    "author": "Tomasz Suslo", 
    "publish": "2004-09-17T10:39:45Z", 
    "summary": "The aim of the paper is to derive the numerical least-squares estimator for\nmean and variance of random variable. In order to do so the following questions\nhave to be answered: (i) what is the statistical model for the estimation\nprocedure? (ii) what are the properties of the estimator, like optimality (in\nwhich class) or asymptotic properties? (iii) how does the estimator work in\npractice, how compared to competing estimators?"
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0411004v1", 
    "title": "Computational Aspects of a Numerical Model for Combustion Flow", 
    "arxiv-id": "cs/0411004v1", 
    "author": "Gianluca Argentini", 
    "publish": "2004-11-02T11:37:44Z", 
    "summary": "A computational method for numeric resolution of a PDEs system, based on a\nFinite Differences schema integrated by interpolations of partial results, and\nan estimate of the error of its solution respect to the normal FD solution."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0501065v1", 
    "title": "Harmonic Analysis", 
    "arxiv-id": "cs/0501065v1", 
    "author": "Vladimir I Clue", 
    "publish": "2005-01-24T00:01:16Z", 
    "summary": "This paper describes a method of calculating the transforms, currently\nobtained via Fourier and reverse Fourier transforms. The method allows\ncalculating efficiently the transforms of a signal having an arbitrary\ndimension of the digital representation by reducing the transform to a\nvector-to-circulant matrix multiplying. There is a connection between harmonic\nequations in rectangular and polar coordinate systems. The connection\nestablished here and used to create a very robust iterative algorithm for a\nconformal mapping calculation. There is also suggested a new ratio (and an\nefficient way of computing it) of two oscillative signals."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0505015v4", 
    "title": "Complex Mean and Variance of Linear Regression Model for High-Noised   Systems by Kriging", 
    "arxiv-id": "cs/0505015v4", 
    "author": "Tomasz Suslo", 
    "publish": "2005-05-07T12:11:56Z", 
    "summary": "The aim of the paper is to derive the complex-valued least-squares estimator\nfor bias-noise mean and variance."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s10509-007-9406-y", 
    "link": "http://arxiv.org/pdf/cs/0505080v1", 
    "title": "Dominance Based Crossover Operator for Evolutionary Multi-objective   Algorithms", 
    "arxiv-id": "cs/0505080v1", 
    "author": "Marc Schoenauer", 
    "publish": "2005-05-29T18:40:44Z", 
    "summary": "In spite of the recent quick growth of the Evolutionary Multi-objective\nOptimization (EMO) research field, there has been few trials to adapt the\ngeneral variation operators to the particular context of the quest for the\nPareto-optimal set. The only exceptions are some mating restrictions that take\nin account the distance between the potential mates - but contradictory\nconclusions have been reported. This paper introduces a particular mating\nrestriction for Evolutionary Multi-objective Algorithms, based on the Pareto\ndominance relation: the partner of a non-dominated individual will be\npreferably chosen among the individuals of the population that it dominates.\nCoupled with the BLX crossover operator, two different ways of generating\noffspring are proposed. This recombination scheme is validated within the\nwell-known NSGA-II framework on three bi-objective benchmark problems and one\nreal-world bi-objective constrained optimization problem. An acceleration of\nthe progress of the population toward the Pareto set is observed on all\nproblems."
},{
    "category": "cs.NA", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0605058v1", 
    "title": "A Monadic, Functional Implementation of Real Numbers", 
    "arxiv-id": "cs/0605058v1", 
    "author": "Russell O'Connor", 
    "publish": "2006-05-14T17:05:14Z", 
    "summary": "Large scale real number computation is an essential ingredient in several\nmodern mathematical proofs. Because such lengthy computations cannot be\nverified by hand, some mathematicians want to use software proof assistants to\nverify the correctness of these proofs. This paper develops a new\nimplementation of the constructive real numbers and elementary functions for\nsuch proofs by using the monad properties of the completion operation on metric\nspaces. Bishop and Bridges's notion of regular sequences is generalized to,\nwhat I call, regular functions which form the completion of any metric space.\nUsing the monad operations, continuous functions on length spaces (a common\nsubclass of metric spaces) are created by lifting continuous functions on the\noriginal space. A prototype Haskell implementation has been created. I believe\nthat this approach yields a real number library that is reasonably efficient\nfor computation, and still simple enough to easily verify its correctness."
},{
    "category": "cs.DC", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0607032v1", 
    "title": "Asymptotic Analysis of a Leader Election Algorithm", 
    "arxiv-id": "cs/0607032v1", 
    "author": "Guy Louchard", 
    "publish": "2006-07-08T08:46:32Z", 
    "summary": "Itai and Rodeh showed that, on the average, the communication of a leader\nelection algorithm takes no more than $LN$ bits, where $L \\simeq 2.441716$ and\n$N$ denotes the size of the ring. We give a precise asymptotic analysis of the\naverage number of rounds M(n) required by the algorithm, proving for example\nthat $\\dis M(\\infty) := \\lim\\_{n\\to \\infty} M(n) = 2.441715879...$, where $n$\nis the number of starting candidates in the election. Accurate asymptotic\nexpressions of the second moment $M^{(2)}(n)$ of the discrete random variable\nat hand, its probability distribution, and the generalization to all moments\nare given. Corresponding asymptotic expansions $(n\\to \\infty)$ are provided for\nsufficiently large $j$, where $j$ counts the number of rounds. Our numerical\nresults show that all computations perfectly fit the observed values. Finally,\nwe investigate the generalization to probability $t/n$, where $t$ is a non\nnegative real parameter. The real function $\\dis M(\\infty,t) := \\lim\\_{n\\to\n\\infty} M(n,t)$ is shown to admit \\textit{one unique minimum} $M(\\infty,t^{*})$\non the real segment $(0,2)$. Furthermore, the variations of $M(\\infty,t)$ on\nthewhole real line are also studied in detail."
},{
    "category": "cs.NA", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0607105v5", 
    "title": "Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric,   Diagonally Dominant Linear Systems", 
    "arxiv-id": "cs/0607105v5", 
    "author": "Shang-Hua Teng", 
    "publish": "2006-07-24T04:02:24Z", 
    "summary": "We present a randomized algorithm that, on input a symmetric, weakly\ndiagonally dominant n-by-n matrix A with m nonzero entries and an n-vector b,\nproduces a y such that $\\norm{y - \\pinv{A} b}_{A} \\leq \\epsilon \\norm{\\pinv{A}\nb}_{A}$ in expected time $O (m \\log^{c}n \\log (1/\\epsilon)),$ for some constant\nc. By applying this algorithm inside the inverse power method, we compute\napproximate Fiedler vectors in a similar amount of time. The algorithm applies\nsubgraph preconditioners in a recursive fashion. These preconditioners improve\nupon the subgraph preconditioners first introduced by Vaidya (1990).\n  For any symmetric, weakly diagonally-dominant matrix A with non-positive\noff-diagonal entries and $k \\geq 1$, we construct in time $O (m \\log^{c} n)$ a\npreconditioner B of A with at most $2 (n - 1) + O ((m/k) \\log^{39} n)$ nonzero\noff-diagonal entries such that the finite generalized condition number\n$\\kappa_{f} (A,B)$ is at most k, for some other constant c.\n  In the special case when the nonzero structure of the matrix is planar the\ncorresponding linear system solver runs in expected time $ O (n \\log^{2} n + n\n\\log n \\ \\log \\log n \\ \\log (1/\\epsilon))$.\n  We hope that our introduction of algorithms of low asymptotic complexity will\nlead to the development of algorithms that are also fast in practice."
},{
    "category": "cs.NA", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0608090v2", 
    "title": "A Condition Number Analysis of a Line-Surface Intersection Algorithm", 
    "arxiv-id": "cs/0608090v2", 
    "author": "Stephen A. Vavasis", 
    "publish": "2006-08-23T03:47:07Z", 
    "summary": "We propose an algorithm based on Newton's method and subdivision for finding\nall zeros of a polynomial system in a bounded region of the plane. This\nalgorithm can be used to find the intersections between a line and a surface,\nwhich has applications in graphics and computer-aided geometric design. The\nalgorithm can operate on polynomials represented in any basis that satisfies a\nfew conditions. The power basis, the Bernstein basis, and the first-kind\nChebyshev basis are among those compatible with the algorithm. The main novelty\nof our algorithm is an analysis showing that its running is bounded only in\nterms of the condition number of the polynomial's zeros and a constant\ndepending on the polynomial basis."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0609001v2", 
    "title": "A Robust Solution Procedure for Hyperelastic Solids with Large Boundary   Deformation", 
    "arxiv-id": "cs/0609001v2", 
    "author": "Stephen A. Vavasis", 
    "publish": "2006-09-01T00:07:41Z", 
    "summary": "Compressible Mooney-Rivlin theory has been used to model hyperelastic solids,\nsuch as rubber and porous polymers, and more recently for the modeling of soft\ntissues for biomedical tissues, undergoing large elastic deformations. We\npropose a solution procedure for Lagrangian finite element discretization of a\nstatic nonlinear compressible Mooney-Rivlin hyperelastic solid. We consider the\ncase in which the boundary condition is a large prescribed deformation, so that\nmesh tangling becomes an obstacle for straightforward algorithms. Our solution\nprocedure involves a largely geometric procedure to untangle the mesh: solution\nof a sequence of linear systems to obtain initial guesses for interior nodal\npositions for which no element is inverted. After the mesh is untangled, we\ntake Newton iterations to converge to a mechanical equilibrium. The Newton\niterations are safeguarded by a line search similar to one used in\noptimization. Our computational results indicate that the algorithm is up to 70\ntimes faster than a straightforward Newton continuation procedure and is also\nmore robust (i.e., able to tolerate much larger deformations). For a few\nextremely large deformations, the deformed mesh could only be computed through\nthe use of an expensive Newton continuation method while using a tight\nconvergence tolerance and taking very small steps."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0609079v4", 
    "title": "Modern Statistics by Kriging", 
    "arxiv-id": "cs/0609079v4", 
    "author": "Tomasz Suslo", 
    "publish": "2006-09-14T12:34:59Z", 
    "summary": "We present statistics (S-statistics) based only on random variable (not\nrandom value) with a mean squared error of mean estimation as a concept of\nerror."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0609081v1", 
    "title": "Recurrence relations and fast algorithms", 
    "arxiv-id": "cs/0609081v1", 
    "author": "Mark Tygert", 
    "publish": "2006-09-14T17:51:11Z", 
    "summary": "We construct fast algorithms for evaluating transforms associated with\nfamilies of functions which satisfy recurrence relations. These include\nalgorithms both for computing the coefficients in linear combinations of the\nfunctions, given the values of these linear combinations at certain points,\nand, vice versa, for evaluating such linear combinations at those points, given\nthe coefficients in the linear combinations; such procedures are also known as\nanalysis and synthesis of series of certain special functions. The algorithms\nof the present paper are efficient in the sense that their computational costs\nare proportional to n (ln n) (ln(1/epsilon))^3, where n is the amount of input\nand output data, and epsilon is the precision of computations. Stated somewhat\nmore precisely, we find a positive real number C such that, for any positive\ninteger n > 10, the algorithms require at most C n (ln n) (ln(1/epsilon))^3\nfloating-point operations and words of memory to evaluate at n appropriately\nchosen points any linear combination of n special functions, given the\ncoefficients in the linear combination, where epsilon is the precision of\ncomputations."
},{
    "category": "cs.CR", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0610036v3", 
    "title": "Optimization of Memory Usage in Tardos's Fingerprinting Codes", 
    "arxiv-id": "cs/0610036v3", 
    "author": "Hideki Imai", 
    "publish": "2006-10-06T14:07:57Z", 
    "summary": "It is known that Tardos's collusion-secure probabilistic fingerprinting code\n(Tardos code; STOC'03) has length of theoretically minimal order with respect\nto the number of colluding users. However, Tardos code uses certain continuous\nprobability distribution in codeword generation, which creates some problems\nfor practical use, in particular, it requires large extra memory. A solution\nproposed so far is to use some finite probability distributions instead. In\nthis paper, we determine the optimal finite distribution in order to decrease\nextra memory amount. By our result, the extra memory is reduced to 1/32 of the\noriginal, or even becomes needless, in some practical setting. Moreover, the\ncode length is also reduced, e.g. to about 20.6% of Tardos code asymptotically.\nFinally, we address some other practical issues such as approximation errors\nwhich are inevitable in any real implementation."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0610108v2", 
    "title": "Doppler Spectrum Estimation by Ramanujan Fourier Transforms", 
    "arxiv-id": "cs/0610108v2", 
    "author": "Messaoud Bensebti", 
    "publish": "2006-10-18T14:43:45Z", 
    "summary": "The Doppler spectrum estimation of a weather radar signal in a classic way\ncan be made by two methods, temporal one based in the autocorrelation of the\nsuccessful signals, whereas the other one uses the estimation of the power\nspectral density PSD by using Fourier transforms. We introduces a new tool of\nsignal processing based on Ramanujan sums cq(n), adapted to the analysis of\narithmetical sequences with several resonances p/q. These sums are almost\nperiodic according to time n of resonances and aperiodic according to the order\nq of resonances. New results will be supplied by the use of Ramanujan Fourier\nTransform (RFT) for the estimation of the Doppler spectrum for the weather\nradar signal."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0610122v1", 
    "title": "Faithful Polynomial Evaluation with Compensated Horner Algorithm", 
    "arxiv-id": "cs/0610122v1", 
    "author": "Nicolas Louvet", 
    "publish": "2006-10-20T11:22:52Z", 
    "summary": "This paper presents two sufficient conditions to ensure a faithful evaluation\nof polynomial in IEEE-754 floating point arithmetic. Faithfulness means that\nthe computed value is one of the two floating point neighbours of the exact\nresult; it can be satisfied using a more accurate algorithm than the classic\nHorner scheme. One condition here provided is an apriori bound of the\npolynomial condition number derived from the error analysis of the compensated\nHorner algorithm. The second condition is both dynamic and validated to check\nat the running time the faithfulness of a given evaluation. Numerical\nexperiments illustrate the behavior of these two conditions and that associated\nrunning time over-cost is really interesting."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0611049v2", 
    "title": "On numerical stability of recursive present value computation method", 
    "arxiv-id": "cs/0611049v2", 
    "author": "Argyn Kuketayev", 
    "publish": "2006-11-13T20:37:42Z", 
    "summary": "We analyze numerical stability of a recursive computation scheme of present\nvalue (PV) amd show that the absolute error increases exponentially for\npositive discount rates. We show that reversing the direction of calculations\nin the recurrence equation yields a robust PV computation routine."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0611061v1", 
    "title": "Multivariate Integral Perturbation Techniques - I (Theory)", 
    "arxiv-id": "cs/0611061v1", 
    "author": "Jan W. Dash", 
    "publish": "2006-11-14T16:58:58Z", 
    "summary": "We present a quasi-analytic perturbation expansion for multivariate\nN-dimensional Gaussian integrals. The perturbation expansion is an infinite\nseries of lower-dimensional integrals (one-dimensional in the simplest\napproximation). This perturbative idea can also be applied to multivariate\nStudent-t integrals. We evaluate the perturbation expansion explicitly through\n2nd order, and discuss the convergence, including enhancement using Pade\napproximants. Brief comments on potential applications in finance are given,\nincluding options, models for credit risk and derivatives, and correlation\nsensitivities."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0701183v1", 
    "title": "Certification of the QR factor R, and of lattice basis reducedness", 
    "arxiv-id": "cs/0701183v1", 
    "author": "Gilles Villard", 
    "publish": "2007-01-29T09:15:35Z", 
    "summary": "Given a lattice basis of n vectors in Z^n, we propose an algorithm using\n12n^3+O(n^2) floating point operations for checking whether the basis is\nLLL-reduced. If the basis is reduced then the algorithm will hopefully answer\n''yes''. If the basis is not reduced, or if the precision used is not\nsufficient with respect to n, and to the numerical properties of the basis, the\nalgorithm will answer ''failed''. Hence a positive answer is a rigorous\ncertificate. For implementing the certificate itself, we propose a floating\npoint algorithm for computing (certified) error bounds for the entries of the R\nfactor of the QR matrix factorization. This algorithm takes into account all\npossible approximation and rounding errors. The cost 12n^3+O(n^2) of the\ncertificate is only six times more than the cost of numerical algorithms for\ncomputing the QR factorization itself, and the certificate may be implemented\nusing matrix library routines only. We report experiments that show that for a\nreduced basis of adequate dimension and quality the certificate succeeds, and\nestablish the effectiveness of the certificate. This effectiveness is applied\nfor certifying the output of fastest existing floating point heuristics of LLL\nreduction, without slowing down the whole process."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s00366-011-0225-y", 
    "link": "http://arxiv.org/pdf/cs/0701188v1", 
    "title": "Faster Inversion and Other Black Box Matrix Computations Using Efficient   Block Projections", 
    "arxiv-id": "cs/0701188v1", 
    "author": "Gilles Villard", 
    "publish": "2007-01-29T18:20:30Z", 
    "summary": "Block projections have been used, in [Eberly et al. 2006], to obtain an\nefficient algorithm to find solutions for sparse systems of linear equations. A\nbound of softO(n^(2.5)) machine operations is obtained assuming that the input\nmatrix can be multiplied by a vector with constant-sized entries in softO(n)\nmachine operations. Unfortunately, the correctness of this algorithm depends on\nthe existence of efficient block projections, and this has been conjectured. In\nthis paper we establish the correctness of the algorithm from [Eberly et al.\n2006] by proving the existence of efficient block projections over sufficiently\nlarge fields. We demonstrate the usefulness of these projections by deriving\nimproved bounds for the cost of several matrix problems, considering, in\nparticular, ``sparse'' matrices that can be be multiplied by a vector using\nsoftO(n) field operations. We show how to compute the inverse of a sparse\nmatrix over a field F using an expected number of softO(n^(2.27)) operations in\nF. A basis for the null space of a sparse matrix, and a certification of its\nrank, are obtained at the same cost. An application to Kaltofen and Villard's\nBaby-Steps/Giant-Steps algorithms for the determinant and Smith Form of an\ninteger matrix yields algorithms requiring softO(n^(2.66)) machine operations.\nThe derived algorithms are all probabilistic of the Las Vegas type."
},{
    "category": "cs.PL", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0701192v5", 
    "title": "The pitfalls of verifying floating-point computations", 
    "arxiv-id": "cs/0701192v5", 
    "author": "David Monniaux", 
    "publish": "2007-01-30T16:26:50Z", 
    "summary": "Current critical systems commonly use a lot of floating-point computations,\nand thus the testing or static analysis of programs containing floating-point\noperators has become a priority. However, correctly defining the semantics of\ncommon implementations of floating-point is tricky, because semantics may\nchange with many factors beyond source-code level, such as choices made by\ncompilers. We here give concrete examples of problems that can appear and\nsolutions to implement in analysis software."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702163v1", 
    "title": "First Passage Time for Multivariate Jump-diffusion Stochastic Models   With Applications in Finance", 
    "arxiv-id": "cs/0702163v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T10:39:15Z", 
    "summary": "The ``first passage-time'' (FPT) problem is an important problem with a wide\nrange of applications in mathematics, physics, biology and finance.\nMathematically, such a problem can be reduced to estimating the probability of\na (stochastic) process first to reach a critical level or threshold. While in\nother areas of applications the FPT problem can often be solved analytically,\nin finance we usually have to resort to the application of numerical\nprocedures, in particular when we deal with jump-diffusion stochastic processes\n(JDP). In this paper, we develop a Monte-Carlo-based methodology for the\nsolution of the FPT problem in the context of a multivariate jump-diffusion\nstochastic process. The developed methodology is tested by using different\nparameters, the simulation results indicate that the developed methodology is\nmuch more efficient than the conventional Monte Carlo method. It is an\nefficient tool for further practical applications, such as the analysis of\ndefault correlation and predicting barrier options in finance."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702164v1", 
    "title": "Monte-Carlo Simulations of the First Passage Time for Multivariate   Jump-Diffusion Processes in Financial Applications", 
    "arxiv-id": "cs/0702164v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T10:51:16Z", 
    "summary": "Many problems in finance require the information on the first passage time\n(FPT) of a stochastic process. Mathematically, such problems are often reduced\nto the evaluation of the probability density of the time for such a process to\ncross a certain level, a boundary, or to enter a certain region. While in other\nareas of applications the FPT problem can often be solved analytically, in\nfinance we usually have to resort to the application of numerical procedures,\nin particular when we deal with jump-diffusion stochastic processes (JDP). In\nthis paper, we propose a Monte-Carlo-based methodology for the solution of the\nfirst passage time problem in the context of multivariate (and correlated)\njump-diffusion processes. The developed technique provide an efficient tool for\na number of applications, including credit risk and option pricing. We\ndemonstrate its applicability to the analysis of the default rates and default\ncorrelations of several different, but correlated firms via a set of empirical\ndata."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702165v1", 
    "title": "Efficient estimation of default correlation for multivariate   jump-diffusion processes", 
    "arxiv-id": "cs/0702165v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T11:22:18Z", 
    "summary": "Evaluation of default correlation is an important task in credit risk\nanalysis. In many practical situations, it concerns the joint defaults of\nseveral correlated firms, the task that is reducible to a first passage time\n(FPT) problem. This task represents a great challenge for jump-diffusion\nprocesses (JDP), where except for very basic cases, there are no analytical\nsolutions for such problems. In this contribution, we generalize our previous\nfast Monte-Carlo method (non-correlated jump-diffusion cases) for multivariate\n(and correlated) jump-diffusion processes. This generalization allows us, among\nother things, to evaluate the default events of several correlated assets based\non a set of empirical data. The developed technique is an efficient tool for a\nnumber of other applications, including credit risk and option pricing."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702166v1", 
    "title": "Solving Stochastic Differential Equations with Jump-Diffusion   Efficiently: Applications to FPT Problems in Credit Risk", 
    "arxiv-id": "cs/0702166v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T11:48:12Z", 
    "summary": "The first passage time (FPT) problem is ubiquitous in many applications. In\nfinance, we often have to deal with stochastic processes with jump-diffusion,\nso that the FTP problem is reducible to a stochastic differential equation with\njump-diffusion. While the application of the conventional Monte-Carlo procedure\nis possible for the solution of the resulting model, it becomes computationally\ninefficient which severely restricts its applicability in many practically\ninteresting cases. In this contribution, we focus on the development of\nefficient Monte-Carlo-based computational procedures for solving the FPT\nproblem under the multivariate (and correlated) jump-diffusion processes. We\nalso discuss the implementation of the developed Monte-Carlo-based technique\nfor multivariate jump-diffusion processes driving by several compound Poisson\nshocks. Finally, we demonstrate the application of the developed methodologies\nfor analyzing the default rates and default correlations of differently rated\nfirms via historical data."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702167v1", 
    "title": "Finite Volume Analysis of Nonlinear Thermo-mechanical Dynamics of Shape   Memory Alloys", 
    "arxiv-id": "cs/0702167v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T13:00:33Z", 
    "summary": "In this paper, the finite volume method is developed to analyze coupled\ndynamic problems of nonlinear thermoelasticity. The major focus is given to the\ndescription of martensitic phase transformations essential in the modelling of\nshape memory alloys. Computational experiments are carried out to study the\nthermo-mechanical wave interactions in a shape memory alloy rod, and a patch.\nBoth mechanically and thermally induced phase transformations, as well as\nhysteresis effects, in a one-dimensional structure are successfully simulated\nwith the developed methodology. In the two-dimensional case, the main focus is\ngiven to square-to-rectangular transformations and examples of martensitic\ncombinations under different mechanical loadings are provided."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702168v1", 
    "title": "Simulation of Phase Combinations in Shape Memory Alloys Patches by   Hybrid Optimization Methods", 
    "arxiv-id": "cs/0702168v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T14:09:42Z", 
    "summary": "In this paper, phase combinations among martensitic variants in shape memory\nalloys patches and bars are simulated by a hybrid optimization methodology. The\nmathematical model is based on the Landau theory of phase transformations. Each\nstable phase is associated with a local minimum of the free energy function,\nand the phase combinations are simulated by minimizing the bulk energy. At low\ntemperature, the free energy function has double potential wells leading to\nnon-convexity of the optimization problem. The methodology proposed in the\npresent paper is based on an initial estimate of the global solution by a\ngenetic algorithm, followed by a refined quasi-Newton procedure to locally\nrefine the optimum. By combining the local and global search algorithms, the\nphase combinations are successfully simulated. Numerical experiments are\npresented for the phase combinations in a SMA patch under several typical\nmechanical loadings."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/cs/0702172v1", 
    "title": "Numerical Model For Vibration Damping Resulting From the First Order   Phase Transformations", 
    "arxiv-id": "cs/0702172v1", 
    "author": "Roderick V. N. Melnik", 
    "publish": "2007-02-28T18:31:19Z", 
    "summary": "A numerical model is constructed for modelling macroscale damping effects\ninduced by the first order martensite phase transformations in a shape memory\nalloy rod. The model is constructed on the basis of the modified\nLandau-Ginzburg theory that couples nonlinear mechanical and thermal fields.\nThe free energy function for the model is constructed as a double well function\nat low temperature, such that the external energy can be absorbed during the\nphase transformation and converted into thermal form. The Chebyshev spectral\nmethods are employed together with backward differentiation for the numerical\nanalysis of the problem. Computational experiments performed for different\nvibration energies demonstrate the importance of taking into account damping\neffects induced by phase transformations."
},{
    "category": "nlin.CD", 
    "doi": "10.1145/1353445.1353446", 
    "link": "http://arxiv.org/pdf/nlin/0405038v3", 
    "title": "When Chaos Meets Computers", 
    "arxiv-id": "nlin/0405038v3", 
    "author": "Shujun Li", 
    "publish": "2004-05-14T12:21:10Z", 
    "summary": "This paper focuses on an interesting phenomenon when chaos meets computers.\nIt is found that digital computers are absolutely incapable of showing true\nlong-time dynamics of some chaotic systems, including the tent map, the\nBernoulli shift map and their analogues, even in a high-precision\nfloating-point arithmetic. Although the results cannot directly generalized to\nmost chaotic systems, the risk of using digital computers to numerically study\ncontinuous dynamical systems is shown clearly. As a result, we reach the old\nsaying that \"it is impossible to do everything with computers only\"."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1063/1.168616", 
    "link": "http://arxiv.org/pdf/physics/0011053v1", 
    "title": "Faster Evaluation of Multidimensional Integrals", 
    "arxiv-id": "physics/0011053v1", 
    "author": "J. F. Traub", 
    "publish": "2000-11-21T18:16:12Z", 
    "summary": "In a recent paper Keister proposed two quadrature rules as alternatives to\nMonte Carlo for certain multidimensional integrals and reported his test\nresults. In earlier work we had shown that the quasi-Monte Carlo method with\ngeneralized Faure points is very effective for a variety of high dimensional\nintegrals occurng in mathematical finance. In this paper we report test results\nof this method on Keister's examples of dimension 9 and 25, and also for\nexamples of dimension 60, 80 and 100. For the 25 dimensional integral we\nachieved accuracy of 0.01 with less than 500 points while the two methods\ntested by Keister used more than 220,000 points. In all of our tests, for n\nsample points we obtained an empirical convergence rate proportional to n^{-1}\nrather than the n^{-1/2} of Monte Carlo."
},{
    "category": "quant-ph", 
    "doi": "10.1063/1.168616", 
    "link": "http://arxiv.org/pdf/quant-ph/0001077v3", 
    "title": "Poly-locality in quantum computing", 
    "arxiv-id": "quant-ph/0001077v3", 
    "author": "Michael H. Freedman", 
    "publish": "2000-01-21T00:14:18Z", 
    "summary": "A polynomial depth quantum circuit effects, by definition a poly-local\nunitary transformation of tensor product state space. It is a physically\nreasonable belief [Fy][L][FKW] that these are precisely the transformations\nwhich will be available from physics to help us solve computational problems.\nThe poly-locality of discrete Fourier transform on cyclic groups is at the\nheart of Shor's factoring algorithm. We describe a class of poly-local\ntransformations, including all the discrete orthogonal wavelet transforms in\nthe hope that these may be helpful in constructing new quantum algorithms. We\nalso observe that even a rather mild violation of poly-locality leads to a\nmodel without one-way functions, giving further evidence that poly-locality is\nan essential concept."
},{
    "category": "quant-ph", 
    "doi": "10.1063/1.168616", 
    "link": "http://arxiv.org/pdf/quant-ph/0304206v2", 
    "title": "Harmonic inversion helps to beat time-energy uncertainty relations", 
    "arxiv-id": "quant-ph/0304206v2", 
    "author": "Zbyszek P. Karkuszewski", 
    "publish": "2003-04-30T16:17:29Z", 
    "summary": "It is impossible to obtain accurate frequencies from time signals of a very\nshort duration. This is a common believe among contemporary physicists. Here I\npresent a practical way of extracting energies to a high precision from very\nshort time signals produced by a quantum system. The product of time span of\nthe signal and the precision of found energies is well bellow the limit imposed\nby the time-energy uncertainty relation."
},{
    "category": "cs.MS", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0705.2626v1", 
    "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in   hypre and PETSc", 
    "arxiv-id": "0705.2626v1", 
    "author": "E. E. Ovtchinnikov", 
    "publish": "2007-05-18T02:25:16Z", 
    "summary": "We describe our software package Block Locally Optimal Preconditioned\nEigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as\na stand-alone serial library, as an external package to PETSc (``Portable,\nExtensible Toolkit for Scientific Computation'', a general purpose suite of\ntools for the scalable solution of partial differential equations and related\nproblems developed by Argonne National Laboratory), and is also built into {\\it\nhypre} (``High Performance Preconditioners'', scalable linear solvers package\ndeveloped by Lawrence Livermore National Laboratory). The present BLOPEX\nrelease includes only one solver--the Locally Optimal Block Preconditioned\nConjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it\nhypre} provides users with advanced high-quality parallel preconditioners for\nlinear systems, in particular, with domain decomposition and multigrid\npreconditioners. With BLOPEX, the same preconditioners can now be efficiently\nused for symmetric eigenvalue problems. PETSc facilitates the integration of\nindependently developed application modules with strict attention to component\ninteroperability, and makes BLOPEX extremely easy to compile and use with\npreconditioners that are available via PETSc. We present the LOBPCG algorithm\nin BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability\nof BLOPEX by testing it on a number of distributed and shared memory parallel\nsystems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron\nworkstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition\nand {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem,\nthe standard 7-point finite-difference approximation of the 3-D Laplacian, with\nthe problem size in the range $10^5-10^8$."
},{
    "category": "cs.NA", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0705.4369v1", 
    "title": "Computing Integer Powers in Floating-Point Arithmetic", 
    "arxiv-id": "0705.4369v1", 
    "author": "Jean-Michel Muller", 
    "publish": "2007-05-30T11:34:39Z", 
    "summary": "We introduce two algorithms for accurately evaluating powers to a positive\ninteger in floating-point arithmetic, assuming a fused multiply-add (fma)\ninstruction is available. We show that our log-time algorithm always produce\nfaithfully-rounded results, discuss the possibility of getting correctly\nrounded results, and show that results correctly rounded in double precision\ncan be obtained if extended-precision is available with the possibility to\nround into double precision (with a single rounding)."
},{
    "category": "cs.PL", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0706.0252v1", 
    "title": "Applying the Z-transform for the static analysis of floating-point   numerical filters", 
    "arxiv-id": "0706.0252v1", 
    "author": "David Monniaux", 
    "publish": "2007-06-02T06:18:48Z", 
    "summary": "Digital linear filters are used in a variety of applications (sound\ntreatment, control/command, etc.), implemented in software, in hardware, or a\ncombination thereof. For safety-critical applications, it is necessary to bound\nall variables and outputs of all filters. We give a compositional, effective\nabstraction for digital linear filters expressed as block diagrams, yielding\nsound, precise bounds for fixed-point or floating-point implementations of the\nfilters."
},{
    "category": "cs.NA", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0707.1515v5", 
    "title": "Properties of polynomial bases used in a line-surface intersection   algorithm", 
    "arxiv-id": "0707.1515v5", 
    "author": "Stephen A. Vavasis", 
    "publish": "2007-07-10T18:56:05Z", 
    "summary": "In [5], Srijuntongsiri and Vavasis propose the \"Kantorovich-Test Subdivision\nalgorithm\", or KTS, which is an algorithm for finding all zeros of a polynomial\nsystem in a bounded region of the plane. This algorithm can be used to find the\nintersections between a line and a surface. The main features of KTS are that\nit can operate on polynomials represented in any basis that satisfies certain\nconditions and that its efficiency has an upper bound that depends only on the\nconditioning of the problem and the choice of the basis representing the\npolynomial system.\n  This article explores in detail the dependence of the efficiency of the KTS\nalgorithm on the choice of basis. Three bases are considered: the power, the\nBernstein, and the Chebyshev bases. These three bases satisfy the basis\nproperties required by KTS. Theoretically, Chebyshev case has the smallest\nupper bound on its running time. The computational results, however, do not\nshow that Chebyshev case performs better than the other two."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0707.1716v1", 
    "title": "Numerical Calculation With Arbitrary Precision", 
    "arxiv-id": "0707.1716v1", 
    "author": "L. G. S. Duarte", 
    "publish": "2007-07-11T22:24:48Z", 
    "summary": "The vast use of computers on scientific numerical computation makes the\nawareness of the limited precision that these machines are able to provide us\nan essential matter. A limited and insufficient precision allied to the\ntruncation and rounding errors may induce the user to incorrect interpretation\nof his/hers answer. In this work, we have developed a computational package to\nminimize this kind of error by offering arbitrary precision numbers and\ncalculation. This is very important in Physics where we can work with numbers\ntoo small and too big simultaneously."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0707.2701v1", 
    "title": "A fixed point iteration for computing the matrix logarithm", 
    "arxiv-id": "0707.2701v1", 
    "author": "Gernot Schaller", 
    "publish": "2007-07-18T11:04:27Z", 
    "summary": "In various areas of applied numerics, the problem of calculating the\nlogarithm of a matrix A emerges. Since series expansions of the logarithm\nusually do not converge well for matrices far away from the identity, the\nstandard numerical method calculates successive square roots. In this article,\na new algorithm is presented that relies on the computation of successive\nmatrix exponentials. Convergence of the method is demonstrated for a large\nclass of initial matrices and favorable choices of the initial matrix are\ndiscussed."
},{
    "category": "cs.DM", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0707.2921v1", 
    "title": "Covering a line segment with variable radius discs", 
    "arxiv-id": "0707.2921v1", 
    "author": "Andrea Pacifici", 
    "publish": "2007-07-19T14:59:47Z", 
    "summary": "The paper addresses the problem of locating sensors with a circular field of\nview so that a given line segment is under full surveillance, which is termed\nas the Disc Covering Problem on a Line. The cost of each sensor includes a\nfixed component, and a variable component that is proportional to the\nfield-of-view area. When only one type of sensor or, in general, one type of\ndisc, is available, then a simple polynomial algorithm solves the problem. When\nthere are different types of sensors in terms of fixed and variable costs, the\nproblem becomes NP-hard. A branch-and-bound algorithm as well as an efficient\nheuristic are developed. The heuristic very often obtains the optimal solution\nas shown in extensive computational testing."
},{
    "category": "cs.DM", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0708.1211v1", 
    "title": "A Deterministic Sub-linear Time Sparse Fourier Algorithm via   Non-adaptive Compressed Sensing Methods", 
    "arxiv-id": "0708.1211v1", 
    "author": "M. A. Iwen", 
    "publish": "2007-08-09T04:07:06Z", 
    "summary": "We study the problem of estimating the best B term Fourier representation for\na given frequency-sparse signal (i.e., vector) $\\textbf{A}$ of length $N \\gg\nB$. More explicitly, we investigate how to deterministically identify B of the\nlargest magnitude frequencies of $\\hat{\\textbf{A}}$, and estimate their\ncoefficients, in polynomial$(B,\\log N)$ time. Randomized sub-linear time\nalgorithms which have a small (controllable) probability of failure for each\nprocessed signal exist for solving this problem. However, for failure\nintolerant applications such as those involving mission-critical hardware\ndesigned to process many signals over a long lifetime, deterministic algorithms\nwith no probability of failure are highly desirable. In this paper we build on\nthe deterministic Compressed Sensing results of Cormode and Muthukrishnan (CM)\n\\cite{CMDetCS3,CMDetCS1,CMDetCS2} in order to develop the first known\ndeterministic sub-linear time sparse Fourier Transform algorithm suitable for\nfailure intolerant applications. Furthermore, in the process of developing our\nnew Fourier algorithm, we present a simplified deterministic Compressed Sensing\nalgorithm which improves on CM's algebraic compressibility results while\nsimultaneously maintaining their results concerning exponential decay."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0708.4149v2", 
    "title": "On the complexity of nonnegative matrix factorization", 
    "arxiv-id": "0708.4149v2", 
    "author": "Stephen A. Vavasis", 
    "publish": "2007-08-30T13:01:33Z", 
    "summary": "Nonnegative matrix factorization (NMF) has become a prominent technique for\nthe analysis of image databases, text databases and other information retrieval\nand clustering applications. In this report, we define an exact version of NMF.\nThen we establish several results about exact NMF: (1) that it is equivalent to\na problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a\npolynomial-time local search heuristic exists."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2007.11.024", 
    "link": "http://arxiv.org/pdf/0708.4399v2", 
    "title": "Type-IV DCT, DST, and MDCT algorithms with reduced numbers of arithmetic   operations", 
    "arxiv-id": "0708.4399v2", 
    "author": "Steven G. Johnson", 
    "publish": "2007-08-31T18:00:33Z", 
    "summary": "We present algorithms for the type-IV discrete cosine transform (DCT-IV) and\ndiscrete sine transform (DST-IV), as well as for the modified discrete cosine\ntransform (MDCT) and its inverse, that achieve a lower count of real\nmultiplications and additions than previously published algorithms, without\nsacrificing numerical accuracy. Asymptotically, the operation count is reduced\nfrom ~2NlogN to ~(17/9)NlogN for a power-of-two transform size N, and the exact\ncount is strictly lowered for all N > 4. These results are derived by\nconsidering the DCT to be a special case of a DFT of length 8N, with certain\nsymmetries, and then pruning redundant operations from a recent improved fast\nFourier transform algorithm (based on a recursive rescaling of the\nconjugate-pair split radix algorithm). The improved algorithms for DST-IV and\nMDCT follow immediately from the improved count for the DCT-IV."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.apnum.2007.04.009", 
    "link": "http://arxiv.org/pdf/0709.0355v1", 
    "title": "Solution of moving-boundary problems by the spectral element method", 
    "arxiv-id": "0709.0355v1", 
    "author": "Michel O. Deville", 
    "publish": "2007-09-04T14:51:56Z", 
    "summary": "This paper describes a novel numerical model aiming at solving\nmoving-boundary problems such as free-surface flows or fluid-structure\ninteraction. This model uses a moving-grid technique to solve the\nNavier--Stokes equations expressed in the arbitrary Lagrangian--Eulerian\nkinematics. The discretization in space is based on the spectral element\nmethod. The coupling of the fluid equations and the moving-grid equations is\nessentially done through the conditions on the moving boundaries. Two- and\nthree-dimensional simulations are presented: translation and rotation of a\ncylinder in a fluid, and large-amplitude sloshing in a rectangular tank. The\naccuracy and robustness of the present numerical model is studied and\ndiscussed."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TIP.2009.2026678", 
    "link": "http://arxiv.org/pdf/0710.0736v1", 
    "title": "Colour image segmentation by the vector-valued Allen-Cahn phase-field   model: a multigrid solution", 
    "arxiv-id": "0710.0736v1", 
    "author": "Alessandro Tomasi", 
    "publish": "2007-10-03T08:51:44Z", 
    "summary": "We propose a new method for the numerical solution of a PDE-driven model for\ncolour image segmentation and give numerical examples of the results. The\nmethod combines the vector-valued Allen-Cahn phase field equation with initial\ndata fitting terms. This method is known to be closely related to the\nMumford-Shah problem and the level set segmentation by Chan and Vese. Our\nnumerical solution is performed using a multigrid splitting of a finite element\nspace, thereby producing an efficient and robust method for the segmentation of\nlarge images."
},{
    "category": "cs.CG", 
    "doi": "10.1109/TIP.2009.2026678", 
    "link": "http://arxiv.org/pdf/0711.4656v2", 
    "title": "A condition number analysis of an algorithm for solving a system of   polynomial equations with one degree of freedom", 
    "arxiv-id": "0711.4656v2", 
    "author": "Stephen A. Vavasis", 
    "publish": "2007-11-29T06:02:16Z", 
    "summary": "This article considers the problem of solving a system of $n$ real polynomial\nequations in $n+1$ variables. We propose an algorithm based on Newton's method\nand subdivision for this problem. Our algorithm is intended only for\nnondegenerate cases, in which case the solution is a 1-dimensional curve. Our\nfirst main contribution is a definition of a condition number measuring\nreciprocal distance to degeneracy that can distinguish poor and well\nconditioned instances of this problem. (Degenerate problems would be infinitely\nill conditioned in our framework.) Our second contribution, which is the main\nnovelty of our algorithm, is an analysis showing that its running time is\nbounded in terms of the condition number of the problem instance as well as $n$\nand the polynomial degrees."
},{
    "category": "cs.CE", 
    "doi": "10.1109/TIP.2009.2026678", 
    "link": "http://arxiv.org/pdf/0712.2789v2", 
    "title": "Trading in Risk Dimensions (TRD)", 
    "arxiv-id": "0712.2789v2", 
    "author": "Lester Ingber", 
    "publish": "2007-12-17T18:11:52Z", 
    "summary": "Previous work, mostly published, developed two-shell recursive trading\nsystems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit\nto incoming market data. A parameterized trading-rule outer-shell uses the\nglobal optimization code Adaptive Simulated Annealing (ASA) to fit the trading\nsystem to historical data. A simple fitting algorithm, usually not requiring\nASA, is used for the inner-shell fit. An additional risk-management\nmiddle-shell has been added to create a three-shell recursive\noptimization/sampling/fitting algorithm. Portfolio-level distributions of\ncopula-transformed multivariate distributions (with constituent markets\npossessing different marginal distributions in returns space) are generated by\nMonte Carlo samplings. ASA is used to importance-sample weightings of these\nmarkets.\n  The core code, Trading in Risk Dimensions (TRD), processes Training and\nTesting trading systems on historical data, and consistently interacts with\nRealTime trading platforms at minute resolutions, but this scale can be\nmodified. This approach transforms constituent probability distributions into a\ncommon space where it makes sense to develop correlations to further develop\nprobability distributions and risk/uncertainty analyses of the full portfolio.\nASA is used for importance-sampling these distributions and for optimizing\nsystem parameters."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2009.2026678", 
    "link": "http://arxiv.org/pdf/0801.0523v1", 
    "title": "Certifying floating-point implementations using Gappa", 
    "arxiv-id": "0801.0523v1", 
    "author": "Guillaume Melquiond", 
    "publish": "2008-01-03T13:34:03Z", 
    "summary": "High confidence in floating-point programs requires proving numerical\nproperties of final and intermediate values. One may need to guarantee that a\nvalue stays within some range, or that the error relative to some ideal value\nis well bounded. Such work may require several lines of proof for each line of\ncode, and will usually be broken by the smallest change to the code (e.g. for\nmaintenance or optimization purpose). Certifying these programs by hand is\ntherefore very tedious and error-prone. This article discusses the use of the\nGappa proof assistant in this context. Gappa has two main advantages over\nprevious approaches: Its input format is very close to the actual C code to\nvalidate, and it automates error evaluation and propagation using interval\narithmetic. Besides, it can be used to incrementally prove complex mathematical\nproperties pertaining to the C code. Yet it does not require any specific\nknowledge about automatic theorem proving, and thus is accessible to a wide\ncommunity. Moreover, Gappa may generate a formal proof of the results that can\nbe checked independently by a lower-level proof assistant like Coq, hence\nproviding an even higher confidence in the certification of the numerical code.\nThe article demonstrates the use of this tool on a real-size example, an\nelementary function with correctly rounded output."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TIP.2009.2026678", 
    "link": "http://arxiv.org/pdf/0801.3249v1", 
    "title": "Complex Eigenvalues for Binary Subdivision Schemes", 
    "arxiv-id": "0801.3249v1", 
    "author": "Christian Kuehn", 
    "publish": "2008-01-21T18:27:09Z", 
    "summary": "Convergence properties of binary stationary subdivision schemes for curves\nhave been analyzed using the techniques of z-transforms and eigenanalysis.\nEigenanalysis provides a way to determine derivative continuity at specific\npoints based on the eigenvalues of a finite matrix. None of the well-known\nsubdivision schemes for curves have complex eigenvalues. We prove when a\nconvergent scheme with palindromic mask can have complex eigenvalues and that a\nlower limit for the size of the mask exists in this case. We find a scheme with\ncomplex eigenvalues achieving this lower bound. Furthermore we investigate this\nscheme numerically and explain from a geometric viewpoint why such a scheme has\nnot yet been used in computer-aided geometric design."
},{
    "category": "cs.CG", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0802.2108v3", 
    "title": "Well-Centered Triangulation", 
    "arxiv-id": "0802.2108v3", 
    "author": "Edgar Ramos", 
    "publish": "2008-02-14T23:04:07Z", 
    "summary": "Meshes composed of well-centered simplices have nice orthogonal dual meshes\n(the dual Voronoi diagram). This is useful for certain numerical algorithms\nthat prefer such primal-dual mesh pairs. We prove that well-centered meshes\nalso have optimality properties and relationships to Delaunay and minmax angle\ntriangulations. We present an iterative algorithm that seeks to transform a\ngiven triangulation in two or three dimensions into a well-centered one by\nminimizing a cost function and moving the interior vertices while keeping the\nmesh connectivity and boundary vertices fixed. The cost function is a direct\nresult of a new characterization of well-centeredness in arbitrary dimensions\nthat we present. Ours is the first optimization-based heuristic for\nwell-centeredness, and the first one that applies in both two and three\ndimensions. We show the results of applying our algorithm to small and large\ntwo-dimensional meshes, some with a complex boundary, and obtain a\nwell-centered tetrahedralization of the cube. We also show numerical evidence\nthat our algorithm preserves gradation and that it improves the maximum and\nminimum angles of acute triangulations created by the best known previous\nmethod."
},{
    "category": "cs.NA", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0803.0018v5", 
    "title": "Another approach to decide on real root existence for univariate   Polynomials, and a multivariate extension for 3-SAT", 
    "arxiv-id": "0803.0018v5", 
    "author": "Deepak Ponvel Chermakani", 
    "publish": "2008-02-29T22:12:53Z", 
    "summary": "We present six Theorems on the univariate real Polynomial, using which we\ndevelop a new algorithm for deciding the existence of atleast one real root for\nunivariate integer Polynomials. Our algorithm outputs that no positive real\nroot exists, if and only if, the given Polynomial is a factor of a real\nPolynomial with positive coefficients. Next, we define a transformation that\ntransforms any instance of 3-SAT into a multivariate real Polynomial with\npositive coefficients, if and only if, the instance is not satisfiable."
},{
    "category": "cs.NA", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0803.0439v1", 
    "title": "Optimizing polynomials for floating-point implementation", 
    "arxiv-id": "0803.0439v1", 
    "author": "Christoph Quirin Lauter", 
    "publish": "2008-03-04T13:49:44Z", 
    "summary": "The floating-point implementation of a function on an interval often reduces\nto polynomial approximation, the polynomial being typically provided by Remez\nalgorithm. However, the floating-point evaluation of a Remez polynomial\nsometimes leads to catastrophic cancellations. This happens when some of the\npolynomial coefficients are very small in magnitude with respects to others. In\nthis case, it is better to force these coefficients to zero, which also reduces\nthe operation count. This technique, classically used for odd or even\nfunctions, may be generalized to a much larger class of functions. An algorithm\nis presented that forces to zero the smaller coefficients of the initial\npolynomial thanks to a modified Remez algorithm targeting an incomplete\nmonomial basis. One advantage of this technique is that it is purely numerical,\nthe function being used as a numerical black box. This algorithm is implemented\nwithin a larger polynomial implementation tool that is demonstrated on a range\nof examples, resulting in polynomials with less coefficients than those\nobtained the usual way."
},{
    "category": "cs.MS", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0803.0874v3", 
    "title": "A Method for Solving Cyclic Block Penta-diagonal Systems of Linear   Equations", 
    "arxiv-id": "0803.0874v3", 
    "author": "Milan Batista", 
    "publish": "2008-03-06T18:45:39Z", 
    "summary": "A method for solving cyclic block three-diagonal systems of equations is\ngeneralized for solving a block cyclic penta-diagonal system of equations.\nIntroducing a special form of two new variables the original system is split\ninto three block pentagonal systems, which can be solved by the known methods.\nAs such method belongs to class of direct methods without pivoting.\nImplementation of the algorithm is discussed in some details and the numerical\nexamples are present."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0803.0988v2", 
    "title": "Faster Approximate Lossy Generalized Flow via Interior Point Algorithms", 
    "arxiv-id": "0803.0988v2", 
    "author": "Daniel A. Spielman", 
    "publish": "2008-03-06T21:57:53Z", 
    "summary": "We present faster approximation algorithms for generalized network flow\nproblems. A generalized flow is one in which the flow out of an edge differs\nfrom the flow into the edge by a constant factor. We limit ourselves to the\nlossy case, when these factors are at most 1.\n  Our algorithm uses a standard interior-point algorithm to solve a linear\nprogram formulation of the network flow problem. The system of linear equations\nthat arises at each step of the interior-point algorithm takes the form of a\nsymmetric M-matrix. We present an algorithm for solving such systems in nearly\nlinear time. The algorithm relies on the Spielman-Teng nearly linear time\nalgorithm for solving linear systems in diagonally-dominant matrices.\n  For a graph with m edges, our algorithm obtains an additive epsilon\napproximation of the maximum generalized flow and minimum cost generalized flow\nin time tildeO(m^(3/2) * log(1/epsilon)). In many parameter ranges, this\nimproves over previous algorithms by a factor of approximately m^(1/2). We also\nobtain a similar improvement for exactly solving the standard min-cost flow\nproblem."
},{
    "category": "cs.SC", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0803.2319v1", 
    "title": "Two Algorithms for Solving A General Backward Pentadiagonal Linear   Systems", 
    "arxiv-id": "0803.2319v1", 
    "author": "A. A. Karawia", 
    "publish": "2008-03-15T20:04:18Z", 
    "summary": "In this paper we present an efficient computational and symbolic algorithms\nfor solving a backward pentadiagonal linear systems. The implementation of the\nalgorithms using Computer Algebra Systems (CAS) such as MAPLE, MACSYMA,\nMATHEMATICA, and MATLAB are straightforward. An examples are given in order to\nillustrate the algorithms. The symbolic algorithm is competitive the other\nmethods for solving a backward pentadiagonal linear systems."
},{
    "category": "cs.NA", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0804.3650v1", 
    "title": "On generic frequency decomposition. Part 1: vectorial decomposition", 
    "arxiv-id": "0804.3650v1", 
    "author": "Sossio Vergara", 
    "publish": "2008-04-23T07:30:38Z", 
    "summary": "The famous Fourier theorem states that, under some restrictions, any periodic\nfunction (or real world signal) can be obtained as a sum of sinusoids, and\nhence, a technique exists for decomposing a signal into its sinusoidal\ncomponents. From this theory an entire branch of research has flourished: from\nthe Short-Time or Windowed Fourier Transform to the Wavelets, the Frames, and\nlately the Generic Frequency Analysis. The aim of this paper is to take the\nFrequency Analysis a step further. It will be shown that keeping the same\nreconstruction algorithm as the Fourier Theorem but changing to a new computing\nmethod for the analysis phase allows the generalization of the Fourier Theorem\nto a large class of nonorthogonal bases. New methods and algorithms can be\nemployed in function decomposition on such generic bases. It will be shown that\nthese algorithms are a generalization of the Fourier analysis, i.e. they are\nreduced to the familiar Fourier tools when using orthogonal bases. The\ndifferences between this tool and the wavelets and frames theories will be\ndiscussed. Examples of analysis and reconstruction of functions using the given\nalgorithms and nonorthogonal bases will be given. In this first part the focus\nwill be on vectorial decomposition, while the second part will be on phased\ndecomposition. The phased decomposition thanks to a single function basis has\nmany interesting consequences and applications."
},{
    "category": "cs.IR", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0805.0120v1", 
    "title": "Nonnegative Matrix Factorization via Rank-One Downdate", 
    "arxiv-id": "0805.0120v1", 
    "author": "Stephen Vavasis", 
    "publish": "2008-05-01T17:59:44Z", 
    "summary": "Nonnegative matrix factorization (NMF) was popularized as a tool for data\nmining by Lee and Seung in 1999. NMF attempts to approximate a matrix with\nnonnegative entries by a product of two low-rank matrices, also with\nnonnegative entries. We propose an algorithm called rank-one downdate (R1D) for\ncomputing a NMF that is partly motivated by singular value decomposition. This\nalgorithm computes the dominant singular values and vectors of adaptively\ndetermined submatrices of a matrix. On each iteration, R1D extracts a rank-one\nsubmatrix from the dataset according to an objective function. We establish a\ntheoretical result that maximizing this objective function corresponds to\ncorrectly classifying articles in a nearly separable corpus. We also provide\ncomputational experiments showing the success of this method in identifying\nfeatures in realistic datasets."
},{
    "category": "cs.NI", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0805.2675v2", 
    "title": "MAPEL: Achieving Global Optimality for a Non-convex Wireless Power   Control Problem", 
    "arxiv-id": "0805.2675v2", 
    "author": "Jianwei Huang", 
    "publish": "2008-05-17T15:04:38Z", 
    "summary": "Achieving weighted throughput maximization (WTM) through power control has\nbeen a long standing open problem in interference-limited wireless networks.\nThe complicated coupling between the mutual interferences of links gives rise\nto a non-convex optimization problem. Previous work has considered the WTM\nproblem in the high signal to interference-and-noise ratio (SINR) regime, where\nthe problem can be approximated and transformed into a convex optimization\nproblem through proper change of variables. In the general SINR regime,\nhowever, the approximation and transformation approach does not work. This\npaper proposes an algorithm, MAPEL, which globally converges to a global\noptimal solution of the WTM problem in the general SINR regime. The MAPEL\nalgorithm is designed based on three key observations of the WTM problem: (1)\nthe objective function is monotonically increasing in SINR, (2) the objective\nfunction can be transformed into a product of exponentiated linear fraction\nfunctions, and (3) the feasible set of the equivalent transformed problem is\nalways normal although not necessarily convex. The MAPLE algorithm finds the\ndesired optimal power control solution by constructing a series of polyblocks\nthat approximate the feasible SINR region in increasing precision. Furthermore,\nby tuning the approximation factor in MAPEL, we could engineer a desirable\ntradeoff between optimality and convergence time. MAPEL provides an important\nbenchmark for performance evaluation of other heuristic algorithms targeting\nthe same problem. With the help of MAPEL, we evaluate the performance of\nseveral respective algorithms through extensive simulations."
},{
    "category": "cs.CG", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0806.2332v2", 
    "title": "Triangulation of Simple 3D Shapes with Well-Centered Tetrahedra", 
    "arxiv-id": "0806.2332v2", 
    "author": "Damrong Guoy", 
    "publish": "2008-06-13T20:54:14Z", 
    "summary": "A completely well-centered tetrahedral mesh is a triangulation of a three\ndimensional domain in which every tetrahedron and every triangle contains its\ncircumcenter in its interior. Such meshes have applications in scientific\ncomputing and other fields. We show how to triangulate simple domains using\ncompletely well-centered tetrahedra. The domains we consider here are space,\ninfinite slab, infinite rectangular prism, cube and regular tetrahedron. We\nalso demonstrate single tetrahedra with various combinations of the properties\nof dihedral acuteness, 2-well-centeredness and 3-well-centeredness."
},{
    "category": "math.OC", 
    "doi": "10.1137/090748214", 
    "link": "http://arxiv.org/pdf/0806.3015v1", 
    "title": "Randomized Methods for Linear Constraints: Convergence Rates and   Conditioning", 
    "arxiv-id": "0806.3015v1", 
    "author": "A. S. Lewis", 
    "publish": "2008-06-18T14:29:16Z", 
    "summary": "We study randomized variants of two classical algorithms: coordinate descent\nfor systems of linear equations and iterated projections for systems of linear\ninequalities. Expanding on a recent randomized iterated projection algorithm of\nStrohmer and Vershynin for systems of linear equations, we show that, under\nappropriate probability distributions, the linear rates of convergence (in\nexpectation) can be bounded in terms of natural linear-algebraic condition\nnumbers for the problems. We relate these condition measures to distances to\nill-posedness, and discuss generalizations to convex systems under metric\nregularity assumptions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2009.07.016", 
    "link": "http://arxiv.org/pdf/0807.2108v2", 
    "title": "On dual Schur domain decomposition method for linear first-order   transient problems", 
    "arxiv-id": "0807.2108v2", 
    "author": "K. D. Hjelmstad", 
    "publish": "2008-07-14T08:10:33Z", 
    "summary": "This paper addresses some numerical and theoretical aspects of dual Schur\ndomain decomposition methods for linear first-order transient partial\ndifferential equations. In this work, we consider the trapezoidal family of\nschemes for integrating the ordinary differential equations (ODEs) for each\nsubdomain and present four different coupling methods, corresponding to\ndifferent algebraic constraints, for enforcing kinematic continuity on the\ninterface between the subdomains.\n  Method 1 (d-continuity) is based on the conventional approach using\ncontinuity of the primary variable and we show that this method is unstable for\na lot of commonly used time integrators including the mid-point rule. To\nalleviate this difficulty, we propose a new Method 2 (Modified d-continuity)\nand prove its stability for coupling all time integrators in the trapezoidal\nfamily (except the forward Euler). Method 3 (v-continuity) is based on\nenforcing the continuity of the time derivative of the primary variable.\nHowever, this constraint introduces a drift in the primary variable on the\ninterface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte\nstabilization to limit this drift and we derive bounds for the stabilization\nparameter to ensure stability.\n  Our stability analysis is based on the ``energy'' method, and one of the main\ncontributions of this paper is the extension of the energy method (which was\npreviously introduced in the context of numerical methods for ODEs) to assess\nthe stability of numerical formulations for index-2 differential-algebraic\nequations (DAEs)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2009.07.016", 
    "link": "http://arxiv.org/pdf/0807.2269v1", 
    "title": "An Efficient Algorithm for a Sharp Approximation of Universally   Quantified Inequalities", 
    "arxiv-id": "0807.2269v1", 
    "author": "Michel Rueher", 
    "publish": "2008-07-15T14:45:38Z", 
    "summary": "This paper introduces a new algorithm for solving a sub-class of quantified\nconstraint satisfaction problems (QCSP) where existential quantifiers precede\nuniversally quantified inequalities on continuous domains. This class of QCSPs\nhas numerous applications in engineering and design. We propose here a new\ngeneric branch and prune algorithm for solving such continuous QCSPs. Standard\npruning operators and solution identification operators are specialized for\nuniversally quantified inequalities. Special rules are also proposed for\nhandling the parameters of the constraints. First experimentation show that our\nalgorithm outperforms the state of the art methods."
},{
    "category": "cond-mat.mtrl-sci", 
    "doi": "10.1063/1.2995850", 
    "link": "http://arxiv.org/pdf/0807.2387v1", 
    "title": "Fast computation of magnetostatic fields by Non-uniform Fast Fourier   Transforms", 
    "arxiv-id": "0807.2387v1", 
    "author": "Olivier Fruchart", 
    "publish": "2008-07-15T14:45:14Z", 
    "summary": "The bottleneck of micromagnetic simulations is the computation of the\nlong-ranged magnetostatic fields. This can be tackled on regular N-node grids\nwith Fast Fourier Transforms in time N logN, whereas the geometrically more\nversatile finite element methods (FEM) are bounded to N^4/3 in the best case.\nWe report the implementation of a Non-uniform Fast Fourier Transform algorithm\nwhich brings a N logN convergence to FEM, with no loss of accuracy in the\nresults."
},{
    "category": "cs.LO", 
    "doi": "10.1063/1.2995850", 
    "link": "http://arxiv.org/pdf/0807.2961v1", 
    "title": "Perturbed affine arithmetic for invariant computation in numerical   program analysis", 
    "arxiv-id": "0807.2961v1", 
    "author": "Sylvie Putot", 
    "publish": "2008-07-18T12:41:52Z", 
    "summary": "We completely describe a new domain for abstract interpretation of numerical\nprograms. Fixpoint iteration in this domain is proved to converge to finite\nprecise invariants for (at least) the class of stable linear recursive filters\nof any order. Good evidence shows it behaves well also for some non-linear\nschemes. The result, and the structure of the domain, rely on an interesting\ninterplay between order and topology."
},{
    "category": "cs.LO", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0809.1552v2", 
    "title": "A computer verified, monadic, functional implementation of the integral", 
    "arxiv-id": "0809.1552v2", 
    "author": "Bas Spitters", 
    "publish": "2008-09-08T20:54:36Z", 
    "summary": "We provide a computer verified exact monadic functional implementation of the\nRiemann integral in type theory. Together with previous work by O'Connor, this\nmay be seen as the beginning of the realization of Bishop's vision to use\nconstructive mathematics as a programming language for exact analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0810.0877v1", 
    "title": "Bias-Variance Techniques for Monte Carlo Optimization: Cross-validation   for the CE Method", 
    "arxiv-id": "0810.0877v1", 
    "author": "David Wolpert", 
    "publish": "2008-10-06T04:58:44Z", 
    "summary": "In this paper, we examine the CE method in the broad context of Monte Carlo\nOptimization (MCO) and Parametric Learning (PL), a type of machine learning. A\nwell-known overarching principle used to improve the performance of many PL\nalgorithms is the bias-variance tradeoff. This tradeoff has been used to\nimprove PL algorithms ranging from Monte Carlo estimation of integrals, to\nlinear estimation, to general statistical estimation. Moreover, as described\nby, MCO is very closely related to PL. Owing to this similarity, the\nbias-variance tradeoff affects MCO performance, just as it does PL performance.\n  In this article, we exploit the bias-variance tradeoff to enhance the\nperformance of MCO algorithms. We use the technique of cross-validation, a\ntechnique based on the bias-variance tradeoff, to significantly improve the\nperformance of the Cross Entropy (CE) method, which is an MCO algorithm. In\nprevious work we have confirmed that other PL techniques improve the perfomance\nof other MCO algorithms. We conclude that the many techniques pioneered in PL\ncould be investigated as ways to improve MCO algorithms in general, and the CE\nmethod in particular."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0810.4196v1", 
    "title": "Interval Semantics for Standard Floating-Point Arithmetic", 
    "arxiv-id": "0810.4196v1", 
    "author": "M. H. van Emden", 
    "publish": "2008-10-23T03:32:47Z", 
    "summary": "If the non-zero finite floating-point numbers are interpreted as point\nintervals, then the effect of rounding can be interpreted as computing one of\nthe bounds of the result according to interval arithmetic. We give an interval\ninterpretation for the signed zeros and infinities, so that the undefined\noperations 0*inf, inf - inf, inf/inf, and 0/0 become defined.\n  In this way no operation remains that gives rise to an error condition.\nMathematically questionable features of the floating-point standard become\nwell-defined sets of reals. Interval semantics provides a basis for the\nverification of numerical algorithms. We derive the results of the newly\ndefined operations and consider the implications for hardware implementation."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0811.2847v2", 
    "title": "Boosting the Accuracy of Finite Difference Schemes via Optimal Time Step   Selection and Non-Iterative Defect Correction", 
    "arxiv-id": "0811.2847v2", 
    "author": "Kevin T. Chu", 
    "publish": "2008-11-18T07:33:30Z", 
    "summary": "In this article, we present a simple technique for boosting the order of\naccuracy of finite difference schemes for time dependent partial differential\nequations by optimally selecting the time step used to advance the numerical\nsolution and adding defect correction terms in a non-iterative manner. The\npower of the technique is its ability to extract as much accuracy as possible\nfrom existing finite difference schemes with minimal additional effort. Through\nstraightforward numerical analysis arguments, we explain the origin of the\nboost in accuracy and estimate the computational cost of the resulting\nnumerical method. We demonstrate the utility of optimal time step (OTS)\nselection combined with non-iterative defect correction (NIDC) on several\ndifferent types of finite difference schemes for a wide array of classical\nlinear and semilinear PDEs in one and more space dimensions on both regular and\nirregular domains."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0811.3247v1", 
    "title": "An experimental analysis of Lemke-Howson algorithm", 
    "arxiv-id": "0811.3247v1", 
    "author": "Marino Pagan", 
    "publish": "2008-11-20T00:32:16Z", 
    "summary": "We present an experimental investigation of the performance of the\nLemke-Howson algorithm, which is the most widely used algorithm for the\ncomputation of a Nash equilibrium for bimatrix games. Lemke-Howson algorithm is\nbased upon a simple pivoting strategy, which corresponds to following a path\nwhose endpoint is a Nash equilibrium. We analyze both the basic Lemke-Howson\nalgorithm and a heuristic modification of it, which we designed to cope with\nthe effects of a 'bad' initial choice of the pivot. Our experimental findings\nshow that, on uniformly random games, the heuristics achieves a linear running\ntime, while the basic Lemke-Howson algorithm runs in time roughly proportional\nto a polynomial of degree seven. To conduct the experiments, we have developed\nour own implementation of Lemke-Howson algorithm, which turns out to be\nsignificantly faster than state-of-the-art software. This allowed us to run the\nalgorithm on a much larger set of data, and on instances of much larger size,\ncompared with previous work."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0812.2769v2", 
    "title": "Geometric scaling: a simple preconditioner for certain linear systems   with discontinuous coefficients", 
    "arxiv-id": "0812.2769v2", 
    "author": "Rachel Gordon", 
    "publish": "2008-12-15T11:35:17Z", 
    "summary": "Linear systems with large differences between coefficients (\"discontinuous\ncoefficients\") arise in many cases in which partial differential\nequations(PDEs) model physical phenomena involving heterogeneous media. The\nstandard approach to solving such problems is to use domain decomposition\ntechniques, with domain boundaries conforming to the boundaries between the\ndifferent media. This approach can be difficult to implement when the geometry\nof the domain boundaries is complicated or the grid is unstructured. This work\nexamines the simple preconditioning technique of scaling the equations by\ndividing each equation by the Lp-norm of its coefficients. This preconditioning\nis called geometric scaling (GS). It has long been known that diagonal scaling\ncan be useful in improving convergence, but there is no study on the general\nusefulness of this approach for discontinuous coefficients. GS was tested on\nseveral nonsymmetric linear systems with discontinuous coefficients derived\nfrom convection-diffusion elliptic PDEs with small to moderate convection\nterms. It is shown that GS improved the convergence properties of restarted\nGMRES and Bi-CGSTAB, with and without the ILUT preconditioner. GS was also\nshown to improve the distribution of the eigenvalues by reducing their\nconcentration around the origin very significantly."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0901.2682v1", 
    "title": "Self-stabilizing Numerical Iterative Computation", 
    "arxiv-id": "0901.2682v1", 
    "author": "Danny Dolev", 
    "publish": "2009-01-18T06:56:59Z", 
    "summary": "Many challenging tasks in sensor networks, including sensor calibration,\nranking of nodes, monitoring, event region detection, collaborative filtering,\ncollaborative signal processing, {\\em etc.}, can be formulated as a problem of\nsolving a linear system of equations. Several recent works propose different\ndistributed algorithms for solving these problems, usually by using linear\niterative numerical methods.\n  The main problem with previous approaches is that once the problem inputs\nchange during the process of computation, the computation may output unexpected\nresults. In real life settings, sensor measurements are subject to varying\nenvironmental conditions and to measurement noise.\n  We present a simple iterative scheme called SS-Iterative for solving systems\nof linear equations, and examine its properties in the self-stabilizing\nperspective. We analyze the behavior of the proposed scheme under changing\ninput sequences using two different assumptions on the input: a box bound, and\na probabilistic distribution.\n  As a case study, we discuss the sensor calibration problem and provide\nsimulation results to support the applicability of our approach."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0901.3348v1", 
    "title": "Nuclear norm minimization for the planted clique and biclique problems", 
    "arxiv-id": "0901.3348v1", 
    "author": "Stephen Vavasis", 
    "publish": "2009-01-21T20:08:21Z", 
    "summary": "We consider the problems of finding a maximum clique in a graph and finding a\nmaximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write\nboth problems as matrix-rank minimization and then relax them using the nuclear\nnorm. This technique, which may be regarded as a generalization of compressive\nsensing, has recently been shown to be an effective way to solve rank\noptimization problems. In the special cases that the input graph has a planted\nclique or biclique (i.e., a single large clique or biclique plus diversionary\nedges), our algorithm successfully provides an exact solution to the original\ninstance. For each problem, we provide two analyses of when our algorithm\nsucceeds. In the first analysis, the diversionary edges are placed by an\nadversary. In the second, they are placed at random. In the case of random\nedges for the planted clique problem, we obtain the same bound as Alon,\nKrivelevich and Sudakov as well as Feige and Krauthgamer, but we use different\ntechniques."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.tcs.2010.05.031", 
    "link": "http://arxiv.org/pdf/0902.3088v1", 
    "title": "Automatic generation of non-uniform random variates for arbitrary   pointwise computable probability densities by tiling", 
    "arxiv-id": "0902.3088v1", 
    "author": "Guido Germano", 
    "publish": "2009-02-18T10:28:45Z", 
    "summary": "We present a rejection method based on recursive covering of the probability\ndensity function with equal tiles. The concept works for any probability\ndensity function that is pointwise computable or representable by tabular data.\nBy the implicit construction of piecewise constant majorizing and minorizing\nfunctions that are arbitrarily close to the density function the production of\nrandom variates is arbitrarily independent of the computation of the density\nfunction and extremely fast. The method works unattended for probability\ndensities with discontinuities (jumps and poles). The setup time is short,\nmarginally independent of the shape of the probability density and linear in\ntable size. Recently formulated requirements to a general and automatic\nnon-uniform random number generator are topped. We give benchmarks together\nwith a similar rejection method and with a transformation method."
},{
    "category": "cs.MS", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0902.3207v1", 
    "title": "Random numbers from the tails of probability distributions using the   transformation method", 
    "arxiv-id": "0902.3207v1", 
    "author": "Guido Germano", 
    "publish": "2009-02-18T17:53:36Z", 
    "summary": "The speed of many one-line transformation methods for the production of, for\nexample, Levy alpha-stable random numbers, which generalize Gaussian ones, and\nMittag-Leffler random numbers, which generalize exponential ones, is very high\nand satisfactory for most purposes. However, for the class of decreasing\nprobability densities fast rejection implementations like the Ziggurat by\nMarsaglia and Tsang promise a significant speed-up if it is possible to\ncomplement them with a method that samples the tails of the infinite support.\nThis requires the fast generation of random numbers greater or smaller than a\ncertain value. We present a method to achieve this, and also to generate random\nnumbers within any arbitrary interval. We demonstrate the method showing the\nproperties of the transform maps of the above mentioned distributions as\nexamples of stable and geometric stable random numbers used for the stochastic\nsolution of the space-time fractional diffusion equation."
},{
    "category": "cs.NA", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0903.2816v1", 
    "title": "A Note on Preconditioning by Low-Stretch Spanning Trees", 
    "arxiv-id": "0903.2816v1", 
    "author": "Jaeoh Woo", 
    "publish": "2009-03-16T18:04:31Z", 
    "summary": "Boman and Hendrickson observed that one can solve linear systems in Laplacian\nmatrices in time $\\bigO{m^{3/2 + o (1)} \\ln (1/\\epsilon)}$ by preconditioning\nwith the Laplacian of a low-stretch spanning tree. By examining the\ndistribution of eigenvalues of the preconditioned linear system, we prove that\nthe preconditioned conjugate gradient will actually solve the linear system in\ntime $\\softO{m^{4/3} \\ln (1/\\epsilon)}$."
},{
    "category": "cs.NA", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0903.4530v2", 
    "title": "Nonnegative approximations of nonnegative tensors", 
    "arxiv-id": "0903.4530v2", 
    "author": "Pierre Comon", 
    "publish": "2009-03-26T08:39:45Z", 
    "summary": "We study the decomposition of a nonnegative tensor into a minimal sum of\nouter product of nonnegative vectors and the associated parsimonious naive\nBayes probabilistic model. We show that the corresponding approximation\nproblem, which is central to nonnegative PARAFAC, will always have optimal\nsolutions. The result holds for any choice of norms and, under a mild\nassumption, even Bregman divergences."
},{
    "category": "cs.LO", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0904.3525v1", 
    "title": "On using floating-point computations to help an exact linear arithmetic   decision procedure", 
    "arxiv-id": "0904.3525v1", 
    "author": "David Monniaux", 
    "publish": "2009-04-22T18:40:01Z", 
    "summary": "We consider the decision problem for quantifier-free formulas whose atoms are\nlinear inequalities interpreted over the reals or rationals. This problem may\nbe decided using satisfiability modulo theory (SMT), using a mixture of a SAT\nsolver and a simplex-based decision procedure for conjunctions.\nState-of-the-art SMT solvers use simplex implementations over rational numbers,\nwhich perform well for typical problems arising from model-checking and program\nanalysis (sparse inequalities, small coefficients) but are slow for other\napplications (denser problems, larger coefficients). We propose a simple\npreprocessing phase that can be adapted on existing SMT solvers and that may be\noptionally triggered. Despite using floating-point computations, our method is\nsound and complete - it merely affects efficiency. We implemented the method\nand provide benchmarks showing that this change brings a naive and slow\ndecision procedure (\"textbook simplex\" with rational numbers) up to the\nefficiency of recent SMT solvers, over test cases arising from model-checking,\nand makes it definitely faster than state-of-the-art SMT solvers on dense\nexamples."
},{
    "category": "cs.CE", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0905.4771v3", 
    "title": "Variational structure of the optimal artificial diffusion method for the   advection-diffusion equation", 
    "arxiv-id": "0905.4771v3", 
    "author": "A. J. Valocchi", 
    "publish": "2009-05-29T01:49:38Z", 
    "summary": "In this research note we provide a variational basis for the optimal\nartificial diffusion method, which has been a cornerstone in developing many\nstabilized methods. The optimal artificial diffusion method produces exact\nnodal solutions when applied to one-dimensional problems with constant\ncoefficients and forcing function. We first present a variational principle for\na multi-dimensional advective-diffusive system, and then derive a new stable\nweak formulation. When applied to one-dimensional problems with constant\ncoefficients and forcing function, this resulting weak formulation will be\nequivalent to the optimal artificial diffusion method. We present\nrepresentative numerical results to corroborate our theoretical findings."
},{
    "category": "cs.SC", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0906.4917v2", 
    "title": "Simultaneous Integer Relation Detection and Its an Application", 
    "arxiv-id": "0906.4917v2", 
    "author": "Zhang Jing-zhong", 
    "publish": "2009-06-26T13:07:52Z", 
    "summary": "Let $\\mathbf{x_1}, ..., \\mathbf{x_t} \\in \\mathbb{R}^{n}$. A simultaneous\ninteger relation (SIR) for $\\mathbf{x_1}, ..., \\mathbf{x_t}$ is a vector\n$\\mathbf{m} \\in \\mathbb{Z}^{n}\\setminus\\{\\textbf{0}\\}$ such that\n$\\mathbf{x_i}^T\\mathbf{m} = 0$ for $i = 1, ..., t$. In this paper, we propose\nan algorithm SIRD to detect an SIR for real vectors, which constructs an SIR\nwithin $\\mathcal {O}(n^4 + n^3 \\log \\lambda(X))$ arithmetic operations, where\n$\\lambda(X)$ is the least Euclidean norm of SIRs for $\\mathbf{x_1}, >...,\n\\mathbf{x_t}$. One can easily generalize SIRD to complex number field.\nExperimental results show that SIRD is practical and better than another\ndetecting algorithm in the literature. In its application, we present a new\nalgorithm for finding the minimal polynomial of an arbitrary complex algebraic\nnumber from its an approximation, which is not based on LLL. We also provide a\nsufficient condition on the precision of the approximate value, which depends\nonly on the height and the degree of the algebraic number."
},{
    "category": "cs.NA", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0906.5561v2", 
    "title": "An Improved Algorithm based on Shannon-Happ Formula for Calculating   Transfer Function from Signal Flow Graph and Its Visualization", 
    "arxiv-id": "0906.5561v2", 
    "author": "Shanglian Bao", 
    "publish": "2009-06-30T15:47:42Z", 
    "summary": "A new method based on Shannon-Happ formula to calculate transfer function\nfrom Signal Flow Graph (SFG) is presented. The algorithm provides an explicit\napproach to get the transfer function in a format with both numerical and\nsymbolic expressions. The adoption of the symbolic variable in SFG, which could\nrepresent the nonlinear item or the independent sub-system, is achieved by\nvariable separation approach. An investigation is given for the solutions of\nseveral special conditions of SFG. To improve the efficiency of the algorithm,\na new technique combined with Johnson method for generating the combinations of\nthe non-touching loops is developed. It uses the previous combinations in lower\norder to get the ones in higher order. There is an introduction about the\nvisualization of SFG and the subroutines for system performance analysis in the\nsoftware, AVANT."
},{
    "category": "cs.CG", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0907.2423v3", 
    "title": "Computing Multidimensional Persistence", 
    "arxiv-id": "0907.2423v3", 
    "author": "Afra Zomorodian", 
    "publish": "2009-07-14T18:29:49Z", 
    "summary": "The theory of multidimensional persistence captures the topology of a\nmultifiltration -- a multiparameter family of increasing spaces.\nMultifiltrations arise naturally in the topological analysis of scientific\ndata. In this paper, we give a polynomial time algorithm for computing\nmultidimensional persistence. We recast this computation as a problem within\ncomputational algebraic geometry and utilize algorithms from this area to solve\nit. While the resulting problem is Expspace-complete and the standard\nalgorithms take doubly-exponential time, we exploit the structure inherent\nwithing multifiltrations to yield practical algorithms. We implement all\nalgorithms in the paper and provide statistical experiments to demonstrate\ntheir feasibility."
},{
    "category": "cs.SC", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0907.5038v1", 
    "title": "An Explicit Construction of Gauss-Jordan Elimination Matrix", 
    "arxiv-id": "0907.5038v1", 
    "author": "Yi Li", 
    "publish": "2009-07-29T02:01:45Z", 
    "summary": "A constructive approach to get the reduced row echelon form of a given matrix\n$A$ is presented. It has been shown that after the $k$th step of the\nGauss-Jordan procedure, each entry $a^k_{ij}(i<>j; j > k)$ in the new matrix\n$A^k$ can always be expressed as a ratio of two determinants whose entries are\nfrom the original matrix $A$. The new method also gives a more general\ngeneralization of Cramer's rule than existing methods."
},{
    "category": "math.DG", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0909.1305v1", 
    "title": "Conformal Structures and Period Matrices of Polyhedral Surfaces", 
    "arxiv-id": "0909.1305v1", 
    "author": "Markus Schmies", 
    "publish": "2009-09-07T18:46:03Z", 
    "summary": "We recall the theory of linear discrete Riemann surfaces and show how to use\nit in order to interpret a surface embedded in R^3 as a discrete Riemann\nsurface and compute its basis of holomorphic forms on it. We present numerical\nexamples, recovering known results to test the numerics and giving the yet\nunknown period matrix of the Lawson genus-2 surface."
},{
    "category": "cs.MS", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0909.4888v1", 
    "title": "Approximating Mathematical Semantic Web Services Using Approximation   Formulas and Numerical Methods", 
    "arxiv-id": "0909.4888v1", 
    "author": "Mugurel Ionut Andreica", 
    "publish": "2009-09-26T18:52:59Z", 
    "summary": "Mathematical semantic web services are very useful in practice, but only a\nsmall number of research results are reported in this area. In this paper we\npresent a method of obtaining an approximation of a mathematical semantic web\nservice, from its semantic description, using existing mathematical semantic\nweb services, approximation formulas, and numerical methods techniques. We also\ngive a method for automatic comparison of two complexity functions. In\naddition, we present a method for classifying the numerical methods\nmathematical semantic web services from a library."
},{
    "category": "cs.LG", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0910.0921v2", 
    "title": "Low-rank Matrix Completion with Noisy Observations: a Quantitative   Comparison", 
    "arxiv-id": "0910.0921v2", 
    "author": "Sewoong Oh", 
    "publish": "2009-10-06T04:41:05Z", 
    "summary": "We consider a problem of significant practical importance, namely, the\nreconstruction of a low-rank data matrix from a small subset of its entries.\nThis problem appears in many areas such as collaborative filtering, computer\nvision and wireless sensor networks. In this paper, we focus on the matrix\ncompletion problem in the case when the observed samples are corrupted by\nnoise. We compare the performance of three state-of-the-art matrix completion\nalgorithms (OptSpace, ADMiRA and FPCA) on a single simulation platform and\npresent numerical results. We show that in practice these efficient algorithms\ncan be used to reconstruct real data matrices, as well as randomly generated\nmatrices, accurately."
},{
    "category": "cs.NA", 
    "doi": "10.3970/cmes.2011.075.189", 
    "link": "http://arxiv.org/pdf/0910.4049v2", 
    "title": "A Geometric Approach to Solve Fuzzy Linear Systems", 
    "arxiv-id": "0910.4049v2", 
    "author": "O. Akin", 
    "publish": "2009-10-21T11:20:37Z", 
    "summary": "In this paper, linear systems with a crisp real coefficient matrix and with a\nvector of fuzzy triangular numbers on the right-hand side are studied. A new\nmethod, which is based on the geometric representations of linear\ntransformations, is proposed to find solutions. The method uses the fact that a\nvector of fuzzy triangular numbers forms a rectangular prism in n-dimensional\nspace and that the image of a parallelepiped is also a parallelepiped under a\nlinear transformation. The suggested method clarifies why in general case\ndifferent approaches do not generate solutions as fuzzy numbers. It is\ngeometrically proved that if the coefficient matrix is a generalized\npermutation matrix, then the solution of a fuzzy linear system (FLS) is a\nvector of fuzzy numbers irrespective of the vector on the right-hand side. The\nmost important difference between this and previous papers on FLS is that the\nsolution is sought as a fuzzy set of vectors (with real components) rather than\na vector of fuzzy numbers. Each vector in the solution set solves the given FLS\nwith a certain possibility. The suggested method can also be applied in the\ncase when the right-hand side is a vector of fuzzy numbers in parametric form.\nHowever, in this case, -cuts of the solution can not be determined by geometric\nsimilarity and additional computations are needed."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.trc.2012.12.007", 
    "link": "http://arxiv.org/pdf/0910.5260v2", 
    "title": "A Gradient Descent Algorithm on the Grassman Manifold for Matrix   Completion", 
    "arxiv-id": "0910.5260v2", 
    "author": "Sewoong Oh", 
    "publish": "2009-10-27T22:19:31Z", 
    "summary": "We consider the problem of reconstructing a low-rank matrix from a small\nsubset of its entries. In this paper, we describe the implementation of an\nefficient algorithm called OptSpace, based on singular value decomposition\nfollowed by local manifold optimization, for solving the low-rank matrix\ncompletion problem. It has been shown that if the number of revealed entries is\nlarge enough, the output of singular value decomposition gives a good estimate\nfor the original matrix, so that local optimization reconstructs the correct\nmatrix with high probability. We present numerical results which show that this\nalgorithm can reconstruct the low rank matrix exactly from a very small subset\nof its entries. We further study the robustness of the algorithm with respect\nto noise, and its performance on actual collaborative filtering datasets."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.trc.2012.12.007", 
    "link": "http://arxiv.org/pdf/0910.5947v2", 
    "title": "Topological De-Noising: Strengthening the Topological Signal", 
    "arxiv-id": "0910.5947v2", 
    "author": "Gunnar Carlsson", 
    "publish": "2009-10-30T19:08:44Z", 
    "summary": "Topological methods, including persistent homology, are powerful tools for\nanalysis of high-dimensional data sets but these methods rely almost\nexclusively on thresholding techniques. In noisy data sets, thresholding does\nnot always allow for the recovery of topological information. We present an\neasy to implement, computationally efficient pre-processing algorithm to\nprepare noisy point cloud data sets for topological data analysis. The\ntopological de-noising algorithm allows for the recovery of topological\ninformation that is inaccessible by thresholding methods. We apply the\nalgorithm to synthetically-generated noisy data sets and show the recovery of\ntopological information which is impossible to obtain by thresholding. We also\napply the algorithm to natural image data in R^8 and show a very clean recovery\nof topological information previously only available with large amounts of\nthresholding. Finally, we discuss future directions for improving this\nalgorithm using zig-zag persistence methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.trc.2012.12.007", 
    "link": "http://arxiv.org/pdf/0911.0137v5", 
    "title": "On the vibrations of lumped parameter systems governed by   differential-algebraic equations", 
    "arxiv-id": "0911.0137v5", 
    "author": "K. R. Rajagopal", 
    "publish": "2009-11-01T07:23:03Z", 
    "summary": "In this paper, we consider the vibratory motions of lumped parameter systems\nwherein the components of the system cannot be described by constitutive\nexpressions for the force in terms of appropriate kinematical quantities. Such\nphysical systems reduce to a system of differential-algebraic equations, which\ninvariably need to be solved numerically. To illustrate the issues with\nclarity, we consider a simple system in which the dashpot is assumed to contain\na \"Bingham\" fluid for which one cannot describe the force in the dashpot as a\nfunction of the velocity. On the other hand, one can express the velocity as a\nfunction of the force."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.trc.2012.12.007", 
    "link": "http://arxiv.org/pdf/0912.1178v1", 
    "title": "Algebraic Change-Point Detection", 
    "arxiv-id": "0912.1178v1", 
    "author": "Mamadou Mboup", 
    "publish": "2009-12-07T07:57:08Z", 
    "summary": "Elementary techniques from operational calculus, differential algebra, and\nnoncommutative algebra lead to a new approach for change-point detection, which\nis an important field of investigation in various areas of applied sciences and\nengineering. Several successful numerical experiments are presented."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/0912.2572v1", 
    "title": "QR Factorization of Tall and Skinny Matrices in a Grid Computing   Environment", 
    "arxiv-id": "0912.2572v1", 
    "author": "Julien Langou", 
    "publish": "2009-12-14T04:01:05Z", 
    "summary": "Previous studies have reported that common dense linear algebra operations do\nnot achieve speed up by using multiple geographical sites of a computational\ngrid. Because such operations are the building blocks of most scientific\napplications, conventional supercomputers are still strongly predominant in\nhigh-performance computing and the use of grids for speeding up large-scale\nscientific problems is limited to applications exhibiting parallelism at a\nhigher level. We have identified two performance bottlenecks in the distributed\nmemory algorithms implemented in ScaLAPACK, a state-of-the-art dense linear\nalgebra library. First, because ScaLAPACK assumes a homogeneous communication\nnetwork, the implementations of ScaLAPACK algorithms lack locality in their\ncommunication pattern. Second, the number of messages sent in the ScaLAPACK\nalgorithms is significantly greater than other algorithms that trade flops for\ncommunication. In this paper, we present a new approach for computing a QR\nfactorization -- one of the main dense linear algebra kernels -- of tall and\nskinny matrices in a grid computing environment that overcomes these two\nbottlenecks. Our contribution is to articulate a recently proposed algorithm\n(Communication Avoiding QR) with a topology-aware middleware (QCG-OMPI) in\norder to confine intensive communications (ScaLAPACK calls) within the\ndifferent geographical sites. An experimental study conducted on the Grid'5000\nplatform shows that the resulting performance increases linearly with the\nnumber of geographical sites on large-scale problems (and is in particular\nconsistently higher than ScaLAPACK's)."
},{
    "category": "cs.NA", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/0912.3927v1", 
    "title": "Logarithmic Barrier Optimization Problem Using Neural Network", 
    "arxiv-id": "0912.3927v1", 
    "author": "D. Mallick", 
    "publish": "2009-12-19T18:59:22Z", 
    "summary": "The combinatorial optimization problem is one of the important applications\nin neural network computation. The solutions of linearly constrained continuous\noptimization problems are difficult with an exact algorithm, but the algorithm\nfor the solution of such problems is derived by using logarithm barrier\nfunction. In this paper we have made an attempt to solve the linear constrained\noptimization problem by using general logarithm barrier function to get an\napproximate solution. In this case the barrier parameters behave as temperature\ndecreasing to zero from sufficiently large positive number satisfying convexity\nof the barrier function. We have developed an algorithm to generate decreasing\nsequence of solution converging to zero limit."
},{
    "category": "cs.DS", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/0912.4226v2", 
    "title": "Computing Least Fixed Points of Probabilistic Systems of Polynomials", 
    "arxiv-id": "0912.4226v2", 
    "author": "Stefan Kiefer", 
    "publish": "2009-12-21T19:14:12Z", 
    "summary": "We study systems of equations of the form X1 = f1(X1, ..., Xn), ..., Xn =\nfn(X1, ..., Xn), where each fi is a polynomial with nonnegative coefficients\nthat add up to 1. The least nonnegative solution, say mu, of such equation\nsystems is central to problems from various areas, like physics, biology,\ncomputational linguistics and probabilistic program verification. We give a\nsimple and strongly polynomial algorithm to decide whether mu=(1, ..., 1)\nholds. Furthermore, we present an algorithm that computes reliable sequences of\nlower and upper bounds on mu, converging linearly to mu. Our algorithm has\nthese features despite using inexact arithmetic for efficiency. We report on\nexperiments that show the performance of our algorithms."
},{
    "category": "cs.SC", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/0912.4438v1", 
    "title": "The weighted difference substitutions and Nonnegativity Decision of   Forms", 
    "arxiv-id": "0912.4438v1", 
    "author": "Junwei Shao", 
    "publish": "2009-12-22T16:31:16Z", 
    "summary": "In this paper, we study the weighted difference substitutions from\ngeometrical views. First, we give the geometric meanings of the weighted\ndifference substitutions, and introduce the concept of convergence of the\nsequence of substitution sets. Then it is proven that the sequence of the\nsuccessive weighted difference substitution sets is convergent. Based on the\nconvergence of the sequence of the successive weighted difference sets, a new,\nsimpler method to prove that if the form F is positive definite on T_n, then\nthe sequence of sets {SDS^m(F)} is positively terminating is presented, which\nis different from the one given in [11]. That is, we can decide the\nnonnegativity of a positive definite form by successively running the weighted\ndifference substitutions finite times. Finally, an algorithm for deciding an\nindefinite form with a counter-example is obtained, and some examples are\nlisted by using the obtained algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/1001.0340v3", 
    "title": "Computing the Least Fixed Point of Positive Polynomial Systems", 
    "arxiv-id": "1001.0340v3", 
    "author": "Michael Luttenberger", 
    "publish": "2010-01-04T17:44:41Z", 
    "summary": "We consider equation systems of the form X_1 = f_1(X_1, ..., X_n), ..., X_n =\nf_n(X_1, ..., X_n) where f_1, ..., f_n are polynomials with positive real\ncoefficients. In vector form we denote such an equation system by X = f(X) and\ncall f a system of positive polynomials, short SPP. Equation systems of this\nkind appear naturally in the analysis of stochastic models like stochastic\ncontext-free grammars (with numerous applications to natural language\nprocessing and computational biology), probabilistic programs with procedures,\nweb-surfing models with back buttons, and branching processes. The least\nnonnegative solution mu f of an SPP equation X = f(X) is of central interest\nfor these models. Etessami and Yannakakis have suggested a particular version\nof Newton's method to approximate mu f.\n  We extend a result of Etessami and Yannakakis and show that Newton's method\nstarting at 0 always converges to mu f. We obtain lower bounds on the\nconvergence speed of the method. For so-called strongly connected SPPs we prove\nthe existence of a threshold k_f such that for every i >= 0 the (k_f+i)-th\niteration of Newton's method has at least i valid bits of mu f. The proof\nyields an explicit bound for k_f depending only on syntactic parameters of f.\nWe further show that for arbitrary SPP equations Newton's method still\nconverges linearly: there are k_f>=0 and alpha_f>0 such that for every i>=0 the\n(k_f+alpha_f i)-th iteration of Newton's method has at least i valid bits of mu\nf. The proof yields an explicit bound for alpha_f; the bound is exponential in\nthe number of equations, but we also show that it is essentially optimal.\nConstructing a bound for k_f is still an open problem. Finally, we also provide\na geometric interpretation of Newton's method for SPPs."
},{
    "category": "cs.SC", 
    "doi": "10.1109/IPDPS.2010.5470475", 
    "link": "http://arxiv.org/pdf/1001.2940v1", 
    "title": "Parallel computation of real solving bivariate polynomial systems by   zero-matching method", 
    "arxiv-id": "1001.2940v1", 
    "author": "Jingzhong Zhang", 
    "publish": "2010-01-18T01:57:49Z", 
    "summary": "We present a new algorithm for solving the real roots of a bivariate\npolynomial system $\\Sigma=\\{f(x,y),g(x,y)\\}$ with a finite number of solutions\nby using a zero-matching method. The method is based on a lower bound for\nbivariate polynomial system when the system is non-zero. Moreover, the\nmultiplicities of the roots of $\\Sigma=0$ can be obtained by a given\nneighborhood. From this approach, the parallelization of the method arises\nnaturally. By using a multidimensional matching method this principle can be\ngeneralized to the multivariate equation systems."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1002.2236v3", 
    "title": "A Logical Product Approach to Zonotope Intersection", 
    "arxiv-id": "1002.2236v3", 
    "author": "Sylvie Putot", 
    "publish": "2010-02-10T23:08:49Z", 
    "summary": "We define and study a new abstract domain which is a fine-grained combination\nof zonotopes with polyhedric domains such as the interval, octagon, linear\ntemplates or polyhedron domain. While abstract transfer functions are still\nrather inexpensive and accurate even for interpreting non-linear computations,\nwe are able to also interpret tests (i.e. intersections) efficiently. This\nfixes a known drawback of zonotopic methods, as used for reachability analysis\nfor hybrid sys- tems as well as for invariant generation in abstract\ninterpretation: intersection of zonotopes are not always zonotopes, and there\nis not even a best zonotopic over-approximation of the intersection. We\ndescribe some examples and an im- plementation of our method in the APRON\nlibrary, and discuss some further in- teresting combinations of zonotopes with\nnon-linear or non-convex domains such as quadratic templates and maxplus\npolyhedra."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1002.4002v1", 
    "title": "Multi-Objective Geometric Programming Problem Being Cost Coefficients as   Continous Function with Weighted Mean Method", 
    "arxiv-id": "1002.4002v1", 
    "author": "A. K. Das", 
    "publish": "2010-02-21T19:36:55Z", 
    "summary": "Geometric programming problems occur frequently in engineering design and\nmanagement. In multiobjective optimization, the trade-off information between\ndifferent objective functions is probably the most important piece of\ninformation in a solution process to reach the most preferred solution. In this\npaper we have discussed the basic concepts and principles of multiple objective\noptimization problems and developed a solution procedure to solve this\noptimization problem where the cost coefficients are continuous functions using\nweighted method to obtain the non-inferior solutions."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1002.4057v1", 
    "title": "Towards an Efficient Tile Matrix Inversion of Symmetric Positive   Definite Matrices on Multicore Architectures", 
    "arxiv-id": "1002.4057v1", 
    "author": "Lee Rosenberg", 
    "publish": "2010-02-22T06:11:41Z", 
    "summary": "The algorithms in the current sequential numerical linear algebra libraries\n(e.g. LAPACK) do not parallelize well on multicore architectures. A new family\nof algorithms, the tile algorithms, has recently been introduced. Previous\nresearch has shown that it is possible to write efficient and scalable tile\nalgorithms for performing a Cholesky factorization, a (pseudo) LU\nfactorization, and a QR factorization. In this extended abstract, we attack the\nproblem of the computation of the inverse of a symmetric positive definite\nmatrix. We observe that, using a dynamic task scheduler, it is relatively\npainless to translate existing LAPACK code to obtain a ready-to-be-executed\ntile algorithm. However we demonstrate that non trivial compiler techniques\n(array renaming, loop reversal and pipelining) need then to be applied to\nfurther increase the parallelism of our application. We present preliminary\nexperimental results."
},{
    "category": "cs.SD", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1003.2441v1", 
    "title": "Up-sampling and Natural Sample Value Computation for Digital Pulse Width   Modulators", 
    "arxiv-id": "1003.2441v1", 
    "author": "Dilip V. Sarwate", 
    "publish": "2010-03-11T23:00:15Z", 
    "summary": "Digital pulse width modulation has been considered for high-fidelity and\nhigh-efficiency audio amplifiers for several years. It has been shown that the\ndistortion can be reduced and the implementation of the system can be\nsimplified if the switching frequency is much higher than the Nyquist rate of\nthe modulating waveform. Hence, the input digital source is normally upsampled\nto a higher frequency. It was also proved that converting uniform samples to\nnatural samples will decrease the harmonic distortion. Thus, in this paper, we\nexamine a new approach that combines upsampling, digital interpolation and\nnatural sampling conversion. This approach uses poly-phase implementation of\nthe digital interpolation filter and digital differentiators. We will show that\nthe structure consists of an FIR-type linear stage and a nonlinear stage. Some\nspectral simulation results of a pulse width modulation system based on this\napproach will also be presented. Finally, we will discuss the improvement of\nthe new approach over old algorithms."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1003.2724v1", 
    "title": "Particle Swarm Optimization Based Diophantine Equation Solver", 
    "arxiv-id": "1003.2724v1", 
    "author": "Mukund Sanglikar", 
    "publish": "2010-03-13T16:36:47Z", 
    "summary": "The paper introduces particle swarm optimization as a viable strategy to find\nnumerical solution of Diophantine equation, for which there exists no general\nmethod of finding solutions. The proposed methodology uses a population of\ninteger particles. The candidate solutions in the feasible space are optimized\nto have better positions through particle best and global best positions. The\nmethodology, which follows fully connected neighborhood topology, can offer\nmany solutions of such equations."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-14295-6_22", 
    "link": "http://arxiv.org/pdf/1003.3689v2", 
    "title": "A Highly Efficient Parallel Algorithm for Computing the Fiedler Vector", 
    "arxiv-id": "1003.3689v2", 
    "author": "Murat Manguoglu", 
    "publish": "2010-03-18T22:56:57Z", 
    "summary": "This paper has been withdrawn by the author."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-15769-1_12", 
    "link": "http://arxiv.org/pdf/1004.0202v4", 
    "title": "Interval Slopes as Numerical Abstract Domain for Floating-Point   Variables", 
    "arxiv-id": "1004.0202v4", 
    "author": "Alexandre Chapoutot", 
    "publish": "2010-04-01T18:43:10Z", 
    "summary": "The design of embedded control systems is mainly done with model-based tools\nsuch as Matlab/Simulink. Numerical simulation is the central technique of\ndevelopment and verification of such tools. Floating-point arithmetic, that is\nwell-known to only provide approximated results, is omnipresent in this\nactivity. In order to validate the behaviors of numerical simulations using\nabstract interpretation-based static analysis, we present, theoretically and\nwith experiments, a new partially relational abstract domain dedicated to\nfloating-point variables. It comes from interval expansion of non-linear\nfunctions using slopes and it is able to mimic all the behaviors of the\nfloating-point arithmetic. Hence it is adapted to prove the absence of run-time\nerrors or to analyze the numerical precision of embedded control systems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-15769-1_12", 
    "link": "http://arxiv.org/pdf/1004.1220v1", 
    "title": "Relaxation-based coarsening and multiscale graph organization", 
    "arxiv-id": "1004.1220v1", 
    "author": "Achi Brandt", 
    "publish": "2010-04-08T02:05:02Z", 
    "summary": "In this paper we generalize and improve the multiscale organization of graphs\nby introducing a new measure that quantifies the \"closeness\" between two nodes.\nThe calculation of the measure is linear in the number of edges in the graph\nand involves just a small number of relaxation sweeps. A similar notion of\ndistance is then calculated and used at each coarser level. We demonstrate the\nuse of this measure in multiscale methods for several important combinatorial\noptimization problems and discuss the multiscale graph organization."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-15769-1_12", 
    "link": "http://arxiv.org/pdf/1004.1253v1", 
    "title": "Spectral Methods for Matrices and Tensors", 
    "arxiv-id": "1004.1253v1", 
    "author": "Ravindran Kannan", 
    "publish": "2010-04-08T06:36:48Z", 
    "summary": "While Spectral Methods have long been used for Principal Component Analysis,\nthis survey focusses on work over the last 15 years with three salient\nfeatures: (i) Spectral methods are useful not only for numerical problems, but\nalso discrete optimization problems (Constraint Optimization Problems - CSP's)\nlike the max. cut problem and similar mathematical considerations underlie both\nareas. (ii) Spectral methods can be extended to tensors. The theory and\nalgorithms for tensors are not as simple/clean as for matrices, but the survey\ndescribes methods for low-rank approximation which extend to tensors. These\ntensor approximations help us solve Max-$r$-CSP's for $r>2$ as well as\nnumerical tensor problems. (iii) Sampling on the fly plays a prominent role in\nthese methods. A primary result is that for any matrix, a random submatrix of\nrows/columns picked with probabilities proportional to the squared lengths (of\nrows/columns), yields estimates of the singular values as well as an\napproximation to the whole matrix."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-15769-1_12", 
    "link": "http://arxiv.org/pdf/1004.3374v1", 
    "title": "On the precision attainable with various floating-point number systems", 
    "arxiv-id": "1004.3374v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-04-20T08:17:24Z", 
    "summary": "For scientific computations on a digital computer the set of real number is\nusually approximated by a finite set F of \"floating-point\" numbers. We compare\nthe numerical accuracy possible with difference choices of F having\napproximately the same range and requiring the same word length. In particular,\nwe compare different choices of base (or radix) in the usual floating-point\nsystems. The emphasis is on the choice of F, not on the details of the number\nrepresentation or the arithmetic, but both rounded and truncated arithmetic are\nconsidered. Theoretical results are given, and some simulations of typical\nfloating-point computations (forming sums, solving systems of linear equations,\nfinding eigenvalues) are described. If the leading fraction bit of a normalized\nbase 2 number is not stored explicitly (saving a bit), and the criterion is to\nminimize the mean square roundoff error, then base 2 is best. If unnormalized\nnumbers are allowed, so the first bit must be stored explicitly, then base 4\n(or sometimes base 8) is the best of the usual systems."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-15769-1_12", 
    "link": "http://arxiv.org/pdf/1004.3621v1", 
    "title": "Unrestricted algorithms for elementary and special functions", 
    "arxiv-id": "1004.3621v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-04-21T05:31:42Z", 
    "summary": "We describe some \"unrestricted\" algorithms which are useful for the\ncomputation of elementary and special functions when the precision required is\nnot known in advance. Several general classes of algorithms are identified and\nillustrated by examples. The topics include: power series methods, use of\nhalving identities, asymptotic expansions, continued fractions, recurrence\nrelations, Newton's method, numerical contour integration, and the\narithmetic-geometric mean. Most of the algorithms discussed are implemented in\nthe MP package (arXiv:1004.3173)."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00041-008-9036-y", 
    "link": "http://arxiv.org/pdf/1004.4329v1", 
    "title": "Analysis of Basis Pursuit Via Capacity Sets", 
    "arxiv-id": "1004.4329v1", 
    "author": "Michael Elad", 
    "publish": "2010-04-25T06:28:58Z", 
    "summary": "Finding the sparsest solution $\\alpha$ for an under-determined linear system\nof equations $D\\alpha=s$ is of interest in many applications. This problem is\nknown to be NP-hard. Recent work studied conditions on the support size of\n$\\alpha$ that allow its recovery using L1-minimization, via the Basis Pursuit\nalgorithm. These conditions are often relying on a scalar property of $D$\ncalled the mutual-coherence. In this work we introduce an alternative set of\nfeatures of an arbitrarily given $D$, called the \"capacity sets\". We show how\nthose could be used to analyze the performance of the basis pursuit, leading to\nimproved bounds and predictions of performance. Both theoretical and numerical\nmethods are presented, all using the capacity values, and shown to lead to\nimproved assessments of the basis pursuit success in finding the sparest\nsolution of $D\\alpha=s$."
},{
    "category": "math.NA", 
    "doi": "10.1007/s00041-008-9036-y", 
    "link": "http://arxiv.org/pdf/1004.5510v1", 
    "title": "On the stability of the Bareiss and related Toeplitz factorization   algorithms", 
    "arxiv-id": "1004.5510v1", 
    "author": "Douglas R. Sweet", 
    "publish": "2010-04-30T12:01:45Z", 
    "summary": "This report contains a numerical stability analysis of factorization\nalgorithms for computing the Cholesky decomposition of symmetric positive\ndefinite matrices of displacement rank 2. The algorithms in the class can be\nexpressed as sequences of elementary downdating steps. The stability of the\nfactorization algorithms follows directly from the numerical properties of\nalgorithms for realizing elementary downdating operations. It is shown that the\nBareiss algorithm for factorizing a symmetric positive definite Toeplitz matrix\nis in the class and hence the Bareiss algorithm is stable. Some numerical\nexperiments that compare behavior of the Bareiss algorithm and the Levinson\nalgorithm are presented. These experiments indicate that in general (when the\nreflection coefficients are not all positive) the Levinson algorithm is not\nstable; certainly it can give much larger residuals than the Bareiss algorithm."
},{
    "category": "math.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.0503v1", 
    "title": "A weakly stable algorithm for general Toeplitz systems", 
    "arxiv-id": "1005.0503v1", 
    "author": "Frank R. de Hoog", 
    "publish": "2010-05-04T12:27:15Z", 
    "summary": "We show that a fast algorithm for the QR factorization of a Toeplitz or\nHankel matrix A is weakly stable in the sense that R^T.R is close to A^T.A.\nThus, when the algorithm is used to solve the semi-normal equations R^T.Rx =\nA^Tb, we obtain a weakly stable method for the solution of a nonsingular\nToeplitz or Hankel linear system Ax = b. The algorithm also applies to the\nsolution of the full-rank Toeplitz or Hankel least squares problem."
},{
    "category": "math.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.0667v1", 
    "title": "Error analysis of a partial pivoting method for structured matrices", 
    "arxiv-id": "1005.0667v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-05-05T04:28:52Z", 
    "summary": "Many matrices that arise in the solution of signal processing problems have a\nspecial displacement structure. For example, adaptive filtering and\ndirection-of-arrival estimation yield matrices of Toeplitz type. A recent\nmethod of Gohberg, Kailath and Olshevsky (GKO) allows fast Gaussian elimination\nwith partial pivoting for such structured matrices. In this paper, a rounding\nerror analysis is performed on the Cauchy and Toeplitz variants of the GKO\nmethod. It is shown the error growth depends on the growth in certain auxiliary\nvectors, the generators, which are computed by the GKO algorithms. It is also\nshown that in certain circumstances, the growth in the generators can be large,\nand so the error growth is much larger than would be encountered with normal\nGaussian elimination with partial pivoting. A modification of the algorithm to\nperform a type of row-column pivoting is proposed; it may ameliorate this\nproblem."
},{
    "category": "math.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.0671v1", 
    "title": "Stability of fast algorithms for structured linear systems", 
    "arxiv-id": "1005.0671v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-05-05T04:59:19Z", 
    "summary": "We survey the numerical stability of some fast algorithms for solving systems\nof linear equations and linear least squares problems with a low\ndisplacement-rank structure. For example, the matrices involved may be Toeplitz\nor Hankel. We consider algorithms which incorporate pivoting without destroying\nthe structure, and describe some recent results on the stability of these\nalgorithms. We also compare these results with the corresponding stability\nresults for the well known algorithms of Schur/Bareiss and Levinson, and for\nalgorithms based on the semi-normal equations."
},{
    "category": "q-bio.QM", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.0747v1", 
    "title": "Hybrid Numerical Solution of the Chemical Master Equation", 
    "arxiv-id": "1005.0747v1", 
    "author": "Verena Wolf", 
    "publish": "2010-05-05T13:19:13Z", 
    "summary": "We present a numerical approximation technique for the analysis of\ncontinuous-time Markov chains that describe networks of biochemical reactions\nand play an important role in the stochastic modeling of biological systems.\nOur approach is based on the construction of a stochastic hybrid model in which\ncertain discrete random variables of the original Markov chain are approximated\nby continuous deterministic variables. We compute the solution of the\nstochastic hybrid model using a numerical algorithm that discretizes time and\nin each step performs a mutual update of the transient probability distribution\nof the discrete stochastic variables and the values of the continuous\ndeterministic variables. We implemented the algorithm and we demonstrate its\nusefulness and efficiency on several case studies from systems biology."
},{
    "category": "math.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.0762v1", 
    "title": "Note on Computing Ratings from Eigenvectors", 
    "arxiv-id": "1005.0762v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-05-05T14:23:45Z", 
    "summary": "We consider the problem of computing ratings using the results of games\nplayed between a set of n players, and show how this problem can be reduced to\ncomputing the positive eigenvectors corresponding to the dominant eigenvalues\nof certain n by n matrices. There is a close connection with the stationary\nprobability distributions of certain Markov chains. In practice, if n is large,\nthen the matrices involved will be sparse, and the power method may be used to\nsolve the eigenvalue problems efficiently. We give an algorithm based on the\npower method, and also derive the same algorithm by an independent method."
},{
    "category": "cs.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.1417v1", 
    "title": "Using vector divisions in solving linear complementarity problem", 
    "arxiv-id": "1005.1417v1", 
    "author": "Mohamed Khaladi", 
    "publish": "2010-05-09T18:38:02Z", 
    "summary": "The linear complementarity problem is to find vector $z$ in $\\mathrm{IR}^{n}$\nsatisfying $z^{T}(Mz+q)=0$, $Mz+q\\geqslant0,$ $z\\geqslant0$, where $M$ as a\nmatrix and $q$ as a vector, are given data; this problem becomes in present the\nsubject of much important research because it arises in many areas and it\nincludes important fields, we cite for example the linear and nonlinear\nprogramming, the convex quadratic programming and the variational inequalities\nproblems, ... It is known that the linear complementarity problem is completely\nequivalent to solving nonlinear equation $F(x)=0$ with $F$ is a function from\n$\\mathrm{IR}^{n}$ into itself defined by $F(x)=(M+I)x+(M-I)|x|+q$. In this\npaper we propose a globally convergent hybrid algorithm for solving this\nequation; this method is based on an algorithm given by Shi \\cite{Y. Shi}, he\nuses vector divisions with the secant method; but for using this method we must\nhave a function continuous with partial derivatives on an open set of\n$\\mathrm{IR}^{n}$; so we built a sequence of functions $\\tilde{F}(p,x)\\in\nC^{\\infty}$ which converges uniformly to the function $F(x)$; and we show that\nfinding the zero of the function $F$ is completely equivalent to finding the\nzero of the sequence of the functions $\\tilde{F}(p,x)$. We close our paper with\nsome numerical simulation examples to illustrate our theoretical results."
},{
    "category": "cs.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.1497v1", 
    "title": "Fast Digital Convolutions using Bit-Shifts", 
    "arxiv-id": "1005.1497v1", 
    "author": "Shekhar S. Chandra", 
    "publish": "2010-05-10T11:28:50Z", 
    "summary": "An exact, one-to-one transform is presented that not only allows digital\ncircular convolutions, but is free from multiplications and quantisation errors\nfor transform lengths of arbitrary powers of two. The transform is analogous to\nthe Discrete Fourier Transform, with the canonical harmonics replaced by a set\nof cyclic integers computed using only bit-shifts and additions modulo a prime\nnumber. The prime number may be selected to occupy contemporary word sizes or\nto be very large for cryptographic or data hiding applications. The transform\nis an extension of the Rader Transforms via Carmichael's Theorem. These\nproperties allow for exact convolutions that are impervious to numerical\noverflow and to utilise Fast Fourier Transform algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.2086v1", 
    "title": "On a new class of additive (splitting) operator-difference schemes", 
    "arxiv-id": "1005.2086v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2010-05-12T13:23:06Z", 
    "summary": "Many applied time-dependent problems are characterized by an additive\nrepresentation of the problem operator. Additive schemes are constructed using\nsuch a splitting and associated with the transition to a new time level on the\nbasis of the solution of more simple problems for the individual operators in\nthe additive decomposition. We consider a new class of additive schemes for\nproblems with additive representation of the operator at the time derivative.\nIn this paper we construct and study the vector operator-difference schemes,\nwhich are characterized by a transition from one initial the evolution equation\nto a system of such equations."
},{
    "category": "cs.LG", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.2146v1", 
    "title": "On the Finite Time Convergence of Cyclic Coordinate Descent Methods", 
    "arxiv-id": "1005.2146v1", 
    "author": "Ambuj Tewari", 
    "publish": "2010-05-12T16:25:46Z", 
    "summary": "Cyclic coordinate descent is a classic optimization method that has witnessed\na resurgence of interest in machine learning. Reasons for this include its\nsimplicity, speed and stability, as well as its competitive performance on\n$\\ell_1$ regularized smooth optimization problems. Surprisingly, very little is\nknown about its finite time convergence behavior on these problems. Most\nexisting results either just prove convergence or provide asymptotic rates. We\nfill this gap in the literature by proving $O(1/k)$ convergence rates (where\n$k$ is the iteration counter) for two variants of cyclic coordinate descent\nunder an isotonicity assumption. Our analysis proceeds by comparing the\nobjective values attained by the two variants with each other, as well as with\nthe gradient descent algorithm. We show that the iterates generated by the\ncyclic coordinate descent methods remain better than those of gradient descent\nuniformly over time."
},{
    "category": "math.NA", 
    "doi": "10.1007/BF02140770", 
    "link": "http://arxiv.org/pdf/1005.4732v2", 
    "title": "Tensor sparsification via a bound on the spectral norm of random tensors", 
    "arxiv-id": "1005.4732v2", 
    "author": "Trac D. Tran", 
    "publish": "2010-05-26T04:17:48Z", 
    "summary": "Given an order-$d$ tensor $\\tensor A \\in \\R^{n \\times n \\times...\\times n}$,\nwe present a simple, element-wise sparsification algorithm that zeroes out all\nsufficiently small elements of $\\tensor A$, keeps all sufficiently large\nelements of $\\tensor A$, and retains some of the remaining elements with\nprobabilities proportional to the square of their magnitudes. We analyze the\napproximation accuracy of the proposed algorithm using a powerful inequality\nthat we derive. This inequality bounds the spectral norm of a random tensor and\nis of independent interest. As a result, we obtain novel bounds for the tensor\nsparsification problem."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.24.15", 
    "link": "http://arxiv.org/pdf/1006.0401v1", 
    "title": "Making big steps in trajectories", 
    "arxiv-id": "1006.0401v1", 
    "author": "Margarita Korovina", 
    "publish": "2010-06-02T14:30:40Z", 
    "summary": "We consider the solution of initial value problems within the context of\nhybrid systems and emphasise the use of high precision approximations (in\nsoftware for exact real arithmetic). We propose a novel algorithm for the\ncomputation of trajectories up to the area where discontinuous jumps appear,\napplicable for holomorphic flow functions. Examples with a prototypical\nimplementation illustrate that the algorithm might provide results with higher\nprecision than well-known ODE solvers at a similar computation time."
},{
    "category": "math.NA", 
    "doi": "10.4204/EPTCS.24.15", 
    "link": "http://arxiv.org/pdf/1006.1043v1", 
    "title": "Polyharmonic Daubechies type wavelets in Image Processing and Astronomy,   I", 
    "arxiv-id": "1006.1043v1", 
    "author": "Damyan Kalaglarsky", 
    "publish": "2010-06-05T13:25:19Z", 
    "summary": "We introduce a new family of multivariate wavelets which are obtained by\n\"polyharmonic subdivision\". They generalize directly the original compactly\nsupported Daubechies wavelets."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1016/j.cpc.2010.04.011", 
    "link": "http://arxiv.org/pdf/1006.1296v1", 
    "title": "Estimating multidimensional probability fields using the Field Estimator   for Arbitrary Spaces (FiEstAS) with applications to astrophysics", 
    "arxiv-id": "1006.1296v1", 
    "author": "Yago Ascasibar", 
    "publish": "2010-06-07T16:49:05Z", 
    "summary": "The Field Estimator for Arbitrary Spaces (FiEstAS) computes the continuous\nprobability density field underlying a given discrete data sample in multiple,\nnon-commensurate dimensions. The algorithm works by constructing a\nmetric-independent tessellation of the data space based on a recursive binary\nsplitting. Individual, data-driven bandwidths are assigned to each point,\nscaled so that a constant \"mass\" M0 is enclosed. Kernel density estimation may\nthen be performed for different kernel shapes, and a combination of balloon and\nsample point estimators is proposed as a compromise between resolution and\nvariance. A bias correction is evaluated for the particular (yet common) case\nwhere the density is computed exactly at the locations of the data points\nrather than at an uncorrelated set of locations. By default, the algorithm\ncombines a top-hat kernel with M0=2.0 with the balloon estimator and applies\nthe corresponding bias correction. These settings are shown to yield reasonable\nresults for a simple test case, a two-dimensional ring, that illustrates the\nperformance for oblique distributions, as well as for a six-dimensional\nHernquist sphere, a fairly realistic model of the dynamical structure of\nstellar bulges in galaxies and dark matter haloes in cosmological N-body\nsimulations. Results for different parameter settings are discussed in order to\nprovide a guideline to select an optimal configuration in other cases. Source\ncode is available upon request."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.entcs.2010.09.004", 
    "link": "http://arxiv.org/pdf/1006.3159v1", 
    "title": "Abstract Fixpoint Computations with Numerical Acceleration Methods", 
    "arxiv-id": "1006.3159v1", 
    "author": "Alexandre Chapoutot", 
    "publish": "2010-06-16T08:39:12Z", 
    "summary": "Static analysis by abstract interpretation aims at automatically proving\nproperties of computer programs. To do this, an over-approximation of program\nsemantics, defined as the least fixpoint of a system of semantic equations,\nmust be computed. To enforce the convergence of this computation, widening\noperator is used but it may lead to coarse results. We propose a new method to\naccelerate the computation of this fixpoint by using standard techniques of\nnumerical analysis. Our goal is to automatically and dynamically adapt the\nwidening operator in order to maintain precision."
},{
    "category": "cs.NA", 
    "doi": "10.1145/1824801.1824804", 
    "link": "http://arxiv.org/pdf/1006.3962v1", 
    "title": "Increasing the Reliability of Adaptive Quadrature Using Explicit   Interpolants", 
    "arxiv-id": "1006.3962v1", 
    "author": "Pedro Gonnet", 
    "publish": "2010-06-20T21:52:01Z", 
    "summary": "We present two new adaptive quadrature routines. Both routines differ from\npreviously published algorithms in many aspects, most significantly in how they\nrepresent the integrand, how they treat non-numerical values of the integrand,\nhow they deal with improper divergent integrals and how they estimate the\nintegration error. The main focus of these improvements is to increase the\nreliability of the algorithms without significantly impacting their efficiency.\nBoth algorithms are implemented in Matlab and tested using both the \"families\"\nsuggested by Lyness and Kaganove and the battery test used by Gander and\nGautschi and Kahaner. They are shown to be more reliable, albeit in some cases\nless efficient, than other commonly-used adaptive integrators."
},{
    "category": "math.NA", 
    "doi": "10.1145/1824801.1824804", 
    "link": "http://arxiv.org/pdf/1006.5252v1", 
    "title": "Decomposition Approach for Low-rank Matrix Completion", 
    "arxiv-id": "1006.5252v1", 
    "author": "Samuel Cheng", 
    "publish": "2010-06-28T01:36:15Z", 
    "summary": "In this paper, we describe a low-rank matrix completion method based on\nmatrix decomposition. An incomplete matrix is decomposed into submatrices which\nare filled with a proposed trimming step and then are recombined to form a\nlow-rank completed matrix. The divide-and-conquer approach can significantly\nreduce computation complexity and storage requirement. Moreover, the proposed\ndecomposition method can be naturally incorporated into any existing matrix\ncompletion methods to attain further gain. Unlike most existing approaches, the\nproposed method is not based on norm minimization nor SVD decomposition. This\nmakes it possible to be applied beyond real domain and can be used in arbitrary\nfields including finite fields."
},{
    "category": "math.NA", 
    "doi": "10.1145/1824801.1824804", 
    "link": "http://arxiv.org/pdf/1006.5311v1", 
    "title": "Linear Algebra in the vector space of intervals", 
    "arxiv-id": "1006.5311v1", 
    "author": "Nicolas Goze", 
    "publish": "2010-06-28T10:41:39Z", 
    "summary": "In a previous paper, we have given an algebraic model to the set of\nintervals. Here, we apply this model in a linear frame. We define a notion of\ndiagonalization of square matrices whose coefficients are intervals. But in\nthis case, with respect to the real case, a matrix of order $n$ could have more\nthan $n$ eigenvalues (the set of intervals is not factorial). We consider a\nnotion of central eigenvalues permits to describe criterium of diagonalization.\nAs application, we define a notion of Exponential mapping."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-011-9473-6", 
    "link": "http://arxiv.org/pdf/1008.0201v2", 
    "title": "Novel Modifications of Parallel Jacobi Algorithms", 
    "arxiv-id": "1008.0201v2", 
    "author": "Vedran Dunjko", 
    "publish": "2010-08-01T19:21:45Z", 
    "summary": "We describe two main classes of one-sided trigonometric and hyperbolic\nJacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitian\nmatrices. These types of algorithms exhibit significant advantages over many\nother eigenvalue algorithms. If the matrices permit, both types of algorithms\ncompute the eigenvalues and eigenvectors with high relative accuracy.\n  We present novel parallelization techniques for both trigonometric and\nhyperbolic classes of algorithms, as well as some new ideas on how pivoting in\neach cycle of the algorithm can improve the speed of the parallel one-sided\nalgorithms. These parallelization approaches are applicable to both\ndistributed-memory and shared-memory machines.\n  The numerical testing performed indicates that the hyperbolic algorithms may\nbe superior to the trigonometric ones, although, in theory, the latter seem\nmore natural."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10543-011-0333-5", 
    "link": "http://arxiv.org/pdf/1008.1371v2", 
    "title": "A GPU-based hyperbolic SVD algorithm", 
    "arxiv-id": "1008.1371v2", 
    "author": "Sanja Singer", 
    "publish": "2010-08-07T23:00:42Z", 
    "summary": "A one-sided Jacobi hyperbolic singular value decomposition (HSVD) algorithm,\nusing a massively parallel graphics processing unit (GPU), is developed. The\nalgorithm also serves as the final stage of solving a symmetric indefinite\neigenvalue problem. Numerical testing demonstrates the gains in speed and\naccuracy over sequential and MPI-parallelized variants of similar Jacobi-type\nHSVD algorithms. Finally, possibilities of hybrid CPU--GPU parallelism are\ndiscussed."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cam.2011.07.017", 
    "link": "http://arxiv.org/pdf/1008.1700v2", 
    "title": "A domain decomposing parallel sparse linear system solver", 
    "arxiv-id": "1008.1700v2", 
    "author": "Murat Manguoglu", 
    "publish": "2010-08-10T12:14:09Z", 
    "summary": "The solution of large sparse linear systems is often the most time-consuming\npart of many science and engineering applications. Computational fluid\ndynamics, circuit simulation, power network analysis, and material science are\njust a few examples of the application areas in which large sparse linear\nsystems need to be solved effectively. In this paper we introduce a new\nparallel hybrid sparse linear system solver for distributed memory\narchitectures that contains both direct and iterative components. We show that\nby using our solver one can alleviate the drawbacks of direct and iterative\nsolvers, achieving better scalability than with direct solvers and more\nrobustness than with classical preconditioned iterative solvers. Comparisons to\nwell-known direct and iterative solvers on a parallel architecture are\nprovided."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cam.2011.07.017", 
    "link": "http://arxiv.org/pdf/1008.2715v1", 
    "title": "The FEM approach to the 2D Poisson equation in 'meshes' optimized with   the Metropolis algorithm", 
    "arxiv-id": "1008.2715v1", 
    "author": "Poland", 
    "publish": "2010-08-16T17:22:47Z", 
    "summary": "The presented article contains a 2D mesh generation routine optimized with\nthe Metropolis algorithm. The procedure enables to produce meshes with a\nprescribed size h of elements. These finite element meshes can serve as\nstandard discrete patterns for the Finite Element Method (FEM). Appropriate\nmeshes together with the FEM approach constitute an effective tool to deal with\ndifferential problems. Thus, having them both one can solve the 2D Poisson\nproblem. It can be done for different domains being either of a regular\n(circle, square) or of a non--regular type. The proposed routine is even\ncapable to deal with non--convex shapes."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cam.2011.07.017", 
    "link": "http://arxiv.org/pdf/1008.3845v2", 
    "title": "A stabilized mixed formulation for unsteady Brinkman equation based on   the method of horizontal lines", 
    "arxiv-id": "1008.3845v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2010-07-15T21:47:04Z", 
    "summary": "In this paper, we present a stabilized mixed formulation for unsteady\nBrinkman equation. The formulation is systematically derived based on the\nvariational multiscale formalism and the method of horizontal lines. The\nderivation does not need the assumption that the fine-scale variables do not\ndepend on the time, which is the case with the conventional derivation of\nmultiscale stabilized formulations for transient mixed problems. An expression\nfor the stabilization parameter is obtained in terms of a bubble function, and\nappropriate bubble functions for various finite elements are also presented.\nUnder the proposed formulation, equal-order interpolation for the velocity and\npressure (which is computationally the most convenient) is stable.\nRepresentative numerical results are presented to illustrate the performance of\nthe proposed formulation. Spatial and temporal convergence studies are also\nperformed, and the proposed formulation performed well."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.11.067", 
    "link": "http://arxiv.org/pdf/1008.4166v1", 
    "title": "Three-Level Parallel J-Jacobi Algorithms for Hermitian Matrices", 
    "arxiv-id": "1008.4166v1", 
    "author": "Aleksandar Uscumlic", 
    "publish": "2010-08-24T22:13:50Z", 
    "summary": "The paper describes several efficient parallel implementations of the\none-sided hyperbolic Jacobi-type algorithm for computing eigenvalues and\neigenvectors of Hermitian matrices. By appropriate blocking of the algorithms\nan almost ideal load balancing between all available processors/cores is\nobtained. A similar blocking technique can be used to exploit local cache\nmemory of each processor to further speed up the process. Due to diversity of\nmodern computer architectures, each of the algorithms described here may be the\nmethod of choice for a particular hardware and a given matrix size. All\nproposed block algorithms compute the eigenvalues with relative accuracy\nsimilar to the original non-blocked Jacobi algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.11.067", 
    "link": "http://arxiv.org/pdf/1008.4870v1", 
    "title": "On Euclidean Norm Approximations", 
    "arxiv-id": "1008.4870v1", 
    "author": "Hassan A. Kingravi", 
    "publish": "2010-08-28T14:29:21Z", 
    "summary": "Euclidean norm calculations arise frequently in scientific and engineering\napplications. Several approximations for this norm with differing complexity\nand accuracy have been proposed in the literature. Earlier approaches were\nbased on minimizing the maximum error. Recently, Seol and Cheun proposed an\napproximation based on minimizing the average error. In this paper, we first\nexamine these approximations in detail, show that they fit into a single\nmathematical formulation, and compare their average and maximum errors. We then\nshow that the maximum errors given by Seol and Cheun are significantly\noptimistic."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.amc.2011.11.067", 
    "link": "http://arxiv.org/pdf/1008.5391v2", 
    "title": "Parallel Evolutionary Computation in Very Large Scale Eigenvalue   Problems", 
    "arxiv-id": "1008.5391v2", 
    "author": "Amir H. Assadi", 
    "publish": "2010-08-31T18:56:04Z", 
    "summary": "The history of research on eigenvalue problems is rich with many outstanding\ncontributions. Nonetheless, the rapidly increasing size of data sets requires\nnew algorithms for old problems in the context of extremely large matrix\ndimensions. This paper reports on a new method for finding eigenvalues of very\nlarge matrices by a synthesis of evolutionary computation, parallel\nprogramming, and empirical stochastic search. The direct design of our method\nhas the added advantage that it could be adapted to extend many algorithmic\nvariants of solutions of generalized eigenvalue problems to improve the\naccuracy of our algorithms. The preliminary evaluation results are encouraging\nand demonstrate the method's efficiency and practicality."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1009.0938v2", 
    "title": "Linearly scaling direct method for accurately inverting sparse banded   matrices", 
    "arxiv-id": "1009.0938v2", 
    "author": "Pablo Echenique", 
    "publish": "2010-09-05T18:05:22Z", 
    "summary": "In many problems in Computational Physics and Chemistry, one finds a special\nkind of sparse matrices, termed \"banded matrices\". These matrices, which are\ndefined as having non-zero entries only within a given distance from the main\ndiagonal, need often to be inverted in order to solve the associated linear\nsystem of equations. In this work, we introduce a new O(n) algorithm for\nsolving such a system, being n X n the size of the matrix. We produce the\nanalytical recursive expressions that allow to directly obtain the solution, as\nwell as the pseudocode for its computer implementation. Moreover, we review the\ndifferent options for possibly parallelizing the method, we describe the\nextension to deal with matrices that are banded plus a small number of non-zero\nentries outside the band, and we use the same ideas to produce a method for\nobtaining the full inverse matrix. Finally, we show that the New Algorithm is\ncompetitive, both in accuracy and in numerical efficiency, when compared to a\nstandard method based in Gaussian elimination. We do this using sets of large\nrandom banded matrices, as well as the ones that appear when one tries to solve\nthe 1D Poisson equation by finite differences."
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1010.0014v1", 
    "title": "Improved Approximation Guarantees for Sublinear-Time Fourier Algorithms", 
    "arxiv-id": "1010.0014v1", 
    "author": "M. A. Iwen", 
    "publish": "2010-09-30T20:36:05Z", 
    "summary": "In this paper modified variants of the sparse Fourier transform algorithms\nfrom [14] are presented which improve on the approximation error bounds of the\noriginal algorithms. In addition, simple methods for extending the improved\nsparse Fourier transforms to higher dimensional settings are developed. As a\nconsequence, approximate Fourier transforms are obtained which will identify a\nnear-optimal k-term Fourier series for any given input function, $f : [0, 2 pi]\n-> C, in O(k^2 \\cdot D^4)$ time (neglecting logarithmic factors). Faster\nrandomized Fourier algorithm variants with runtime complexities that scale\nlinearly in the sparsity parameter k are also presented."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1010.0301v1", 
    "title": "A Microwave Imaging and Enhancement Technique from Noisy Synthetic Data", 
    "arxiv-id": "1010.0301v1", 
    "author": "Sugata Sanyal", 
    "publish": "2010-10-02T07:52:48Z", 
    "summary": "An inverse iterative algorithm for microwave imaging based on moment method\nsolution is presented here. The iterative scheme has been developed on\nconstrained optimization technique and is certain to converge. Different mesh\nsize for the model has been used here to overcome the Inverse Crime. The\nsynthetic data at the receivers is contaminated with different percentage of\nnoise. The ill-posedness of the problem is solved by Levenberg-Marquardt\nmethod. The algorithm is applied to synthetic data and the reconstructed image\nis then further enhanced through the Image enhancement technique"
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1010.0707v2", 
    "title": "Block tensors and symmetric embeddings", 
    "arxiv-id": "1010.0707v2", 
    "author": "Charles F. Van Loan", 
    "publish": "2010-10-04T21:06:06Z", 
    "summary": "Well known connections exist between the singular value decomposition of a\nmatrix A and the Schur decomposition of its symmetric embedding sym(A) = [ 0 A;\nA' 0]. In particular, if s is a singular value of A then +s and -s are\neigenvalues of the symmetric embedding. The top and bottom halves of sym(A)'s\neigenvectors are singular vectors for A. Power methods applied to A can be\nrelated to power methods applied to sym(A). The rank of sym(A) is twice the\nrank of A. In this paper we show how to embed a general order-d tensor A into\nan order-d symmetric tensor sym(A). Through the embedding we relate (a) power\nmethods for A's singular values to power methods for sym(A)'s eigenvalues and\n(b) the rank of A to the rank of sym(A)."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1010.2832v2", 
    "title": "A simple finite difference method for time-dependent, variable   coefficient Stokes flow on irregular domains", 
    "arxiv-id": "1010.2832v2", 
    "author": "Robert Bridson", 
    "publish": "2010-10-14T06:07:55Z", 
    "summary": "We present a simple and efficient variational finite difference method for\nsimulating time-dependent Stokes flow in the presence of irregular free\nsurfaces and moving solid boundaries. The method uses an embedded boundary\napproach on staggered Cartesian grids, avoiding the need for expensive\nremeshing operations, and can be applied to flows in both two and three\ndimensions. It uses fully implicit backwards Euler integration to provide\nstability and supports spatially varying density and viscosity, while requiring\nthe solution of just a single sparse, symmetric positive-definite linear system\nper time step. By expressing the problem in a variational form, challenging\nirregular domains are supported implicitly through the use of natural boundary\nconditions. In practice, the discretization requires only centred finite\ndifference stencils and per-cell volume fractions, and is straightforward to\nimplement. The variational form further permits generalizations to coupling\nother mechanics, all the while reducing to a sparse symmetric positive definite\nmatrix. We demonstrate consistent first order convergence of velocity in L1 and\nLinf norms on a range of analytical test cases in two dimensions. Furthermore,\nwe apply our method as part of a simple Navier-Stokes solver to illustrate that\nit can reproduce the characteristic jet buckling phenomenon of Newtonian\nliquids at moderate viscosities, in both two and three dimensions."
},{
    "category": "cs.LG", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1010.5290v2", 
    "title": "Converged Algorithms for Orthogonal Nonnegative Matrix Factorizations", 
    "arxiv-id": "1010.5290v2", 
    "author": "Andri Mirzal", 
    "publish": "2010-10-26T00:28:36Z", 
    "summary": "This paper proposes uni-orthogonal and bi-orthogonal nonnegative matrix\nfactorization algorithms with robust convergence proofs. We design the\nalgorithms based on the work of Lee and Seung [1], and derive the converged\nversions by utilizing ideas from the work of Lin [2]. The experimental results\nconfirm the theoretical guarantees of the convergences."
},{
    "category": "cs.SC", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1011.3721v1", 
    "title": "On the Inverse Of General Cyclic Heptadiagonal and Anti-Heptadiagonal   Matrices", 
    "arxiv-id": "1011.3721v1", 
    "author": "A. A. Karawia", 
    "publish": "2010-11-15T11:42:22Z", 
    "summary": "In the current work, the author present a symbolic algorithm for finding the\ndeterminant of any general nonsingular cyclic heptadiagonal matrices and\ninverse of anti-cyclic heptadiagonal matrices. The algorithms are mainly based\non the work presented in [A. A. KARAWIA, A New Algorithm for Inverting General\nCyclic Heptadiagonal Matrices Recursively, arXiv:1011.2306v1 [cs.SC]]. The\nsymbolic algorithms are suited for implementation using Computer Algebra\nSystems (CAS) such as MATLAB, MAPLE and MATHEMATICA. An illustrative example is\ngiven."
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1011.4810v2", 
    "title": "Application of Operator Splitting to Solve Reaction Diffusion Equations", 
    "arxiv-id": "1011.4810v2", 
    "author": "Tam\u00e1s Ladics", 
    "publish": "2010-11-22T13:48:31Z", 
    "summary": "Approximate solutions of the Fisher equation obtained by different splitting\nmethods are investigated. The error of this nonlinear problem is analyzed. The\norder of different splitting methods coupled with numerical methods of\ndifferent order is calculated numerically and symbolically."
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1101.0578v1", 
    "title": "Locally exact modifications of numerical integrators", 
    "arxiv-id": "1101.0578v1", 
    "author": "Jan L. Cie\u015bli\u0144ski", 
    "publish": "2011-01-03T18:58:51Z", 
    "summary": "We present a new class of exponential integrators for ordinary differential\nequations. They are locally exact, i.e., they preserve the linearization of the\noriginal system at every point. Their construction consists in modifying\nexisting numerical schemes in order to make them locally exact. The resulting\nschemes preserve all fixed points and are A-stable. The most promising results\nconcern the discrete gradient method (modified implicit midpoint rule) where we\nsucceeded to preserve essential geometric properties and the final results have\na relatively simple form. In the case of one-dimensional Hamiltonian systems\nnumerical experiments show that our modifications can increase the accuracy by\nseveral orders of magnitude. The main result of this paper is the construction\nof energy-preserving locally exact discrete gradient schemes for arbitrary\nmultidimensional Hamiltonian systems in canonical coordinates."
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1101.2005v3", 
    "title": "Block Tensor Unfoldings", 
    "arxiv-id": "1101.2005v3", 
    "author": "Charles F. Van Loan", 
    "publish": "2011-01-11T02:29:11Z", 
    "summary": "Within the field of numerical multilinear algebra, block tensors are\nincreasingly important. Accordingly, it is appropriate to develop an\ninfrastructure that supports reasoning about block tensor computation. In this\npaper we establish concise notation that is suitable for the analysis and\ndevelopment of block tensor algorithms, prove several useful block tensor\nidentities, and make precise the notion of a block tensor unfolding."
},{
    "category": "math.NA", 
    "doi": "10.1088/1751-8113/45/6/065204", 
    "link": "http://arxiv.org/pdf/1102.0919v1", 
    "title": "A Self-learning Algebraic Multigrid Method for Extremal Singular   Triplets and Eigenpairs", 
    "arxiv-id": "1102.0919v1", 
    "author": "Hans De Sterck", 
    "publish": "2011-02-04T14:08:54Z", 
    "summary": "A self-learning algebraic multigrid method for dominant and minimal singular\ntriplets and eigenpairs is described. The method consists of two multilevel\nphases. In the first, multiplicative phase (setup phase), tentative singular\ntriplets are calculated along with a multigrid hierarchy of interpolation\noperators that approximately fit the tentative singular vectors in a collective\nand self-learning manner, using multiplicative update formulas. In the second,\nadditive phase (solve phase), the tentative singular triplets are improved up\nto the desired accuracy by using an additive correction scheme with fixed\ninterpolation operators, combined with a Ritz update. A suitable generalization\nof the singular value decomposition is formulated that applies to the coarse\nlevels of the multilevel cycles. The proposed algorithm combines and extends\ntwo existing multigrid approaches for symmetric positive definite eigenvalue\nproblems to the case of dominant and minimal singular triplets. Numerical tests\non model problems from different areas show that the algorithm converges to\nhigh accuracy in a modest number of iterations, and is flexible enough to deal\nwith a variety of problems due to its self-learning properties."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.02.078", 
    "link": "http://arxiv.org/pdf/1102.5239v1", 
    "title": "Uncertainty Updating in the Description of Coupled Heat and Moisture   Transport in Heterogeneous Materials", 
    "arxiv-id": "1102.5239v1", 
    "author": "Jan Sykora", 
    "publish": "2011-02-25T13:23:38Z", 
    "summary": "To assess the durability of structures, heat and moisture transport need to\nbe analyzed. To provide a reliable estimation of heat and moisture distribution\nin a certain structure, one needs to include all available information about\nthe loading conditions and material parameters. Moreover, the information\nshould be accompanied by a corresponding evaluation of its credibility. Here,\nthe Bayesian inference is applied to combine different sources of information,\nso as to provide a more accurate estimation of heat and moisture fields [1].\nThe procedure is demonstrated on the probabilistic description of heterogeneous\nmaterial where the uncertainties consist of a particular value of individual\nmaterial characteristic and spatial fluctuations. As for the heat and moisture\ntransfer, it is modelled in coupled setting [2]."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.02.078", 
    "link": "http://arxiv.org/pdf/1102.5597v1", 
    "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition   Algorithms", 
    "arxiv-id": "1102.5597v1", 
    "author": "Radim \u0158eh{\u016f}\u0159ek", 
    "publish": "2011-02-28T05:26:58Z", 
    "summary": "With the explosion of the size of digital dataset, the limiting factor for\ndecomposition algorithms is the \\emph{number of passes} over the input, as the\ninput is often stored out-of-core or even off-site. Moreover, we're only\ninterested in algorithms that operate in \\emph{constant memory} w.r.t. to the\ninput size, so that arbitrarily large input can be processed. In this paper, we\npresent a practical comparison of two such algorithms: a distributed method\nthat operates in a single pass over the input vs. a streamed two-pass\nstochastic algorithm. The experiments track the effect of distributed\ncomputing, oversampling and memory trade-offs on the accuracy and performance\nof the two algorithms. To ensure meaningful results, we choose the input to be\na real dataset, namely the whole of the English Wikipedia, in the application\nsettings of Latent Semantic Analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.02.078", 
    "link": "http://arxiv.org/pdf/1103.2405v1", 
    "title": "Fast Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph   Mining", 
    "arxiv-id": "1103.2405v1", 
    "author": "Ponnuswamy Sadayappan", 
    "publish": "2011-03-12T01:04:56Z", 
    "summary": "Scaling up the sparse matrix-vector multiplication kernel on modern Graphics\nProcessing Units (GPU) has been at the heart of numerous studies in both\nacademia and industry. In this article we present a novel non-parametric,\nself-tunable, approach to data representation for computing this kernel,\nparticularly targeting sparse matrices representing power-law graphs. Using\nreal data, we show how our representation scheme, coupled with a novel tiling\nalgorithm, can yield significant benefits over the current state of the art GPU\nefforts on a number of core data mining algorithms such as PageRank, HITS and\nRandom Walk with Restart."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1731022.1731030", 
    "link": "http://arxiv.org/pdf/1103.6248v1", 
    "title": "DOLFIN: Automated Finite Element Computing", 
    "arxiv-id": "1103.6248v1", 
    "author": "Garth N. Wells", 
    "publish": "2011-03-31T17:29:27Z", 
    "summary": "We describe here a library aimed at automating the solution of partial\ndifferential equations using the finite element method. By employing novel\ntechniques for automated code generation, the library combines a high level of\nexpressiveness with efficient computation. Finite element variational forms may\nbe expressed in near mathematical notation, from which low-level code is\nautomatically generated, compiled and seamlessly integrated with efficient\nimplementations of computational meshes and high-performance linear algebra.\nEasy-to-use object-oriented interfaces to the library are provided in the form\nof a C++ library and a Python module. This paper discusses the mathematical\nabstractions and methods used in the design of the library and its\nimplementation. A number of examples are presented to demonstrate the use of\nthe library in application code."
},{
    "category": "cs.NA", 
    "doi": "10.1145/1731022.1731030", 
    "link": "http://arxiv.org/pdf/1105.0706v3", 
    "title": "A mixed formulation for a modification to Darcy equation based on Picard   linearization and numerical solutions to large-scale realistic problems", 
    "arxiv-id": "1105.0706v3", 
    "author": "D. Z. Turner", 
    "publish": "2011-05-03T23:00:45Z", 
    "summary": "In this paper we consider a modification to Darcy equation by taking into\naccount the dependence of viscosity on the pressure. We present a stabilized\nmixed formulation for the resulting governing equations. Equal-order\ninterpolation for the velocity and pressure is considered, and shown to be\nstable (which is not the case under the classical mixed formulation). The\nproposed mixed formulation is tested using a wide variety of numerical\nexamples. The proposed formulation is also implemented in a parallel setting,\nand the performance of the formulation for large-scale problems is illustrated\nusing a representative problem. Two practical and technologically important\nproblems, one each on enhanced oil recovery and geological carbon-dioxide\nsequestration, are solved using the proposed formulation. The numerical\nexamples show that the predictions based on Darcy model are qualitatively and\nquantitatively different from that of the predictions based on the modified\nDarcy model, which takes into account the dependence of the viscosity on the\npressure. In particular, the numerical example on the geological carbon-dioxide\nsequestration shows that Darcy model over-predicts the leakage into an\nabandoned well when compared to that of the modified Darcy model. On the other\nhand, the modified Darcy model predicts higher pressures and higher pressure\ngradients near the injection well. These predictions have dire consequences in\npredicting damage and fracture zones, and designing the seal, whose integrity\nis crucial to the safety of a geological carbon-dioxide sequestration\ngeosystem."
},{
    "category": "math.NA", 
    "doi": "10.1145/1731022.1731030", 
    "link": "http://arxiv.org/pdf/1105.5331v1", 
    "title": "A Nonlinear GMRES Optimization Algorithm for Canonical Tensor   Decomposition", 
    "arxiv-id": "1105.5331v1", 
    "author": "Hans De Sterck", 
    "publish": "2011-05-26T16:00:23Z", 
    "summary": "A new algorithm is presented for computing a canonical rank-R tensor\napproximation that has minimal distance to a given tensor in the Frobenius\nnorm, where the canonical rank-R tensor consists of the sum of R rank-one\ncomponents. Each iteration of the method consists of three steps. In the first\nstep, a tentative new iterate is generated by a stand-alone one-step process,\nfor which we use alternating least squares (ALS). In the second step, an\naccelerated iterate is generated by a nonlinear generalized minimal residual\n(GMRES) approach, recombining previous iterates in an optimal way, and\nessentially using the stand-alone one-step process as a preconditioner. In\nparticular, the nonlinear extension of GMRES is used that was proposed by\nWashio and Oosterlee in [ETNA Vol. 15 (2003), pp. 165-185] for nonlinear\npartial differential equation problems. In the third step, a line search is\nperformed for globalization. The resulting nonlinear GMRES (N-GMRES)\noptimization algorithm is applied to dense and sparse tensor decomposition test\nproblems. The numerical tests show that ALS accelerated by N-GMRES may\nsignificantly outperform both stand-alone ALS and a standard nonlinear\nconjugate gradient optimization method, especially when highly accurate\nstationary points are desired for difficult problems. The proposed N-GMRES\noptimization algorithm is based on general concepts and may be applied to other\nnonlinear optimization problems."
},{
    "category": "cs.NA", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.2176v2", 
    "title": "A Tuned and Scalable Fast Multipole Method as a Preeminent Algorithm for   Exascale Systems", 
    "arxiv-id": "1106.2176v2", 
    "author": "Lorena Barba", 
    "publish": "2011-06-10T21:02:40Z", 
    "summary": "Among the algorithms that are likely to play a major role in future exascale\ncomputing, the fast multipole method (FMM) appears as a rising star. Our\nprevious recent work showed scaling of an FMM on GPU clusters, with problem\nsizes in the order of billions of unknowns. That work led to an extremely\nparallel FMM, scaling to thousands of GPUs or tens of thousands of CPUs. This\npaper reports on a a campaign of performance tuning and scalability studies\nusing multi-core CPUs, on the Kraken supercomputer. All kernels in the FMM were\nparallelized using OpenMP, and a test using 10^7 particles randomly distributed\nin a cube showed 78% efficiency on 8 threads. Tuning of the\nparticle-to-particle kernel using SIMD instructions resulted in 4x speed-up of\nthe overall algorithm on single-core tests with 10^3 - 10^7 particles. Parallel\nscalability was studied in both strong and weak scaling. The strong scaling\ntest used 10^8 particles and resulted in 93% parallel efficiency on 2048\nprocesses for the non-SIMD code and 54% for the SIMD-optimized code (which was\nstill 2x faster). The weak scaling test used 10^6 particles per process, and\nresulted in 72% efficiency on 32,768 processes, with the largest calculation\ntaking about 40 seconds to evaluate more than 32 billion unknowns. This work\nbuilds up evidence for our view that FMM is poised to play a leading role in\nexascale computing, and we end the paper with a discussion of the features that\nmake it a particularly favorable algorithm for the emerging heterogeneous and\nmassively parallel architectural landscape."
},{
    "category": "cs.NA", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.2327v1", 
    "title": "A framework for coupled deformation-diffusion analysis with application   to degradation/healing", 
    "arxiv-id": "1106.2327v1", 
    "author": "K. B. Nakshatrala", 
    "publish": "2011-06-12T18:16:07Z", 
    "summary": "This paper deals with the formulation and numerical implementation of a fully\ncoupled continuum model for deformation-diffusion in linearized elastic solids.\nThe mathematical model takes into account the effect of the deformation on the\ndiffusion process, and the affect of the transport of an inert chemical species\non the deformation of the solid. We then present a robust computational\nframework for solving the proposed mathematical model, which consists of\ncoupled non-linear partial differential equations. It should be noted that many\npopular numerical formulations may produce unphysical negative values for the\nconcentration, particularly, when the diffusion process is anisotropic. The\nviolation of the non-negative constraint by these numerical formulations is not\nmere numerical noise. In the proposed computational framework we employ a novel\nnumerical formulation that will ensure that the concentration of the diffusant\nbe always non-negative, which is one of the main contributions of this paper.\nRepresentative numerical examples are presented to show the robustness,\nconvergence, and performance of the proposed computational framework. Another\ncontribution of this paper is to systematically study the affect of transport\nof the diffusant on the deformation of the solid and vice-versa, and their\nimplication in modeling degradation/healing of materials. We show that the\ncoupled response is both qualitatively and quantitatively different from the\nuncoupled response."
},{
    "category": "math.OC", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.2407v2", 
    "title": "A Semidefinite Programming approach for minimizing ordered weighted   averages of rational functions", 
    "arxiv-id": "1106.2407v2", 
    "author": "J. Puerto", 
    "publish": "2011-06-13T09:54:00Z", 
    "summary": "This paper considers the problem of minimizing the ordered weighted average\n(or ordered median) function of finitely many rational functions over compact\nsemi-algebraic sets. Ordered weighted averages of rational functions are not,\nin general, neither rational functions nor the supremum of rational functions\nso that current results available for the minimization of rational functions\ncannot be applied to handle these problems. We prove that the problem can be\ntransformed into a new problem embedded in a higher dimension space where it\nadmits a convenient representation. This reformulation admits a hierarchy of\nSDP relaxations that approximates, up to any degree of accuracy, the optimal\nvalue of those problems. We apply this general framework to a broad family of\ncontinuous location problems showing that some difficult problems (convex and\nnon-convex) that up to date could only be solved on the plane and with\nEuclidean distance, can be reasonably solved with different $\\ell_p$-norms and\nin any finite dimension space. We illustrate this methodology with some\nextensive computational results on location problems in the plane and the\n3-dimension space."
},{
    "category": "cs.NA", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.3694v1", 
    "title": "A time-parallel algorithm for almost integrable Hamiltonian systems", 
    "arxiv-id": "1106.3694v1", 
    "author": "Jacques Laskar", 
    "publish": "2011-06-18T22:37:28Z", 
    "summary": "We introduce a time-parallel algorithm for solving numerically almost\nintegrable Hamiltonian systems in action-angle coordinates. This algorithm is a\nrefinement of that introduced by Saha, Stadel and Tremaine in 1997 (SST97) for\nthe same type of problems. Our refined algorithm has a better convergence\nobtained from the use of derivatives of the perturbing term not considered in\nthe original SST97 algorithm. An advantage of this algorithm is its\nindependence of the step-size for the parallelized procedures which can be\nconsider as a particular case of the parareal scheme."
},{
    "category": "cs.NA", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1107.3667v1", 
    "title": "A new algebraic and arithmetic framework for interval computations", 
    "arxiv-id": "1107.3667v1", 
    "author": "Elisabeth Remm", 
    "publish": "2011-07-19T10:00:22Z", 
    "summary": "In this paper we propose some very promissing results in interval arithmetics\nwhich permit to build well-defined arithmetics including distributivity of\nmultiplication and division according addition and substraction. Thus, it\nallows to build all algebraic operations and functions on intervals. This will\navoid completely the wrapping effects and data dependance. Some simple\napplications for matrix eigenvalues calculations, inversion of symmetric\nmatrices and finally optimization are exhibited in the object-oriented\nprogramming language python."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.2713", 
    "link": "http://arxiv.org/pdf/1107.3729v1", 
    "title": "On the approximation in the smoothed finite element method (SFEM)", 
    "arxiv-id": "1107.3729v1", 
    "author": "Sundararajan Natarajan", 
    "publish": "2011-07-19T14:46:28Z", 
    "summary": "This letter aims at resolving the issues raised in the recent short\ncommunication [1] and answered by [2] by proposing a systematic approximation\nscheme based on non-mapped shape functions, which both allows to fully exploit\nthe unique advantages of the smoothed finite element method (SFEM) [3, 4, 5, 6,\n7, 8, 9] and resolve the existence, linearity and positivity deficiencies\npointed out in [1]. We show that Wachspress interpolants [10] computed in the\nphysical coordinate system are very well suited to the SFEM, especially when\nelements are heavily distorted (obtuse interior angles). The proposed\napproximation leads to results which are almost identical to those of the SFEM\ninitially proposed in [3]. These results that the proposed approximation scheme\nforms a strong and rigorous basis for construction of smoothed finite element\nmethods."
},{
    "category": "math.PR", 
    "doi": "10.3934/dcdsb.2012.17.1455", 
    "link": "http://arxiv.org/pdf/1108.0307v3", 
    "title": "The Euler-Maruyama approximation for the absorption time of the CEV   diffusion", 
    "arxiv-id": "1108.0307v3", 
    "author": "Fima C. Klebaner", 
    "publish": "2011-08-01T13:19:56Z", 
    "summary": "A standard convergence analysis of the simulation schemes for the hitting\ntimes of diffusions typically requires non-degeneracy of their coefficients on\nthe boundary, which excludes the possibility of absorption. In this paper we\nconsider the CEV diffusion from the mathematical finance and show how a weakly\nconsistent approximation for the absorption time can be constructed, using the\nEuler-Maruyama scheme."
},{
    "category": "cs.NA", 
    "doi": "10.3934/dcdsb.2012.17.1455", 
    "link": "http://arxiv.org/pdf/1108.0952v1", 
    "title": "On the performance of high-order finite elements with respect to maximum   principles and the non-negative constraint for diffusion-type equations", 
    "arxiv-id": "1108.0952v1", 
    "author": "J. N. Reddy", 
    "publish": "2011-08-03T22:21:45Z", 
    "summary": "The main aim of this paper is to document the performance of $p$-refinement\nwith respect to maximum principles and the non-negative constraint. The model\nproblem is (steady-state) anisotropic diffusion with decay (which is a\nsecond-order elliptic partial differential equation). We considered the\nstandard single-field formulation (which is based on the Galerkin formalism)\nand two least-squares-based mixed formulations. We have employed non-uniform\nLagrange polynomials for altering the polynomial order in each element, and we\nhave used $p = 1, ..., 10$.\n  It will be shown that the violation of the non-negative constraint will not\nvanish with $p$-refinement for anisotropic diffusion. We shall illustrate the\nperformance of $p$-refinement using several representative problems. The\nintended outcome of the paper is twofold. Firstly, this study will caution the\nusers of high-order approximations about its performance with respect to\nmaximum principles and the non-negative constraint. Secondly, this study will\nhelp researchers to develop new methodologies for enforcing maximum principles\nand the non-negative constraint under high-order approximations."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.07.051", 
    "link": "http://arxiv.org/pdf/1108.1042v1", 
    "title": "On strong homogeneity of two global optimization algorithms based on   statistical models of multimodal objective functions", 
    "arxiv-id": "1108.1042v1", 
    "author": "Antanas Zilinskas", 
    "publish": "2011-08-04T10:43:17Z", 
    "summary": "The implementation of global optimization algorithms, using the arithmetic of\ninfinity, is considered. A relatively simple version of implementation is\nproposed for the algorithms that possess the introduced property of strong\nhomogeneity. It is shown that the P-algorithm and the one-step Bayesian\nalgorithm are strongly homogeneous."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.amc.2011.07.051", 
    "link": "http://arxiv.org/pdf/1108.1235v2", 
    "title": "Technical Report: Modeling of Composite Piezoelectric Structures with   the Finite Volume Method", 
    "arxiv-id": "1108.1235v2", 
    "author": "Mary C. Pugh", 
    "publish": "2011-08-05T00:47:23Z", 
    "summary": "Piezoelectric devices, such as piezoelectric traveling wave rotary ultrasonic\nmotors, have composite piezoelectric structures. A composite piezoelectric\nstructure consists of a combination of two or more bonded materials, where at\nleast one of them is a piezoelectric transducer. Numerical modeling of\npiezoelectric structures has been done in the past mainly with the finite\nelement method. Alternatively, a finite volume based approach offers the\nfollowing advantages: (a) the ordinary differential equations resulting from\nthe discretization process can be interpreted directly as corresponding\ncircuits and (b) phenomena occurring at boundaries can be treated exactly. This\nreport extends the work of IEEE Transactions on UFFC 57(2010)7:1673-1691 by\npresenting a method for implementing the boundary conditions between the bonded\nmaterials in composite piezoelectric structures. The report concludes with one\nmodeling example of a unimorph structure."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.amc.2011.07.051", 
    "link": "http://arxiv.org/pdf/1108.3367v2", 
    "title": "On the convergence acceleration of some continued fractions", 
    "arxiv-id": "1108.3367v2", 
    "author": "Rafa\u0142 Nowak", 
    "publish": "2011-08-16T22:15:55Z", 
    "summary": "A well known method for convergence acceleration of continued fraction\n$\\K(a_n/b_n)$ is to use the modified approximants $S_n(\\omega_n)$ in place of\nthe classical approximants $S_n(0)$, where $\\omega_n$ are close to tails\n$f^{(n)}$ of continued fraction. Recently, author proposed a method of\niterative character producing tail approximations whose asymptotic expansion's\naccuracy is improving in each step. This method can be applied to continued\nfractions $\\K(a_n/b_n)$, where $a_n$, $b_n$ are polynomials in $n$ ($\\deg\na_n=2$, $\\deg b_n\\leq 1$) for sufficiently large $n$. The purpose of this paper\nis to extend this idea for the class of continued fractions $\\K(a_n/b_n +\na_n'/b_n')$, where $a_n$, $a_n'$, $b_n$, $b_n'$ are polynomials in $n$ ($\\deg\na_n=\\deg a_n', \\deg b_n=\\deg b_n'$). We give examples involving such continued\nfraction expansions of some mathematical constants, as well as elementary and\nspecial functions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2011.07.051", 
    "link": "http://arxiv.org/pdf/1108.5359v4", 
    "title": "Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering", 
    "arxiv-id": "1108.5359v4", 
    "author": "Zhixun Su", 
    "publish": "2011-08-26T17:40:30Z", 
    "summary": "In the past decades, exactly recovering the intrinsic data structure from\ncorrupted observations, which is known as robust principal component analysis\n(RPCA), has attracted tremendous interests and found many applications in\ncomputer vision. Recently, this problem has been formulated as recovering a\nlow-rank component and a sparse component from the observed data matrix. It is\nproved that under some suitable conditions, this problem can be exactly solved\nby principal component pursuit (PCP), i.e., minimizing a combination of nuclear\nnorm and $l_1$ norm. Most of the existing methods for solving PCP require\nsingular value decompositions (SVD) of the data matrix, resulting in a high\ncomputational complexity, hence preventing the applications of RPCA to very\nlarge scale computer vision problems. In this paper, we propose a novel\nalgorithm, called $l_1$ filtering, for \\emph{exactly} solving PCP with an\n$O(r^2(m+n))$ complexity, where $m\\times n$ is the size of data matrix and $r$\nis the rank of the matrix to recover, which is supposed to be much smaller than\n$m$ and $n$. Moreover, $l_1$ filtering is \\emph{highly parallelizable}. It is\nthe first algorithm that can \\emph{exactly} solve a nuclear norm minimization\nproblem in \\emph{linear time} (with respect to the data size). Experiments on\nboth synthetic data and real applications testify to the great advantage of\n$l_1$ filtering in speed over state-of-the-art algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MCSE.2012.1", 
    "link": "http://arxiv.org/pdf/1108.5815v2", 
    "title": "Hierarchical N-body simulations with auto-tuning for heterogeneous   systems", 
    "arxiv-id": "1108.5815v2", 
    "author": "Lorena A. Barba", 
    "publish": "2011-08-30T03:27:14Z", 
    "summary": "With the current hybridization of treecodes and FMMs, combined with\nauto-tuning capabilities on heterogeneous architectures, the flexibility of\nfast N-body methods has been greatly enhanced. These features are a requirement\nto developing a black-box software library for fast N-body algorithms on\nheterogeneous systems, which is our immediate goal."
},{
    "category": "math.NA", 
    "doi": "10.1051/proc/201235018", 
    "link": "http://arxiv.org/pdf/1108.6210v2", 
    "title": "A well-balanced finite volume scheme for 1D hemodynamic simulations", 
    "arxiv-id": "1108.6210v2", 
    "author": "Pierre-Yves Lagr\u00e9e", 
    "publish": "2011-08-31T12:45:06Z", 
    "summary": "We are interested in simulating blood flow in arteries with variable\nelasticity with a one dimensional model. We present a well-balanced finite\nvolume scheme based on the recent developments in shallow water equations\ncontext. We thus get a mass conservative scheme which also preserves equilibria\nof Q=0. This numerical method is tested on analytical tests."
},{
    "category": "cs.LG", 
    "doi": "10.1051/proc/201235018", 
    "link": "http://arxiv.org/pdf/1108.6296v2", 
    "title": "Infinite Tucker Decomposition: Nonparametric Bayesian Models for   Multiway Data Analysis", 
    "arxiv-id": "1108.6296v2", 
    "author": "Qi", 
    "publish": "2011-08-31T17:36:26Z", 
    "summary": "Tensor decomposition is a powerful computational tool for multiway data\nanalysis. Many popular tensor decomposition approaches---such as the Tucker\ndecomposition and CANDECOMP/PARAFAC (CP)---amount to multi-linear\nfactorization. They are insufficient to model (i) complex interactions between\ndata entities, (ii) various data types (e.g. missing data and binary data), and\n(iii) noisy observations and outliers. To address these issues, we propose\ntensor-variate latent nonparametric Bayesian models, coupled with efficient\ninference methods, for multiway data analysis. We name these models InfTucker.\nUsing these InfTucker, we conduct Tucker decomposition in an infinite feature\nspace. Unlike classical tensor decomposition models, our new approaches handle\nboth continuous and binary data in a probabilistic framework. Unlike previous\nBayesian models on matrices and tensors, our models are based on latent\nGaussian or $t$ processes with nonlinear covariance functions. To efficiently\nlearn the InfTucker from data, we develop a variational inference technique on\ntensors. Compared with classical implementation, the new technique reduces both\ntime and space complexities by several orders of magnitude. Our experimental\nresults on chemometrics and social network datasets demonstrate that our new\nmodels achieved significantly higher prediction accuracy than the most\nstate-of-art tensor decomposition"
},{
    "category": "cs.NA", 
    "doi": "10.1051/proc/201235018", 
    "link": "http://arxiv.org/pdf/1110.0569v1", 
    "title": "A Modulus-Squared Dirichlet Boundary Condition for Time-Dependent   Complex Partial Differential Equations and its Application to the Nonlinear   Schr\u007f\u00f6dinger Equation", 
    "arxiv-id": "1110.0569v1", 
    "author": "R. Carretero-Gonz\u00e1lez", 
    "publish": "2011-10-04T03:17:21Z", 
    "summary": "An easy to implement modulus-squared Dirichlet (MSD) boundary condition is\nformulated for numerical simulations of time-dependent complex partial\ndifferential equations in multidimensional settings. The MSD boundary condition\napproximates a constant modulus-square value of the solution at the boundaries.\nApplication of the MSD boundary condition to the nonlinear Schr\\\"odinger\nequation is shown, and numerical simulations are performed to demonstrate its\nusefulness and advantages over other simple boundary conditions."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.0895v4", 
    "title": "Robust inversion via semistochastic dimensionality reduction", 
    "arxiv-id": "1110.0895v4", 
    "author": "Tristan van Leeuwen", 
    "publish": "2011-10-05T04:55:59Z", 
    "summary": "We consider a class of inverse problems where it is possible to aggregate the\nresults of multiple experiments. This class includes problems where the forward\nmodel is the solution operator to linear ODEs or PDEs. The tremendous size of\nsuch problems motivates dimensionality reduction techniques based on randomly\nmixing experiments. These techniques break down, however, when robust\ndata-fitting formulations are used, which are essential in cases of missing\ndata, unusually large errors, and systematic features in the data unexplained\nby the forward model. We survey robust methods within a statistical framework,\nand propose a semistochastic optimization approach that allows dimensionality\nreduction. The efficacy of the methods are demonstrated for a large-scale\nseismic inverse problem using the robust Student's t-distribution, where a\nuseful synthetic velocity model is recovered in the extreme scenario of 60%\ndata missing at random. The semistochastic approach achieves this recovery\nusing 20% of the effort required by a direct robust approach."
},{
    "category": "math.NA", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.4193v2", 
    "title": "Sublinear randomized algorithms for skeleton decompositions", 
    "arxiv-id": "1110.4193v2", 
    "author": "Laurent Demanet", 
    "publish": "2011-10-19T06:36:02Z", 
    "summary": "Let $A$ be a $n$ by $n$ matrix. A skeleton decomposition is any factorization\nof the form $CUR$ where $C$ comprises columns of $A$, and $R$ comprises rows of\n$A$. In this paper, we consider uniformly sampling $\\l\\simeq k \\log n$ rows and\ncolumns to produce a skeleton decomposition. The algorithm runs in $O(\\l^3)$\ntime, and has the following error guarantee. Let $\\norm{\\cdot}$ denote the\n2-norm. Suppose $A\\simeq X B Y^T$ where $X,Y$ each have $k$ orthonormal\ncolumns. Assuming that $X,Y$ are incoherent, we show that with high\nprobability, the approximation error $\\norm{A-CUR}$ will scale with\n$(n/\\l)\\norm{A-X B Y^T}$ or better. A key step in this algorithm involves\nregularization. This step is crucial for a nonsymmetric $A$ as empirical\nresults suggest. Finally, we use our proof framework to analyze two existing\nalgorithms in an intuitive way."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.4437v2", 
    "title": "Effective Stiffness: Generalizing Effective Resistance Sampling to   Finite Element Matrices", 
    "arxiv-id": "1110.4437v2", 
    "author": "Sivan Toledo", 
    "publish": "2011-10-20T03:54:53Z", 
    "summary": "We define the notion of effective stiffness and show that it can used to\nbuild sparsifiers, algorithms that sparsify linear systems arising from\nfinite-element discretizations of PDEs. In particular, we show that sampling\n$O(n\\log n)$ elements according to probabilities derived from effective\nstiffnesses yields a high quality preconditioner that can be used to solve the\nlinear system in a small number of iterations. Effective stiffness generalizes\nthe notion of effective resistance, a key ingredient of recent progress in\ndeveloping nearly linear symmetric diagonally dominant (SDD) linear solvers.\nSolving finite elements problems is of considerably more interest than the\nsolution of SDD linear systems, since the finite element method is frequently\nused to numerically solve PDEs arising in scientific and engineering\napplications. Unlike SDD systems, which are relatively easy to solve, there has\nbeen limited success in designing fast solvers for finite element systems, and\nprevious algorithms usually target discretization of limited class of PDEs like\nscalar elliptic or 2D trusses. Our sparsifier is general; it applies to a wide\nrange of finite-element discretizations. A sparsifier does not constitute a\ncomplete linear solver. To construct a solver, one needs additional components\n(e.g., an efficient elimination or multilevel scheme for the sparsified\nsystem). Still, sparsifiers have been a critical tools in efficient SDD\nsolvers, and we believe that our sparsifier will become a key ingredient in\nfuture fast finite-element solvers."
},{
    "category": "math.NA", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.5305v1", 
    "title": "The spectral norm error of the naive Nystrom extension", 
    "arxiv-id": "1110.5305v1", 
    "author": "Alex Gittens", 
    "publish": "2011-10-24T18:57:57Z", 
    "summary": "The naive Nystrom extension forms a low-rank approximation to a\npositive-semidefinite matrix by uniformly randomly sampling from its columns.\nThis paper provides the first relative-error bound on the spectral norm error\nincurred in this process. This bound follows from a natural connection between\nthe Nystrom extension and the column subset selection problem. The main tool is\na matrix Chernoff bound for sampling without replacement."
},{
    "category": "cs.NE", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.5992v1", 
    "title": "User preference extraction using dynamic query sliders in conjunction   with UPS-EMO algorithm", 
    "arxiv-id": "1110.5992v1", 
    "author": "Suvi Tarkkanen", 
    "publish": "2011-10-27T06:40:45Z", 
    "summary": "One drawback of evolutionary multiobjective optimization algorithms (EMOA)\nhas traditionally been high computational cost to create an approximation of\nthe Pareto front: number of required objective function evaluations usually\ngrows high. On the other hand, for the decision maker (DM) it may be difficult\nto select one of the many produced solutions as the final one, especially in\nthe case of more than two objectives.\n  To overcome the above mentioned drawbacks number of EMOA's incorporating the\ndecision makers preference information have been proposed. In this case, it is\npossible to save objective function evaluations by generating only the part of\nthe front the DM is interested in, thus also narrowing down the pool of\npossible selections for the final solution.\n  Unfortunately, most of the current EMO approaches utilizing preferences are\nnot very intuitive to use, i.e. they may require tweaking of unintuitive\nparameters, and it is not always clear what kind of results one can get with\ngiven set of parameters. In this study we propose a new approach to visually\ninspect produced solutions, and to extract preference information from the DM\nto further guide the search. Our approach is based on intuitive use of dynamic\nquery sliders, which serve as a means to extract preference information and are\npart of the graphical user interface implemented for the efficient UPS-EMO\nalgorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10107-012-0571-6", 
    "link": "http://arxiv.org/pdf/1110.6834v1", 
    "title": "High-order finite volume schemes for layered atmospheric models", 
    "arxiv-id": "1110.6834v1", 
    "author": "Eleuterio F. Toro", 
    "publish": "2011-10-31T15:32:50Z", 
    "summary": "We present a numerical scheme for the solution of a class of atmospheric\nmodels where high horizontal resolution is required while a coarser vertical\nstructure is allowed. The proposed scheme considers a layering procedure for\nthe original set of equations, and the use of high-order ADER finite volume\nschemes for the solution of the system of balance laws arising from the\ndimensional reduction procedure. We present several types of layering based\nupon Galerkin discretizations of the vertical structure, and we study the\neffect of incrementing the order of horizontal approximation. Numerical\nexperiments for the computational validation of the convergence of the scheme\ntogether with the study of physical phenomena are performed over 2D linear\nadvective models, including a set of equations for an isothermal atmosphere."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TIT.2013.2271378", 
    "link": "http://arxiv.org/pdf/1111.2262v4", 
    "title": "Improved Bound for the Nystrom's Method and its Application to Kernel   Classification", 
    "arxiv-id": "1111.2262v4", 
    "author": "Zhi-Hua Zhou", 
    "publish": "2011-11-09T16:34:55Z", 
    "summary": "We develop two approaches for analyzing the approximation error bound for the\nNystr\\\"{o}m method, one based on the concentration inequality of integral\noperator, and one based on the compressive sensing theory. We show that the\napproximation error, measured in the spectral norm, can be improved from\n$O(N/\\sqrt{m})$ to $O(N/m^{1 - \\rho})$ in the case of large eigengap, where $N$\nis the total number of data points, $m$ is the number of sampled data points,\nand $\\rho \\in (0, 1/2)$ is a positive constant that characterizes the eigengap.\nWhen the eigenvalues of the kernel matrix follow a $p$-power law, our analysis\nbased on compressive sensing theory further improves the bound to $O(N/m^{p -\n1})$ under an incoherence assumption, which explains why the Nystr\\\"{o}m method\nworks well for kernel matrix with skewed eigenvalues. We present a kernel\nclassification approach based on the Nystr\\\"{o}m method and derive its\ngeneralization performance using the improved bound. We show that when the\neigenvalues of kernel matrix follow a $p$-power law, we can reduce the number\nof support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p >\n1+\\sqrt{2}$, without seriously sacrificing its generalization performance."
},{
    "category": "cs.NA", 
    "doi": "10.4204/EPTCS.154.7", 
    "link": "http://arxiv.org/pdf/1111.4385v2", 
    "title": "Model Checking CSL for Markov Population Models", 
    "arxiv-id": "1111.4385v2", 
    "author": "Lijun Zhang", 
    "publish": "2011-11-18T15:05:13Z", 
    "summary": "Markov population models (MPMs) are a widely used modelling formalism in the\narea of computational biology and related areas. The semantics of a MPM is an\ninfinite-state continuous-time Markov chain. In this paper, we use the\nestablished continuous stochastic logic (CSL) to express properties of Markov\npopulation models. This allows us to express important measures of biological\nsystems, such as probabilistic reachability, survivability, oscillations,\nswitching times between attractor regions, and various others. Because of the\ninfinite state space, available analysis techniques only apply to a very\nrestricted subset of CSL properties. We present a full algorithm for model\nchecking CSL for MPMs, and provide experimental evidence showing that our\nmethod is effective."
},{
    "category": "physics.comp-ph", 
    "doi": "10.4204/EPTCS.154.7", 
    "link": "http://arxiv.org/pdf/1112.2160v1", 
    "title": "On the role of enrichment and statical admissibility of recovered fields   in a-posteriori error estimation for enriched finite element methods", 
    "arxiv-id": "1112.2160v1", 
    "author": "Eugenio Giner", 
    "publish": "2011-12-09T17:41:45Z", 
    "summary": "Purpose: This paper aims at assessing the effect of (1) the statical\nadmissibility of the recovered solution; (2) the ability of the recovered\nsolution to represent the singular solution; on the accuracy, local and global\neffectivity of recovery-based error estimators for enriched finite element\nmethods (e.g. the extended finite element method, XFEM).\nDesign/methodology/approach: We study the performance of two recovery\ntechniques. The first is a recently developed superconvergent patch recovery\nprocedure with equilibration and enrichment (SPR-CX). The second is known as\nthe extended moving least squares recovery (XMLS), which enriches the recovered\nsolutions but does not enforce equilibrium constraints. Both are extended\nrecovery techniques as the polynomial basis used in the recovery process is\nenriched with singular terms for a better description of the singular nature of\nthe solution. Findings: Numerical results comparing the convergence and the\neffectivity index of both techniques with those obtained without the enrichment\nenhancement clearly show the need for the use of extended recovery techniques\nin Zienkiewicz-Zhu type error estimators for this class of problems. The\nresults also reveal significant improvements in the effectivities yielded by\nstatically admissible recovered solutions. Originality/value: This work shows\nthat both extended recovery procedures and statical admissibility are key to an\naccurate assessment of the quality of enriched finite element approximations."
},{
    "category": "cs.CV", 
    "doi": "10.4204/EPTCS.154.7", 
    "link": "http://arxiv.org/pdf/1112.3166v2", 
    "title": "Higher-Order Momentum Distributions and Locally Affine LDDMM   Registration", 
    "arxiv-id": "1112.3166v2", 
    "author": "Xavier Pennec", 
    "publish": "2011-12-14T11:10:08Z", 
    "summary": "To achieve sparse parametrizations that allows intuitive analysis, we aim to\nrepresent deformation with a basis containing interpretable elements, and we\nwish to use elements that have the description capacity to represent the\ndeformation compactly. To accomplish this, we introduce in this paper\nhigher-order momentum distributions in the LDDMM registration framework. While\nthe zeroth order moments previously used in LDDMM only describe local\ndisplacement, the first-order momenta that are proposed here represent a basis\nthat allows local description of affine transformations and subsequent compact\ndescription of non-translational movement in a globally non-rigid deformation.\nThe resulting representation contains directly interpretable information from\nboth mathematical and modeling perspectives. We develop the mathematical\nconstruction of the registration framework with higher-order momenta, we show\nthe implications for sparse image registration and deformation description, and\nwe provide examples of how the parametrization enables registration with a very\nlow number of parameters. The capacity and interpretability of the\nparametrization using higher-order momenta lead to natural modeling of\narticulated movement, and the method promises to be useful for quantifying\nventricle expansion and progressing atrophy during Alzheimer's disease."
},{
    "category": "cs.NA", 
    "doi": "10.4204/EPTCS.154.7", 
    "link": "http://arxiv.org/pdf/1112.4238v1", 
    "title": "Vertex-centroid finite volume scheme on tetrahedral grids for   conservation laws", 
    "arxiv-id": "1112.4238v1", 
    "author": "Ashish Garg", 
    "publish": "2011-12-19T04:54:47Z", 
    "summary": "Vertex-centroid schemes are cell-centered finite volume schemes for\nconservation laws which make use of vertex values to construct high resolution\nschemes. The vertex values must be obtained through a consistent averaging\n(interpolation) procedure. A modified interpolation scheme is proposed which is\nbetter than existing schemes in giving positive weights in the interpolation\nformula. A simplified reconstruction scheme is also proposed which is also more\naccurate and efficient. For scalar conservation laws, we develop limited\nversions of the schemes which are stable in maximum norm by constructing\nsuitable limiters. The schemes are applied to compressible flows governed by\nthe Euler equations of inviscid gas dynamics."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.154.7", 
    "link": "http://arxiv.org/pdf/1201.0127v4", 
    "title": "Faster Subset Selection for Matrices and Applications", 
    "arxiv-id": "1201.0127v4", 
    "author": "Christos Boutsidis", 
    "publish": "2011-12-30T13:54:29Z", 
    "summary": "We study subset selection for matrices defined as follows: given a matrix\n$\\matX \\in \\R^{n \\times m}$ ($m > n$) and an oversampling parameter $k$ ($n \\le\nk \\le m$), select a subset of $k$ columns from $\\matX$ such that the\npseudo-inverse of the subsampled matrix has as smallest norm as possible. In\nthis work, we focus on the Frobenius and the spectral matrix norms. We describe\nseveral novel (deterministic and randomized) approximation algorithms for this\nproblem with approximation bounds that are optimal up to constant factors.\nAdditionally, we show that the combinatorial problem of finding a low-stretch\nspanning tree in an undirected graph corresponds to subset selection, and\ndiscuss various implications of this reduction."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-33134-3_34", 
    "link": "http://arxiv.org/pdf/1201.2878v1", 
    "title": "Implementation of the Continuous-Discontinuous Galerkin Finite Element   Method", 
    "arxiv-id": "1201.2878v1", 
    "author": "Max Jensen", 
    "publish": "2012-01-13T16:08:47Z", 
    "summary": "For the stationary advection-diffusion problem the standard continuous\nGalerkin method is unstable without some additional control on the mesh or\nmethod. The interior penalty discontinuous Galerkin method is stable but at the\nexpense of an increased number of degrees of freedom. The hybrid method\nproposed in [5] combines the computational complexity of the continuous method\nwith the stability of the discontinuous method without a significant increase\nin degrees of freedom. We discuss the implementation of this method using the\nfinite element library deal.ii and present some numerical experiments."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1201.4049v1", 
    "title": "Parameter Identification in a Probabilistic Setting", 
    "arxiv-id": "1201.4049v1", 
    "author": "Hermann G. Matthies", 
    "publish": "2012-01-19T13:00:34Z", 
    "summary": "Parameter identification problems are formulated in a probabilistic language,\nwhere the randomness reflects the uncertainty about the knowledge of the true\nvalues. This setting allows conceptually easily to incorporate new information,\ne.g. through a measurement, by connecting it to Bayes's theorem. The unknown\nquantity is modelled as a (may be high-dimensional) random variable. Such a\ndescription has two constituents, the measurable function and the measure. One\ngroup of methods is identified as updating the measure, the other group changes\nthe measurable function. We connect both groups with the relatively recent\nmethods of functional approximation of stochastic problems, and introduce\nespecially in combination with the second group of methods a new procedure\nwhich does not need any sampling, hence works completely deterministically. It\nalso seems to be the fastest and more reliable when compared with other\nmethods. We show by example that it also works for highly nonlinear non-smooth\nproblems with non-Gaussian measures."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1201.6035v1", 
    "title": "How Accurate is inv(A)*b?", 
    "arxiv-id": "1201.6035v1", 
    "author": "Sivan Toledo", 
    "publish": "2012-01-29T12:55:30Z", 
    "summary": "Several widely-used textbooks lead the reader to believe that solving a\nlinear system of equations Ax = b by multiplying the vector b by a computed\ninverse inv(A) is inaccurate. Virtually all other textbooks on numerical\nanalysis and numerical linear algebra advise against using computed inverses\nwithout stating whether this is accurate or not. In fact, under reasonable\nassumptions on how the inverse is computed, x = inv(A)*b is as accurate as the\nsolution computed by the best backward-stable solvers. This fact is not new,\nbut obviously obscure. We review the literature on the accuracy of this\ncomputation and present a self-contained numerical analysis of it."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1202.0988v2", 
    "title": "Improving non-linear fits", 
    "arxiv-id": "1202.0988v2", 
    "author": "Massimo Di Pierro", 
    "publish": "2012-02-05T18:22:37Z", 
    "summary": "In this notes we describe an algorithm for non-linear fitting which\nincorporates some of the features of linear least squares into a general\nminimum $\\chi^2$ fit and provide a pure Python implementation of the algorithm.\nIt consists of the variable projection method (varpro), combined with a Newton\noptimizer and stabilized using the steepest descent with an adaptative step.\nThe algorithm includes a term to account for Bayesian priors. We performed\ntests of the algorithm using simulated data. This method is suitable, for\nexample, for fitting with sums of exponentials as often needed in Lattice\nQuantum Chromodynamics."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1202.1490v1", 
    "title": "Singular Values using Cholesky Decomposition", 
    "arxiv-id": "1202.1490v1", 
    "author": "Kenan Kocagoez", 
    "publish": "2012-02-07T18:37:07Z", 
    "summary": "In this paper two ways to compute singular values are presented which use\nCholesky decomposition as their basic operation."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1202.3108v1", 
    "title": "D-iteration based asynchronous distributed computation", 
    "arxiv-id": "1202.3108v1", 
    "author": "Dohy Hong", 
    "publish": "2012-02-14T18:07:32Z", 
    "summary": "The aim of this paper is to explain how the D-iteration can be used for an\nefficient asynchronous distributed computation. We present the main ideas of\nthe method and illustrate them through very simple examples."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.engstruct.2012.12.029", 
    "link": "http://arxiv.org/pdf/1202.3856v3", 
    "title": "Technical Report on Hypergraph-Partitioning-Based Models and Methods for   Exploiting Cache Locality in Sparse-Matrix Vector Multiplication", 
    "arxiv-id": "1202.3856v3", 
    "author": "Cevdet Aykanat", 
    "publish": "2012-02-17T09:28:24Z", 
    "summary": "The sparse matrix-vector multiplication (SpMxV) is a kernel operation widely\nused in iterative linear solvers. The same sparse matrix is multiplied by a\ndense vector repeatedly in these solvers. Matrices with irregular sparsity\npatterns make it difficult to utilize cache locality effectively in SpMxV\ncomputations. In this work, we investigate single- and multiple-SpMxV\nframeworks for exploiting cache locality in SpMxV computations. For the\nsingle-SpMxV framework, we propose two cache-size-aware top-down\nrow/column-reordering methods based on 1D and 2D sparse matrix partitioning by\nutilizing the column-net and enhancing the row-column-net hypergraph models of\nsparse matrices. The multiple-SpMxV framework depends on splitting a given\nmatrix into a sum of multiple nonzero-disjoint matrices so that the SpMxV\noperation is performed as a sequence of multiple input- and output- dependent\nSpMxV operations. For an effective matrix splitting required in this framework,\nwe propose a cache- size-aware top-down approach based on 2D sparse matrix\npartitioning by utilizing the row-column-net hypergraph model. For this\nframework, we also propose two methods for effective ordering of individual\nSpMxV operations. The primary objective in all of the three methods is to\nmaximize the exploitation of temporal locality. We evaluate the validity of our\nmodels and methods on a wide range of sparse matrices using both cache-miss\nsimulations and actual runs by using OSKI. Experimental results show that\nproposed methods and models outperform state-of-the-art schemes."
},{
    "category": "cs.NA", 
    "doi": "10.1145/2442829.2442849", 
    "link": "http://arxiv.org/pdf/1202.4407v1", 
    "title": "On the complexity of solving initial value problems", 
    "arxiv-id": "1202.4407v1", 
    "author": "Amaury Pouly", 
    "publish": "2012-02-20T18:09:18Z", 
    "summary": "In this paper we prove that computing the solution of an initial-value\nproblem $\\dot{y}=p(y)$ with initial condition $y(t_0)=y_0\\in\\R^d$ at time\n$t_0+T$ with precision $e^{-\\mu}$ where $p$ is a vector of polynomials can be\ndone in time polynomial in the value of $T$, $\\mu$ and $Y=\\sup_{t_0\\leqslant\nu\\leqslant T}\\infnorm{y(u)}$. Contrary to existing results, our algorithm works\nfor any vector of polynomials $p$ over any bounded or unbounded domain and has\na guaranteed complexity and precision. In particular we do not assume $p$ to be\nfixed, nor the solution to lie in a compact domain, nor we assume that $p$ has\na Lipschitz constant."
},{
    "category": "cs.NA", 
    "doi": "10.1145/2442829.2442849", 
    "link": "http://arxiv.org/pdf/1202.5844v3", 
    "title": "Divide-and-Conquer Method for L1 Norm Matrix Factorization in the   Presence of Outliers and Missing Data", 
    "arxiv-id": "1202.5844v3", 
    "author": "Zongben Xu", 
    "publish": "2012-02-27T07:57:04Z", 
    "summary": "The low-rank matrix factorization as a L1 norm minimization problem has\nrecently attracted much attention due to its intrinsic robustness to the\npresence of outliers and missing data. In this paper, we propose a new method,\ncalled the divide-and-conquer method, for solving this problem. The main idea\nis to break the original problem into a series of smallest possible\nsub-problems, each involving only unique scalar parameter. Each of these\nsubproblems is proved to be convex and has closed-form solution. By recursively\noptimizing these small problems in an analytical way, efficient algorithm,\nentirely avoiding the time-consuming numerical optimization as an inner loop,\nfor solving the original problem can naturally be constructed. The\ncomputational complexity of the proposed algorithm is approximately linear in\nboth data size and dimensionality, making it possible to handle large-scale L1\nnorm matrix factorization problems. The algorithm is also theoretically proved\nto be convergent. Based on a series of experiment results, it is substantiated\nthat our method always achieves better results than the current\nstate-of-the-art methods on $L1$ matrix factorization calculation in both\ncomputational time and accuracy, especially on large-scale applications such as\nface recognition and structure from motion."
},{
    "category": "math.NA", 
    "doi": "10.1145/2442829.2442849", 
    "link": "http://arxiv.org/pdf/1203.1554v2", 
    "title": "Generating nested quadrature formulas for general weight functions with   known moments", 
    "arxiv-id": "1203.1554v2", 
    "author": "D\u00e1vid Papp", 
    "publish": "2012-03-07T17:46:46Z", 
    "summary": "We revisit the problem of extending quadrature formulas for general weight\nfunctions, and provide a generalization of Patterson's method for the constant\nweight function. The method can be used to compute a nested sequence of\nquadrature formulas for integration with respect to any continuous probability\nmeasure on the real line with finite moments. The advantages of the method\ninclude that it works directly with the moments of the underlying distribution,\nand that for distributions with rational moments the existence of the formulas\ncan be verified by exact rational arithmetic."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2442829.2442849", 
    "link": "http://arxiv.org/pdf/1203.2210v2", 
    "title": "Fixed-Rank Representation for Unsupervised Visual Learning", 
    "arxiv-id": "1203.2210v2", 
    "author": "Zhixun Su", 
    "publish": "2012-03-09T23:35:52Z", 
    "summary": "Subspace clustering and feature extraction are two of the most commonly used\nunsupervised learning techniques in computer vision and pattern recognition.\nState-of-the-art techniques for subspace clustering make use of recent advances\nin sparsity and rank minimization. However, existing techniques are\ncomputationally expensive and may result in degenerate solutions that degrade\nclustering performance in the case of insufficient data sampling. To partially\nsolve these problems, and inspired by existing work on matrix factorization,\nthis paper proposes fixed-rank representation (FRR) as a unified framework for\nunsupervised visual learning. FRR is able to reveal the structure of multiple\nsubspaces in closed-form when the data is noiseless. Furthermore, we prove that\nunder some suitable conditions, even with insufficient observations, FRR can\nstill reveal the true subspace memberships. To achieve robustness to outliers\nand noise, a sparse regularizer is introduced into the FRR framework. Beyond\nsubspace clustering, FRR can be used for unsupervised feature extraction. As a\nnon-trivial byproduct, a fast numerical solver is developed for FRR.\nExperimental results on both synthetic data and real applications validate our\ntheoretical analysis and demonstrate the benefits of FRR for unsupervised\nvisual learning."
},{
    "category": "math.NA", 
    "doi": "10.1145/2442829.2442849", 
    "link": "http://arxiv.org/pdf/1203.2377v1", 
    "title": "Matrix Stretching for Linear Equations", 
    "arxiv-id": "1203.2377v1", 
    "author": "Joseph F. Grcar", 
    "publish": "2012-03-11T21:27:07Z", 
    "summary": "Stretching is a new sparse matrix method that makes matrices sparser by\nmaking them larger. Stretching has implications for computational complexity\ntheory and applications in scientific and parallel computing. It changes matrix\nsparsity patterns to render linear equations more easily solved by parallel and\nsparse techniques. Some stretchings increase matrix condition numbers only\nmoderately, and thus solve linear equations stably. For example, these\nstretchings solve arrow equations with accuracy and expense preferable to other\nsolution methods."
},{
    "category": "math.OC", 
    "doi": "10.1080/10556788.2012.684353", 
    "link": "http://arxiv.org/pdf/1203.2742v1", 
    "title": "Logarithmic barriers for sparse matrix cones", 
    "arxiv-id": "1203.2742v1", 
    "author": "Lieven Vandenberghe", 
    "publish": "2012-03-13T08:59:26Z", 
    "summary": "Algorithms are presented for evaluating gradients and Hessians of logarithmic\nbarrier functions for two types of convex cones: the cone of positive\nsemidefinite matrices with a given sparsity pattern, and its dual cone, the\ncone of sparse matrices with the same pattern that have a positive semidefinite\ncompletion. Efficient large-scale algorithms for evaluating these barriers and\ntheir derivatives are important in interior-point methods for nonsymmetric\nconic formulations of sparse semidefinite programs. The algorithms are based on\nthe multifrontal method for sparse Cholesky factorization."
},{
    "category": "cs.NI", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1203.3623v2", 
    "title": "An Improved Traffic Matrix Decomposition Method with Frequency-Domain   Regularization", 
    "arxiv-id": "1203.3623v2", 
    "author": "Baolin Yin", 
    "publish": "2012-03-16T06:57:59Z", 
    "summary": "We propose a novel network traffic matrix decomposition method named Stable\nPrincipal Component Pursuit with Frequency-Domain Regularization (SPCP-FDR),\nwhich improves the Stable Principal Component Pursuit (SPCP) method by using a\nfrequency-domain noise regularization function. An experiment demonstrates the\nfeasibility of this new decomposition method."
},{
    "category": "cs.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1203.3800v7", 
    "title": "Power Series Method applied to Inverse Analysis in Chemical Kinetics   Problem", 
    "arxiv-id": "1203.3800v7", 
    "author": "A. R. Samana", 
    "publish": "2012-03-16T12:41:14Z", 
    "summary": "Power Series Solution Method has been traditionally used to solve Ordinary\nand Partial Linear Differential Equations. However, despite their usefulness\nthe application of this method has been limited to this particular kind of\nequations. In this work we use the method of power series to solve nonlinear\npartial differential equations. The method is applied to solve three versions\nof nonlinear time-dependent Burgers-Type differential equations in order to\ndemonstrate its scope and applicability."
},{
    "category": "cs.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1203.4617v1", 
    "title": "An Arithmetic and Geometric Mean Invariant", 
    "arxiv-id": "1203.4617v1", 
    "author": "Vibeke Libby", 
    "publish": "2012-03-20T22:45:57Z", 
    "summary": "A positive real interval, [a, b], can be partitioned into sub-intervals such\nthat sub-interval widths divided by sub-interval \"average\" values remains\nconstant. That both Arithmetic Mean and Geometric Mean \"average\" values produce\nconstant ratios for the same log scale is the stated invariance proved in this\nshort note. The continuous analog is briefly considered and shown to have\nsimilar properties."
},{
    "category": "cs.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1203.6030v1", 
    "title": "Revisiting the D-iteration method: from theoretical to practical   computation cost", 
    "arxiv-id": "1203.6030v1", 
    "author": "Dohy Hong", 
    "publish": "2012-03-27T17:20:17Z", 
    "summary": "In this paper, we revisit the D-iteration algorithm in order to better\nexplain its connection to the Gauss-Seidel method and different performance\nresults that were observed. In particular, we study here the practical\ncomputation cost based on the execution runtime compared to the theoretical\nnumber of iterations. We also propose an exact formula of the error for\nPageRank class of equations."
},{
    "category": "math.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1205.1768v1", 
    "title": "On the Fourier expansion method for highly accurate computation of the   Voigt/complex error function in a rapid algorithm", 
    "arxiv-id": "1205.1768v1", 
    "author": "B. M. Quine", 
    "publish": "2012-05-07T17:24:32Z", 
    "summary": "In our recent publication [1] we presented an exponential series\napproximation suitable for highly accurate computation of the complex error\nfunction in a rapid algorithm. In this Short Communication we describe how a\nsimplified representation of the proposed complex error function approximation\nmakes possible further algorithmic optimization resulting in a considerable\ncomputational acceleration without compromise on accuracy."
},{
    "category": "cs.MS", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1205.2107v2", 
    "title": "High-Performance Solvers for Dense Hermitian Eigenproblems", 
    "arxiv-id": "1205.2107v2", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-05-09T21:20:55Z", 
    "summary": "We introduce a new collection of solvers - subsequently called EleMRRR - for\nlarge-scale dense Hermitian eigenproblems. EleMRRR solves various types of\nproblems: generalized, standard, and tridiagonal eigenproblems. Among these,\nthe last is of particular importance as it is a solver on its own right, as\nwell as the computational kernel for the first two; we present a fast and\nscalable tridiagonal solver based on the Algorithm of Multiple Relatively\nRobust Representations - referred to as PMRRR. Like the other EleMRRR solvers,\nPMRRR is part of the freely available Elemental library, and is designed to\nfully support both message-passing (MPI) and multithreading parallelism (SMP).\nAs a result, the solvers can equally be used in pure MPI or in hybrid MPI-SMP\nfashion. We conducted a thorough performance study of EleMRRR and ScaLAPACK's\nsolvers on two supercomputers. Such a study, performed with up to 8,192 cores,\nprovides precise guidelines to assemble the fastest solver within the ScaLAPACK\nframework; it also indicates that EleMRRR outperforms even the fastest solvers\nbuilt from ScaLAPACK's components."
},{
    "category": "math.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1205.5672v1", 
    "title": "On Local Super-Penalization of Interior Penalty Discontinuous Galerkin   Methods", 
    "arxiv-id": "1205.5672v1", 
    "author": "Max Jensen", 
    "publish": "2012-05-25T12:02:10Z", 
    "summary": "We prove in an abstract setting that standard (continuous) Galerkin finite\nelement approximations are the limit of interior penalty discontinuous Galerkin\napproximations as the penalty parameter tends to infinity. We apply this result\nto equations of non-negative characteristic form and the non-linear, time\ndependent system of incompressible miscible displacement. Moreover, we\ninvestigate varying the penalty parameter on only a subset of a triangulation\nand the effects of local super-penalization on the stability of the method,\nresulting in a partly continuous, partly discontinuous method in the limit. An\niterative automatic procedure is also proposed for the determination of the\ncontinuous region of the domain without loss of stability of the method."
},{
    "category": "math.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1205.5824v2", 
    "title": "High-Resolution Finite Volume Modeling of Wave Propagation in   Orthotropic Poroelastic Media", 
    "arxiv-id": "1205.5824v2", 
    "author": "Randall J. LeVeque", 
    "publish": "2012-05-25T21:11:19Z", 
    "summary": "Poroelasticity theory models the dynamics of porous, fluid-saturated media.\nIt was pioneered by Maurice Biot in the 1930s through 1960s, and has\napplications in several fields, including geophysics and modeling of in vivo\nbone. A wide variety of methods have been used to model poroelasticity,\nincluding finite difference, finite element, pseudospectral, and discontinuous\nGalerkin methods. In this work we use a Cartesian-grid high-resolution finite\nvolume method to numerically solve Biot's equations in the time domain for\northotropic materials, with the stiff relaxation source term in the equations\nincorporated using operator splitting. This class of finite volume method has\nseveral useful properties, including the ability to use wave limiters to reduce\nnumerical artifacts in the solution, ease of incorporating material\ninhomogeneities, low memory overhead, and an explicit time-stepping approach.\nTo the authors' knowledge, this is the first use of high-resolution finite\nvolume methods to model poroelasticity. The solution code uses the CLAWPACK\nfinite volume method software, which also includes block-structured adaptive\nmesh refinement in its AMRCLAW variant. We present convergence results for\nknown analytic plane wave solutions, achieving second-order convergence rates\noutside of the stiff regime of the system. Our convergence rates are degraded\nin the stiff regime, but we still achieve similar levels of error on the finest\ngrids examined. We also demonstrate good agreement against other numerical\nresults from the literature. To aid in reproducibility, we provide all of the\ncode used to generate the results of this paper, at\nhttps://bitbucket.org/grady_lemoine/poro-2d-cartesian-archive ."
},{
    "category": "cs.NA", 
    "doi": "10.1587/transinf.E96.D.731", 
    "link": "http://arxiv.org/pdf/1205.5911v1", 
    "title": "A Hough Transform Approach to Solving Linear Min-Max Problems", 
    "arxiv-id": "1205.5911v1", 
    "author": "Carmi Grushko", 
    "publish": "2012-05-26T19:20:00Z", 
    "summary": "Several ways to accelerate the solution of 2D/3D linear min-max problems in\n$n$ constraints are discussed. We also present an algorithm for solving such\nproblems in the 2D case, which is superior to CGAL's linear programming solver,\nboth in performance and in stability."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1007/s10686-012-9300-7", 
    "link": "http://arxiv.org/pdf/1205.6654v1", 
    "title": "Reduced Ambiguity Calibration for LOFAR", 
    "arxiv-id": "1205.6654v1", 
    "author": "Sarod Yatawatta", 
    "publish": "2012-05-30T13:01:24Z", 
    "summary": "Interferometric calibration always yields non unique solutions. It is\ntherefore essential to remove these ambiguities before the solutions could be\nused in any further modeling of the sky, the instrument or propagation effects\nsuch as the ionosphere. We present a method for LOFAR calibration which does\nnot yield a unitary ambiguity, especially under ionospheric distortions. We\nalso present exact ambiguities we get in our solutions, in closed form. Casting\nthis as an optimization problem, we also present conditions for this approach\nto work. The proposed method enables us to use the solutions obtained via\ncalibration for further modeling of instrumental and propagation effects. We\nprovide extensive simulation results on the performance of our method.\nMoreover, we also give cases where due to degeneracy, this method fails to\nperform as expected and in such cases, we suggest exploiting diversity in time,\nspace and frequency."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.patrec.2012.03.002", 
    "link": "http://arxiv.org/pdf/1206.2061v1", 
    "title": "Comments on \"On Approximating Euclidean Metrics by Weighted t-Cost   Distances in Arbitrary Dimension\"", 
    "arxiv-id": "1206.2061v1", 
    "author": "Fatih Celiker", 
    "publish": "2012-06-10T22:13:45Z", 
    "summary": "Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently\nintroduced a class of distance functions called weighted t-cost distances that\ngeneralize m-neighbor, octagonal, and t-cost distances. He proved that weighted\nt-cost distances form a family of metrics and derived an approximation for the\nEuclidean norm in $\\mathbb{Z}^n$. In this note we compare this approximation to\ntwo previously proposed Euclidean norm approximations and demonstrate that the\nempirical average errors given by Mukherjee are significantly optimistic in\n$\\mathbb{R}^n$. We also propose a simple normalization scheme that improves the\naccuracy of his approximation substantially with respect to both average and\nmaximum relative errors."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.patrec.2012.03.002", 
    "link": "http://arxiv.org/pdf/1206.2948v1", 
    "title": "The cost of continuity: performance of iterative solvers on isogeometric   finite elements", 
    "arxiv-id": "1206.2948v1", 
    "author": "V. M. Calo", 
    "publish": "2012-06-13T21:54:08Z", 
    "summary": "In this paper we study how the use of a more continuous set of basis\nfunctions affects the cost of solving systems of linear equations resulting\nfrom a discretized Galerkin weak form. Specifically, we compare performance of\nlinear solvers when discretizing using $C^0$ B-splines, which span traditional\nfinite element spaces, and $C^{p-1}$ B-splines, which represent maximum\ncontinuity. We provide theoretical estimates for the increase in cost of the\nmatrix-vector product as well as for the construction and application of\nblack-box preconditioners. We accompany these estimates with numerical results\nand study their sensitivity to various grid parameters such as element size $h$\nand polynomial order of approximation $p$. Finally, we present timing results\nfor a range of preconditioning options for the Laplace problem. We conclude\nthat the matrix-vector product operation is at most $\\slfrac{33p^2}{8}$ times\nmore expensive for the more continuous space, although for moderately low $p$,\nthis number is significantly reduced. Moreover, if static condensation is not\nemployed, this number further reduces to at most a value of 8, even for high\n$p$. Preconditioning options can be up to $p^3$ times more expensive to setup,\nalthough this difference significantly decreases for some popular\npreconditioners such as Incomplete LU factorization."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.patrec.2012.03.002", 
    "link": "http://arxiv.org/pdf/1206.3177v1", 
    "title": "Optimizing the eigenvector computation algorithm with diffusion approach", 
    "arxiv-id": "1206.3177v1", 
    "author": "Philippe Jacquet", 
    "publish": "2012-06-14T16:44:10Z", 
    "summary": "In this paper, we apply the ideas of the matrix column based diffusion\napproach to define a new eigenvector computation algorithm of a stationary\nprobability of a Markov chain."
},{
    "category": "cs.AI", 
    "doi": "10.5120/4837-7097", 
    "link": "http://arxiv.org/pdf/1206.4329v1", 
    "title": "An Improved Gauss-Newtons Method based Back-propagation Algorithm for   Fast Convergence", 
    "arxiv-id": "1206.4329v1", 
    "author": "Achintya Das", 
    "publish": "2012-06-19T20:20:56Z", 
    "summary": "The present work deals with an improved back-propagation algorithm based on\nGauss-Newton numerical optimization method for fast convergence. The steepest\ndescent method is used for the back-propagation. The algorithm is tested using\nvarious datasets and compared with the steepest descent back-propagation\nalgorithm. In the system, optimization is carried out using multilayer neural\nnetwork. The efficacy of the proposed method is observed during the training\nperiod as it converges quickly for the dataset used in test. The requirement of\nmemory for computing the steps of algorithm is also analyzed."
},{
    "category": "cs.NA", 
    "doi": "10.5120/4837-7097", 
    "link": "http://arxiv.org/pdf/1206.4481v2", 
    "title": "Parsimonious Mahalanobis Kernel for the Classification of High   Dimensional Data", 
    "arxiv-id": "1206.4481v2", 
    "author": "J. A. Benediktsson", 
    "publish": "2012-06-20T12:49:48Z", 
    "summary": "The classification of high dimensional data with kernel methods is considered\nin this article. Exploit- ing the emptiness property of high dimensional\nspaces, a kernel based on the Mahalanobis distance is proposed. The computation\nof the Mahalanobis distance requires the inversion of a covariance matrix. In\nhigh dimensional spaces, the estimated covariance matrix is ill-conditioned and\nits inversion is unstable or impossible. Using a parsimonious statistical\nmodel, namely the High Dimensional Discriminant Analysis model, the specific\nsignal and noise subspaces are estimated for each considered class making the\ninverse of the class specific covariance matrix explicit and stable, leading to\nthe definition of a parsimonious Mahalanobis kernel. A SVM based framework is\nused for selecting the hyperparameters of the parsimonious Mahalanobis kernel\nby optimizing the so-called radius-margin bound. Experimental results on three\nhigh dimensional data sets show that the proposed kernel is suitable for\nclassifying high dimensional data, providing better classification accuracies\nthan the conventional Gaussian kernel."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.laa.2012.12.022", 
    "link": "http://arxiv.org/pdf/1208.3805v3", 
    "title": "Paved with Good Intentions: Analysis of a Randomized Block Kaczmarz   Method", 
    "arxiv-id": "1208.3805v3", 
    "author": "Joel A. Tropp", 
    "publish": "2012-08-19T03:51:16Z", 
    "summary": "The block Kaczmarz method is an iterative scheme for solving overdetermined\nleast-squares problems. At each step, the algorithm projects the current\niterate onto the solution space of a subset of the constraints. This paper\ndescribes a block Kaczmarz algorithm that uses a randomized control scheme to\nchoose the subset at each step. This algorithm is the first block Kaczmarz\nmethod with an (expected) linear rate of convergence that can be expressed in\nterms of the geometric properties of the matrix and its submatrices. The\nanalysis reveals that the algorithm is most effective when it is given a good\nrow paving of the matrix, a partition of the rows into well-conditioned blocks.\nThe operator theory literature provides detailed information about the\nexistence and construction of good row pavings. Together, these results yield\nan efficient block Kaczmarz scheme that applies to many overdetermined\nleast-squares problem."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1208.4578v2", 
    "title": "Signal Analysis based on Complex Wavelet Signs", 
    "arxiv-id": "1208.4578v2", 
    "author": "Peter Massopust", 
    "publish": "2012-08-22T19:05:28Z", 
    "summary": "We propose a signal analysis tool based on the sign (or the phase) of complex\nwavelet coefficients, which we call a signature. The signature is defined as\nthe fine-scale limit of the signs of a signal's complex wavelet coefficients.\nWe show that the signature equals zero at sufficiently regular points of a\nsignal whereas at salient features, such as jumps or cusps, it is non-zero. At\nsuch feature points, the orientation of the signature in the complex plane can\nbe interpreted as an indicator of local symmetry and antisymmetry. We establish\nthat the signature rotates in the complex plane under fractional Hilbert\ntransforms. We show that certain random signals, such as white Gaussian noise\nand Brownian motions, have a vanishing signature. We derive an appropriate\ndiscretization and show the applicability to signal analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1208.5649v1", 
    "title": "Numerical Methods for Solving Convection-Diffusion Problems", 
    "arxiv-id": "1208.5649v1", 
    "author": "P. Vabishchevich", 
    "publish": "2012-08-28T13:06:55Z", 
    "summary": "Convection-diffusion equations provide the basis for describing heat and mass\ntransfer phenomena as well as processes of continuum mechanics. To handle flows\nin porous media, the fundamental issue is to model correctly the convective\ntransport of individual phases. Moreover, for compressible media, the pressure\nequation itself is just a time-dependent convection-diffusion equation.\n  For different problems, a convection-diffusion equation may be be written in\nvarious forms. The most popular formulation of convective transport employs the\ndivergent (conservative) form. In some cases, the nondivergent (characteristic)\nform seems to be preferable. The so-called skew-symmetric form of convective\ntransport operators that is the half-sum of the operators in the divergent and\nnondivergent forms is of great interest in some applications.\n  Here we discuss the basic classes of discretization in space: finite\ndifference schemes on rectangular grids, approximations on general polyhedra\n(the finite volume method), and finite element procedures. The key properties\nof discrete operators are studied for convective and diffusive transport. We\nemphasize the problems of constructing approximations for convection and\ndiffusion operators that satisfy the maximum principle at the discrete level\n--- they are called monotone approximations.\n  Two- and three-level schemes are investigated for transient problems.\nUnconditionally stable explicit-implicit schemes are developed for\nconvection-diffusion problems. Stability conditions are obtained both in\nfinite-dimensional Hilbert spaces and in Banach spaces depending on the form in\nwhich the convection-diffusion equation is written."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1208.6140v1", 
    "title": "Unconditionally stable schemes for non-stationary convection-diffusion   equations", 
    "arxiv-id": "1208.6140v1", 
    "author": "M. Vasil'eva", 
    "publish": "2012-08-30T11:31:29Z", 
    "summary": "Convection-diffusion problem are the base for continuum mechanics. The main\nfeatures of these problems are associated with an indefinite operator the\nproblem. In this work we construct unconditionally stable scheme for\nnon-stationary convection-diffusion equations, which are based on use of new\nvariables. Also, we consider these equations in the form of\nconvection-diffusion-reaction and construct unconditionally stable schemes when\nexplicit-implicit approximations are used with splitting of the reaction\noperator."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.1406v2", 
    "title": "Adaptive Smolyak Pseudospectral Approximations", 
    "arxiv-id": "1209.1406v2", 
    "author": "Youssef M. Marzouk", 
    "publish": "2012-09-06T20:35:21Z", 
    "summary": "Polynomial approximations of computationally intensive models are central to\nuncertainty quantification. This paper describes an adaptive method for\nnon-intrusive pseudospectral approximation, based on Smolyak's algorithm with\ngeneralized sparse grids. We rigorously analyze and extend the non-adaptive\nmethod proposed in [6], and compare it to a common alternative approach for\nusing sparse grids to construct polynomial approximations, direct quadrature.\nAnalysis of direct quadrature shows that O(1) errors are an intrinsic property\nof some configurations of the method, as a consequence of internal aliasing. We\nprovide precise conditions, based on the chosen polynomial basis and quadrature\nrules, under which this aliasing error occurs. We then establish theoretical\nresults on the accuracy of Smolyak pseudospectral approximation, and show that\nthe Smolyak approximation avoids internal aliasing and makes far more effective\nuse of sparse function evaluations. These results are applicable to broad\nchoices of quadrature rule and generalized sparse grids. Exploiting this\nflexibility, we introduce a greedy heuristic for adaptive refinement of the\npseudospectral approximation. We numerically demonstrate convergence of the\nalgorithm on the Genz test functions, and illustrate the accuracy and\nefficiency of the adaptive approach on a realistic chemical kinetics problem."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.1910v1", 
    "title": "On Implementation and Evaluation of Inverse Iteration Algorithm with   compact WY Orthogonalization", 
    "arxiv-id": "1209.1910v1", 
    "author": "Yoshimasa Nakamura", 
    "publish": "2012-09-10T09:04:13Z", 
    "summary": "A new inverse iteration algorithm that can be used to compute all the\neigenvectors of a real symmetric tri-diagonal matrix on parallel computers is\ndeveloped. The modified Gram-Schmidt orthogonalization is used in the classical\ninverse iteration. This algorithm is sequential and causes a bottleneck in\nparallel computing. In this paper, the use of the compact WY representation is\nproposed in the orthogonalization process of the inverse iteration with the\nHouseholder transformation. This change results in drastically reduced\nsynchronization cost in parallel computing. The new algorithm is evaluated on\nboth an 8-core and a 32-core parallel computer, and it is shown that the new\nalgorithm is greatly faster than the classical inverse iteration algorithm in\ncomputing all the eigenvectors of matrices with several thousand dimensions."
},{
    "category": "cs.PF", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.3253v1", 
    "title": "A framework for the analytical performance assessment of matrix and   tensor-based ESPRIT-type algorithms", 
    "arxiv-id": "1209.3253v1", 
    "author": "Martin Haardt", 
    "publish": "2012-09-14T16:36:49Z", 
    "summary": "In this paper we present a generic framework for the asymptotic performance\nanalysis of subspace-based parameter estimation schemes. It is based on earlier\nresults on an explicit first-order expansion of the estimation error in the\nsignal subspace obtained via an SVD of the noisy observation matrix. We extend\nthese results in a number of aspects. Firstly, we derive an explicit\nfirst-order expansion of the Higher- Order SVD (HOSVD)-based subspace estimate.\nSecondly, we show how to obtain explicit first-order expansions of the\nestimation error of ESPRIT-type algorithms and provide the expressions for\nmatrix-based and tensor-based Standard ESPRIT and Unitary ESPRIT. Thirdly, we\nderive closed-form expressions for the mean square error (MSE) and show that\nthey only depend on the second-order moments of the noise. Hence, we only need\nthe noise to be zero mean and possess finite second order moments. Fourthly, we\ninvestigate the effect of using Structured Least Squares (SLS) to solve the\noverdetermined shift invariance equations in ESPRIT and provide an explicit\nfirst-order expansion as well as a closed-form MSE expression. Finally, we\nsimplify the MSE for the special case of a single source and compute the\nasymptotic efficiency of the investigated ESPRIT-type algorithms in compact\nclosed-form expressions which only depend on the array size and the effective\nSNR. Our results are more general than existing results on the performance\nanalysis of ESPRIT-type algorithms since (a) we do not need any assumptions\nabout the noise except for the mean to be zero and the second-order moments to\nbe finite (in contrast to earlier results that require Gaussianity or\nsecond-order circular symmetry); (b) our results are asymptotic in the\neffective SNR, i.e., we do not require the number of samples to be large; (c)\nwe present a framework that incorporates various ESPRIT-type algorithms in one\nunified manner."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.4508v1", 
    "title": "Deterministic algorithms for skewed matrix products", 
    "arxiv-id": "1209.4508v1", 
    "author": "Konstantin Kutzkov", 
    "publish": "2012-09-20T12:27:15Z", 
    "summary": "Recently, Pagh presented a randomized approximation algorithm for the\nmultiplication of real-valued matrices building upon work for detecting the\nmost frequent items in data streams. We continue this line of research and\npresent new {\\em deterministic} matrix multiplication algorithms.\n  Motivated by applications in data mining, we first consider the case of\nreal-valued, nonnegative $n$-by-$n$ input matrices $A$ and $B$, and show how to\nobtain a deterministic approximation of the weights of individual entries, as\nwell as the entrywise $p$-norm, of the product $AB$. The algorithm is simple,\nspace efficient and runs in one pass over the input matrices. For a user\ndefined $b \\in (0, n^2)$ the algorithm runs in time $O(nb +\nn\\cdot\\text{Sort}(n))$ and space $O(n + b)$ and returns an approximation of the\nentries of $AB$ within an additive factor of $\\|AB\\|_{E1}/b$, where $\\|C\\|_{E1}\n= \\sum_{i, j} |C_{ij}|$ is the entrywise 1-norm of a matrix $C$ and\n$\\text{Sort}(n)$ is the time required to sort $n$ real numbers in linear space.\nBuilding upon a result by Berinde et al. we show that for skewed matrix\nproducts (a common situation in many real-life applications) the algorithm is\nmore efficient and achieves better approximation guarantees than previously\nknown randomized algorithms.\n  When the input matrices are not restricted to nonnegative entries, we present\na new deterministic group testing algorithm detecting nonzero entries in the\nmatrix product with large absolute value. The algorithm is clearly outperformed\nby randomized matrix multiplication algorithms, but as a byproduct we obtain\nthe first $O(n^{2 + \\varepsilon})$-time deterministic algorithm for matrix\nproducts with $O(\\sqrt{n})$ nonzero entries."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.5173v1", 
    "title": "On the Parametric Instability Caused by Step Size Variation in   Runge-Kutta-Nystr\u00f6m Methods", 
    "arxiv-id": "1209.5173v1", 
    "author": "Robert Pich\u00e9", 
    "publish": "2012-09-24T07:33:19Z", 
    "summary": "The parametric instability arising when ordinary differential equations\n(ODEs) are numerically integrated with Runge-Kutta-Nystr\\\"om (RKN) methods with\nvarying step sizes is investigated. It is shown that when linear constant\ncoefficient ODEs are integrated with RKN methods that are based on A-stable\nRunge-Kutta methods, the solution is nonincreasing in some norm for all\npositive step sizes, constant or varying. Perturbation methods are used to\nquantify the critical step sizes associated with parametric instability."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.acha.2015.08.005", 
    "link": "http://arxiv.org/pdf/1209.5198v2", 
    "title": "Fast matrix decomposition in F2", 
    "arxiv-id": "1209.5198v2", 
    "author": "Anna Rimoldi", 
    "publish": "2012-09-24T08:58:34Z", 
    "summary": "In this work an efficient algorithm to perform a block decomposition (and so\nto compute the rank) of large dense rectangular matrices with entries in\n$\\FF_2$ is presented. Depending on the way the matrix is stored, the operations\nacting on rows or block of consecutive columns (stored as one integer) should\nbe preferred. In this paper, an algorithm that completely avoids the column\npermutations is given. In particular, a block decomposition is presented and\nits running times are compared with the ones adopted into SAGE."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1093/mnras/sts069", 
    "link": "http://arxiv.org/pdf/1209.5492v1", 
    "title": "On the interpolation of calibration solutions obtained in radio   interferometry", 
    "arxiv-id": "1209.5492v1", 
    "author": "Sarod Yatawatta", 
    "publish": "2012-09-25T04:51:31Z", 
    "summary": "Full polarimetric radio interferometric calibration is performed by\nestimating 2 by 2 Jones matrices representing instrumental and propagation\neffects. The solutions obtained in this way differ from the true solutions by a\n2 by 2 unitary matrix ambiguity. This ambiguity is common to all stations for\nwhich a solution is obtained but it is different for solutions obtained at\ndifferent time and frequency intervals. Therefore, straightforward\ninterpolation of solutions obtained at different time and frequency intervals\nis not possible. In this paper, we propose to use the theory of quotient\nmanifolds for obtaining correct interpolants that are immune to unitary matrix\nambiguities."
},{
    "category": "cs.DS", 
    "doi": "10.1093/mnras/sts069", 
    "link": "http://arxiv.org/pdf/1209.5647v3", 
    "title": "Additive Update Algorithm for Nonnegative Matrix Factorization", 
    "arxiv-id": "1209.5647v3", 
    "author": "Pham Van At", 
    "publish": "2012-09-25T15:36:32Z", 
    "summary": "Nonnegative matrix factorization (NMF) is an emerging technique with a wide\nspectrum of potential applications in data analysis. Mathematically, NMF can be\nformulated as a minimization problem with nonnegative constraints. This problem\nis currently attracting much attention from researchers for theoretical reasons\nand for potential applications. Currently, the most popular approach to solve\nNMF is the multiplicative update algorithm proposed by D.D. Lee and H.S. Seung.\nIn this paper, we propose an additive update algorithm, that has faster\ncomputational speed than the algorithm of D.D. Lee and H.S. Seung."
},{
    "category": "cs.MS", 
    "doi": "10.1093/mnras/sts069", 
    "link": "http://arxiv.org/pdf/1210.4539v2", 
    "title": "A Robust Complex Division in Scilab", 
    "arxiv-id": "1210.4539v2", 
    "author": "Robert L. Smith", 
    "publish": "2012-10-16T19:52:16Z", 
    "summary": "The most widely used algorithm for floating point complex division, known as\nSmith's method, may fail more often than expected. This document presents two\nimproved complex division algorithms. We present a proof of the robustness of\nthe first improved algorithm. Numerical simulations show that this algorithm\nperforms well in practice and is significantly more robust than other known\nimplementations. By combining additionnal scaling methods with this first\nalgorithm, we were able to create a second algorithm, which rarely fails."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2013.07.010", 
    "link": "http://arxiv.org/pdf/1210.5290v4", 
    "title": "A numerical framework for diffusion-controlled bimolecular-reactive   systems to enforce maximum principles and non-negative constraint", 
    "arxiv-id": "1210.5290v4", 
    "author": "A. J. Valocchi", 
    "publish": "2012-10-19T01:06:02Z", 
    "summary": "We present a novel computational framework for diffusive-reactive systems\nthat satisfies the non-negative constraint and maximum principles on general\ncomputational grids. The governing equations for the concentration of reactants\nand product are written in terms of tensorial diffusion-reaction equations. %\nWe restrict our studies to fast irreversible bimolecular reactions. If one\nassumes that the reaction is diffusion-limited and all chemical species have\nthe same diffusion coefficient, one can employ a linear transformation to\nrewrite the governing equations in terms of invariants, which are unaffected by\nthe reaction. This results in two uncoupled tensorial diffusion equations in\nterms of these invariants, which are solved using a novel non-negative solver\nfor tensorial diffusion-type equations. The concentrations of the reactants and\nthe product are then calculated from invariants using algebraic manipulations.\nThe novel aspect of the proposed computational framework is that it will always\nproduce physically meaningful non-negative values for the concentrations of all\nchemical species. Several representative numerical examples are presented to\nillustrate the robustness, convergence, and the numerical performance of the\nproposed computational framework. We will also compare the proposed framework\nwith other popular formulations. In particular, we will show that the Galerkin\nformulation (which is the standard single-field formulation) does not produce\nreliable solutions, and the reason can be attributed to the fact that the\nsingle-field formulation does not guarantee non-negative solutions. We will\nalso show that the clipping procedure (which produces non-negative solutions\nbut is considered as a variational crime) does not give accurate results when\ncompared with the proposed computational framework."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2013.07.010", 
    "link": "http://arxiv.org/pdf/1210.5844v3", 
    "title": "Epigraphical splitting for solving constrained convex formulations of   inverse problems with proximal tools", 
    "arxiv-id": "1210.5844v3", 
    "author": "B\u00e9atrice Pesquet-Popescu", 
    "publish": "2012-10-22T09:12:48Z", 
    "summary": "We propose a proximal approach to deal with a class of convex variational\nproblems involving nonlinear constraints. A large family of constraints, proven\nto be effective in the solution of inverse problems, can be expressed as the\nlower level set of a sum of convex functions evaluated over different, but\npossibly overlapping, blocks of the signal. For such constraints, the\nassociated projection operator generally does not have a simple form. We\ncircumvent this difficulty by splitting the lower level set into as many\nepigraphs as functions involved in the sum. A closed half-space constraint is\nalso enforced, in order to limit the sum of the introduced epigraphical\nvariables to the upper bound of the original lower level set. In this paper, we\nfocus on a family of constraints involving linear transforms of distance\nfunctions to a convex set or $\\ell_{1,p}$ norms with $p\\in \\{1,2,\\infty\\}$. In\nthese cases, the projection onto the epigraph of the involved function has a\nclosed form expression.\n  The proposed approach is validated in the context of image restoration with\nmissing samples, by making use of constraints based on Non-Local Total\nVariation. Experiments show that our method leads to significant improvements\nin term of convergence speed over existing algorithms for solving similar\nconstrained problems. A second application to a pulse shape design problem is\nprovided in order to illustrate the flexibility of the proposed approach."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2013.07.010", 
    "link": "http://arxiv.org/pdf/1210.8072v1", 
    "title": "Strict localization of eigenvectors and eigenvalues", 
    "arxiv-id": "1210.8072v1", 
    "author": "Jacek Tabor", 
    "publish": "2012-10-30T16:31:08Z", 
    "summary": "In this article we show and implement a simple and effcient method to\nstrictly locate eigenvectors and eigenvalues of a given matrix, based on the\nmodified cone condition. As a consequence we can also effectively localize\nzeros of complex polynomials."
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.2517v2", 
    "title": "A SVD accelerated kernel-independent fast multipole method and its   application to BEM", 
    "arxiv-id": "1211.2517v2", 
    "author": "Junjie Rong", 
    "publish": "2012-11-12T06:27:22Z", 
    "summary": "The kernel-independent fast multipole method (KIFMM) proposed in [1] is of\nalmost linear complexity. In the original KIFMM the time-consuming M2L\ntranslations are accelerated by FFT. However, when more equivalent points are\nused to achieve higher accuracy, the efficiency of the FFT approach tends to be\nlower because more auxiliary volume grid points have to be added. In this\npaper, all the translations of the KIFMM are accelerated by using the singular\nvalue decomposition (SVD) based on the low-rank property of the translating\nmatrices. The acceleration of M2L is realized by first transforming the\nassociated translating matrices into more compact form, and then using low-rank\napproximations. By using the transform matrices for M2L, the orders of the\ntranslating matrices in upward and downward passes are also reduced. The\nimproved KIFMM is then applied to accelerate BEM. The performance of the\nproposed algorithms are demonstrated by three examples. Numerical results show\nthat, compared with the original KIFMM, the present method can reduce about 40%\nof the iterating time and 25% of the memory requirement."
},{
    "category": "stat.AP", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.3211v1", 
    "title": "Effectiveness of sparse Bayesian algorithm for MVAR coefficient   estimation in MEG/EEG source-space causality analysis", 
    "arxiv-id": "1211.3211v1", 
    "author": "Srikantan S. Nagarajan", 
    "publish": "2012-11-14T06:36:15Z", 
    "summary": "This paper examines the effectiveness of a sparse Bayesian algorithm to\nestimate multivariate autoregressive coefficients when a large amount of\nbackground interference exists. This paper employs computer experiments to\ncompare two methods in the source-space causality analysis: the conventional\nleast-squares method and a sparse Bayesian method. Results of our computer\nexperiments show that the interference affects the least-squares method in a\nvery severe manner. It produces large false-positive results, unless the\nsignal-to-interference ratio is very high. On the other hand, the sparse\nBayesian method is relatively insensitive to the existence of interference.\nHowever, this robustness of the sparse Bayesian method is attained on the\nscarifies of the detectability of true causal relationship. Our experiments\nalso show that the surrogate data bootstrapping method tends to give a\nstatistical threshold that are too low for the sparse method.\n  The permutation-test-based method gives a higher (more conservative)\nthreshold and it should be used with the sparse Bayesian method whenever the\ncontrol period is available."
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.3821v1", 
    "title": "Report: Error estimation of recovered solution in FE analysis", 
    "arxiv-id": "1211.3821v1", 
    "author": "Francisco Javier Fuenmayor Fern\u00e1ndez", 
    "publish": "2012-11-16T08:28:29Z", 
    "summary": "The recovery type error estimators introduced by Zienkiewicz and Zhu use a\nrecovered stress field evaluated from the Finite Element (FE) solution. Their\naccuracy depends on the quality of the recovered field. In this sense, accurate\nresults are obtained using recovery procedures based on the Superconvergent\nPatch recovery technique (SPR). These error estimators can be easily\nimplemented and provide accurate estimates. Another important feature is that\nthe recovered solution is of a better quality than the FE solution and can\ntherefore be used as an enhanced solution. We have developed an SPR-type\nrecovery technique that considers equilibrium and displacements constraints to\nobtain a very accurate recovered displacements field from which a recovered\nstress field can also be evaluated. We propose the use of these recovered\nfields as the standard output of the FE code instead of the raw FE solution.\nTechniques to quantify the error of the recovered solution are therefore\nneeded. In this report we present an error estimation technique that accurately\nevaluates the error of the recovered solution both at global and local levels\nin the FEM and XFEM frameworks. We have also developed an h-adaptive mesh\nrefinement strategy based on the error of the recovered solution. As the\nconverge rate of the error of the recovered solution is higher than that of the\nFE one, the computational cost required to obtain a solution with a prescribed\naccuracy is smaller than for traditional h-adaptive processes."
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.4516v1", 
    "title": "Numerical comparison of different algorithms for construction of wavelet   matrices", 
    "arxiv-id": "1211.4516v1", 
    "author": "Lasha Ephremidze", 
    "publish": "2012-11-19T17:37:59Z", 
    "summary": "Factorization of compact wavelet matrices into primitive ones has been known\nfor more than 20 years. This method makes it possible to generate wavelet\nmatrix coefficients and also to specify them by their first row. Recently, a\nnew parametrization of compact wavelet matrices of the same order and degree\nhas been introduced by the last author. This method also enables us to fulfill\nthe above mentioned tasks of matrix constructions. In the present paper, we\nbriefly describe the corresponding algorithms based on two different methods,\nand numerically compare their performance"
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.5052v1", 
    "title": "A Mathematical Random Number Generator (MRNG)", 
    "arxiv-id": "1211.5052v1", 
    "author": "Guillermo Oviedo", 
    "publish": "2012-11-21T15:06:41Z", 
    "summary": "A novel Mathematical Random Number Generator (MRNG) is presented here. In\nthis case, \"mathematical\" refers to the fact that to construct that generator\nit is not necessary to resort to a physical phenomenon, such as the thermal\nnoise of an electronic device, but rather to a mathematical procedure. The MRNG\ngenerates binary strings - in principle, as long as desired - which may be\nconsidered genuinely random in the sense that they pass the statistical tests\ncurrently accepted to evaluate the randomness of those strings. From those\nstrings, the MRNG also generates random numbers expressed in base 10. An MRNG\nhas been installed as a facility on the following web page:\nhttp://www.appliedmathgroup.org. This generator may be used for applications in\ntasks in: a) computational simulation of probabilistic-type systems, and b) the\nrandom selection of samples of different populations. Users interested in\napplications in cryptography can build another MRNG, but they would have to\nwithhold information - specified in section 5 - from people who are not\nauthorized to decode messages encrypted using that resource."
},{
    "category": "math.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1211.5082v1", 
    "title": "The Monogenic Synchrosqueezed Wavelet Transform: A tool for the   Decomposition/Demodulation of AM-FM images", 
    "arxiv-id": "1211.5082v1", 
    "author": "Val\u00e9rie Perrier", 
    "publish": "2012-11-20T08:23:30Z", 
    "summary": "The synchrosqueezing method aims at decomposing 1D functions as\nsuperpositions of a small number of \"Intrinsic Modes\", supposed to be well\nseparated both in time and frequency. Based on the unidimensional wavelet\ntransform and its reconstruction properties, the synchrosqueezing transform\nprovides a powerful representation of multicomponent signals in the\ntime-frequency plane, together with a reconstruction of each mode.\n  In this paper, a bidimensional version of the synchrosqueezing transform is\ndefined, by considering a well-adapted extension of the concept of analytic\nsignal to images: the monogenic signal. The natural bidimensional counterpart\nof the notion of Intrinsic Mode is then the concept of \"Intrinsic Monogenic\nMode\" that we define. Thereafter, we investigate the properties of its\nassociated Monogenic Wavelet Decomposition. This leads to a natural bivariate\nextension of the Synchrosqueezed Wavelet Transform, for decomposing and\nprocessing multicomponent images. Numerical tests validate the effectiveness of\nthe method for different examples."
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1212.0417v1", 
    "title": "An inverse iteration method for eigenvalue problems with eigenvector   nonlinearities", 
    "arxiv-id": "1212.0417v1", 
    "author": "Wim Michiels", 
    "publish": "2012-12-03T15:31:14Z", 
    "summary": "Consider a symmetric matrix $A(v)\\in\\RR^{n\\times n}$ depending on a vector\n$v\\in\\RR^n$ and satisfying the property $A(\\alpha v)=A(v)$ for any\n$\\alpha\\in\\RR\\backslash{0}$. We will here study the problem of finding\n$(\\lambda,v)\\in\\RR\\times \\RR^n\\backslash\\{0\\}$ such that $(\\lambda,v)$ is an\neigenpair of the matrix $A(v)$ and we propose a generalization of inverse\niteration for eigenvalue problems with this type of eigenvector nonlinearity.\nThe convergence of the proposed method is studied and several convergence\nproperties are shown to be analogous to inverse iteration for standard\neigenvalue problems, including local convergence properties. The algorithm is\nalso shown to be equivalent to a particular discretization of an associated\nordinary differential equation, if the shift is chosen in a particular way. The\nalgorithm is adapted to a variant of the Schr\\\"odinger equation known as the\nGross-Pitaevskii equation. We use numerical simulations toillustrate the\nconvergence properties, as well as the efficiency of the algorithm and the\nadaption."
},{
    "category": "cs.LO", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1212.1251v2", 
    "title": "Transient Reward Approximation for Continuous-Time Markov Chains", 
    "arxiv-id": "1212.1251v2", 
    "author": "Bernd Becker", 
    "publish": "2012-12-06T07:27:29Z", 
    "summary": "We are interested in the analysis of very large continuous-time Markov chains\n(CTMCs) with many distinct rates. Such models arise naturally in the context of\nreliability analysis, e.g., of computer network performability analysis, of\npower grids, of computer virus vulnerability, and in the study of crowd\ndynamics. We use abstraction techniques together with novel algorithms for the\ncomputation of bounds on the expected final and accumulated rewards in\ncontinuous-time Markov decision processes (CTMDPs). These ingredients are\ncombined in a partly symbolic and partly explicit (symblicit) analysis\napproach. In particular, we circumvent the use of multi-terminal decision\ndiagrams, because the latter do not work well if facing a large number of\ndifferent rates. We demonstrate the practical applicability and efficiency of\nthe approach on two case studies."
},{
    "category": "cs.NA", 
    "doi": "10.2495/BEM360351", 
    "link": "http://arxiv.org/pdf/1212.1992v1", 
    "title": "A direct solver with reutilization of previously-computed LU   factorizations for h-adaptive finite element grids with point singularities", 
    "arxiv-id": "1212.1992v1", 
    "author": "David Pardo", 
    "publish": "2012-12-10T08:31:14Z", 
    "summary": "This paper describes a direct solver algorithm for a sequence of finite\nelement meshes that are h-refined towards one or several point singularities.\nFor such a sequence of grids, the solver delivers linear computational cost\nO(N) in terms of CPU time and memory with respect to the number of unknowns N.\nThe linear computational cost is achieved by utilizing the recursive structure\nprovided by the sequence of h-adaptive grids with a special construction of the\nelimination tree that allows for reutilization of previously computed partial\nLU factorizations over the entire unrefined part of the computational mesh. The\nreutilization technique reduces the computational cost of the entire sequence\nof h-refined grids from O(N^2) down to O(N). Theoretical estimates are\nillustrated with numerical results on two- and three-dimensional model problems\nexhibiting one or several point singularities."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1212.6680v3", 
    "title": "Nonsymmetric multigrid preconditioning for conjugate gradient methods", 
    "arxiv-id": "1212.6680v3", 
    "author": "Andrew V. Knyazev", 
    "publish": "2012-12-30T01:15:51Z", 
    "summary": "We numerically analyze the possibility of turning off post-smoothing\n(relaxation) in geometric multigrid when used as a preconditioner in conjugate\ngradient linear and eigenvalue solvers for the 3D Laplacian. The geometric\nSemicoarsening Multigrid (SMG) method is provided by the hypre parallel\nsoftware package. We solve linear systems using two variants (standard and\nflexible) of the preconditioned conjugate gradient (PCG) and preconditioned\nsteepest descent (PSD) methods. The eigenvalue problems are solved using the\nlocally optimal block preconditioned conjugate gradient (LOBPCG) method\navailable in hypre through BLOPEX software. We observe that turning off the\npost-smoothing in SMG dramatically slows down the standard PCG-SMG. For\nflexible PCG and LOBPCG, our numerical results show that post-smoothing can be\navoided, resulting in overall acceleration, due to the high costs of smoothing\nand relatively insignificant decrease in convergence speed. We numerically\ndemonstrate for linear systems that PSD-SMG and flexible PCG-SMG converge\nsimilarly if SMG post-smoothing is off. We experimentally show that the effect\nof acceleration is independent of memory interconnection. A theoretical\njustification is provided."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1302.1314v2", 
    "title": "Error Estimates with Explicit Constants for Sinc Quadrature and Sinc   Indefinite Integration over Infinite Intervals", 
    "arxiv-id": "1302.1314v2", 
    "author": "Tomoaki Okayama", 
    "publish": "2013-02-06T10:31:17Z", 
    "summary": "The Sinc quadrature and the Sinc indefinite integration are approximation\nformulas for definite integration and indefinite integration, respectively,\nwhich can be applied on any interval by using an appropriate variable\ntransformation. Their convergence rates have been analyzed for typical cases\nincluding finite, semi-infinite, and infinite intervals. In addition, for\nverified automatic integration, more explicit error bounds that are computable\nhave been recently given on a finite interval. In this paper, such explicit\nerror bounds are given in the remaining cases on semi-infinite and infinite\nintervals."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1302.2689v2", 
    "title": "Partitioned and implicit-explicit general linear methods for ordinary   differential equations", 
    "arxiv-id": "1302.2689v2", 
    "author": "Adrian Sandu", 
    "publish": "2013-02-12T03:11:06Z", 
    "summary": "Implicit-explicit (IMEX) time stepping methods can efficiently solve\ndifferential equa- tions with both stiff and nonstiff components. IMEX\nRunge-Kutta methods and IMEX linear multistep methods have been studied in the\nliterature. In this pa- per we study new implicit-explicit methods of general\nlinear type (IMEX-GLMs). We develop an order conditions theory for high stage\norder partitioned GLMs that share the same abscissae, and show that no\nadditional coupling order conditions are needed. Consequently, GLMs offer an\nexcellent framework for the construction of multi-method integration\nalgorithms. Next, we propose a family of IMEX schemes based on\ndiagonally-implicit multi-stage integration methods and construct practical\nschemes of order three. Numerical results confirm the theoretical findings."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1302.4317v1", 
    "title": "Introducing One Step Back Iterative Approach to Solve Linear and Non   Linear Fixed Point Problem", 
    "arxiv-id": "1302.4317v1", 
    "author": "Dohy Hong", 
    "publish": "2013-02-18T15:38:20Z", 
    "summary": "In this paper, we introduce a new iterative method which we call one step\nback approach: the main idea is to anticipate the consequence of the iterative\ncomputation per coordinate and to optimize on the choice of the sequence of the\ncoordinates on which the iterative update computations are done. The method\nrequires the increase of the size of the state vectors and one iteration step\nloss from the initial vector. We illustrate the approach in linear and non\nlinear iterative equations."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1302.6189v1", 
    "title": "A decomposition method with minimum communication amount for   parallelization of multi-dimensional FFTs", 
    "arxiv-id": "1302.6189v1", 
    "author": "Taisuke Ozaki", 
    "publish": "2013-02-06T04:43:26Z", 
    "summary": "The fast Fourier transform (FFT) is undoubtedly an essential primitive that\nhas been applied in various fields of science and engineering. In this paper,\nwe present a decomposition method for parallelization of multi-dimensional FFTs\nwith smallest communication amount for all ranges of the number of processes\ncompared to previously proposed methods. This is achieved by two distinguishing\nfeatures: adaptive decomposition and transpose order awareness. In the proposed\nmethod, the FFT data are decomposed based on a row-wise basis that maps the\nmulti-dimensional data into one-dimensional data, and translates the\ncorresponding coordinates from multi-dimensions into one-dimension so that the\nresultant one-dimensional data can be divided and allocated equally to the\nprocesses. As a result, differently from previous works that have the\ndimensions of decomposition pre-defined, our method can adaptively decompose\nthe FFT data on the lowest possible dimensions depending on the number of\nprocesses. In addition, this row-wise decomposition provides plenty of\nalternatives in data transpose, and different transpose order results in\ndifferent amount of communication. We identify the best transpose orders with\nsmallest communication amounts for the 3-D, 4-D, and 5-D FFTs by analyzing all\npossible cases. Given both communication efficiency and scalability, our method\nis promising in development of highly efficient parallel packages for the FFT."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1302.7283v1", 
    "title": "Source Separation using Regularized NMF with MMSE Estimates under GMM   Priors with Online Learning for The Uncertainties", 
    "arxiv-id": "1302.7283v1", 
    "author": "Hakan Erdogan", 
    "publish": "2013-02-28T18:56:56Z", 
    "summary": "We propose a new method to enforce priors on the solution of the nonnegative\nmatrix factorization (NMF). The proposed algorithm can be used for denoising or\nsingle-channel source separation (SCSS) applications. The NMF solution is\nguided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian\nmixture prior models (GMM) for the source signal. In SCSS applications, the\nspectra of the observed mixed signal are decomposed as a weighted linear\ncombination of trained basis vectors for each source using NMF. In this work,\nthe NMF decomposition weight matrices are treated as a distorted image by a\ndistortion operator, which is learned directly from the observed signals. The\nMMSE estimate of the weights matrix under GMM prior and log-normal distribution\nfor the distortion is then found to improve the NMF decomposition results. The\nMMSE estimate is embedded within the optimization objective to form a novel\nregularized NMF cost function. The corresponding update rules for the new\nobjectives are derived in this paper. Experimental results show that, the\nproposed regularized NMF algorithm improves the source separation performance\ncompared with using NMF without prior or with other prior models."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1303.1264v1", 
    "title": "Discovery of factors in matrices with grades", 
    "arxiv-id": "1303.1264v1", 
    "author": "Vilem Vychodil", 
    "publish": "2013-03-06T07:58:14Z", 
    "summary": "We present an approach to decomposition and factor analysis of matrices with\nordinal data. The matrix entries are grades to which objects represented by\nrows satisfy attributes represented by columns, e.g. grades to which an image\nis red, a product has a given feature, or a person performs well in a test. We\nassume that the grades form a bounded scale equipped with certain aggregation\noperators and conforms to the structure of a complete residuated lattice. We\npresent a greedy approximation algorithm for the problem of decomposition of\nsuch matrix in a product of two matrices with grades under the restriction that\nthe number of factors be small. Our algorithm is based on a geometric insight\nprovided by a theorem identifying particular rectangular-shaped submatrices as\noptimal factors for the decompositions. These factors correspond to formal\nconcepts of the input data and allow an easy interpretation of the\ndecomposition. We present illustrative examples and experimental evaluation."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1303.2238v1", 
    "title": "Some numerical aspects of the conservative PSM scheme in a 4D   drift-kinetic code", 
    "arxiv-id": "1303.2238v1", 
    "author": "Eric Sonnendr\u00fccker", 
    "publish": "2013-03-09T16:55:30Z", 
    "summary": "The purpose of this work is simulation of magnetised plasmas in the ITER\nproject framework. In this context, kinetic Vlasov-Poisson like models are used\nto simulate core turbulence in the tokamak in a toroidal geometry. This leads\nto heavy simulations because a 6D dimensional problem has to be solved, even if\nreduced to a 5D in so called gyrokinetic models. Accurate schemes, parallel\nalgorithms need to be designed to bear these simulations. This paper describes\nthe numerical studies to improve robustness of the conservative PSM scheme in\nthe context of its development in the GYSELA code. In this paper, we only\nconsider the 4D drift-kinetic model which is the backbone of the 5D gyrokinetic\nmodels and relevant to build a robust and accurate numerical method."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1303.4207v7", 
    "title": "Improving CUR Matrix Decomposition and the Nystr\u00f6m Approximation via   Adaptive Sampling", 
    "arxiv-id": "1303.4207v7", 
    "author": "Zhihua Zhang", 
    "publish": "2013-03-18T11:17:55Z", 
    "summary": "The CUR matrix decomposition and the Nystr\\\"{o}m approximation are two\nimportant low-rank matrix approximation techniques. The Nystr\\\"{o}m method\napproximates a symmetric positive semidefinite matrix in terms of a small\nnumber of its columns, while CUR approximates an arbitrary data matrix by a\nsmall number of its columns and rows. Thus, CUR decomposition can be regarded\nas an extension of the Nystr\\\"{o}m approximation.\n  In this paper we establish a more general error bound for the adaptive\ncolumn/row sampling algorithm, based on which we propose more accurate CUR and\nNystr\\\"{o}m algorithms with expected relative-error bounds. The proposed CUR\nand Nystr\\\"{o}m algorithms also have low time complexity and can avoid\nmaintaining the whole data matrix in RAM. In addition, we give theoretical\nanalysis for the lower error bounds of the standard Nystr\\\"{o}m method and the\nensemble Nystr\\\"{o}m method. The main theoretical results established in this\npaper are novel, and our analysis makes no special assumption on the data\nmatrices."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1303.4942v1", 
    "title": "A Novel Algorithm for Linear Programming", 
    "arxiv-id": "1303.4942v1", 
    "author": "K. Eswaran", 
    "publish": "2013-03-20T13:57:11Z", 
    "summary": "The problem of optimizing a linear objective function,given a number of\nlinear constraints has been a long standing problem ever since the times of\nKantorovich, Dantzig and von Neuman. These developments have been followed by a\ndifferent approach pioneered by Khachiyan and Karmarkar.\n  In this paper we present an entirely new method for solving an old\noptimization problem in a novel manner, a technique that reduces the dimension\nof the problem step by step and interestingly is recursive. A theorem which\nproves the correctness of the approach is given.\n  The method can be extended to other types of optimization problems in convex\nspace, e.g. for solving a linear optimization problem subject to nonlinear\nconstraints in a convex region."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1308.1827v2", 
    "title": "Factorization approach to structured low-rank approximation with   applications", 
    "arxiv-id": "1308.1827v2", 
    "author": "Ivan Markovsky", 
    "publish": "2013-08-08T12:34:09Z", 
    "summary": "We consider the problem of approximating an affinely structured matrix, for\nexample a Hankel matrix, by a low-rank matrix with the same structure. This\nproblem occurs in system identification, signal processing and computer\nalgebra, among others. We impose the low-rank by modeling the approximation as\na product of two factors with reduced dimension. The structure of the low-rank\nmodel is enforced by introducing a penalty term in the objective function. The\nproposed local optimization algorithm is able to solve the weighted structured\nlow-rank approximation problem, as well as to deal with the cases of missing or\nfixed elements. In contrast to approaches based on kernel representations (in\nlinear algebraic sense), the proposed algorithm is designed to address the case\nof small targeted rank. We compare it to existing approaches on numerical\nexamples of system identification, approximate greatest common divisor problem,\nand symmetric tensor decomposition and demonstrate its consistently good\nperformance."
},{
    "category": "cs.SY", 
    "doi": "10.1016/j.procs.2015.05.241", 
    "link": "http://arxiv.org/pdf/1308.2426v1", 
    "title": "Bias of the SIR filter in estimation of the state transition noise", 
    "arxiv-id": "1308.2426v1", 
    "author": "Tiancheng Li", 
    "publish": "2013-08-11T20:15:49Z", 
    "summary": "This Note investigates the bias of the sampling importance resampling (SIR)\nfilter in estimation of the state transition noise in the state space model.\nThe SIR filter may suffer from sample impoverishment that is caused by the\nresampling and therefore will benefit from a sampling proposal that has a\nheavier tail, e.g. the state transition noise simulated for particle\npreparation is bigger than the true noise involved with the state dynamics.\nThis is because a comparably big transition noise used for particle propagation\ncan spread overlapped particles to counteract impoverishment, giving better\napproximation of the posterior. As such, the SIR filter tends to yield a biased\n(bigger-than-the-truth) estimate of the transition noise if it is unknown and\nneeds to be estimated, at least, in the forward-only filtering estimation. The\nbias is elaborated via the direct roughening approach by means of both\nqualitative logical deduction and quantitative numerical simulation."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.2475v2", 
    "title": "Improved bounds on sample size for implicit matrix trace estimators", 
    "arxiv-id": "1308.2475v2", 
    "author": "Uri Ascher", 
    "publish": "2013-08-12T06:57:32Z", 
    "summary": "This article is concerned with Monte-Carlo methods for the estimation of the\ntrace of an implicitly given matrix $A$ whose information is only available\nthrough matrix-vector products. Such a method approximates the trace by an\naverage of $N$ expressions of the form $\\ww^t (A\\ww)$, with random vectors\n$\\ww$ drawn from an appropriate distribution. We prove, discuss and experiment\nwith bounds on the number of realizations $N$ required in order to guarantee a\nprobabilistic bound on the relative error of the trace estimation upon\nemploying Rademacher (Hutchinson), Gaussian and uniform unit vector (with and\nwithout replacement) probability distributions.\n  In total, one necessary bound and six sufficient bounds are proved, improving\nupon and extending similar estimates obtained in the seminal work of Avron and\nToledo (2011) in several dimensions. We first improve their bound on $N$ for\nthe Hutchinson method, dropping a term that relates to $rank(A)$ and making the\nbound comparable with that for the Gaussian estimator.\n  We further prove new sufficient bounds for the Hutchinson, Gaussian and the\nunit vector estimators, as well as a necessary bound for the Gaussian\nestimator, which depend more specifically on properties of the matrix $A$. As\nsuch they may suggest for what type of matrices one distribution or another\nprovides a particularly effective or relatively ineffective stochastic\nestimation method."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.3339v4", 
    "title": "Fast Multipole Preconditioners for Sparse Matrices Arising from Elliptic   Equations", 
    "arxiv-id": "1308.3339v4", 
    "author": "David Keyes", 
    "publish": "2013-08-15T08:53:25Z", 
    "summary": "Among optimal hierarchical algorithms for the computational solution of\nelliptic problems, the Fast Multipole Method (FMM) stands out for its\nadaptability to emerging architectures, having high arithmetic intensity,\ntunable accuracy, and relaxable global synchronization requirements. We\ndemonstrate that, beyond its traditional use as a solver in problems for which\nexplicit free-space kernel representations are available, the FMM has\napplicability as a preconditioner in finite domain elliptic boundary value\nproblems, by equipping it with boundary integral capability for satisfying\nconditions at finite boundaries and by wrapping it in a Krylov method for\nextensibility to more general operators. Here, we do not discuss the well\ndeveloped applications of FMM to implement matrix-vector multiplications within\nKrylov solvers of boundary element methods. Instead, we propose using FMM for\nthe volume-to-volume contribution of inhomogeneous Poisson-like problems, where\nthe boundary integral is a small part of the overall computation. Our method\nmay be used to precondition sparse matrices arising from finite\ndifference/element discretizations, and can handle a broader range of\nscientific applications. Compared with multigrid methods, it is capable of\ncomparable algebraic convergence rates down to the truncation error of the\ndiscretized PDE, and it offers potentially superior multicore and distributed\nmemory scalability properties on commodity architecture supercomputers.\nCompared with other methods exploiting the low rank character of off-diagonal\nblocks of the dense resolvent operator, FMM-preconditioned Krylov iteration may\nreduce the amount of communication because it is matrix-free and exploits the\ntree structure of FMM. We describe our tests in reproducible detail with freely\navailable codes and outline directions for further extensibility."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.3558v1", 
    "title": "Fast Stochastic Alternating Direction Method of Multipliers", 
    "arxiv-id": "1308.3558v1", 
    "author": "James T. Kwok", 
    "publish": "2013-08-16T05:48:29Z", 
    "summary": "In this paper, we propose a new stochastic alternating direction method of\nmultipliers (ADMM) algorithm, which incrementally approximates the full\ngradient in the linearized ADMM formulation. Besides having a low per-iteration\ncomplexity as existing stochastic ADMM algorithms, the proposed algorithm\nimproves the convergence rate on convex problems from $O(\\frac 1 {\\sqrt{T}})$\nto $O(\\frac 1 T)$, where $T$ is the number of iterations. This matches the\nconvergence rate of the batch ADMM algorithm, but without the need to visit all\nthe samples in each iteration. Experiments on the graph-guided fused lasso\ndemonstrate that the new algorithm is significantly faster than\nstate-of-the-art stochastic and batch ADMM algorithms."
},{
    "category": "math.OC", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.5294v1", 
    "title": "Solving Multiple-Block Separable Convex Minimization Problems Using   Two-Block Alternating Direction Method of Multipliers", 
    "arxiv-id": "1308.5294v1", 
    "author": "Zhi-Quan Luo", 
    "publish": "2013-08-24T04:20:28Z", 
    "summary": "In this paper, we consider solving multiple-block separable convex\nminimization problems using alternating direction method of multipliers (ADMM).\nMotivated by the fact that the existing convergence theory for ADMM is mostly\nlimited to the two-block case, we analyze in this paper, both theoretically and\nnumerically, a new strategy that first transforms a multi-block problem into an\nequivalent two-block problem (either in the primal domain or in the dual\ndomain) and then solves it using the standard two-block ADMM. In particular, we\nderive convergence results for this two-block ADMM approach to solve\nmulti-block separable convex minimization problems, including an improved\nO(1/\\epsilon) iteration complexity result. Moreover, we compare the numerical\nefficiency of this approach with the standard multi-block ADMM on several\nseparable convex minimization problems which include basis pursuit, robust\nprincipal component analysis and latent variable Gaussian graphical model\nselection. The numerical results show that the multiple-block ADMM, although\nlacks theoretical convergence guarantees, typically outperforms two-block\nADMMs."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.5697v1", 
    "title": "Randomized algorithms for low-rank matrix factorizations: sharp   performance bounds", 
    "arxiv-id": "1308.5697v1", 
    "author": "Emmanuel Candes", 
    "publish": "2013-08-26T21:00:35Z", 
    "summary": "The development of randomized algorithms for numerical linear algebra, e.g.\nfor computing approximate QR and SVD factorizations, has recently become an\nintense area of research. This paper studies one of the most frequently\ndiscussed algorithms in the literature for dimensionality\nreduction---specifically for approximating an input matrix with a low-rank\nelement. We introduce a novel and rather intuitive analysis of the algorithm in\nMartinsson et al. (2008), which allows us to derive sharp estimates and give\nnew insights about its performance. This analysis yields theoretical guarantees\nabout the approximation error and at the same time, ultimate limits of\nperformance (lower bounds) showing that our upper bounds are tight. Numerical\nexperiments complement our study and show the tightness of our predictions\ncompared with empirical observations."
},{
    "category": "math.NA", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1308.6320v1", 
    "title": "Three-Dimensional Mapped-Grid Finite Volume Modeling of   Poroelastic-Fluid Wave Propagation", 
    "arxiv-id": "1308.6320v1", 
    "author": "Grady I. Lemoine", 
    "publish": "2013-08-28T21:43:16Z", 
    "summary": "This paper extends the author's previous two-dimensional work with Ou and\nLeVeque to high-resolution finite volume modeling of systems of fluids and\nporoelastic media in three dimensions, using logically rectangular mapped\ngrids. A method is described for calculating consistent cell face areas and\nnormal vectors for a finite volume method on a general non-rectilinear\nhexahedral grid. A novel limiting algorithm is also developed to cope with\ndifficulties encountered in implementing high-resolution finite volume methods\nfor anisotropic media on non-rectilinear grids; the new limiting approach is\ncompatible with any limiter function, and typically reduces solution error even\nin situations where it is not necessary for correct functioning of the\nnumerical method. Dimensional splitting is used to reduce the computational\ncost of the solution. The code implementing the three-dimensional algorithms is\nverified against known plane wave solutions, with particular attention to the\nperformance of the new limiter algorithm in comparison to the classical one. An\nacoustic wave in brine striking an uneven bed of orthotropic layered sandstone\nis also simulated in order to demonstrate the capabilities of the simulation\ncode."
},{
    "category": "math.NA", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1309.0212v1", 
    "title": "An Error-Resilient Redundant Subspace Correction Method", 
    "arxiv-id": "1309.0212v1", 
    "author": "Chen-Song Zhang", 
    "publish": "2013-09-01T12:24:52Z", 
    "summary": "As we stride toward the exascale era, due to increasing complexity of\nsupercomputers, hard and soft errors are causing more and more problems in\nhigh-performance scientific and engineering computation. In order to improve\nreliability (increase the mean time to failure) of computing systems, a lot of\nefforts have been devoted to developing techniques to forecast, prevent, and\nrecover from errors at different levels, including architecture, application,\nand algorithm. In this paper, we focus on algorithmic error resilient iterative\nlinear solvers and introduce a redundant subspace correction method. Using a\ngeneral framework of redundant subspace corrections, we construct iterative\nmethods, which have the following properties: (1) Maintain convergence when\nerror occurs assuming it is detectable; (2) Introduce low computational\noverhead when no error occurs; (3) Require only small amount of local\n(point-to-point) communication compared to traditional methods and maintain\ngood load balance; (4) Improve the mean time to failure. With the proposed\nmethod, we can improve reliability of many scientific and engineering\napplications. Preliminary numerical experiments demonstrate the efficiency and\neffectiveness of the new subspace correction method."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10208-014-9220-1", 
    "link": "http://arxiv.org/pdf/1309.0392v1", 
    "title": "Hierarchization for the Sparse Grid Combination Technique", 
    "arxiv-id": "1309.0392v1", 
    "author": "Philipp Hupp", 
    "publish": "2013-08-12T16:52:04Z", 
    "summary": "The sparse grid combination technique provides a framework to solve high\ndimensional numerical problems with standard solvers. Hierarchization is\npreprocessing step facilitating the communication needed for the combination\ntechnique. The derived hierarchization algorithm outperforms the baseline by up\nto 30x and achieves close to 5% of peak performance. It also shows stable\nperformance for the tested data sets of up to 1 GB."
},{
    "category": "cs.NA", 
    "doi": "10.4236/am.2011.29147", 
    "link": "http://arxiv.org/pdf/1309.6064v1", 
    "title": "Numerical solutions of a class of second order boundary value problems   on using Bernoulli Polynomials", 
    "arxiv-id": "1309.6064v1", 
    "author": "Afroza Shirin", 
    "publish": "2013-09-24T07:11:16Z", 
    "summary": "The aim of this paper is to find the numerical solutions of the second order\nlinear and nonlinear differential equations with Dirichlet, Neumann and Robin\nboundary conditions. We use the Bernoulli polynomials as linear combination to\nthe approximate solutions of 2nd order boundary value problems. Here the\nBernoulli polynomials over the interval [0, 1] are chosen as trial functions so\nthat care has been taken to satisfy the corresponding homogeneous form of the\nDirichlet boundary conditions in the Galerkin weighted residual method. In\naddition to that the given differential equation over arbitrary finite domain\n[a, b] and the boundary conditions are converted into its equivalent form over\nthe interval [0, 1]. All the formulas are verified by considering numerical\nexamples. The approximate solutions are compared with the exact solutions, and\nalso with the solutions of the existing methods. A reliable good accuracy is\nobtained in all cases."
},{
    "category": "cs.NA", 
    "doi": "10.4236/am.2011.29147", 
    "link": "http://arxiv.org/pdf/1309.6290v3", 
    "title": "Coefficient Matrices Computation of Structural Vector Autoregressive   Model", 
    "arxiv-id": "1309.6290v3", 
    "author": "Aravindh Krishnamoorthy", 
    "publish": "2013-09-23T15:18:14Z", 
    "summary": "In this paper we present the Large Inverse Cholesky (LIC) method, an\nefficient method for computing the coefficient matrices of a Structural Vector\nAutoregressive (SVAR) model."
},{
    "category": "cs.NA", 
    "doi": "10.3329/jsr.v2i2.4483", 
    "link": "http://arxiv.org/pdf/1309.6311v1", 
    "title": "Numerical Solutions of Fredholm Integral Equations Using Bernstein   Polynomials", 
    "arxiv-id": "1309.6311v1", 
    "author": "Md. Shafiqul Islam", 
    "publish": "2013-09-24T07:48:17Z", 
    "summary": "In this paper, Bernstein piecewise polynomials are used to solve the integral\nequations numerically. A matrix formulation is given for a non-singular linear\nFredholm Integral Equation by the technique of Galerkin method. In the Galerkin\nmethod, the Bernstein polynomials are used as the approximation of basis\nfunctions. Examples are considered to verify the effectiveness of the proposed\nderivations, and the numerical solutions guarantee the desired accuracy."
},{
    "category": "cs.NA", 
    "doi": "10.3329/jsr.v2i2.4483", 
    "link": "http://arxiv.org/pdf/1309.7001v2", 
    "title": "Kaczmarz Algorithm with Soft Constraints for User Interface Layout", 
    "arxiv-id": "1309.7001v2", 
    "author": "Gerald Weber", 
    "publish": "2013-09-26T18:22:41Z", 
    "summary": "The Kaczmarz method is an iterative method for solving large systems of\nequations that projects iterates orthogonally onto the solution space of each\nequation. In contrast to direct methods such as Gaussian elimination or\nQR-factorization, this algorithm is efficient for problems with sparse\nmatrices, as they appear in constraint-based user interface (UI) layout\nspecifications. However, the Kaczmarz method as described in the literature has\nits limitations: it considers only equality constraints and does not support\nsoft constraints, which makes it inapplicable to the UI layout problem.\n  In this paper we extend the Kaczmarz method for solving specifications\ncontaining soft constraints, using the prioritized IIS detection algorithm.\nFurthermore, the performance and convergence of the proposed algorithms are\nevaluated empirically using randomly generated UI layout specifications of\nvarious sizes. The results show that these methods offer improvements in\nperformance over standard methods like Matlab's LINPROG, a well-known efficient\nlinear programming solver."
},{
    "category": "math.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1309.7828v3", 
    "title": "A search for extensible low-WAFOM point sets", 
    "arxiv-id": "1309.7828v3", 
    "author": "Shin Harase", 
    "publish": "2013-09-30T13:01:34Z", 
    "summary": "Matsumoto, Saito, and Matoba recently proposed the Walsh figure of merit\n(WAFOM), which is a computable criterion for quasi-Monte Carlo point sets using\ndigital nets. Several algorithms have been proposed for finding low-WAFOM point\nsets. In the existing algorithms, the number of points is fixed in advance, but\nextensible point sets are preferred in some applications. In this paper, we\npropose a random search algorithm for extensible low-WAFOM point sets. For\nthis, we introduce a method that uses lookup tables to compute WAFOM faster.\nNumerical results show that our extensible low-WAFOM point sets are comparable\nwith Niederreiter--Xing sequences for some low-dimensional and smooth test\nfunctions."
},{
    "category": "cs.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.2780v1", 
    "title": "A priori estimation of a time step for numerically solving parabolic   problems", 
    "arxiv-id": "1311.2780v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2013-11-12T13:47:55Z", 
    "summary": "This work deals with the problem of choosing a time step for the numerical\nsolution of boundary value problems for parabolic equations. The problem\nsolution is derived using the fully implicit scheme, whereas a time step is\nselected via explicit calculations. The selection strategy consists of the\nfollowing steps. First, using the explicit scheme, we calculate the solution at\na new time level. Next, we employ this solution in order to obtain the solution\nat the previous time level (the implicit scheme, explicit calculations). This\nsolution should be close to the solution of our problem at this time level with\na prescribed accuracy. Such an algorithm leads to explicit formulas for the\ncalculation of the time step and takes into account both the dynamics of the\nproblem solution and changes in coefficients of the equation and in its\nright-hand side. The same formulas for the evaluation of the time step we get\nusing a comparison of two approximate solutions, which are obtained using the\nexplicit scheme with the primary time step and the step that is reduced by\nhalf. Numerical results are presented for a model parabolic boundary value\nproblem, which demonstrate the robustness of the developed algorithm for the\ntime step selection."
},{
    "category": "cs.LG", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.2854v3", 
    "title": "Spectral Clustering via the Power Method -- Provably", 
    "arxiv-id": "1311.2854v3", 
    "author": "Prabhanjan Kambadur", 
    "publish": "2013-11-12T17:42:34Z", 
    "summary": "Spectral clustering is one of the most important algorithms in data mining\nand machine intelligence; however, its computational complexity limits its\napplication to truly large scale data analysis. The computational bottleneck in\nspectral clustering is computing a few of the top eigenvectors of the\n(normalized) Laplacian matrix corresponding to the graph representing the data\nto be clustered. One way to speed up the computation of these eigenvectors is\nto use the \"power method\" from the numerical linear algebra literature.\nAlthough the power method has been empirically used to speed up spectral\nclustering, the theory behind this approach, to the best of our knowledge,\nremains unexplored. This paper provides the \\emph{first} such rigorous\ntheoretical justification, arguing that a small number of power iterations\nsuffices to obtain near-optimal partitionings using the approximate\neigenvectors. Specifically, we prove that solving the $k$-means clustering\nproblem on the approximate eigenvectors obtained via the power method gives an\nadditive-error approximation to solving the $k$-means problem on the optimal\neigenvectors."
},{
    "category": "cs.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.3286v1", 
    "title": "An Efficient Parallel Solver for SDD Linear Systems", 
    "arxiv-id": "1311.3286v1", 
    "author": "Daniel A. Spielman", 
    "publish": "2013-11-13T20:41:01Z", 
    "summary": "We present the first parallel algorithm for solving systems of linear\nequations in symmetric, diagonally dominant (SDD) matrices that runs in\npolylogarithmic time and nearly-linear work. The heart of our algorithm is a\nconstruction of a sparse approximate inverse chain for the input matrix: a\nsequence of sparse matrices whose product approximates its inverse. Whereas\nother fast algorithms for solving systems of equations in SDD matrices exploit\nlow-stretch spanning trees, our algorithm only requires spectral graph\nsparsifiers."
},{
    "category": "math.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.3358v1", 
    "title": "Generating Equidistributed Meshes in 2D via Domain Decomposition", 
    "arxiv-id": "1311.3358v1", 
    "author": "Alexander J. M. Howse", 
    "publish": "2013-11-14T01:29:15Z", 
    "summary": "In this paper we consider Schwarz domain decomposition applied to the\ngeneration of 2D spatial meshes by a local equidistribution principle. We\nbriefly review the derivation of the local equidistribution principle and the\nappropriate choice of boundary conditions. We then introduce classical and\noptimized Schwarz domain decomposition methods to solve the resulting system of\nnonlinear equations. The implementation of these iterations are discussed, and\nwe conclude with numerical examples to illustrate the performance of the\napproach."
},{
    "category": "cs.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.3766v1", 
    "title": "Splitting schemes for poroelasticity and thermoelasticity problems", 
    "arxiv-id": "1311.3766v1", 
    "author": "M. V. Vasilyeva", 
    "publish": "2013-11-15T08:33:42Z", 
    "summary": "In this work, we consider the coupled systems of linear unsteady partial\ndifferential equations, which arise in the modeling of poroelasticity\nprocesses. Stability estimates of weighted difference schemes for the coupled\nsystem of equations are presented. Approximation in space is based on the\nfinite element method. We construct splitting schemes and give some numerical\ncomparisons for typical poroelasticity problems. The results of numerical\nsimulation of a 3D problem are presented. Special attention is given to using\nhight performance computing systems."
},{
    "category": "math.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1311.4257v1", 
    "title": "A parallel directional Fast Multipole Method", 
    "arxiv-id": "1311.4257v1", 
    "author": "Lexing Ying", 
    "publish": "2013-11-18T03:47:36Z", 
    "summary": "This paper introduces a parallel directional fast multipole method (FMM) for\nsolving N-body problems with highly oscillatory kernels, with a focus on the\nHelmholtz kernel in three dimensions. This class of oscillatory kernels\nrequires a more restrictive low-rank criterion than that of the low-frequency\nregime, and thus effective parallelizations must adapt to the modified data\ndependencies. We propose a simple partition at a fixed level of the octree and\nshow that, if the partitions are properly balanced between p processes, the\noverall runtime is essentially O(N log N/p+ p). By the structure of the\nlow-rank criterion, we are able to avoid communication at the top of the\noctree. We demonstrate the effectiveness of our parallelization on several\nchallenging models."
},{
    "category": "math.NA", 
    "doi": "10.1515/mcma-2016-0119", 
    "link": "http://arxiv.org/pdf/1402.0642v1", 
    "title": "kappa_SQ: A Matlab package for randomized sampling of matrices with   orthonormal columns", 
    "arxiv-id": "1402.0642v1", 
    "author": "Ilse Ipsen", 
    "publish": "2014-02-04T07:17:01Z", 
    "summary": "The kappa_SQ software package is designed to assist researchers working on\nrandomized row sampling. The package contains a collection of Matlab functions\nalong with a GUI that ties them all together and provides a platform for the\nuser to perform experiments. In particular, kappa_SQ is designed to do\nexperiments related to the two-norm condition number of a sampled matrix,\n$\\kappa(SQ)$, where $S$ is a row sampling matrix and $Q$ is a tall and skinny\nmatrix with orthonormal columns. Via a simple GUI, kappa_SQ can generate test\nmatrices, perform various types of row sampling, measure $\\kappa(SQ)$,\ncalculate bounds and produce high quality plots of the results. All of the\nimportant codes are written in separate Matlab function files in a standard\nformat which makes it easy for a user to either use the codes by themselves or\nincorporate their own codes into the kappa_SQ package."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.11.022", 
    "link": "http://arxiv.org/pdf/1402.1636v1", 
    "title": "Numerical solving the boundary value problem for fractional powers of   elliptic operators", 
    "arxiv-id": "1402.1636v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2014-02-07T13:42:18Z", 
    "summary": "A boundary value problem for a fractional power of the second-order elliptic\noperator is considered. It is solved numerically using a time-dependent problem\nfor a pseudo-parabolic equation. For the auxiliary Cauchy problem, the standard\ntwo-level schemes with weights are applied. Stability conditions are obtained\nfor the fully discrete schemes under the consideration. The numerical results\nare presented for a model two-dimensional boundary value problem wit a\nfractional power of an elliptic operator. The dependence of accuracy on grids\nin time and in space is studied."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.11.022", 
    "link": "http://arxiv.org/pdf/1402.1673v3", 
    "title": "Non-Orthogonal Tensor Diagonalization", 
    "arxiv-id": "1402.1673v3", 
    "author": "Andrzej Cichocki", 
    "publish": "2014-02-07T15:50:21Z", 
    "summary": "Tensor diagonalization means transforming a given tensor to an exactly or\nnearly diagonal form through multiplying the tensor by non-orthogonal\ninvertible matrices along selected dimensions of the tensor. It is\ngeneralization of approximate joint diagonalization (AJD) of a set of matrices.\nIn particular, we derive (1) a new algorithm for symmetric AJD, which is called\ntwo-sided symmetric diagonalization of order-three tensor, (2) a similar\nalgorithm for non-symmetric AJD, also called general two-sided diagonalization\nof an order-3 tensor, and (3) an algorithm for three-sided diagonalization of\norder-3 or order-4 tensors. The latter two algorithms may serve for canonical\npolyadic (CP) tensor decomposition, and they can outperform other CP tensor\ndecomposition methods in terms of computational speed under the restriction\nthat the tensor rank does not exceed the tensor multilinear rank. Finally, we\npropose (4) similar algorithms for tensor block diagonalization, which is\nrelated to the tensor block-term decomposition."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.3946", 
    "link": "http://arxiv.org/pdf/1402.2018v1", 
    "title": "Comparison of POD reduced order strategies for the nonlinear 2D Shallow   Water Equations", 
    "arxiv-id": "1402.2018v1", 
    "author": "Ionel M. Navon", 
    "publish": "2014-02-10T02:20:09Z", 
    "summary": "This paper introduces tensorial calculus techniques in the framework of\nProper Orthogonal Decomposition (POD) to reduce the computational complexity of\nthe reduced nonlinear terms. The resulting method, named tensorial POD, can be\napplied to polynomial nonlinearities of any degree $p$. Such nonlinear terms\nhave an on-line complexity of $\\mathcal{O}(k^{p+1})$, where $k$ is the\ndimension of POD basis, and therefore is independent of full space dimension.\nHowever it is efficient only for quadratic nonlinear terms since for higher\nnonlinearities standard POD proves to be less time consuming once the POD basis\ndimension $k$ is increased. Numerical experiments are carried out with a two\ndimensional shallow water equation (SWE) test problem to compare the\nperformance of tensorial POD, standard POD, and POD/Discrete Empirical\nInterpolation Method (DEIM). Numerical results show that tensorial POD\ndecreases by $76\\times$ times the computational cost of the on-line stage of\nstandard POD for configurations using more than $300,000$ model variables. The\ntensorial POD SWE model was only $2-8\\times$ slower than the POD/DEIM SWE model\nbut the implementation effort is considerably increased. Tensorial calculus was\nagain employed to construct a new algorithm allowing POD/DEIM shallow water\nequation model to compute its off-line stage faster than the standard and\ntensorial POD approaches."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.3946", 
    "link": "http://arxiv.org/pdf/1402.2991v1", 
    "title": "On the maximum relative error when computing x^n in floating-point   arithmetic", 
    "arxiv-id": "1402.2991v1", 
    "author": "Jean-Michel Muller", 
    "publish": "2014-02-11T20:13:14Z", 
    "summary": "In this paper, we improve the usual relative error bound for the computation\nof x^n through iterated multiplications by x in binary floating-point\narithmetic. The obtained error bound is only slightly better than the usual\none, but it is simpler. We also discuss the more general problem of computing\nthe product of n terms."
},{
    "category": "math.NA", 
    "doi": "10.1002/fld.3946", 
    "link": "http://arxiv.org/pdf/1402.5287v2", 
    "title": "On fast matrix-vector multiplication with a Hankel matrix in   multiprecision arithmetics", 
    "arxiv-id": "1402.5287v2", 
    "author": "Gleb Beliakov", 
    "publish": "2014-02-21T13:03:18Z", 
    "summary": "We present two fast algorithms for matrix-vector multiplication $y=Ax$, where\n$A$ is a Hankel matrix. The current asymptotically fastest method is based on\nthe Fast Fourier Transform (FFT), however in multiprecision arithmetics with\nvery high accuracy FFT method is actually slower than schoolbook multiplication\nfor matrix sizes up to $n=8000$. One method presented is based on a\ndecomposition of multiprecision numbers into sums, and applying standard or\ndouble precision FFT. The second method, inspired by Karatsuba multiplication,\nis based on recursively performing multiplications with matrices of half-size\nof the original. Its complexity in terms of the matrix size $n$ is\n$\\Theta(n^{\\log 3})$. Both methods are applicable to Toeplitz matrices and to\ncirculant matrices."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.jcp.2014.07.048", 
    "link": "http://arxiv.org/pdf/1402.6081v3", 
    "title": "A parallel fast multipole method for elliptic difference equations", 
    "arxiv-id": "1402.6081v3", 
    "author": "Tim Colonius", 
    "publish": "2014-02-25T08:18:58Z", 
    "summary": "A new fast multipole formulation for solving elliptic difference equations on\nunbounded domains and its parallel implementation are presented. These\ndifference equations can arise directly in the description of physical systems,\ne.g. crystal structures, or indirectly through the discretization of PDEs. In\nthe analog to solving continuous inhomogeneous differential equations using\nGreen's functions, the proposed method uses the fundamental solution of the\ndiscrete operator on an infinite grid, or lattice Green's function. Fast\nsolutions $\\mathcal{O}(N)$ are achieved by using a kernel-independent\ninterpolation-based fast multipole method. Unlike other fast multipole\nalgorithms, our approach exploits the regularity of the underlying Cartesian\ngrid and the efficiency of FFTs to reduce the computation time. Our parallel\nimplementation allows communications and computations to be overlapped and\nrequires minimal global synchronization. The accuracy, efficiency, and parallel\nperformance of the method are demonstrated through numerical experiments on the\ndiscrete 3D Poisson equation."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.4800", 
    "link": "http://arxiv.org/pdf/1404.0442v3", 
    "title": "Adaptive $h$-refinement for reduced-order models", 
    "arxiv-id": "1404.0442v3", 
    "author": "Kevin Carlberg", 
    "publish": "2014-04-02T03:29:43Z", 
    "summary": "This work presents a method to adaptively refine reduced-order models \\emph{a\nposteriori} without requiring additional full-order-model solves. The technique\nis analogous to mesh-adaptive $h$-refinement: it enriches the reduced-basis\nspace online by `splitting' a given basis vector into several vectors with\ndisjoint support. The splitting scheme is defined by a tree structure\nconstructed offline via recursive $k$-means clustering of the state variables\nusing snapshot data. The method identifies the vectors to split online using a\ndual-weighted-residual approach that aims to reduce error in an output quantity\nof interest. The resulting method generates a hierarchy of subspaces online\nwithout requiring large-scale operations or full-order-model solves. Further,\nit enables the reduced-order model to satisfy \\emph{any prescribed error\ntolerance} regardless of its original fidelity, as a completely refined\nreduced-order model is mathematically equivalent to the original full-order\nmodel. Experiments on a parameterized inviscid Burgers equation highlight the\nability of the method to capture phenomena (e.g., moving shocks) not contained\nin the span of the original reduced basis."
},{
    "category": "cs.LG", 
    "doi": "10.1002/nme.4800", 
    "link": "http://arxiv.org/pdf/1404.0466v2", 
    "title": "piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for   Efficient Approximate Cross-Validation", 
    "arxiv-id": "1404.0466v2", 
    "author": "Raffay Hamid", 
    "publish": "2014-04-02T05:33:41Z", 
    "summary": "The dominant cost in solving least-square problems using Newton's method is\noften that of factorizing the Hessian matrix over multiple values of the\nregularization parameter ($\\lambda$). We propose an efficient way to\ninterpolate the Cholesky factors of the Hessian matrix computed over a small\nset of $\\lambda$ values. This approximation enables us to optimally minimize\nthe hold-out error while incurring only a fraction of the cost compared to\nexact cross-validation. We provide a formal error bound for our approximation\nscheme and present solutions to a set of key implementation challenges that\nallow our approach to maximally exploit the compute power of modern\narchitectures. We present a thorough empirical analysis over multiple datasets\nto show the effectiveness of our approach."
},{
    "category": "math.NA", 
    "doi": "10.1002/nme.4800", 
    "link": "http://arxiv.org/pdf/1404.0812v1", 
    "title": "A Radial Basis Function (RBF)-Finite Difference (FD) Method for   Diffusion and Reaction-Diffusion Equations on Surfaces", 
    "arxiv-id": "1404.0812v1", 
    "author": "Aaron L. Fogelson", 
    "publish": "2014-04-03T09:09:03Z", 
    "summary": "In this paper, we present a method based on Radial Basis Function\n(RBF)-generated Finite Differences (FD) for numerically solving diffusion and\nreaction-diffusion equations (PDEs) on closed surfaces embedded in\n$\\mathbb{R}^d$. Our method uses a method-of-lines formulation, in which surface\nderivatives that appear in the PDEs are approximated locally using RBF\ninterpolation. The method requires only scattered nodes representing the\nsurface and normal vectors at those scattered nodes. All computations use only\nextrinsic coordinates, thereby avoiding coordinate distortions and\nsingularities. We also present an optimization procedure that allows for the\nstabilization of the discrete differential operators generated by our RBF-FD\nmethod by selecting shape parameters for each stencil that correspond to a\nglobal target condition number. We show the convergence of our method on two\nsurfaces for different stencil sizes, and present applications to nonlinear\nPDEs simulated both on implicit/parametric surfaces and more general surfaces\nrepresented by point clouds."
},{
    "category": "math.NA", 
    "doi": "10.1166/jcsmd.2014.1040", 
    "link": "http://arxiv.org/pdf/1404.1165v2", 
    "title": "A new multiresolution finite element method based on a multiresolution   quadrilateral plate element", 
    "arxiv-id": "1404.1165v2", 
    "author": "YiMing Xia", 
    "publish": "2014-04-04T06:59:15Z", 
    "summary": "A new multiresolution quadrilateral plate element is proposed and a\nmultiresolution finite element method is hence presented. The multiresolution\nanalysis (MRA) framework is formulated out of a mutually nesting displacement\nsubspace sequence, whose basis functions are constructed of scaling and\nshifting on the element domain of basic node shape function. The basic node\nshape function is constructed by extending shape function around a specific\nnode. The MRA endows the proposed element with the resolution level (RL) to\nadjust the element node number, thus modulating structural analysis accuracy\naccordingly. As a result, the traditional 4-node quadrilateral plate element\nand method is a monoresolution one and also a special case of the proposed\nelement and method. The meshing for the monoresolution plate element model is\nbased on the empiricism while the RL adjusting for the multiresolution is laid\non the rigorous mathematical basis. The accuracy of a structural analysis is\nactually determined by the RL, not by the mesh. The rational MRA enable the\nimplementation of the multiresolution element method to be more rational and\nefficient than that of the conventional monoresolution plate element method or\nother corresponding MRA methods such as the wavelet finite element method, the\nmeshfree method, and the natural element method etc."
},{
    "category": "math.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.1610v1", 
    "title": "An Efficient Approach for Computing Optimal Low-Rank Regularized Inverse   Matrices", 
    "arxiv-id": "1404.1610v1", 
    "author": "Matthias Chung", 
    "publish": "2014-04-06T19:06:37Z", 
    "summary": "Standard regularization methods that are used to compute solutions to\nill-posed inverse problems require knowledge of the forward model. In many\nreal-life applications, the forward model is not known, but training data is\nreadily available. In this paper, we develop a new framework that uses training\ndata, as a substitute for knowledge of the forward model, to compute an optimal\nlow-rank regularized inverse matrix directly, allowing for very fast\ncomputation of a regularized solution. We consider a statistical framework\nbased on Bayes and empirical Bayes risk minimization to analyze theoretical\nproperties of the problem. We propose an efficient rank update approach for\ncomputing an optimal low-rank regularized inverse matrix for various error\nmeasures. Numerical experiments demonstrate the benefits and potential\napplications of our approach to problems in signal and image processing."
},{
    "category": "cs.DS", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.1810v1", 
    "title": "A class of AM-QFT algorithms for power-of-two FFT", 
    "arxiv-id": "1404.1810v1", 
    "author": "Lorenzo Pasquini", 
    "publish": "2014-04-07T15:06:19Z", 
    "summary": "This paper proposes a class of power-of-two FFT (Fast Fourier Transform)\nalgorithms, called AM-QFT algorithms, that contains the improved QFT (Quick\nFourier Transform), an algorithm recently published, as a special case. The\nmain idea is to apply the Amplitude Modulation Double Sideband - Suppressed\nCarrier (AM DSB-SC) to convert odd-indices signals into even-indices signals,\nand to insert this elaboration into the improved QFT algorithm, substituting\nthe multiplication by secant function. The 8 variants of this class are\nobtained by re-elaboration of the AM DSB-SC idea, and by means of duality. As a\nresult the 8 variants have both the same computational cost and the same memory\nrequirements than improved QFT. Differently, comparing this class of 8 variants\nof AM-QFT algorithm with the split-radix 3add/3mul (one of the most performing\nFFT approach appeared in the literature), we obtain the same number of\nadditions and multiplications, but employing half of the trigonometric\nconstants. This makes the proposed FFT algorithms interesting and useful for\nfixed-point implementations. Some of these variants show advantages versus the\nimproved QFT. In fact one of this variant slightly enhances the numerical\naccuracy of improved QFT, while other four variants use trigonometric constants\nthat are faster to compute in `on the fly' implementations."
},{
    "category": "cs.DS", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.3327v1", 
    "title": "Supremum-Norm Convergence for Step-Asynchronous Successive   Overrelaxation on M-matrices", 
    "arxiv-id": "1404.3327v1", 
    "author": "Sebastiano Vigna", 
    "publish": "2014-04-12T23:25:52Z", 
    "summary": "Step-asynchronous successive overrelaxation updates the values contained in a\nsingle vector using the usual Gau\\ss-Seidel-like weighted rule, but arbitrarily\nmixing old and new values, the only constraint being temporal coherence: you\ncannot use a value before it has been computed. We show that given a\nnonnegative real matrix $A$, a $\\sigma\\geq\\rho(A)$ and a vector $\\boldsymbol\nw>0$ such that $A\\boldsymbol w\\leq\\sigma\\boldsymbol w$, every iteration of\nstep-asynchronous successive overrelaxation for the problem $(sI- A)\\boldsymbol\nx=\\boldsymbol b$, with $s >\\sigma$, reduces geometrically the $\\boldsymbol\nw$-norm of the current error by a factor that we can compute explicitly. Then,\nwe show that given a $\\sigma>\\rho(A)$ it is in principle always possible to\ncompute such a $\\boldsymbol w$. This property makes it possible to estimate the\nsupremum norm of the absolute error at each iteration without any additional\nhypothesis on $A$, even when $A$ is so large that computing the product\n$A\\boldsymbol x$ is feasible, but estimating the supremum norm of $(sI-A)^{-1}$\nis not."
},{
    "category": "math.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.3451v1", 
    "title": "A fast direct solver for high frequency scattering from a large cavity   in two dimensions", 
    "arxiv-id": "1404.3451v1", 
    "author": "Leslie F. Greengard", 
    "publish": "2014-04-14T02:38:34Z", 
    "summary": "We present a fast direct solver for the simulation of electromagnetic\nscattering from an arbitrarily-shaped, large, empty cavity embedded in an\ninfinite perfectly conducting half space. The governing Maxwell equations are\nreformulated as a well-conditioned second kind integral equation and the\nresulting linear system is solved in nearly linear time using a hierarchical\nmatrix factorization technique. We illustrate the performance of the scheme\nwith several numerical examples for complex cavity shapes over a wide range of\nfrequencies."
},{
    "category": "cs.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.4132v4", 
    "title": "Projection Algorithms for Non-Convex Minimization with Application to   Sparse Principal Component Analysis", 
    "arxiv-id": "1404.4132v4", 
    "author": "Jia-Jie Zhu", 
    "publish": "2014-04-16T03:57:42Z", 
    "summary": "We consider concave minimization problems over non-convex sets.Optimization\nproblems with this structure arise in sparse principal component analysis. We\nanalyze both a gradient projection algorithm and an approximate Newton\nalgorithm where the Hessian approximation is a multiple of the identity.\nConvergence results are established. In numerical experiments arising in sparse\nprincipal component analysis, it is seen that the performance of the gradient\nprojection algorithm is very similar to that of the truncated power method and\nthe generalized power method. In some cases, the approximate Newton algorithm\nwith a Barzilai-Borwein (BB) Hessian approximation can be substantially faster\nthan the other algorithms, and can converge to a better solution."
},{
    "category": "math.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.5363v2", 
    "title": "A constraint on extensible quadrature rules", 
    "arxiv-id": "1404.5363v2", 
    "author": "Art B. Owen", 
    "publish": "2014-04-22T01:26:04Z", 
    "summary": "When the worst case integration error in a family of functions decays as\n$n^{-\\alpha}$ for some $\\alpha>1$ and simple averages along an extensible\nsequence match that rate at a set of sample sizes $n_1<n_2<\\dots<\\infty$, then\nthese sample sizes must grow at least geometrically. More precisely,\n$n_{k+1}/n_k\\ge \\rho$ must hold for a value $1<\\rho<2$ that increases with\n$\\alpha$. This result always rules out arithmetic sequences but never rules out\nsample size doubling. The same constraint holds in a root mean square setting."
},{
    "category": "cs.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.5525v1", 
    "title": "Global Newton Iteration over Archimedean and non-Archimedean Fields", 
    "arxiv-id": "1404.5525v1", 
    "author": "Agnes Szanto", 
    "publish": "2014-04-17T20:00:33Z", 
    "summary": "In this paper, we study iterative methods on the coefficients of the rational\nunivariate representation (RUR) of a given algebraic set, called global Newton\niteration. We compare two natural approaches to define locally quadratically\nconvergent iterations: the first one involves Newton iteration applied to the\napproximate roots individually and then interpolation to find the RUR of these\napproximate roots; the second one considers the coefficients in the exact RUR\nas zeroes of a high dimensional map defined by polynomial reduction, and\napplies Newton iteration on this map. We prove that over fields with a p-adic\nvaluation these two approaches give the same iteration function, but over\nfields equipped with the usual Archimedean absolute value, they are not\nequivalent. In the latter case, we give explicitly the iteration function for\nboth approaches. Finally, we analyze the parallel complexity of the different\nversions of the global Newton iteration, compare them, and demonstrate that\nthey can be efficiently computed. The motivation for this study comes from the\ncertification of approximate roots of overdetermined and singular polynomial\nsystems via the recovery of an exact RUR from approximate numerical data."
},{
    "category": "math.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.6817v4", 
    "title": "Novel Approach to Real Polynomial Root-finding and Matrix Eigen-solving", 
    "arxiv-id": "1404.6817v4", 
    "author": "Victor Y. Pan", 
    "publish": "2014-04-27T19:58:07Z", 
    "summary": "Univariate polynomial root-finding is both classical and important for modern\ncomputing. Frequently one seeks just the real roots of a polynomial with real\ncoefficients. They can be approximated at a low computational cost if the\npolynomial has no nonreal roots, but typically nonreal roots are much more\nnumerous than the real ones. We dramatically accelerate the known algorithms in\nthis case by exploiting the correlation between the computations with matrices\nand polynomials, extending the techniques of the matrix sign iteration, and\nexploiting the structure of the companion matrix of the input polynomial. We\nextend some of the proposed techniques to the approximation of the real\neigenvalues of a real nonsymmetric matrix."
},{
    "category": "cs.NA", 
    "doi": "10.1088/0266-5611/30/11/114009", 
    "link": "http://arxiv.org/pdf/1404.6871v1", 
    "title": "Proximal Iteratively Reweighted Algorithm with Multiple Splitting for   Nonconvex Sparsity Optimization", 
    "arxiv-id": "1404.6871v1", 
    "author": "Shuicheng Yan", 
    "publish": "2014-04-28T05:52:30Z", 
    "summary": "This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for\nsolving a general problem, which involves a large body of nonconvex sparse and\nstructured sparse related problems. Comparing with previous iterative solvers\nfor nonconvex sparse problem, PIRE is much more general and efficient. The\ncomputational cost of PIRE in each iteration is usually as low as the\nstate-of-the-art convex solvers. We further propose the PIRE algorithm with\nParallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating\n(PIRE-AU) to handle the multi-variable problems. In theory, we prove that our\nproposed methods converge and any limit solution is a stationary point.\nExtensive experiments on both synthesis and real data sets demonstrate that our\nmethods achieve comparative learning performance, but are much more efficient,\nby comparing with previous nonconvex solvers."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2014.09.035", 
    "link": "http://arxiv.org/pdf/1406.0306v3", 
    "title": "Fast Isogeometric Boundary Element Method based on Independent Field   Approximation", 
    "arxiv-id": "1406.0306v3", 
    "author": "Thomas-Peter Fries", 
    "publish": "2014-06-02T09:33:19Z", 
    "summary": "An isogeometric boundary element method for problems in elasticity is\npresented, which is based on an independent approximation for the geometry,\ntraction and displacement field. This enables a flexible choice of refinement\nstrategies, permits an efficient evaluation of geometry related information, a\nmixed collocation scheme which deals with discontinuous tractions along\nnon-smooth boundaries and a significant reduction of the right hand side of the\nsystem of equations for common boundary conditions. All these benefits are\nachieved without any loss of accuracy compared to conventional isogeometric\nformulations. The system matrices are approximated by means of hierarchical\nmatrices to reduce the computational complexity for large scale analysis. For\nthe required geometrical bisection of the domain, a strategy for the evaluation\nof bounding boxes containing the supports of NURBS basis functions is\npresented. The versatility and accuracy of the proposed methodology is\ndemonstrated by convergence studies showing optimal rates and real world\nexamples in two and three dimensions."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cma.2014.09.035", 
    "link": "http://arxiv.org/pdf/1406.0907v2", 
    "title": "Computing GCRDs of Approximate Differential Polynomials", 
    "arxiv-id": "1406.0907v2", 
    "author": "Joseph Haraldson", 
    "publish": "2014-06-03T23:52:08Z", 
    "summary": "Differential (Ore) type polynomials with approximate polynomial coefficients\nare introduced. These provide a useful representation of approximate\ndifferential operators with a strong algebraic structure, which has been used\nsuccessfully in the exact, symbolic, setting. We then present an algorithm for\nthe approximate Greatest Common Right Divisor (GCRD) of two approximate\ndifferential polynomials, which intuitively is the differential operator whose\nsolutions are those common to the two inputs operators. More formally, given\napproximate differential polynomials $f$ and $g$, we show how to find \"nearby\"\npolynomials $\\widetilde f$ and $\\widetilde g$ which have a non-trivial GCRD.\nHere \"nearby\" is under a suitably defined norm. The algorithm is a\ngeneralization of the SVD-based method of Corless et al. (1995) for the\napproximate GCD of regular polynomials. We work on an appropriately\n\"linearized\" differential Sylvester matrix, to which we apply a block SVD. The\nalgorithm has been implemented in Maple and a demonstration of its robustness\nis presented."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.camwa.2014.12.004", 
    "link": "http://arxiv.org/pdf/1406.1933v1", 
    "title": "On the error propagation of semi-Lagrange and Fourier methods for   advection problems", 
    "arxiv-id": "1406.1933v1", 
    "author": "Alexander Ostermann", 
    "publish": "2014-06-07T21:28:02Z", 
    "summary": "In this paper we study the error propagation of numerical schemes for the\nadvection equation in the case where high precision is desired. The numerical\nmethods considered are based on the fast Fourier transform, polynomial\ninterpolation (semi-Lagrangian methods using a Lagrange or spline\ninterpolation), and a discontinuous Galerkin semi-Lagrangian approach (which is\nconservative and has to store more than a single value per cell).\n  We demonstrate, by carrying out numerical experiments, that the worst case\nerror estimates given in the literature provide a good explanation for the\nerror propagation of the interpolation-based semi-Lagrangian methods. For the\ndiscontinuous Galerkin semi-Lagrangian method, however, we find that the\ncharacteristic property of semi-Lagrangian error estimates (namely the fact\nthat the error increases proportionally to the number of time steps) is not\nobserved. We provide an explanation for this behavior and conduct numerical\nsimulations that corroborate the different qualitative features of the error in\nthe two respective types of semi-Lagrangian methods.\n  The method based on the fast Fourier transform is exact but, due to round-off\nerrors, susceptible to a linear increase of the error in the number of time\nsteps. We show how to modify the Cooley--Tukey algorithm in order to obtain an\nerror growth that is proportional to the square root of the number of time\nsteps.\n  Finally, we show, for a simple model, that our conclusions hold true if the\nadvection solver is used as part of a splitting scheme."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.1967v4", 
    "title": "Quasi-Monte Carlo point sets with small $t$-values and WAFOM", 
    "arxiv-id": "1406.1967v4", 
    "author": "Shin Harase", 
    "publish": "2014-06-08T10:03:26Z", 
    "summary": "The $t$-value of a $(t, m, s)$-net is an important criterion of point sets\nfor quasi-Monte Carlo integration, and many point sets are constructed in terms\nof the $t$-values, as this leads to small integration error bounds. Recently,\nMatsumoto, Saito, and Matoba proposed the Walsh figure of merit (WAFOM) as a\nquickly computable criterion of point sets that ensures higher order\nconvergence for function classes of very high smoothness. In this paper, we\nconsider a search algorithm for point sets whose $t$-value and WAFOM are both\nsmall, so as to be effective for a wider range of function classes. For this,\nwe fix digital $(t, m, s)$-nets with small $t$-values (e.g., Sobol' or\nNiederreiter--Xing nets) in advance, apply random linear scrambling, and select\nscrambled digital $(t, m, s)$-nets in terms of WAFOM. Experiments show that the\nresulting point sets improve the rates of convergence for smooth functions and\nare robust for non-smooth functions."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.1974v1", 
    "title": "Communication Complexity of the Fast Multipole Method and its Algebraic   Variants", 
    "arxiv-id": "1406.1974v1", 
    "author": "David Keyes", 
    "publish": "2014-06-08T11:48:44Z", 
    "summary": "A combination of hierarchical tree-like data structures and data access\npatterns from fast multipole methods and hierarchical low-rank approximation of\nlinear operators from H-matrix methods appears to form an algorithmic path\nforward for efficient implementation of many linear algebraic operations of\nscientific computing at the exascale. The combination provides asymptotically\noptimal computational and communication complexity and applicability to large\nclasses of operators that commonly arise in scientific computing applications.\nA convergence of the mathematical theories of the fast multipole and H-matrix\nmethods has been underway for over a decade. We recap this mathematical\nunification and describe implementation aspects of a hybrid of these two\ncompelling hierarchical algorithms on hierarchical distributed-shared memory\narchitectures, which are likely to be the first to reach the exascale. We\npresent a new communication complexity estimate for fast multipole methods on\nsuch architectures. We also show how the data structures and access patterns of\nH-matrices for low-rank operators map onto those of fast multipole, leading to\nan algebraically generalized form of fast multipole that compromises none of\nits architecturally ideal properties."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.2010v2", 
    "title": "On low complexity Acceleration Techniques for Randomized Optimization:   Supplementary Online Material", 
    "arxiv-id": "1406.2010v2", 
    "author": "Sebastian U. Stich", 
    "publish": "2014-06-08T18:24:08Z", 
    "summary": "Recently it was shown by Nesterov (2011) that techniques form convex\noptimization can be used to successfully accelerate simple derivative-free\nrandomized optimization methods. The appeal of those schemes lies in their low\ncomplexity, which is only $\\Theta(n)$ per iteration---compared to $\\Theta(n^2)$\nfor algorithms storing second-order information or covariance matrices. From a\nhigh-level point of view, those accelerated schemes employ correlations between\nsuccessive iterates---a concept looking similar to the evolution path used in\nCovariance Matrix Adaptation Evolution Strategies (CMA-ES).\n  In this contribution, we (i) implement and empirically test a simple\naccelerated random search scheme (SARP). Our study is the first to provide\nnumerical evidence that SARP can effectively be implemented with adaptive step\nsize control and does not require access to gradient or advanced line search\noracles. We (ii) try to empirically verify the supposed analogy between the\nevolution path and SARP. We propose an algorithm CMA-EP that uses only the\nevolution path to bias the search. This algorithm can be generalized to a\nfamily of low memory schemes, with complexity $\\Theta(mn)$ per iteration,\nfollowing a recent approach by Loshchilov (2014). The study shows that the\nperformance of CMA-EP heavily depends on the spectra of the objective function\nand thus it cannot accelerate as consistently as SARP."
},{
    "category": "quant-ph", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.2040v2", 
    "title": "Quantum arithmetic and numerical analysis using Repeat-Until-Success   circuits", 
    "arxiv-id": "1406.2040v2", 
    "author": "Martin Roetteler", 
    "publish": "2014-06-08T23:24:49Z", 
    "summary": "We develop a method for approximate synthesis of single--qubit rotations of\nthe form $e^{-i f(\\phi_1,\\ldots,\\phi_k)X}$ that is based on the\nRepeat-Until-Success (RUS) framework for quantum circuit synthesis. We\ndemonstrate how smooth computable functions $f$ can be synthesized from two\nbasic primitives. This synthesis approach constitutes a manifestly quantum form\nof arithmetic that differs greatly from the approaches commonly used in quantum\nalgorithms. The key advantage of our approach is that it requires far fewer\nqubits than existing approaches: as a case in point, we show that using as few\nas $3$ ancilla qubits, one can obtain RUS circuits for approximate\nmultiplication and reciprocals. We also analyze the costs of performing\nmultiplication and inversion on a quantum computer using conventional\napproaches and find that they can require too many qubits to execute on a small\nquantum computer, unlike our approach."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.2817v1", 
    "title": "Isogeometric Boundary Element Method with Hierarchical Matrices", 
    "arxiv-id": "1406.2817v1", 
    "author": "Thomas-Peter Fries", 
    "publish": "2014-06-11T08:21:55Z", 
    "summary": "In this work we address the complexity problem of the isogeometric Boundary\nElement Method by proposing a collocation scheme for practical problems in\nlinear elasticity and the application of hierarchical matrices. For mixed\nboundary value problems, a block system of matrices similar to Galerkin\nformulations is constructed allowing an effective application of that matrix\nformat. We introduce a strategy for the geometric bisection of surfaces based\non NURBS patches. The approximation of system matrices is carried out by means\nof kernel interpolation. Numerical results are shown that prove the success of\nthe formulation."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.3499v1", 
    "title": "Boundary Element Analysis with trimmed NURBS and a generalized IGA   approach", 
    "arxiv-id": "1406.3499v1", 
    "author": "Thomas-Peter Fries", 
    "publish": "2014-06-13T10:54:34Z", 
    "summary": "A novel approach to the simulation with the boundary element method using\ntrimmed NURBS patches is presented. The advantage of this approach is its\nefficiency and easy implementation. The analysis with trimmed NURBS is achieved\nby double mapping. The variation of the unknowns on the boundary is specified\nin a local coordinate system and is completely independent of the description\nof the geometry. The method is tested on a branched tunnel and the results\ncompared with those obtained from a conventional analysis. The conclusion is\nthat the proposed approach is superior in terms of number of unknowns and\neffort required."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.amc.2014.12.144", 
    "link": "http://arxiv.org/pdf/1406.4473v3", 
    "title": "Structural index reduction algorithms for differential algebraic   equations via fixed-point iteration", 
    "arxiv-id": "1406.4473v3", 
    "author": "Yong Feng", 
    "publish": "2014-06-13T14:51:56Z", 
    "summary": "Motivated by Pryce's structural index reduction method for differential\nalgebraic equations (DAEs), we show the complexity of the fixed-point iteration\nalgorithm and propose a fixed-point iteration method with parameters. It leads\nto a block fixed-point iteration method which can be applied to large-scale\nDAEs with block upper triangular structure. Moreover, its complexity analysis\nis also given in this paper."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2015.2421476", 
    "link": "http://arxiv.org/pdf/1406.4802v2", 
    "title": "Homotopy based algorithms for $\\ell_0$-regularized least-squares", 
    "arxiv-id": "1406.4802v2", 
    "author": "David Brie", 
    "publish": "2014-01-31T22:26:17Z", 
    "summary": "Sparse signal restoration is usually formulated as the minimization of a\nquadratic cost function $\\|y-Ax\\|_2^2$, where A is a dictionary and x is an\nunknown sparse vector. It is well-known that imposing an $\\ell_0$ constraint\nleads to an NP-hard minimization problem. The convex relaxation approach has\nreceived considerable attention, where the $\\ell_0$-norm is replaced by the\n$\\ell_1$-norm. Among the many efficient $\\ell_1$ solvers, the homotopy\nalgorithm minimizes $\\|y-Ax\\|_2^2+\\lambda\\|x\\|_1$ with respect to x for a\ncontinuum of $\\lambda$'s. It is inspired by the piecewise regularity of the\n$\\ell_1$-regularization path, also referred to as the homotopy path. In this\npaper, we address the minimization problem $\\|y-Ax\\|_2^2+\\lambda\\|x\\|_0$ for a\ncontinuum of $\\lambda$'s and propose two heuristic search algorithms for\n$\\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backward\ngreedy strategy extending the Single Best Replacement algorithm, previously\nproposed for $\\ell_0$-minimization at a given $\\lambda$. The adaptive search of\nthe $\\lambda$-values is inspired by $\\ell_1$-homotopy. $\\ell_0$ Regularization\nPath Descent is a more complex algorithm exploiting the structural properties\nof the $\\ell_0$-regularization path, which is piecewise constant with respect\nto $\\lambda$. Both algorithms are empirically evaluated for difficult inverse\nproblems involving ill-conditioned dictionaries. Finally, we show that they can\nbe easily coupled with usual methods of model order selection."
},{
    "category": "math.NA", 
    "doi": "10.1007/s12046-014-0313-y", 
    "link": "http://arxiv.org/pdf/1406.7357v1", 
    "title": "On the solution of a class of fuzzy system of linear equations", 
    "arxiv-id": "1406.7357v1", 
    "author": "Davod Khojasteh Salkuyeh", 
    "publish": "2014-06-28T05:34:02Z", 
    "summary": "In this paper, we consider the system of linear equations $Ax=b$, where $A\\in\n\\Bbb{R}^{n \\times n}$ is a crisp H-matrix and $b$ is a fuzzy $n$-vector. We\nthen investigate the existence and uniqueness of a fuzzy solution to this\nsystem. The results can also be used for the class of M-matrices and strictly\ndiagonally dominant matrices. Finally, some numerical examples are given to\nillustrate the presented theoretical results."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1406.7426v2", 
    "title": "Approximation of skewed interfaces with tensor-based model reduction   procedures: application to the reduced basis hierarchical model reduction   approach", 
    "arxiv-id": "1406.7426v2", 
    "author": "Kathrin Smetana", 
    "publish": "2014-06-28T18:09:34Z", 
    "summary": "In this article we introduce a procedure, which allows to recover the\npotentially very good approximation properties of tensor-based model reduction\nprocedures for the solution of partial differential equations in the presence\nof interfaces or strong gradients in the solution which are skewed with respect\nto the coordinate axes. The two key ideas are the location of the interface\neither by solving a lower-dimensional partial differential equation or by using\ndata functions and the subsequent removal of the interface of the solution by\nchoosing the determined interface as the lifting function of the Dirichlet\nboundary conditions. We demonstrate in numerical experiments for linear\nelliptic equations and the reduced basis-hierarchical model reduction approach\nthat the proposed procedure locates the interface well and yields a\nsignificantly improved convergence behavior even in the case when we only\nconsider an approximation of the interface."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.0074v1", 
    "title": "Software for Computing the Spheroidal Wave Functions Using Arbitrary   Precision Arithmetic", 
    "arxiv-id": "1408.0074v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-08-01T04:29:30Z", 
    "summary": "The spheroidal wave functions, which are the solutions to the Helmholtz\nequation in spheroidal coordinates, are notoriously difficult to compute.\nBecause of this, practically no programming language comes equipped with the\nmeans to compute them. This makes problems that require their use hard to\ntackle. We have developed computational software for calculating these special\nfunctions. Our software is called spheroidal and includes several novel\nfeatures, such as: using arbitrary precision arithmetic; adaptively choosing\nthe number of expansion coefficients to compute and use; and using the\nWronskian to choose from several different methods for computing the spheroidal\nradial functions to improve their accuracy. There are two types of spheroidal\nwave functions: the prolate kind when prolate spheroidal coordinates are used;\nand the oblate kind when oblate spheroidal coordinate are used. In this paper,\nwe describe both, methods for computing them, and our software. We have made\nour software freely available on our webpage."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.0210v3", 
    "title": "A Fast Summation Method for translation invariant kernels", 
    "arxiv-id": "1408.0210v3", 
    "author": "Fabien Casenave", 
    "publish": "2014-08-01T15:49:16Z", 
    "summary": "We derive a Fast Multipole Method (FMM) where a low-rank approximation of the\nkernel is obtained using the Empirical Interpolation Method (EIM). Contrary to\nclassical interpolation-based FMM, where the interpolation points and basis are\nfixed beforehand, the EIM is a nonlinear approximation method which constructs\ninterpolation points and basis which are adapted to the kernel under\nconsideration. The basis functions are obtained using evaluations of the kernel\nitself. We restrict ourselves to translation-invariant kernels, for which a\nmodified version of the EIM approximation can be used in a multilevel FMM\ncontext; we call the obtained algorithm Empirical Interpolation Fast Multipole\nMethod (EIFMM). An important feature of the EIFMM is a built-in error\nestimation of the interpolation error made by the low-rank approximation of the\nfar-field behavior of the kernel: the algorithm selects the optimal number of\ninterpolation points required to ensure a given accuracy for the result,\nleading to important gains for inhomogeneous kernels."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.1693v1", 
    "title": "Computing the log-determinant of symmetric, diagonally dominant matrices   in near-linear time", 
    "arxiv-id": "1408.1693v1", 
    "author": "Alexandre Bayen", 
    "publish": "2014-08-08T05:15:37Z", 
    "summary": "We present new algorithms for computing the log-determinant of symmetric,\ndiagonally dominant matrices. Existing algorithms run with cubic complexity\nwith respect to the size of the matrix in the worst case. Our algorithm\ncomputes an approximation of the log-determinant in time near-linear with\nrespect to the number of non-zero entries and with high probability. This\nalgorithm builds upon the utra-sparsifiers introduced by Spielman and Teng for\nLaplacian matrices and ultimately uses their refined versions introduced by\nKoutis, Miller and Peng in the context of solving linear systems. We also\npresent simpler algorithms that compute upper and lower bounds and that may be\nof more immediate practical interest."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.2207v1", 
    "title": "A new operational matrix based on Bernoulli polynomials", 
    "arxiv-id": "1408.2207v1", 
    "author": "K. Parand", 
    "publish": "2014-08-10T09:43:53Z", 
    "summary": "In this research, the Bernoulli polynomials are introduced. The properties of\nthese polynomials are employed to construct the operational matrices of\nintegration together with the derivative and product. These properties are then\nutilized to transform the differential equation to a matrix equation which\ncorresponds to a system of algebraic equations with unknown Bernoulli\ncoefficients. This method can be used for many problems such as differential\nequations, integral equations and so on. Numerical examples show the method is\ncomputationally simple and also illustrate the efficiency and accuracy of the\nmethod."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.2209v1", 
    "title": "The meshless method for solving radiative transfer problems in a slab   medium based on radial basis functions", 
    "arxiv-id": "1408.2209v1", 
    "author": "K. Parand", 
    "publish": "2014-08-10T10:23:54Z", 
    "summary": "In this paper a numerical meshless method for solving the radiative transfer\nequations in a slab medium with an isotropic scattering is considered. The\nmethod is based on radial basis functions to approximate the solution of an\nintegral-partial differential equation by using collocation method. For this\npurpose different applications of RBFs are used. To this end the numerical\nsolutions are obtained without any mesh generation into the domain of the\nproblems. The results of numerical experiments are compared with the existing\nresults in illustrative examples to confirm the accuracy and efficiency of the\npresented scheme. Also the norm of the residual functions are obtained to show\nthe convergence of the method."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.2940v1", 
    "title": "Optimal preconditioners for Nitsche-XFEM discretizations of interface   problems", 
    "arxiv-id": "1408.2940v1", 
    "author": "Arnold Reusken", 
    "publish": "2014-08-13T08:35:27Z", 
    "summary": "In the past decade, a combination of unfitted finite elements (or XFEM) with\nthe Nitsche method has become a popular discretization method for elliptic\ninterface problems. This development started with the introduction and analysis\nof this Nitsche-XFEM technique in the paper [A. Hansbo, P. Hansbo, Comput.\nMethods Appl. Mech. Engrg. 191 (2002)]. In general, the resulting linear\nsystems have very large condition numbers, which depend not only on the mesh\nsize $h$, but also on how the interface intersects the mesh. This paper is\nconcerned with the design and analysis of optimal preconditioners for such\nlinear systems. We propose an additive subspace preconditioner which is optimal\nin the sense that the resulting condition number is independent of the mesh\nsize $h$ and the interface position. We further show that already the simple\ndiagonal scaling of the stifness matrix results in a condition number that is\nbounded by $ch^{-2}$, with a constant $c$ that does not depend on the location\nof the interface. Both results are proven for the two-dimensional case. Results\nof numerical experiments in two and three dimensions are presented, which\nillustrate the quality of the preconditioner."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.06.021", 
    "link": "http://arxiv.org/pdf/1408.4536v1", 
    "title": "Discretization of functionals involving the Monge-Amp\u00e8re operator", 
    "arxiv-id": "1408.4536v1", 
    "author": "Edouard Oudet", 
    "publish": "2014-08-20T06:22:41Z", 
    "summary": "Gradient flows in the Wasserstein space have become a powerful tool in the\nanalysis of diffusion equations, following the seminal work of Jordan,\nKinderlehrer and Otto (JKO). The numerical applications of this formulation\nhave been limited by the difficulty to compute the Wasserstein distance in\ndimension >= 2. One step of the JKO scheme is equivalent to a variational\nproblem on the space of convex functions, which involves the Monge-Amp\\`ere\noperator. Convexity constraints are notably difficult to handle numerically,\nbut in our setting the internal energy plays the role of a barrier for these\nconstraints. This enables us to introduce a consistent discretization, which\ninherits convexity properties of the continuous variational problem. We show\nthe effectiveness of our approach on nonlinear diffusion and crowd-motion\nmodels."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140979381", 
    "link": "http://arxiv.org/pdf/1408.5535v3", 
    "title": "A Preconditioned Hybrid SVD Method for Computing Accurately Singular   Triplets of Large Matrices", 
    "arxiv-id": "1408.5535v3", 
    "author": "Andreas Stathopoulos", 
    "publish": "2014-08-23T23:19:39Z", 
    "summary": "The computation of a few singular triplets of large, sparse matrices is a\nchallenging task, especially when the smallest magnitude singular values are\nneeded in high accuracy. Most recent efforts try to address this problem\nthrough variations of the Lanczos bidiagonalization method, but they are still\nchallenged even for medium matrix sizes due to the difficulty of the problem.\nWe propose a novel SVD approach that can take advantage of preconditioning and\nof any well designed eigensolver to compute both largest and smallest singular\ntriplets. Accuracy and efficiency is achieved through a hybrid, two-stage\nmeta-method, PHSVDS. In the first stage, PHSVDS solves the normal equations up\nto the best achievable accuracy. If further accuracy is required, the method\nswitches automatically to an eigenvalue problem with the augmented matrix. Thus\nit combines the advantages of the two stages, faster convergence and accuracy,\nrespectively. For the augmented matrix, solving the interior eigenvalue is\nfacilitated by a proper use of the good initial guesses from the first stage\nand an efficient implementation of the refined projection method. We also\ndiscuss how to precondition PHSVDS and to cope with some issues that arise.\nNumerical experiments illustrate the efficiency and robustness of the method."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140979381", 
    "link": "http://arxiv.org/pdf/1408.5946v2", 
    "title": "Algorithms that satisfy a stopping criterion, probably", 
    "arxiv-id": "1408.5946v2", 
    "author": "Farbod Roosta-Khorasani", 
    "publish": "2014-08-25T23:23:56Z", 
    "summary": "Iterative numerical algorithms are typically equipped with a stopping\ncriterion, where the iteration process is terminated when some error or misfit\nmeasure is deemed to be below a given tolerance. This is a useful setting for\ncomparing algorithm performance, among other purposes. However, in practical\napplications a precise value for such a tolerance is rarely known; rather, only\nsome possibly vague idea of the desired quality of the numerical approximation\nis at hand. We discuss four case studies from different areas of numerical\ncomputation, where uncertainty in the error tolerance value and in the stopping\ncriterion is revealed in different ways. This leads us to think of approaches\nto relax the notion of exactly satisfying a tolerance value. We then\nconcentrate on a {\\em probabilistic} relaxation of the given tolerance. This\nallows, for instance, derivation of proven bounds on the sample size of certain\nMonte Carlo methods. We describe an algorithm that becomes more efficient in a\ncontrolled way as the uncertainty in the tolerance increases, and demonstrate\nthis in the context of some particular applications of inverse problems."
},{
    "category": "math.NA", 
    "doi": "10.1007/s00466-011-0645-y", 
    "link": "http://arxiv.org/pdf/1408.6143v1", 
    "title": "An enhanced method with local energy minimization for the robust a   posteriori construction of equilibrated stress fields in finite element   analyses", 
    "arxiv-id": "1408.6143v1", 
    "author": "Pierre Ladev\u00e8ze", 
    "publish": "2014-08-21T18:35:54Z", 
    "summary": "In the context of global/goal-oriented error estimation applied to\ncomputational mechanics, the need to obtain reliable and guaranteed bounds on\nthe discretization error has motivated the use of residual error estimators.\nThese estimators require the construction of admissible stress fields verifying\nthe equilibrium exactly. This article focuses on a recent method, based on a\nflux-equilibration procedure and called the element equilibration + star-patch\ntechnique (EESPT), that provides for such stress fields. The standard version\nrelies on a strong prolongation condition in order to calculate equilibrated\ntractions along finite element boundaries. Here, we propose an enhanced\nversion, which is based on a weak prolongation condition resulting in a local\nminimization of the complementary energy and leads to optimal tractions in\nselected regions. Geometric and error estimate criteria are introduced to\nselect the relevant zones for optimizing the tractions. We demonstrate how this\noptimization procedure is important and relevant to produce sharper estimators\nat affordable computational cost, especially when the error estimate criterion\nis used. Two- and three-dimensional numerical experiments demonstrate the\nefficiency of the improved technique."
},{
    "category": "math.NA", 
    "doi": "10.1137/15M1010798", 
    "link": "http://arxiv.org/pdf/1408.6497v3", 
    "title": "FFT, FMM, or Multigrid? A comparative Study of State-Of-the-Art Poisson   Solvers for Uniform and Nonuniform Grids in the Unit Cube", 
    "arxiv-id": "1408.6497v3", 
    "author": "George Biros", 
    "publish": "2014-08-27T19:17:39Z", 
    "summary": "In this work, we benchmark and discuss the performance of the scalable\nmethods for the Poisson problem which are used widely in practice: the fast\nFourier transform (FFT), the fast multipole method (FMM), the geometric\nmultigrid (GMG), and algebraic multigrid (AMG). In total we compare five\ndifferent codes, three of which are developed in our group. Our FFT, GMG, and\nFMM are parallel solvers that use high-order approximation schemes for Poisson\nproblems with continuous forcing functions (the source or right-hand side). We\nexamine and report results for weak scaling, strong scaling, and time to\nsolution for uniform and highly refined grids. We present results on the\nStampede system at the Texas Advanced Computing Center and on the Titan system\nat the Oak Ridge National Laboratory. In our largest test case, we solved a\nproblem with 600 billion unknowns on 229,379 cores of Titan. Overall, all\nmethods scale quite well to these problem sizes. We have tested all of the\nmethods with different source functions (the right-hand side in the Poisson\nproblem). Our results indicate that FFT is the method of choice for smooth\nsource functions that require uniform resolution. However, FFT loses its\nperformance advantage when the source function has highly localized features\nlike internal sharp layers. FMM and GMG considerably outperform FFT for those\ncases. The distinction between FMM and GMG is less pronounced and is sensitive\nto the quality (from a performance point of view) of the underlying\nimplementations. The high-order accurate versions of GMG and FMM significantly\noutperform their low-order accurate counterparts."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.1465v1", 
    "title": "Multilinear PageRank", 
    "arxiv-id": "1409.1465v1", 
    "author": "Yongyang Yu", 
    "publish": "2014-09-04T15:19:31Z", 
    "summary": "In this paper, we first extend the celebrated PageRank modification to a\nhigher-order Markov chain. Although this system has attractive theoretical\nproperties, it is computationally intractable for many interesting problems. We\nnext study a computationally tractable approximation to the higher-order\nPageRank vector that involves a system of polynomial equations called\nmultilinear PageRank, which is a type of tensor PageRank vector. It is\nmotivated by a novel \"spacey random surfer\" model, where the surfer remembers\nbits and pieces of history and is influenced by this information. The\nunderlying stochastic process is an instance of a vertex-reinforced random\nwalk. We develop convergence theory for a simple fixed-point method, a shifted\nfixed-point method, and a Newton iteration in a particular parameter regime. In\nmarked contrast to the case of the PageRank vector of a Markov chain where the\nsolution is always unique and easy to compute, there are parameter regimes of\nmultilinear PageRank where solutions are not unique and simple algorithms do\nnot converge. We provide a repository of these non-convergent cases that we\nencountered through exhaustive enumeration and randomly sampling that we\nbelieve is useful for future study of the problem."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.2056v1", 
    "title": "A One-Line Proof of the Fundamental Theorem of Algebra with Newton's   Method as a Consequence", 
    "arxiv-id": "1409.2056v1", 
    "author": "Bahman Kalantari", 
    "publish": "2014-09-06T20:26:36Z", 
    "summary": "Many proofs of the fundamental theorem of algebra rely on the fact that the\nminimum of the modulus of a complex polynomial over the complex plane is\nattained at some complex number. The proof then follows by arguing the minimum\nvalue is zero. This can be done by proving that at any complex number that is\nnot a zero of the polynomial we can exhibit a direction of descent for the\nmodulus. In this note we present a very short and simple proof of the existence\nof such descent direction. In particular, our descent direction gives rise to\nNewton's method for solving a polynomial equation via modulus minimization and\nalso makes the iterates definable at any critical point."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.2902v1", 
    "title": "The Hildreth's Algorithm with Applications to Soft Constraints for User   Interface Layout", 
    "arxiv-id": "1409.2902v1", 
    "author": "Alex Cloninger", 
    "publish": "2014-09-09T21:16:56Z", 
    "summary": "The Hildreth's algorithm is a row action method for solving large systems of\ninequalities. This algorithm is efficient for problems with sparse matrices, as\nopposed to direct methods such as Gaussian elimination or QR-factorization. We\napply the Hildreth's algorithm, as well as a randomized version, along with\nprioritized selection of the inequalities, to efficiently detect the highest\npriority feasible subsystem of equations. We prove convergence results and\nfeasibility criteria for both cyclic and randomized Hildreth's algorithm, as\nwell as a mixed algorithm which uses Hildreth's algorithm for inequalities and\nKaczmarz algorithm for equalities. These prioritized, sparse systems of\ninequalities commonly appear in constraint-based user interface (UI) layout\nspecifications. The performance and convergence of these proposed algorithms\nare evaluated empirically using randomly generated UI layout specifications of\nvarious sizes. The results show that these methods offer improvements in\nperformance over standard methods like Matlab's LINPROG, a well-known efficient\nlinear programming solver, and the recent developed Kaczmarz algorithm with\nprioritized IIS detection."
},{
    "category": "cs.LG", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.4018v1", 
    "title": "EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization", 
    "arxiv-id": "1409.4018v1", 
    "author": "Anna Goldenberg", 
    "publish": "2014-09-14T05:15:18Z", 
    "summary": "Nonnegative matrix factorization (NMF) methods have proved to be powerful\nacross a wide range of real-world clustering applications. Integrating multiple\ntypes of measurements for the same objects/subjects allows us to gain a deeper\nunderstanding of the data and refine the clustering. We have developed a novel\nGraph-reguarized multiview NMF-based method for data integration called\nEquiNMF. The parameters for our method are set in a completely automated\ndata-specific unsupervised fashion, a highly desirable property in real-world\napplications. We performed extensive and comprehensive experiments on multiview\nimaging data. We show that EquiNMF consistently outperforms other single-view\nNMF methods used on concatenated data and multi-view NMF methods with different\ntypes of regularizations."
},{
    "category": "math.NA", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.4926v3", 
    "title": "Symmetric Tensor Decomposition by an Iterative Eigendecomposition   Algorithm", 
    "arxiv-id": "1409.4926v3", 
    "author": "Ngai Wong", 
    "publish": "2014-09-17T09:48:29Z", 
    "summary": "We present an iterative algorithm, called the symmetric tensor eigen-rank-one\niterative decomposition (STEROID), for decomposing a symmetric tensor into a\nreal linear combination of symmetric rank-1 unit-norm outer factors using only\neigendecompositions and least-squares fitting. Originally designed for a\nsymmetric tensor with an order being a power of two, STEROID is shown to be\napplicable to any order through an innovative tensor embedding technique.\nNumerical examples demonstrate the high efficiency and accuracy of the proposed\nscheme even for large scale problems. Furthermore, we show how STEROID readily\nsolves a problem in nonlinear block-structured system identification and\nnonlinear state-space identification."
},{
    "category": "physics.chem-ph", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.4992v1", 
    "title": "An adaptive mass algorithm for Car-Parrinello and Ehrenfest ab initio   molecular dynamics", 
    "arxiv-id": "1409.4992v1", 
    "author": "Anders Szepessy", 
    "publish": "2014-09-17T13:39:39Z", 
    "summary": "Ehrenfest and Car-Parrinello molecular dynamics are computational\nalternatives to approximate Born-Oppenheimer molecular dynamics without solving\nthe electron eigenvalue problem at each time-step. A non-trivial issue is to\nchoose the artificial electron mass parameter appearing in the Car-Parrinello\nmethod to achieve both good accuracy and high computational efficiency. In this\npaper, we propose an algorithm, motivated by the Landau-Zener probability, to\nsystematically choose an artificial mass dynamically, which makes the\nCar-Parrinello and Ehrenfest molecular dynamics methods dependent only on the\nproblem data. Numerical experiments for simple model problems show that the\ntime-dependent adaptive artificial mass parameter improves the efficiency of\nthe Car-Parrinello and Ehrenfest molecular dynamics."
},{
    "category": "math.NA", 
    "doi": "10.1137/140985160", 
    "link": "http://arxiv.org/pdf/1409.5506v3", 
    "title": "Efficient approximation of sparse Jacobians for time-implicit reduced   order models", 
    "arxiv-id": "1409.5506v3", 
    "author": "Adrian Sandu", 
    "publish": "2014-09-19T03:29:32Z", 
    "summary": "This paper introduces a sparse matrix discrete interpolation method to\neffectively compute matrix approximations in the reduced order modeling\nframework. The sparse algorithm developed herein relies on the discrete\nempirical interpolation method and uses only samples of the nonzero entries of\nthe matrix series. The proposed approach can approximate very large matrices,\nunlike the current matrix discrete empirical interpolation method which is\nlimited by its large computational memory requirements. The empirical\ninterpolation indexes obtained by the sparse algorithm slightly differ from the\nones computed by the matrix discrete empirical interpolation method as a\nconsequence of the singular vectors round-off errors introduced by the economy\nor full singular value decomposition (SVD) algorithms when applied to the full\nmatrix snapshots. When appropriately padded with zeros the economy SVD\nfactorization of the nonzero elements of the snapshots matrix is a valid\neconomy SVD for the full snapshots matrix. Numerical experiments are performed\nwith the 1D Burgers and 2D Shallow Water Equations test problems where the\nquadratic reduced nonlinearities are computed via tensorial calculus. The\nsparse matrix approximation strategy is compared against five existing methods\nfor computing reduced Jacobians: a) matrix discrete empirical interpolation\nmethod, b) discrete empirical interpolation method, c) tensorial calculus, d)\nfull Jacobian projection onto the reduced basis subspace, and e) directional\nderivatives of the model along the reduced basis functions. The sparse matrix\nmethod outperforms all other algorithms. The use of traditional matrix discrete\nempirical interpolation method is not possible for very large instances due to\nits excessive memory requirements."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2015.04.022", 
    "link": "http://arxiv.org/pdf/1409.5677v2", 
    "title": "A high-order Boris integrator", 
    "arxiv-id": "1409.5677v2", 
    "author": "Daniel Ruprecht", 
    "publish": "2014-09-19T14:28:09Z", 
    "summary": "This work introduces the high-order Boris-SDC method for integrating the\nequations of motion for electrically charged particles in an electric and\nmagnetic field. Boris-SDC relies on a combination of the Boris-integrator with\nspectral deferred corrections (SDC). SDC can be considered as preconditioned\nPicard iteration to compute the stages of a collocation method. In this\ninterpretation, inverting the preconditioner corresponds to a sweep with a\nlow-order method. In Boris-SDC, the Boris method, a second-order Lorentz force\nintegrator based on velocity-Verlet, is used as a sweeper/preconditioner. The\npresented method provides a generic way to extend the classical Boris\nintegrator, which is widely used in essentially all particle-based plasma\nphysics simulations involving magnetic fields, to a high-order method.\nStability, convergence order and conservation properties of the method are\ndemonstrated for different simulation setups. Boris-SDC reproduces the expected\nhigh order of convergence for a single particle and for the center-of-mass of a\nparticle cloud in a Penning trap and shows good long-term energy stability."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2015.04.022", 
    "link": "http://arxiv.org/pdf/1409.8072v1", 
    "title": "Revisiting the stability of computing the roots of a quadratic   polynomial", 
    "arxiv-id": "1409.8072v1", 
    "author": "Van Dooren Paul", 
    "publish": "2014-09-29T11:13:09Z", 
    "summary": "We show in this paper that the roots $x_1$ and $x_2$ of a scalar quadratic\npolynomial $ax^2+bx+c=0$ with real or complex coefficients $a$, $b$ $c$ can be\ncomputed in a element-wise mixed stable manner, measured in a relative sense.\nWe also show that this is a stronger property than norm-wise backward\nstability, but weaker than element-wise backward stability. We finally show\nthat there does not exist any method that can compute the roots in an\nelement-wise backward stable sense, which is also illustrated by some numerical\nexperiments."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.jcp.2015.04.022", 
    "link": "http://arxiv.org/pdf/1409.8083v1", 
    "title": "Variational Inference For Probabilistic Latent Tensor Factorization with   KL Divergence", 
    "arxiv-id": "1409.8083v1", 
    "author": "Evrim Acar", 
    "publish": "2014-09-29T11:45:36Z", 
    "summary": "Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed\nprobabilistic framework for modelling multi-way data. Not only the common\ntensor factorization models but also any arbitrary tensor factorization\nstructure can be realized by the PLTF framework. This paper presents full\nBayesian inference via variational Bayes that facilitates more powerful\nmodelling and allows more sophisticated inference on the PLTF framework. We\nillustrate our approach on model order selection and link prediction."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10618-015-0420-3", 
    "link": "http://arxiv.org/pdf/1501.00696v1", 
    "title": "Clustering Boolean Tensors", 
    "arxiv-id": "1501.00696v1", 
    "author": "Pauli Miettinen", 
    "publish": "2015-01-04T17:01:03Z", 
    "summary": "Tensor factorizations are computationally hard problems, and in particular,\nare often significantly harder than their matrix counterparts. In case of\nBoolean tensor factorizations -- where the input tensor and all the factors are\nrequired to be binary and we use Boolean algebra -- much of that hardness comes\nfrom the possibility of overlapping components. Yet, in many applications we\nare perfectly happy to partition at least one of the modes. In this paper we\ninvestigate what consequences does this partitioning have on the computational\ncomplexity of the Boolean tensor factorizations and present a new algorithm for\nthe resulting clustering problem. This algorithm can alternatively be seen as a\nparticularly regularized clustering algorithm that can handle extremely\nhigh-dimensional observations. We analyse our algorithms with the goal of\nmaximizing the similarity and argue that this is more meaningful than\nminimizing the dissimilarity. As a by-product we obtain a PTAS and an efficient\n0.828-approximation algorithm for rank-1 binary factorizations. Our algorithm\nfor Boolean tensor clustering achieves high scalability, high similarity, and\ngood generalization to unseen data with both synthetic and real-world data\nsets."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-015-0420-3", 
    "link": "http://arxiv.org/pdf/1501.00828v1", 
    "title": "A new algorithm for multiplying two Dirac numbers", 
    "arxiv-id": "1501.00828v1", 
    "author": "Galina Cariowa", 
    "publish": "2015-01-05T12:00:11Z", 
    "summary": "In this work a rationalized algorithm for Dirac numbers multiplication is\npresented. This algorithm has a low computational complexity feature and is\nwell suited to FPGA implementation. The computation of two Dirac numbers\nproduct using the na\\\"ive method takes 256 real multiplications and 240 real\nadditions, while the proposed algorithm can compute the same result in only 88\nreal multiplications and 256 real additions. During synthesis of the discussed\nalgorithm we use the fact that Dirac numbers product may be represented as\nvector-matrix product. The matrix participating in the product has unique\nstructural properties that allow performing its advantageous decomposition.\nNamely this decomposition leads to significant reducing of the computational\ncomplexity."
},{
    "category": "stat.ME", 
    "doi": "10.1049/el.2014.3561", 
    "link": "http://arxiv.org/pdf/1501.01946v1", 
    "title": "Multi-Beam RF Aperture Using Multiplierless FFT Approximation", 
    "arxiv-id": "1501.01946v1", 
    "author": "A. Madanayake", 
    "publish": "2015-01-08T20:21:05Z", 
    "summary": "Multiple independent radio frequency (RF) beams find applications in\ncommunications, radio astronomy, radar, and microwave imaging. An $N$-point FFT\napplied spatially across an array of receiver antennas provides $N$-independent\nRF beams at $\\frac{N}{2}\\log_2N$ multiplier complexity. Here, a low-complexity\nmultiplierless approximation for the 8-point FFT is presented for RF\nbeamforming, using only 26 additions. The algorithm provides eight beams that\nclosely resemble the antenna array patterns of the traditional FFT-based\nbeamformer albeit without using multipliers. The proposed FFT-like algorithm is\nuseful for low-power RF multi-beam receivers; being synthesized in 45 nm CMOS\ntechnology at 1.1 V supply, and verified on-chip using a Xilinx Virtex-6 Lx240T\nFPGA device. The CMOS simulation and FPGA implementation indicate bandwidths of\n588 MHz and 369 MHz, respectively, for each of the independent receive-mode RF\nbeams."
},{
    "category": "cs.NA", 
    "doi": "10.4208/cicp.OA-2016-0007", 
    "link": "http://arxiv.org/pdf/1501.02581v2", 
    "title": "Mechanics-based solution verification for porous media models", 
    "arxiv-id": "1501.02581v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2015-01-12T09:41:43Z", 
    "summary": "This paper presents a new approach to verify accuracy of computational\nsimulations. We develop mathematical theorems which can serve as robust a\nposteriori error estimation techniques to identify numerical pollution, check\nthe performance of adaptive meshes, and verify numerical solutions. We\ndemonstrate performance of this methodology on problems from flow thorough\nporous media. However, one can extend it to other models. We construct\nmathematical properties such that the solutions to Darcy and Darcy-Brinkman\nequations satisfy them. The mathematical properties include the total minimum\nmechanical power, minimum dissipation theorem, reciprocal relation, and maximum\nprinciple for the vorticity. All the developed theorems have firm mechanical\nbases and are independent of numerical methods. So, these can be utilized for\nsolution verification of finite element, finite volume, finite difference,\nlattice Boltzmann methods and so forth. In particular, we show that, for a\ngiven set of boundary conditions, Darcy velocity has the minimum total\nmechanical power of all the kinematically admissible vector fields. We also\nshow that a similar result holds for Darcy-Brinkman velocity. We then show for\na conservative body force, the Darcy and Darcy-Brinkman velocities have the\nminimum total dissipation among their respective kinematically admissible\nvector fields. Using numerical examples, we show that the minimum dissipation\nand total mechanical power theorems can be utilized to identify pollution\nerrors in numerical solutions. The solutions to Darcy and Darcy-Brinkman\nequations are shown to satisfy a reciprocal relation, which has the potential\nto identify errors in the numerical implementation of boundary conditions."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2014.12.010", 
    "link": "http://arxiv.org/pdf/1501.06741v1", 
    "title": "A simple approach to the numerical simulation with trimmed CAD surfaces", 
    "arxiv-id": "1501.06741v1", 
    "author": "J\u00fcrgen Zechner", 
    "publish": "2015-01-27T11:01:00Z", 
    "summary": "In this work a novel method for the analysis with trimmed CAD surfaces is\npresented. The method involves an additional mapping step and the attraction\nstems from its sim- plicity and ease of implementation into existing Finite\nElement (FEM) or Boundary Element (BEM) software. The method is first verified\nwith classical test examples in structural mechanics. Then two practical\napplications are presented one using the FEM, the other the BEM, that show the\napplicability of the method."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2014.12.010", 
    "link": "http://arxiv.org/pdf/1007.0380v1", 
    "title": "Additive Non-negative Matrix Factorization for Missing Data", 
    "arxiv-id": "1007.0380v1", 
    "author": "Mithun Das Gupta", 
    "publish": "2010-07-01T17:40:01Z", 
    "summary": "Non-negative matrix factorization (NMF) has previously been shown to be a\nuseful decomposition for multivariate data. We interpret the factorization in a\nnew way and use it to generate missing attributes from test data. We provide a\njoint optimization scheme for the missing attributes as well as the NMF\nfactors. We prove the monotonic convergence of our algorithms. We present\nclassification results for cases with missing attributes."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cma.2014.12.010", 
    "link": "http://arxiv.org/pdf/1007.3753v4", 
    "title": "Fast L1-Minimization Algorithms For Robust Face Recognition", 
    "arxiv-id": "1007.3753v4", 
    "author": "Yi Ma", 
    "publish": "2010-07-21T20:26:26Z", 
    "summary": "L1-minimization refers to finding the minimum L1-norm solution to an\nunderdetermined linear system b=Ax. Under certain conditions as described in\ncompressive sensing theory, the minimum L1-norm solution is also the sparsest\nsolution. In this paper, our study addresses the speed and scalability of its\nalgorithms. In particular, we focus on the numerical implementation of a\nsparsity-based classification framework in robust face recognition, where\nsparse representation is sought to recover human identities from very\nhigh-dimensional facial images that may be corrupted by illumination, facial\ndisguise, and pose variation. Although the underlying numerical problem is a\nlinear program, traditional algorithms are known to suffer poor scalability for\nlarge-scale applications. We investigate a new solution based on a classical\nconvex optimization framework, known as Augmented Lagrangian Methods (ALM). The\nnew convex solvers provide a viable solution to real-world, time-critical\napplications such as face recognition. We conduct extensive experiments to\nvalidate and compare the performance of the ALM algorithms against several\npopular L1-minimization solvers, including interior-point method, Homotopy,\nFISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peer\nevaluation, the code for all the algorithms has been made publicly available."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cma.2014.12.010", 
    "link": "http://arxiv.org/pdf/1007.3881v2", 
    "title": "Orthogonal multifilters image processing of astronomical images from   scanned photographic plates", 
    "arxiv-id": "1007.3881v2", 
    "author": "Vasil Kolev", 
    "publish": "2010-07-22T13:22:50Z", 
    "summary": "In this paper orthogonal multifilters for astronomical image processing are\npresented. We obtained new orthogonal multifilters based on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\na more powerful multiscale analysis tool. It adds several degrees of freedom in\nmultifilter design and makes it possible to have several useful properties such\nas symmetry, orthogonality, short support, and a higher number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned photographic\nplates with astronomical images is made."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.cma.2014.12.010", 
    "link": "http://arxiv.org/pdf/1007.5510v2", 
    "title": "An algorithm for the principal component analysis of large data sets", 
    "arxiv-id": "1007.5510v2", 
    "author": "Mark Tygert", 
    "publish": "2010-07-30T18:24:23Z", 
    "summary": "Recently popularized randomized methods for principal component analysis\n(PCA) efficiently and reliably produce nearly optimal accuracy --- even on\nparallel processors --- unlike the classical (deterministic) alternatives. We\nadapt one of these randomized methods for use with data sets that are too large\nto be stored in random-access memory (RAM). (The traditional terminology is\nthat our procedure works efficiently \"out-of-core.\") We illustrate the\nperformance of the algorithm via several numerical examples. For example, we\nreport on the PCA of a data set stored on disk that is so large that less than\na hundredth of it can fit in our computer's RAM."
},{
    "category": "math.NA", 
    "doi": "10.1093/imanum/drs007", 
    "link": "http://arxiv.org/pdf/1104.2084v3", 
    "title": "Adaptative Step Size Selection for Homotopy Methods to Solve Polynomial   Equations", 
    "arxiv-id": "1104.2084v3", 
    "author": "Michael Shub", 
    "publish": "2011-04-11T23:03:01Z", 
    "summary": "Given a C^1 path of systems of homogeneous polynomial equations f_t, t in\n[a,b] and an approximation x_a to a zero zeta_a of the initial system f_a, we\nshow how to adaptively choose the step size for a Newton based homotopy method\nso that we approximate the lifted path (f_t,zeta_t) in the space of (problems,\nsolutions) pairs.\n  The total number of Newton iterations is bounded in terms of the length of\nthe lifted path in the condition metric."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.nima.2011.11.084", 
    "link": "http://arxiv.org/pdf/1109.0910v2", 
    "title": "A probability-conserving cross-section biasing mechanism for variance   reduction in Monte Carlo particle transport calculations", 
    "arxiv-id": "1109.0910v2", 
    "author": "Robert A. Weller", 
    "publish": "2011-09-05T14:20:58Z", 
    "summary": "In Monte Carlo particle transport codes, it is often important to adjust\nreaction cross sections to reduce the variance of calculations of relatively\nrare events, in a technique known as non-analogous Monte Carlo. We present the\ntheory and sample code for a Geant4 process which allows the cross section of a\nG4VDiscreteProcess to be scaled, while adjusting track weights so as to\nmitigate the effects of altered primary beam depletion induced by the cross\nsection change. This makes it possible to increase the cross section of nuclear\nreactions by factors exceeding 10^4 (in appropriate cases), without distorting\nthe results of energy deposition calculations or coincidence rates. The\nprocedure is also valid for bias factors less than unity, which is useful, for\nexample, in problems that involve computation of particle penetration deep into\na target, such as occurs in atmospheric showers or in shielding."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.nima.2011.11.084", 
    "link": "http://arxiv.org/pdf/1109.1342v1", 
    "title": "Trace Norm Regularized Tensor Classification and Its Online Learning   Approaches", 
    "arxiv-id": "1109.1342v1", 
    "author": "Jiqing Han", 
    "publish": "2011-09-07T02:46:40Z", 
    "summary": "In this paper we propose an algorithm to classify tensor data. Our\nmethodology is built on recent studies about matrix classification with the\ntrace norm constrained weight matrix and the tensor trace norm. Similar to\nmatrix classification, the tensor classification is formulated as a convex\noptimization problem which can be solved by using the off-the-shelf accelerated\nproximal gradient (APG) method. However, there are no analytic solutions as the\nmatrix case for the updating of the weight tensors via the proximal gradient.\nTo tackle this problem, the Douglas-Rachford splitting technique and the\nalternating direction method of multipliers (ADM) used in tensor completion are\nadapted to update the weight tensors. Further more, due to the demand of real\napplications, we also propose its online learning approaches. Experiments\ndemonstrate the efficiency of the methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.nima.2011.11.084", 
    "link": "http://arxiv.org/pdf/1109.3411v1", 
    "title": "Demonstrating the Applicability of PAINT to Computationally Expensive   Real-life Multiobjective Optimization", 
    "arxiv-id": "1109.3411v1", 
    "author": "Vesa Ojalehto", 
    "publish": "2011-09-15T17:39:56Z", 
    "summary": "We demonstrate the applicability of a new PAINT method to speed up iterations\nof interactive methods in multiobjective optimization. As our test case, we\nsolve a computationally expensive non-linear, five-objective problem of\ndesigning and operating a wastewater treatment plant. The PAINT method\ninterpolates between a given set of Pareto optimal outcomes and constructs a\ncomputationally inexpensive mixed integer linear surrogate problem for the\noriginal problem. We develop an IND-NIMBUS(R) PAINT module to combine the\ninteractive NIMBUS method and the PAINT method and to find a preferred solution\nto the original problem. With the PAINT method, the solution process with the\nNIMBUS method take a comparatively short time even though the original problem\nis computationally expensive."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.nima.2011.11.084", 
    "link": "http://arxiv.org/pdf/1109.6279v1", 
    "title": "When Newton meets Descartes: A Simple and Fast Algorithm to Isolate the   Real Roots of a Polynomial", 
    "arxiv-id": "1109.6279v1", 
    "author": "Michael Sagraloff", 
    "publish": "2011-09-28T17:39:07Z", 
    "summary": "We introduce a new algorithm denoted DSC2 to isolate the real roots of a\nunivariate square-free polynomial f with integer coefficients. The algorithm\niteratively subdivides an initial interval which is known to contain all real\nroots of f. The main novelty of our approach is that we combine Descartes' Rule\nof Signs and Newton iteration. More precisely, instead of using a fixed\nsubdivision strategy such as bisection in each iteration, a Newton step based\non the number of sign variations for an actual interval is considered, and,\nonly if the Newton step fails, we fall back to bisection. Following this\napproach, our analysis shows that, for most iterations, we can achieve\nquadratic convergence towards the real roots. In terms of complexity, our\nmethod induces a recursion tree of almost optimal size O(nlog(n tau)), where n\ndenotes the degree of the polynomial and tau the bitsize of its coefficients.\nThe latter bound constitutes an improvement by a factor of tau upon all\nexisting subdivision methods for the task of isolating the real roots. In\naddition, we provide a bit complexity analysis showing that DSC2 needs only\n\\tilde{O}(n^3tau) bit operations to isolate all real roots of f. This matches\nthe best bound known for this fundamental problem. However, in comparison to\nthe much more involved algorithms by Pan and Sch\\\"onhage (for the task of\nisolating all complex roots) which achieve the same bit complexity, DSC2\nfocuses on real root isolation, is very easy to access and easy to implement."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.0111v4", 
    "title": "A parallel sweeping preconditioner for heterogeneous 3D Helmholtz   equations", 
    "arxiv-id": "1204.0111v4", 
    "author": "Lexing Ying", 
    "publish": "2012-03-31T16:00:50Z", 
    "summary": "A parallelization of a sweeping preconditioner for 3D Helmholtz equations\nwithout large cavities is introduced and benchmarked for several challenging\nvelocity models. The setup and application costs of the sequential\npreconditioner are shown to be O({\\gamma}^2 N^{4/3}) and O({\\gamma} N log N),\nwhere {\\gamma}({\\omega}) denotes the modestly frequency-dependent number of\ngrid points per Perfectly Matched Layer. Several computational and memory\nimprovements are introduced relative to using black-box sparse-direct solvers\nfor the auxiliary problems, and competitive runtimes and iteration counts are\nreported for high-frequency problems distributed over thousands of cores. Two\nopen-source packages are released along with this paper: \"Parallel Sweeping\nPreconditioner (PSP)\" and the underlying distributed multifrontal solver,\n\"Clique\"."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.1423v1", 
    "title": "D-iteration: application to differential equations", 
    "arxiv-id": "1204.1423v1", 
    "author": "Dohy Hong", 
    "publish": "2012-04-06T07:32:47Z", 
    "summary": "In this paper, we study how the D-iteration algorithm can be applied to\nnumerically solve the differential equations such as heat equation in 2D or 3D.\nThe method can be applied on the class of problems that can be addressed by the\nGauss-Seidel iteration, based on the linear approximation of the differential\nequations."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.1586v1", 
    "title": "On Fast Computation of Gradients for CANDECOMP/PARAFAC Algorithms", 
    "arxiv-id": "1204.1586v1", 
    "author": "Andrzej Cichocki", 
    "publish": "2012-04-07T01:45:26Z", 
    "summary": "Product between mode-$n$ unfolding $\\bY_{(n)}$ of an $N$-D tensor $\\tY$ and\nKhatri-Rao products of $(N-1)$ factor matrices $\\bA^{(m)}$, $m = 1,..., n-1,\nn+1, ..., N$ exists in algorithms for CANDECOMP/PARAFAC (CP). If $\\tY$ is an\nerror tensor of a tensor approximation, this product is the gradient of a cost\nfunction with respect to factors, and has the largest workload in most CP\nalgorithms. In this paper, a fast method to compute this product is proposed.\nExperimental verification shows that the fast CP gradient can accelerate the\nCP_ALS algorithm 2 times and 8 times faster for factorizations of 3-D and 4-D\ntensors, and the speed-up ratios can be 20-30 times for higher dimensional\ntensors."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.1718v1", 
    "title": "Computational complexity and memory usage for multi-frontal direct   solvers in structured mesh finite elements", 
    "arxiv-id": "1204.1718v1", 
    "author": "Victor M. Calo", 
    "publish": "2012-04-08T08:07:47Z", 
    "summary": "The multi-frontal direct solver is the state-of-the-art algorithm for the\ndirect solution of sparse linear systems. This paper provides computational\ncomplexity and memory usage estimates for the application of the multi-frontal\ndirect solver algorithm on linear systems resulting from B-spline-based\nisogeometric finite elements, where the mesh is a structured grid. Specifically\nwe provide the estimates for systems resulting from $C^{p-1}$ polynomial\nB-spline spaces and compare them to those obtained using $C^0$ spaces."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.1726v1", 
    "title": "Dissecting the FEAST algorithm for generalized eigenproblems", 
    "arxiv-id": "1204.1726v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-04-08T10:32:18Z", 
    "summary": "We analyze the FEAST method for computing selected eigenvalues and\neigenvectors of large sparse matrix pencils. After establishing the close\nconnection between FEAST and the well-known Rayleigh-Ritz method, we identify\nseveral critical issues that influence convergence and accuracy of the solver:\nthe choice of the starting vector space, the stopping criterion, how the inner\nlinear systems impact the quality of the solution, and the use of FEAST for\ncomputing eigenpairs from multiple intervals. We complement the study with\nnumerical examples, and hint at possible improvements to overcome the existing\nproblems."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.3105v1", 
    "title": "Alternative Tilings for the Fast Multipole Method on the Plane", 
    "arxiv-id": "1204.3105v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2012-04-13T20:49:34Z", 
    "summary": "The fast multipole method (FMM) performs fast approximate kernel summation to\na specified tolerance $\\epsilon$ by using a hierarchical division of the\ndomain, which groups source and receiver points into regions that satisfy local\nseparation and the well-separated pair decomposition properties. While square\ntilings and quadtrees are commonly used in 2D, we investigate alternative\ntilings and associated spatial data structures: regular hexagons (septree) and\ntriangles (triangle-quadtree). We show that both structures satisfy separation\nproperties for the FMM and prove their theoretical error bounds and\ncomputational costs. Empirical runtime and error analysis of our\nimplementations are provided."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.3241v1", 
    "title": "Computing without a computer: a new approach for solving nonlinear   differential equations", 
    "arxiv-id": "1204.3241v1", 
    "author": "Andrey Stroganov", 
    "publish": "2012-04-15T08:05:58Z", 
    "summary": "The well-known Turing machine is an example of a theoretical digital\ncomputer, and it was the logical basis of constructing real electronic\ncomputers. In the present paper we propose an alternative, namely, by\nformalising arithmetic operations in the ordinary computing device, we attempt\nto go to the analytical procedure (for calculations). The method creates\npossibilities for solving nonlinear differential equations and systems. Our\ntheoretical computer model requires retaining a finite number of terms to\nrepresent numbers, and utilizes digit carry procedure. The solution is\nrepresented in the form of a segment of a series in the powers of the step size\nof the independent variable in the finite-difference scheme. The algorithm\ngenerates a schematic representation that approximates the convergent\nfinite-difference scheme, which, in turn, approximates the equation under\nconsideration. The use of probabilistic methods allows us to average the\nrecurrent calculations and exclude intermediate levels of computation. All the\nstages of formalizing operations of the classical computer result in \"the\nmethod of the computer analogy\". The proposed method leads to an explicit\nanalytical representation of the solution. We present the general features of\nthe algorithm which are illustrated by an example of solutions for a system of\nnonlinear equations."
},{
    "category": "stat.ME", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.4763v1", 
    "title": "Better estimation of small Sobol' sensitivity indices", 
    "arxiv-id": "1204.4763v1", 
    "author": "Art B. Owen", 
    "publish": "2012-04-21T00:15:20Z", 
    "summary": "A new method for estimating Sobol' indices is proposed. The new method makes\nuse of 3 independent input vectors rather than the usual 2. It attains much\ngreater accuracy on problems where the target Sobol' index is small, even\noutperforming some oracles which adjust using the true but unknown mean of the\nfunction. When the target Sobol' index is quite large, the oracles do better\nthan the new method."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.5429v1", 
    "title": "Understanding differential equations through diffusion point of view", 
    "arxiv-id": "1204.5429v1", 
    "author": "Dohy Hong", 
    "publish": "2012-04-24T16:58:20Z", 
    "summary": "In this paper, we propose a new adaptation of the D-iteration algorithm to\nnumerically solve the differential equations. This problem can be reinterpreted\nin 2D or 3D (or higher dimensions) as a limit of a diffusion process where the\nboundary or initial conditions are replaced by fluid catalysts. Pre-computing\nthe diffusion process for an elementary catalyst case as a fundamental block of\na class of differential equations, we show that the computation efficiency can\nbe greatly improved. The method can be applied on the class of problems that\ncan be addressed by the Gauss-Seidel iteration, based on the linear\napproximation of the differential equations."
},{
    "category": "cs.NA", 
    "doi": "10.1137/120871985", 
    "link": "http://arxiv.org/pdf/1204.6255v1", 
    "title": "Revisiting the D-iteration method: runtime comparison", 
    "arxiv-id": "1204.6255v1", 
    "author": "Philippe Raoult", 
    "publish": "2012-04-27T15:48:38Z", 
    "summary": "In this paper, we revisit the D-iteration algorithm in order to better\nexplain different performance results that were observed for the numerical\ncomputation of the eigenvector associated to the PageRank score. We revisit\nhere the practical computation cost based on the execution runtime compared to\nthe theoretical number of iterations."
},{
    "category": "cs.NA", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1207.3450v1", 
    "title": "Flux-splitting schemes for parabolic problems", 
    "arxiv-id": "1207.3450v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2012-07-14T18:50:02Z", 
    "summary": "To solve numerically boundary value problems for parabolic equations with\nmixed derivatives, the construction of difference schemes with prescribed\nquality faces essential difficulties. In parabolic problems, some possibilities\nare associated with the transition to a new formulation of the problem, where\nthe fluxes (derivatives with respect to a spatial direction) are treated as\nunknown quantities. In this case, the original problem is rewritten in the form\nof a boundary value problem for the system of equations in the fluxes. This\nwork deals with studying schemes with weights for parabolic equations written\nin the flux coordinates. Unconditionally stable flux locally one-dimensional\nschemes of the first and second order of approximation in time are constructed\nfor parabolic equations without mixed derivatives. A peculiarity of the system\nof equations written in flux variables for equations with mixed derivatives is\nthat there do exist coupled terms with time derivatives."
},{
    "category": "math.NA", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.0967v2", 
    "title": "A general approach to enhance slope limiters on non-uniform rectilinear   grids", 
    "arxiv-id": "1301.0967v2", 
    "author": "Xianyi Zeng", 
    "publish": "2013-01-06T03:12:38Z", 
    "summary": "Most slope limiter functions in high-resolution finite volume methods to\nsolve hyperbolic conservation laws are designed assuming one-dimensional\nuniform grids, and they are also used to compute slope limiters in computations\non non-uniform rectilinear grids. However, this strategy may lead to either\nloss of total variation diminishing (TVD) stability for 1D linear problems or\nthe loss of formal second-order accuracy if the grid is highly non-uniform.\nThis is especially true when the limiter function is not piecewise linear.\nNumerical evidences are provided to support this argument for two popular\nfinite volume strategies: MUSCL in space and method of lines in time\n(MUSCL-MOL), and capacity-form differencing. In order to deal with this issue,\nthis paper presents a general approach to study and enhance the slope limiter\nfunctions for highly non-uniform grids in the MUSCL-MOL framework. This\napproach extends the classical reconstruct-evolve-project procedure to general\ngrids, and it gives sufficient conditions for a slope limiter function leading\nto a TVD stable, formal second-order accuracy in space, and symmetry preserving\nnumerical scheme on arbitrary grids. Several widely used limiter functions,\nincluding the smooth ones by van Leer and van Albada, are enhanced to satisfy\nthese conditions. These properties are confirmed by solving various\none-dimensional and two-dimensional benchmark problems using the enhanced\nlimiters on highly non-uniform rectilinear grids."
},{
    "category": "cs.DC", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.1071v1", 
    "title": "Direct QR factorizations for tall-and-skinny matrices in MapReduce   architectures", 
    "arxiv-id": "1301.1071v1", 
    "author": "James Demmel", 
    "publish": "2013-01-06T22:56:36Z", 
    "summary": "The QR factorization and the SVD are two fundamental matrix decompositions\nwith applications throughout scientific computing and data analysis. For\nmatrices with many more rows than columns, so-called \"tall-and-skinny\nmatrices,\" there is a numerically stable, efficient, communication-avoiding\nalgorithm for computing the QR factorization. It has been used in traditional\nhigh performance computing and grid computing environments. For MapReduce\nenvironments, existing methods to compute the QR decomposition use a\nnumerically unstable approach that relies on indirectly computing the Q factor.\nIn the best case, these methods require only two passes over the data. In this\npaper, we describe how to compute a stable tall-and-skinny QR factorization on\na MapReduce architecture in only slightly more than 2 passes over the data. We\ncan compute the SVD with only a small change and no difference in performance.\nWe present a performance comparison between our new direct TSQR method, a\nstandard unstable implementation for MapReduce (Cholesky QR), and the classic\nstable algorithm implemented for MapReduce (Householder QR). We find that our\nnew stable method has a large performance advantage over the Householder QR\nmethod. This holds both in a theoretical performance model as well as in an\nactual implementation."
},{
    "category": "cs.NA", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.1107v4", 
    "title": "Spectral Condition-Number Estimation of Large Sparse Matrices", 
    "arxiv-id": "1301.1107v4", 
    "author": "Sivan Toledo", 
    "publish": "2013-01-07T04:31:19Z", 
    "summary": "We describe a Krylov-subspace method for estimating the spectral condition\nnumber of a real matrix A or indicating that it is numerically rank deficient.\nThe main difficulty in estimating the condition number is the estimation of the\nsmallest singular value \\sigma_{\\min} of A. Our method estimates this value by\nsolving a consistent linear least-squares problem with a known solution using a\nspecific Krylov-subspace method called LSQR. In this method, the forward error\ntends to concentrate in the direction of a singular vector corresponding to\n\\sigma_{\\min}. Extensive experiments show that the method is reliable. It is\noften much faster than a dense SVD and it can sometimes estimate the condition\nnumber when running a dense SVD would be impractical due to the computational\ncost or the memory requirements. The method uses very little memory (it\ninherits this property from LSQR) and it works equally well on square and\nrectangular matrices."
},{
    "category": "cs.NA", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.3389v2", 
    "title": "The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization", 
    "arxiv-id": "1301.3389v2", 
    "author": "Hugo Van hamme", 
    "publish": "2013-01-15T15:59:46Z", 
    "summary": "Non-negative matrix factorization (NMF) has become a popular machine learning\napproach to many problems in text mining, speech and image processing,\nbio-informatics and seismic data analysis to name a few. In NMF, a matrix of\nnon-negative data is approximated by the low-rank product of two matrices with\nnon-negative entries. In this paper, the approximation quality is measured by\nthe Kullback-Leibler divergence between the data and its low-rank\nreconstruction. The existence of the simple multiplicative update (MU)\nalgorithm for computing the matrix factors has contributed to the success of\nNMF. Despite the availability of algorithms showing faster convergence, MU\nremains popular due to its simplicity. In this paper, a diagonalized Newton\nalgorithm (DNA) is proposed showing faster convergence while the implementation\nremains simple and suitable for high-rank problems. The DNA algorithm is\napplied to various publicly available data sets, showing a substantial speed-up\non modern hardware."
},{
    "category": "cs.LG", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.3527v2", 
    "title": "Block Coordinate Descent for Sparse NMF", 
    "arxiv-id": "1301.3527v2", 
    "author": "Thomas P. Hayes", 
    "publish": "2013-01-15T23:11:05Z", 
    "summary": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data\nanalysis. An important variant is the sparse NMF problem which arises when we\nexplicitly require the learnt features to be sparse. A natural measure of\nsparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms,\nsuch as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based\non intuitive attributes that such measures need to satisfy. This is in contrast\nto computationally cheaper alternatives such as the plain L$_1$ norm. However,\npresent algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow\nand other formulations for sparse NMF have been proposed such as those based on\nL$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm\nsparsity constraints while not sacrificing computation time. We present\nexperimental evidence on real-world datasets that shows our new algorithm\nperforms an order of magnitude faster compared to the current state-of-the-art\nsolvers optimizing the mixed norm and is suitable for large-scale datasets."
},{
    "category": "cs.LG", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.3584v7", 
    "title": "Revisiting Natural Gradient for Deep Networks", 
    "arxiv-id": "1301.3584v7", 
    "author": "Yoshua Bengio", 
    "publish": "2013-01-16T04:47:02Z", 
    "summary": "We evaluate natural gradient, an algorithm originally proposed in Amari\n(1997), for learning deep models. The contributions of this paper are as\nfollows. We show the connection between natural gradient and three other\nrecently proposed methods for training deep models: Hessian-Free (Martens,\n2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et\nal., 2008). We describe how one can use unlabeled data to improve the\ngeneralization error obtained by natural gradient and empirically evaluate the\nrobustness of the algorithm to the ordering of the training set compared to\nstochastic gradient descent. Finally we extend natural gradient to incorporate\nsecond order information alongside the manifold information and provide a\nbenchmark of the new algorithm using a truncated Newton approach for inverting\nthe metric matrix instead of using a diagonal approximation of it."
},{
    "category": "cs.NA", 
    "doi": "10.1134/S0965542512080106", 
    "link": "http://arxiv.org/pdf/1301.4438v1", 
    "title": "Simultaneous computation of the row and column rank profiles", 
    "arxiv-id": "1301.4438v1", 
    "author": "Ziad Sultan", 
    "publish": "2013-01-18T17:23:01Z", 
    "summary": "Gaussian elimination with full pivoting generates a PLUQ matrix\ndecomposition. Depending on the strategy used in the search for pivots, the\npermutation matrices can reveal some information about the row or the column\nrank profiles of the matrix. We propose a new pivoting strategy that makes it\npossible to recover at the same time both row and column rank profiles of the\ninput matrix and of any of its leading sub-matrices. We propose a\nrank-sensitive and quad-recursive algorithm that computes the latter PLUQ\ntriangular decomposition of an m \\times n matrix of rank r in O(mnr^{\\omega-2})\nfield operations, with \\omega the exponent of matrix multiplication. Compared\nto the LEU decomposition by Malashonock, sharing a similar recursive structure,\nits time complexity is rank sensitive and has a lower leading constant. Over a\nword size finite field, this algorithm also improveLs the practical efficiency\nof previously known implementations."
},{
    "category": "cs.NA", 
    "doi": "10.4236/am.2010.13025", 
    "link": "http://arxiv.org/pdf/1301.4749v1", 
    "title": "On the Behavior of the Residual in Conjugate Gradient Method", 
    "arxiv-id": "1301.4749v1", 
    "author": "Teruyoshi Washizawa", 
    "publish": "2013-01-21T04:08:52Z", 
    "summary": "In conjugate gradient method, it is well known that the recursively computed\nresidual differs from true one as the iteration proceeds in finite arithmetic.\nSome work have been devoted to analyze this be-havior and to evaluate the lower\nand the upper bounds of the difference. This paper focuses on the behavior of\nthese two kinds of residuals, especially their lower bounds caused by the loss\nof trailing digit, respectively."
},{
    "category": "cs.DS", 
    "doi": "10.4236/am.2010.13025", 
    "link": "http://arxiv.org/pdf/1301.6628v1", 
    "title": "A Simple, Combinatorial Algorithm for Solving SDD Systems in   Nearly-Linear Time", 
    "arxiv-id": "1301.6628v1", 
    "author": "Zeyuan Allen Zhu", 
    "publish": "2013-01-28T18:06:21Z", 
    "summary": "In this paper, we present a simple combinatorial algorithm that solves\nsymmetric diagonally dominant (SDD) linear systems in nearly-linear time. It\nuses very little of the machinery that previously appeared to be necessary for\na such an algorithm. It does not require recursive preconditioning, spectral\nsparsification, or even the Chebyshev Method or Conjugate Gradient. After\nconstructing a \"nice\" spanning tree of a graph associated with the linear\nsystem, the entire algorithm consists of the repeated application of a simple\n(non-recursive) update rule, which it implements using a lightweight data\nstructure. The algorithm is numerically stable and can be implemented without\nthe increased bit-precision required by previous solvers. As such, the\nalgorithm has the fastest known running time under the standard unit-cost RAM\nmodel. We hope that the simplicity of the algorithm and the insights yielded by\nits analysis will be useful in both theory and practice."
},{
    "category": "math.NA", 
    "doi": "10.1002/nme.4441", 
    "link": "http://arxiv.org/pdf/1301.7530v1", 
    "title": "Total and selective reuse of Krylov subspaces for the resolution of   sequences of nonlinear structural problems", 
    "arxiv-id": "1301.7530v1", 
    "author": "Julien Pebrel", 
    "publish": "2013-01-31T07:50:11Z", 
    "summary": "This paper deals with the definition and optimization of augmentation spaces\nfor faster convergence of the conjugate gradient method in the resolution of\nsequences of linear systems. Using advanced convergence results from the\nliterature, we present a procedure based on a selection of relevant\napproximations of the eigenspaces for extracting, selecting and reusing\ninformation from the Krylov subspaces generated by previous solutions in order\nto accelerate the current iteration. Assessments of the method are proposed in\nthe cases of both linear and nonlinear structural problems."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.4441", 
    "link": "http://arxiv.org/pdf/1304.1760v1", 
    "title": "Note: interpreting iterative methods convergence with diffusion point of   view", 
    "arxiv-id": "1304.1760v1", 
    "author": "Dohy Hong", 
    "publish": "2013-04-05T16:52:21Z", 
    "summary": "In this paper, we explain the convergence speed of different iteration\nschemes with the fluid diffusion view when solving a linear fixed point\nproblem. This interpretation allows one to better understand why power\niteration or Jacobi iteration may converge faster or slower than Gauss-Seidel\niteration."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.4441", 
    "link": "http://arxiv.org/pdf/1304.1864v3", 
    "title": "Improved Accuracy and Parallelism for MRRR-based Eigensolvers -- A Mixed   Precision Approach", 
    "arxiv-id": "1304.1864v3", 
    "author": "Paolo Bientinesi", 
    "publish": "2013-04-06T08:14:25Z", 
    "summary": "The real symmetric tridiagonal eigenproblem is of outstanding importance in\nnumerical computations; it arises frequently as part of eigensolvers for\nstandard and generalized dense Hermitian eigenproblems that are based on a\nreduction to tridiagonal form. For its solution, the algorithm of Multiple\nRelatively Robust Representations (MRRR) is among the fastest methods. Although\nfast, the solvers based on MRRR do not deliver the same accuracy as competing\nmethods like Divide & Conquer or the QR algorithm. In this paper, we\ndemonstrate that the use of mixed precisions leads to improved accuracy of\nMRRR-based eigensolvers with limited or no performance penalty. As a result, we\nobtain eigensolvers that are not only equally or more accurate than the best\navailable methods, but also -in most circumstances- faster and more scalable\nthan the competition."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463469", 
    "link": "http://arxiv.org/pdf/1304.1978v2", 
    "title": "Constructing Low Star Discrepancy Point Sets with Genetic Algorithms", 
    "arxiv-id": "1304.1978v2", 
    "author": "Francois-Michel De Rainville", 
    "publish": "2013-04-07T10:29:41Z", 
    "summary": "Geometric discrepancies are standard measures to quantify the irregularity of\ndistributions. They are an important notion in numerical integration. One of\nthe most important discrepancy notions is the so-called \\emph{star\ndiscrepancy}. Roughly speaking, a point set of low star discrepancy value\nallows for a small approximation error in quasi-Monte Carlo integration. It is\nthus the most studied discrepancy notion.\n  In this work we present a new algorithm to compute point sets of low star\ndiscrepancy. The two components of the algorithm (for the optimization and the\nevaluation, respectively) are based on evolutionary principles. Our algorithm\nclearly outperforms existing approaches. To the best of our knowledge, it is\nalso the first algorithm which can be adapted easily to optimize inverse star\ndiscrepancies."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463469", 
    "link": "http://arxiv.org/pdf/1304.2097v1", 
    "title": "Solving Linear Equations by Classical Jacobi-SR Based Hybrid   Evolutionary Algorithm with Uniform Adaptation Technique", 
    "arxiv-id": "1304.2097v1", 
    "author": "Md. Bazlar Rahman", 
    "publish": "2013-04-08T04:23:55Z", 
    "summary": "Solving a set of simultaneous linear equations is probably the most important\ntopic in numerical methods. For solving linear equations, iterative methods are\npreferred over the direct methods especially when the coefficient matrix is\nsparse. The rate of convergence of iteration method is increased by using\nSuccessive Relaxation (SR) technique. But SR technique is very much sensitive\nto relaxation factor, {\\omega}. Recently, hybridization of classical\nGauss-Seidel based successive relaxation technique with evolutionary\ncomputation techniques have successfully been used to solve large set of linear\nequations in which relaxation factors are self-adapted. In this paper, a new\nhybrid algorithm is proposed in which uniform adaptive evolutionary computation\ntechniques and classical Jacobi based SR technique are used instead of\nclassical Gauss-Seidel based SR technique. The proposed Jacobi-SR based uniform\nadaptive hybrid algorithm, inherently, can be implemented in parallel\nprocessing environment efficiently. Whereas Gauss-Seidel-SR based hybrid\nalgorithms cannot be implemented in parallel computing environment efficiently.\nThe convergence theorem and adaptation theorem of the proposed algorithm are\nproved theoretically. And the performance of the proposed Jacobi-SR based\nuniform adaptive hybrid evolutionary algorithm is compared with Gauss-Seidel-SR\nbased uniform adaptive hybrid evolutionary algorithm as well as with both\nclassical Jacobi-SR method and Gauss-Seidel-SR method in the experimental\ndomain. The proposed Jacobi-SR based hybrid algorithm outperforms the\nGauss-Seidel-SR based hybrid algorithm as well as both classical Jacobi-SR\nmethod and Gauss-Seidel-SR method in terms of convergence speed and\neffectiveness."
},{
    "category": "cs.NA", 
    "doi": "10.1145/2463372.2463469", 
    "link": "http://arxiv.org/pdf/1304.2276v1", 
    "title": "Extrapolation-based implicit-explicit general linear methods", 
    "arxiv-id": "1304.2276v1", 
    "author": "Adrian Sandu", 
    "publish": "2013-04-08T17:22:23Z", 
    "summary": "For many systems of differential equations modeling problems in science and\nengineering, there are natural splittings of the right hand side into two\nparts, one non-stiff or mildly stiff, and the other one stiff. For such systems\nimplicit-explicit (IMEX) integration combines an explicit scheme for the\nnon-stiff part with an implicit scheme for the stiff part.\n  In a recent series of papers two of the authors (Sandu and Zhang) have\ndeveloped IMEX GLMs, a family of implicit-explicit schemes based on general\nlinear methods. It has been shown that, due to their high stage order, IMEX\nGLMs require no additional coupling order conditions, and are not marred by\norder reduction.\n  This work develops a new extrapolation-based approach to construct practical\nIMEX GLM pairs of high order. We look for methods with large absolute stability\nregion, assuming that the implicit part of the method is A- or L-stable. We\nprovide examples of IMEX GLMs with optimal stability properties. Their\napplication to a two dimensional test problem confirms the theoretical\nfindings."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2463372.2463469", 
    "link": "http://arxiv.org/pdf/1304.2545v1", 
    "title": "For Solving Linear Equations Recombination is a Needless Operation in   Time-Variant Adaptive Hybrid Algorithms", 
    "arxiv-id": "1304.2545v1", 
    "author": "M. M. A. Hashem", 
    "publish": "2013-04-09T12:00:43Z", 
    "summary": "Recently hybrid evolutionary computation (EC) techniques are successfully\nimplemented for solving large sets of linear equations. All the recently\ndeveloped hybrid evolutionary algorithms, for solving linear equations, contain\nboth the recombination and the mutation operations. In this paper, two modified\nhybrid evolutionary algorithms contained time-variant adaptive evolutionary\ntechnique are proposed for solving linear equations in which recombination\noperation is absent. The effectiveness of the recombination operator has been\nstudied for the time-variant adaptive hybrid algorithms for solving large set\nof linear equations. Several experiments have been carried out using both the\nproposed modified hybrid evolutionary algorithms (in which the recombination\noperation is absent) and corresponding existing hybrid algorithms (in which the\nrecombination operation is present) to solve large set of linear equations. It\nis found that the number of generations required by the existing hybrid\nalgorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA)\nhybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybrid\nalgorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SR\nbased time variant adaptive (MGSBTVA) hybrid algorithm and the modified\nJacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) are\ncomparable. Also the proposed modified algorithms require less amount of\ncomputational time in comparison to the corresponding existing hybrid\nalgorithms. As the proposed modified hybrid algorithms do not contain\nrecombination operation, so they require less computational effort, and also\nthey are more efficient, effective and easy to implement."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.camwa.2013.04.015", 
    "link": "http://arxiv.org/pdf/1304.2695v1", 
    "title": "Locally exact modifications of numerical schemes", 
    "arxiv-id": "1304.2695v1", 
    "author": "Jan L. Cie\u015bli\u0144ski", 
    "publish": "2013-04-09T18:48:18Z", 
    "summary": "We present a new class of exponential integrators for ordinary differential\nequations: locally exact modifications of known numerical schemes. Local\nexactness means that they preserve the linearization of the original system at\nevery point. In particular, locally exact integrators preserve all fixed points\nand are A-stable. We apply this approach to popular schemes including Euler\nschemes, implicit midpoint rule and trapezoidal rule. We found locally exact\nmodifications of discrete gradient schemes (for symmetric discrete gradients\nand coordinate increment discrete gradients) preserving their main geometric\nproperty: exact conservation of the energy integral (for arbitrary\nmultidimensional Hamiltonian systems in canonical coordinates). Numerical\nexperiments for a 2-dimensional anharmonic oscillator show that locally exact\nschemes have very good accuracy in the neighbourhood of stable equilibrium,\nmuch higher than suggested by the order of new schemes (locally exact\nmodification sometimes increases the order but in many cases leaves it\nunchanged)."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.camwa.2013.04.015", 
    "link": "http://arxiv.org/pdf/1304.3200v1", 
    "title": "An Approach to Solve Linear Equations Using a Time-Variant Adaptation   Based Hybrid Evolutionary Algorithm", 
    "arxiv-id": "1304.3200v1", 
    "author": "Md. Bazlar Rahman", 
    "publish": "2013-04-11T05:36:53Z", 
    "summary": "For small number of equations, systems of linear (and sometimes nonlinear)\nequations can be solved by simple classical techniques. However, for large\nnumber of systems of linear (or nonlinear) equations, solutions using classical\nmethod become arduous. On the other hand evolutionary algorithms have mostly\nbeen used to solve various optimization and learning problems. Recently,\nhybridization of evolutionary algorithm with classical Gauss-Seidel based\nSuccessive Over Relaxation (SOR) method has successfully been used to solve\nlarge number of linear equations; where a uniform adaptation (UA) technique of\nrelaxation factor is used. In this paper, a new hybrid algorithm is proposed in\nwhich a time-variant adaptation (TVA) technique of relaxation factor is used\ninstead of uniform adaptation technique to solve large number of linear\nequations. The convergence theorems of the proposed algorithms are proved\ntheoretically. And the performance of the proposed TVA-based algorithm is\ncompared with the UA-based hybrid algorithm in the experimental domain. The\nproposed algorithm outperforms the hybrid one in terms of efficiency."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.camwa.2013.04.015", 
    "link": "http://arxiv.org/pdf/1304.3919v1", 
    "title": "Modular Analysis of Almost Block Diagonal Systems of Equations", 
    "arxiv-id": "1304.3919v1", 
    "author": "Tarek M. A. El-Mistikawy", 
    "publish": "2013-04-14T14:59:00Z", 
    "summary": "Almost block diagonal linear systems of equations can be exemplified by two\nmodules. This makes it possible to construct all sequential forms of band\nand/or block elimination methods, six old and fourteen new. It allows easy\nassessment of the methods on the basis of their operation counts, storage\nneeds, and admissibility of partial pivoting. It unveils a robust partial\npivoting strategy- local pivoting. Extension of modular analysis to bordered\nsystems is also included."
},{
    "category": "math.NA", 
    "doi": "10.1080/10556788.2015.1009977", 
    "link": "http://arxiv.org/pdf/1304.4964v4", 
    "title": "Newton-Based Optimization for Kullback-Leibler Nonnegative Tensor   Factorizations", 
    "arxiv-id": "1304.4964v4", 
    "author": "Tamara G. Kolda", 
    "publish": "2013-04-17T20:35:37Z", 
    "summary": "Tensor factorizations with nonnegative constraints have found application in\nanalyzing data from cyber traffic, social networks, and other areas. We\nconsider application data best described as being generated by a Poisson\nprocess (e.g., count data), which leads to sparse tensors that can be modeled\nby sparse factor matrices. In this paper we investigate efficient techniques\nfor computing an appropriate canonical polyadic tensor factorization based on\nthe Kullback-Leibler divergence function. We propose novel subproblem solvers\nwithin the standard alternating block variable approach. Our new methods\nexploit structure and reformulate the optimization problem as small independent\nsubproblems. We employ bound-constrained Newton and quasi-Newton methods. We\ncompare our algorithms against other codes, demonstrating superior speed for\nhigh accuracy results and the ability to quickly find sparse solutions."
},{
    "category": "cs.MS", 
    "doi": "10.1080/10556788.2015.1009977", 
    "link": "http://arxiv.org/pdf/1304.5546v1", 
    "title": "Solving Wave Equations on Unstructured Geometries", 
    "arxiv-id": "1304.5546v1", 
    "author": "Jan S. Hesthaven", 
    "publish": "2013-04-19T21:07:10Z", 
    "summary": "Waves are all around us--be it in the form of sound, electromagnetic\nradiation, water waves, or earthquakes. Their study is an important basic tool\nacross engineering and science disciplines. Every wave solver serving the\ncomputational study of waves meets a trade-off of two figures of merit--its\ncomputational speed and its accuracy. Discontinuous Galerkin (DG) methods fall\non the high-accuracy end of this spectrum. Fortuitously, their computational\nstructure is so ideally suited to GPUs that they also achieve very high\ncomputational speeds. In other words, the use of DG methods on GPUs\nsignificantly lowers the cost of obtaining accurate solutions. This article\naims to give the reader an easy on-ramp to the use of this technology, based on\na sample implementation which demonstrates a highly accurate, GPU-capable,\nreal-time visualizing finite element solver in about 1500 lines of code."
},{
    "category": "cs.NA", 
    "doi": "10.1080/10556788.2015.1009977", 
    "link": "http://arxiv.org/pdf/1304.5923v1", 
    "title": "Numerical solving the identification problem for the lower coefficient   of parabolic equation", 
    "arxiv-id": "1304.5923v1", 
    "author": "V. I. Vasil'ev", 
    "publish": "2013-04-22T12:19:15Z", 
    "summary": "In the theory and practice of inverse problems for partial differential\nequations (PDEs) much attention is paid to the problem of the identification of\ncoefficients from some additional information. This work deals with the problem\nof determining in a multidimensional parabolic equation the lower coefficient\nthat depends on time only. To solve numerically a nonlinear inverse problem,\nlinearized approximations in time are constructed using standard finite element\nprocedures in space. The computational algorithm is based on a special\ndecomposition, where the transition to a new time level is implemented via\nsolving two standard elliptic problems. The numerical results presented here\nfor a model 2D problem demonstrate capabilities of the proposed computational\nalgorithms for approximate solving inverse problems."
},{
    "category": "math.NA", 
    "doi": "10.1080/10556788.2015.1009977", 
    "link": "http://arxiv.org/pdf/1304.6508v1", 
    "title": "Theoretical analysis of Sinc-collocation methods and Sinc-Nystr\u00f6m   methods for initial value problems", 
    "arxiv-id": "1304.6508v1", 
    "author": "Tomoaki Okayama", 
    "publish": "2013-04-24T08:25:59Z", 
    "summary": "A Sinc-collocation method has been proposed by Stenger, and he also gave\ntheoretical analysis of the method in the case of a `scalar' equation. This\npaper extends the theoretical results to the case of a `system' of equations.\nFurthermore, this paper proposes more efficient method by replacing the\nvariable transformation employed in Stenger's method. The efficiency is\nconfirmed by both of theoretical analysis and numerical experiments. In\naddition to the existing and newly-proposed Sinc-collocation methods, this\npaper also gives similar theoretical results for Sinc-Nystr\\\"{o}m methods\nproposed by Nurmuhammad et al. From a viewpoint of the computational cost, it\nturns out that the newly-proposed Sinc-collocation method is the most efficient\namong those methods."
},{
    "category": "math.NA", 
    "doi": "10.3970/cmc.2012.032.107", 
    "link": "http://arxiv.org/pdf/1304.6996v1", 
    "title": "Virtual Delamination Testing through Non-Linear Multi-Scale   Computational Methods: Some Recent Progress", 
    "arxiv-id": "1304.6996v1", 
    "author": "Karin Saavedra", 
    "publish": "2013-04-25T19:57:03Z", 
    "summary": "This paper deals with the parallel simulation of delamination problems at the\nmeso-scale by means of multi-scale methods, the aim being the Virtual\nDelamination Testing of Composite parts. In the non-linear context, Domain\nDecomposition Methods are mainly used as a solver for the tangent problem to be\nsolved at each iteration of a Newton-Raphson algorithm. In case of strongly\nnonlinear and heterogeneous problems, this procedure may lead to severe\ndifficulties. The paper focuses on methods to circumvent these problems, which\ncan now be expressed using a relatively general framework, even though the\ndifferent ingredients of the strategy have emerged separately. We rely here on\nthe micro-macro framework proposed in (Ladev\\`eze, Loiseau, and Dureisseix,\n2001). The method proposed in this paper introduces three additional features:\n(i) the adaptation of the macro-basis to situations where classical\nhomogenization does not provide a good preconditioner, (ii) the use of\nnon-linear relocalization to decrease the number of global problems to be\nsolved in the case of unevenly distributed non-linearities, (iii) the\nadaptation of the approximation of the local Schur complement which governs the\nconvergence of the proposed iterative technique. Computations of delamination\nand delamination-buckling interaction with contact on potentially large\ndelaminated areas are used to illustrate those aspects."
},{
    "category": "math.NA", 
    "doi": "10.3970/cmc.2012.032.107", 
    "link": "http://arxiv.org/pdf/1304.7479v3", 
    "title": "Augmenting the Immersed Boundary Method with Radial Basis Functions   (RBFs) for the Modeling of Platelets in Hemodynamic Flows", 
    "arxiv-id": "1304.7479v3", 
    "author": "Aaron L. Fogelson", 
    "publish": "2013-04-28T15:31:46Z", 
    "summary": "We present a new computational method by extending the Immersed Boundary (IB)\nmethod with a spectrally-accurate geometric model based on Radial Basis\nFunction (RBF) interpolation of the Lagrangian structures. Our specific\nmotivation is the modeling of platelets in hemodynamic flows, though we\nanticipate that our method will be useful in other applications as well. The\nefficacy of our new RBF-IB method is shown through a series of numerical\nexperiments. Specifically, we compare our method with the traditional IB method\nin terms of convergence and accuracy, computational cost, maximum stable\ntime-step size and volume loss. We conclude that the RBF-IB method has\nadvantages over the traditional Immersed Boundary method, and is well-suited\nfor modeling of platelets in hemodynamic flows."
},{
    "category": "cs.DS", 
    "doi": "10.3970/cmc.2012.032.107", 
    "link": "http://arxiv.org/pdf/1305.0526v2", 
    "title": "Matrix Inversion Is As Easy As Exponentiation", 
    "arxiv-id": "1305.0526v2", 
    "author": "Nisheeth K. Vishnoi", 
    "publish": "2013-05-02T18:06:54Z", 
    "summary": "We prove that the inverse of a positive-definite matrix can be approximated\nby a weighted-sum of a small number of matrix exponentials. Combining this with\na previous result [OSV12], we establish an equivalence between matrix inversion\nand exponentiation up to polylogarithmic factors. In particular, this\nconnection justifies the use of Laplacian solvers for designing fast\nsemi-definite programming based algorithms for certain graph problems. The\nproof relies on the Euler-Maclaurin formula and certain bounds derived from the\nRiemann zeta function."
},{
    "category": "cs.NA", 
    "doi": "10.3970/cmc.2012.032.107", 
    "link": "http://arxiv.org/pdf/1305.2384v3", 
    "title": "Asymptotic metrics on the space of matrices under the commutation   relation", 
    "arxiv-id": "1305.2384v3", 
    "author": "Michael M. Bronstein", 
    "publish": "2013-05-07T10:56:25Z", 
    "summary": "We show that the norm of the commutator defines \"almost a metric\" on the\nquotient space of commuting matrices, in the sense that it is a semi-metric\nsatisfying the triangle inequality asymptotically for large matrices drawn from\na \"good\" distribution."
},{
    "category": "cs.NA", 
    "doi": "10.3970/cmc.2012.032.107", 
    "link": "http://arxiv.org/pdf/1305.2494v1", 
    "title": "Computing Solution Operators of Boundary-value Problems for Some Linear   Hyperbolic Systems of PDEs", 
    "arxiv-id": "1305.2494v1", 
    "author": "Victor Selivanov", 
    "publish": "2013-05-11T10:31:32Z", 
    "summary": "We discuss possibilities of application of Numerical Analysis methods to\nproving computability, in the sense of the TTE approach, of solution operators\nof boundary-value problems for systems of PDEs. We prove computability of the\nsolution operator for a symmetric hyperbolic system with computable real\ncoefficients and dissipative boundary conditions, and of the Cauchy problem for\nthe same system (in this case we also prove computable dependence on the\ncoefficients) in a cube $Q\\subseteq\\mathbb R^m$. Such systems describe a wide\nvariety of physical processes (e.g. elasticity, acoustics, Maxwell equations).\nMoreover, many boundary-value problems for the wave equation also can be\nreduced to this case, thus we partially answer a question raised in\n\\cite{wz02}. Compared with most of other existing methods of proving\ncomputability for PDEs, this method does not require existence of explicit\nsolution formulas and is thus applicable to a broader class of (systems of)\nequations."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.compositesb.2013.12.018", 
    "link": "http://arxiv.org/pdf/1305.2738v1", 
    "title": "Isogeometric cohesive elements for two and three dimensional composite   delamination analysis", 
    "arxiv-id": "1305.2738v1", 
    "author": "Stephane Bordas", 
    "publish": "2013-05-13T11:00:45Z", 
    "summary": "Isogeometric cohesive elements are presented for modeling two and three\ndimensional delaminated composite structures. We exploit the knot insertion\nalgorithm offered by NURBS (Non Uniform Rational B-splines) to generate\ncohesive elements along delamination planes in an automatic fashion. A complete\ncomputational framework is presented including pre-processing, processing and\npost-processing. They are explained in details and implemented in MIGFEM--an\nopen source Matlab Isogemetric Analysis code developed by the authors. The\ncomposite laminates are modeled using both NURBS solid and shell elements.\nSeveral two and three dimensional examples ranging from standard delamination\ntests (the mixed mode bending test), the L-shaped specimen with a fillet, three\ndimensional (3D) double cantilever beam and a 3D singly curved thick-walled\nlaminate are provided. To the authors' knowledge, it is the first time that\nNURBS-based isogeometric analysis for two/three dimensional delamination\nmodeling is presented. For all examples considered, the proposed framework\noutperforms conventional Lagrange finite elements."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.compositesb.2013.12.018", 
    "link": "http://arxiv.org/pdf/1305.2952v1", 
    "title": "Finite Volume Modeling of Poroelastic-Fluid Wave Propagation with Mapped   Grids", 
    "arxiv-id": "1305.2952v1", 
    "author": "M. Yvonne Ou", 
    "publish": "2013-05-13T20:57:24Z", 
    "summary": "In this work we develop a high-resolution mapped-grid finite volume method\ncode to model wave propagation in two dimensions in systems of multiple\northotropic poroelastic media and/or fluids, with curved interfaces between\ndifferent media. We use a unified formulation to simplify modeling of the\nvarious interface conditions -- open pores, imperfect hydraulic contact, or\nsealed pores -- that may exist between such media. Our numerical code is based\non the Clawpack framework, but in order to obtain correct results at a material\ninterface we use a modified transverse Riemann solution scheme, and at such\ninterfaces are forced to drop the second-order correction term typical of\nhigh-resolution finite volume methods. We verify our code against analytical\nsolutions for reflection and transmission of waves at a material interface, and\nfor scattering of an acoustic wave train around an isotropic poroelastic\ncylinder. For reflection and transmission at a flat interface, we achieve\nsecond-order convergence in the 1-norm, and first-order in the max-norm; for\nthe cylindrical scatterer, the highly distorted grid mapping degrades\nperformance but we still achieve convergence at a reduced rate. We also\nsimulate an acoustic pulse striking a simplified model of a human femur bone,\nas an example of the capabilities of the code. To aid in reproducibility, at\nthe web site http://dx.doi.org/10.6084/m9.figshare.701483 we provide all of the\ncode used to generate the results here."
},{
    "category": "stat.ME", 
    "doi": "10.1016/j.csda.2014.06.018", 
    "link": "http://arxiv.org/pdf/1305.3312v2", 
    "title": "Stable Estimation of a Covariance Matrix Guided by Nuclear Norm   Penalties", 
    "arxiv-id": "1305.3312v2", 
    "author": "Kenneth Lange", 
    "publish": "2013-05-14T22:03:00Z", 
    "summary": "Estimation of covariance matrices or their inverses plays a central role in\nmany statistical methods. For these methods to work reliably, estimated\nmatrices must not only be invertible but also well-conditioned. In this paper\nwe present an intuitive prior that shrinks the classic sample covariance\nestimator towards a stable target. We prove that our estimator is consistent\nand asymptotically efficient. Thus, it gracefully transitions towards the\nsample covariance matrix as the number of samples grows relative to the number\nof covariates. We also demonstrate the utility of our estimator in two standard\nsituations -- discriminant analysis and EM clustering -- when the number of\nsamples is dominated by or comparable to the number of covariates."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.02.004", 
    "link": "http://arxiv.org/pdf/1305.3639v1", 
    "title": "Local error estimates for adaptive simulation of the Reaction-Diffusion   Master Equation via operator splitting", 
    "arxiv-id": "1305.3639v1", 
    "author": "Linda Petzold", 
    "publish": "2013-05-15T21:18:04Z", 
    "summary": "The efficiency of exact simulation methods for the reaction-diffusion master\nequation (RDME) is severely limited by the large number of diffusion events if\nthe mesh is fine or if diffusion constants are large. Furthermore, inherent\nproperties of exact kinetic-Monte Carlo simulation methods limit the efficiency\nof parallel implementations. Several approximate and hybrid methods have\nappeared that enable more efficient simulation of the RDME. A common feature to\nmost of them is that they rely on splitting the system into its reaction and\ndiffusion parts and updating them sequentially over a discrete timestep. This\nuse of operator splitting enables more efficient simulation but it comes at the\nprice of a temporal discretization error that depends on the size of the\ntimestep. So far, existing methods have not attempted to estimate or control\nthis error in a systematic manner. This makes the solvers hard to use for\npractitioners since they must guess an appropriate timestep. It also makes the\nsolvers potentially less efficient than if the timesteps are adapted to control\nthe error. Here, we derive estimates of the local error and propose a strategy\nto adaptively select the timestep when the RDME is simulated via a first order\noperator splitting. While the strategy is general and applicable to a wide\nrange of approximate and hybrid methods, we exemplify it here by extending a\npreviously published approximate method, the Diffusive Finite-State Projection\n(DFSP) method, to incorporate temporal adaptivity."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.02.004", 
    "link": "http://arxiv.org/pdf/1305.4650v2", 
    "title": "A parallel butterfly algorithm", 
    "arxiv-id": "1305.4650v2", 
    "author": "Lexing Ying", 
    "publish": "2013-05-20T20:33:40Z", 
    "summary": "The butterfly algorithm is a fast algorithm which approximately evaluates a\ndiscrete analogue of the integral transform \\int K(x,y) g(y) dy at large\nnumbers of target points when the kernel, K(x,y), is approximately low-rank\nwhen restricted to subdomains satisfying a certain simple geometric condition.\nIn d dimensions with O(N^d) quasi-uniformly distributed source and target\npoints, when each appropriate submatrix of K is approximately rank-r, the\nrunning time of the algorithm is at most O(r^2 N^d log N). A parallelization of\nthe butterfly algorithm is introduced which, assuming a message latency of\n\\alpha and per-process inverse bandwidth of \\beta, executes in at most O(r^2\nN^d/p log N + \\beta r N^d/p + \\alpha)log p) time using p processes. This\nparallel algorithm was then instantiated in the form of the open-source\nDistButterfly library for the special case where K(x,y)=exp(i \\Phi(x,y)), where\n\\Phi(x,y) is a black-box, sufficiently smooth, real-valued phase function.\nExperiments on Blue Gene/Q demonstrate impressive strong-scaling results for\nimportant classes of phase functions. Using quasi-uniform sources, hyperbolic\nRadon transforms and an analogue of a 3D generalized Radon transform were\nrespectively observed to strong-scale from 1-node/16-cores up to\n1024-nodes/16,384-cores with greater than 90% and 82% efficiency, respectively."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.05.034", 
    "link": "http://arxiv.org/pdf/1305.6355v2", 
    "title": "On multi-time-step monolithic coupling algorithms for elastodynamics", 
    "arxiv-id": "1305.6355v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2013-05-28T03:09:13Z", 
    "summary": "We present a way of constructing multi-time-step monolithic coupling methods\nfor elastodynamics. The governing equations for constrained multiple subdomains\nare written in dual Schur form and enforce the continuity of velocities at\nsystem time levels. The resulting equations will be in the form of\ndifferential-algebraic equations. To crystallize the ideas we shall employ\nNewmark family of time-stepping schemes. The proposed method can handle\nmultiple subdomains, and allows different time-steps as well as different time\nstepping schemes from the Newmark family in different subdomains. We shall use\nthe energy method to assess the numerical stability, and quantify the influence\nof perturbations under the proposed coupling method. We also discuss the\nconditions under which the proposed method will be energy preserving, and the\nconditions under which the method will be energy conserving. Several numerical\nexamples are presented to illustrate the accuracy and stability properties of\nthe proposed method. We shall also compare the proposed multi-time-step\ncoupling method with some other similar methods available in the literature."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.4421", 
    "link": "http://arxiv.org/pdf/1305.6857v1", 
    "title": "An adaptive time integration strategy based on displacement history   curvature", 
    "arxiv-id": "1305.6857v1", 
    "author": "A. C. Frery", 
    "publish": "2013-05-29T16:31:46Z", 
    "summary": "This work introduces a time-adaptive strategy that uses a refinement\nestimator based on the first Frenet curvature. In dynamics, a time-adaptive\nstrategy is a mechanism that interactively proposes changes to the time step\nused in iterative methods of solution. These changes aim to improve the\nrelation between quality of response and computational cost. The method here\nproposed is suitable for a variety of numerical time integration problems,\ne.g., in the study of bodies subjected to dynamical loads. The motion equation\nin its space-discrete form is used as reference to derive the formulation\npresented in this paper. Our method is contrasted with other ones based on\nlocal error estimator and apparent frequencies. We check the performance of our\nproposal when employed with the central difference, the explicit\ngeneralized-alpha and the Chung-Lee integration methods. The proposed\nrefinement estimator demands low computational resources, being easily applied\nto several direct integration methods."
},{
    "category": "cs.LG", 
    "doi": "10.1002/nme.4421", 
    "link": "http://arxiv.org/pdf/1306.2663v1", 
    "title": "Large Margin Low Rank Tensor Analysis", 
    "arxiv-id": "1306.2663v1", 
    "author": "Mohamed Cheriet", 
    "publish": "2013-06-11T21:39:56Z", 
    "summary": "Other than vector representations, the direct objects of human cognition are\ngenerally high-order tensors, such as 2D images and 3D textures. From this\nfact, two interesting questions naturally arise: How does the human brain\nrepresent these tensor perceptions in a \"manifold\" way, and how can they be\nrecognized on the \"manifold\"? In this paper, we present a supervised model to\nlearn the intrinsic structure of the tensors embedded in a high dimensional\nEuclidean space. With the fixed point continuation procedures, our model\nautomatically and jointly discovers the optimal dimensionality and the\nrepresentations of the low dimensional embeddings. This makes it an effective\nsimulation of the cognitive process of human brain. Furthermore, the\ngeneralization of our model based on similarity between the learned low\ndimensional embeddings can be viewed as counterpart of recognition of human\nbrain. Experiments on applications for object recognition and face recognition\ndemonstrate the superiority of our proposed model over state-of-the-art\napproaches."
},{
    "category": "cs.NA", 
    "doi": "10.1002/nme.4421", 
    "link": "http://arxiv.org/pdf/1306.3391v2", 
    "title": "Local Convergence of an Algorithm for Subspace Identification from   Partial Data", 
    "arxiv-id": "1306.3391v2", 
    "author": "Stephen J. Wright", 
    "publish": "2013-06-14T13:28:49Z", 
    "summary": "GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an iterative\nalgorithm for identifying a linear subspace of R^n from data consisting of\npartial observations of random vectors from that subspace. This paper examines\nlocal convergence properties of GROUSE, under assumptions on the randomness of\nthe observed vectors, the randomness of the subset of elements observed at each\niteration, and incoherence of the subspace with the coordinate directions.\nConvergence at an expected linear rate is demonstrated under certain\nassumptions. The case in which the full random vector is revealed at each\niteration allows for much simpler analysis, and is also described. GROUSE is\nrelated to incremental SVD methods and to gradient projection algorithms in\noptimization."
},{
    "category": "cs.LG", 
    "doi": "10.1002/nme.4421", 
    "link": "http://arxiv.org/pdf/1306.4080v3", 
    "title": "Parallel Coordinate Descent Newton Method for Efficient   $\\ell_1$-Regularized Minimization", 
    "arxiv-id": "1306.4080v3", 
    "author": "Ming-Hsuan Yang", 
    "publish": "2013-06-18T07:03:16Z", 
    "summary": "The recent years have witnessed advances in parallel algorithms for large\nscale optimization problems. Notwithstanding demonstrated success, existing\nalgorithms that parallelize over features are usually limited by divergence\nissues under high parallelism or require data preprocessing to alleviate these\nproblems. In this work, we propose a Parallel Coordinate Descent Newton\nalgorithm using multidimensional approximate Newton steps (PCDN), where the\noff-diagonal elements of the Hessian are set to zero to enable parallelization.\nIt randomly partitions the feature set into $b$ bundles/subsets with size of\n$P$, and sequentially processes each bundle by first computing the descent\ndirections for each feature in parallel and then conducting $P$-dimensional\nline search to obtain the step size. We show that: (1) PCDN is guaranteed to\nconverge globally despite increasing parallelism; (2) PCDN converges to the\nspecified accuracy $\\epsilon$ within the limited iteration number of\n$T_\\epsilon$, and $T_\\epsilon$ decreases with increasing parallelism (bundle\nsize $P$). Using the implementation technique of maintaining intermediate\nquantities, we minimize the data transfer and synchronization cost of the\n$P$-dimensional line search. For concreteness, the proposed PCDN algorithm is\napplied to $\\ell_1$-regularized logistic regression and $\\ell_2$-loss SVM.\nExperimental evaluations on six benchmark datasets show that the proposed PCDN\nalgorithm exploits parallelism well and outperforms the state-of-the-art\nmethods in speed without losing accuracy."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcss.2015.06.002", 
    "link": "http://arxiv.org/pdf/1306.4905v1", 
    "title": "From-Below Approximations in Boolean Matrix Factorization: Geometry and   New Algorithm", 
    "arxiv-id": "1306.4905v1", 
    "author": "Martin Trnecka", 
    "publish": "2013-06-20T15:19:22Z", 
    "summary": "We present new results on Boolean matrix factorization and a new algorithm\nbased on these results. The results emphasize the significance of\nfactorizations that provide from-below approximations of the input matrix.\nWhile the previously proposed algorithms do not consider the possibly different\nsignificance of different matrix entries, our results help measure such\nsignificance and suggest where to focus when computing factors. An experimental\nevaluation of the new algorithm on both synthetic and real data demonstrates\nits good performance in terms of good coverage by the first k factors as well\nas a small number of factors needed for exact decomposition and indicates that\nthe algorithm outperforms the available ones in these terms. We also propose\nfuture research topics."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcss.2015.06.002", 
    "link": "http://arxiv.org/pdf/1306.5013v2", 
    "title": "Randomized Interpolative Decomposition of Separated Representations", 
    "arxiv-id": "1306.5013v2", 
    "author": "Gregory Beylkin", 
    "publish": "2013-06-20T22:30:09Z", 
    "summary": "We introduce tensor Interpolative Decomposition (tensor ID) for the reduction\nof the separation rank of Canonical Tensor Decompositions (CTDs). Tensor ID\nselects, for a user-defined accuracy \\epsilon, a near optimal subset of terms\nof a CTD to represent the remaining terms via a linear combination of the\nselected terms. Tensor ID can be used as an alternative to or a step of the\nAlternating Least Squares (ALS) algorithm. In addition, we briefly discuss\nQ-factorization to reduce the size of components within an ALS iteration.\nCombined, tensor ID and Q-factorization lead to a new paradigm for the\nreduction of the separation rank of CTDs. In this context, we also discuss the\nspectral norm as a computational alternative to the Frobenius norm.\n  We reduce the problem of finding tensor IDs to that of constructing\nInterpolative Decompositions of certain matrices. These matrices are generated\nvia either randomized projection or randomized sampling of the given tensor. We\nprovide cost estimates and several examples of the new approach to the\nreduction of separation rank."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcss.2015.06.002", 
    "link": "http://arxiv.org/pdf/1306.6615v3", 
    "title": "Explicit error bound for modified numerical iterated integration by   means of Sinc methods", 
    "arxiv-id": "1306.6615v3", 
    "author": "Tomoaki Okayama", 
    "publish": "2013-06-27T19:27:30Z", 
    "summary": "This paper reinforces numerical iterated integration developed by\nMuhammad--Mori in the following two points: 1) the approximation formula is\nmodified so that it can achieve a better convergence rate in more general\ncases, and 2) explicit error bound is given in a computable form for the\nmodified formula. The formula works quite efficiently, especially if the\nintegrand is of a product type. Numerical examples that confirm it are also\npresented."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.jcss.2015.06.002", 
    "link": "http://arxiv.org/pdf/1307.4097v1", 
    "title": "Some notes on applying computational divided differencing in   optimization", 
    "arxiv-id": "1307.4097v1", 
    "author": "Stephen Vavasis", 
    "publish": "2013-07-15T20:21:43Z", 
    "summary": "We consider the problem of accurate computation of the finite difference\n$f(\\x+\\s)-f(\\x)$ when $\\Vert\\s\\Vert$ is very small. Direct evaluation of this\ndifference in floating point arithmetic succumbs to cancellation error and\nyields 0 when $\\s$ is sufficiently small. Nonetheless, accurate computation of\nthis finite difference is required by many optimization algorithms for a\n\"sufficient decrease\" test. Reps and Rall proposed a programmatic\ntransformation called \"computational divided differencing\" reminiscent of\nautomatic differentiation to compute these differences with high accuracy. The\nrunning time to compute the difference is a small constant multiple of the\nrunning time to compute $f$. Unlike automatic differentiation, however, the\ntechnique is not fully general because of a difficulty with branching code\n(i.e., `if' statements). We make several remarks about the application of\ncomputational divided differencing to optimization. One point is that the\ntechnique can be used effectively as a stagnation test."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.jcss.2015.06.002", 
    "link": "http://arxiv.org/pdf/1307.4457v2", 
    "title": "A Stochastic Successive Minimization Method for Nonsmooth Nonconvex   Optimization with Applications to Transceiver Design in Wireless   Communication Networks", 
    "arxiv-id": "1307.4457v2", 
    "author": "Zhi-Quan Luo", 
    "publish": "2013-07-17T00:19:59Z", 
    "summary": "Consider the problem of minimizing the expected value of a cost function\nparameterized by a random variable. The classical sample average approximation\n(SAA) method for solving this problem requires minimization of an ensemble\naverage of the objective at each step, which can be expensive. In this paper,\nwe propose a stochastic successive upper-bound minimization method (SSUM) which\nminimizes an approximate ensemble average at each iteration. To ensure\nconvergence and to facilitate computation, we require the approximate ensemble\naverage to be a locally tight upper-bound of the expected cost function and be\neasily optimized. The main contributions of this work include the development\nand analysis of the SSUM method as well as its applications in linear\ntransceiver design for wireless communication networks and online dictionary\nlearning. Moreover, using the SSUM framework, we extend the classical\nstochastic (sub-)gradient (SG) method to the case of minimizing a nonsmooth\nnonconvex objective function and establish its convergence."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1307.7345v1", 
    "title": "A novel method based on the Tikhonov functional for non-negative   solution of a system of linear equations with non-negative coefficients", 
    "arxiv-id": "1307.7345v1", 
    "author": "Fiks Ilya", 
    "publish": "2013-07-28T10:13:28Z", 
    "summary": "We propose a novel method for a solution of a system of linear equations with\nthe non-negativity condition. The method is based on the Tikhonov functional\nand has better accuracy and stability than other well-known algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1307.7454v2", 
    "title": "Relative Errors for Deterministic Low-Rank Matrix Approximations", 
    "arxiv-id": "1307.7454v2", 
    "author": "Jeff M. Phillips", 
    "publish": "2013-07-29T03:50:16Z", 
    "summary": "We consider processing an n x d matrix A in a stream with row-wise updates\naccording to a recent algorithm called Frequent Directions (Liberty, KDD 2013).\nThis algorithm maintains an l x d matrix Q deterministically, processing each\nrow in O(d l^2) time; the processing time can be decreased to O(d l) with a\nslight modification in the algorithm and a constant increase in space. We show\nthat if one sets l = k+ k/eps and returns Q_k, a k x d matrix that is the best\nrank k approximation to Q, then we achieve the following properties: ||A -\nA_k||_F^2 <= ||A||_F^2 - ||Q_k||_F^2 <= (1+eps) ||A - A_k||_F^2 and where\npi_{Q_k}(A) is the projection of A onto the rowspace of Q_k then ||A -\npi_{Q_k}(A)||_F^2 <= (1+eps) ||A - A_k||_F^2.\n  We also show that Frequent Directions cannot be adapted to a sparse version\nin an obvious way that retains the l original rows of the matrix, as opposed to\na linear combination or sketch of the rows."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.1709v1", 
    "title": "General inner approximation of vector-valued functions", 
    "arxiv-id": "1310.1709v1", 
    "author": "Sylvie Putot", 
    "publish": "2013-10-07T09:15:54Z", 
    "summary": "This paper addresses the problem of evaluating a subset of the range of a\nvector-valued function. It is based on a work by Gold- sztejn and Jaulin which\nprovides methods based on interval analysis to address this problem when the\ndimension of the domain and co-domain of the function are equal. This paper\nextends this result to vector-valued functions with domain and co-domain of\ndifferent dimensions. This ex- tension requires the knowledge of the rank of\nthe Jacobian function on the whole domain. This leads to the sub-problem of\nextracting an in- terval sub-matrix of maximum rank from a given interval\nmatrix. Three different techniques leading to approximate solutions of this\nextraction are proposed and compared."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.1847v1", 
    "title": "Isogeometric finite element analysis of functionally graded plates using   a refined plate theory", 
    "arxiv-id": "1310.1847v1", 
    "author": "S. P. A. Bordas", 
    "publish": "2013-10-07T16:19:34Z", 
    "summary": "We propose in this paper a novel inverse tangent transverse shear deformation\nformulation for functionally graded material (FGM) plates. The isogeometric\nfinite element analysis (IGA) of static, free vibration and buckling problems\nof FGM plates is then addressed using a refined plate theory (RPT). The RPT\nenables us to describe the non-linear distribution of shear stresses through\nthe plate thickness without any requirement of shear correction factors (SCF).\nIGA utilizes basis functions, namely B-splines or non-uniform rational\nB-splines (NURBS), which achieve easily the smoothness of any arbitrary order.\nIt hence satisfies the C1 requirement of the RPT model. The present method\napproximates the displacement field of four degrees of freedom per each control\npoint and retains the computational efficiency while ensuring the high accuracy\nin solution."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.1906v2", 
    "title": "The application of the exact operational matrices for solving the   Emden-Fowler equations, arising in astrophysics", 
    "arxiv-id": "1310.1906v2", 
    "author": "J. A. Rad", 
    "publish": "2013-10-07T19:51:54Z", 
    "summary": "The objective of this paper is to apply the well-known exact operational\nmatrices (EOMs) idea for solving the Emden-Fowler equations, illustrating the\nsuperiority of EOMs versus ordinary operational matrices (OOMs). Up to now, a\nfew studies have been conducted on EOMs and the differential equations solved\nby them do not have high-degree nonlinearity and the reported results are not\nregarded as appropriate criteria for the excellence of the new method. So, we\nchose Emden-Fowler type differential equations and solved them by this method.\nTo confirm the accuracy of the new method and to show the preeminence of EOMs\nversus OOMs, the norm1 of the residual and error function of both methods are\nevaluated for multiple $m$ values, where $m$ is the degree of the Bernstein\npolynomials. We reported the results in form of plots to illustrate the error\nconvergence of both methods to zero and also to show the primacy of the new\nmethod versus OOMs. The obtained results have demonstrated the increased\naccuracy of the new method."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.3422v3", 
    "title": "GPU-Acceleration of Parallel Unconditionally Stable Group Explicit   Finite Difference Method", 
    "arxiv-id": "1310.3422v3", 
    "author": "Sayyed A. Hossayni", 
    "publish": "2013-10-12T20:55:35Z", 
    "summary": "Graphics Processing Units (GPUs) are high performance co-processors\noriginally intended to improve the use and quality of computer graphics\napplications. Once, researchers and practitioners noticed the potential of\nusing GPU for general purposes, GPUs applications have been extended from\ngraphics applications to other fields. The main objective of this paper is to\nevaluate the impact of using GPU in solution of the transient diffusion type\nequation by parallel and stable group explicit finite difference method and\nencourage the researchers in this field to immigrate from implementing their\nalgorithms in CPU to the GPU emerging world. For comparing them, we implemented\nthe method in both GPU and CPU (multi-core) programming context. Moreover, we\nproposed an optimal synchronization arrangement for the implementation\npseudo-code. Also, the interrelation of GPU parallel programming and\ninitializing the algorithm variables were discussed, taking advantage of\nnumerical experiences. The GPU-approach results are faster than those obtained\nfrom a much expensive parallel 8-thread CPU-based programming. The GPU used in\nthis paper, is an ordinary old laptop GPU (GT 335M, launched at 2010) and is\naccessible for everyone and the newer generations of GPU (as discussed in\npaper) have even more performance priority over the similar-price GPUs. Then,\nthe results are expected to encourage the entire research society to take\nadvantage of GPUs and improve the time efficiency of their studies."
},{
    "category": "math.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.4288v1", 
    "title": "Higher Order Aitken Extrapolation with Application to Converging and   Diverging Gauss-Seidel Iterations", 
    "arxiv-id": "1310.4288v1", 
    "author": "Ababu Teklemariam Tiruneh", 
    "publish": "2013-10-16T07:46:42Z", 
    "summary": "Aitken extrapolation normally applied to convergent fixed point iteration is\nextended to extrapolate the solution of a divergent iteration. In addition,\nhigher order Aitken extrapolation is introduced that enables successive\ndecomposition of high Eigen values of the iteration matrix to enable\nconvergence. While extrapolation of a convergent fixed point iteration using a\ngeometric series sum is a known form of Aitken acceleration, it is shown in\nthis paper that the same formula can be used to estimate the solution of sets\nof linear equations from diverging Gauss Seidel iterations. In both convergent\nand divergent iterations, the ratios of differences among the consecutive\nvalues of iteration eventually form a convergent or divergent series with a\nfactor equal to the largest Eigen value of the iteration matrix. Higher order\nAitken extrapolation is shown to eliminate the influence of dominant Eigen\nvalues of the iteration matrix in successive order until the iteration is\ndetermined by the lowest possible Eigen value. For the convergent part of the\nGauss Seidel iteration, further acceleration is made possible by coupling of\nthe extrapolation technique with the successive over relaxation method.\nApplication examples from both convergent and divergent iterations have been\nprovided. Coupling of the extrapolation with the successive over relaxation\ntechnique is also illustrated for a steady state two dimensional heat flow\nproblem which was solved using MATLAB programming."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.5573v1", 
    "title": "A class of generalized additive Runge-Kutta methods", 
    "arxiv-id": "1310.5573v1", 
    "author": "Michael Guenther", 
    "publish": "2013-10-21T14:44:22Z", 
    "summary": "This work generalizes the additively partitioned Runge-Kutta methods by\nallowing for different stage values as arguments of different components of the\nright hand side. An order conditions theory is developed for the new family of\ngeneralized additive methods, and stability and monotonicity investigations are\ncarried out. The paper discusses the construction and properties of\nimplicit-explicit and implicit-implicit,methods in the new framework. The new\nfamily, named GARK, introduces additional flexibility when compared to\ntraditional partitioned Runge-Kutta methods, and therefore offers additional\nopportunities for the development of flexible solvers for systems with multiple\nscales, or driven by multiple physical processes."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.6055v1", 
    "title": "Multirate generalized additive Runge Kutta methods", 
    "arxiv-id": "1310.6055v1", 
    "author": "Adrian Sandu", 
    "publish": "2013-10-22T20:49:26Z", 
    "summary": "This work constructs a new class of multirate schemes based on the recently\ndeveloped generalized additive Runge-Kutta (GARK) methods (Sandu and Guenther,\n2013). Multirate schemes use different step sizes for different components and\nfor different partitions of the right-hand side based on the local activity\nlevels. We show that the new multirate GARK family includes many well-known\nmultirate schemes as special cases. The order conditions theory follows\ndirectly from the GARK accuracy theory. Nonlinear stability and monotonicity\ninvestigations show that these properties are inherited from the base schemes\nprovided that additional coupling conditions hold."
},{
    "category": "math.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.7202v4", 
    "title": "Randomized LU Decomposition", 
    "arxiv-id": "1310.7202v4", 
    "author": "Amir Averbuch", 
    "publish": "2013-10-27T14:55:29Z", 
    "summary": "We present a fast randomized algorithm that computes a low rank LU\ndecomposition. Our algorithm uses random projections type techniques to\nefficiently compute a low rank approximation of large matrices. The randomized\nLU algorithm can be parallelized and further accelerated by using sparse random\nmatrices in its projection step. Several different error bounds are proven for\nthe algorithm approximations. To prove these bounds, recent results from random\nmatrix theory related to subgaussian matrices are used. As an application, we\nalso show how the algorithm can be utilized to solve problems such as the\nrank-deficient least squares problem. Numerical examples, which illustrate the\nperformance of the algorithm and compare it to other decomposition methods, are\npresented."
},{
    "category": "math.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1310.7708v3", 
    "title": "Theoretical analysis of a Sinc-Nystr\u00f6m method for Volterra   integro-differential equations and its improvement", 
    "arxiv-id": "1310.7708v3", 
    "author": "Tomoaki Okayama", 
    "publish": "2013-10-29T08:32:32Z", 
    "summary": "A Sinc-Nystr\\\"om method for Volterra integro-differential equations was\ndeveloped by Zarebnia in 2010. This paper reinforces the method by presenting\ntwo theoretical results: 1) the regularity of the solution, which is required\nto implement the method, is analyzed, and 2) its convergence rate is rigorously\nanalyzed. Moreover, this paper improves the method so that a much higher\nconvergence rate can be attained, and theoretical results similar to those\nlisted above are provided. Numerical comparisons are also provided."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0219876213500710", 
    "link": "http://arxiv.org/pdf/1312.0707v2", 
    "title": "Data completion and stochastic algorithms for PDE inversion problems   with many measurements", 
    "arxiv-id": "1312.0707v2", 
    "author": "Uri Ascher", 
    "publish": "2013-12-03T06:05:01Z", 
    "summary": "Inverse problems involving systems of partial differential equations (PDEs)\nwith many measurements or experiments can be very expensive to solve\nnumerically. In a recent paper we examined dimensionality reduction methods,\nboth stochastic and deterministic, to reduce this computational burden,\nassuming that all experiments share the same set of receivers. In the present\narticle we consider the more general and practically important case where\nreceivers are not shared across experiments. We propose a data completion\napproach to alleviate this problem. This is done by means of an approximation\nusing an appropriately restricted gradient or Laplacian regularization,\nextending existing data for each experiment to the union of all receiver\nlocations. Results using the method of simultaneous sources (SS) with the\ncompleted data are then compared to those obtained by a more general but slower\nrandom subset (RS) method which requires no modifications."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jocs.2015.12.002", 
    "link": "http://arxiv.org/pdf/1312.2333v1", 
    "title": "Exploiting Data Representation for Fault Tolerance", 
    "arxiv-id": "1312.2333v1", 
    "author": "Frank Mueller", 
    "publish": "2013-12-09T08:11:12Z", 
    "summary": "We explore the link between data representation and soft errors in dot\nproducts. We present an analytic model for the absolute error introduced should\na soft error corrupt a bit in an IEEE-754 floating-point number. We show how\nthis finding relates to the fundamental linear algebra concepts of\nnormalization and matrix equilibration. We present a case study illustrating\nthat the probability of experiencing a large error in a dot product is\nminimized when both vectors are normalized. Furthermore, when data is\nnormalized we show that the absolute error is less than one or very large,\nwhich allows us to detect large errors. We demonstrate how this finding can be\nused by instrumenting the GMRES iterative solver. We count all possible errors\nthat can be introduced through faults in arithmetic in the computationally\nintensive orthogonalization phase, and show that when scaling is used the\nabsolute error can be bounded above by one."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TC.2014.2322593", 
    "link": "http://arxiv.org/pdf/1312.3300v1", 
    "title": "Numerical Reproducibility and Parallel Computations: Issues for Interval   Algorithms", 
    "arxiv-id": "1312.3300v1", 
    "author": "Philippe Th\u00e9veny", 
    "publish": "2013-12-11T20:09:33Z", 
    "summary": "What is called \"numerical reproducibility\" is the problem of getting the same\nresult when the scientific computation is run several times, either on the same\nmachine or on different machines, with different types and numbers of\nprocessing units, execution environments, computational loads etc. This problem\nis especially stringent for HPC numerical simulations. In what follows, the\nfocus is on parallel implementations of interval arithmetic using\nfloating-point arithmetic. For interval computations, numerical reproducibility\nis of course an issue for testing and debugging purposes. However, as long as\nthe computed result encloses the exact and unknown result, the inclusion\nproperty, which is the main property of interval arithmetic, is satisfied and\ngetting bit for bit identical results may not be crucial. Still, implementation\nissues may invalidate the inclusion property. Several ways to preserve the\ninclusion property are presented, on the example of the product of matrices\nwith interval coefficients."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TC.2014.2322593", 
    "link": "http://arxiv.org/pdf/1312.6872v1", 
    "title": "Matrix recovery using Split Bregman", 
    "arxiv-id": "1312.6872v1", 
    "author": "Angshul Majumdar", 
    "publish": "2013-12-17T15:12:48Z", 
    "summary": "In this paper we address the problem of recovering a matrix, with inherent\nlow rank structure, from its lower dimensional projections. This problem is\nfrequently encountered in wide range of areas including pattern recognition,\nwireless sensor networks, control systems, recommender systems, image/video\nreconstruction etc. Both in theory and practice, the most optimal way to solve\nthe low rank matrix recovery problem is via nuclear norm minimization. In this\npaper, we propose a Split Bregman algorithm for nuclear norm minimization. The\nuse of Bregman technique improves the convergence speed of our algorithm and\ngives a higher success rate. Also, the accuracy of reconstruction is much\nbetter even for cases where small number of linear measurements are available.\nOur claim is supported by empirical results obtained using our algorithm and\nits comparison to other existing methods for matrix recovery. The algorithms\nare compared on the basis of NMSE, execution time and success rate for varying\nranks and sampling ratios."
},{
    "category": "cs.NE", 
    "doi": "10.1109/TC.2014.2322593", 
    "link": "http://arxiv.org/pdf/1312.7852v1", 
    "title": "Evolutionary Design of Numerical Methods: Generating Finite Difference   and Integration Schemes by Differential Evolution", 
    "arxiv-id": "1312.7852v1", 
    "author": "P. M. A. Sloot", 
    "publish": "2013-12-30T20:21:44Z", 
    "summary": "Classical and new numerical schemes are generated using evolutionary\ncomputing. Differential Evolution is used to find the coefficients of finite\ndifference approximations of function derivatives, and of single and multi-step\nintegration methods. The coefficients are reverse engineered based on samples\nfrom a target function and its derivative used for training. The Runge-Kutta\nschemes are trained using the order condition equations. An appealing feature\nof the evolutionary method is the low number of model parameters. The\npopulation size, termination criterion and number of training points are\ndetermined in a sensitivity analysis. Computational results show good agreement\nbetween evolved and analytical coefficients. In particular, a new fifth-order\nRunge-Kutta scheme is computed which adheres to the order conditions with a sum\nof absolute errors of order 10^-14. Execution of the evolved schemes proved the\nintended orders of accuracy. The outcome of this study is valuable for future\ndevelopments in the design of complex numerical methods that are out of reach\nby conventional means."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TC.2014.2322593", 
    "link": "http://arxiv.org/pdf/1401.0159v1", 
    "title": "Speeding-Up Convergence via Sequential Subspace Optimization: Current   State and Future Directions", 
    "arxiv-id": "1401.0159v1", 
    "author": "Michael Zibulevsky", 
    "publish": "2013-12-31T15:25:50Z", 
    "summary": "This is an overview paper written in style of research proposal. In recent\nyears we introduced a general framework for large-scale unconstrained\noptimization -- Sequential Subspace Optimization (SESOP) and demonstrated its\nusefulness for sparsity-based signal/image denoising, deconvolution,\ncompressive sensing, computed tomography, diffraction imaging, support vector\nmachines. We explored its combination with Parallel Coordinate Descent and\nSeparable Surrogate Function methods, obtaining state of the art results in\nabove-mentioned areas. There are several methods, that are faster than plain\nSESOP under specific conditions: Trust region Newton method - for problems with\neasily invertible Hessian matrix; Truncated Newton method - when fast\nmultiplication by Hessian is available; Stochastic optimization methods - for\nproblems with large stochastic-type data; Multigrid methods - for problems with\nnested multilevel structure. Each of these methods can be further improved by\nmerge with SESOP. One can also accelerate Augmented Lagrangian method for\nconstrained optimization problems and Alternating Direction Method of\nMultipliers for problems with separable objective function and non-separable\nconstraints."
},{
    "category": "math.NA", 
    "doi": "10.1109/TC.2014.2322593", 
    "link": "http://arxiv.org/pdf/1401.1154v2", 
    "title": "Monte Carlo Computation of the Vassiliev knot invariant of degree 2 in   the integral representation", 
    "arxiv-id": "1401.1154v2", 
    "author": "Yani Zhao", 
    "publish": "2014-01-06T17:45:17Z", 
    "summary": "In mathematics there is a wide class of knot invariants that may be expressed\nin the form of multiple line integrals computed along the trajectory C\ndescribing the spatial conformation of the knot. In this work it is addressed\nthe problem of evaluating invariants of this kind in the case in which the knot\nis discrete, i.e. its trajectory is constructed by joining together a set of\nsegments of constant length. Such discrete knots appear almost everywhere in\nnumerical simulations of systems containing one dimensional ring-shaped\nobjects. Examples are polymers, the vortex lines in fluids and superfluids like\nhelium and other quantum liquids. Formally, the trajectory of a discrete knot\nis a piecewise smooth curve characterized by sharp corners at the joints\nbetween contiguous segments. The presence of these corners spoils the\ntopological invariance of the knot invariants considered here and prevents the\ncorrect evaluation of their values. To solve this problem, a smoothing\nprocedure is presented, which eliminates the sharp corners and transforms the\noriginal path C into a curve that is everywhere differentiable. The procedure\nis quite general and can be applied to any discrete knot defined off or on\nlattice. This smoothing algorithm is applied to the computation of the\nVassiliev knot invariant of degree 2 denoted here with the symbol r(C). This is\nthe simplest knot invariant that admits a definition in terms of multiple line\nintegrals. For a fast derivation of r(C), it is used a Monte Carlo integration\ntechnique. It is shown that, after the smoothing, the values of r(C) may be\nevaluated with an arbitrary precision. Several algorithms for the fast\ncomputation of the Vassiliev knot invariant of degree 2 are provided."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1401.2125v2", 
    "title": "Exponential-Krylov methods for ordinary differential equations", 
    "arxiv-id": "1401.2125v2", 
    "author": "Adrian Sandu", 
    "publish": "2014-01-09T19:17:40Z", 
    "summary": "This paper develops a new class of exponential-type integrators where all the\nmatrix exponentiations are performed in a single Krylov space of low dimension.\nThe new family, called Lightly Implicit Krylov-Exponential (LIKE), is well\nsuited for solving large scale systems of ODEs or semi-discrete PDEs. The time\ndiscretization and the Krylov space approximation are treated as a single\ncomputational process, and the Krylov space properties are an integral part of\nthe new LIKE order condition theory developed herein. Consequently, LIKE\nmethods require a small number of basis vectors determined solely by the\ntemporal order of accuracy. The subspace size is independent of the ODE under\nconsideration, and there is no need to monitor the errors in linear system\nsolutions at each stage. Numerical results illustrate the favorable properties\nof new family of methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1401.2844v1", 
    "title": "Inifnite hypercomplex number system factorization methods", 
    "arxiv-id": "1401.2844v1", 
    "author": "Iana V. Khitsko", 
    "publish": "2014-01-13T14:22:32Z", 
    "summary": "The method of obtaining the set of noncanonical hypercomplex number systems\nby conversion of infinite hypercomplex number system to finite hypercomplex\nnumber system depending on multiplication rules and factorization method is\ndescribed. Systems obtained by this method starting from the 3rddimension are\nnoncanonical. The obtained systems of even dimension can be re-factorized. As a\nresult of it hypercomplex number system of two times less dimension are got."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1401.5148v1", 
    "title": "Solving Cubic Equations By the Quadratic Formula", 
    "arxiv-id": "1401.5148v1", 
    "author": "Bahman Kalantari", 
    "publish": "2014-01-21T02:19:26Z", 
    "summary": "Let $p(z)$ be a monic cubic complex polynomial with distinct roots and\ndistinct critical points. We say a critical point has the {\\it Voronoi\nproperty} if it lies in the Voronoi cell of a root $\\theta$, $V(\\theta)$, i.e.\nthe set of points that are closer to $\\theta$ than to the other roots. We prove\nat least one critical point has the Voronoi property and characterize the cases\nwhen both satisfy this property. It is known that for any $\\xi \\in V(\\theta)$,\nthe sequence $B_m(\\xi) =\\xi - p(\\xi) d_{m-2}/d_{m-1}$ converges to $\\theta$,\nwhere $d_m$ satisfies the recurrence $d_m =p'(\\xi)d_{m-1}-0.5\np(\\xi)p''(\\xi)d_{m-2} +p^2(\\xi)d_{m-3}$, $d_0 =1, d_{-1}=d_{-2}=0$. Thus by the\nVoronoi property, there is a solution $c$ of $p'(z)=0$ where $B_m(c)$ converges\nto a root of $p(z)$. The speed of convergence is dependent on the ratio of the\ndistances between $c$ and the closest and the second closest roots of $p(z)$.\nThis results in a different algorithm for solving a cubic equation than the\nclassical methods. We give polynomiography for an example."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1401.5522v1", 
    "title": "Designing LU-QR hybrid solvers for performance and stability", 
    "arxiv-id": "1401.5522v1", 
    "author": "Jack Dongarra", 
    "publish": "2014-01-21T23:45:16Z", 
    "summary": "This paper introduces hybrid LU-QR al- gorithms for solving dense linear\nsystems of the form Ax = b. Throughout a matrix factorization, these al-\ngorithms dynamically alternate LU with local pivoting and QR elimination steps,\nbased upon some robustness criterion. LU elimination steps can be very\nefficiently parallelized, and are twice as cheap in terms of floating- point\noperations, as QR steps. However, LU steps are not necessarily stable, while QR\nsteps are always stable. The hybrid algorithms execute a QR step when a\nrobustness criterion detects some risk for instability, and they execute an LU\nstep otherwise. Ideally, the choice between LU and QR steps must have a small\ncomputational overhead and must provide a satisfactory level of stability with\nas few QR steps as possible. In this paper, we introduce several robustness\ncriteria and we establish upper bounds on the growth factor of the norm of the\nupdated matrix incurred by each of these criteria. In addition, we describe the\nimplementation of the hybrid algorithms through an exten- sion of the PaRSEC\nsoftware to allow for dynamic choices during execution. Finally, we analyze\nboth stability and performance results compared to state-of-the-art linear\nsolvers on parallel distributed multicore platforms."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1401.7227v3", 
    "title": "A Two-Level Variant of Additive Schwarz Preconditioning for Use in   Reservoir Simulation", 
    "arxiv-id": "1401.7227v3", 
    "author": "David Ponting", 
    "publish": "2014-01-28T15:43:58Z", 
    "summary": "The computation time for reservoir simulation is dominated by the linear\nsolver. The sets of linear equations which arise in reservoir simulation have\ntwo distinctive features: the problems are usually highly anisotropic, with a\ndominant vertical flow direction, and the commonly used fully implicit method\nrequires a simultaneous solution for pressure and saturation or molar\nconcentration variables. These variables behave quite differently, with the\npressure feeling long-range effects while the saturations vary locally. In this\npaper we review preconditioned iterative methods used for solving the linear\nsystem equations in reservoir simulation and their parallelisation. We then\npropose a variant of the classical additive Schwarz preconditioner designed to\nachieve better results on a large number of processors and discuss some\ndirections for future research."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.1649v1", 
    "title": "A GPU Accelerated Aggregation Algebraic Multigrid Method", 
    "arxiv-id": "1403.1649v1", 
    "author": "Yongpeng Zhang", 
    "publish": "2014-03-07T03:57:16Z", 
    "summary": "We present an efficient, robust and fully GPU-accelerated aggregation-based\nalgebraic multigrid preconditioning technique for the solution of large sparse\nlinear systems. These linear systems arise from the discretization of elliptic\nPDEs. The method involves two stages, setup and solve. In the setup stage,\nhierarchical coarse grids are constructed through aggregation of the fine grid\nnodes. These aggregations are obtained using a set of maximal independent nodes\nfrom the fine grid nodes. We use a ``fine-grain'' parallel algorithm for\nfinding a maximal independent set from a graph of strong negative connections.\nThe aggregations are combined with a piece-wise constant (unsmooth)\ninterpolation from the coarse grid solution to the fine grid solution, ensuring\nlow setup and interpolation cost. The grid independent convergence is achieved\nby using recursive Krylov iterations (K-cycles) in the solve stage. An\nefficient combination of K-cycles and standard multigrid V-cycles is used as\nthe preconditioner for Krylov iterative solvers such as generalized minimal\nresidual and conjugate gradient. We compare the solver performance with other\nsolvers based on smooth aggregation and classical algebraic multigrid methods."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.2294v1", 
    "title": "Non-linear mass-spring system for large soft tissue deformations   modeling", 
    "arxiv-id": "1403.2294v1", 
    "author": "Sergei Nikolaev", 
    "publish": "2014-03-10T16:34:40Z", 
    "summary": "Implant placement under soft tissues operation is described. In this\noperation tissues can reach such deformations that nonlinear properties are\nappeared. A mass-spring model modification for modeling nonlinear tissue\noperation is developed. A method for creating elasticity module using splines\nis described. For Poisson ratio different stiffness for different types of\nsprings in cubic grid is used. For stiffness finding an equation system that\ndescribed material tension is solved. The model is verified with quadratic\nsample tension experiment. These tests show that sample tension under external\nforces is equal to defined nonlinear elasticity module. The accuracy of Poisson\nratio modeling is thirty five percent that is better the results of available\nratio modeling method."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.2740v2", 
    "title": "A consistent model for cardiac deformation estimation under abnormal   ventricular muscle conditions", 
    "arxiv-id": "1403.2740v2", 
    "author": "Hamid Abrishami Moghaddam", 
    "publish": "2014-02-23T00:14:19Z", 
    "summary": "Deformation modeling of cardiac muscle is an important issue in the field of\ncardiac analysis. For this reason, many approaches have been developed to best\nestimate the cardiac muscle deformation, and to obtain a practical model to use\nin diagnostic procedures. But there are some conditions, like in case of\nmyocardial infarction, in which the regular modeling approaches are not useful.\nIn this section, using a point-wise approach in deformation estimation, we try\nto estimate the deformation under some abnormal conditions of cardiac muscle.\nFirst, the endocardial and epicardial contour points are ordered with respect\nto the center of gravity of endocardial contour and boundary point displacement\nvectors are extracted. Then to solve the governing equation of deformation,\nwhich is an elliptic equation, we apply boundary conditions in accordance with\nthe computed displacement vectors and then the Finite Element method (FEM) will\nbe used to solve the governing equation. Using obtained displacement field\nthrough the cardiac muscle, strain map is extracted to show the mechanical\nbehavior of cardiac muscle. To validate the proposed algorithm in case of\ninfracted muscle, a non-homogeneous ring is modeled using ANSYS under a uniform\ntime varying internal pressure, which is the case in real cardiac muscle\ndeformation and then the proposed algorithm implemented in MATLAB and the\nresults for such problem are extracted."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.3046v1", 
    "title": "Improvement of the monotonicity properties of the difference schemes by   building in them of the monotonizing operators", 
    "arxiv-id": "1403.3046v1", 
    "author": "Y. V. Troshchiev", 
    "publish": "2014-03-12T17:49:33Z", 
    "summary": "The method of monotonization of difference schemes is being considered in the\npaper. The method was earlier proposed by the author for stationary problems.\nIt is investigated in the paper more profoundly. The idea of the method is to\nbuild the monotonizing operators into the schemes so that the balance relations\nfrom point to point are not violated. Different monotonizing operators can be\nused to be installed in the schemes. Propositions concerning approximation and\nstability of the monotonized schemes are formulated and proved. Also a\nproposition significant for practical use of the schemes is formulated and\nproved. The idea is to use the monotonized schemes in the cases when the\nproposition conditions are fulfilled. The proposition is based on closeness of\nsolutions of the initial and auxiliary schemes. Constructions for solving of\ntime dependent problems are also written in the paper. One dimensional example\nand three-dimensional hydrodynamic example are considered. The method allows to\nconsiderably decrease value of calculations in many cases."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.4267v2", 
    "title": "Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit", 
    "arxiv-id": "1403.4267v2", 
    "author": "Laurent Daudet", 
    "publish": "2014-03-17T20:34:18Z", 
    "summary": "We investigate the methods that simultaneously enforce sparsity and low-rank\nstructure in a matrix as often employed for sparse phase retrieval problems or\nphase calibration problems in compressive sensing. We propose a new approach\nfor analyzing the trade off between the sparsity and low rank constraints in\nthese approaches which not only helps to provide guidelines to adjust the\nweights between the aforementioned constraints, but also enables new simulation\nstrategies for evaluating performance. We then provide simulation results for\nphase retrieval and phase calibration cases both to demonstrate the consistency\nof the proposed method with other approaches and to evaluate the change of\nperformance with different weights for the sparsity and low rank structure\nconstraints."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.08.013", 
    "link": "http://arxiv.org/pdf/1403.4428v3", 
    "title": "Two recursive GMRES-type methods for shifted linear systems with general   preconditioning", 
    "arxiv-id": "1403.4428v3", 
    "author": "Kirk M. Soodhalter", 
    "publish": "2014-03-18T12:20:45Z", 
    "summary": "We present two minimum residual methods for solving sequences of shifted\nlinear systems, the right-preconditioned shifted GMRES and shifted recycled\nGMRES algorithms which use a seed projection strategy often employed to solve\nmultiple related problems. These methods are compatible with general\npreconditioning of all systems, and when restricted to right preconditioning,\nrequire no extra applications of the operator or preconditioner. These seed\nprojection methods perform a minimum residual iteration for the base system\nwhile improving the approximations for the shifted systems at little additional\ncost. The iteration continues until the base system approximation is of\nsatisfactory quality. The method is then recursively called for the remaining\nunconverged systems. We present both methods inside of a general framework\nwhich allows these techniques to be extended to the setting of flexible\npreconditioning and inexact Krylov methods. We present some analysis of such\nmethods and numerical experiments demonstrating the effectiveness of the\nalgorithms we have derived."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2015.10.012", 
    "link": "http://arxiv.org/pdf/1403.5337v3", 
    "title": "A Fast Block Low-Rank Dense Solver with Applications to Finite-Element   Matrices", 
    "arxiv-id": "1403.5337v3", 
    "author": "Eric Darve", 
    "publish": "2014-03-21T01:07:26Z", 
    "summary": "This article presents a fast solver for the dense \"frontal\" matrices that\narise from the multifrontal sparse elimination process of 3D elliptic PDEs. The\nsolver relies on the fact that these matrices can be efficiently represented as\na hierarchically off-diagonal low-rank (HODLR) matrix. To construct the\nlow-rank approximation of the off-diagonal blocks, we propose a new\npseudo-skeleton scheme, the boundary distance low-rank approximation, that\npicks rows and columns based on the location of their corresponding vertices in\nthe sparse matrix graph. We compare this new low-rank approximation method to\nthe adaptive cross approximation (ACA) algorithm and show that it achieves\nbetters speedup specially for unstructured meshes. Using the HODLR direct\nsolver as a preconditioner (with a low tolerance) to the GMRES iterative\nscheme, we can reach machine accuracy much faster than a conventional LU\nsolver. Numerical benchmarks are provided for frontal matrices arising from 3D\nfinite element problems corresponding to a wide range of applications."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jcp.2015.10.012", 
    "link": "http://arxiv.org/pdf/1403.5805v1", 
    "title": "Parallel Implementations of the Jacobi Linear Algebraic Systems Solve", 
    "arxiv-id": "1403.5805v1", 
    "author": "Manos Roumeliotis", 
    "publish": "2014-03-23T21:41:37Z", 
    "summary": "The objective of this research is to construct parallel implementations of\nthe Jacobi algorithm used for the solution of linear algebraic systems, to\nmeasure their speedup with respect to the serial case and to compare each\nother, regarding their efficiency. The programming paradigm used in this\nimplementation is the message passing model, while, the used MPI implementation\nis the MPICH implementation of the Argonne National Laboratory."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2588555.2610519", 
    "link": "http://arxiv.org/pdf/1403.6968v2", 
    "title": "LINVIEW: Incremental View Maintenance for Complex Analytical Queries", 
    "arxiv-id": "1403.6968v2", 
    "author": "Christoph Koch", 
    "publish": "2014-03-27T10:22:32Z", 
    "summary": "Many analytics tasks and machine learning problems can be naturally expressed\nby iterative linear algebra programs. In this paper, we study the incremental\nview maintenance problem for such complex analytical queries. We develop a\nframework, called LINVIEW, for capturing deltas of linear algebra programs and\nunderstanding their computational cost. Linear algebra operations tend to cause\nan avalanche effect where even very local changes to the input matrices spread\nout and infect all of the intermediate results and the final view, causing\nincremental view maintenance to lose its performance benefit over\nre-evaluation. We develop techniques based on matrix factorizations to contain\nsuch epidemics of change. As a consequence, our techniques make incremental\nview maintenance of linear algebra practical and usually substantially cheaper\nthan re-evaluation. We show, both analytically and experimentally, the\nusefulness of these techniques when applied to standard analytics tasks. Our\nevaluation demonstrates the efficiency of LINVIEW in generating parallel\nincremental programs that outperform re-evaluation techniques by more than an\norder of magnitude."
},{
    "category": "cs.CE", 
    "doi": "10.1145/2588555.2610519", 
    "link": "http://arxiv.org/pdf/1405.0560v1", 
    "title": "Numerical Investigation of Effects of Compound Angle and Length to   Diameter Ratio on Adiabatic Film Cooling Effectiveness", 
    "arxiv-id": "1405.0560v1", 
    "author": "Ashish Garg", 
    "publish": "2014-05-03T08:15:32Z", 
    "summary": "A modification has been done in the normal injection hole of 35 degree, by\ninjecting the cold fluid at different angles(compound angle) in lateral\ndirection, providing a significant change in the shape of holes which later we\nfound in our numerical investigation giving good quality of effectiveness in\ncooling. Different L/D ratios are also studied for each compound angle. The\nnumerical simulation is performed based on Reynolds Averaged\nNavier-Stokes(RANS) equations with k-epsilon turbulence model by using\nFluent(Commercial Software). Adiabatic Film Cooling Effectiveness has been\nstudied for compound angles of (0, 30, 45 and 60 degrees) and L/D ratios of (1,\n2, 3 and 4) on a hole of 6mm diameter with blowing ratio 0.5. The findings are\nobtained from the results, concludes that the trend of laterally averaged\nadiabatic effectiveness is the function of L/D ratio and compound angle."
},{
    "category": "stat.CO", 
    "doi": "10.1145/2588555.2610519", 
    "link": "http://arxiv.org/pdf/1405.1250v1", 
    "title": "New tight approximations for Fisher's exact test", 
    "arxiv-id": "1405.1250v1", 
    "author": "Wilhelmiina H\u00e4m\u00e4l\u00e4inen", 
    "publish": "2014-05-06T12:58:39Z", 
    "summary": "Fisher's exact test is often a preferred method to estimate the significance\nof statistical dependence. However, in large data sets the test is usually too\nworksome to be applied, especially in an exhaustive search (data mining). The\ntraditional solution is to approximate the significance with the\n$\\chi^2$-measure, but the accuracy is often unacceptable. As a solution, we\nintroduce a family of upper bounds, which are fast to calculate and approximate\nFisher's $p$-value accurately. In addition, the new approximations are not\nsensitive to the data size, distribution, or smallest expected counts like the\n$\\chi^2$-based approximation. According to both theoretical and experimental\nanalysis, the new approximations produce accurate results for all sufficiently\nstrong dependencies. The basic form of the approximation can fail with weak\ndependencies, but the general form of the upper bounds can be adjusted to be\narbitrarily accurate."
},{
    "category": "cs.NA", 
    "doi": "10.1145/2588555.2610519", 
    "link": "http://arxiv.org/pdf/1405.2948v1", 
    "title": "Characterizing the Topography of Multi-dimensional Energy Landscapes", 
    "arxiv-id": "1405.2948v1", 
    "author": "John A. Scales", 
    "publish": "2014-02-02T22:54:51Z", 
    "summary": "A basic issue in optimization, inverse theory,neural networks, computational\nchemistry and many other problems is the geometrical characterization of high\ndimensional functions. In inverse calculations one aims to characterize the set\nof models that fit the data (among other constraints). If the data misfit\nfunction is unimodal then one can find its peak by local optimization methods\nand characterize its width (related to the range of data-fitting models) by\nestimating derivatives at this peak. On the other hand, if there are local\nextrema, then a number of interesting and difficult problems arise. Are the\nlocal extrema important compared to the global or can they be eliminated (e.g.,\nby smoothing) without significant loss of information? Is there a sufficiently\nsmall number of local extrema that they can be enumerated via local\noptimization? What are the basins of attraction of these local extrema? Can two\nextrema be joined by a path that never goes uphill? Can the whole problem be\nreduced to one of enumerating the local extrema and their basins of attraction?\nFor locally ill-conditioned functions, premature convergence of local\noptimization can be confused with the presence of local extrema. Addressing any\nof these issues requires topographic information about the functions under\nstudy. But in many applications these functions may have hundreds or thousands\nof variables and can only be evaluated pointwise (by some numerical method for\ninstance). In this paper we describe systematic (but generic) methods of\nanalysing the topography of high dimensional functions using local optimization\nmethods applied to randomly chosen starting models. We provide a number of\nquantitative measures of function topography that have proven to be useful in\npractical problems along with error estimates."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cma.2014.10.003", 
    "link": "http://arxiv.org/pdf/1405.3230v2", 
    "title": "A monolithic multi-time-step computational framework for first-order   transient systems with disparate scales", 
    "arxiv-id": "1405.3230v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2014-05-13T16:53:01Z", 
    "summary": "Developing robust simulation tools for problems involving multiple\nmathematical scales has been a subject of great interest in computational\nmathematics and engineering. A desirable feature to have in a numerical\nformulation for multiscale transient problems is to be able to employ different\ntime-steps (multi-time-step coupling), and different time integrators and\ndifferent numerical formulations (mixed methods) in different regions of the\ncomputational domain. We present two new monolithic multi-time-step mixed\ncoupling methods for first-order transient systems. We shall employ unsteady\nadvection-diffusion-reaction equation with linear decay as the model problem,\nwhich offers several unique challenges in terms of non-self-adjoint spatial\noperator and rich features in the solutions. We shall employ the dual Schur\ndomain decomposition technique to handle the decomposition of domain into\nsubdomains. Two different methods of enforcing compatibility along the\nsubdomain interface will be used in the time discrete setting. A systematic\ntheoretical analysis (which includes numerical stability, influence of\nperturbations, bounds on drift along the subdomain interface) will be\nperformed. The first coupling method ensures that there is no drift along the\nsubdomain interface but does not facilitate explicit/implicit coupling. The\nsecond coupling method allows explicit/implicit coupling with controlled (but\nnon-zero) drift in the solution along the subdomain interface. Several\ncanonical problems will be solved to numerically verify the theoretical\npredictions, and to illustrate the overall performance of the proposed coupling\nmethods. Finally, we shall illustrate the robustness of the proposed coupling\nmethods using a multi-time-step transient simulation of a fast bimolecular\nadvective-diffusive-reactive system."
},{
    "category": "cs.NA", 
    "doi": "10.5120/4709-6876", 
    "link": "http://arxiv.org/pdf/1405.3235v1", 
    "title": "An Alternating KMF Algorithm to Solve the Cauchy Problem for Laplaces   Equation", 
    "arxiv-id": "1405.3235v1", 
    "author": "Jaafar Abouchabaka", 
    "publish": "2014-05-13T17:19:13Z", 
    "summary": "This work concerns the use of the iterative algorithm (KMF algorithm)\nproposed by Kozlov, Mazya and Fomin to solve the Cauchy problem for Laplaces\nequation. This problem consists to recovering the lacking data on some part of\nthe boundary using the over specified conditions on the other part of the\nboundary. We describe an alternating formulation of the KMF algorithm and its\nrelationship with a classical formulation. The implementation of this algorithm\nfor a regular domain is performed by the finite element method using the\nsoftware Freefem. The numerical tests developed show the effectiveness of the\nproposed algorithm since it allows to have more accurate results as well as\nreducing the number of iterations needed for convergence."
},{
    "category": "cs.NA", 
    "doi": "10.5120/4709-6876", 
    "link": "http://arxiv.org/pdf/1405.3468v1", 
    "title": "An error estimate of Gaussian Recursive Filter in 3Dvar problem", 
    "arxiv-id": "1405.3468v1", 
    "author": "L. Marcellino", 
    "publish": "2014-05-14T12:29:28Z", 
    "summary": "Computational kernel of the three-dimensional variational data assimilation\n(3D-Var) problem is a linear system, generally solved by means of an iterative\nmethod. The most costly part of each iterative step is a matrix-vector product\nwith a very large covariance matrix having Gaussian correlation structure. This\noperation may be interpreted as a Gaussian convolution, that is a very\nexpensive numerical kernel. Recursive Filters (RFs) are a well known way to\napproximate the Gaussian convolution and are intensively applied in the\nmeteorology, in the oceanography and in forecast models. In this paper, we deal\nwith an oceanographic 3D-Var data assimilation scheme, named OceanVar, where\nthe linear system is solved by using the Conjugate Gradient (GC) method by\nreplacing, at each step, the Gaussian convolution with RFs. Here we give\ntheoretical issues on the discrete convolution approximation with a first order\n(1st-RF) and a third order (3rd-RF) recursive filters. Numerical experiments\nconfirm given error bounds and show the benefits, in terms of accuracy and\nperformance, of the 3-rd RF."
},{
    "category": "cs.MS", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1405.4644v1", 
    "title": "Changing Computing Paradigms Towards Power Efficiency", 
    "arxiv-id": "1405.4644v1", 
    "author": "Alessandro Curioni", 
    "publish": "2014-05-19T09:03:58Z", 
    "summary": "Power awareness is fast becoming immensely important in computing, ranging\nfrom the traditional High Performance Computing applications, to the new\ngeneration of data centric workloads.\n  In this work we describe our efforts towards a power efficient computing\nparadigm that combines low precision and high precision arithmetic. We showcase\nour ideas for the widely used kernel of solving systems of linear equations\nthat finds numerous applications in scientific and engineering disciplines as\nwell as in large scale data analytics, statistics and machine learning.\n  Towards this goal we developed tools for the seamless power profiling of\napplications at a fine grain level. In addition, we verify here previous work\non post FLOPS/Watt metrics and show that these can shed much more light in the\npower/energy profile of important applications."
},{
    "category": "cs.NA", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1405.6139v1", 
    "title": "Development of the method of computer analogy for studying and solving   complex nonlinear systems", 
    "arxiv-id": "1405.6139v1", 
    "author": "Andrey Stroganov", 
    "publish": "2014-05-11T18:01:37Z", 
    "summary": "A method of representation of a solution as segments of the series in powers\nof the step of the independent variable is expanded for solving complex systems\nof ordinary differential equations (ODE): the Lorenz system and other systems.\nA new procedure of reduction of the representation of the solution to a sum of\ntwo parts (regular and random) is performed. A shifting procedure is applied in\neach level of the independent variable to the random part and it acts as the\nfilter that extracts the values to the regular part. In certain cases it is\npossible to omit the random part and construct the approximation which does not\nconverge but still provides the qualitative information about the full solution\n(a linear approximation provides a simple exact solution). Evaluation of the\nerror for this case is performed. Constructing the analytical representation of\nthe solutions for these systems by the developed method is presented."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.1061v1", 
    "title": "Enhancing adaptive sparse grid approximations and improving refinement   strategies using adjoint-based a posteriori error estimates", 
    "arxiv-id": "1407.1061v1", 
    "author": "Timothy Wildey", 
    "publish": "2014-07-03T21:07:54Z", 
    "summary": "In this paper we present an algorithm for adaptive sparse grid approximations\nof quantities of interest computed from discretized partial differential\nequations. We use adjoint-based a posteriori error estimates of the physical\ndiscretization error and the interpolation error in the sparse grid to enhance\nthe sparse grid approximation and to drive adaptivity of the sparse grid.\nUtilizing these error estimates provides significantly more accurate functional\nvalues for random samples of the sparse grid approximation. We also demonstrate\nthat alternative refinement strategies based upon a posteriori error estimates\ncan lead to further increases in accuracy in the approximation over traditional\nhierarchical surplus based strategies. Throughout this paper we also provide\nand test a framework for balancing the physical discretization error with the\nstochastic interpolation error of the enhanced sparse grid approximation."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.1399v1", 
    "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", 
    "arxiv-id": "1407.1399v1", 
    "author": "James Cheng", 
    "publish": "2014-07-05T11:58:30Z", 
    "summary": "Higher-order tensors are becoming prevalent in many scientific areas such as\ncomputer vision, social network analysis, data mining and neuroscience.\nTraditional tensor decomposition approaches face three major challenges: model\nselecting, gross corruptions and computational efficiency. To address these\nproblems, we first propose a parallel trace norm regularized tensor\ndecomposition method, and formulate it as a convex optimization problem. This\nmethod does not require the rank of each mode to be specified beforehand, and\ncan automatically determine the number of factors in each mode through our\noptimization scheme. By considering the low-rank structure of the observed\ntensor, we analyze the equivalent relationship of the trace norm between a\nlow-rank tensor and its core tensor. Then, we cast a non-convex tensor\ndecomposition model into a weighted combination of multiple much smaller-scale\nmatrix trace norm minimization. Finally, we develop two parallel alternating\ndirection methods of multipliers (ADMM) to solve our problems. Experimental\nresults verify that our regularized formulation is effective, and our methods\nare robust to noise or outliers."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.1572v1", 
    "title": "The Inverse Fast Multipole Method", 
    "arxiv-id": "1407.1572v1", 
    "author": "Eric Darve", 
    "publish": "2014-07-07T03:16:40Z", 
    "summary": "This article introduces a new fast direct solver for linear systems arising\nout of wide range of applications, integral equations, multivariate statistics,\nradial basis interpolation, etc., to name a few. \\emph{The highlight of this\nnew fast direct solver is that the solver scales linearly in the number of\nunknowns in all dimensions.} The solver, termed as Inverse Fast Multipole\nMethod (abbreviated as IFMM), works on the same data-structure as the Fast\nMultipole Method (abbreviated as FMM). More generally, the solver can be\nimmediately extended to the class of hierarchical matrices, denoted as\n$\\mathcal{H}^2$ matrices with strong admissibility criteria (weak low-rank\nstructure), i.e., \\emph{the interaction between neighboring cluster of\nparticles is full-rank whereas the interaction between particles corresponding\nto well-separated clusters can be efficiently represented as a low-rank\nmatrix}. The algorithm departs from existing approaches in the fact that\nthroughout the algorithm the interaction corresponding to neighboring clusters\nare always treated as full-rank interactions. Our approach relies on two major\nideas: (i) The $N \\times N$ matrix arising out of FMM (from now on termed as\nFMM matrix) can be represented as an extended sparser matrix of size $M \\times\nM$, where $M \\approx 3N$. (ii) While solving the larger extended sparser\nmatrix, \\emph{the fill-in's that arise in the matrix blocks corresponding to\nwell-separated clusters are hierarchically compressed}. The ordering of the\nequations and the unknowns in the extended sparser matrix is strongly related\nto the local and multipole coefficients in the FMM~\\cite{greengard1987fast} and\n\\emph{the order of elimination is different from the usual nested dissection\napproach}. Numerical benchmarks on $2$D manifold confirm the linear scaling of\nthe algorithm."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.1593v2", 
    "title": "A Constructive Algorithm for Decomposing a Tensor into a Finite Sum of   Orthonormal Rank-1 Terms", 
    "arxiv-id": "1407.1593v2", 
    "author": "Ngai Wong", 
    "publish": "2014-07-07T06:42:57Z", 
    "summary": "We propose a constructive algorithm that decomposes an arbitrary real tensor\ninto a finite sum of orthonormal rank-1 outer products. The algorithm, named\nTTr1SVD, works by converting the tensor into a tensor-train rank-1 (TTr1)\nseries via the singular value decomposition (SVD). TTr1SVD naturally\ngeneralizes the SVD to the tensor regime with properties such as uniqueness for\na fixed order of indices, orthogonal rank-1 outer product terms, and easy\ntruncation error quantification. Using an outer product column table it also\nallows, for the first time, a complete characterization of all tensors\northogonal with the original tensor. Incidentally, this leads to a strikingly\nsimple constructive proof showing that the maximum rank of a real $2 \\times 2\n\\times 2$ tensor over the real field is 3. We also derive a conversion of the\nTTr1 decomposition into a Tucker decomposition with a sparse core tensor.\nNumerical examples illustrate each of the favorable properties of the TTr1\ndecomposition."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.2337v1", 
    "title": "High Order Implicit-Explicit General Linear Methods with Optimized   Stability Regions", 
    "arxiv-id": "1407.2337v1", 
    "author": "Sebastien Blaise", 
    "publish": "2014-07-09T02:21:46Z", 
    "summary": "In the numerical solution of partial differential equations using a\nmethod-of-lines approach, the availability of high order spatial discretization\nschemes motivates the development of sophisticated high order time integration\nmethods. For multiphysics problems with both stiff and non-stiff terms\nimplicit-explicit (IMEX) time stepping methods attempt to combine the lower\ncost advantage of explicit schemes with the favorable stability properties of\nimplicit schemes. Existing high order IMEX Runge Kutta or linear multistep\nmethods, however, suffer from accuracy or stability reduction.\n  This work shows that IMEX general linear methods (GLMs) are competitive\nalternatives to classic IMEX schemes for large problems arising in practice.\nHigh order IMEX-GLMs are constructed in the framework developed by the authors\n[34]. The stability regions of the new schemes are optimized numerically. The\nresulting IMEX-GLMs have similar stability properties as IMEX Runge-Kutta\nmethods, but they do not suffer from order reduction, and are superior in terms\nof accuracy and efficiency. Numerical experiments with two and three\ndimensional test problems illustrate the potential of the new schemes to speed\nup complex applications."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.2802v1", 
    "title": "Rigorous uniform approximation of D-finite functions using Chebyshev   expansions", 
    "arxiv-id": "1407.2802v1", 
    "author": "Marc Mezzarobba", 
    "publish": "2014-07-10T14:27:23Z", 
    "summary": "A wide range of numerical methods exists for computing polynomial\napproximations of solutions of ordinary differential equations based on\nChebyshev series expansions or Chebyshev interpolation polynomials. We consider\nthe application of such methods in the context of rigorous computing (where we\nneed guarantees on the accuracy of the result), and from the complexity point\nof view. It is well-known that the order-n truncation of the Chebyshev\nexpansion of a function over a given interval is a near-best uniform polynomial\napproximation of the function on that interval. In the case of solutions of\nlinear differential equations with polynomial coefficients, the coefficients of\nthe expansions obey linear recurrence relations with polynomial coefficients.\nUnfortunately, these recurrences do not lend themselves to a direct recursive\ncomputation of the coefficients, owing among other things to a lack of initial\nconditions. We show how they can nevertheless be used, as part of a validated\nprocess, to compute good uniform approximations of D-finite functions together\nwith rigorous error bounds, and we study the complexity of the resulting\nalgorithms. Our approach is based on a new view of a classical numerical method\ngoing back to Clenshaw, combined with a functional enclosure method."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2014.09.014", 
    "link": "http://arxiv.org/pdf/1407.3124v2", 
    "title": "Tensor Networks for Big Data Analytics and Large-Scale Optimization   Problems", 
    "arxiv-id": "1407.3124v2", 
    "author": "Andrzej Cichocki", 
    "publish": "2014-07-11T12:08:14Z", 
    "summary": "In this paper we review basic and emerging models and associated algorithms\nfor large-scale tensor networks, especially Tensor Train (TT) decompositions\nusing novel mathematical and graphical representations. We discus the concept\nof tensorization (i.e., creating very high-order tensors from lower-order\noriginal data) and super compression of data achieved via quantized tensor\ntrain (QTT) networks. The purpose of a tensorization and quantization is to\nachieve, via low-rank tensor approximations \"super\" compression, and\nmeaningful, compact representation of structured data. The main objective of\nthis paper is to show how tensor networks can be used to solve a wide class of\nbig data optimization problems (that are far from tractable by classical\nnumerical methods) by applying tensorization and performing all operations\nusing relatively small size matrices and tensors and applying iteratively\noptimized and approximative tensor contractions.\n  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product\nstates (MPS), matrix product operators (MPO), basic tensor operations,\ntensorization, distributed representation od data optimization problems for\nvery large-scale problems: generalized eigenvalue decomposition (GEVD),\nPCA/SVD, canonical correlation analysis (CCA)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cpc.2016.04.015", 
    "link": "http://arxiv.org/pdf/1407.3189v1", 
    "title": "A modern resistive magnetohydrodynamics solver using C++ and the Boost   library", 
    "arxiv-id": "1407.3189v1", 
    "author": "Lukas Einkemmer", 
    "publish": "2014-07-11T15:05:33Z", 
    "summary": "In this paper we describe the implementation of our C++ resistive\nmagnetohydrodynamics solver. The framework developed facilitates the separation\nof the code implementing the specific numerical method and the physical model,\non the one hand, from the handling of boundary conditions and the management of\nthe computational domain, on the other hand. In particular, this will allow us\nto use finite difference stencils which are only defined in the interior of the\ndomain (the boundary conditions are handled automatically). We will discuss\nthis and other design considerations and their impact on performance in some\ndetail. In addition, we provide a documentation of the code developed and\ndemonstrate that a performance comparable to Fortran can be achieved, while\nstill maintaining a maximum of code readability and extensibility."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2016.04.015", 
    "link": "http://arxiv.org/pdf/1407.5516v2", 
    "title": "A DEIM Induced CUR Factorization", 
    "arxiv-id": "1407.5516v2", 
    "author": "M. Embree", 
    "publish": "2014-07-21T14:48:27Z", 
    "summary": "We derive a CUR matrix factorization based on the Discrete Empirical\nInterpolation Method (DEIM). For a given matrix $A$, such a factorization\nprovides a low rank approximate decomposition of the form $A \\approx C U R$,\nwhere $C$ and $R$ are subsets of the columns and rows of $A$, and $U$ is\nconstructed to make $CUR$ a good approximation. Given a low-rank singular value\ndecomposition $A \\approx V S W^T$, the DEIM procedure uses $V$ and $W$ to\nselect the columns and rows of $A$ that form $C$ and $R$. Through an error\nanalysis applicable to a general class of CUR factorizations, we show that the\naccuracy tracks the optimal approximation error within a factor that depends on\nthe conditioning of submatrices of $V$ and $W$. For large-scale problems, $V$\nand $W$ can be approximated using an incremental QR algorithm that makes one\npass through $A$. Numerical examples illustrate the favorable performance of\nthe DEIM-CUR method, compared to CUR approximations based on leverage scores."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cpc.2016.04.015", 
    "link": "http://arxiv.org/pdf/1407.5593v1", 
    "title": "Deterministic Versus Randomized Kaczmarz Iterative Projection", 
    "arxiv-id": "1407.5593v1", 
    "author": "Ali Sekmen", 
    "publish": "2014-07-21T18:38:38Z", 
    "summary": "Kaczmarz's alternating projection method has been widely used for solving a\nconsistent (mostly over-determined) linear system of equations Ax=b. Because of\nits simple iterative nature with light computation, this method was\nsuccessfully applied in computerized tomography. Since tomography generates a\nmatrix A with highly coherent rows, randomized Kaczmarz algorithm is expected\nto provide faster convergence as it picks a row for each iteration at random,\nbased on a certain probability distribution. It was recently shown that picking\na row at random, proportional with its norm, makes the iteration converge\nexponentially in expectation with a decay constant that depends on the scaled\ncondition number of A and not the number of equations. Since Kaczmarz's method\nis a subspace projection method, the convergence rate for simple Kaczmarz\nalgorithm was developed in terms of subspace angles. This paper provides\nanalyses of simple and randomized Kaczmarz algorithms and explain the link\nbetween them. It also propose new versions of randomization that may speed up\nconvergence."
},{
    "category": "math.FA", 
    "doi": "10.1016/j.cpc.2016.04.015", 
    "link": "http://arxiv.org/pdf/1407.6202v2", 
    "title": "Estimates of the distance to the exact solution of parabolic problems   based on local Poincar\u00e9 type inequalities", 
    "arxiv-id": "1407.6202v2", 
    "author": "Sergey Repin", 
    "publish": "2014-07-23T13:16:38Z", 
    "summary": "The goal of the paper is to derive two-sided bounds of the distance between\nthe exact solution of the evolutionary reaction-diffusion problem with mixed\nDirichlet--Robin boundary conditions and any function in the admissible energy\nspace."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2015.02.025", 
    "link": "http://arxiv.org/pdf/1407.8093v1", 
    "title": "Enhancing $\\ell_1$-minimization estimates of polynomial chaos expansions   using basis selection", 
    "arxiv-id": "1407.8093v1", 
    "author": "Khachik Sargsyan", 
    "publish": "2014-07-30T15:32:28Z", 
    "summary": "In this paper we present a basis selection method that can be used with\n$\\ell_1$-minimization to adaptively determine the large coefficients of\npolynomial chaos expansions (PCE). The adaptive construction produces\nanisotropic basis sets that have more terms in important dimensions and limits\nthe number of unimportant terms that increase mutual coherence and thus degrade\nthe performance of $\\ell_1$-minimization. The important features and the\naccuracy of basis selection are demonstrated with a number of numerical\nexamples. Specifically, we show that for a given computational budget, basis\nselection produces a more accurate PCE than would be obtained if the basis is\nfixed a priori. We also demonstrate that basis selection can be applied with\nnon-uniform random variables and can leverage gradient information."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2015.02.025", 
    "link": "http://arxiv.org/pdf/1410.0642v1", 
    "title": "A Note on Archetypal Analysis and the Approximation of Convex Hulls", 
    "arxiv-id": "1410.0642v1", 
    "author": "Christian Bauckhage", 
    "publish": "2014-09-27T12:59:55Z", 
    "summary": "We briefly review the basic ideas behind archetypal analysis for matrix\nfactorization and discuss its behavior in approximating the convex hull of a\ndata sample. We then ask how good such approximations can be and consider\ndifferent cases. Understanding archetypal analysis as the problem of computing\na convexity constrained low-rank approximation of the identity matrix provides\nestimates for archetypal analysis and the SiVM heuristic."
},{
    "category": "cs.PF", 
    "doi": "10.1016/j.jcp.2015.02.025", 
    "link": "http://arxiv.org/pdf/1410.0804v3", 
    "title": "Multi-step Uniformization with Steady-State Detection in Nonstationary   M/M/s Queuing Systems", 
    "arxiv-id": "1410.0804v3", 
    "author": "Maciej Burak", 
    "publish": "2014-10-03T10:13:03Z", 
    "summary": "A new approach to the steady state detection in the uniformization method of\nsolving continuous time Markov chains is introduced. The method is particularly\nuseful in solving inhomogenous CTMC's in multiple steps, where the desired\nerror bound of the whole solution can be distributed not proportionally to the\nlengths of the respective intervals, but rather in a way, that maximizes the\nchances of detecting a steady state. Additionally, the convergence properties\nof the underlying DTMC are used to further enhance the computational savings\ndue to the steady state detection. The method is applied to the problem of\nmodeling a Call Center using inhomogenous CTMC model of a M(t)/M(t)/s(t)\nqueuing system."
},{
    "category": "math.NA", 
    "doi": "10.14495/jsiaml.6.81", 
    "link": "http://arxiv.org/pdf/1410.1599v1", 
    "title": "Accelerated Multiple Precision Matrix Multiplication using Strassen's   Algorithm and Winograd's Variant", 
    "arxiv-id": "1410.1599v1", 
    "author": "Tomonori Kouya", 
    "publish": "2014-10-07T02:13:43Z", 
    "summary": "The Strassen algorithm and Winograd's variant accelerate matrix\nmultiplication by using fewer arithmetic operations than standard matrix\nmultiplication. Although many papers have been published to accelerate single-\nas well as double-precision matrix multiplication by using these algorithms, no\nresearch to date has been undertaken to accelerate multiple precision matrix\nmultiplication. In this paper, we propose a multiple precision matrix\nmultiplication program for matrices of any size and test its performance. We\nalso reveal special properties of our program through its application to LU\ndecomposition."
},{
    "category": "cs.NA", 
    "doi": "10.14495/jsiaml.6.81", 
    "link": "http://arxiv.org/pdf/1410.2697v2", 
    "title": "A Fast and Memory Efficient Sparse Solver with Applications to   Finite-Element Matrices", 
    "arxiv-id": "1410.2697v2", 
    "author": "Eric Darve", 
    "publish": "2014-10-10T07:44:01Z", 
    "summary": "In this article, we introduce a fast and memory efficient solver for sparse\nmatrices arising from the finite element discretization of elliptic partial\ndifferential equations (PDEs). We use a fast direct (but approximate)\nmultifrontal solver as a preconditioner, and use an iterative solver to achieve\na desired accuracy. This approach combines the advantages of direct and\niterative schemes to arrive at a fast, robust and accurate solver. We will show\nthat this solver is faster ($\\sim$ 2x) and more memory efficient ($\\sim$ 2--3x)\nthan a conventional direct multifrontal solver. Furthermore, we will\ndemonstrate that the solver is both a faster and more effective preconditioner\nthan other preconditioners such as the incomplete LU preconditioner. Specific\nspeed-ups depend on the matrix size and improve as the size of the matrix\nincreases. The solver can be applied to both structured and unstructured meshes\nin a similar manner. We build on our previous work and utilize the fact that\ndense frontal and update matrices, in the multifrontal algorithm, can be\nrepresented as hierarchically off-diagonal low-rank (HODLR) matrices. Using\nthis idea, we replace all large dense matrix operations in the multifrontal\nelimination process with $O(N)$ HODLR operations to arrive at a faster and more\nmemory efficient solver."
},{
    "category": "cs.LG", 
    "doi": "10.14495/jsiaml.6.81", 
    "link": "http://arxiv.org/pdf/1410.2786v1", 
    "title": "New SVD based initialization strategy for Non-negative Matrix   Factorization", 
    "arxiv-id": "1410.2786v1", 
    "author": "Hanli Qiao", 
    "publish": "2014-10-10T13:56:58Z", 
    "summary": "There are two problems need to be dealt with for Non-negative Matrix\nFactorization (NMF): choose a suitable rank of the factorization and provide a\ngood initialization method for NMF algorithms. This paper aims to solve these\ntwo problems using Singular Value Decomposition (SVD). At first we extract the\nnumber of main components as the rank, actually this method is inspired from\n[1, 2]. Second, we use the singular value and its vectors to initialize NMF\nalgorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titled\nNNDSVD to enhance initialization of NMF algorithms. They extracted the positive\nsection and respective singular triplet information of the unit matrices\n{C(j)}k j=1 which were obtained from singular vector pairs. This strategy aims\nto use positive section to cope with negative elements of the singular vectors,\nbut in experiments we found that even replacing negative elements by their\nabsolute values could get better results than NNDSVD. Hence, we give another\nmethod based SVD to fulfil initialization for NMF algorithms (SVD-NMF).\nNumerical experiments on two face databases ORL and YALE [16, 17] show that our\nmethod is better than NNDSVD."
},{
    "category": "math.NA", 
    "doi": "10.14495/jsiaml.6.81", 
    "link": "http://arxiv.org/pdf/1410.4060v1", 
    "title": "Decoupling Multivariate Polynomials Using First-Order Information", 
    "arxiv-id": "1410.4060v1", 
    "author": "Johan Schoukens", 
    "publish": "2014-10-15T13:45:52Z", 
    "summary": "We present a method to decompose a set of multivariate real polynomials into\nlinear combinations of univariate polynomials in linear forms of the input\nvariables. The method proceeds by collecting the first-order information of the\npolynomials in a set of operating points, which is captured by the Jacobian\nmatrix evaluated at the operating points. The polyadic canonical decomposition\nof the three-way tensor of Jacobian matrices directly returns the unknown\nlinear relations, as well as the necessary information to reconstruct the\nunivariate polynomials. The conditions under which this decoupling procedure\nworks are discussed, and the method is illustrated on several numerical\nexamples."
},{
    "category": "math.NA", 
    "doi": "10.1007/s10107-015-0895-0", 
    "link": "http://arxiv.org/pdf/1410.4536v2", 
    "title": "Numerical Optimization for Symmetric Tensor Decomposition", 
    "arxiv-id": "1410.4536v2", 
    "author": "Tamara G. Kolda", 
    "publish": "2014-10-16T19:09:23Z", 
    "summary": "We consider the problem of decomposing a real-valued symmetric tensor as the\nsum of outer products of real-valued vectors. Algebraic methods exist for\ncomputing complex-valued decompositions of symmetric tensors, but here we focus\non real-valued decompositions, both unconstrained and nonnegative, for problems\nwith low-rank structure. We discuss when solutions exist and how to formulate\nthe mathematical program. Numerical results show the properties of the proposed\nformulations (including one that ignores symmetry) on a set of test problems\nand illustrate that these straightforward formulations can be effective even\nthough the problem is nonconvex."
},{
    "category": "nlin.CD", 
    "doi": "10.1007/s10107-015-0895-0", 
    "link": "http://arxiv.org/pdf/1410.4919v1", 
    "title": "On the relation between reliable computation time, float-point precision   and the Lyapunov exponent in chaotic systems", 
    "arxiv-id": "1410.4919v1", 
    "author": "JianPing Li", 
    "publish": "2014-10-18T08:35:14Z", 
    "summary": "The relation among reliable computation time, Tc, float-point precision, K,\nand the Lyapunov exponent, {\\lambda}, is obtained as Tc= (lnB/{\\lambda})K+C,\nwhere B is the base of the float-point system and C is a constant dependent\nonly on the chaotic equation. The equation shows good agreement with numerical\nexperimental results, especially the scale factors."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10107-015-0895-0", 
    "link": "http://arxiv.org/pdf/1410.5029v1", 
    "title": "Generalized Summation-by-Parts Operators for the Second Derivative with   Variable Coefficients", 
    "arxiv-id": "1410.5029v1", 
    "author": "David W. Zingg", 
    "publish": "2014-10-19T02:31:57Z", 
    "summary": "The comprehensive generalization of summation-by-parts of Del Rey Fern\\'andez\net al.\\ (J. Comput. Phys., 266, 2014) is extended to approximations of second\nderivatives with variable coefficients. This enables the construction of\nsecond-derivative operators with one or more of the following characteristics:\ni) non-repeating interior stencil, ii) nonuniform nodal distributions, and iii)\nexclusion of one or both boundary nodes. Definitions are proposed that give\nrise to generalized SBP operators that result in consistent, conservative, and\nstable discretizations of PDEs with or without mixed derivatives. It is proven\nthat such operators can be constructed using a correction to the application of\nthe first-derivative operator twice that is the same as used for the\nconstant-coefficient operator. Moreover, for operators with a repeating\ninterior stencil, a decomposition is proposed that makes the application of\nsuch operators particularly simple. A number of novel operators are\nconstructed, including operators on pseudo-spectral nodal distributions and\noperators that have a repeating interior stencil, but unequal nodal spacing\nnear boundaries. The various operators are compared to the application of the\nfirst-derivative operator twice in the context of the linear\nconvection-diffusion equation with constant and variable coefficients."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10107-015-0895-0", 
    "link": "http://arxiv.org/pdf/1410.5977v1", 
    "title": "Optimization-based smoothing algorithm for triangle meshes over   arbitrarily shaped domains", 
    "arxiv-id": "1410.5977v1", 
    "author": "Daniel Aubram", 
    "publish": "2014-10-22T10:03:48Z", 
    "summary": "This paper describes a node relocation algorithm based on nonlinear\noptimization which delivers excellent results for both unstructured and\nstructured plane triangle meshes over convex as well as non-convex domains with\nhigh curvature. The local optimization scheme is a damped Newton's method in\nwhich the gradient and Hessian of the objective function are evaluated exactly.\nThe algorithm has been developed in order to continuously rezone the mesh in\narbitrary Lagrangian-Eulerian (ALE) methods for large deformation penetration\nproblems, but it is also suitable for initial mesh improvement. Numerical\nexamples highlight the capabilities of the algorithm."
},{
    "category": "math.NA", 
    "doi": "10.1137/140983410", 
    "link": "http://arxiv.org/pdf/1410.6895v2", 
    "title": "Very Large-Scale Singular Value Decomposition Using Tensor Train   Networks", 
    "arxiv-id": "1410.6895v2", 
    "author": "Andrzej Cichocki", 
    "publish": "2014-10-25T07:47:00Z", 
    "summary": "We propose new algorithms for singular value decomposition (SVD) of very\nlarge-scale matrices based on a low-rank tensor approximation technique called\nthe tensor train (TT) format. The proposed algorithms can compute several\ndominant singular values and corresponding singular vectors for large-scale\nstructured matrices given in a TT format. The computational complexity of the\nproposed methods scales logarithmically with the matrix size under the\nassumption that both the matrix and the singular vectors admit low-rank TT\ndecompositions. The proposed methods, which are called the alternating least\nsquares for SVD (ALS-SVD) and modified alternating least squares for SVD\n(MALS-SVD), compute the left and right singular vectors approximately through\nblock TT decompositions. The very large-scale optimization problem is reduced\nto sequential small-scale optimization problems, and each core tensor of the\nblock TT decompositions can be updated by applying any standard optimization\nmethods. The optimal ranks of the block TT decompositions are determined\nadaptively during iteration process, so that we can achieve high approximation\naccuracy. Extensive numerical simulations are conducted for several types of\nTT-structured matrices such as Hilbert matrix, Toeplitz matrix, random matrix\nwith prescribed singular values, and tridiagonal matrix. The simulation results\ndemonstrate the effectiveness of the proposed methods compared with standard\nSVD algorithms and TT-based algorithms developed for symmetric eigenvalue\ndecomposition."
},{
    "category": "cs.MS", 
    "doi": "10.1137/140983410", 
    "link": "http://arxiv.org/pdf/1410.7176v2", 
    "title": "Efficient implementation of elementary functions in the medium-precision   range", 
    "arxiv-id": "1410.7176v2", 
    "author": "Fredrik Johansson", 
    "publish": "2014-10-27T10:35:42Z", 
    "summary": "We describe a new implementation of the elementary transcendental functions\nexp, sin, cos, log and atan for variable precision up to approximately 4096\nbits. Compared to the MPFR library, we achieve a maximum speedup ranging from a\nfactor 3 for cos to 30 for atan. Our implementation uses table-based argument\nreduction together with rectangular splitting to evaluate Taylor series. We\ncollect denominators to reduce the number of divisions in the Taylor series,\nand avoid overhead by doing all multiprecision arithmetic using the mpn layer\nof the GMP library. Our implementation provides rigorous error bounds."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.camwa.2012.12.006", 
    "link": "http://arxiv.org/pdf/1410.7254v1", 
    "title": "Optimization of the Multigrid-Convergence Rate on Semi-structured Meshes   by Local Fourier Analysis", 
    "arxiv-id": "1410.7254v1", 
    "author": "U. R\u00fcde", 
    "publish": "2014-10-27T14:30:19Z", 
    "summary": "In this paper a local Fourier analysis for multigrid methods on tetrahedral\ngrids is presented. Different smoothers for the discretization of the Laplace\noperator by linear finite elements on such grids are analyzed. A four-color\nsmoother is presented as an efficient choice for regular tetrahedral grids,\nwhereas line and plane relaxations are needed for poorly shaped tetrahedra. A\nnovel partitioning of the Fourier space is proposed to analyze the four-color\nsmoother. Numerical test calculations validate the theoretical predictions. A\nmultigrid method is constructed in a block-wise form, by using different\nsmoothers and different numbers of pre- and post-smoothing steps in each\ntetrahedron of the coarsest grid of the domain. Some numerical experiments are\npresented to illustrate the efficiency of this multigrid algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.camwa.2012.12.006", 
    "link": "http://arxiv.org/pdf/1410.8104v2", 
    "title": "Solving Stress and Compliance Constrained Volume Minimization using   Anisotropic Mesh Adaptation, the Method of Moving Asymptotes and a Global   p-norm", 
    "arxiv-id": "1410.8104v2", 
    "author": "Kristian Jensen", 
    "publish": "2014-10-28T09:24:21Z", 
    "summary": "The p-norm often used in stress constrained topology optimisation supposedly\nmimics a delta function and it is thus characterised by a small length scale\nand ideally one would also prefer to have the solid-void transition occur over\na small length scale, since the material in this transition does not have a\nclear physical interpretation. We propose to resolve these small length scales\nusing anisotropic mesh adaptation. We use the method of moving asymptotes with\ninterpolation of sensitivities, asymptotes and design variables between\niterations. We demonstrate this combination for the portal and L-bracket\nproblems with p=10, and we are able to investigate mesh dependence. Finally, we\nsuggest relaxing the L-bracket problem statement by introducing a rounded\ncorner."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-015-0067-6", 
    "link": "http://arxiv.org/pdf/1411.0583v5", 
    "title": "A Hitchhiker's Guide to Automatic Differentiation", 
    "arxiv-id": "1411.0583v5", 
    "author": "Philipp H. W. Hoffmann", 
    "publish": "2014-11-03T17:52:50Z", 
    "summary": "This article provides an overview of some of the mathematical principles of\nAutomatic Differentiation (AD). In particular, we summarise different\ndescriptions of the Forward Mode of AD, like the matrix-vector product based\napproach, the idea of lifting functions to the algebra of dual numbers, the\nmethod of Taylor series expansion on dual numbers and the application of the\npush-forward operator, and explain why they all reduce to the same actual chain\nof computations. We further give a short mathematical description of some\nmethods of higher-order Forward AD and, at the end of this paper, briefly\ndescribe the Reverse Mode of Automatic Differentiation."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.0814v1", 
    "title": "A random algorithm for low-rank decomposition of large-scale matrices   with missing entries", 
    "arxiv-id": "1411.0814v1", 
    "author": "Yiguang Liu", 
    "publish": "2014-11-04T07:43:15Z", 
    "summary": "A Random SubMatrix method (RSM) is proposed to calculate the low-rank\ndecomposition of large-scale matrices with known entry percentage \\rho. RSM is\nvery fast as the floating-point operations (flops) required are compared\nfavorably with the state-of-the-art algorithms. Meanwhile RSM is very\nmemory-saving. With known entries homogeneously distributed in the given\nmatrix, sub-matrices formed by known entries are randomly selected. According\nto the just proved theorem that subspace related to smaller singular values is\nless perturbed by noise, the null vectors or the right singular vectors\nassociated with the minor singular values are calculated for each submatrix.\nThe vectors are the null vectors of the corresponding submatrix in the ground\ntruth of the given large-scale matrix. If enough sub-matrices are randomly\nchosen, the low-rank decomposition is estimated. The experimental results on\nrandom synthetical matrices with sizes such as 131072X1024 and on real data\nsets indicate that RSM is much faster and memory-saving, and, meanwhile, has\nconsiderable high precision achieving or approximating to the best."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.1503v1", 
    "title": "Tensor Transpose and Its Properties", 
    "arxiv-id": "1411.1503v1", 
    "author": "Ran Pan", 
    "publish": "2014-11-06T05:38:02Z", 
    "summary": "Tensor transpose is a higher order generalization of matrix transpose. In\nthis paper, we use permutations and symmetry group to define? the tensor\ntranspose. Then we discuss the classification and composition of tensor\ntransposes. Properties of tensor transpose are studied in relation to tensor\nmultiplication, tensor eigenvalues, tensor decompositions and tensor rank."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.1972v1", 
    "title": "Better Late Than Never: Filling a Void in the History of Fast Matrix   Multiplication and Tensor Decompositions", 
    "arxiv-id": "1411.1972v1", 
    "author": "Victor Y. Pan", 
    "publish": "2014-11-06T17:09:03Z", 
    "summary": "Multilinear and tensor decompositions are a popular tool in linear and\nmultilinear algebra and have a wide range of important applications to modern\ncomputing. Our paper of 1972 presented the first nontrivial application of such\ndecompositions to fundamental matrix computations and was also a landmark in\nthe history of the acceleration of matrix multiplication. Published in 1972 in\nRussian, it has never been translated into English. It has been very rarely\ncited in the Western literature on matrix multiplication and never in the works\non multilinear and tensor decompositions. This motivates us to present its\ntranslation into English, together with our brief comments on its impact on the\ntwo fields."
},{
    "category": "math.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.2377v1", 
    "title": "A Highly Efficient Implementation of Multiple Precision Sparse   Matrix-Vector Multiplication and Its Application to Product-type Krylov   Subspace Methods", 
    "arxiv-id": "1411.2377v1", 
    "author": "Tomonori Kouya", 
    "publish": "2014-11-10T10:58:18Z", 
    "summary": "We evaluate the performance of the Krylov subspace method by using highly\nefficient multiple precision sparse matrix-vector multiplication (SpMV).\nBNCpack is our multiple precision numerical computation library based on\nMPFR/GMP, which is one of the most efficient arbitrary precision floating-point\narithmetic libraries. However, it does not include functions that can\nmanipulate multiple precision sparse matrices. Therefore, by using benchmark\ntests, we show that SpMV implemented in these functions can be more efficient.\nFinally, we also show that product-type Krylov subspace methods such as BiCG\nand GPBiCG in which we have embedded SpMV, can efficiently solve large-scale\nlinear systems of equations provided in the UF sparse matrix collections in a\nmemory-restricted computing environment."
},{
    "category": "math.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.4324v2", 
    "title": "Fast algorithms for Higher-order Singular Value Decomposition from   incomplete data", 
    "arxiv-id": "1411.4324v2", 
    "author": "Yangyang Xu", 
    "publish": "2014-11-16T23:26:22Z", 
    "summary": "Higher-order singular value decomposition (HOSVD) is an efficient way for\ndata reduction and also eliciting intrinsic structure of multi-dimensional\narray data. It has been used in many applications, and some of them involve\nincomplete data. To obtain HOSVD of the data with missing values, one can first\nimpute the missing entries through a certain tensor completion method and then\nperform HOSVD to the reconstructed data. However, the two-step procedure can be\ninefficient and does not make reliable decomposition.\n  In this paper, we formulate an incomplete HOSVD problem and combine the two\nsteps into solving a single optimization problem, which simultaneously achieves\nimputation of missing values and also tensor decomposition. We also present two\nalgorithms for solving the problem based on block coordinate update. Global\nconvergence of both algorithms is shown under mild assumptions. The convergence\nof the second algorithm implies that of the popular higher-order orthogonality\niteration (HOOI) method, and thus we, for the first time, give global\nconvergence of HOOI.\n  In addition, we compare the proposed methods to state-of-the-art ones for\nsolving incomplete HOSVD and also low-rank tensor completion problems and\ndemonstrate the superior performance of our methods over other compared ones.\nFurthermore, we apply them to face recognition and MRI image reconstruction to\nshow their practical performance."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TIP.2015.2458176", 
    "link": "http://arxiv.org/pdf/1411.6296v2", 
    "title": "Approximating Matrices with Multiple Symmetries", 
    "arxiv-id": "1411.6296v2", 
    "author": "Joseph Vokt", 
    "publish": "2014-11-23T20:46:19Z", 
    "summary": "If a tensor with various symmetries is properly unfolded, then the resulting\nmatrix inherits those symmetries. As tensor computations become increasingly\nimportant it is imperative that we develop efficient structure preserving\nmethods for matrices with multiple symmetries. In this paper we consider how to\nexploit and preserve structure in the pivoted Cholesky factorization when\napproximating a matrix $A$ that is both symmetric ($A=A^T$) and what we call\n{\\em perfect shuffle symmetric}, or {\\em perf-symmetric}. The latter property\nmeans that $A = \\Pi A\\Pi$ where $\\Pi$ is a permutation with the property that\n$\\Pi v = v$ if $v$ is the vec of a symmetric matrix and $\\Pi v = -v$ if $v$ is\nthe vec of a skew-symmetric matrix. Matrices with this structure can arise when\nan order-4 tensor $\\cal A$ is unfolded and its elements satisfy ${\\cal\nA}(i_{1},i_{2},i_{3},i_{4}) = {\\cal A}(i_{2},i_{1},i_{3},i_{4}) ={\\cal\nA}(i_{1},i_{2},i_{4},i_{3}) ={\\cal A}(i_{3},i_{4},i_{1},i_{2}).$ This is the\ncase in certain quantum chemistry applications where the tensor entries are\nelectronic repulsion integrals. Our technique involves a closed-form block\ndiagonalization followed by one or two half-sized pivoted Cholesky\nfactorizations. This framework allows for a lazy evaluation feature that is\nimportant if the entries in $\\cal A$ are expensive to compute. In addition to\nbeing a structure preserving rank reduction technique, we find that this\napproach for obtaining the Cholesky factorization reduces the work by up to a\nfactor of 4."
},{
    "category": "math.NA", 
    "doi": "10.1080/00207160.2016.1247443", 
    "link": "http://arxiv.org/pdf/1411.7018v3", 
    "title": "A multigrid scheme for 3D Monge-Amp\u00e8re equations", 
    "arxiv-id": "1411.7018v3", 
    "author": "Mingqing Xiao", 
    "publish": "2014-11-25T20:47:00Z", 
    "summary": "The elliptic Monge-Amp\\`ere equation is a fully nonlinear partial\ndifferential equation which has been the focus of increasing attention from the\nscientific computing community. Fast three dimensional solvers are needed, for\nexample in medical image registration but are not yet available. We build fast\nsolvers for smooth solutions in three dimensions using a nonlinear\nfull-approximation storage multigrid method. Starting from a second-order\naccurate centered finite difference approximation, we present a nonlinear\nGauss-Seidel iterative method which has a mechanism for selecting the convex\nsolution of the equation. The iterative method is used as an effective\nsmoother, combined with the full-approximation storage multigrid method.\nNumerical experiments are provided to validate the accuracy of the finite\ndifference scheme and illustrate the computational efficiency of the proposed\nmultigrid solver."
},{
    "category": "cs.NA", 
    "doi": "10.1080/00207160.2016.1247443", 
    "link": "http://arxiv.org/pdf/1411.7663v1", 
    "title": "A Two Stage CVT / Eikonal Convection Mesh Deformation Approach for Large   Nodal Deformations", 
    "arxiv-id": "1411.7663v1", 
    "author": "Stephan Schmidt", 
    "publish": "2014-11-27T17:44:25Z", 
    "summary": "A two step mesh deformation approach for large nodal deformations, typically\narising from non-parametric shape optimization, fluid-structure interaction or\ncomputer graphics, is considered. Two major difficulties, collapsed cells and\nan undesirable parameterization, are overcome by considering a special form of\nray tracing paired with a centroid Voronoi reparameterization. The ray\ndirection is computed by solving an Eikonal equation. With respect to the\nHadamard form of the shape derivative, both steps are within the kernel of the\nobjective and have no negative impact on the minimizer. The paper concludes\nwith applications in 2D and 3D fluid dynamics and automatic code generation and\nmanages to solve these problems without any remeshing. The methodology is\navailable as a FEniCS shape optimization add-on at\nhttp://www.mathematik.uni-wuerzburg.de/~schmidt/femorph."
},{
    "category": "math.NA", 
    "doi": "10.1080/00207160.2016.1247443", 
    "link": "http://arxiv.org/pdf/1411.7801v4", 
    "title": "Stagnation of block GMRES and its relationship to block FOM", 
    "arxiv-id": "1411.7801v4", 
    "author": "Kirk M. Soodhalter", 
    "publish": "2014-11-28T10:27:42Z", 
    "summary": "We analyze the the convergence behavior of block GMRES and characterize the\nphenomenon of stagnation which is then related to the behavior of the block FOM\nmethod. We generalize the block FOM method to generate well-defined\napproximations in the case that block FOM would normally break down, and these\ngeneralized solutions are used in our analysis. This behavior is also related\nto the principal angles between the column-space of the previous block GMRES\nresidual and the current minimum residual constraint space. At iteration $j$,\nit is shown that the proper generalization of GMRES stagnation to the block\nsetting relates to the columnspace of the $j$th block Arnoldi vector. Our\nanalysis covers both the cases of normal iterations as well as block Arnoldi\nbreakdown wherein dependent basis vectors are replaced with random ones.\nNumerical examples are given to illustrate what we have proven, including a\nsmall application problem to demonstrate the validity of the analysis in a less\npathological case."
},{
    "category": "math.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.0393v2", 
    "title": "Block Krylov subspace recycling for shifted systems with unrelated   right-hand sides", 
    "arxiv-id": "1412.0393v2", 
    "author": "Kirk M. Soodhalter", 
    "publish": "2014-12-01T09:33:58Z", 
    "summary": "Many Krylov subspace methods for shifted linear systems take advantage of the\ninvariance of the Krylov subspace under a shift of the matrix. However,\nexploiting this fact in the non-Hermitian case introduces restrictions; e.g.,\ninitial residuals must be collinear and this collinearity must be maintained at\nrestart. Thus we cannot simultaneously solve shifted systems with unrelated\nright-hand sides using this strategy, and all shifted residuals cannot be\nsimultaneously minimized over a Krylov subspace such that collinearity is\nmaintained. It has been shown that this renders them generally incompatible\nwith techniques of subspace recycling [Soodhalter et al. APNUM '14].\n  This problem, however, can be overcome. By interpreting a family of shifted\nsystems as one Sylvester equation, we can take advantage of the known \"shift\ninvariance\" of the Krylov subspace generated by the Sylvester operator. Thus we\ncan simultaneously solve all systems over one block Krylov subspace using FOM\nor GMRES type methods, even when they have unrelated right-hand sides. Because\nresidual collinearity is no longer a requirement at restart, these methods are\nfully compatible with subspace recycling techniques. Furthermore, we realize\nthe benefits of block sparse matrix operations which arise in the context of\nhigh-performance computing applications.\n  In this paper, we discuss exploiting this Sylvester equation point of view\nwhich has yielded methods for shifted systems which are compatible with\nunrelated right-hand sides. From this, we propose a recycled GMRES method for\nsimultaneous solution of shifted systems.Numerical experiments demonstrate the\neffectiveness of the methods."
},{
    "category": "math.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.1687v1", 
    "title": "The Approximate Bilinear Algorithm of Length 46 for Multiplication of 4   x 4 Matrices", 
    "arxiv-id": "1412.1687v1", 
    "author": "A. V. Smirnov", 
    "publish": "2014-11-20T15:30:47Z", 
    "summary": "We propose the arbitrary precision approximate (APA) bilinear algorithm of\nlength 46 for multiplication of 4 x 4 and 4 x 4 matrices. The algorithm has\npolynomial order 3 and 352 nonzero coefficients from total 2208."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.1885v2", 
    "title": "Decomposition of Big Tensors With Low Multilinear Rank", 
    "arxiv-id": "1412.1885v2", 
    "author": "Shengli Xie", 
    "publish": "2014-12-05T03:04:32Z", 
    "summary": "Tensor decompositions are promising tools for big data analytics as they\nbring multiple modes and aspects of data to a unified framework, which allows\nus to discover complex internal structures and correlations of data.\nUnfortunately most existing approaches are not designed to meet the major\nchallenges posed by big data analytics. This paper attempts to improve the\nscalability of tensor decompositions and provides two contributions: A flexible\nand fast algorithm for the CP decomposition (FFCP) of tensors based on their\nTucker compression; A distributed randomized Tucker decomposition approach for\narbitrarily big tensors but with relatively low multilinear rank. These two\nalgorithms can deal with huge tensors, even if they are dense. Extensive\nsimulations provide empirical evidence of the validity and efficiency of the\nproposed algorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.2700v2", 
    "title": "Subspace based low rank and joint sparse matrix recovery", 
    "arxiv-id": "1412.2700v2", 
    "author": "Mathews Jacob", 
    "publish": "2014-12-05T18:42:14Z", 
    "summary": "We consider the recovery of a low rank and jointly sparse matrix from under\nsampled measurements of its columns. This problem is highly relevant in the\nrecovery of dynamic MRI data with high spatio-temporal resolution, where each\ncolumn of the matrix corresponds to a frame in the image time series; the\nmatrix is highly low-rank since the frames are highly correlated. Similarly the\nnon-zero locations of the matrix in appropriate transform/frame domains (e.g.\nwavelet, gradient) are roughly the same in different frame. The superset of the\nsupport can be safely assumed to be jointly sparse. Unlike the classical\nmultiple measurement vector (MMV) setup that measures all the snapshots using\nthe same matrix, we consider each snapshot to be measured using a different\nmeasurement matrix. We show that this approach reduces the total number of\nmeasurements, especially when the rank of the matrix is much smaller than than\nits sparsity. Our experiments in the context of dynamic imaging shows that this\napproach is very useful in realizing free breathing cardiac MRI."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.3335v1", 
    "title": "Towards a Broader View of Theory of Computing", 
    "arxiv-id": "1412.3335v1", 
    "author": "Narendra Karmarkar", 
    "publish": "2014-12-10T15:16:23Z", 
    "summary": "Beginning with the projectively invariant method for linear programming,\ninterior point methods have led to powerful algorithms for many difficult\ncomputing problems, in combinatorial optimization, logic, number theory and\nnon-convex optimization. Algorithms for convex optimization benefitted from\nmany pre-established ideas from classical mathematics, but non-convex problems\nrequire new concepts. Lecture series I am presenting at the conference on\nFoundations of Computational Mathematics, 2014, outlines some of these\nconcepts{computational models based on the concept of the continuum, algorithms\ninvariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on\nco-ordinate representation, extended proof systems for more efficient\ncertificates of optimality, extensions of Grassmanns extension theory,\nefficient evaluation methods for the effect of exponential number of\nconstraints, theory of connected sets based on graded connectivity, theory of\ncurved spaces adapted to the problem data, and concept of relatively algebraic\nsets in curved space. Since this conference does not have a proceedings, the\npurpose of this article is to provide the material being presented at the\nconference in more widely accessible form."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.5706v1", 
    "title": "Numerical solution of nonstationary problems for a space-fractional   diffusion equation", 
    "arxiv-id": "1412.5706v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2014-12-18T02:26:59Z", 
    "summary": "An unsteady problem is considered for a space-fractional diffusion equation\nin a bounded domain. A first-order evolutionary equation containing a\nfractional power of an elliptic operator of second order is studied for general\nboundary conditions of Robin type. Finite element approximation in space is\nemployed. To construct approximation in time, regularized two-level schemes are\nused. The numerical implementation is based on solving the equation with the\nfractional power of the elliptic operator using an auxiliary Cauchy problem for\na pseudo-parabolic equation. The results of numerical experiments are presented\nfor a model two-dimensional problem."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.6291v1", 
    "title": "Perona-Malik equation and its numerical properties", 
    "arxiv-id": "1412.6291v1", 
    "author": "Maciek Wielgus", 
    "publish": "2014-12-19T11:17:38Z", 
    "summary": "This work concerns the Perona-Malik equation, which plays essential role in\nimage processing. The first part gives a survey of results on existance,\nuniqueness and stability of solutions, the second part introduces\ndiscretisations of equation and deals with an analysis of discrete problem. In\nthe last part I present some numerical results, in particular with algorithms\napplied to real images."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.6293v1", 
    "title": "Semi-Stochastic Coordinate Descent", 
    "arxiv-id": "1412.6293v1", 
    "author": "Peter Richt\u00e1rik", 
    "publish": "2014-12-19T11:20:13Z", 
    "summary": "We propose a novel stochastic gradient method---semi-stochastic coordinate\ndescent (S2CD)---for the problem of minimizing a strongly convex function\nrepresented as the average of a large number of smooth convex functions:\n$f(x)=\\tfrac{1}{n}\\sum_i f_i(x)$. Our method first performs a deterministic\nstep (computation of the gradient of $f$ at the starting point), followed by a\nlarge number of stochastic steps. The process is repeated a few times, with the\nlast stochastic iterate becoming the new starting point where the deterministic\nstep is taken. The novelty of our method is in how the stochastic steps are\nperformed. In each such step, we pick a random function $f_i$ and a random\ncoordinate $j$---both using nonuniform distributions---and update a single\ncoordinate of the decision vector only, based on the computation of the\n$j^{th}$ partial derivative of $f_i$ at two different points. Each random step\nof the method constitutes an unbiased estimate of the gradient of $f$ and\nmoreover, the squared norm of the steps goes to zero in expectation, meaning\nthat the stochastic estimate of the gradient progressively improves. The\ncomplexity of the method is the sum of two terms: $O(n\\log(1/\\epsilon))$\nevaluations of gradients $\\nabla f_i$ and $O(\\hat{\\kappa}\\log(1/\\epsilon))$\nevaluations of partial derivatives $\\nabla_j f_i$, where $\\hat{\\kappa}$ is a\nnovel condition number."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.7364v1", 
    "title": "Erasure coding for fault oblivious linear system solvers", 
    "arxiv-id": "1412.7364v1", 
    "author": "Yao Zhu", 
    "publish": "2014-12-23T14:04:34Z", 
    "summary": "Dealing with hardware and software faults is an important problem as parallel\nand distributed systems scale to millions of processing cores and wide area\nnetworks. Traditional methods for dealing with faults include\ncheckpoint-restart, active replicas, and deterministic replay. Each of these\ntechniques has associated resource overheads and constraints. In this paper, we\npropose an alternate approach to dealing with faults, based on input\naugmentation. This approach, which is an algorithmic analog of erasure coded\nstorage, applies a minimally modified algorithm on the augmented input to\nproduce an augmented output. The execution of such an algorithm proceeds\ncompletely oblivious to faults in the system. In the event of one or more\nfaults, the real solution is recovered using a rapid reconstruction method from\nthe augmented output. We demonstrate this approach on the problem of solving\nsparse linear systems using a conjugate gradient solver. We present input\naugmentation and output recovery techniques. Through detailed experiments, we\nshow that our approach can be made oblivious to a large number of faults with\nlow computational overhead. Specifically, we demonstrate cases where a single\nfault can be corrected with less than 10% overhead in time, and even in extreme\ncases (fault rates of 20%), our approach is able to compute a solution with\nreasonable overhead. These results represent a significant improvement over the\nstate of the art."
},{
    "category": "math.OC", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.8045v1", 
    "title": "Action constrained quasi-Newton methods", 
    "arxiv-id": "1412.8045v1", 
    "author": "Jacek Gondzio", 
    "publish": "2014-12-27T13:40:10Z", 
    "summary": "At the heart of Newton based optimization methods is a sequence of symmetric\nlinear systems. Each consecutive system in this sequence is similar to the\nnext, so solving them separately is a waste of computational effort. Here we\ndescribe automatic preconditioning techniques for iterative methods for solving\nsuch sequences of systems by maintaining an estimate of the inverse system\nmatrix. We update the estimate of the inverse system matrix with quasi-Newton\ntype formulas based on what we call an action constraint instead of the secant\nequation. We implement the estimated inverses as preconditioners in a Newton-CG\nmethod and prove quadratic termination. Our implementation is the first\nparallel quasi-Newton preconditioners, in full and limited memory variants.\nTests on logistic Support Vector Machine problems reveal that our method is\nvery efficient, converging in wall clock time before a Newton-CG method without\npreconditioning. Further tests on a set of classic test problems reveal that\nthe method is robust. The action constraint makes these updates flexible enough\nto mesh with trust-region and active set methods, a flexibility that is not\npresent in classic quasi-Newton methods."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.8185v1", 
    "title": "Generalized quaternions and their relations with Grassmann-Clifford   procedure of doubling", 
    "arxiv-id": "1412.8185v1", 
    "author": "Yana V. Khitsko", 
    "publish": "2014-12-28T16:44:30Z", 
    "summary": "The class of non-commutative hypercomplex number systems (HNS) of\n4-dimension, constructed by using of non-commutative Grassmann-Clifford\nprocedure of doubling of 2-dimensional systems is investigated in the article\nand established here are their relationships with the generalized quaternions.\nAlgorithms of performance of operations and methods of algebraic\ncharacteristics calculation in them, such as conjugation, normalization, a type\nof zero divisors are investigated. The considered arithmetic and algebraic\noperations and procedures in this class HNS allow to use these HNS in\nmathematical modeling."
},{
    "category": "math.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1412.8447v3", 
    "title": "Efficient Algorithms for CUR and Interpolative Matrix Decompositions", 
    "arxiv-id": "1412.8447v3", 
    "author": "Per-Gunnar Martinsson", 
    "publish": "2014-12-29T20:27:33Z", 
    "summary": "The manuscript describes efficient algorithms for the computation of the CUR\nand ID decompositions. The methods used are based on simple modifications to\nthe classical truncated pivoted QR decomposition, which means that highly\noptimized library codes can be utilized for implementation. For certain\napplications, further acceleration can be attained by incorporating techniques\nbased on randomized projections. Numerical experiments demonstrate advantageous\nperformance compared to existing techniques for computing CUR factorizations."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1502.01424v1", 
    "title": "Close Approximations for Daublets and their Spectra", 
    "arxiv-id": "1502.01424v1", 
    "author": "H. M. de Oliveira", 
    "publish": "2015-02-05T03:33:34Z", 
    "summary": "This paper offers a new regard on compactly supported wavelets derived from\nFIR filters. Although being continuous wavelets, analytical formulation are\nlacking for such wavelets. Close approximations for daublets (Daubechies\nwavelets) and their spectra are introduced here. The frequency detection\nproperties of daublets are investigated through scalograms derived from these\nnew analytical expressions. These near-daublets have been implemented on the\nMatlab wavelet toolbox and a few scalograms presented. This approach can be\nvaluable for wavelet synthesis from hardware or for application involving\ncontinuous wavelet-based systems, such as wavelet OFDM."
},{
    "category": "math.OC", 
    "doi": "10.1137/140998214", 
    "link": "http://arxiv.org/pdf/1502.02009v3", 
    "title": "A General Analysis of the Convergence of ADMM", 
    "arxiv-id": "1502.02009v3", 
    "author": "Michael I. Jordan", 
    "publish": "2015-02-06T20:01:58Z", 
    "summary": "We provide a new proof of the linear convergence of the alternating direction\nmethod of multipliers (ADMM) when one of the objective terms is strongly\nconvex. Our proof is based on a framework for analyzing optimization algorithms\nintroduced in Lessard et al. (2014), reducing algorithm convergence to\nverifying the stability of a dynamical system. This approach generalizes a\nnumber of existing results and obviates any assumptions about specific choices\nof algorithm parameters. On a numerical example, we demonstrate that minimizing\nthe derived bound on the convergence rate provides a practical approach to\nselecting algorithm parameters for particular ADMM instances. We complement our\nupper bound by constructing a nearly-matching lower bound on the worst-case\nrate of convergence."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00211-015-0697-6", 
    "link": "http://arxiv.org/pdf/1502.02280v1", 
    "title": "A comparison of the Extrapolated Successive Overrelaxation and the   Preconditioned Simultaneous Displacement methods for augmented linear systems", 
    "arxiv-id": "1502.02280v1", 
    "author": "N. M. Missirlis", 
    "publish": "2015-02-08T17:49:48Z", 
    "summary": "In this paper we study the impact of two types of preconditioning on the\nnumerical solution of large sparse augmented linear systems. The first\npreconditioning matrix is the lower triangular part whereas the second is the\nproduct of the lower triangular part with the upper triangular part of the\naugmented system's coefficient matrix. For the first preconditioning matrix we\nform the Generalized Modified Extrapolated Successive Overrelaxation (GMESOR)\nmethod, whereas the second preconditioning matrix yields the Generalized\nModified Preconditioned Simultaneous Displacement (GMPSD) method, which is an\nextrapolated form of the Symmetric Successive Overrelaxation method. We find\nsufficient conditions for each aforementioned iterative method to converge. In\naddition, we develop a geometric approach, for determining the optimum values\nof their parameters and corresponding spectral radii. It is shown that both\niterative methods studied (GMESOR and GMPSD) attain the same rate of\nconvergence. Numerical results confirm our theoretical expectations."
},{
    "category": "math.NT", 
    "doi": "10.1007/s00211-015-0697-6", 
    "link": "http://arxiv.org/pdf/1502.03371v1", 
    "title": "The Z Transform over Finite Fields", 
    "arxiv-id": "1502.03371v1", 
    "author": "D. Silva", 
    "publish": "2015-02-11T16:56:30Z", 
    "summary": "Finite field transforms have many applications and, in many cases, can be\nimplemented with a low computational complexity. In this paper, the Z Transform\nover a finite field is introduced and some of its properties are presented."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s00211-015-0697-6", 
    "link": "http://arxiv.org/pdf/1502.03543v1", 
    "title": "Primal Dual Affine Scaling on GPUs", 
    "arxiv-id": "1502.03543v1", 
    "author": "Nithish Divakar", 
    "publish": "2015-02-12T05:27:30Z", 
    "summary": "Here we present an implementation of Primal-Dual Affine scaling method to\nsolve linear optimization problem on GPU based systems. Strategies to convert\nthe system generated by complementary slackness theorem into a symmetric system\nare given. A new CUDA friendly technique to solve the resulting symmetric\npositive definite subsystem is also developed. Various strategies to reduce the\nmemory transfer and storage requirements were also explored."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s00211-015-0697-6", 
    "link": "http://arxiv.org/pdf/1502.04390v2", 
    "title": "Equilibrated adaptive learning rates for non-convex optimization", 
    "arxiv-id": "1502.04390v2", 
    "author": "Yoshua Bengio", 
    "publish": "2015-02-15T23:41:33Z", 
    "summary": "Parameter-specific adaptive learning rate methods are computationally\nefficient ways to reduce the ill-conditioning problems encountered when\ntraining large deep networks. Following recent work that strongly suggests that\nmost of the critical points encountered when training such networks are saddle\npoints, we find how considering the presence of negative eigenvalues of the\nHessian could help us design better suited adaptive learning rate schemes. We\nshow that the popular Jacobi preconditioner has undesirable behavior in the\npresence of both positive and negative curvature, and present theoretical and\nempirical evidence that the so-called equilibration preconditioner is\ncomparatively better suited to non-convex problems. We introduce a novel\nadaptive learning rate scheme, called ESGD, based on the equilibration\npreconditioner. Our experiments show that ESGD performs as well or better than\nRMSProp in terms of convergence speed, always clearly improving over plain\nstochastic gradient descent."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.05880v1", 
    "title": "A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast   Fourier and Hartley Transforms", 
    "arxiv-id": "1502.05880v1", 
    "author": "E. J. P. Santos", 
    "publish": "2015-02-20T14:14:50Z", 
    "summary": "This paper describes a flexible architecture for implementing a new fast\ncomputation of the discrete Fourier and Hartley transforms, which is based on a\nmatrix Laurent series. The device calculates the transforms based on a single\nbit selection operator. The hardware structure and synthesis are presented,\nwhich handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E\ndevice."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.06164v1", 
    "title": "On mesh restrictions to satisfy comparison principles, maximum   principles, and the non-negative constraint: Recent developments and new   results", 
    "arxiv-id": "1502.06164v1", 
    "author": "K. B. Nakshatrala", 
    "publish": "2015-02-22T01:42:34Z", 
    "summary": "This paper concerns with mesh restrictions that are needed to satisfy several\nimportant mathematical properties -- maximum principles, comparison principles,\nand the non-negative constraint -- for a general linear second-order elliptic\npartial differential equation. We critically review some recent developments in\nthe field of discrete maximum principles, derive new results, and discuss some\npossible future research directions in this area. In particular, we derive\nrestrictions for a three-node triangular (T3) element and a four-node\nquadrilateral (Q4) element to satisfy comparison principles, maximum\nprinciples, and the non-negative constraint under the standard single-field\nGalerkin formulation. Analysis is restricted to uniformly elliptic linear\ndifferential operators in divergence form with Dirichlet boundary conditions\nspecified on the entire boundary of the domain. Various versions of maximum\nprinciples and comparison principles are discussed in both continuous and\ndiscrete settings. In the literature, it is well-known that an acute-angled\ntriangle is sufficient to satisfy the discrete weak maximum principle for pure\nisotropic diffusion. An iterative algorithm is developed to construct\nsimplicial meshes that preserves discrete maximum principles using existing\nopen source mesh generators. Various numerical examples based on different\ntypes of triangulations are presented to show the pros and cons of placing\nrestrictions on a computational mesh. We also quantify local and global mass\nconservation errors using representative numerical examples, and illustrate the\nperformance of metric-based meshes with respect to mass conservation."
},{
    "category": "cs.CV", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.06220v2", 
    "title": "Boosting of Image Denoising Algorithms", 
    "arxiv-id": "1502.06220v2", 
    "author": "Michael Elad", 
    "publish": "2015-02-22T12:40:48Z", 
    "summary": "In this paper we propose a generic recursive algorithm for improving image\ndenoising methods. Given the initial denoised image, we suggest repeating the\nfollowing \"SOS\" procedure: (i) (S)trengthen the signal by adding the previous\ndenoised image to the degraded input image, (ii) (O)perate the denoising method\non the strengthened image, and (iii) (S)ubtract the previous denoised image\nfrom the restored signal-strengthened outcome. The convergence of this process\nis studied for the K-SVD image denoising and related algorithms. Still in the\ncontext of K-SVD image denoising, we introduce an interesting interpretation of\nthe SOS algorithm as a technique for closing the gap between the local\npatch-modeling and the global restoration task, thereby leading to improved\nperformance. In a quest for the theoretical origin of the SOS algorithm, we\nprovide a graph-based interpretation of our method, where the SOS recursive\nupdate effectively minimizes a penalty function that aims to denoise the image,\nwhile being regularized by the graph Laplacian. We demonstrate the SOS boosting\nalgorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL),\nshowing tendency to further improve denoising performance."
},{
    "category": "stat.CO", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.06777v2", 
    "title": "Statistical efficiency of structured cpd estimation applied to   Wiener-Hammerstein modeling", 
    "arxiv-id": "1502.06777v2", 
    "author": "Pierre Comon", 
    "publish": "2015-02-24T12:07:50Z", 
    "summary": "The computation of a structured canonical polyadic decomposition (CPD) is\nuseful to address several important modeling problems in real-world\napplications. In this paper, we consider the identification of a nonlinear\nsystem by means of a Wiener-Hammerstein model, assuming a high-order Volterra\nkernel of that system has been previously estimated. Such a kernel, viewed as a\ntensor, admits a CPD with banded circulant factors which comprise the model\nparameters. To estimate them, we formulate specialized estimators based on\nrecently proposed algorithms for the computation of structured CPDs. Then,\nconsidering the presence of additive white Gaussian noise, we derive a\nclosed-form expression for the Cramer-Rao bound (CRB) associated with this\nestimation problem. Finally, we assess the statistical performance of the\nproposed estimators via Monte Carlo simulations, by comparing their mean-square\nerror with the CRB."
},{
    "category": "math.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.07838v2", 
    "title": "Rectangular maximum-volume submatrices and their applications", 
    "arxiv-id": "1502.07838v2", 
    "author": "I. V. Oseledets", 
    "publish": "2015-02-27T09:29:07Z", 
    "summary": "A generalized and applicable to rectangular matrices definition of a volume\nof a matrix is given. Special case of this definition, so-called 2-volume, is\nstudied: we generalize results for square maximum-volume submatrices to the\ncase of rectangular maximal-volume submatrices and provide estimates for the\ngrowth of the coefficients and approximation error. Three promising\napplications of such submatrices are presented: recommender systems, finding\nmaximal elements in low-rank matrices and preconditioning of overdetermined\nlinear systems. The code is available online at\n\\url{https://bitbucket.org/muxas/maxvolpy}."
},{
    "category": "math.RA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1502.08014v3", 
    "title": "Localization theorems for matrices and bounds for the zeros of   polynomials over a quaternion division algebra", 
    "arxiv-id": "1502.08014v3", 
    "author": "Istkhar Ali", 
    "publish": "2015-01-17T06:58:38Z", 
    "summary": "In this paper, Ostrowski and Brauer type theorems are derived for the left\nand right eigenvalues of a quaternionic matrix. Generalizations of Gerschgorin\ntype theorems are discussed for the left and the right eigenvalues of a\nquaternionic matrix. Thereafter a sufficient condition for the stability of a\nquaternionic matrix is given that generalizes the stability condition for a\ncomplex matrix. Finally, a characterization of bounds for the zeros of\nquaternionic polynomials is presented."
},{
    "category": "math.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.01375v1", 
    "title": "Symmetric Orthogonal Tensor Decomposition is Trivial", 
    "arxiv-id": "1503.01375v1", 
    "author": "Tamara G. Kolda", 
    "publish": "2015-03-04T16:39:51Z", 
    "summary": "We consider the problem of decomposing a real-valued symmetric tensor as the\nsum of outer products of real-valued, pairwise orthogonal vectors. Such\ndecompositions do not generally exist, but we show that some symmetric tensor\ndecomposition problems can be converted to orthogonal problems following the\nwhitening procedure proposed by Anandkumar et al. (2012). If an orthogonal\ndecomposition of an $m$-way $n$-dimensional symmetric tensor exists, we propose\na novel method to compute it that reduces to an $n \\times n$ symmetric matrix\neigenproblem. We provide numerical results demonstrating the effectiveness of\nthe method."
},{
    "category": "math.OC", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.01889v1", 
    "title": "Parallelizing the dual revised simplex method", 
    "arxiv-id": "1503.01889v1", 
    "author": "J. A. J. Hall", 
    "publish": "2015-03-06T09:40:41Z", 
    "summary": "This paper introduces the design and implementation of two parallel dual\nsimplex solvers for general large scale sparse linear programming problems. One\napproach, called PAMI, extends a relatively unknown pivoting strategy called\nsuboptimization and exploits parallelism across multiple iterations. The other,\ncalled SIP, exploits purely single iteration parallelism by overlapping\ncomputational components when possible. Computational results show that the\nperformance of PAMI is superior to that of the leading open-source simplex\nsolver, and that SIP complements PAMI in achieving speedup when PAMI results in\nslowdown. One of the authors has implemented the techniques underlying PAMI\nwithin the FICO Xpress simplex solver and this paper presents computational\nresults demonstrating their value. This performance increase is sufficiently\nvaluable for the achievement to be used as the basis of promotional material by\nFICO. In developing the first parallel revised simplex solver of general\nutility and commercial importance, this work represents a significant\nachievement in computational optimization."
},{
    "category": "cs.LG", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.02828v2", 
    "title": "Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal   Riemannian Gradient", 
    "arxiv-id": "1503.02828v2", 
    "author": "Qinfeng Shi", 
    "publish": "2015-03-10T09:42:17Z", 
    "summary": "Nuclear-norm regularization plays a vital role in many learning tasks, such\nas low-rank matrix recovery (MR), and low-rank representation (LRR). Solving\nthis problem directly can be computationally expensive due to the unknown rank\nof variables or large-rank singular value decompositions (SVDs). To address\nthis, we propose a proximal Riemannian gradient (PRG) scheme which can\nefficiently solve trace-norm regularized problems defined on real-algebraic\nvariety $\\mMLr$ of real matrices of rank at most $r$. Based on PRG, we further\npresent a simple and novel subspace pursuit (SP) paradigm for general\ntrace-norm regularized problems without the explicit rank constraint $\\mMLr$.\nThe proposed paradigm is very scalable by avoiding large-rank SVDs. Empirical\nstudies on several tasks, such as matrix completion and LRR based subspace\nclustering, demonstrate the superiority of the proposed paradigms over existing\nmethods."
},{
    "category": "cs.CV", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.03004v3", 
    "title": "Fast and Robust Fixed-Rank Matrix Recovery", 
    "arxiv-id": "1503.03004v3", 
    "author": "Julio Guerrero", 
    "publish": "2015-03-10T17:35:46Z", 
    "summary": "We address the problem of efficient sparse fixed-rank (S-FR) matrix\ndecomposition, i.e., splitting a corrupted matrix $M$ into an uncorrupted\nmatrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rank\nconstraints are usually imposed by the physical restrictions of the system\nunder study. Here we propose a method to perform accurate and very efficient\nS-FR decomposition that is more suitable for large-scale problems than existing\napproaches. Our method is a grateful combination of geometrical and algebraical\ntechniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).\nInstead, a polar factorization is used to exploit the manifold structure of\nfixed-rank problems as the product of two Stiefel and an SPD manifold, leading\nto a better convergence and stability. Then, closed-form projectors help to\nspeed up each iteration of the method. We introduce a novel and fast projector\nfor the $\\text{SPD}$ manifold and a proof of its validity. Further acceleration\nis achieved using a Nystrom scheme. Extensive experiments with synthetic and\nreal data in the context of robust photometric stereo and spectral clustering\nshow that our proposals outperform the state of the art."
},{
    "category": "math.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.04500v4", 
    "title": "A Residual Based Sparse Approximate Inverse Preconditioning Procedure   for Large Sparse Linear Systems", 
    "arxiv-id": "1503.04500v4", 
    "author": "Wenjie Kang", 
    "publish": "2015-03-16T02:05:26Z", 
    "summary": "The SPAI algorithm, a sparse approximate inverse preconditioning technique\nfor large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci.\nComput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and\ncomputes a sparse approximate inverse $M$ of a large sparse matrix $A$\nadaptively. However, SPAI may be costly to seek the most profitable indices at\neach loop and $M$ may be ineffective for preconditioning. In this paper, we\npropose a residual based sparse approximate inverse preconditioning procedure\n(RSAI), which, unlike SPAI, is based on only the {\\em dominant} rather than all\ninformation on the current residual and augments sparsity patterns adaptively\nduring the loops. RSAI is less costly to seek indices and is more effective to\ncapture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control\nthe sparsity of $M$ and reduce computational cost, we develop a practical\nRSAI($tol$) algorithm that drops small nonzero entries adaptively during the\nprocess. Numerical experiments are reported to demonstrate that RSAI($tol$) is\nat least competitive with SPAI and can be considerably more efficient and\neffective than SPAI. They also indicate that RSAI($tol$) is comparable to the\nPSAI($tol$) algorithm proposed by one of the authors in 2009."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.06561v1", 
    "title": "A Comparative Analysis of Tensor Decomposition Models Using Hyper   Spectral Image", 
    "arxiv-id": "1503.06561v1", 
    "author": "Ashish Oberoi", 
    "publish": "2015-03-23T09:08:50Z", 
    "summary": "Hyper spectral imaging is a remote sensing technology, providing variety of\napplications such as material identification, space object identification,\nplanetary exploitation etc. It deals with capturing continuum of images of the\nearth surface from different angles. Due to the multidimensional nature of the\nimage, multi-way arrays are one of the possible solutions for analyzing hyper\nspectral data. This multi-way array is called tensor. Our approach deals with\nimplementing three decomposition models LMLRA, BTD and CPD to the sample data\nfor choosing the best decomposition of the data set. The results have proved\nthat Block Term Decomposition (BTD) is the best tensor model for decomposing\nthe hyper spectral image in to resultant factor matrices."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.08509v1", 
    "title": "A Finite Element Based P3M Method for N-body Problems", 
    "arxiv-id": "1503.08509v1", 
    "author": "Jonathan B. Freund", 
    "publish": "2015-03-30T00:12:51Z", 
    "summary": "We introduce a fast mesh-based method for computing N-body interactions that\nis both scalable and accurate. The method is founded on a\nparticle-particle--particle-mesh P3M approach, which decomposes a potential\ninto rapidly decaying short-range interactions and smooth, mesh-resolvable\nlong-range interactions. However, in contrast to the traditional approach of\nusing Gaussian screen functions to accomplish this decomposition, our method\nemploys specially designed polynomial bases to construct the screened\npotentials. Because of this form of the screen, the long-range component of the\npotential is then solved exactly with a finite element method, leading\nultimately to a sparse matrix problem that is solved efficiently with standard\nmultigrid methods. Moreover, since this system represents an exact\ndiscretization, the optimal resolution properties of the FFT are unnecessary,\nthough the short-range calculation is now more involved than P3M/PME methods.\nWe introduce the method, analyze its key properties, and demonstrate the\naccuracy of the algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.08601v2", 
    "title": "Finding a low-rank basis in a matrix subspace", 
    "arxiv-id": "1503.08601v2", 
    "author": "Andr\u00e9 Uschmajew", 
    "publish": "2015-03-30T09:04:40Z", 
    "summary": "For a given matrix subspace, how can we find a basis that consists of\nlow-rank matrices? This is a generalization of the sparse vector problem. It\nturns out that when the subspace is spanned by rank-1 matrices, the matrices\ncan be obtained by the tensor CP decomposition. For the higher rank case, the\nsituation is not as straightforward. In this work we present an algorithm based\non a greedy process applicable to higher rank problems. Our algorithm first\nestimates the minimum rank by applying soft singular value thresholding to a\nnuclear norm relaxation, and then computes a matrix with that rank using the\nmethod of alternating projections. We provide local convergence results, and\ncompare our algorithm with several alternative approaches. Applications include\ndata compression beyond the classical truncated SVD, computing accurate\neigenvectors of a near-multiple eigenvalue, image separation and graph\nLaplacian eigenproblems."
},{
    "category": "math.AP", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1503.09079v1", 
    "title": "Analytic solutions for the Burgers equation with source terms", 
    "arxiv-id": "1503.09079v1", 
    "author": "Gino I. Montecinos", 
    "publish": "2015-03-29T22:00:19Z", 
    "summary": "Analytic solutions for Burgers equations with source terms, possibly stiff,\nrepresent an important element to assess numerical schemes. Here we present a\nprocedure, based on the characteristic technique to obtain analytic solutions\nfor these equations with smooth initial conditions."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1504.00907v1", 
    "title": "The Discretely-Discontinuous Galerkin Coarse Grid for Domain   Decomposition", 
    "arxiv-id": "1504.00907v1", 
    "author": "Robert Bridson", 
    "publish": "2015-04-03T18:27:01Z", 
    "summary": "We present an algebraic method for constructing a highly effective coarse\ngrid correction to accelerate domain decomposition. The coarse problem is\nconstructed from the original matrix and a small set of input vectors that span\na low-degree polynomial space, but no further knowledge of meshes or continuous\nfunctionals is used. We construct a coarse basis by partitioning the problem\ninto subdomains and using the restriction of each input vector to each\nsubdomain as its own basis function. This basis resembles a Discontinuous\nGalerkin basis on subdomain-sized elements. Constructing the coarse problem by\nGalerkin projection, we prove a high-order convergent error bound for the\ncoarse solutions. Used in a two-level symmetric multiplicative overlapping\nSchwarz preconditioner, the resulting conjugate gradient solver shows optimal\nscaling. Convergence requires a constant number of iterations, independent of\nfine problem size, on a range of scalar and vector-valued second-order and\nfourth-order PDEs."
},{
    "category": "cs.NA", 
    "doi": "10.1109/SPL.2010.5483017", 
    "link": "http://arxiv.org/pdf/1504.01809v1", 
    "title": "Multi-Block ADMM for Big Data Optimization in Modern Communication   Networks", 
    "arxiv-id": "1504.01809v1", 
    "author": "Zhu Han", 
    "publish": "2015-04-08T02:48:37Z", 
    "summary": "In this paper, we review the parallel and distributed optimization algorithms\nbased on the alternating direction method of multipliers (ADMM) for solving\n\"big data\" optimization problems in modern communication networks. We first\nintroduce the canonical formulation of the large-scale optimization problem.\nNext, we describe the general form of ADMM and then focus on several direct\nextensions and sophisticated modifications of ADMM from $2$-block to $N$-block\nsettings to deal with the optimization problem. The iterative schemes and\nconvergence properties of each extension/modification are given, and the\nimplementation on large-scale computing facilities is also illustrated.\nFinally, we numerate several applications in communication networks, such as\nthe security constrained optimal power flow problem in smart grid networks and\nmobile data offloading problem in software defined networks (SDNs)."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.02214v1", 
    "title": "A deterministic sparse FFT algorithm for vectors with small support", 
    "arxiv-id": "1504.02214v1", 
    "author": "Katrin Wannenwetsch", 
    "publish": "2015-04-09T07:50:42Z", 
    "summary": "In this paper we consider the special case where a discrete signal ${\\bf x}$\nof length N is known to vanish outside a support interval of length $m < N$. If\nthe support length $m$ of ${\\bf x}$ or a good bound of it is a-priori known we\nderive a sublinear deterministic algorithm to compute ${\\bf x}$ from its\ndiscrete Fourier transform. In case of exact Fourier measurements we require\nonly ${\\cal O}(m \\log m)$ arithmetical operations. For noisy measurements, we\npropose a stable ${\\cal O}(m \\log N)$ algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.03749v3", 
    "title": "Galerkin v. least-squares Petrov--Galerkin projection in nonlinear model   reduction", 
    "arxiv-id": "1504.03749v3", 
    "author": "Harbir Antil", 
    "publish": "2015-04-15T00:19:15Z", 
    "summary": "Least-squares Petrov--Galerkin (LSPG) model-reduction techniques such as the\nGauss--Newton with Approximated Tensors (GNAT) method have shown promise, as\nthey have generated stable, accurate solutions for large-scale turbulent,\ncompressible flow problems where standard Galerkin techniques have failed.\nHowever, there has been limited comparative analysis of the two approaches.\nThis is due in part to difficulties arising from the fact that Galerkin\ntechniques perform optimal projection associated with residual minimization at\nthe time-continuous level, while LSPG techniques do so at the time-discrete\nlevel. This work provides a detailed theoretical and computational comparison\nof the two techniques for two common classes of time integrators: linear\nmultistep schemes and Runge--Kutta schemes. We present a number of new\nfindings, including conditions under which the LSPG ROM has a time-continuous\nrepresentation, conditions under which the two techniques are equivalent, and\ntime-discrete error bounds for the two approaches. Perhaps most surprisingly,\nwe demonstrate both theoretically and computationally that decreasing the time\nstep does not necessarily decrease the error for the LSPG ROM; instead, the\ntime step should be `matched' to the spectral content of the reduced basis. In\nnumerical experiments carried out on a turbulent compressible-flow problem with\nover one million unknowns, we show that increasing the time step to an\nintermediate value decreases both the error and the simulation time of the LSPG\nreduced-order model by an order of magnitude."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.04179v1", 
    "title": "Factorized schemes of second-order accuracy for numerical solving   unsteady problems", 
    "arxiv-id": "1504.04179v1", 
    "author": "P. N. Vabishchevich", 
    "publish": "2015-04-16T10:45:27Z", 
    "summary": "Schemes with the second-order approximation in time are considered for\nnumerical solving the Cauchy problem for an evolutionary equation of first\norder with a self-adjoint operator. The implicit two-level scheme based on the\nPad\\'{e} polynomial approximation is unconditionally stable. It demonstrates\ngood asymptotic properties in time and provides an adequate evolution in time\nfor individual harmonics of the solution (has spectral mimetic stability). In\nfact, the only drawback of this scheme is the necessity to solve an equation\nwith an operator polynomial of second degree at each time level. We consider\nmodifications of these schemes, which are based on solving equations with\noperator polynomials of first degree. Such computational implementations occur,\nfor example, if we apply the fully implicit two-level scheme (the backward\nEuler scheme). A three-level modification of the SM-stable scheme is proposed.\nIts unconditional stability is established in the corresponding norms. The\nemphasis is on the scheme, where the numerical algorithm involves two stages,\nnamely, the backward Euler scheme of first order at the first (prediction)\nstage and the following correction of the approximate solution using a\nfactorized operator. The SM-stability is established for the proposed scheme.\nTo illustrate the theoretical results of the work, a model problem is solved\nnumerically."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.04913v1", 
    "title": "Un An\u00e1lisis Comparativo de los M\u00e9todos Mim\u00e9ticos, Diferencias   Finitas y Elementos Finitos para problemas Estacionarios", 
    "arxiv-id": "1504.04913v1", 
    "author": "Giovanni Calder\u00f3n", 
    "publish": "2015-04-20T01:49:57Z", 
    "summary": "Numerical methods: mimetic finite differences and finite elements, are\nanalyzed from a numerical point of view. It seeks to conclude on the\nefficiency, order of convergence and computational cost of these methods. The\nanalysis is done in boundary value problems one-dimensional\n(convection-diffusion equation at steady) with different variations in the\ngradient, diffusion coefficient and convective velocity.\n  Key Words: Mimetics methods, Finite Element methods, Finite differences\nmethods, Conservative methods, Convergence."
},{
    "category": "math.CA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.05262v2", 
    "title": "Wavelets for Elliptical Waveguide Problems", 
    "arxiv-id": "1504.05262v2", 
    "author": "R. M. Campello de Souza", 
    "publish": "2015-04-20T23:48:18Z", 
    "summary": "New elliptic cylindrical wavelets are introduced, which exploit the\nrelationship between analysing filters and Floquet's solution of Mathieu\ndifferential equations. It is shown that the transfer function of both\nmultiresolution filters is related to the solution of a Mathieu equation of odd\ncharacteristic exponent. The number of notches of these analysing filters can\nbe easily designed. Wavelets derived by this method have potential application\nin the fields of optics, microwaves and electromagnetism."
},{
    "category": "math.OC", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.05400v2", 
    "title": "Ergodic convergence of a stochastic proximal point algorithm", 
    "arxiv-id": "1504.05400v2", 
    "author": "Pascal Bianchi", 
    "publish": "2015-04-21T12:19:57Z", 
    "summary": "The purpose of this paper is to establish the almost sure weak ergodic\nconvergence of a sequence of iterates $(x_n)$ given by $x_{n+1} = (I+\\lambda_n\nA(\\xi_{n+1},\\,.\\,))^{-1}(x_n)$ where $(A(s,\\,.\\,):s\\in E)$ is a collection of\nmaximal monotone operators on a separable Hilbert space, $(\\xi_n)$ is an\nindependent identically distributed sequence of random variables on $E$ and\n$(\\lambda_n)$ is a positive sequence in $\\ell^2\\backslash \\ell^1$. The weighted\naveraged sequence of iterates is shown to converge weakly to a zero (assumed to\nexist) of the Aumann expectation ${\\mathbb E}(A(\\xi_1,\\,.\\,))$ under the\nassumption that the latter is maximal. We consider applications to stochastic\noptimization problems of the form $\\min {\\mathbb E}(f(\\xi_1,x))$ w.r.t. $x\\in\n\\bigcap_{i=1}^m X_i$ where $f$ is a normal convex integrand and $(X_i)$ is a\ncollection of closed convex sets. In this case, the iterations are closely\nrelated to a stochastic proximal algorithm recently proposed by Wang and\nBertsekas."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-015-0028-0", 
    "link": "http://arxiv.org/pdf/1504.05705v1", 
    "title": "Convergence of a finite difference scheme to weak solutions of the   system of partial differential equation arising in mean field games", 
    "arxiv-id": "1504.05705v1", 
    "author": "Alessio Porretta", 
    "publish": "2015-04-22T09:36:54Z", 
    "summary": "Mean field type models describing the limiting behavior of stochastic\ndifferential games as the number of players tends to +$\\infty$, have been\nrecently introduced by J-M. Lasry and P-L. Lions. Under suitable assumptions,\nthey lead to a system of two coupled partial differential equations, a forward\nBellman equation and a backward Fokker-Planck equations. Finite difference\nschemes for the approximation of such systems have been proposed in previous\nworks. Here, we prove the convergence of these schemes towards a weak solution\nof the system of partial differential equations."
},{
    "category": "math.CA", 
    "doi": "10.14209/jcis.2004.2", 
    "link": "http://arxiv.org/pdf/1504.06106v1", 
    "title": "A Short Survey on Arithmetic Transforms and the Arithmetic Hartley   Transform", 
    "arxiv-id": "1504.06106v1", 
    "author": "H. M. de Oliveira", 
    "publish": "2015-04-23T09:37:28Z", 
    "summary": "Arithmetic complexity has a main role in the performance of algorithms for\nspectrum evaluation. Arithmetic transform theory offers a method for computing\ntrigonometrical transforms with minimal number of multiplications. In this\npaper, the proposed algorithms for the arithmetic Fourier transform are\nsurveyed. A new arithmetic transform for computing the discrete Hartley\ntransform is introduced: the Arithmetic Hartley transform. The interpolation\nprocess is shown to be the key element of the arithmetic transform theory."
},{
    "category": "cs.NA", 
    "doi": "10.14209/jcis.2004.2", 
    "link": "http://arxiv.org/pdf/1504.07791v2", 
    "title": "On the Global Convergence of Majorization Minimization Algorithms for   Nonconvex Optimization Problems", 
    "arxiv-id": "1504.07791v2", 
    "author": "Wu-Jun Li", 
    "publish": "2015-04-29T10:06:57Z", 
    "summary": "In this paper, we study the global convergence of majorization minimization\n(MM) algorithms for solving nonconvex regularized optimization problems. MM\nalgorithms have received great attention in machine learning. However, when\napplied to nonconvex optimization problems, the convergence of MM algorithms is\na challenging issue. We introduce theory of the Kurdyka- Lojasiewicz inequality\nto address this issue. In particular, we show that many nonconvex problems\nenjoy the Kurdyka- Lojasiewicz property and establish the global convergence\nresult of the corresponding MM procedure. We also extend our result to a well\nknown method that called CCCP (concave-convex procedure)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.01519v1", 
    "title": "Numerical investigation of a space-fractional model of turbulent fluid   flow in rectangular ducts", 
    "arxiv-id": "1505.01519v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2015-05-06T21:07:28Z", 
    "summary": "The models that are based of fractional derivatives should be highlighted\namong promising new models to describe turbulent fluid flows. In the present\nwork, a steady-state flow in a duct is considered under the condition that the\nturbulent diffusion is governed by a fractional power of the Laplace operator.\nTo study numerically flows in rectangular channels, finite-difference\napproximations are employed. For approximate solving the corresponding boundary\nvalue problem, the iterative method of conjugate gradients is used. At each\niteration, the problem with a fractional power of the grid Laplace operator is\nsolved. Predictions of turbulent flows in ducts at different Reynolds numbers\nare presented via mean velocity fields."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.01599v1", 
    "title": "Filter characteristics in image decomposition with singular spectrum   analysis", 
    "arxiv-id": "1505.01599v1", 
    "author": "Naoko Nose-Togawa", 
    "publish": "2015-05-07T06:21:15Z", 
    "summary": "Singular spectrum analysis is developed as a nonparametric spectral\ndecomposition of a time series. It can be easily extended to the decomposition\nof multidimensional lattice-like data through the filtering interpretation. In\nthis viewpoint, the singular spectrum analysis can be understood as the\nadaptive and optimal generation of the filters and their two-step\npoint-symmetric operation to the original data. In this paper, we point out\nthat, when applied to the multidimensional data, the adaptively generated\nfilters exhibit symmetry properties resulting from the bisymmetric nature of\nthe lag-covariance matrices. The eigenvectors of the lag-covariance matrix are\neither symmetric or antisymmetric, and for the 2D image data, these lead to the\ndifferential-type filters with even- or odd-order derivatives. The dominant\nfilter is a smoothing filter, reflecting the dominance of low-frequency\ncomponents of the photo images. The others are the edge-enhancement or the\nnoise filters corresponding to the band-pass or the high-pass filters. The\nimplication of the decomposition to the image denoising is briefly discussed."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.03445v1", 
    "title": "Symbolic-numeric methods for improving structural analysis of   differential-algebraic equation systems", 
    "arxiv-id": "1505.03445v1", 
    "author": "John D. Pryce", 
    "publish": "2015-05-13T16:11:42Z", 
    "summary": "Systems of differential-algebraic equations (DAEs) are generated routinely by\nsimulation and modeling environments such as Modelica and MapleSim. Before a\nsimulation starts and a numerical solution method is applied, some kind of\nstructural analysis is performed to determine the structure and the index of a\nDAE. Structural analysis methods serve as a necessary preprocessing stage, and\namong them, Pantelides's algorithm is widely used.\n  Recently Pryce's $\\Sigma$-method is becoming increasingly popular, owing to\nits straightforward approach and capability of analyzing high-order systems.\nBoth methods are equivalent in the sense that when one succeeds, producing a\nnonsingular system Jacobian, the other also succeeds, and the two give the same\nstructural index.\n  Although provably successful on fairly many problems of interest, the\nstructural analysis methods can fail on some simple, solvable DAEs and give\nincorrect structural information including the index. In this report, we focus\non the $\\Sigma$-method. We investigate its failures, and develop two\nsymbolic-numeric conversion methods for converting a DAE, on which the\n$\\Sigma$-method fails, to an equivalent problem on which this method succeeds.\nAimed at making structural analysis methods more reliable, our conversion\nmethods exploit structural information of a DAE, and require a symbolic tool\nfor their implementation."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.04103v1", 
    "title": "A splitting scheme to solve an equation for fractional powers of   elliptic operators", 
    "arxiv-id": "1505.04103v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2015-05-15T15:59:16Z", 
    "summary": "An equation containing a fractional power of an elliptic operator of second\norder is studied for Dirichlet boundary conditions. Finite difference\napproximations in space are employed. The proposed numerical algorithm is based\non solving an auxiliary Cauchy problem for a pseudo-parabolic equation.\nUnconditionally stable vector additive schemes (splitting schemes) are\nconstructed. Numerical results for a model problem in a rectangle calculated\nusing the splitting with respect to spatial variables are presented."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.04340v1", 
    "title": "Schur Complement based domain decomposition preconditioners with   Low-rank corrections", 
    "arxiv-id": "1505.04340v1", 
    "author": "Yousef Saad", 
    "publish": "2015-05-16T23:55:25Z", 
    "summary": "This paper introduces a robust preconditioner for general sparse symmetric\nmatrices, that is based on low-rank approximations of the Schur complement in a\nDomain Decomposition (DD) framework. In this \"Schur Low Rank\" (SLR)\npreconditioning approach, the coefficient matrix is first decoupled by DD, and\nthen a low-rank correction is exploited to compute an approximate inverse of\nthe Schur complement associated with the interface points. The method avoids\nexplicit formation of the Schur complement matrix. We show the feasibility of\nthis strategy for a model problem, and conduct a detailed spectral analysis for\nthe relationship between the low-rank correction and the quality of the\npreconditioning. Numerical experiments on general matrices illustrate the\nrobustness and efficiency of the proposed approach."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.06.009", 
    "link": "http://arxiv.org/pdf/1505.04341v2", 
    "title": "Low-rank correction methods for algebraic domain decomposition   preconditioners", 
    "arxiv-id": "1505.04341v2", 
    "author": "Yousef Saad", 
    "publish": "2015-05-16T23:56:32Z", 
    "summary": "This paper presents a parallel preconditioning method for distributed sparse\nlinear systems, based on an approximate inverse of the original matrix, that\nadopts a general framework of distributed sparse matrices and exploits the\ndomain decomposition method and low-rank corrections. The domain decomposition\napproach decouples the matrix and once inverted, a low-rank approximation is\napplied by exploiting the Sherman-Morrison-Woodbury formula, which yields two\nvariants of the preconditioning methods. The low-rank expansion is computed by\nthe Lanczos procedure with reorthogonalizations. Numerical experiments indicate\nthat, when combined with Krylov subspace accelerators, this preconditioner can\nbe efficient and robust for solving symmetric sparse linear systems.\nComparisons with other distributed-memory preconditioning methods are\npresented."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.02.040", 
    "link": "http://arxiv.org/pdf/1505.04515v1", 
    "title": "A Time-parallel Approach to Strong-constraint Four-dimensional   Variational Data Assimilation", 
    "arxiv-id": "1505.04515v1", 
    "author": "Adrian Sandu", 
    "publish": "2015-05-18T05:33:50Z", 
    "summary": "A parallel-in-time algorithm based on an augmented Lagrangian approach is\nproposed to solve four-dimensional variational (4D-Var) data assimilation\nproblems. The assimilation window is divided into multiple sub-intervals that\nallows to parallelize cost function and gradient computations. Solution\ncontinuity equations across interval boundaries are added as constraints. The\naugmented Lagrangian approach leads to a different formulation of the\nvariational data assimilation problem than weakly constrained 4D-Var. A\ncombination of serial and parallel 4D-Vars to increase performance is also\nexplored. The methodology is illustrated on data assimilation problems with\nLorenz-96 and the shallow water models."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.02.040", 
    "link": "http://arxiv.org/pdf/1505.04724v1", 
    "title": "A Hybrid Monte-Carlo Sampling Smoother for Four Dimensional Data   Assimilation", 
    "arxiv-id": "1505.04724v1", 
    "author": "Adrian Sandu", 
    "publish": "2015-05-18T17:15:49Z", 
    "summary": "This paper constructs an ensemble-based sampling smoother for\nfour-dimensional data assimilation using a Hybrid/Hamiltonian Monte-Carlo\napproach. The smoother samples efficiently from the posterior probability\ndensity of the solution at the initial time. Unlike the well-known ensemble\nKalman smoother, which is optimal only in the linear Gaussian case, the\nproposed methodology naturally accommodates non-Gaussian errors and non-linear\nmodel dynamics and observation operators. Unlike the four-dimensional\nvariational met\\-hod, which only finds a mode of the posterior distribution,\nthe smoother provides an estimate of the posterior uncertainty. One can use the\nensemble mean as the minimum variance estimate of the state, or can use the\nensemble in conjunction with the variational approach to estimate the\nbackground errors for subsequent assimilation windows. Numerical results\ndemonstrate the advantages of the proposed method compared to the traditional\nvariational and ensemble-based smoothing methods."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1505.05840v1", 
    "title": "Comparative Evaluation of Symmetric SVD Algorithms for Real-time Face   and Eye Tracking", 
    "arxiv-id": "1505.05840v1", 
    "author": "Bibek Kabi", 
    "publish": "2015-05-21T18:57:10Z", 
    "summary": "Computation of singular value decomposition (SVD) has been a topic of concern\nby many numerical linear algebra researchers. Fast SVD has been a very\neffective tool in computer vision in a number of aspects, such as: face\nrecognition, eye tracking etc. At the present state of the art fast and\nfixed-point power efficient SVD algorithm needs to be developed for real-time\nembedded computing. The work in this paper is the genesis of an attempt to\nbuild an on-board real-time face and eye tracking system for human drivers to\ndetect loss of attention due to drowsiness or fatigue. A major function of this\non-board system is quick customization. This is carried out when a new driver\ncomes in. The face and eye images are recorded while instructing the driver for\nmaking specific poses. The eigen faces and eigen eyes are generated at several\nresolution levels and stored in the on-board computer. The discriminating eigen\nspace of face and eyes are determined and stored in the on-board flash memory\nfor detection and tracking of face and eyes and classification of eyes (open or\nclosed) as well. Therefore, fast SVD of image covariance matrix at various\nlevels of resolution needs to be carried out to generate the eigen database. As\na preliminary step, we review the existing symmetric SVD algorithms and\nevaluate their feasibility for such an application. In this article, we compare\nthe performance of (1) Jacobi's, (2) Hestenes', (3) Golub-Kahan, (4)\nTridiagonalization and Symmetric QR iteration and (5) Tridiagonalization and\nDivide and Conquer algorithms. A case study has been demonstrated as an\nexample."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1505.06195v3", 
    "title": "Pivoted Cholesky decomposition by Cross Approximation for efficient   solution of kernel systems", 
    "arxiv-id": "1505.06195v3", 
    "author": "Hermann G. Matthies", 
    "publish": "2015-05-21T13:11:13Z", 
    "summary": "Large kernel systems are prone to be ill-conditioned. Pivoted Cholesky\ndecomposition (PCD) render a stable and efficient solution to the systems\nwithout a perturbation of regularization. This paper proposes a new PCD\nalgorithm by tuning Cross Approximation (CA) algorithm to kernel matrices which\nmerges the merits of PCD and CA, and proves as well as numerically exemplifies\nthat it solves large kernel systems two-order more efficiently than those\nresorts to regularization. As a by-product, a diagonal-pivoted CA technique is\nalso shown efficient in eigen-decomposition of large covariance matrices in an\nuncertainty quantification problem."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1505.06582v4", 
    "title": "Implementing 64-bit Maximally Equidistributed Mersenne Twisters", 
    "arxiv-id": "1505.06582v4", 
    "author": "Takamitsu Kimoto", 
    "publish": "2015-05-25T09:54:00Z", 
    "summary": "CPUs and operating systems are moving from 32 to 64 bits, and hence it is\nimportant to have good pseudorandom number generators designed to fully exploit\nthese word lengths. However, existing 64-bit very long period generators based\non linear recurrences modulo 2 are not completely optimized in terms of the\nequidistribution properties. Here we develop 64-bit maximally equidistributed\npseudorandom number generators that are optimal in this respect and have speeds\nequivalent to 64-bit Mersenne Twisters. We provide a table of specific\nparameters with period lengths from $2^{607}-1$ to $2^{44497}-1$. (An online\nappendix is available at http://www.ritsumei.ac.jp/~harase/memt-64-app.pdf)"
},{
    "category": "math.OC", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1505.07676v1", 
    "title": "MADMM: a generic algorithm for non-smooth optimization on manifolds", 
    "arxiv-id": "1505.07676v1", 
    "author": "Michael M. Bronstein", 
    "publish": "2015-05-28T12:44:25Z", 
    "summary": "Numerous problems in machine learning are formulated as optimization with\nmanifold constraints. In this paper, we propose the Manifold alternating\ndirections method of multipliers (MADMM), an extension of the classical ADMM\nscheme for manifold-constrained non-smooth optimization problems and show its\napplication to several challenging problems in dimensionality reduction, data\nanalysis, and manifold learning."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1505.08115v1", 
    "title": "Blocked rank-revealing QR factorizations: How randomized sampling can be   used to avoid single-vector pivoting", 
    "arxiv-id": "1505.08115v1", 
    "author": "P. G. Martinsson", 
    "publish": "2015-05-29T17:31:35Z", 
    "summary": "Given a matrix $A$ of size $m\\times n$, the manuscript describes a algorithm\nfor computing a QR factorization $AP=QR$ where $P$ is a permutation matrix, $Q$\nis orthonormal, and $R$ is upper triangular. The algorithm is blocked, to allow\nit to be implemented efficiently. The need for single vector pivoting in\nclassical algorithms for computing QR factorizations is avoided by the use of\nrandomized sampling to find blocks of pivot vectors at once. The advantage of\nblocking becomes particularly pronounced when $A$ is very large, and possibly\nstored out-of-core, or on a distributed memory machine. The manuscript also\ndescribes a generalization of the QR factorization that allows $P$ to be a\ngeneral orthonormal matrix. In this setting, one can at moderate cost compute a\n\\textit{rank-revealing} factorization where the mass of $R$ is concentrated to\nthe diagonal entries. Moreover, the diagonal entries of $R$ closely approximate\nthe singular values of $A$. The algorithms described have asymptotic flop count\n$O(m\\,n\\,\\min(m,n))$, just like classical deterministic methods. The scaling\nconstant is slightly higher than those of classical techniques, but this is\nmore than made up for by reduced communication and the ability to block the\ncomputation."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-30232-9_13", 
    "link": "http://arxiv.org/pdf/1506.00059v3", 
    "title": "Saddle-free Hessian-free Optimization", 
    "arxiv-id": "1506.00059v3", 
    "author": "Martin Arjovsky", 
    "publish": "2015-05-30T02:42:21Z", 
    "summary": "Nonconvex optimization problems such as the ones in training deep neural\nnetworks suffer from a phenomenon called saddle point proliferation. This means\nthat there are a vast number of high error saddle points present in the loss\nfunction. Second order methods have been tremendously successful and widely\nadopted in the convex optimization community, while their usefulness in deep\nlearning remains limited. This is due to two problems: computational complexity\nand the methods being driven towards the high error saddle points. We introduce\na novel algorithm specially designed to solve these two issues, providing a\ncrucial first step to take the widely known advantages of Newton's method to\nthe nonconvex optimization community, especially in high dimensional settings."
},{
    "category": "math.NA", 
    "doi": "10.5540/tema.2013.014.03.0319", 
    "link": "http://arxiv.org/pdf/1506.00289v1", 
    "title": "Semi-Discrete Formulations for 1D Burgers Equation", 
    "arxiv-id": "1506.00289v1", 
    "author": "Eliandro R. Cirilo", 
    "publish": "2015-05-31T20:53:57Z", 
    "summary": "In this work we compare semi-discrete formulations to obtain numerical\nsolutions for the 1D Burgers equation. The formulations consist in the\ndiscretization of the time-domain via multi-stage methods of second and fourth\norder: R_{11} and R_{22} Pad\\'e approximants, and of the spatial-domain via\nfinite element methods: least-squares (MEFMQ), Galerkin (MEFG) and\nStreamline-Upwind Petrov-Galerkin (SUPG). Knowing the analytical solutions of\nthe 1D Burgues equation, for different initial and boundary conditions,\nanalyzes were performed for numerical errors from L_{2} and L_{\\infinity} norm.\nWe found that the R_{22} Pad\\'e approximants, added to the MEFMQ, MEFG, and\nSUPG formulations, increased the region of convergence of the numerical\nsolutions, and showed greater accuracy when compared to the solutions obtained\nby the R_{11} Pad\\'e approximants. We note that the R_{22} Pad\\'e approximants\nsoftened the oscillations of the numerical solutions associated to the MEFG and\nSUPG formulations."
},{
    "category": "math.NA", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.01959v4", 
    "title": "Regularized Computation of Approximate Pseudoinverse of Large Matrices   Using Low-Rank Tensor Train Decompositions", 
    "arxiv-id": "1506.01959v4", 
    "author": "Andrzej Cichocki", 
    "publish": "2015-06-05T16:23:37Z", 
    "summary": "We propose a new method for low-rank approximation of Moore-Penrose\npseudoinverses (MPPs) of large-scale matrices using tensor networks. The\ncomputed pseudoinverses can be useful for solving or preconditioning of\nlarge-scale overdetermined or underdetermined systems of linear equations. The\ncomputation is performed efficiently and stably based on the modified\nalternating least squares (MALS) scheme using low-rank tensor train (TT)\ndecompositions and tensor network contractions. The formulated large-scale\noptimization problem is reduced to sequential smaller-scale problems for which\nany standard and stable algorithms can be applied. Regularization technique is\nincorporated in order to alleviate ill-posedness and obtain robust low-rank\napproximations. Numerical simulation results illustrate that the regularized\npseudoinverses of a wide class of non-square or nonsymmetric matrices admit\ngood approximate low-rank TT representations. Moreover, we demonstrated that\nthe computational cost of the proposed method is only logarithmic in the matrix\nsize given that the TT-ranks of a data matrix and its approximate pseudoinverse\nare bounded. It is illustrated that a strongly nonsymmetric\nconvection-diffusion problem can be efficiently solved by using the\npreconditioners computed by the proposed method."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.02649v1", 
    "title": "Faster SGD Using Sketched Conditioning", 
    "arxiv-id": "1506.02649v1", 
    "author": "Shai Shalev-Shwartz", 
    "publish": "2015-06-08T15:08:37Z", 
    "summary": "We propose a novel method for speeding up stochastic optimization algorithms\nvia sketching methods, which recently became a powerful tool for accelerating\nalgorithms for numerical linear algebra. We revisit the method of conditioning\nfor accelerating first-order methods and suggest the use of sketching methods\nfor constructing a cheap conditioner that attains a significant speedup with\nrespect to the Stochastic Gradient Descent (SGD) algorithm. While our\ntheoretical guarantees assume convexity, we discuss the applicability of our\nmethod to deep neural networks, and experimentally demonstrate its merits."
},{
    "category": "math.OC", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.02857v2", 
    "title": "Overapproximating the Reachable Values Set of Piecewise Affine Systems   Coupling Policy Iterations with Piecewise Quadratic Lyapunov Functions", 
    "arxiv-id": "1506.02857v2", 
    "author": "Assal\u00e9 Adj\u00e9", 
    "publish": "2015-06-09T10:47:54Z", 
    "summary": "We have recently constructed a piecewise quadratic Lyapunov function to prove\nthe boundedness of the reachable values set of piecewise affine discrete-time\nsystems. The method developed also provided an overapproximation of the\nreachable values set. In this paper, we refine the latter overapproximation\nextending previous works combining policy iterations with quadratic Lyapunov\nfunctions."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.03771v1", 
    "title": "Fast Methods for Eikonal Equations: an Experimental Survey", 
    "arxiv-id": "1506.03771v1", 
    "author": "Luis Moreno", 
    "publish": "2015-06-11T18:47:11Z", 
    "summary": "The Fast Marching Method is a very popular algorithm to compute\ntimes-of-arrival maps (distances map measured in time units). Since their\nproposal in 1995, it has been applied to many different applications such as\nrobotics, medical computer vision, fluid simulation, etc. Many alternatives\nhave been proposed with two main objectives: to reduce its computational time\nand to improve its accuracy. In this paper, we collect the main approaches\nwhich improve the computational time of the standard Fast Marching Method,\nfocusing on single-threaded methods and isotropic environments. 9 different\nmethods are studied under a common mathematical framework and experimentally in\nrepresentative environments: Fast Marching Method with binary heap, Fast\nMarching Method with Fibonacci Heap, Simplified Fast Marching Method, Untidy\nFast Marching Method, Fast Iterative Method, Group Marching Method, Fast\nSweeping Method, Lock Sweeping Method and Double Dynamic Queue Method."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.03914v1", 
    "title": "The Isogeometric Nystr\u00f6m Method", 
    "arxiv-id": "1506.03914v1", 
    "author": "Thomas-Peter Fries", 
    "publish": "2015-06-12T06:52:00Z", 
    "summary": "In this paper the isogeometric Nystr\\\"om method is presented. It's\noutstanding features are: it allows the analysis of domains described by many\ndifferent geometrical mapping methods in computer aided geometric design and it\nrequires only pointwise function evaluations just like isogeometric collocation\nmethods. The analysis of the computational domain is carried out by means of\nboundary integral equations, therefor only the boundary representation is\nrequired. The method is thoroughly integrated into the isogeometric framework.\nFor example, the regularization of the arising singular integrals performed\nwith local correction as well as the interpolation of the pointwise existing\nresults are carried out by means of Bezier elements.\n  The presented isogeometric Nystr\\\"om method is applied to practical problems\nsolved by the Laplace and the Lame-Navier equation. Numerical tests show higher\norder convergence in two and three dimensions. It is concluded that the\npresented approach provides a simple and flexible alternative to currently used\nmethods for solving boundary integral equations, but has some limitations."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1028479", 
    "link": "http://arxiv.org/pdf/1506.03963v1", 
    "title": "Efficient algorithm for computing large scale systems of differential   algebraic equations", 
    "arxiv-id": "1506.03963v1", 
    "author": "Peter Fritzson", 
    "publish": "2015-06-12T09:59:39Z", 
    "summary": "In many mathematical models of physical phenomenons and engineering fields,\nsuch as electrical circuits or mechanical multibody systems, which generate the\ndifferential algebraic equations (DAEs) systems naturally. In general, the\nfeature of DAEs is a sparse large scale system of fully nonlinear and high\nindex. To make use of its sparsity, this paper provides a simple and efficient\nalgorithm for computing the large scale DAEs system. We exploit the shortest\naugmenting path algorithm for finding maximum value transversal (MVT) as well\nas block triangular forms (BTF). We also present the extended signature matrix\nmethod with the block fixed point iteration and its complexity results.\nFurthermore, a range of nontrivial problems are demonstrated by our algorithm."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.04631v5", 
    "title": "Approximation with Random Bases: Pro et Contra", 
    "arxiv-id": "1506.04631v5", 
    "author": "Konstantin I. Sofeikov", 
    "publish": "2015-06-15T15:22:06Z", 
    "summary": "In this work we discuss the problem of selecting suitable approximators from\nfamilies of parameterized elementary functions that are known to be dense in a\nHilbert space of functions. We consider and analyze published procedures, both\nrandomized and deterministic, for selecting elements from these families that\nhave been shown to ensure the rate of convergence in $L_2$ norm of order\n$O(1/N)$, where $N$ is the number of elements. We show that both randomized and\ndeterministic procedures are successful if additional information about the\nfamilies of functions to be approximated is provided. In the absence of such\nadditional information one may observe exponential growth of the number of\nterms needed to approximate the function and/or extreme sensitivity of the\noutcome of the approximation to parameters. Implications of our analysis for\napplications of neural networks in modeling and control are illustrated with\nexamples."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.04971v1", 
    "title": "Tensor Deflation for CANDECOMP/PARAFAC. Part 3: Rank Splitting", 
    "arxiv-id": "1506.04971v1", 
    "author": "Andrzej Cichocki", 
    "publish": "2015-06-16T13:51:38Z", 
    "summary": "CANDECOMP/PARAFAC (CPD) approximates multiway data by sum of rank-1 tensors.\nOur recent study has presented a method to rank-1 tensor deflation, i.e.\nsequential extraction of the rank-1 components. In this paper, we extend the\nmethod to block deflation problem. When at least two factor matrices have full\ncolumn rank, one can extract two rank-1 tensors simultaneously, and rank of the\ndata tensor is reduced by 2. For decomposition of order-3 tensors of size R x R\nx R and rank-R, the block deflation has a complexity of O(R^3) per iteration\nwhich is lower than the cost O(R^4) of the ALS algorithm for the overall CPD."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.04972v2", 
    "title": "A Unified Successive Pseudo-Convex Approximation Framework", 
    "arxiv-id": "1506.04972v2", 
    "author": "Marius Pesavento", 
    "publish": "2015-06-16T13:54:00Z", 
    "summary": "In this paper, we propose a successive pseudo-convex approximation algorithm\nto efficiently compute stationary points for a large class of possibly\nnonconvex optimization problems. The stationary points are obtained by solving\na sequence of successively refined approximate problems, each of which is much\neasier to solve than the original problem. To achieve convergence, the\napproximate problem only needs to exhibit a weak form of convexity, namely,\npseudo-convexity. We show that the proposed framework not only includes as\nspecial cases a number of existing methods, for example, the gradient method\nand the Jacobi algorithm, but also leads to new algorithms which enjoy easier\nimplementation and faster convergence speed. We also propose a novel line\nsearch method for nondifferentiable optimization problems, which is carried out\nover a properly constructed differentiable function with the benefit of a\nsimplified implementation as compared to state-of-the-art line search\ntechniques that directly operate on the original nondifferentiable objective\nfunction. The advantages of the proposed algorithm are shown, both\ntheoretically and numerically, by several example applications, namely, MIMO\nbroadcast channel capacity computation, energy efficiency maximization in\nmassive MIMO systems and LASSO in sparse signal recovery."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.06099v2", 
    "title": "On enforcing maximum principles and achieving element-wise species   balance for advection-diffusion-reaction equations under the finite element   method", 
    "arxiv-id": "1506.06099v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2015-06-19T17:59:16Z", 
    "summary": "We present a robust computational framework for advective-diffusive-reactive\nsystems that satisfies maximum principles, the non-negative constraint, and\nelement-wise species balance property. The proposed methodology is valid on\ngeneral computational grids, can handle heterogeneous anisotropic media, and\nprovides accurate numerical solutions even for very high P\\'eclet numbers. The\nsignificant contribution of this paper is to incorporate advection (which makes\nthe spatial part of the differential operator non-self-adjoint) into the\nnon-negative computational framework, and overcome numerical challenges\nassociated with advection. We employ low-order mixed finite element\nformulations based on least-squares formalism, and enforce explicit constraints\non the discrete problem to meet the desired properties. The resulting\nconstrained discrete problem belongs to convex quadratic programming for which\na unique solution exists. Maximum principles and the non-negative constraint\ngive rise to bound constraints while element-wise species balance gives rise to\nequality constraints. The resulting convex quadratic programming problems are\nsolved using an interior-point algorithm. Several numerical results pertaining\nto advection-dominated problems are presented to illustrate the robustness,\nconvergence, and the overall performance of the proposed computational\nframework."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.06185v1", 
    "title": "Resilience for Multigrid Software at the Extreme Scale", 
    "arxiv-id": "1506.06185v1", 
    "author": "Barbara Wohlmuth", 
    "publish": "2015-06-20T00:03:16Z", 
    "summary": "Fault tolerant algorithms for the numerical approximation of elliptic partial\ndifferential equations on modern supercomputers play a more and more important\nrole in the future design of exa-scale enabled iterative solvers. Here, we\ncombine domain partitioning with highly scalable geometric multigrid schemes to\nobtain fast and fault-robust solvers in three dimensions. The recovery strategy\nis based on a hierarchical hybrid concept where the values on lower dimensional\nprimitives such as faces are stored redundantly and thus can be recovered\neasily in case of a failure. The lost volume unknowns in the faulty region are\nre-computed approximately with multigrid cycles by solving a local Dirichlet\nproblem on the faulty subdomain. Different strategies are compared and\nevaluated with respect to performance, computational cost, and speed up.\nEspecially effective are strategies in which the local recovery in the faulty\nregion is executed in parallel with global solves and when the local recovery\nis additionally accelerated. This results in an asynchronous multigrid\niteration that can fully compensate faults. Excellent parallel performance on a\ncurrent peta-scale system is demonstrated."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.08350v2", 
    "title": "Stochastic Gradient Made Stable: A Manifold Propagation Approach for   Large-Scale Optimization", 
    "arxiv-id": "1506.08350v2", 
    "author": "Wei Fan", 
    "publish": "2015-06-28T03:33:38Z", 
    "summary": "Stochastic gradient descent (SGD) holds as a classical method to build large\nscale machine learning models over big data. A stochastic gradient is typically\ncalculated from a limited number of samples (known as mini-batch), so it\npotentially incurs a high variance and causes the estimated parameters bounce\naround the optimal solution. To improve the stability of stochastic gradient,\nrecent years have witnessed the proposal of several semi-stochastic gradient\ndescent algorithms, which distinguish themselves from standard SGD by\nincorporating global information into gradient computation. In this paper we\ncontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm\nto this nascent research area, accelerating the optimization of a large family\nof composite convex functions. Though theoretically converging faster, prior\nsemi-stochastic algorithms are found to suffer from high iteration complexity,\nwhich makes them even slower than SGD in practice on many datasets. In our\nproposed S3GD, the semi-stochastic gradient is calculated based on efficient\nmanifold propagation, which can be numerically accomplished by sparse matrix\nmultiplications. This way S3GD is able to generate a highly-accurate estimate\nof the exact gradient from each mini-batch with largely-reduced computational\ncomplexity. Theoretic analysis reveals that the proposed S3GD elegantly\nbalances the geometric algorithmic convergence rate against the space and time\ncomplexities during the optimization. The efficacy of S3GD is also\nexperimentally corroborated on several large-scale benchmark datasets."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1506.08938v1", 
    "title": "Accelerated Parallel and Distributed Algorithm using Limited Internal   Memory for Nonnegative Matrix Factorization", 
    "arxiv-id": "1506.08938v1", 
    "author": "Tu-Bao Ho", 
    "publish": "2015-06-30T04:58:10Z", 
    "summary": "Nonnegative matrix factorization (NMF) is a powerful technique for dimension\nreduction, extracting latent factors and learning part-based representation.\nFor large datasets, NMF performance depends on some major issues: fast\nalgorithms, fully parallel distributed feasibility and limited internal memory.\nThis research aims to design a fast fully parallel and distributed algorithm\nusing limited internal memory to reach high NMF performance for large datasets.\nIn particular, we propose a flexible accelerated algorithm for NMF with all its\n$L_1$ $L_2$ regularized variants based on full decomposition, which is a\ncombination of an anti-lopsided algorithm and a fast block coordinate descent\nalgorithm. The proposed algorithm takes advantages of both these algorithms to\nachieve a linear convergence rate of $\\mathcal{O}(1-\\frac{1}{||Q||_2})^k$ in\noptimizing each factor matrix when fixing the other factor one in the sub-space\nof passive variables, where $r$ is the number of latent components; where\n$\\sqrt{r} \\leq ||Q||_2 \\leq r$. In addition, the algorithm can exploit the data\nsparseness to run on large datasets with limited internal memory of machines.\nFurthermore, our experimental results are highly competitive with 7\nstate-of-the-art methods about three significant aspects of convergence,\noptimality and average of the iteration number. Therefore, the proposed\nalgorithm is superior to fast block coordinate descent methods and accelerated\nmethods."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.00272v3", 
    "title": "Numerical instability of resultant methods for multidimensional   rootfinding", 
    "arxiv-id": "1507.00272v3", 
    "author": "Alex Townsend", 
    "publish": "2015-07-01T16:00:40Z", 
    "summary": "Hidden-variable resultant methods are a class of algorithms for solving\nmultidimensional polynomial rootfinding problems. In two dimensions, when\nsignificant care is taken, they are competitive practical rootfinders. However,\nin higher dimensions they are known to miss zeros, calculate roots to low\nprecision, and introduce spurious solutions. We show that the hidden variable\nresultant method based on the Cayley (Dixon or B\\'ezout) matrix is inherently\nand spectacularly numerically unstable by a factor that grows exponentially\nwith the dimension. We also show that the Sylvester matrix for solving\nbivariate polynomial systems can square the condition number of the problem. In\nother words, two popular hidden variable resultant methods are numerically\nunstable, and this mathematically explains the difficulties that are frequently\nreported by practitioners. Regardless of how the constructed polynomial\neigenvalue problem is solved, severe numerical difficulties will be present.\nAlong the way, we prove that the Cayley resultant is a generalization of\nCramer's rule for solving linear systems and generalize Clenshaw's algorithm to\nan evaluation scheme for polynomials expressed in a degree-graded polynomial\nbasis."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.00644v2", 
    "title": "Compressed Manifold Modes: Fast Calculation and Natural Ordering", 
    "arxiv-id": "1507.00644v2", 
    "author": "Kevin Houston", 
    "publish": "2015-07-02T16:06:26Z", 
    "summary": "Compressed manifold modes are locally supported analogues of eigenfunctions\nof the Laplace-Beltrami operator of a manifold. In this paper we describe an\nalgorithm for the calculation of modes for discrete manifolds that, in\nexperiments, requires on average 47% fewer iterations and 44% less time than\nthe previous algorithm. We show how to naturally order the modes in an\nanalogous way to eigenfunctions, that is we define a compressed eigenvalue.\nFurthermore, in contrast to the previous algorithm we permit unlumped mass\nmatrices for the operator and we show, unlike the case of eigenfunctions, that\nmodes can, in general, be oriented."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.00702v1", 
    "title": "Centralized and Distributed Newton Methods for Network Optimization and   Extensions", 
    "arxiv-id": "1507.00702v1", 
    "author": "Dimitri P. Bertsekas", 
    "publish": "2015-07-02T19:06:47Z", 
    "summary": "We consider Newton methods for common types of single commodity and\nmulti-commodity network flow problems. Despite the potentially very large\ndimension of the problem, they can be implemented using the conjugate gradient\nmethod and low-dimensional network operations, as shown nearly thirty years\nago. We revisit these methods, compare them to more recent proposals, and\ndescribe how they can be implemented in a distributed computing system. We also\ndiscuss generalizations, including the treatment of arc gains, linear side\nconstraints, and related special structures."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.04417v1", 
    "title": "A quadrilateral 'mini' finite element for the Stokes problem using a   single bubble function", 
    "arxiv-id": "1507.04417v1", 
    "author": "Bishnu P. Lamichhane", 
    "publish": "2015-07-16T00:16:02Z", 
    "summary": "We consider a quadrilateral 'mini' finite element for approximating the\nsolution of Stokes equations using a quadrilateral mesh. We use the standard\nbilinear finite element space enriched with element-wise defined bubble\nfunctions for the velocity and the standard bilinear finite element space for\nthe pressure space. With a simple modification of the standard bubble function\nwe show that a single bubble function is sufficient to ensure the inf-sup\ncondition. We have thus improved an earlier result on the quadrilateral 'mini'\nelement, where more than one bubble function are used to get the stability."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.05281v1", 
    "title": "On Dual-Finite Volume Methods for Extended Porous Medium Equations", 
    "arxiv-id": "1507.05281v1", 
    "author": "Hidekazu Yoshioka", 
    "publish": "2015-07-19T12:37:13Z", 
    "summary": "This article shows that the unconditional stability of the Dual-Finite Volume\nMethod, which is at least valid for linear problems, is not true for generic\nnonlinear differential equations including the PMEs unless the coefficient\nappearing in the numerical fluxes are appropriately evaluated. This article\nprovides a theoretically truly isotone numerical fluxes specialized for solving\nthe PMEs presented, which is still as simple as the conventional fully-upwind\ncounterpart."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.05593v1", 
    "title": "An Efficient Solver for Sparse Linear Systems Based on Rank-Structured   Cholesky Factorization", 
    "arxiv-id": "1507.05593v1", 
    "author": "David S. Bindel", 
    "publish": "2015-07-20T19:01:30Z", 
    "summary": "Direct factorization methods for the solution of large, sparse linear systems\nthat arise from PDE discretizations are robust, but typically show poor time\nand memory scalability for large systems. In this paper, we describe an\nefficient sparse, rank-structured Cholesky algorithm for solution of the\npositive definite linear system $A x = b$ when $A$ comes from a discretized\npartial-differential equation. Our approach combines the efficient memory\naccess patterns of conventional supernodal Cholesky algorithms with the memory\nefficiency of rank-structured direct solvers. For several test problems arising\nfrom PDE discretizations, our method takes less memory than standard sparse\nCholesky solvers and less wall-clock time than standard preconditioned\niterations."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.06174v2", 
    "title": "Direct Inversion of the 3D Pseudo-polar Fourier Transform", 
    "arxiv-id": "1507.06174v2", 
    "author": "Yoel Shkolnisky", 
    "publish": "2015-07-22T13:19:27Z", 
    "summary": "The pseudo-polar Fourier transform is a specialized non-equally spaced\nFourier transform, which evaluates the Fourier transform on a near-polar grid,\nknown as the pseudo-polar grid. The advantage of the pseudo-polar grid over\nother non-uniform sampling geometries is that the transformation, which samples\nthe Fourier transform on the pseudo-polar grid, can be inverted using a fast\nand stable algorithm. For other sampling geometries, even if the non-equally\nspaced Fourier transform can be inverted, the only known algorithms are\niterative. The convergence speed of these algorithms as well as their accuracy\nare difficult to control, as they depend both on the sampling geometry as well\nas on the unknown reconstructed object. In this paper, we present a direct\ninversion algorithm for the three-dimensional pseudo-polar Fourier transform.\nThe algorithm is based only on one-dimensional resampling operations, and is\nshown to be significantly faster than existing iterative inversion algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1507.08805v3", 
    "title": "A constructive arbitrary-degree Kronecker product decomposition of   tensors", 
    "arxiv-id": "1507.08805v3", 
    "author": "Ngai Wong", 
    "publish": "2015-07-31T09:19:47Z", 
    "summary": "We propose the tensor Kronecker product singular value decomposition~(TKPSVD)\nthat decomposes a real $k$-way tensor $\\mathcal{A}$ into a linear combination\nof tensor Kronecker products with an arbitrary number of $d$ factors\n$\\mathcal{A} = \\sum_{j=1}^R \\sigma_j\\, \\mathcal{A}^{(d)}_j \\otimes \\cdots\n\\otimes \\mathcal{A}^{(1)}_j$. We generalize the matrix Kronecker product to\ntensors such that each factor $\\mathcal{A}^{(i)}_j$ in the TKPSVD is a $k$-way\ntensor. The algorithm relies on reshaping and permuting the original tensor\ninto a $d$-way tensor, after which a polyadic decomposition with orthogonal\nrank-1 terms is computed. We prove that for many different structured tensors,\nthe Kronecker product factors $\\mathcal{A}^{(1)}_j,\\ldots,\\mathcal{A}^{(d)}_j$\nare guaranteed to inherit this structure. In addition, we introduce the new\nnotion of general symmetric tensors, which includes many different structures\nsuch as symmetric, persymmetric, centrosymmetric, Toeplitz and Hankel tensors."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.ins.2015.09.021", 
    "link": "http://arxiv.org/pdf/1508.01835v2", 
    "title": "The inverse fast multipole method: using a fast approximate direct   solver as a preconditioner for dense linear systems", 
    "arxiv-id": "1508.01835v2", 
    "author": "Eric Darve", 
    "publish": "2015-08-07T23:22:43Z", 
    "summary": "Although some preconditioners are available for solving dense linear systems,\nthere are still many matrices for which preconditioners are lacking, in\nparticular in cases where the size of the matrix $N$ becomes very large. There\nremains hence a great need to develop general purpose preconditioners whose\ncost scales well with the matrix size $N$. In this paper, we propose a\npreconditioner with broad applicability and with cost $\\mathcal{O}(N)$ for\ndense matrices, when the matrix is given by a smooth kernel. Extending the\nmethod using the same framework to general $\\mathcal{H}^2$-matrices is\nrelatively straightforward. These preconditioners have a controlled accuracy\n(machine accuracy can be achieved if needed) and scale linearly with $N$. They\nare based on an approximate direct solve of the system. The linear scaling of\nthe algorithm is achieved by means of two key ideas. First, the\n$\\mathcal{H}^2$-structure of the dense matrix is exploited to obtain an\nextended sparse system of equations. Second, fill-ins arising when performing\nthe elimination are compressed as low-rank matrices if they correspond to\nwell-separated interactions. This ensures that the sparsity pattern of the\nextended sparse matrix is preserved throughout the elimination, hence resulting\nin a very efficient algorithm with $\\mathcal{O}(N \\log(1/\\varepsilon)^2 )$\ncomputational cost and $\\mathcal{O}(N \\log 1/\\varepsilon )$ memory requirement,\nfor an error tolerance $0 < \\varepsilon < 1$. The solver is inexact, although\nthe error can be controlled and made as small as needed. These solvers are\nrelated to ILU in the sense that the fill-in is controlled. However, in ILU,\nmost of the fill-in is simply discarded whereas here it is approximated using\nlow-rank blocks, with a prescribed tolerance. Numerical examples are discussed\nto demonstrate the linear scaling of the method and to illustrate its\neffectiveness as a preconditioner."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.02345v1", 
    "title": "Numerical experiments on the efficiency of local grid refinement based   on truncation error estimates", 
    "arxiv-id": "1508.02345v1", 
    "author": "Apostolos Goulas", 
    "publish": "2015-08-10T18:26:27Z", 
    "summary": "Local grid refinement aims to optimise the relationship between accuracy of\nthe results and number of grid nodes. In the context of the finite volume\nmethod no single local refinement criterion has been globally established as\noptimum for the selection of the control volumes to subdivide, since it is not\neasy to associate the discretisation error with an easily computable quantity\nin each control volume. Often the grid refinement criterion is based on an\nestimate of the truncation error in each control volume, because the truncation\nerror is a natural measure of the discrepancy between the algebraic\nfinite-volume equations and the original differential equations. However, it is\nnot a straightforward task to associate the truncation error with the optimum\ngrid density because of the complexity of the relationship between truncation\nand discretisation errors. In the present work several criteria based on a\ntruncation error estimate are tested and compared on a regularised lid-driven\ncavity case at various Reynolds numbers. It is shown that criteria where the\ntruncation error is weighted by the volume of the grid cells perform better\nthan using just the truncation error as the criterion. Also it is observed that\nthe efficiency of local refinement increases with the Reynolds number. The\ntruncation error is estimated by restricting the solution to a coarser grid and\napplying the coarse grid discrete operator. The complication that high\ntruncation error develops at grid level interfaces is also investigated and\nseveral treatments are tested."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.02439v2", 
    "title": "Unified Acceleration Method for Packing and Covering Problems via   Diameter Reduction", 
    "arxiv-id": "1508.02439v2", 
    "author": "Michael W. Mahoney", 
    "publish": "2015-08-10T21:56:20Z", 
    "summary": "The linear coupling method was introduced recently by Allen-Zhu and Orecchia\nfor solving convex optimization problems with first order methods, and it\nprovides a conceptually simple way to integrate a gradient descent step and\nmirror descent step in each iteration. The high-level approach of the linear\ncoupling method is very flexible, and it has shown initial promise by providing\nimproved algorithms for packing and covering linear programs. Somewhat\nsurprisingly, however, while the dependence of the convergence rate on the\nerror parameter $\\epsilon$ for packing problems was improved to\n$O(1/\\epsilon)$, which corresponds to what accelerated gradient methods are\ndesigned to achieve, the dependence for covering problems was only improved to\n$O(1/\\epsilon^{1.5})$, and even that required a different more complicated\nalgorithm. Given the close connections between packing and covering problems\nand since previous algorithms for these very related problems have led to the\nsame $\\epsilon$ dependence, this discrepancy is surprising, and it leaves open\nthe question of the exact role that the linear coupling is playing in\ncoordinating the complementary gradient and mirror descent step of the\nalgorithm. In this paper, we clarify these issues for linear coupling\nalgorithms for packing and covering linear programs, illustrating that the\nlinear coupling method can lead to improved $O(1/\\epsilon)$ dependence for both\npacking and covering problems in a unified manner, i.e., with the same\nalgorithm and almost identical analysis. Our main technical result is a novel\ndiameter reduction method for covering problems that is of independent interest\nand that may be useful in applying the accelerated linear coupling method to\nother combinatorial problems."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.04758v2", 
    "title": "Rapidly Computing Sparse Legendre Expansions via Sparse Fourier   Transforms", 
    "arxiv-id": "1508.04758v2", 
    "author": "Hyejin Kim", 
    "publish": "2015-08-19T19:56:59Z", 
    "summary": "In this paper we propose a general strategy for rapidly computing sparse\nLegendre expansions. The resulting methods yield a new class of fast algorithms\ncapable of approximating a given function $f:[-1,1] \\rightarrow \\mathbb{R}$\nwith a near-optimal linear combination of $s$ Legendre polynomials of degree\n$\\leq N$ in just $(s \\log N)^{\\mathcal{O}(1)}$-time. When $s \\ll N$ these\nalgorithms exhibit sublinear runtime complexities in $N$, as opposed to\ntraditional $\\Omega(N \\log N)$-time methods for computing all of the first $N$\nLegendre coefficients of $f$. Theoretical as well as numerical results\ndemonstrate the promise of the proposed approach."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.05856v2", 
    "title": "A $N$-Body Solver for Square Root Iteration", 
    "arxiv-id": "1508.05856v2", 
    "author": "Nicolas Bock", 
    "publish": "2015-08-24T16:03:49Z", 
    "summary": "We develop the Sparse Approximate Matrix Multiply ($\\tt SpAMM$) $n$-body\nsolver for first order Newton Schulz iteration of the matrix square root and\ninverse square root. The solver performs recursive two-sided metric queries on\na modified Cauchy-Schwarz criterion, culling negligible sub-volumes of the\nproduct-tensor for problems with structured decay in the sub-space metric.\nThese sub-structures are shown to bound the relative error in the matrix-matrix\nproduct, and in favorable cases, to enjoy a reduced computational complexity\ngoverned by dimensionality reduction of the product volume. A main contribution\nis demonstration of a new, algebraic locality that develops under contractive\nidentity iteration, with collapse of the metric-subspace onto the identity's\nplane diagonal, resulting in a stronger $\\tt SpAMM$ bound. Also, we carry out a\nfirst order {Fr\\'{e}chet} analyses for single and dual channel instances of the\nsquare root iteration, and look at bifurcations due to ill-conditioning and a\ntoo aggressive $\\tt SpAMM$ approximation. Then, we show that extreme $\\tt\nSpAMM$ approximation and contractive identity iteration can be achieved for\nill-conditioned systems through regularization, and we demonstrate the\npotential for acceleration with a scoping, product representation of the\ninverse factor."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.05873v1", 
    "title": "Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for   Stationary Gaussian Inputs and Slow Learning", 
    "arxiv-id": "1508.05873v1", 
    "author": "Jos\u00e9 Carlos M. Bermudez", 
    "publish": "2015-08-24T16:26:38Z", 
    "summary": "Some system identification problems impose nonnegativity constraints on the\nparameters to estimate due to inherent physical characteristics of the unknown\nsystem. The nonnegative least-mean-square (NNLMS) algorithm and its variants\nallow to address this problem in an online manner. A nonnegative least mean\nfourth (NNLMF) algorithm has been recently proposed to improve the performance\nof these algorithms in cases where the measurement noise is not Gaussian. This\npaper provides a first theoretical analysis of the stochastic behavior of the\nNNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation\nresults illustrate the accuracy of the proposed analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.06103v1", 
    "title": "Two-level space-time domain decomposition methods for unsteady inverse   problems", 
    "arxiv-id": "1508.06103v1", 
    "author": "Jun Zou", 
    "publish": "2015-08-25T10:51:51Z", 
    "summary": "As the number of processor cores on supercomputers becomes larger and larger,\nalgorithms with high degree of parallelism attract more attention. In this\nwork, we propose a novel space-time coupled algorithm for solving an inverse\nproblem associated with the time-dependent convection-diffusion equation in\nthree dimensions. We introduce a mixed finite element/finite difference method\nand a one-level and a two-level space-time parallel domain decomposition\npreconditioner for the Karush-Kuhn-Tucker (KKT) system induced from\nreformulating the inverse problem as an output least-squares optimization\nproblem in the space-time domain. The new full space approach eliminates the\nsequential steps of the optimization outer loop and the inner forward and\nbackward time marching processes, thus achieves high degree of parallelism.\nNumerical experiments validate that this approach is effective and robust for\nrecovering unsteady moving sources. We report strong scalability results\nobtained on a supercomputer with more than 1,000 processors."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2012.06.023", 
    "link": "http://arxiv.org/pdf/1508.06429v2", 
    "title": "Improved Analyses of the Randomized Power Method and Block Lanczos   Method", 
    "arxiv-id": "1508.06429v2", 
    "author": "Tong Zhang", 
    "publish": "2015-08-26T09:58:30Z", 
    "summary": "The power method and block Lanczos method are popular numerical algorithms\nfor computing the truncated singular value decomposition (SVD) and eigenvalue\ndecomposition problems. Especially in the literature of randomized numerical\nlinear algebra, the power method is widely applied to improve the quality of\nrandomized sketching, and relative-error bounds have been well established.\nRecently, Musco & Musco (2015) proposed a block Krylov subspace method that\nfully exploits the intermediate results of the power iteration to accelerate\nconvergence. They showed spectral gap-independent bounds which are stronger\nthan the power method by order-of-magnitude. This paper offers novel error\nanalysis techniques and significantly improves the bounds of both the\nrandomized power method and the block Lanczos method. This paper also\nestablishes the first gap-independent bound for the warm-start block Lanczos\nmethod."
},{
    "category": "cs.NA", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1508.07240v1", 
    "title": "Rational Chebyshev of Second Kind Collocation Method for Solving a Class   of Astrophysics Problems", 
    "arxiv-id": "1508.07240v1", 
    "author": "Sajjad Khaleqi", 
    "publish": "2015-08-28T15:18:17Z", 
    "summary": "The Lane-Emden equation has been used to model several phenomenas in\ntheoretical physics, mathematical physics and astrophysics such as the theory\nof stellar structure. This study is an attempt to utilize the collocation\nmethod with the Rational Chebyshev of Second Kind function (RSC) to solve the\nLane-Emden equation over the semi-infinit interval [0; +infinity). According to\nwell-known results and comparing with previous methods, it can be said that\nthis method is efficient and applicable."
},{
    "category": "cs.NA", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.00206v1", 
    "title": "Fast Algorithms for the computation of Fourier Extensions of arbitrary   length", 
    "arxiv-id": "1509.00206v1", 
    "author": "Daan Huybrechs", 
    "publish": "2015-09-01T10:02:12Z", 
    "summary": "Fourier series of smooth, non-periodic functions on $[-1,1]$ are known to\nexhibit the Gibbs phenomenon, and exhibit overall slow convergence. One way of\novercoming these problems is by using a Fourier series on a larger domain, say\n$[-T,T]$ with $T>1$, a technique called Fourier extension or Fourier\ncontinuation. When constructed as the discrete least squares minimizer in\nequidistant points, the Fourier extension has been shown shown to converge\ngeometrically in the truncation parameter $N$. A fast ${\\mathcal O}(N \\log^2\nN)$ algorithm has been described to compute Fourier extensions for the case\nwhere $T=2$, compared to ${\\mathcal O}(N^3)$ for solving the dense discrete\nleast squares problem. We present two ${\\mathcal O}(N\\log^2 N )$ algorithms for\nthe computation of these approximations for the case of general $T$, made\npossible by exploiting the connection between Fourier extensions and Prolate\nSpheroidal Wave theory. The first algorithm is based on the explicit\ncomputation of so-called periodic discrete prolate spheroidal sequences, while\nthe second algorithm is purely algebraic and only implicitly based on the\ntheory."
},{
    "category": "cs.MS", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.01347v3", 
    "title": "Verificarlo: checking floating point accuracy through Monte Carlo   Arithmetic", 
    "arxiv-id": "1509.01347v3", 
    "author": "Eric Petit", 
    "publish": "2015-09-04T06:20:18Z", 
    "summary": "Numerical accuracy of floating point computation is a well studied topic\nwhich has not made its way to the end-user in scientific computing. Yet, it has\nbecome a critical issue with the recent requirements for code modernization to\nharness new highly parallel hardware and perform higher resolution computation.\nTo democratize numerical accuracy analysis, it is important to propose tools\nand methodologies to study large use cases in a reliable and automatic way. In\nthis paper, we propose verificarlo, an extension to the LLVM compiler to\nautomatically use Monte Carlo Arithmetic in a transparent way for the end-user.\nIt supports all the major languages including C, C++, and Fortran. Unlike\nsource-to-source approaches, our implementation captures the influence of\ncompiler optimizations on the numerical accuracy. We illustrate how Monte Carlo\nArithmetic using the verificarlo tool outperforms the existing approaches on\nvarious use cases and is a step toward automatic numerical analysis."
},{
    "category": "cs.CV", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.02223v2", 
    "title": "Diffusion tensor imaging with deterministic error bounds", 
    "arxiv-id": "1509.02223v2", 
    "author": "Tuomo Valkonen", 
    "publish": "2015-09-07T23:15:51Z", 
    "summary": "Errors in the data and the forward operator of an inverse problem can be\nhandily modelled using partial order in Banach lattices. We present some\nexisting results of the theory of regularisation in this novel framework, where\nerrors are represented as bounds by means of the appropriate partial order.\n  We apply the theory to Diffusion Tensor Imaging, where correct noise\nmodelling is challenging: it involves the Rician distribution and the nonlinear\nStejskal-Tanner equation. Linearisation of the latter in the statistical\nframework would complicate the noise model even further. We avoid this using\nthe error bounds approach, which preserves simple error structure under\nmonotone transformations."
},{
    "category": "cs.LG", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.03946v1", 
    "title": "Parametric Maxflows for Structured Sparse Learning with Convex   Relaxations of Submodular Functions", 
    "arxiv-id": "1509.03946v1", 
    "author": "Yutaro Yamaguchi", 
    "publish": "2015-09-14T04:11:02Z", 
    "summary": "The proximal problem for structured penalties obtained via convex relaxations\nof submodular functions is known to be equivalent to minimizing separable\nconvex functions over the corresponding submodular polyhedra. In this paper, we\nreveal a comprehensive class of structured penalties for which penalties this\nproblem can be solved via an efficiently solvable class of parametric maxflow\noptimization. We then show that the parametric maxflow algorithm proposed by\nGallo et al. and its variants, which runs, in the worst-case, at the cost of\nonly a constant factor of a single computation of the corresponding maxflow\noptimization, can be adapted to solve the proximal problems for those\npenalties. Several existing structured penalties satisfy these conditions;\nthus, regularized learning with these penalties is solvable quickly using the\nparametric maxflow algorithm. We also investigate the empirical runtime\nperformance of the proposed framework."
},{
    "category": "math.NA", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.05669v1", 
    "title": "A High-Order Radial Basis Function (RBF) Leray Projection Method for the   Solution of the Incompressible Unsteady Stokes Equations", 
    "arxiv-id": "1509.05669v1", 
    "author": "Grady B. Wright", 
    "publish": "2015-09-18T15:39:17Z", 
    "summary": "A new projection method based on radial basis functions (RBFs) is presented\nfor discretizing the incompressible unsteady Stokes equations in irregular\ngeometries. The novelty of the method comes from the application of a new\ntechnique for computing the Leray-Helmholtz projection of a vector field using\ngeneralized interpolation with divergence-free and curl-free RBFs. Unlike\ntraditional projection methods, this new method enables matching both\ntangential and normal components of divergence-free vector fields on the domain\nboundary. This allows incompressibility of the velocity field to be enforced\nwithout any time-splitting or pressure boundary conditions. Spatial derivatives\nare approximated using collocation with global RBFs so that the method only\nrequires samples of the field at (possibly scattered) nodes over the domain.\nNumerical results are presented demonstrating high-order convergence in both\nspace (between 5th and 6th order) and time (up to 4th order) for some model\nproblems in two dimensional irregular geometries."
},{
    "category": "cs.NA", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.06265v1", 
    "title": "Exact Real Arithmetic with Perturbation Analysis and Proof of   Correctness", 
    "arxiv-id": "1509.06265v1", 
    "author": "Jan Friso Groote", 
    "publish": "2015-09-21T15:20:26Z", 
    "summary": "In this article, we consider a simple representation for real numbers and\npropose top-down procedures to approximate various algebraic and transcendental\noperations with arbitrary precision. Detailed algorithms and proofs are\nprovided to guarantee the correctness of the approximations. Moreover, we\ndevelop and apply a perturbation analysis method to show that our approximation\nprocedures only recompute expressions when unavoidable.\n  In the last decade, various theories have been developed and implemented to\nrealize real computations with arbitrary precision. Proof of correctness for\nexisting approaches typically consider basic algebraic operations, whereas\ndetailed arguments about transcendental operations are not available. Another\nimportant observation is that in each approach some expressions might require\niterative computations to guarantee the desired precision. However, no formal\nreasoning is provided to prove that such iterative calculations are essential\nin the approximation procedures. In our approximations of real functions, we\nexplicitly relate the precision of the inputs to the guaranteed precision of\nthe output, provide full proofs and a precise analysis of the necessity of\niterations."
},{
    "category": "cs.NA", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.08323v1", 
    "title": "On the geometry of border rank algorithms for n x 2 by 2 x 2 matrix   multiplication", 
    "arxiv-id": "1509.08323v1", 
    "author": "Nicholas Ryder", 
    "publish": "2015-09-28T14:03:33Z", 
    "summary": "We make an in-depth study of the known border rank (i.e. approximate)\nalgorithms for the matrix multiplication tensor encoding the multiplication of\nan n x 2 matrix by a 2 x 2 matrix."
},{
    "category": "cs.SI", 
    "doi": "10.1140/epjp/i2016-16024-8", 
    "link": "http://arxiv.org/pdf/1509.08863v1", 
    "title": "Accelerated Spectral Clustering Using Graph Filtering Of Random Signals", 
    "arxiv-id": "1509.08863v1", 
    "author": "Pierre Vandergheynst", 
    "publish": "2015-09-29T17:32:48Z", 
    "summary": "We build upon recent advances in graph signal processing to propose a faster\nspectral clustering algorithm. Indeed, classical spectral clustering is based\non the computation of the first k eigenvectors of the similarity matrix'\nLaplacian, whose computation cost, even for sparse matrices, becomes\nprohibitive for large datasets. We show that we can estimate the spectral\nclustering distance matrix without computing these eigenvectors: by graph\nfiltering random signals. Also, we take advantage of the stochasticity of these\nrandom vectors to estimate the number of clusters k. We compare our method to\nclassical spectral clustering on synthetic data, and show that it reaches equal\nperformance while being faster by a factor at least two for large datasets."
},{
    "category": "cs.SY", 
    "doi": "10.1109/ACC.2016.7525157", 
    "link": "http://arxiv.org/pdf/1510.01728v1", 
    "title": "Learning-based Reduced Order Model Stabilization for Partial   Differential Equations: Application to the Coupled Burgers Equation", 
    "arxiv-id": "1510.01728v1", 
    "author": "Piyush Grover", 
    "publish": "2015-10-06T19:53:40Z", 
    "summary": "We present results on stabilization for reduced order models (ROM) of partial\ndifferential equations using learning. Stabilization is achieved via closure\nmodels for ROMs, where we use a model-free extremum seeking (ES) dither-based\nalgorithm to learn the best closure models' parameters, for optimal ROM\nstabilization. We first propose to auto-tune linear closure models using ES,\nand then extend the results to a closure model combining linear and nonlinear\nterms, for better stabilization performance. The coupled Burgers' equation is\nemployed as a test-bed for the proposed tuning method."
},{
    "category": "math.AP", 
    "doi": "10.1109/ACC.2016.7525157", 
    "link": "http://arxiv.org/pdf/1510.03588v1", 
    "title": "Time Asymptotics for a Critical Case in Fragmentation and   Growth-Fragmentation Equations", 
    "arxiv-id": "1510.03588v1", 
    "author": "Miguel Escobedo", 
    "publish": "2015-10-13T09:27:42Z", 
    "summary": "Fragmentation and growth-fragmentation equations is a family of problems with\nvaried and wide applications. This paper is devoted to description of the long\ntime time asymptotics of two critical cases of these equations, when the\ndivision rate is constant and the growth rate is linear or zero. The study of\nthese cases may be reduced to the study of the following fragmentation\nequation:$$\\frac{\\partial}{\\partial t} u(t,x) + u(t,x)=\\int\\limits\\_x^\\infty\nk\\_0(\\frac{x}{y}) u(t,y) dy.$$Using the Mellin transform of the equation, we\ndetermine the long time behavior of the solutions. Our results show in\nparticular the strong dependence of this asymptotic behavior with respect to\nthe initial data."
},{
    "category": "cs.NA", 
    "doi": "10.1109/IPDPS.2016.67", 
    "link": "http://arxiv.org/pdf/1510.06689v2", 
    "title": "Parallel Tensor Compression for Large-Scale Scientific Data", 
    "arxiv-id": "1510.06689v2", 
    "author": "Tamara G. Kolda", 
    "publish": "2015-10-22T17:06:12Z", 
    "summary": "As parallel computing trends towards the exascale, scientific data produced\nby high-fidelity simulations are growing increasingly massive. For instance, a\nsimulation on a three-dimensional spatial grid with 512 points per dimension\nthat tracks 64 variables per grid point for 128 time steps yields 8~TB of data,\nassuming double precision. By viewing the data as a dense five-way tensor, we\ncan compute a Tucker decomposition to find inherent low-dimensional multilinear\nstructure, achieving compression ratios of up to 5000 on real-world data sets\nwith negligible loss in accuracy. So that we can operate on such massive data,\nwe present the first-ever distributed-memory parallel implementation for the\nTucker decomposition, whose key computations correspond to parallel linear\nalgebra operations, albeit with nonstandard data layouts. Our approach\nspecifies a data distribution for tensors that avoids any tensor data\nredistribution, either locally or in parallel. We provide accompanying analysis\nof the computation and communication costs of the algorithms. To demonstrate\nthe compression and accuracy of the method, we apply our approach to real-world\ndata sets from combustion science simulations. We also provide detailed\nperformance results, including parallel performance in both weak and strong\nscaling experiments."
},{
    "category": "math.NA", 
    "doi": "10.1109/IPDPS.2016.67", 
    "link": "http://arxiv.org/pdf/1510.08297v1", 
    "title": "Numerical solving unsteady space-fractional problems with the square   root of an elliptic operator", 
    "arxiv-id": "1510.08297v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2015-10-28T13:17:01Z", 
    "summary": "An unsteady problem is considered for a space-fractional equation in a\nbounded domain. A first-order evolutionary equation involves the square root of\nan elliptic operator of second order. Finite element approximation in space is\nemployed. To construct approximation in time, regularized two-level schemes are\nused. The numerical implementation is based on solving the equation with the\nsquare root of the elliptic operator using an auxiliary Cauchy problem for a\npseudo-parabolic equation. The scheme of the second-order accuracy in time is\nbased on a regularization of the three-level explicit Adams scheme. More\ngeneral problems for the equation with convective terms are considered, too.\nThe results of numerical experiments are presented for a model two-dimensional\nproblem."
},{
    "category": "math.NA", 
    "doi": "10.1109/IPDPS.2016.67", 
    "link": "http://arxiv.org/pdf/1511.01353v1", 
    "title": "A N-Body Solver for Free Mesh Interpolation", 
    "arxiv-id": "1511.01353v1", 
    "author": "Matt Challacombe", 
    "publish": "2015-10-31T15:58:41Z", 
    "summary": "Factorization of the Gaussian RBF kernel is developed for free-mesh\ninterpolation in the flat, polynomial limit corresponding to Taylor expansion\nand the Vandermonde basis of geometric moments. With this spectral\napproximation, a top-down octree-scoping of an interpolant is found by\nrecursively decomposing the residual, similar to the work of Driscoll and\nHeryudono (2007), except that in the current approach the grid is decoupled\nfrom the low rank approximation, allowing partial separation of sampling errors\n(the mesh) from representation errors (the polynomial order). Then, it is\npossible to demonstrate roughly 5 orders of magnitude improvement in free-mesh\ninterpolation errors for the three-dimensional Franke function, relative to\nprevious benchmarks. As in related work on $N$-body methods for factorization\nby square root iteration (Challacombe 2015), some emphasis is placed on\nresolution of the identity."
},{
    "category": "math.NA", 
    "doi": "10.1109/IPDPS.2016.67", 
    "link": "http://arxiv.org/pdf/1511.01593v1", 
    "title": "Robust data assimilation using $L_1$ and Huber norms", 
    "arxiv-id": "1511.01593v1", 
    "author": "Elias Nino-Ruiz", 
    "publish": "2015-11-05T03:17:42Z", 
    "summary": "Data assimilation is the process to fuse information from priors,\nobservations of nature, and numerical models, in order to obtain best estimates\nof the parameters or state of a physical system of interest. Presence of large\nerrors in some observational data, e.g., data collected from a faulty\ninstrument, negatively affect the quality of the overall assimilation results.\n  This work develops a systematic framework for robust data assimilation. The\nnew algorithms continue to produce good analyses in the presence of observation\noutliers. The approach is based on replacing the traditional $\\L_2$ norm\nformulation of data assimilation problems with formulations based on $\\L_1$ and\nHuber norms. Numerical experiments using the Lorenz-96 and the shallow water on\nthe sphere models illustrate how the new algorithms outperform traditional data\nassimilation approaches in the presence of data outliers."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.enganabound.2015.11.010", 
    "link": "http://arxiv.org/pdf/1511.03925v1", 
    "title": "The scalability of the matrices in direct Trefftz method in 2D Laplace   problem", 
    "arxiv-id": "1511.03925v1", 
    "author": "M. Borkowski", 
    "publish": "2015-11-12T15:25:39Z", 
    "summary": "This paper presents an interesting property of the matrices that may be\nobtained with the use of direct Trefftz method. It is proved analytically for\n2D Laplace problem that values of the elements of matrices describing the\ncapacitance of two scaled domains are inversely proportional to the scalability\nfactor. As an example of the application the capacitance extraction problem is\nchosen. Concise description of the algorithm in which the scalability property\ncan be utilized is given. Furthermore some numerical results of the algorithm\nare presented."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.04695v1", 
    "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete   Multiway Tensors", 
    "arxiv-id": "1511.04695v1", 
    "author": "Bing Zeng", 
    "publish": "2015-11-15T12:56:36Z", 
    "summary": "We consider the problem of low-rank decomposition of incomplete multiway\ntensors. Since many real-world data lie on an intrinsically low dimensional\nsubspace, tensor low-rank decomposition with missing entries has applications\nin many data analysis problems such as recommender systems and image\ninpainting. In this paper, we focus on Tucker decomposition which represents an\nNth-order tensor in terms of N factor matrices and a core tensor via\nmultilinear operations. To exploit the underlying multilinear low-rank\nstructure in high-dimensional datasets, we propose a group-based log-sum\npenalty functional to place structural sparsity over the core tensor, which\nleads to a compact representation with smallest core tensor. The method for\nTucker decomposition is developed by iteratively minimizing a surrogate\nfunction that majorizes the original objective function, which results in an\niterative reweighted process. In addition, to reduce the computational\ncomplexity, an over-relaxed monotone fast iterative shrinkage-thresholding\ntechnique is adapted and embedded in the iterative reweighted process. The\nproposed method is able to determine the model complexity (i.e. multilinear\nrank) in an automatic way. Simulation results show that the proposed algorithm\noffers competitive performance compared with other existing algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.05208v3", 
    "title": "HOID: Higher Order Interpolatory Decomposition for tensors based on   Tucker representation", 
    "arxiv-id": "1511.05208v3", 
    "author": "Arvind K. Saibaba", 
    "publish": "2015-11-16T22:36:26Z", 
    "summary": "We derive a CUR-type factorization for tensors in the Tucker format based on\ninterpolatory decomposition, which we will denote as Higher Order Interpolatory\nDecomposition (HOID). Given a tensor $\\mathcal{X}$, the algorithm provides a\nset of column vectors $\\{ \\mathbf{C}_n\\}_{n=1}^d$ which are columns extracted\nfrom the mode-$n$ tensor unfolding, along with a core tensor $\\mathcal{G}$ and\ntogether, they satisfy some error bounds. Compared to the Higher Order SVD\n(HOSVD) algorithm, the HOID provides a decomposition that preserves certain\nimportant features of the original tensor such as sparsity, non-negativity,\ninteger values, etc. Error bounds along with detailed estimates of\ncomputational costs are provided. The algorithms proposed in this paper have\nbeen validated against carefully chosen numerical examples which highlight the\nfavorable properties of the algorithms. Related methods for subset selection\nproposed for matrix CUR decomposition, such as Discrete Empirical Interpolation\nmethod (DEIM) and leverage score sampling, have also been extended to tensors\nand are compared against our proposed algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.06029v2", 
    "title": "A Tensor-Train accelerated solver for integral equations in complex   geometries", 
    "arxiv-id": "1511.06029v2", 
    "author": "Denis Zorin", 
    "publish": "2015-11-19T00:11:08Z", 
    "summary": "We present a framework using the Quantized Tensor Train (QTT) decomposition\nto accurately and efficiently solve volume and boundary integral equations in\nthree dimensions. We describe how the QTT decomposition can be used as a\nhierarchical compression and inversion scheme for matrices arising from the\ndiscretization of integral equations. For a broad range of problems,\ncomputational and storage costs of the inversion scheme are extremely modest\n$O(\\log N)$ and once the inverse is computed, it can be applied in $O(N \\log\nN)$.\n  We analyze the QTT ranks for hierarchically low rank matrices and discuss its\nrelationship to commonly used hierarchical compression techniques such as FMM\nand HSS. We prove that the QTT ranks are bounded for translation-invariant\nsystems and argue that this behavior extends to non-translation invariant\nvolume and boundary integrals.\n  For volume integrals, the QTT decomposition provides an efficient direct\nsolver requiring significantly less memory compared to other fast direct\nsolvers. We present results demonstrating the remarkable performance of the\nQTT-based solver when applied to both translation and non-translation invariant\nvolume integrals in 3D.\n  For boundary integral equations, we demonstrate that using a QTT\ndecomposition to construct preconditioners for a Krylov subspace method leads\nto an efficient and robust solver with a small memory footprint. We test the\nQTT preconditioners in the iterative solution of an exterior elliptic boundary\nvalue problem (Laplace) formulated as a boundary integral equation in complex,\nmultiply connected geometries."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.06468v1", 
    "title": "Faster Parallel Solver for Positive Linear Programs via   Dynamically-Bucketed Selective Coordinate Descent", 
    "arxiv-id": "1511.06468v1", 
    "author": "Satish Rao", 
    "publish": "2015-11-20T01:10:13Z", 
    "summary": "We provide improved parallel approximation algorithms for the important class\nof packing and covering linear programs. In particular, we present new parallel\n$\\epsilon$-approximate packing and covering solvers which run in\n$\\tilde{O}(1/\\epsilon^2)$ expected time, i.e., in expectation they take\n$\\tilde{O}(1/\\epsilon^2)$ iterations and they do $\\tilde{O}(N/\\epsilon^2)$\ntotal work, where $N$ is the size of the constraint matrix and $\\epsilon$ is\nthe error parameter, and where the $\\tilde{O}$ hides logarithmic factors. To\nachieve our improvement, we introduce an algorithmic technique of broader\ninterest: dynamically-bucketed selective coordinate descent (DB-SCD). At each\nstep of the iterative optimization algorithm, the DB-SCD method dynamically\nbuckets the coordinates of the gradient into those of roughly equal magnitude,\nand it updates all the coordinates in one of the buckets. This\ndynamically-bucketed updating permits us to take steps along several\ncoordinates with similar-sized gradients, thereby permitting more appropriate\nstep sizes at each step of the algorithm. In particular, this technique allows\nus to use in a straightforward manner the recent analysis from the breakthrough\nresults of Allen-Zhu and Orecchia [2] to achieve our still-further improved\nbounds. More generally, this method addresses \"interference\" among coordinates,\nby which we mean the impact of the update of one coordinate on the gradients of\nother coordinates. Such interference is a core issue in parallelizing\noptimization routines that rely on smoothness properties. Since our DB-SCD\nmethod reduces interference via updating a selective subset of variables at\neach iteration, we expect it may also have more general applicability in\noptimization."
},{
    "category": "quant-ph", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.08253v1", 
    "title": "Quantum Algorithms and Circuits for Scientific Computing", 
    "arxiv-id": "1511.08253v1", 
    "author": "Iasonas Petras", 
    "publish": "2015-11-25T23:39:16Z", 
    "summary": "Quantum algorithms for scientific computing require modules implementing\nfundamental functions, such as the square root, the logarithm, and others. We\nrequire algorithms that have a well-controlled numerical error, that are\nuniformly scalable and reversible (unitary), and that can be implemented\nefficiently. We present quantum algorithms and circuits for computing the\nsquare root, the natural logarithm, and arbitrary fractional powers. We provide\nperformance guarantees in terms of their worst-case accuracy and cost. We\nfurther illustrate their performance by providing tests comparing them to the\nrespective floating point implementations found in widely used numerical\nsoftware."
},{
    "category": "math.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1511.08264v2", 
    "title": "Efficient degree reduction of B\u00e9zier curves with box constraints using   dual bases", 
    "arxiv-id": "1511.08264v2", 
    "author": "Pawe\u0142 Wo\u017any", 
    "publish": "2015-11-24T17:20:23Z", 
    "summary": "In this paper, we give an efficient algorithm of degree reduction of B\\'ezier\ncurves with box constraints. The idea is to combine the previous iterative\napproach, that has been presented recently in (P. Gospodarczyk, Comput. Aided\nDes. 62 (2015), 143--151), with a fast method of construction of dual bases\nfrom (P. Wo\\'zny, J. Comput. Appl. Math. 260 (2014), 301--311) and a new\nefficient method of modification of dual bases."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1512.01748v1", 
    "title": "Restricted Low-Rank Approximation via ADMM", 
    "arxiv-id": "1512.01748v1", 
    "author": "Ying Zhang", 
    "publish": "2015-12-06T06:12:15Z", 
    "summary": "The matrix low-rank approximation problem with additional convex constraints\ncan find many applications and has been extensively studied before. However,\nthis problem is shown to be nonconvex and NP-hard; most of the existing\nsolutions are heuristic and application-dependent. In this paper, we show that,\nother than tons of application in current literature, this problem can be used\nto recover a feasible solution for SDP relaxation. By some sophisticated\ntricks, it can be equivalently posed in an appropriate form for the Alternating\nDirection Method of Multipliers (ADMM) to solve. The two updates of ADMM\ninclude the basic matrix low-rank approximation and projection onto a convex\nset. Different from the general non-convex problems, the sub-problems in each\nstep of ADMM can be solved exactly and efficiently in spite of their\nnon-convexity. Moreover, the algorithm will converge exponentially under proper\nconditions. The simulation results confirm its superiority over existing\nsolutions. We believe that the results in this paper provide a useful tool for\nthis important problem and will help to extend the application of ADMM to the\nnon-convex regime."
},{
    "category": "stat.ML", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1512.01904v2", 
    "title": "Gauss quadrature for matrix inverse forms with applications", 
    "arxiv-id": "1512.01904v2", 
    "author": "Stefanie Jegelka", 
    "publish": "2015-12-07T04:13:45Z", 
    "summary": "We present a framework for accelerating a spectrum of machine learning\nalgorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$,\nwhere $A$ is a positive definite matrix and $u$ a given vector. Our framework\nis built on Gauss-type quadrature and easily scales to large, sparse matrices.\nFurther, it allows retrospective computation of lower and upper bounds on\n$u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that\nthese bounds tighten iteratively and converge at a linear (geometric) rate. To\nour knowledge, ours is the first work to demonstrate these key properties of\nGauss-type quadrature, which is a classical and deeply studied topic. We\nillustrate empirical consequences of our results by using quadrature to\naccelerate machine learning tasks involving determinantal point processes and\nsubmodular optimization, and observe tremendous speedups in several instances."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1512.02390v2", 
    "title": "Nonuniformly weighted Schwarz smoothers for spectral element multigrid", 
    "arxiv-id": "1512.02390v2", 
    "author": "Joerg Stiller", 
    "publish": "2015-12-08T10:21:29Z", 
    "summary": "A hybrid Schwarz/multigrid method for spectral element solvers to the Poisson\nequation in $\\mathbb R^2$ is presented. It extends the additive Schwarz method\nstudied by J. Lottes and P. Fischer (J. Sci. Comput. 24:45--78, 2005) by\nintroducing nonuniform weight distributions based on the smoothed sign\nfunction. Using a V-cycle with only one pre-smoothing, the new method attains\nlogarithmic convergence rates in the range from 1.2 to 1.9, which corresponds\nto residual reductions of almost two orders of magnitude. Compared to the\noriginal method, it reduces the iteration count by a factor of 1.5 to 3,\nleading to runtime savings of about 50 percent. In numerical experiments the\nmethod proved robust with respect to the mesh size and polynomial orders up to\n32. Used as a preconditioner for the (inexact) CG method it is also suited for\nanisotropic meshes and easily extended to diffusion problems with variable\ncoefficients."
},{
    "category": "math.NA", 
    "doi": "10.1109/TSP.2016.2572047", 
    "link": "http://arxiv.org/pdf/1512.02671v2", 
    "title": "Householder QR Factorization with Randomization for Column Pivoting   (HQRRP). FLAME Working Note #78", 
    "arxiv-id": "1512.02671v2", 
    "author": "Robert van de Geijn", 
    "publish": "2015-12-08T21:51:53Z", 
    "summary": "A fundamental problem when adding column pivoting to the Householder QR\nfactorization is that only about half of the computation can be cast in terms\nof high performing matrix-matrix multiplications, which greatly limits the\nbenefits that can be derived from so-called blocking of algorithms. This paper\ndescribes a technique for selecting groups of pivot vectors by means of\nrandomized projections. It is demonstrated that the asymptotic flop count for\nthe proposed method is $2mn^2 - (2/3)n^3$ for an $m\\times n$ matrix, identical\nto that of the best classical unblocked Householder QR factorization algorithm\n(with or without pivoting). Experiments demonstrate acceleration in speed of\nclose to an order of magnitude relative to the {\\sc geqp3} function in LAPACK,\nwhen executed on a modern CPU with multiple cores. Further, experiments\ndemonstrate that the quality of the randomized pivot selection strategy is\nroughly the same as that of classical column pivoting. The described algorithm\nis made available under Open Source license and can be used with LAPACK or\nlibflame."
},{
    "category": "cs.NA", 
    "doi": "10.12988/ams.2015.510644", 
    "link": "http://arxiv.org/pdf/1512.03251v1", 
    "title": "Histogram Arithmetic under Uncertainty of Probability Density Function", 
    "arxiv-id": "1512.03251v1", 
    "author": "D. A. Korolev", 
    "publish": "2015-12-10T13:42:35Z", 
    "summary": "In this article we propose a method of performing arithmetic operations on\nvaria-bles with unknown distribution. The approach to the evaluation results of\narithme-tic operations can select probability intervals of the algebraic\nequations and their systems solutions, of differential equations and their\nsystems in case of histogram evaluation of the empirical density distributions\nof random parameters."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.09.035", 
    "link": "http://arxiv.org/pdf/1512.03335v2", 
    "title": "Adaptive multi-stage integrators for optimal energy conservation in   molecular simulations", 
    "arxiv-id": "1512.03335v2", 
    "author": "J. M. Sanz-Serna", 
    "publish": "2015-12-10T17:31:21Z", 
    "summary": "We introduce a new Adaptive Integration Approach (AIA) to be used in a wide\nrange of molecular simulations. Given a simulation problem and a step size, the\nmethod automatically chooses the optimal scheme out of an available family of\nnumerical integrators. Although we focus on two-stage splitting integrators,\nthe idea may be used with more general families. In each instance, the\nsystem-specific integrating scheme identified by our approach is optimal in the\nsense that it provides the best conservation of energy for harmonic forces. The\nAIA method has been implemented in the BCAM-modified GROMACS software package.\nNumerical tests in molecular dynamics and hybrid Monte Carlo simulations of\nconstrained and unconstrained physical systems show that the method\nsuccessfully realises the fail-safe strategy. In all experiments, and for each\nof the criteria employed, the AIA is at least as good as, and often\nsignificantly outperforms the standard Verlet scheme, as well as fixed\nparameter, optimized two-stage integrators. In particular, the sampling\nefficiency found in simulations using the AIA is up to 5 times better than the\none achieved with other tested schemes."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.09.035", 
    "link": "http://arxiv.org/pdf/1512.03357v1", 
    "title": "Construction of ODE systems from time series data by a highly flexible   modelling approach", 
    "arxiv-id": "1512.03357v1", 
    "author": "Thomas Dierkes", 
    "publish": "2015-12-10T18:14:33Z", 
    "summary": "In this paper, a down-to-earth approach to purely data-based modelling of\nunknown dynamical systems is presented. Starting from a classical, explicit ODE\nformulation $y' = f(t,y)$ of a dynamical system, a method determining the\nunknown right-hand side $f(t,y)$ from some trajectory data $y_{k}(t_{j})$,\npossibly very sparse, is given. As illustrative examples, a semi-standard\npredator-prey model is reconstructed from a data set describing the population\nnumbers of hares and lynxes over a period of twenty years, and a simple damped\npendulum system with a highly non-linear right-hand side is recovered from some\nartificial but very sparse data."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.jcp.2016.09.035", 
    "link": "http://arxiv.org/pdf/1512.04468v1", 
    "title": "A Method to Calculate the Exit Time in Stochastic Simulations", 
    "arxiv-id": "1512.04468v1", 
    "author": "Basil S. Bayati", 
    "publish": "2015-11-23T19:40:17Z", 
    "summary": "A novel method is presented to compute the exit time for the stochastic\nsimulation algorithm. The method is based on the addition of a series of random\nvariables and is derived using the convolution theorem. The final distribution\nis derived and approximated in the frequency domain. The distribution for the\nfinal time is transformed back to the real domain and can be sampled from in a\nsimulation. The result is an approximation of the classical stochastic\nsimulation algorithm that requires fewer random variates. An analysis of the\nerror and speedup compared to the stochastic simulation algorithm is presented."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.jcp.2016.09.035", 
    "link": "http://arxiv.org/pdf/1512.06500v1", 
    "title": "Inexact Krylov Subspace Algorithms for Large Matrix Exponential   Eigenproblem from Dimensionality Reduction", 
    "arxiv-id": "1512.06500v1", 
    "author": "Meng Yang", 
    "publish": "2015-12-21T05:47:12Z", 
    "summary": "Matrix exponential discriminant analysis (EDA) is a generalized discriminant\nanalysis method based on matrix exponential. It can essentially overcome the\nintrinsic difficulty of small sample size problem that exists in the classical\nlinear discriminant analysis (LDA). However, for data with high dimension, one\nhas to solve a large matrix exponential eigenproblem in this method, and the\ntime complexity is dominated by the computation of exponential of large\nmatrices. In this paper, we propose two inexact Krylov subspace algorithms for\nsolving the large matrix exponential eigenproblem effectively. The contribution\nof this work is threefold. First, we consider how to compute matrix\nexponential-vector products efficiently, which is the key step in the Krylov\nsubspace method. Second, we compare the discriminant analysis criterion of EDA\nand that of LDA from a theoretical point of view. Third, we establish a\nrelationship between the accuracy of the approximate eigenvectors and the\ndistance to nearest neighbour classifier, and show why the matrix exponential\neigenproblem can be solved approximately in practice. Numerical experiments on\nsome real-world databases show superiority of our new algorithms over many\nstate-of-the-art algorithms for face recognition."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1512.06626v2", 
    "title": "Banded operational matrices for Bernstein polynomials and application to   the fractional advection-dispersion equation", 
    "arxiv-id": "1512.06626v2", 
    "author": "D. Bhatta", 
    "publish": "2015-12-21T13:53:15Z", 
    "summary": "In the papers dealing with derivation and applications of operational\nmatrices of Bernstein polynomials, a basis transformation, commonly a\ntransformation to power basis, is used. The main disadvantage of this method is\nthat the transformation may be ill-conditioned. Moreover, when applied to the\nnumerical simulation of a functional differential equation, it leads to dense\noperational matrices and so a dense coefficient matrix is obtained. In this\npaper, we present a new property for Bernstein polynomials. Using this\nproperty, we build exact banded operational matrices for derivatives of\nBernstein polynomials. Next, as an application, we propose a new numerical\nmethod based on a Petrov-Galerkin variational formulation and the new\noperational matrices utilizing the dual Bernstein basis for the time-fractional\nadvection-dispersion equation. Finally, we show that the proposed method leads\nto a narrow-banded linear system and so less computational effort is required\nto obtain the desired accuracy for the approximate solution. Some numerical\nexamples are provided to demonstrate the efficiency of the method."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1512.08716v3", 
    "title": "On Approximating Univariate NP-Hard Integrals", 
    "arxiv-id": "1512.08716v3", 
    "author": "Avishy Carmi", 
    "publish": "2015-12-26T22:20:24Z", 
    "summary": "Approximating a definite integral of product of cosines to within an accuracy\nof n binary digits where the integrand depends on input integers x[k] given in\nbinary radix, is equivalent to counting the number of equal-sum partitions of\nthe integers and is thus a #P problem. Similarly, integrating this function\nfrom zero to infinity and deciding whether the result is either zero or\ninfinity is an NP-Complete problem. Efficient numerical integration methods\nsuch as the double exponential formula and the sinc approximation have been\naround since the mid 70's. Noting the hardness of approximating the integral we\nargue that the proven rates of convergence of such methods cannot possibly be\ncorrect since they give rise to an anomalous result as P=#P."
},{
    "category": "math.OC", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1601.01469v2", 
    "title": "Tensor and Its Tucker Core: the Invariance Relationships", 
    "arxiv-id": "1601.01469v2", 
    "author": "Shuzhong Zhang", 
    "publish": "2016-01-07T10:34:23Z", 
    "summary": "In [13], Hillar and Lim famously demonstrated that \"multilinear (tensor)\nanalogues of many efficiently computable problems in numerical linear algebra\nare NP-hard\". Despite many recent advancements, the state-of-the-art methods\nfor computing such `tensor analogues' still suffer severely from the curse of\ndimensionality. In this paper we show that the Tucker core of a tensor however,\nretains many properties of the original tensor, including the CP rank, the\nborder rank, the tensor Schatten quasi norms, and the Z-eigenvalues. When the\ncore tensor is smaller than the original tensor, this property leads to\nconsiderable computational advantages as confirmed by our numerical\nexperiments. In our analysis, we in fact work with a generalized Tucker-like\ndecomposition that can accommodate any full column-rank factor matrices."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1601.03597v1", 
    "title": "Geometric Integration Over Irregular Domains with topologic Guarantees", 
    "arxiv-id": "1601.03597v1", 
    "author": "Andreas N\u00fc\u00dfing", 
    "publish": "2016-01-14T14:00:27Z", 
    "summary": "Implicitly described domains are a well established tool in the simulation of\ntime dependent problems, e.g. using level-set methods. In order to solve\npartial differential equations on such domains, a range of numerical methods\nwas developed, e.g. the Immersed Boundary method, Unfitted Finite Element or\nUnfitted discontinuous Galerkin methods, eXtended or Generalised Finite Element\nmethods, just to name a few. Many of these methods involve integration over\ncut-cells or their boundaries, as they are described by sub-domains of the\noriginal level-set mesh. We present a new algorithm to geometrically evaluate\nthe integrals over domains described by a first-order, conforming level-set\nfunction. The integration is based on a polyhedral reconstruction of the\nimplicit geometry, following the concepts of the Marching Cubes algorithm. The\nalgorithm preserves various topological properties of the implicit geometry in\nits polyhedral reconstruction, making it suitable for Finite Element\ncomputations. Numerical experiments show second order accuracy of the\nintegration. An implementation of the algorithm is available as free software,\nwhich allows for an easy incorporation into other projects. The software is in\nproductive use within the DUNE framework."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1601.04280v1", 
    "title": "Randomized LU Decomposition Using Sparse Projections", 
    "arxiv-id": "1601.04280v1", 
    "author": "Amir Averbuch", 
    "publish": "2016-01-17T12:07:51Z", 
    "summary": "A fast algorithm for the approximation of a low rank LU decomposition is\npresented. In order to achieve a low complexity, the algorithm uses sparse\nrandom projections combined with FFT-based random projections. The asymptotic\napproximation error of the algorithm is analyzed and a theoretical error bound\nis presented. Finally, numerical examples illustrate that for a similar\napproximation error, the sparse LU algorithm is faster than recent\nstate-of-the-art methods. The algorithm is completely parallelizable that\nenables to run on a GPU. The performance is tested on a GPU card, showing a\nsignificant improvement in the running time in comparison to sequential\nexecution."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-016-0229-1", 
    "link": "http://arxiv.org/pdf/1601.06681v2", 
    "title": "eHDG:An Exponentially Convergent Iterative Solver for HDG   Discretizations of Hyperbolic Partial Differential Equations", 
    "arxiv-id": "1601.06681v2", 
    "author": "Tan Bui-Thanh", 
    "publish": "2016-01-25T17:16:13Z", 
    "summary": "We present a scalable and efficient iterative solver for high-order\nhybridized discontinuous Galerkin (HDG) discretizations of hyperbolic partial\ndifferential equations. It is an interplay between domain decomposition methods\nand HDG discretizations. In particular, the method is a fixed-point approach\nthat requires only independent element-by-element local solves in each\niteration. As such, it is well-suited for current and future computing systems\nwith massive concurrencies. We rigorously show that the proposed method is\nexponentially convergent in the number of iterations for transport and\nlinearized shallow water equations. Furthermore, the convergence is independent\nof the solution order. Various 2D and 3D numerical results for steady and\ntime-dependent problems are presented to verify our theoretical findings."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1601.07010v2", 
    "title": "A Distributed and Incremental SVD Algorithm for Agglomerative Data   Analysis on Large Networks", 
    "arxiv-id": "1601.07010v2", 
    "author": "B. W. Ong", 
    "publish": "2016-01-26T13:18:43Z", 
    "summary": "In this paper, we show that the SVD of a matrix can be constructed\nefficiently in a hierarchical approach. Our algorithm is proven to recover the\nsingular values and left singular vectors if the rank of the input matrix $A$\nis known. Further, the hierarchical algorithm can be used to recover the $d$\nlargest singular values and left singular vectors with bounded error. We also\nshow that the proposed method is stable with respect to roundoff errors or\ncorruption of the original matrix entries. Numerical experiments validate the\nproposed algorithms and parallel cost analysis."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1601.07721v1", 
    "title": "Distributed Low Rank Approximation of Implicit Functions of a Matrix", 
    "arxiv-id": "1601.07721v1", 
    "author": "Peilin Zhong", 
    "publish": "2016-01-28T10:58:27Z", 
    "summary": "We study distributed low rank approximation in which the matrix to be\napproximated is only implicitly represented across the different servers. For\nexample, each of $s$ servers may have an $n \\times d$ matrix $A^t$, and we may\nbe interested in computing a low rank approximation to $A = f(\\sum_{t=1}^s\nA^t)$, where $f$ is a function which is applied entrywise to the matrix\n$\\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to\nefficiently compute a $d \\times d$ rank-$k$ projection matrix $P$ for which\n$\\|A - AP\\|_F^2 \\leq \\|A - [A]_k\\|_F^2 + \\varepsilon \\|A\\|_F^2$, where $AP$\ndenotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the\nbest rank-$k$ approximation to $A$ given by the singular value decomposition.\nThe communication cost of our protocols is $d \\cdot (sk/\\varepsilon)^{O(1)}$,\nand they succeed with high probability. Our framework allows us to efficiently\ncompute a low rank approximation to an entry-wise softmax, to a Gaussian kernel\nexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low\nrank approximation). We also show that our additive error approximation is best\npossible, in the sense that any protocol achieving relative error for these\nproblems requires significantly more communication. Finally, we experimentally\nvalidate our algorithms on real datasets."
},{
    "category": "math.OC", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1601.07800v1", 
    "title": "Weighted tensor decomposition for approximate decoupling of multivariate   polynomials", 
    "arxiv-id": "1601.07800v1", 
    "author": "Johan Schoukens", 
    "publish": "2016-01-28T15:26:12Z", 
    "summary": "Multivariate polynomials arise in many different disciplines. Representing\nsuch a polynomial as a vector of univariate polynomials can offer useful\ninsight, as well as more intuitive understanding. For this, techniques based on\ntensor methods are known, but these have only been studied in the exact case.\nIn this paper, we generalize an existing method to the noisy case, by\nintroducing a weight factor in the tensor decomposition. Finally, we apply the\nproposed weighted decoupling algorithm in the domain of system identification,\nand observe smaller model errors."
},{
    "category": "cs.CE", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1601.07941v1", 
    "title": "Automatic calibration of damping layers in finite element time domain   simulations", 
    "arxiv-id": "1601.07941v1", 
    "author": "Koen Van Den Abeele", 
    "publish": "2016-01-28T22:36:06Z", 
    "summary": "Matched layers are commonly used in numerical simulations of wave propagation\nto model (semi-)infinite domains. Attenuation functions describe the damping in\nlayers, and provide a matching of the wave impedance at the interface between\nthe domain of interest and the absorbing region. Selecting parameters in the\nattenuation functions is non-trivial. In this work, an optimisation procedure\nfor automatically calibrating matched layers is presented. The procedure is\nbased on solving optimisation problems constrained by partial differential\nequations with polynomial and piecewise-constant attenuation functions. We show\nexperimentally that, for finite element time domain simulations,\npiecewise-constant attenuation function are at least as efficient as quadratic\nattenuation functions. This observation leads us to introduce consecutive\nmatched layers as an alternative to perfectly matched layers, which can easily\nbe employed for problems with arbitrary geometries. Moreover, the use of\nconsecutive matched layers leads to a reduction in computational cost compared\nto perfectly matched layers. Examples are presented for acoustic, elastodynamic\nand electromagnetic problems. Numerical simulations are performed with the\nlibraries FEniCS/DOLFIN and dolfin-adjoint, and the computer code to reproduce\nall numerical examples is made freely available."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1601.08179v1", 
    "title": "Factorizing the factorization - a spectral-element solver for elliptic   equations with linear operation count", 
    "arxiv-id": "1601.08179v1", 
    "author": "Jochen Fr\u00f6hlich", 
    "publish": "2016-01-29T16:21:36Z", 
    "summary": "High-order methods gain more and more attention in computational fluid\ndynamics. However, the potential advantage of these methods depends critically\non the availability of efficient elliptic solvers. With spectral-element\nmethods, static condensation is a common approach to reduce the number of\ndegree of freedoms and to improve the condition of the algebraic equations. The\nresulting system is block-structured and the face-based operator well suited\nfor matrix-matrix multiplications. However, a straight-forward implementation\nscales super-linearly with the number of unknowns and, therefore, prohibits the\napplication to high polynomial degrees. This paper proposes a novel\nfactorization technique, which yields a linear operation count of just 13N\nmultiplications, where N is the total number of unknowns. In comparison to\nprevious work it saves a factor larger than 3 and clearly outpaces unfactored\nvariants for all polynomial degrees. Using the new technique as a building\nblock for a preconditioned conjugate gradient method resulted in a runtime\nscaling linearly with N for polynomial degrees $2 \\leq p \\leq 32$ . Moreover\nthe solver proved remarkably robust for aspect ratios up to 128."
},{
    "category": "math.OC", 
    "doi": "10.1137/16M1058467", 
    "link": "http://arxiv.org/pdf/1602.01506v1", 
    "title": "Level-set methods for convex optimization", 
    "arxiv-id": "1602.01506v1", 
    "author": "Scott Roy", 
    "publish": "2016-02-03T22:58:44Z", 
    "summary": "Convex optimization problems arising in applications often have favorable\nobjective functions and complicated constraints, thereby precluding first-order\nmethods from being immediately applicable. We describe an approach that\nexchanges the roles of the objective and constraint functions, and instead\napproximately solves a sequence of parametric level-set problems. A\nzero-finding procedure, based on inexact function evaluations and possibly\ninexact derivative information, leads to an efficient solution scheme for the\noriginal problem. We describe the theoretical and practical properties of this\napproach for a broad range of problems, including low-rank semidefinite\noptimization, sparse optimization, and generalized linear models for inference."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1602.01626v2", 
    "title": "Spectral deferred corrections with fast-wave slow-wave splitting", 
    "arxiv-id": "1602.01626v2", 
    "author": "Robert Speck", 
    "publish": "2016-02-04T10:45:39Z", 
    "summary": "The paper investigates a variant of semi-implicit spectral deferred\ncorrections (SISDC) in which the stiff, fast dynamics correspond to fast\npropagating waves (\"fast-wave slow-wave problem\"). We show that for a scalar\ntest problem with two imaginary eigenvalues $i \\lambda_{fast}$, $i\n\\lambda_{slow}$, having $\\Delta t \\left(\\left| \\lambda_{fast} \\right| + \\left|\n\\lambda_{slow} \\right| \\right) < 1$ is sufficient for the fast-wave slow-wave\nSDC (FWSW-SDC) iteration to converge and that in the limit of infinitely fast\nwaves the convergence rate of the non-split version is retained. Stability\nfunction and discrete dispersion relation are derived and show that the method\nis stable for essentially arbitrary fast-wave CFL numbers as long as the slow\ndynamics are resolved. The method causes little numerical diffusion and its\nsemi-discrete phase speed is accurate also for large wave number modes.\nPerformance is studied for an acoustic-advection problem and for the linearised\nBoussinesq equations, describing compressible, stratified flow. FWSW-SDC is\ncompared to a diagonally implicit Runge-Kutta (DIRK) and IMEX Runge-Kutta\n(IMEX) method and found to be competitive in terms of both accuracy and cost."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1602.01768v3", 
    "title": "Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion   Algorithms", 
    "arxiv-id": "1602.01768v3", 
    "author": "Peter Richt\u00e1rik", 
    "publish": "2016-02-04T18:08:22Z", 
    "summary": "We develop and analyze a broad family of stochastic/randomized algorithms for\ninverting a matrix. We also develop specialized variants maintaining symmetry\nor positive definiteness of the iterates. All methods in the family converge\nglobally and linearly (i.e., the error decays exponentially), with explicit\nrates. In special cases, we obtain stochastic block variants of several\nquasi-Newton updates, including bad Broyden (BB), good Broyden (GB),\nPowell-symmetric-Broyden (PSB), Davidon-Fletcher-Powell (DFP) and\nBroyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the first stochastic versions\nof these updates shown to converge to an inverse of a fixed matrix. Through a\ndual viewpoint we uncover a fundamental link between quasi-Newton updates and\napproximate inverse preconditioning. Further, we develop an adaptive variant of\nrandomized block BFGS, where we modify the distribution underlying the\nstochasticity of the method throughout the iterative process to achieve faster\nconvergence. By inverting several matrices from varied applications, we\ndemonstrate that AdaRBFGS is highly competitive when compared to the well\nestablished Newton-Schulz and minimal residual methods. In particular, on\nlarge-scale problems our method outperforms the standard methods by orders of\nmagnitude. Development of efficient methods for estimating the inverse of very\nlarge matrices is a much needed tool for preconditioning and variable metric\noptimization methods in the advent of the big data era."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1602.05950v1", 
    "title": "An Efficient, Sparsity-Preserving, Online Algorithm for Data   Approximation", 
    "arxiv-id": "1602.05950v1", 
    "author": "Ming Gu", 
    "publish": "2016-02-18T20:58:20Z", 
    "summary": "The Singular Value Decomposition (SVD) is a longstanding standard for data\napproximation because it is optimal in the 2 and Frobenius norms. The SVD,\nnevertheless, suffers from many setbacks, including computational cost, loss of\nsparsity in the decomposition, and the inability to be updated easily when new\ninformation arrives. Additionally, the SVD provides limited information on data\nfeatures and variables that best represent the data. In this work, we present a\ntruncated LU factorization called {\\bf Spectrum-Revealing LU} (SRLU) for\neffective low-rank matrix approximation, and develop the first algorithm to\ncompute an SRLU factorization, which is both efficient and reliable. Our\nalgorithm uses randomization and a novel LU updating technique with partial\npivoting, which is more stable than any other known LU updating algorithm. We\nprovide both approximation error bounds and singular value bounds for the SRLU\napproximation computed by our algorithm. Our analysis suggests that SRLU is\ncompetitive with the best low-rank matrix approximation methods, deterministic\nor randomized, in both computational complexity and approximation quality.\nNumeric experiments illustrate that SRLU preserves sparsity, highlights\nimportant data features and variables, can be efficiently updated, and\ncalculates data approximations nearly as accurately as the SVD. To the best of\nour knowledge this is the first practical variant of the LU decomposition for\nefficient and effective low-rank matrix approximation."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1602.08895v3", 
    "title": "New properties of a certain method of summation of generalized   hypergeometric series", 
    "arxiv-id": "1602.08895v3", 
    "author": "Pawe\u0142 Wo\u017any", 
    "publish": "2016-02-29T10:32:13Z", 
    "summary": "In a recent paper (Appl. Math. Comput. 215, 1622--1645, 2009), the authors\nproposed a method of summation of some slowly convergent series. The purpose of\nthis note is to give more theoretical analysis for this transformation,\nincluding the convergence acceleration theorem in the case of summation of\ngeneralized hypergeometric series. Some new theoretical results and\nillustrative numerical examples are given."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.00175v2", 
    "title": "The structure of the polynomials in preconditioned BiCG algorithms and   the switching direction of preconditioned systems", 
    "arxiv-id": "1603.00175v2", 
    "author": "Masaaki Sugihara", 
    "publish": "2016-03-01T08:14:33Z", 
    "summary": "We present a theorem that defines the direction of a preconditioned system\nfor the bi-conjugate gradient (BiCG) method, and we extend it to preconditioned\nbi-Lanczos-type algorithms. We show that the direction of a preconditioned\nsystem is switched by construction and by the settings of the initial shadow\nresidual vector. We analyze and compare the polynomial structures of four\npreconditioned BiCG algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.00176v2", 
    "title": "The structure of the Krylov subspace in various preconditioned CGS   algorithms", 
    "arxiv-id": "1603.00176v2", 
    "author": "Masaaki Sugihara", 
    "publish": "2016-03-01T08:15:52Z", 
    "summary": "An improved preconditioned conjugate gradient squared (PCGS) algorithm has\nrecently been proposed, and it performs much better than the conventional PCGS\nalgorithm. In this paper, the improved PCGS algorithm is verified as a\ncoordinative to the left-preconditioned system, and it has the advantages of\nboth the conventional and the left-PCGS; this is done by comparing, analyzing,\nand executing numerical examinations of various PCGS algorithms, including\nanother improved one. We show that the direction of the preconditioned system\nfor the CGS method is determined by the operations of $\\alpha_k$ and $\\beta_k$\nin the PCGS algorithm. By comparing the logical structures of these algorithms,\nwe show that the direction of the preconditioned system can be switched by the\nconstruction and setting of the initial shadow residual vector."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.00491v1", 
    "title": "Wanted: Floating-Point Add Round-off Error instruction", 
    "arxiv-id": "1603.00491v1", 
    "author": "Jason Riedy", 
    "publish": "2016-03-01T21:12:09Z", 
    "summary": "We propose a new instruction (FPADDRE) that computes the round-off error in\nfloating-point addition. We explain how this instruction benefits\nhigh-precision arithmetic operations in applications where double precision is\nnot sufficient. Performance estimates on Intel Haswell, Intel Skylake, and AMD\nSteamroller processors, as well as Intel Knights Corner co-processor,\ndemonstrate that such an instruction would improve the latency of double-double\naddition by up to 55% and increase double-double addition throughput by up to\n103%, with smaller, but non-negligible benefits for double-double\nmultiplication. The new instruction delivers up to 2x speedups on three\nbenchmarks that use high-precision floating-point arithmetic: double-double\nmatrix-matrix multiplication, compensated dot product, and polynomial\nevaluation via the compensated Horner scheme."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.01562v2", 
    "title": "A Data-Scalable Randomized Misfit Approach for Solving Large-Scale   PDE-Constrained Inverse Problems", 
    "arxiv-id": "1603.01562v2", 
    "author": "Quoc P. Nguyen", 
    "publish": "2016-03-04T18:26:27Z", 
    "summary": "A randomized misfit approach is presented for the efficient solution of\nlarge-scale PDE-constrained inverse problems with high-dimensional data. The\npurpose of this paper is to offer a theory-based framework for random\nprojections in this inverse problem setting. The stochastic approximation to\nthe misfit is analyzed using random projection theory. By expanding beyond mean\nestimator convergence, a practical characterization of randomized misfit\nconvergence can be achieved. The theoretical results developed hold with any\nvalid random projection in the literature. The class of feasible distributions\nis broad yet simple to characterize compared to previous stochastic misfit\nmethods. This class includes very sparse random projections which provide\nadditional computational benefit. A different proof for a variant of the\nJohnson-Lindenstrauss lemma is also provided. This leads to a different\nintuition for the $O(\\epsilon^{-2})$ factor in bounds for Johnson-Lindenstrauss\nresults. The main contribution of this paper is a theoretical result showing\nthe method guarantees a valid solution for small reduced misfit dimensions. The\ninterplay between Johnson-Lindenstrauss theory and Morozov's discrepancy\nprinciple is shown to be essential to the result. The computational cost\nsavings for large-scale PDE-constrained problems with high- dimensional data is\ndiscussed. Numerical verification of the developed theory is presented for\nmodel problems of estimating a distributed parameter in an elliptic partial\ndifferential equation. Results with different random projections are presented\nto demonstrate the viability and accuracy of the proposed approach."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.01793v1", 
    "title": "Coupling of finite element method with boundary algebraic equations", 
    "arxiv-id": "1603.01793v1", 
    "author": "A. V. Shanin", 
    "publish": "2016-03-06T06:21:30Z", 
    "summary": "Recently, a combined approach of CFIE--BAE has been proposed by authors for\nsolving external scattering problems in acoustics. CFIE stands for\ncombined-field integral equations, and BAE is the method of boundary\nalgebraical equation. The combined method is, essentially, a discrete analogue\nof the boundary element method (BEM), having none of its disadvantages. Namely,\ndue to the discrete nature of BAE one should not compute quadratures of\noversingular integrals. Moreover, due to CFIE formulation, the method does not\npossess spurious resonances.\n  However, the CFIE--BAE method has an important drawback. Since the modelling\nis performed in a regular discrete space, the shape of the obstacle should be\nassembled of elementary \"bricks\", so smooth scatterers (like spheres,\ncylinders, etc) are approximated with a poor accuracy. This loss of accuracy\nbecomes the bottleneck of the method. Here this disadvantage is overcome. The\nCFIE--BAE method developed for regular meshing of the outer space is coupled in\na standard way with a relatively small irregular mesh enabling one to describe\nthe shape of the obstacle accurately enough."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.03650v2", 
    "title": "A Penalty Function Promoting Sparsity Within and Across Groups", 
    "arxiv-id": "1603.03650v2", 
    "author": "Sava\u015fkan Bulek", 
    "publish": "2016-03-11T14:47:38Z", 
    "summary": "We introduce a new weakly-convex penalty function for signals with a group\nbehavior. The penalty promotes signals with a few number of active groups,\nwhere within each group, only a few high magnitude coefficients are active. We\nderive the threshold function associated with the proposed penalty and study\nits properties. We discuss how the proposed penalty/threshold function can be\nuseful for signals with isolated non-zeros, such as audio with isolated\nharmonics along the frequency axis, or reflection functions in exploration\nseismology where the non-zeros occur on the boundaries of subsoil layers. We\ndemonstrate the use of the proposed penalty/threshold functions in a convex\ndenoising and a non-convex deconvolution formulation. We provide convergent\nalgorithms for both formulations and compare the performance with\nstate-of-the-art methods."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.03945v1", 
    "title": "On the Petras algorithm for verified integration of piecewise analytic   functions", 
    "arxiv-id": "1603.03945v1", 
    "author": "Piotr Zgliczy\u0144ski", 
    "publish": "2016-03-12T17:24:34Z", 
    "summary": "We consider the algorithm for verified integration of piecewise analytic\nfunctions given by Petras. The analysis of the algorithm contained in Patras'\npaper is limited to a narrow class of functions and gives upper bounds only. We\npresent an estimation of the complexity (measured by a number of evaluations of\nan integrand) of the algorithm, both upper and lower bounds, for a wider class\nof functions. We show examples with complexity $\\Theta(|\\ln\\eps|/\\eps^{p-1})$,\nfor any $p >1$, where $\\eps$ is the desired accuracy of the computed integral."
},{
    "category": "math.OC", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.05719v2", 
    "title": "Efficient evaluation of scaled proximal operators", 
    "arxiv-id": "1603.05719v2", 
    "author": "Gabriel Goh", 
    "publish": "2016-03-17T22:53:22Z", 
    "summary": "Quadratic-support functions [Aravkin, Burke, and Pillonetto; J. Mach. Learn.\nRes. 14(1), 2013] constitute a parametric family of convex functions that\nincludes a range of useful regularization terms found in applications of convex\noptimization. We show how an interior method can be used to efficiently compute\nthe proximal operator of a quadratic-support function under different metrics.\nWhen the metric and the function have the right structure, the proximal map can\nbe computed with cost nearly linear in the input size. We describe how to use\nthis approach to implement quasi-Newton methods for a rich class of nonsmooth\nproblems that arise, for example, in sparse optimization, image denoising, and\nsparse logistic regression."
},{
    "category": "hep-lat", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.05988v2", 
    "title": "Deflation as a Method of Variance Reduction for Estimating the Trace of   a Matrix Inverse", 
    "arxiv-id": "1603.05988v2", 
    "author": "Kostas Orginos", 
    "publish": "2016-03-18T20:34:32Z", 
    "summary": "Many fields require computing the trace of the inverse of a large, sparse\nmatrix. The typical method used for such computations is the Hutchinson method\nwhich is a Monte Carlo (MC) averaging over matrix quadratures. To improve its\nconvergence, several variance reductions techniques have been proposed. In this\npaper, we study the effects of deflating the near null singular value space. We\nmake two main contributions.\n  First, we analyze the variance of the Hutchinson method as a function of the\ndeflated singular values and vectors. Although this provides good intuition in\ngeneral, by assuming additionally that the singular vectors are random unitary\nmatrices, we arrive at concise formulas for the deflated variance that include\nonly the variance and mean of the singular values. We make the remarkable\nobservation that deflation may increase variance for Hermitian matrices but not\nfor non-Hermitian ones. This is a rare, if not unique, property where\nnon-Hermitian matrices outperform Hermitian ones. The theory can be used as a\nmodel for predicting the benefits of deflation.\n  Second, we use deflation in the context of a large scale application of\n\"disconnected diagrams\" in Lattice QCD. On lattices, Hierarchical Probing (HP)\nhas previously provided an order of magnitude of variance reduction over MC by\nremoving \"error\" from neighboring nodes of increasing distance in the lattice.\nAlthough deflation used directly on MC yields a limited improvement of 30% in\nour problem, when combined with HP they reduce variance by a factor of over 60\ncompared to MC. For this, we pre-computated 1000 smallest singular values of an\nill-conditioned matrix of size 25 million. Using PRIMME and a domain-specific\nAlgebraic Multigrid preconditioner, we perform one of the largest eigenvalue\ncomputations in Lattice QCD at a fraction of the cost of our trace computation."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.06912v1", 
    "title": "A note on the convergence of nonconvex line search", 
    "arxiv-id": "1603.06912v1", 
    "author": "Hao Jiang", 
    "publish": "2016-03-19T14:53:54Z", 
    "summary": "In this note, we consider the line search for a class of abstract nonconvex\nalgorithm which have been deeply studied in the Kurdyka-Lojasiewicz theory. We\nprovide a weak convergence result of the line search in general. When the\nobjective function satisfies the Kurdyka-Lojasiewicz property and some certain\nassumption, a global convergence result can be derived. An application is\npresented for the L0-regularized least square minimization in the end of the\npaper."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.09133v3", 
    "title": "\"Compress and eliminate\" solver for symmetric positive definite sparse   matrices", 
    "arxiv-id": "1603.09133v3", 
    "author": "Ivan V. Oseledets", 
    "publish": "2016-03-30T11:27:01Z", 
    "summary": "We propose a new approximate factorization for solving linear systems with\nsymmetric positive definite sparse matrices. In a nutshell the algorithm is to\napply hierarchically block Gaussian elimination and additionally compress the\nfill-in. The systems that have efficient compression of the fill-in mostly\narise from discretization of partial differential equations. We show that the\nresulting factorization can be used as an efficient preconditioner and compare\nthe proposed approach with state-of-art direct and iterative solvers."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.09660v2", 
    "title": "Stable Isogeometric Analysis of Trimmed Geometries", 
    "arxiv-id": "1603.09660v2", 
    "author": "Thomas-Peter Fries", 
    "publish": "2016-03-31T16:19:05Z", 
    "summary": "We explore extended B-splines as a stable basis for isogeometric analysis\nwith trimmed parameter spaces. The stabilization is accomplished by an\nappropriate substitution of B-splines that may lead to ill-conditioned system\nmatrices. The construction for non-uniform knot vectors is presented. The\nproperties of extended B-splines are examined in the context of interpolation,\npotential, and linear elasticity problems and excellent results are attained.\nThe analysis is performed by an isogeometric boundary element formulation using\ncollocation. It is argued that extended B-splines provide a flexible and simple\nstabilization scheme which ideally suits the isogeometric paradigm."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1603.09678v2", 
    "title": "Conformal higher-order remeshing schemes for implicitly defined   interface problems", 
    "arxiv-id": "1603.09678v2", 
    "author": "Thomas-Peter Fries", 
    "publish": "2016-03-31T16:57:23Z", 
    "summary": "A new higher-order accurate method is proposed that combines the advantages\nof the classical $p$-version of the FEM on body-fitted meshes with embedded\ndomain methods. A background mesh composed by higher-order Lagrange elements is\nused. Boundaries and interfaces are described implicitly by the level set\nmethod and are within elements. In the elements cut by the boundaries or\ninterfaces, an automatic decomposition into higher-order accurate sub-elements\nis realized. Therefore, the zero level sets are detected and meshed in a first\nstep which is called reconstruction. Then, based on the topological situation\nin the cut element, higher-order sub-elements are mapped to the two sides of\nthe boundary or interface. The quality of the reconstruction and the mapping\nlargely determines the properties of the resulting, automatically generated\nconforming mesh. It is found that optimal convergence rates are possible\nalthough the resulting sub-elements are not always well-shaped."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.01376v1", 
    "title": "Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms", 
    "arxiv-id": "1604.01376v1", 
    "author": "Marc Sebban", 
    "publish": "2016-04-04T12:39:26Z", 
    "summary": "Many theoretical results in the machine learning domain stand only for\nfunctions that are Lipschitz continuous. Lipschitz continuity is a strong form\nof continuity that linearly bounds the variations of a function. In this paper,\nwe derive tight Lipschitz constants for two families of metrics: Mahalanobis\ndistances and bounded-space bilinear forms. To our knowledge, this is the first\ntime the Mahalanobis distance is formally proved to be Lipschitz continuous and\nthat such tight Lipschitz constants are derived."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.01713v1", 
    "title": "A block Recycled GMRES method with investigations into aspects of solver   performance", 
    "arxiv-id": "1604.01713v1", 
    "author": "Daniel B. Szyld", 
    "publish": "2016-04-06T18:07:19Z", 
    "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al. SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented block Krylov\nsubspace. We offer a clean derivation of the method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
},{
    "category": "stat.ML", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.02100v1", 
    "title": "Hankel Matrix Nuclear Norm Regularized Tensor Completion for   N-dimensional Exponential Signals", 
    "arxiv-id": "1604.02100v1", 
    "author": "Xiaobo Qu", 
    "publish": "2016-04-06T12:51:07Z", 
    "summary": "Signals are usually modeled as a superposition of exponential functions in\nspectroscopy of chemistry, biology and medical imaging. However, for fast data\nacquisition or other inevitable reasons, only a small amount of samples may be\nacquired. How to recover the full signal is then of great interest. Existing\napproaches can not efficiently recover N-dimensional exponential signals with\nN>=3. This paper studies the problem of recovering N-dimensional (particularly\n$N\\geq 3$) exponential signals from partial observations, and we formulate this\nproblem as a low-rank tensor completion problem with exponential factors. The\nfull signal is reconstructed by simultaneously exploiting the CANDECOMP/PARAFAC\n(CP) tensor decomposition and the exponential structure of the associated\nfactors, of which the latter is promoted by minimizing an objective function\ninvolving the nuclear norm of Hankel matrices. Experimental results on\nsimulated and real magnetic resonance spectroscopy data show that the proposed\napproach can successfully recover full signals from very limited samples and is\nrobust to the estimated tensor rank."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.04427v1", 
    "title": "A Singularly Perturbed Boundary Value Problems with Fractional Powers of   Elliptic Operators", 
    "arxiv-id": "1604.04427v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2016-04-15T10:55:47Z", 
    "summary": "A boundary value problem for a fractional power $0 < \\varepsilon < 1$ of the\nsecond-order elliptic operator is considered. The boundary value problem is\nsingularly perturbed when $\\varepsilon \\rightarrow 0$. It is solved numerically\nusing a time-dependent problem for a pseudo-parabolic equation. For the\nauxiliary Cauchy problem, the standard two-level schemes with weights are\napplied. The numerical results are presented for a model two-dimen\\-sional\nboundary value problem with a fractional power of an elliptic operator. Our\nwork focuses on the solution of the boundary value problem with $0 <\n\\varepsilon \\ll 1$."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.04443v1", 
    "title": "Iterative computational identification of a spacewise dependent the   source in a parabolic equations", 
    "arxiv-id": "1604.04443v1", 
    "author": "Petr N. Vabishchevich", 
    "publish": "2016-04-15T11:49:18Z", 
    "summary": "Coefficient inverse problems related to identifying the right-hand side of an\nequation with use of additional information is of interest among inverse\nproblems for partial differential equations. When considering non-stationary\nproblems, tasks of recovering the dependence of the right-hand side on time and\nspatial variables can be treated as independent. These tasks relate to a class\nof linear inverse problems, which sufficiently simplifies their study. This\nwork is devoted to a finding the dependence of right-hand side of\nmultidimensional parabolic equation on spatial variables using additional\nobservations of the solution at the final point of time - the final\noverdetermination. More general problems are associated with some integral\nobservation of the solution on time - the integral overdetermination. The first\nmethod of numerical solution of inverse problems is based on iterative solution\nof boundary value problem for time derivative with non-local acceleration. The\nsecond method is based on the known approach with iterative refinement of\ndesired dependence of the right-hand side on spacial variables. Capabilities of\nproposed methods are illustrated by numerical examples for model\ntwo-dimensional problem of identifying the right-hand side of a parabolic\nequation. The standard finite-element approximation on space is used, the time\ndiscretization is based on fully implicit two-level schemes."
},{
    "category": "cs.CE", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1604.05201v1", 
    "title": "Two-grid algorithms for singularly perturbed reaction-diffusion problems   on layer adapted meshes", 
    "arxiv-id": "1604.05201v1", 
    "author": "Lubin G. Vulkov", 
    "publish": "2016-04-18T15:15:01Z", 
    "summary": "We propose a new two-grid approach based on Bellman-Kalaba quasilinearization\nand Axelsson-Xu finite element two-grid method for the solution of singularly\nperturbed reaction-diffusion equations. The algorithms involve solving one\ninexpensive problem on coarse grid and solving on fine grid one linear problem\nobtained by quasilinearization of the differential equation about an\ninterpolant of the computed solution on the coarse grid. Different meshes (of\nBakhvalov, Shishkin and Vulanovi\\'c types) are examined. All the schemes are\nuniformly convergent with respect to the small parameter. We show theoretically\nand numerically that the global error of the two-grid method is the same as of\nthe nonlinear problem solved directly on the fine layer-adapted mesh."
},{
    "category": "cs.CG", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1605.02626v1", 
    "title": "Low-order continuous finite element spaces on hybrid non-conforming   hexahedral-tetrahedral meshes", 
    "arxiv-id": "1605.02626v1", 
    "author": "Bruno L\u00e9vy", 
    "publish": "2016-05-09T15:42:30Z", 
    "summary": "This article deals with solving partial differential equations with the\nfinite element method on hybrid non-conforming hexahedral-tetrahedral meshes.\nBy non-conforming, we mean that a quadrangular face of a hexahedron can be\nconnected to two triangular faces of tetrahedra. We introduce a set of\nlow-order continuous (C0) finite element spaces defined on these meshes. They\nare built from standard tri-linear and quadratic Lagrange finite elements with\nan extra set of constraints at non-conforming hexahedra-tetrahedra junctions to\nrecover continuity. We consider both the continuity of the geometry and the\ncontinuity of the function basis as follows: the continuity of the geometry is\nachieved by using quadratic mappings for tetrahedra connected to tri-affine\nhexahedra and the continuity of interpolating functions is enforced in a\nsimilar manner by using quadratic Lagrange basis on tetrahedra with constraints\nat non-conforming junctions to match tri-linear hexahedra. The so-defined\nfunction spaces are validated numerically on simple Poisson and linear\nelasticity problems for which an analytical solution is known. We observe that\nusing a hybrid mesh with the proposed function spaces results in an accuracy\nsignificantly better than when using linear tetrahedra and slightly worse than\nwhen solely using tri-linear hexahedra. As a consequence, the proposed function\nspaces may be a promising alternative for complex geometries that are out of\nreach of existing full hexahedral meshing methods."
},{
    "category": "physics.flu-dyn", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1605.07658v2", 
    "title": "Modeling flow in porous media with double porosity/permeability:   Mathematical model, properties, and analytical solutions", 
    "arxiv-id": "1605.07658v2", 
    "author": "R. Ballarini", 
    "publish": "2016-05-20T21:26:40Z", 
    "summary": "Geo-materials such as vuggy carbonates are known to exhibit multiple spatial\nscales. A common manifestation of spatial scales is the presence of (at least)\ntwo different scales of pores, which is commonly referred to as double\nporosity. To complicate things, the pore-network at each scale exhibits\ndifferent permeability, and these networks are connected through fissure and\nconduits. Although some models are available in the literature, they lack a\nstrong theoretical basis. This paper aims to fill this lacuna by providing the\nmuch needed theoretical foundations of the flow in porous media which exhibit\ndouble porosity/permeability. We first obtain a mathematical model for double\nporosity/permeability using the maximization of rate of dissipation hypothesis,\nand thereby providing a firm thermodynamic underpinning. We then present, along\nwith mathematical proofs, several important mathematical properties that the\nsolutions to the double porosity/permeability model satisfy. These properties\nare important in their own right as well as serve as good (mechanics-based) a\nposteriori measures to assess the accuracy of numerical solutions. We also\npresent several canonical problems and obtain the corresponding analytical\nsolutions, which are used to gain insights into the velocity and pressure\nprofiles, and the mass transfer across the two pore-networks. In particular, we\nhighlight how the solutions under the double porosity/permeability differ from\nthe corresponding solutions under Darcy equations."
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1605.07859v1", 
    "title": "How Many Real Attractive Fixed Points Can A Polynomial Have?", 
    "arxiv-id": "1605.07859v1", 
    "author": "Bahman Kalantari", 
    "publish": "2016-05-25T12:42:04Z", 
    "summary": "We prove a complex polynomial of degree $n$ has at most $\\lceil n/2 \\rceil$\nattractive fixed points lying on a line. We also consider the general case."
},{
    "category": "math.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1605.08771v1", 
    "title": "Enhancing the Performance and Robustness of the FEAST Eigensolver", 
    "arxiv-id": "1605.08771v1", 
    "author": "Eric Polizzi", 
    "publish": "2016-05-27T19:54:39Z", 
    "summary": "The FEAST algorithm is a subspace iteration method that uses a spectral\nprojector as a rational filter in order to efficiently solve interior\neigenvalue problems in parallel. Although the solutions from the FEAST\nalgorithm converge rapidly in many cases, convergence can be slow in situations\nwhere the eigenvalues of a matrix are densely populated near the edges of the\nsearch interval of interest, which can be detrimental to parallel load\nbalancing. This work introduces two methods that allow one to improve the\nconvergence robustness of the FEAST algorithm in these situations without\nhaving to increase the amount of computation. Selected numerical examples are\npresented and discussed"
},{
    "category": "cs.NA", 
    "doi": "10.1137/16M1060078", 
    "link": "http://arxiv.org/pdf/1606.00251v1", 
    "title": "Profile-Driven Automated Mixed Precision", 
    "arxiv-id": "1606.00251v1", 
    "author": "Xiaobai Sun", 
    "publish": "2016-06-01T12:27:54Z", 
    "summary": "We present a scheme to automatically set the precision of floating point\nvariables in an application. We design a framework that profiles applications\nto measure undesirable numerical behavior at the floating point operation\nlevel. We use this framework to perform mixed precision analysis to\nheuristically set the precision of all variables in an application based on\ntheir numerical profiles. We experimentally evaluate the mixed precision\nanalysis to show that it can generate a range of results with different\naccuracy and performance characteristics."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.00785v2", 
    "title": "Piano Transcription in the Studio Using an Extensible Alternating   Directions Framework", 
    "arxiv-id": "1606.00785v2", 
    "author": "Mark Sandler", 
    "publish": "2016-06-02T18:15:34Z", 
    "summary": "Given a musical audio recording, the goal of automatic music transcription is\nto determine a score-like representation of the piece underlying the recording.\nDespite significant interest within the research community, several studies\nhave reported on a 'glass ceiling' effect, an apparent limit on the\ntranscription accuracy that current methods seem incapable of overcoming. In\nthis paper, we explore how much this effect can be mitigated by focusing on a\nspecific instrument class and making use of additional information on the\nrecording conditions available in studio or home recording scenarios. In\nparticular, exploiting the availability of single note recordings for the\ninstrument in use we develop a novel signal model employing variable-length\nspectro-temporal patterns as its central building blocks - tailored for pitched\npercussive instruments such as the piano. Temporal dependencies between\nspectral templates are modeled, resembling characteristics of factorial scaled\nhidden Markov models (FS-HMM) and other methods combining Non-Negative Matrix\nFactorization with Markov processes. In contrast to FS-HMMs, our parameter\nestimation is developed in a global, relaxed form within the extensible\nalternating direction method of multipliers (ADMM) framework, which enables the\nsystematic combination of basic regularizers propagating sparsity and local\nstationarity in note activity with more complex regularizers imposing temporal\nsemantics. The proposed method achieves an f-measure of 93-95% for note onsets\non pieces recorded on a Yamaha Disklavier (MAPS DB)."
},{
    "category": "math.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.01396v1", 
    "title": "Completion of Newton's Iterations Initialized at a Quasi-Universal Set", 
    "arxiv-id": "1606.01396v1", 
    "author": "Victor Y. Pan", 
    "publish": "2016-06-04T16:58:37Z", 
    "summary": "Recently Schleicher and Stoll proposed efficient initialization of Newton's\niterations. Given a black box subroutine for the evaluation of the Newton's\nratio of a polynomial and its derivative, their algorithm very fast\napproximates all roots of a univariate polynomial except for a small fraction\nof them. Our recipes for the approximation of the remaining roots within the\nsame asymptotic computational cost should answer the authors' challenge. Our\nwork can be of independent interest as an example of synergistic variation and\ncombination of polynomial root-finding methods towards enhancing their power."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.01500v1", 
    "title": "Efficient tensor completion for color image and video recovery: Low-rank   tensor train", 
    "arxiv-id": "1606.01500v1", 
    "author": "Minh N. Do", 
    "publish": "2016-06-05T12:09:19Z", 
    "summary": "This paper proposes a novel approach to tensor completion, which recovers\nmissing entries of data represented by tensors. The approach is based on the\ntensor train (TT) rank, which is able to capture hidden information from\ntensors thanks to its definition from a well-balanced matricization scheme.\nAccordingly, new optimization formulations for tensor completion are proposed\nas well as two new algorithms for their solution. The first one called simple\nlow-rank tensor completion via tensor train (SiLRTC-TT) is intimately related\nto minimizing a nuclear norm based on TT rank. The second one is from a\nmultilinear matrix factorization model to approximate the TT rank of a tensor,\nand is called tensor completion by parallel matrix factorization via tensor\ntrain (TMac-TT). A tensor augmentation scheme of transforming a low-order\ntensor to higher-orders is also proposed to enhance the effectiveness of\nSiLRTC-TT and TMac-TT. Simulation results for color image and video recovery\nshow the clear advantage of our method over all other methods."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.02468v1", 
    "title": "An efficient mathematically correct scale free CORDIC", 
    "arxiv-id": "1606.02468v1", 
    "author": "Younes Lahbib", 
    "publish": "2016-06-08T09:36:11Z", 
    "summary": "In order to approximate transandental functions, several algorithms were\nproposed.Historically, polynomial interpolation, infinite series, $\\cdots$ and\nother$+,\\times, -$ and $/$ based algorithms were studied for this purpose.The\nCORDIC (COordinate Rotation DIgital Computer)introduced by Jack E. Volder in\n1959, and generalized by J. S. Walther a few years later, is a hardware based\nalgorithmfor the approximation of trigonometric, hyperbolic andlogarithmic\nfunctions.As a consequence, CORDIC is used for applications indiverse areas\nsuch as signal and image processing.For these reasons, several modified\nversions were proposed.In this article, we present anoverview of the CORDIC\nalgorithm for the computation of the circular functions, essentially the\nscaling free version,and we will give a substential improvement to the commonly\nused one."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.02959v3", 
    "title": "Isogeometric computation reuse method for complex objects with   topology-consistent volumetric parameterization", 
    "arxiv-id": "1606.02959v3", 
    "author": "Charlie C. L. Wang", 
    "publish": "2016-06-09T13:27:47Z", 
    "summary": "Volumetric spline parameterization and computational efficiency are two main\nchallenges in isogeometric analysis (IGA). To tackle this problem, we propose a\nframework of computation reuse in IGA on a set of three-dimensional models with\nsimilar semantic features. Given a template domain, B-spline based consistent\nvolumetric parameterization is first constructed for a set of models with\nsimilar semantic features. An efficient quadrature-free method is investigated\nin our framework to compute the entries of stiffness matrix by Bezier\nextraction and polynomial approximation. In our approach, evaluation on the\nstiffness matrix and imposition of the boundary conditions can be pre-computed\nand reused during IGA on a set of CAD models. Examples with complex geometry\nare presented to show the effectiveness of our methods, and efficiency similar\nto the computation in linear finite element analysis can be achieved for IGA\ntaken on a set of models."
},{
    "category": "math.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.03135v3", 
    "title": "The Overlapped Radial Basis Function-Finite Difference (RBF-FD) Method:   A Generalization of RBF-FD", 
    "arxiv-id": "1606.03135v3", 
    "author": "Varun Shankar", 
    "publish": "2016-06-09T22:20:50Z", 
    "summary": "We present a generalization of the RBF-FD method that computes RBF-FD weights\nin finite-sized neighborhoods around the centers of RBF-FD stencils by\nintroducing an overlap parameter $\\delta \\in [0,1]$ such that $\\delta=1$\nrecovers the standard RBF-FD method and $\\delta=0$ results in a full decoupling\nof stencils. We provide experimental evidence to support this generalization,\nand develop an automatic stabilization procedure based on local Lebesgue\nfunctions for the stable selection of stencil weights over a wide range of\n$\\delta$ values. We provide an a priori estimate for the speedup of our method\nover RBF-FD that serves as a good predictor for the true speedup. We apply our\nmethod to parabolic partial differential equations with time-dependent\ninhomogeneous boundary conditions-- Neumann in 2D, and Dirichlet in 3D. Our\nresults show that our method can achieve as high as a 60x speedup in 3D over\nexisting RBF-FD methods in the task of forming differentiation matrices."
},{
    "category": "math.OC", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.04970v2", 
    "title": "The non-convex Burer-Monteiro approach works on smooth semidefinite   programs", 
    "arxiv-id": "1606.04970v2", 
    "author": "Afonso S. Bandeira", 
    "publish": "2016-06-15T20:31:29Z", 
    "summary": "Semidefinite programs (SDP's) can be solved in polynomial time by interior\npoint methods, but scalability can be an issue. To address this shortcoming,\nover a decade ago, Burer and Monteiro proposed to solve SDP's with few equality\nconstraints via rank-restricted, non-convex surrogates. Remarkably, for some\napplications, local optimization methods seem to converge to global optima of\nthese non-convex surrogates reliably. Although some theory supports this\nempirical success, a complete explanation of it remains an open question. In\nthis paper, we consider a class of SDP's which includes applications such as\nmax-cut, community detection in the stochastic block model, robust PCA, phase\nretrieval and synchronization of rotations. We show that the low-rank\nBurer-Monteiro formulation of SDP's in that class almost never has any spurious\nlocal optima."
},{
    "category": "cs.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.05556v2", 
    "title": "A critical analysis of some popular methods for the calculation of the   gradient in finite volume methods, with suggestions for improvements", 
    "arxiv-id": "1606.05556v2", 
    "author": "John Tsamopoulos", 
    "publish": "2016-06-17T15:18:08Z", 
    "summary": "Finite volume methods, especially methods for complex geometries which employ\ncurvilinear or unstructured grids, often make significant use of numerical\nschemes for computing the gradients of the dependent variables. Such gradient\nschemes may also have standalone uses, e.g.\\ in postprocessing. The most\npopular such schemes rely on the divergence theorem and on least squares\nminimisation. This study aims to raise awareness of the fact that the former\nshould be used with caution as it does not always converge to the exact\ngradient with grid refinement, whereas the latter is always at least\nfirst-order accurate. Furthermore, the issue of the best choice of weights for\nthe least squares method is investigated both theoretically and through\nnumerical experiments. It is proved that weighing each equation by the distance\nbetween the cell centres raised to the power $-3/2$ achieves second-order\naccuracy in some cases where the usual weights fail to do so, such as at\nboundary cells of structured grids. Finally, the schemes are used within a\nfinite volume method to solve a simple diffusion equation on a series of highly\ndistorted grids of increasing fineness, revealing that the inconsistency of the\n\"divergence theorem\" (or \"Green-Gauss\") gradient scheme propagates into the\nsolution of the partial differential equation, whereas the \"least squares\"\ngradient scheme produces second-order accurate results. These results are\nreproduced also with the popular public domain PDE solver OpenFOAM, which uses\nthe Green-Gauss gradient scheme by default."
},{
    "category": "math.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.06035v1", 
    "title": "Multilevel Evaluation of Multidimensional Integral Transforms with   Asymptotically Smooth Kernels", 
    "arxiv-id": "1606.06035v1", 
    "author": "C. H. Venner", 
    "publish": "2016-06-20T09:41:46Z", 
    "summary": "In many practical applications of numerical methods a substantial increase in\nefficiency can be obtained by using local grid refinement, since the solution\nis generally smooth in large parts of the domain and large gradients occur only\nlocally. Fast evaluation of integral transforms on such an adaptive grid\nrequires an algorithm that relies on the smoothness of the continuum kernel\nonly, independent of its discrete form. A multilevel algorithm with this\nproperty was presented in [A. Brandt and C.H. Venner, SIAM J. Sci. Stat.\nComput. 19 (1998) pp.468-492] [Bra1998]. Ref. [Bra1998] shows that already on a\nuniform grid the new algorithm is more efficient than earlier fast evaluation\nalgorithms, and elaborates the application to one-dimensional transforms. The\npresent work analyses the extension and implementation of the algorithm for\nmultidimensional transforms. The analysis conveys that the multidimensional\nextension is nontrivial, on account of the occurence of nonlocal corrections.\nHowever, by virtue of the asymptotic smoothness properties of the continuum\nkernel, these corrections can again be evaluated fast. By recursion, it is then\npossible to obtain the optimal work estimates indicated in [Bra1998].\nCurrently, only uniform grids are considered. Detailed numerical results will\nbe presented for a two dimensional model problem. The results demonstrate that\nwith the new algorithm the evaluation of multidimensional transforms is also\nmore efficient than with previous algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.06511v1", 
    "title": "Literature survey on low rank approximation of matrices", 
    "arxiv-id": "1606.06511v1", 
    "author": "Jan Shneider", 
    "publish": "2016-06-21T11:02:01Z", 
    "summary": "Low rank approximation of matrices has been well studied in literature.\nSingular value decomposition, QR decomposition with column pivoting, rank\nrevealing QR factorization (RRQR), Interpolative decomposition etc are\nclassical deterministic algorithms for low rank approximation. But these\ntechniques are very expensive $(O(n^{3})$ operations are required for $n\\times\nn$ matrices). There are several randomized algorithms available in the\nliterature which are not so expensive as the classical techniques (but the\ncomplexity is not linear in n). So, it is very expensive to construct the low\nrank approximation of a matrix if the dimension of the matrix is very large.\nThere are alternative techniques like Cross/Skeleton approximation which gives\nthe low-rank approximation with linear complexity in n . In this article we\nreview low rank approximation techniques briefly and give extensive references\nof many techniques."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.07315v3", 
    "title": "Nearly-optimal Robust Matrix Completion", 
    "arxiv-id": "1606.07315v3", 
    "author": "Prateek Jain", 
    "publish": "2016-06-23T13:57:56Z", 
    "summary": "In this paper, we consider the problem of Robust Matrix Completion (RMC)\nwhere the goal is to recover a low-rank matrix by observing a small number of\nits entries out of which a few can be arbitrarily corrupted. We propose a\nsimple projected gradient descent method to estimate the low-rank matrix that\nalternately performs a projected gradient descent step and cleans up a few of\nthe corrupted entries using hard-thresholding. Our algorithm solves RMC using\nnearly optimal number of observations as well as nearly optimal number of\ncorruptions. Our result also implies significant improvement over the existing\ntime complexity bounds for the low-rank matrix completion problem. Finally, an\napplication of our result to the robust PCA problem (low-rank+sparse matrix\nseparation) leads to nearly linear time (in matrix dimensions) algorithm for\nthe same; existing state-of-the-art methods require quadratic time. Our\nempirical results corroborate our theoretical results and show that even for\nmoderate sized problems, our method for robust PCA is an an order of magnitude\nfaster than the existing methods."
},{
    "category": "math.NA", 
    "doi": "10.1137/15M1026535", 
    "link": "http://arxiv.org/pdf/1606.07643v1", 
    "title": "Stochastic Modeling and Regularity of the Nonlinear Elliptic Curl-Curl   Equation", 
    "arxiv-id": "1606.07643v1", 
    "author": "Thomas Weiland", 
    "publish": "2016-06-24T11:24:43Z", 
    "summary": "This paper addresses the nonlinear elliptic curl-curl equation with\nuncertainties in the material law. It is frequently employed in the numerical\nevaluation of magnetostatic fields, where the uncertainty is ascribed to the\nso-called B-H curve. A truncated Karhunen-Lo\\`eve approximation of the\nstochastic B-H curve is presented and analyzed with regard to monotonicity\nconstraints. A stochastic non-linear curl-curl formulation is introduced and\nnumerically approximated by a finite element and collocation method in the\ndeterministic and stochastic variable, respectively. The stochastic regularity\nis analyzed by a higher order sensitivity analysis. It is shown that, unlike to\nlinear and several nonlinear elliptic problems, the solution is not analytic\nwith respect to the random variables and an algebraic decay of the stochastic\nerror is obtained. Numerical results for both the Karhunen-Lo\\`eve expansion\nand the stochastic curl-curl equation are given for illustration."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1026535", 
    "link": "http://arxiv.org/pdf/1607.00648v4", 
    "title": "Quasi-matrix-free hybrid multigrid on dynamically adaptive Cartesian   grids", 
    "arxiv-id": "1607.00648v4", 
    "author": "Tobias Weinzierl", 
    "publish": "2016-07-03T14:54:45Z", 
    "summary": "We introduce a family of spacetree-based multigrid realizations using the\ntree's multiscale nature to derive coarse grids. They align with matrix-free\ngeometric multigrid solvers as they never assemble the system matrices which is\ncumbersome for dynamically adaptive grids and full multigrid. The most\nsophisticated realizations use BoxMG which defines operator-dependent\nprolongation and restriction in combination with Galerkin/Petrov-Galerkin\ncoarse-grid operators. This yields robust solvers for nontrivial elliptic\nproblems. We propose to embed the algebraic, problem- and grid-dependent\nmultigrid operators as stencils into the grid and to evaluate all matrix-vector\nproducts in-situ throughout the grid traversals. While such a realization idiom\nis not literally matrix-free---the grid carries the matrix---we propose to\nswitch to a hierarchical representation of all operators. Only differences of\nalgebraic operators to their geometric counterparts are held. These\nhierarchical differences can be stored and exchanged with small memory\nfootprint. Our realizations support arbitrary dynamically adaptive grids while\nthey vertically integrate the multilevel operations through spacetree\nlinearization. This yields good memory access characteristics, while standard\ncolouring of mesh entities with domain decomposition allow us to use parallel\nmanycore clusters. Our techniques yield reasonably mature and robust\nimplementation blueprints for non-trivial multigrid solver parts running on\nparallel machines."
},{
    "category": "cs.CE", 
    "doi": "10.1137/15M1026535", 
    "link": "http://arxiv.org/pdf/1607.00968v2", 
    "title": "Full waveform inversion guided by travel time tomography", 
    "arxiv-id": "1607.00968v2", 
    "author": "Eldad Haber", 
    "publish": "2016-07-04T17:33:05Z", 
    "summary": "Full waveform inversion (FWI) is a process in which seismic data is fit by\nchanging the velocity of the media under investigation. The problem is\nnon-linear, and therefore optimization techniques have been used to find a\nreasonable solution to the problem. The main problem in fitting the data is the\nlack of low spatial frequencies. This deficiency often leads to a local minimum\nand to non-plausible solutions. In this work we explore how to obtain low\nfrequency information for FWI. Our approach involves augmenting FWI with travel\ntime tomography, which has low-frequency features. By jointly inverting these\ntwo problems we enrich FWI with information that can replace low frequency\ndata. In addition, we use high order regularization, in a preliminary inversion\nstage, to prevent high frequency features from polluting our model in the\ninitial stages of the reconstruction. This regularization also promotes the\nnon-dominant low-frequency modes that exist in the FWI sensitivity. By applying\na joint FWI and travel time inversion we are able to obtain a smooth model than\ncan later be used to recover a good approximation for the true model. A second\ncontribution of this paper involves the acceleration of the main computational\nbottleneck in FWI--the solution of the Helmholtz equation. We show that the\nsolution time can be reduced by solving the equation for multiple right hand\nsides using block multigrid preconditioned Krylov methods."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.jcp.2016.08.012", 
    "link": "http://arxiv.org/pdf/1607.00973v3", 
    "title": "A fast marching algorithm for the factored eikonal equation", 
    "arxiv-id": "1607.00973v3", 
    "author": "Eldad Haber", 
    "publish": "2016-07-04T17:46:45Z", 
    "summary": "The eikonal equation is instrumental in many applications in several fields\nranging from computer vision to geoscience. This equation can be efficiently\nsolved using the iterative Fast Sweeping (FS) methods and the direct Fast\nMarching (FM) methods. However, when used for a point source, the original\neikonal equation is known to yield inaccurate numerical solutions, because of a\nsingularity at the source. In this case, the factored eikonal equation is often\npreferred, and is known to yield a more accurate numerical solution. One\napplication that requires the solution of the eikonal equation for point\nsources is travel time tomography. This inverse problem may be formulated using\nthe eikonal equation as a forward problem. While this problem has been solved\nusing FS in the past, the more recent choice for applying it involves FM\nmethods because of the efficiency in which sensitivities can be obtained using\nthem. However, while several FS methods are available for solving the factored\nequation, the FM method is available only for the original eikonal equation.\n  In this paper we develop a Fast Marching algorithm for the factored eikonal\nequation, using both first and second order finite-difference schemes. Our\nalgorithm follows the same lines as the original FM algorithm and requires the\nsame computational effort. In addition, we show how to obtain sensitivities\nusing this FM method and apply travel time tomography, formulated as an inverse\nfactored eikonal equation. Numerical results in two and three dimensions show\nthat our algorithm solves the factored eikonal equation efficiently, and\ndemonstrate the achieved accuracy for computing the travel time. We also\ndemonstrate a recovery of a 2D and 3D heterogeneous medium by travel time\ntomography using the eikonal equation for forward modelling and inversion by\nGauss-Newton."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.08.012", 
    "link": "http://arxiv.org/pdf/1607.02548v2", 
    "title": "On the non-uniqueness of the instantaneous frequency", 
    "arxiv-id": "1607.02548v2", 
    "author": "Thomas Y. Hou", 
    "publish": "2016-07-08T23:27:47Z", 
    "summary": "In this article, we investigate the debated Instantaneous Frequency (IF)\ntopic. Here, we show that IF is non-unique inherently. We explain how this\nnon-uniqueness can be quantified and explained from a mathematical perspective.\nThe non-uniqueness of the IF can also be observed if different methods of\nadaptive signal processing are used. We will also show that even if we know the\nphysical origin of an oscillatory signal, e.g. linear second order ordinary\ndifferential equation, the non-uniqueness is still present. All in all, we will\nend up with the conclusion that, without any a priori assumption about the\nrelationship of the envelope and phase function of an oscillatory signal, there\nis not any preferred neither best representation of the IF of such oscillatory\nsignal."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.08.012", 
    "link": "http://arxiv.org/pdf/1607.03092v1", 
    "title": "Inexact Block Coordinate Descent Methods For Symmetric Nonnegative   Matrix Factorization", 
    "arxiv-id": "1607.03092v1", 
    "author": "Meisam Razaviyayn", 
    "publish": "2016-07-11T19:48:41Z", 
    "summary": "Symmetric nonnegative matrix factorization (SNMF) is equivalent to computing\na symmetric nonnegative low rank approximation of a data similarity matrix. It\ninherits the good data interpretability of the well-known nonnegative matrix\nfactorization technique and have better ability of clustering nonlinearly\nseparable data. In this paper, we focus on the algorithmic aspect of the SNMF\nproblem and propose simple inexact block coordinate decent methods to address\nthe problem, leading to both serial and parallel algorithms. The proposed\nalgorithms have guaranteed stationary convergence and can efficiently handle\nlarge-scale and/or sparse SNMF problems. Extensive simulations verify the\neffectiveness of the proposed algorithms compared to recent state-of-the-art\nalgorithms."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.jcp.2016.08.012", 
    "link": "http://arxiv.org/pdf/1607.05073v1", 
    "title": "Higher-Order Block Term Decomposition for Spatially Folded fMRI Data", 
    "arxiv-id": "1607.05073v1", 
    "author": "Sergios Theodoridis", 
    "publish": "2016-07-15T09:28:48Z", 
    "summary": "The growing use of neuroimaging technologies generates a massive amount of\nbiomedical data that exhibit high dimensionality. Tensor-based analysis of\nbrain imaging data has been proved quite effective in exploiting their multiway\nnature. The advantages of tensorial methods over matrix-based approaches have\nalso been demonstrated in the characterization of functional magnetic resonance\nimaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped\n(unfolded) as a single way/mode of the 3-rd order array, the other two ways\ncorresponding to time and subjects. However, such methods are known to be\nineffective in more demanding scenarios, such as the ones with strong noise\nand/or significant overlapping of activated regions. This paper aims at\ninvestigating the possible gains from a better exploitation of the spatial\ndimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.\nIn this context, and in order to increase the degrees of freedom of the\nmodeling process, a higher-order Block Term Decomposition (BTD) is applied, for\nthe first time in fMRI analysis. Its effectiveness is demonstrated via\nextensive simulation results."
},{
    "category": "cs.CC", 
    "doi": "10.1016/j.jcp.2016.08.012", 
    "link": "http://arxiv.org/pdf/1608.00135v1", 
    "title": "Computational complexity of solving polynomial differential equations   over unbounded domains with non-rational coefficients", 
    "arxiv-id": "1608.00135v1", 
    "author": "Amaury Pouly", 
    "publish": "2016-07-30T15:57:16Z", 
    "summary": "In this note, we extend the result of \\cite{PoulyG16} about the complexity of\nsolving polynomial differential equations over unbounded domains to work with\nnon-rational input. In order to deal with arbitrary input, we phrase the result\nin framework of Conputable Analysis \\cite{Ko91}. As a side result, we also get\na uniform result about complexity of the operator, and not just about the\nsolution."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.00514v1", 
    "title": "Dimensionality reduction based on Distance Preservation to Local Mean   (DPLM) for SPD matrices and its application in BCI", 
    "arxiv-id": "1608.00514v1", 
    "author": "Khadijeh Sadatnejad", 
    "publish": "2016-07-29T15:17:16Z", 
    "summary": "In this paper, we propose a nonlinear dimensionality reduction algorithm for\nthe manifold of Symmetric Positive Definite (SPD) matrices that considers the\ngeometry of SPD matrices and provides a low dimensional representation of the\nmanifold with high class discrimination. The proposed algorithm, tries to\npreserve the local structure of the data by preserving distance to local mean\n(DPLM) and also provides an implicit projection matrix. DPLM is linear in terms\nof the number of training samples and may use the label information when they\nare available in order to performance improvement in classification tasks. We\nperformed several experiments on the multi-class dataset IIa from BCI\ncompetition IV. The results show that our approach as dimensionality reduction\ntechnique - leads to superior results in comparison with other competitor in\nthe related literature because of its robustness against outliers. The\nexperiments confirm that the combination of DPLM with FGMDM as the classifier\nleads to the state of the art performance on this dataset."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.01372v1", 
    "title": "Permutation NMF", 
    "arxiv-id": "1608.01372v1", 
    "author": "Giovanni Barbarino", 
    "publish": "2016-08-03T21:59:44Z", 
    "summary": "Nonnegative Matrix Factorization(NMF) is a common used technique in machine\nlearning to extract features out of data such as text documents and images\nthanks to its natural clustering properties. In particular, it is popular in\nimage processing since it can decompose several pictures and recognize common\nparts if they're located in the same position over the photos. This paper's aim\nis to present a way to add the translation invariance to the classical NMF,\nthat is, the algorithms presented are able to detect common features, even when\nthey're shifted, in different original images."
},{
    "category": "math.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.02461v1", 
    "title": "A Matrix-free Preconditioner for the Helmholtz Equation based on the   Fast Multipole Method", 
    "arxiv-id": "1608.02461v1", 
    "author": "David Keyes", 
    "publish": "2016-08-08T14:48:25Z", 
    "summary": "Fast multipole methods (FMM) were originally developed for accelerating\n$N$-body problems for particle-based methods. FMM is more than an $N$-body\nsolver, however. Recent efforts to view the FMM as an elliptic Partial\nDifferential Equation (PDE) solver have opened the possibility to use it as a\npreconditioner for a broader range of applications. FMM can solve Helmholtz\nproblems with optimal $\\mathcal{O}(N \\log N)$ complexity, has compute-bound\ninner kernels, and highly asynchronous communication patterns. The combination\nof these features makes FMM an interesting candidate as a preconditioner for\nsparse solvers on architectures of the future. The use of FMM as a\npreconditioner allows us to use lower order multipole expansions than would be\nrequired as a solver because individual solves need not be accurate. This\nreduces the amount of computation and communication significantly and makes the\ntime-to-solution competitive with state-of-the-art preconditioners.\nFurthermore, the high asynchronicity of FMM allows it to scale to much larger\ncore counts than factorization-based and multilevel methods. We describe our\ntests in reproducible details with freely available codes."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.02702v1", 
    "title": "Steerable Principal Components for Space-Frequency Localized Images", 
    "arxiv-id": "1608.02702v1", 
    "author": "Yoel Shkolnisky", 
    "publish": "2016-08-09T06:35:09Z", 
    "summary": "This paper describes a fast and accurate method for obtaining steerable\nprincipal components from a large dataset of images, assuming the images are\nwell localized in space and frequency. The obtained steerable principal\ncomponents are optimal for expanding the images in the dataset and all of their\nrotations. The method relies upon first expanding the images using a series of\ntwo-dimensional Prolate Spheroidal Wave Functions (PSWFs), where the expansion\ncoefficients are evaluated using a specially designed numerical integration\nscheme. Then, the expansion coefficients are used to construct a\nrotationally-invariant covariance matrix which admits a block-diagonal\nstructure, and the eigen-decomposition of its blocks provides us with the\ndesired steerable principal components. The proposed method is shown to be\nfaster then existing methods, while providing appropriate error bounds which\nguarantee its accuracy."
},{
    "category": "math.OC", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.02875v2", 
    "title": "Revisiting Sub-sampled Newton Methods", 
    "arxiv-id": "1608.02875v2", 
    "author": "Zhihua Zhang", 
    "publish": "2016-08-08T03:30:12Z", 
    "summary": "Many machine learning models depend on solving a large scale optimization\nproblem. Recently, sub-sampled Newton methods have emerged to attract much\nattention for optimization due to their efficiency at each iteration, rectified\na weakness in the ordinary Newton method of suffering a high cost at each\niteration while commanding a high convergence rate. In this work we propose two\nnew efficient Newton-type methods, Refined Sub-sampled Newton and Refined\nSketch Newton. Our methods exhibit a great advantage over existing sub-sampled\nNewton methods, especially when Hessian-vector multiplication can be calculated\nefficiently. Specifically, the proposed methods are shown to converge\nsuperlinearly in general case and quadratically under a little stronger\nassumption. The proposed methods can be generalized to a unifying framework for\nthe convergence proof of several existing sub-sampled Newton methods, revealing\nnew convergence properties. Finally, we empirically evaluate the performance of\nour methods on several standard datasets and the results show consistent\nimprovement in computational efficiency."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.04361v1", 
    "title": "Multi-way Monte Carlo Method for Linear Systems", 
    "arxiv-id": "1608.04361v1", 
    "author": "David F. Gleich", 
    "publish": "2016-08-15T18:45:08Z", 
    "summary": "We study the Monte Carlo method for solving a linear system of the form $x =\nH x + b$. A sufficient condition for the method to work is $\\| H \\| < 1$, which\ngreatly limits the usability of this method. We improve this condition by\nproposing a new multi-way Markov random walk, which is a generalization of the\nstandard Markov random walk. Under our new framework we prove that the\nnecessary and sufficient condition for our method to work is the spectral\nradius $\\rho(H^{+}) < 1$, which is a weaker requirement than $\\| H \\| < 1$. In\naddition to solving more problems, our new method can work faster than the\nstandard algorithm. In numerical experiments on both synthetic and real world\nmatrices, we demonstrate the effectiveness of our new method."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.05787v1", 
    "title": "On Formal Verification in Imperative Multivalued Programming over   Continuous Data Types", 
    "arxiv-id": "1608.05787v1", 
    "author": "Martin Ziegler", 
    "publish": "2016-08-20T06:12:45Z", 
    "summary": "Using fundamental ideas from [Brattka&Hertling'98] and by means of\nobject-oriented overloading of operators, the iRRAM library supports imperative\nprogramming over the reals with a both sound and computable, multivalued\nsemantics of tests. We extend Floyd-Hoare Logic to formally verify the\ncorrectness of symbolic-numerical algorithms employing such primitives for\nthree example problems: truncated binary logarithm, 1D simple root finding, and\nsolving systems of linear equations. This is to be generalized to other hybrid\n(i.e. discrete and continuous) abstract data types."
},{
    "category": "math.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.06473v1", 
    "title": "A two-scale approach for efficient on-the-fly operator assembly in   massively parallel high performance multigrid codes", 
    "arxiv-id": "1608.06473v1", 
    "author": "Barbara Wohlmuth", 
    "publish": "2016-08-23T11:43:20Z", 
    "summary": "Matrix-free finite element implementations of massively parallel geometric\nmultigrid save memory and are often significantly faster than implementations\nusing classical sparse matrix techniques. They are especially well suited for\nhierarchical hybrid grids on polyhedral domains. In the case of constant\ncoefficients all fine grid node stencils in the interior of a coarse macro\nelement are equal. However, for non-polyhedral domains the situation changes.\nThen even for the Laplace operator, the non-linear element mapping leads to\nfine grid stencils that can vary from grid point to grid point. This\nobservation motivates a new two-scale approach that exploits a piecewise\npolynomial approximation of the fine grid operator with respect to the coarse\nmesh size. The low-cost evaluation of these surrogate polynomials results in an\nefficient stencil assembly on-the-fly for non-polyhedral domains that can be\nsignificantly more efficient than matrix-free techniques that are based on an\nelement-wise assembly. The performance analysis and additional hardware-aware\ncode optimizations are based on the Execution-Cache-Memory model. Several\naspects such as two-scale a priori error bounds and double discretization\ntechniques are presented. Weak and strong scaling results illustrate the\nbenefits of the new technique when used within large scale PDE solvers."
},{
    "category": "cs.SC", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.06691v1", 
    "title": "Conversion Methods for Improving Structural Analysis of   Differential-Algebraic Equation Systems", 
    "arxiv-id": "1608.06691v1", 
    "author": "John D. Pryce", 
    "publish": "2016-08-24T02:20:55Z", 
    "summary": "Differential-algebraic equation systems (DAEs) are generated routinely by\nsimulation and modeling environments. Before a simulation starts and a\nnumerical method is applied, some kind of structural analysis (SA) is used to\ndetermine which equations to be differentiated, and how many times. Both\nPantelides's algorithm and Pryce's $\\Sigma$-method are equivalent: if one of\nthem finds correct structural information, the other does also. Nonsingularity\nof the Jacobian produced by SA indicates a success, which occurs on many\nproblems of interest. However, these methods can fail on simple, solvable DAEs\nand give incorrect structural information including the index. This article\ninvestigates $\\Sigma$-method's failures and presents two conversion methods for\nfixing them. Both methods convert a DAE on which the $\\Sigma$-method fails to\nan equivalent problem on which this SA is more likely to succeed."
},{
    "category": "cs.SC", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.06693v1", 
    "title": "Conversion Methods, Block Triangularization, and Structural Analysis of   Differential-Algebraic Equation Systems", 
    "arxiv-id": "1608.06693v1", 
    "author": "John D. Pryce", 
    "publish": "2016-08-24T02:32:50Z", 
    "summary": "In a previous article, the authors developed two conversion methods to\nimprove the $\\Sigma$-method for structural analysis (SA) of\ndifferential-algebraic equations (DAEs). These methods reformulate a DAE on\nwhich the $\\Sigma$-method fails into an equivalent problem on which this SA is\nmore likely to succeed with a generically nonsingular Jacobian. The basic\nversion of these methods processes the DAE as a whole. This article presents\nthe block version that exploits block triangularization of a DAE. Using a block\ntriangular form of a Jacobian sparsity pattern, we identify which diagonal\nblocks of the Jacobian are identically singular and then perform a conversion\non each such block. This approach improves the efficiency of finding a suitable\nconversion for fixing SA's failures. All of our conversion methods can be\nimplemented in a computer algebra system so that every conversion can be\nautomated."
},{
    "category": "math.NA", 
    "doi": "10.1088/1741-2552/aa61bb", 
    "link": "http://arxiv.org/pdf/1608.07641v2", 
    "title": "Batched Stochastic Gradient Descent with Weighted Sampling", 
    "arxiv-id": "1608.07641v2", 
    "author": "Rachel Ward", 
    "publish": "2016-08-27T00:51:16Z", 
    "summary": "We analyze a batched variant of Stochastic Gradient Descent (SGD) with\nweighted sampling distribution for smooth and non-smooth objective functions.\nWe show that by distributing the batches computationally, a significant speedup\nin the convergence rate is provably possible compared to either batched\nsampling or weighted sampling alone. We propose several computationally\nefficient schemes to approximate the optimal weights, and compute proposed\nsampling distributions explicitly for the least squares and hinge loss\nproblems. We show both analytically and experimentally that substantial gains\ncan be obtained."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.00324v1", 
    "title": "Uniform Penalty inversion of two-dimensional NMR Relaxation data", 
    "arxiv-id": "1609.00324v1", 
    "author": "F. Zama", 
    "publish": "2016-09-01T17:33:33Z", 
    "summary": "The inversion of two-dimensional NMR data is an ill-posed problem related to\nthe numerical computation of the inverse Laplace transform. In this paper we\npresent the 2DUPEN algorithm that extends the Uniform Penalty (UPEN) algorithm\n[Borgia, Brown, Fantazzini, {\\em Journal of Magnetic Resonance}, 1998] to\ntwo-dimensional data. The UPEN algorithm, defined for the inversion of\none-dimensional NMR relaxation data, uses Tikhonov-like regularization and\noptionally non-negativity constraints in order to implement locally adapted\nregularization. In this paper, we analyze the regularization properties of this\napproach. Moreover, we extend the one-dimensional UPEN algorithm to the\ntwo-dimensional case and present an efficient implementation based on the\nNewton Projection method. Without any a-priori information on the noise norm,\n2DUPEN automatically computes the locally adapted regularization parameters and\nthe distribution of the unknown NMR parameters by using variable smoothing.\nResults of numerical experiments on simulated and real data are presented in\norder to illustrate the potential of the proposed method in reconstructing\npeaks and flat regions with the same accuracy."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.00671v1", 
    "title": "Structural Convergence Results for Low-Rank Approximations from Block   Krylov Spaces", 
    "arxiv-id": "1609.00671v1", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2016-09-02T17:20:46Z", 
    "summary": "This paper is concerned with approximating the dominant left singular vector\nspace of a real matrix $A$ of arbitrary dimension, from block Krylov spaces\n$\\mathcal{K}_q(AA^T,AX)$. Two classes of results are presented. First are\nbounds on the distance, in the two and Frobenius norms, between the Krylov\nspace and the target space. The distance is expressed in terms of principal\nangles. Second are quality of approximation bounds, relative to the best\nlow-rank approximation in the Frobenius norm. For starting guesses $X$ of full\ncolumn-rank, the bounds depend on the tangent of the principal angles between\n$X$ and the dominant right singular vector space of $A$. The results presented\nhere form the structural foundation for the analysis of randomized Krylov space\nmethods. The innovative feature is a combination of traditional Lanczos\nconvergence analysis with optimal low-rank approximations via least squares\nproblems."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.05434v1", 
    "title": "Consistent Discretization and Minimization of the L1 Norm on Manifolds", 
    "arxiv-id": "1609.05434v1", 
    "author": "Matan Sela", 
    "publish": "2016-09-18T06:56:57Z", 
    "summary": "The L1 norm has been tremendously popular in signal and image processing in\nthe past two decades due to its sparsity-promoting properties. More recently,\nits generalization to non-Euclidean domains has been found useful in shape\nanalysis applications. For example, in conjunction with the minimization of the\nDirichlet energy, it was shown to produce a compactly supported quasi-harmonic\northonormal basis, dubbed as compressed manifold modes. The continuous L1 norm\non the manifold is often replaced by the vector l1 norm applied to sampled\nfunctions. We show that such an approach is incorrect in the sense that it does\nnot consistently discretize the continuous norm and warn against its\nsensitivity to the specific sampling. We propose two alternative\ndiscretizations resulting in an iteratively-reweighed l2 norm. We demonstrate\nthe proposed strategy on the compressed modes problem, which reduces to a\nsequence of simple eigendecomposition problems not requiring non-convex\noptimization on Stiefel manifolds and producing more stable and accurate\nresults."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.06762v1", 
    "title": "Sparsity-Preserving Difference of Positive Semidefinite Matrix   Representation of Indefinite Matrices", 
    "arxiv-id": "1609.06762v1", 
    "author": "Jaehyun Park", 
    "publish": "2016-09-21T21:38:06Z", 
    "summary": "We consider the problem of writing an arbitrary symmetric matrix as the\ndifference of two positive semidefinite matrices. We start with simple ideas\nsuch as eigenvalue decomposition. Then, we develop a simple adaptation of the\nCholesky that returns a difference-of-Cholesky representation of indefinite\nmatrices. Heuristics that promote sparsity can be applied directly to this\nmodification."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.07086v1", 
    "title": "A Randomized Tensor Singular Value Decomposition based on the t-product", 
    "arxiv-id": "1609.07086v1", 
    "author": "Shuchin Aeron", 
    "publish": "2016-09-22T17:55:21Z", 
    "summary": "The tensor Singular Value Decomposition (t-SVD) for third order tensors that\nwas proposed by Kilmer and Martin~\\cite{2011kilmer} has been applied\nsuccessfully in many fields, such as computed tomography, facial recognition,\nand video completion. In this paper, we propose a method that extends a\nwell-known randomized matrix method to the t-SVD. This method can produce a\nfactorization with similar properties to the t-SVD, but is more computationally\nefficient on very large datasets. We present details of the algorithm,\ntheoretical results, and provide numerical results that show the promise of our\napproach for compressing and analyzing datasets. We also present an improved\nanalysis of the randomized subspace iteration for matrices, which may be of\nindependent interest to the scientific community."
},{
    "category": "math.OC", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.07373v2", 
    "title": "Block-proximal methods with spatially adapted acceleration", 
    "arxiv-id": "1609.07373v2", 
    "author": "Tuomo Valkonen", 
    "publish": "2016-09-23T14:18:25Z", 
    "summary": "We study and develop (stochastic) primal--dual block-coordinate descent\nmethods based on the method of Chambolle and Pock. Our methods have known\nconvergence rates for the iterates and the ergodic gap: $O(1/N^2)$ if each each\nblock is strongly convex, $O(1/N)$ if no convexity is present, and more\ngenerally a mixed rate $O(1/N^2)+O(1/N)$ for strongly convex blocks, if only\nsome blocks are strongly convex. Additional novelties of our methods include\nblockwise-adapted step lengths and acceleration, as well as the ability update\nboth the primal and dual variables randomly in blocks under a very light\ncompatibility condition. In other words, these variants of our methods are\ndoubly-stochastic. We test the proposed methods on various image processing\nproblems, where we employ pixelwise-adapted acceleration."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.07429v2", 
    "title": "A Fast Algorithm for Convolutional Structured Low-Rank Matrix Recovery", 
    "arxiv-id": "1609.07429v2", 
    "author": "Mathews Jacob", 
    "publish": "2016-09-23T16:58:11Z", 
    "summary": "Fourier domain structured low-rank matrix priors are emerging as powerful\nalternatives to traditional image recovery methods such as total variation (TV)\nand wavelet regularization. These priors specify that a convolutional\nstructured matrix, i.e., Toeplitz, Hankel, or their multi-level\ngeneralizations, built from Fourier data of the image should be low-rank. The\nmain challenge in applying these schemes to large-scale problems is the\ncomputational complexity and memory demand resulting from a lifting the image\ndata to a large scale matrix. We introduce a fast and memory efficient approach\ncalled the Generic Iterative Reweighted Annihilation Filter (GIRAF) algorithm\nthat exploits the convolutional structure of the lifted matrix to work in the\noriginal un-lifted domain, thus considerably reducing the complexity. Our\nexperiments on the recovery of images from undersampled Fourier measurements\nshow that the resulting algorithm is considerably faster than previously\nproposed algorithms, and can accommodate much larger problem sizes than\npreviously studied."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.08063v1", 
    "title": "Modeling Parallel Wiener-Hammerstein Systems Using Tensor Decomposition   of Volterra Kernels", 
    "arxiv-id": "1609.08063v1", 
    "author": "Mariya Ishteva", 
    "publish": "2016-09-26T16:44:16Z", 
    "summary": "Providing flexibility and user-interpretability in nonlinear system\nidentification can be achieved by means of block-oriented methods. One of such\nblock-oriented system structures is the parallel Wiener-Hammerstein system,\nwhich is a sum of Wiener-Hammerstein branches, consisting of static\nnonlinearities sandwiched between linear dynamical blocks. Parallel\nWiener-Hammerstein models have more descriptive power than their single-branch\ncounterparts, but their identification is a non-trivial task that requires\ntailored system identification methods. In this work, we will tackle the\nidentification problem by performing a tensor decomposition of the Volterra\nkernels obtained from the nonlinear system. We illustrate how the parallel\nWiener-Hammerstein block-structure gives rise to a joint tensor decomposition\nof the Volterra kernels with block-circulant structured factors. The\ncombination of Volterra kernels and tensor methods is a fruitful way to tackle\nthe parallel Wiener-Hammerstein system identification task. In simulation\nexperiments, we were able to reconstruct very accurately the underlying blocks\nunder noisy conditions."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.08438v1", 
    "title": "Flows Generating Nonlinear Eigenfunctions", 
    "arxiv-id": "1609.08438v1", 
    "author": "Guy Gilboa", 
    "publish": "2016-09-27T13:42:13Z", 
    "summary": "Nonlinear variational methods have become very powerful tools for many image\nprocessing tasks. Recently a new line of research has emerged, dealing with\nnonlinear eigenfunctions induced by convex functionals. This has provided new\ninsights and better theoretical understanding of convex regularization and\nintroduced new processing methods. However, the theory of nonlinear eigenvalue\nproblems is still at its infancy. We present a new flow that can generate\nnonlinear eigenfunctions of the form $T(u)=\\lambda u$, where $T(u)$ is a\nnonlinear operator and $\\lambda \\in \\mathbb{R} $ is the eigenvalue. We develop\nthe theory where $T(u)$ is a subgradient element of a regularizing\none-homogeneous functional, such as total-variation (TV) or\ntotal-generalized-variation (TGV). We introduce two flows: a forward flow and\nan inverse flow; for which the steady state solution is a nonlinear\neigenfunction. The forward flow monotonically smooths the solution (with\nrespect to the regularizer) and simultaneously increases the $L^2$ norm. The\ninverse flow has the opposite characteristics. For both flows, the steady state\ndepends on the initial condition, thus different initial conditions yield\ndifferent eigenfunctions. This enables a deeper investigation into the space of\nnonlinear eigenfunctions, allowing to produce numerically diverse examples,\nwhich may be unknown yet. In addition we suggest an indicator to measure the\naffinity of a function to an eigenfunction and relate it to\npseudo-eigenfunctions in the linear case."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1609.09230v1", 
    "title": "Tensor Networks for Latent Variable Analysis. Part I: Algorithms for   Tensor Train Decomposition", 
    "arxiv-id": "1609.09230v1", 
    "author": "Danilo Mandic", 
    "publish": "2016-09-29T07:22:21Z", 
    "summary": "Decompositions of tensors into factor matrices, which interact through a core\ntensor, have found numerous applications in signal processing and machine\nlearning. A more general tensor model which represents data as an ordered\nnetwork of sub-tensors of order-2 or order-3 has, so far, not been widely\nconsidered in these fields, although this so-called tensor network\ndecomposition has been long studied in quantum physics and scientific\ncomputing. In this study, we present novel algorithms and applications of\ntensor network decompositions, with a particular focus on the tensor train\ndecomposition and its variants. The novel algorithms developed for the tensor\ntrain decomposition update, in an alternating way, one or several core tensors\nat each iteration, and exhibit enhanced mathematical tractability and\nscalability to exceedingly large-scale data tensors. The proposed algorithms\nare tested in classic paradigms of blind source separation from a single\nmixture, denoising, and feature extraction, and achieve superior performance\nover the widely used truncated algorithms for tensor train decomposition."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.02570v3", 
    "title": "Real-time Error Control for Surgical Simulation", 
    "arxiv-id": "1610.02570v3", 
    "author": "St\u00e9phane Bordas", 
    "publish": "2016-10-08T19:50:07Z", 
    "summary": "Objective: To present the first real-time a posteriori error-driven adaptive\nfinite element approach for real-time simulation and to demonstrate the method\non a needle insertion problem. Methods: We use corotational elasticity and a\nfrictional needle/tissue interaction model. The problem is solved using finite\nelements within SOFA. The refinement strategy relies upon a hexahedron-based\nfinite element method, combined with a posteriori error estimation driven local\n$h$-refinement, for simulating soft tissue deformation. Results: We control the\nlocal and global error level in the mechanical fields (e.g. displacement or\nstresses) during the simulation. We show the convergence of the algorithm on\nacademic examples, and demonstrate its practical usability on a percutaneous\nprocedure involving needle insertion in a liver. For the latter case, we\ncompare the force displacement curves obtained from the proposed adaptive\nalgorithm with that obtained from a uniform refinement approach. Conclusions:\nError control guarantees that a tolerable error level is not exceeded during\nthe simulations. Local mesh refinement accelerates simulations. Significance:\nOur work provides a first step to discriminate between discretization error and\nmodeling error by providing a robust quantification of discretization error\nduring simulations."
},{
    "category": "stat.ML", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.02962v2", 
    "title": "Low-rank Approximation and Dynamic Mode Decomposition", 
    "arxiv-id": "1610.02962v2", 
    "author": "C\u00e9dric Herzet", 
    "publish": "2016-10-10T15:29:12Z", 
    "summary": "Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing\nthe dynamics of non-linear systems from experimental datasets. Recently,\nseveral attempts have extended DMD to the context of low-rank approximations.\nThis extension is of particular interest for reduced-order modeling in various\napplicative domains, e.g. for climate prediction, to study molecular dynamics\nor micro-electromechanical devices. This low-rank extension takes the form of a\nnonconvex optimization problem. To the best of our knowledge, only sub-optimal\nalgorithms have been proposed in the literature to compute the solution of this\nproblem. In this paper, we prove that there exists a closed-form optimal\nsolution to this problem and design an effective algorithm to compute it based\non Singular Value Decomposition (SVD). Based on this solution, we then propose\nefficient procedures for reduced-order modeling and for the identification of\nthe the low-rank DMD modes and amplitudes. Experiments illustrates the gain in\nperformance of the proposed algorithm compared to state-of-the-art techniques."
},{
    "category": "cs.LG", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.05838v3", 
    "title": "CuMF_SGD: Fast and Scalable Matrix Factorization", 
    "arxiv-id": "1610.05838v3", 
    "author": "Yun Liang", 
    "publish": "2016-10-19T01:28:11Z", 
    "summary": "Matrix factorization (MF) has been widely used in e.g., recommender systems,\ntopic modeling and word embedding. Stochastic gradient descent (SGD) is popular\nin solving MF problems because it can deal with large data sets and is easy to\ndo incremental learning. We observed that SGD for MF is memory bound.\nMeanwhile, single-node CPU systems with caching performs well only for small\ndata sets; distributed systems have higher aggregated memory bandwidth but\nsuffer from relatively slow network connection. This observation inspires us to\naccelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node\nconnection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF\nproblems. On a single CPU, we design two workload schedule schemes, i.e.,\nbatch-Hogwild! and wavefront-update that fully exploit the massive amount of\ncores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes\nthe issue of memory discontinuity. We also develop highly-optimized kernels for\nSGD update, leveraging cache, warp-shuffle instructions and half-precision\nfloats. We also design a partition scheme to utilize multiple GPUs while\naddressing the well-known convergence issue when parallelizing SGD. On three\ndata sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3.1X-28.2X as fast\ncompared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also\nshow that cuMF_SGD scales well on multiple GPUs in large data sets."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.06049v1", 
    "title": "Fast and Accurate Surface Normal Integration on Non-Rectangular Domains", 
    "arxiv-id": "1610.06049v1", 
    "author": "Jean-Denis Durou", 
    "publish": "2016-10-19T15:01:09Z", 
    "summary": "The integration of surface normals for the purpose of computing the shape of\na surface in 3D space is a classic problem in computer vision. However, even\nnowadays it is still a challenging task to devise a method that combines the\nflexibility to work on non-trivial computational domains with high accuracy,\nrobustness and computational efficiency. By uniting a classic approach for\nsurface normal integration with modern computational techniques we construct a\nsolver that fulfils these requirements. Building upon the Poisson integration\nmodel we propose to use an iterative Krylov subspace solver as a core step in\ntackling the task. While such a method can be very efficient, it may only show\nits full potential when combined with a suitable numerical preconditioning and\na problem-specific initialisation. We perform a thorough numerical study in\norder to identify an appropriate preconditioner for our purpose. To address the\nissue of a suitable initialisation we propose to compute this initial state via\na recently developed fast marching integrator. Detailed numerical experiments\nilluminate the benefits of this novel combination. In addition, we show on\nreal-world photometric stereo datasets that the developed numerical framework\nis flexible enough to tackle modern computer vision applications."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.06086v1", 
    "title": "On the Approximation of Functionals of Very Large Hermitian Matrices   represented as Matrix Product Operators", 
    "arxiv-id": "1610.06086v1", 
    "author": "Thomas Huckle", 
    "publish": "2016-10-19T16:10:51Z", 
    "summary": "We present a method to approximate functionals $\\text{Tr} \\, f(A)$ of very\nhigh-dimensional hermitian matrices $A$ represented as Matrix Product Operators\n(MPOs). Our method is based on a reformulation of a block Lanczos algorithm in\ntensor network format. We state main properties of the method and show how to\nadapt the basic Lanczos algorithm to the tensor network formalism to allow for\nhigh-dimensional computations. Additionally, we give an analysis of the\ncomplexity of our method and provide numerical evidence that it yields good\napproximations of the entropy of density matrices represented by MPOs while\nbeing robust against truncations."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.06685v3", 
    "title": "Error estimates with explicit constants for the Sinc approximation over   infinite intervals", 
    "arxiv-id": "1610.06685v3", 
    "author": "Tomoaki Okayama", 
    "publish": "2016-10-21T07:16:27Z", 
    "summary": "The Sinc approximation is a function approximation formula that attains\nexponential convergence for rapidly decaying functions defined on the whole\nreal axis. Even for other functions, the Sinc approximation works accurately\nwhen combined with a proper variable transformation. The convergence rate has\nbeen analyzed for typical cases including finite, semi-infinite, and infinite\nintervals. Recently, for verified numerical computations, a more explicit,\n\"computable\" error bound has been given in the case of a finite interval. In\nthis paper, such explicit error bounds are derived for other cases."
},{
    "category": "cs.LG", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1610.07722v1", 
    "title": "Sparse Hierarchical Tucker Factorization and its Application to   Healthcare", 
    "arxiv-id": "1610.07722v1", 
    "author": "Jimeng Sun", 
    "publish": "2016-10-25T04:08:11Z", 
    "summary": "We propose a new tensor factorization method, called the Sparse\nHierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors.\nSparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker\nmethod, which aims to compute a tree-structured factorization of an input data\nset that may be readily interpreted by a domain expert. However, Sparse\nH-Tucker uses a nested sampling technique to overcome a key scalability problem\nin Hierarchical Tucker, which is the creation of an unwieldy intermediate dense\ncore tensor; the result of our approach is a faster, more space-efficient, and\nmore accurate method. We extensively test our method on a real healthcare\ndataset, which is collected from 30K patients and results in an 18th order\nsparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the\nfull data set on a single multi-threaded machine. It can also do so more\naccurately and in less time than the state-of-the-art: on a 12th order subset\nof the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a\npreviously state-of-the-art method. Even for analyzing low order tensors (e.g.,\n4-order), our method requires close to an order of magnitude less time and over\ntwo orders of magnitude less memory, as compared to traditional tensor\nfactorization methods such as CP and Tucker. Moreover, we observe that Sparse\nH-Tucker scales nearly linearly in the number of non-zero tensor elements. The\nresulting model also provides an interpretable disease hierarchy, which is\nconfirmed by a clinical expert."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.01318v2", 
    "title": "Certified Lower Bounds of Roundoff Errors using Semidefinite Programming", 
    "arxiv-id": "1611.01318v2", 
    "author": "Mountassir Farid", 
    "publish": "2016-11-04T10:43:13Z", 
    "summary": "A longstanding problem related to floating-point implementation of numerical\nprograms is to provide efficient yet precise analysis of output errors.\n  We present a framework to compute lower bounds of absolute roundoff errors\nfor numerical programs implementing polynomial functions with box constrained\ninput variables. Our study relies on semidefinite programming (SDP) relaxations\nand is complementary of over-approximation frameworks, consisting of obtaining\nupper bounds for the absolute roundoff error.\n  Our method is based on a new hierarchy of convergent robust SDP\napproximations for certain classes of polynomial optimization problems. Each\nproblem in this hierarchy can be exactly solved via SDP. By using this\nhierarchy, one can provide a monotone non-decreasing sequence of lower bounds\nconverging to the absolute roundoff error of a program implementing a\npolynomial function.\n  We investigate the efficiency and precision of our method on non-trivial\npolynomial programs coming from space control, optimization and computational\nbiology."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.02862v1", 
    "title": "The Little Engine that Could: Regularization by Denoising (RED)", 
    "arxiv-id": "1611.02862v1", 
    "author": "Peyman Milanfar", 
    "publish": "2016-11-09T09:32:29Z", 
    "summary": "Removal of noise from an image is an extensively studied problem in image\nprocessing. Indeed, the recent advent of sophisticated and highly effective\ndenoising algorithms lead some to believe that existing methods are touching\nthe ceiling in terms of noise removal performance. Can we leverage this\nimpressive achievement to treat other tasks in image processing? Recent work\nhas answered this question positively, in the form of the Plug-and-Play Prior\n($P^3$) method, showing that any inverse problem can be handled by sequentially\napplying image denoising steps. This relies heavily on the ADMM optimization\ntechnique in order to obtain this chained denoising interpretation.\n  Is this the only way in which tasks in image processing can exploit the image\ndenoising engine? In this paper we provide an alternative, more powerful and\nmore flexible framework for achieving the same goal. As opposed to the $P^3$\nmethod, we offer Regularization by Denoising (RED): using the denoising engine\nin defining the regularization of the inverse problem. We propose an explicit\nimage-adaptive Laplacian-based regularization functional, making the overall\nobjective functional clearer and better defined. With a complete flexibility to\nchoose the iterative optimization procedure for minimizing the above\nfunctional, RED is capable of incorporating any image denoising algorithm,\ntreat general inverse problems very effectively, and is guaranteed to converge\nto the globally optimal result. We test this approach and demonstrate\nstate-of-the-art results in the image deblurring and super-resolution problems."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.04074v2", 
    "title": "Accelerated Stochastic ADMM with Variance Reduction", 
    "arxiv-id": "1611.04074v2", 
    "author": "Tengfei Zhou", 
    "publish": "2016-11-13T03:25:39Z", 
    "summary": "Alternating Direction Method of Multipliers (ADMM) is a popular method in\nsolving Machine Learning problems. Stochastic ADMM was firstly proposed in\norder to reduce the per iteration computational complexity, which is more\nsuitable for big data problems. Recently, variance reduction techniques have\nbeen integrated with stochastic ADMM in order to get a fast convergence rate,\nsuch as SAG-ADMM and SVRG-ADMM,but the convergence is still suboptimal w.r.t\nthe smoothness constant. In this paper, we propose a new accelerated stochastic\nADMM algorithm with variance reduction, which enjoys a faster convergence than\nall the other stochastic ADMM algorithms. We theoretically analyze its\nconvergence rate and show its dependence on the smoothness constant is optimal.\nWe also empirically validate its effectiveness and show its priority over other\nstochastic ADMM algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.05253v1", 
    "title": "Strict upper and lower bounds for quantities of interest in static   response sensitivity analysis", 
    "arxiv-id": "1611.05253v1", 
    "author": "Hongzhi Zhong", 
    "publish": "2016-11-16T12:29:09Z", 
    "summary": "In this paper, a goal-oriented error estimation technique for static response\nsensitivity analysis is proposed based on the constitutive relation error (CRE)\nestimation for finite element analysis (FEA). Strict upper and lower bounds of\nvarious quantities of interest (QoI) associated with the response sensitivity\nderivative fields are acquired. Numerical results are presented to validate the\nstrict bounding properties of the proposed technique."
},{
    "category": "cs.CV", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.05963v3", 
    "title": "Reweighted Low-Rank Tensor Decomposition based on t-SVD and its   Applications in Video Denoising", 
    "arxiv-id": "1611.05963v3", 
    "author": "Sudhish N. George", 
    "publish": "2016-11-18T03:26:12Z", 
    "summary": "The t-SVD based Tensor Robust Principal Component Analysis (TRPCA) decomposes\nlow rank multi-linear signal corrupted by gross errors into low multi-rank and\nsparse component by simultaneously minimizing tensor nuclear norm and l 1 norm.\nBut if the multi-rank of the signal is considerably large and/or large amount\nof noise is present, the performance of TRPCA deteriorates. To overcome this\nproblem, this paper proposes a new efficient iterative reweighted tensor\ndecomposition scheme based on t-SVD which significantly improves tensor\nmulti-rank in TRPCA. Further, the sparse component of the tensor is also\nrecovered by reweighted l 1 norm which enhances the accuracy of decomposition.\nThe effectiveness of the proposed method is established by applying it to the\nvideo denoising problem and the experimental results reveal that the proposed\nalgorithm outperforms its counterparts."
},{
    "category": "cs.CE", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.08758v2", 
    "title": "Variational inequality approach to enforce the non-negative constraint   for advection-diffusion equations", 
    "arxiv-id": "1611.08758v2", 
    "author": "K. B. Nakshatrala", 
    "publish": "2016-11-26T23:02:31Z", 
    "summary": "Predictive simulations are crucial for the success of many subsurface\napplications, and it is highly desirable to obtain accurate non-negative\nsolutions for transport equations in these numerical simulations. In this\npaper, we propose a computational framework based on the variational inequality\n(VI) which can also be used to enforce important mathematical properties (e.g.,\nmaximum principles) and physical constraints (e.g., the non-negative\nconstraint). We demonstrate that this framework is not only applicable to\ndiffusion equations but also to non-symmetric advection-diffusion equations. An\nattractive feature of the proposed framework is that it works with with any\nweak formulation for the advection-diffusion equations, including single-field\nformulations, which are computationally attractive. A particular emphasis is\nplaced on the parallel and algorithmic performance of the VI approach across\nlarge-scale and heterogeneous problems. It is also shown that QP and VI are\nequivalent under certain conditions. State-of-the-art QP and VI solvers\navailable from the PETSc library are used on a variety of steady-state 2D and\n3D benchmarks, and a comparative study on the scalability between the QP and VI\nsolvers is presented. We then extend the proposed framework to transient\nproblems by simulating the miscible displacement of fluids in a heterogeneous\nporous medium and illustrate the importance of enforcing maximum principles for\nthese types of coupled problems. Our numerical experiments indicate that VIs\nare indeed a viable approach for enforcing the maximum principles and the\nnon-negative constraint in a large-scale computing environment. Also provided\nare Firedrake project files as well as a discussion on the computer\nimplementation to help facilitate readers in understanding the proposed\nframework."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1611.09646v1", 
    "title": "How to overcome the Courant-Friedrichs-Lewy condition of explicit   discretizations?", 
    "arxiv-id": "1611.09646v1", 
    "author": "Denys Dutykh", 
    "publish": "2016-11-24T16:43:08Z", 
    "summary": "This manuscript contains some thoughts on the discretization of the classical\nheat equation. Namely, we discuss the advantages and disadvantages of explicit\nand implicit schemes. Then, we show how to overcome some disadvantages while\npreserving some advantages. However, since there is no free lunch, there is a\nprice to pay for any improvement in the numerical scheme. This price will be\nthoroughly discussed below.In particular, we like explicit discretizations for\nthe ease of their implementation even for nonlinear problems. Unfortunately,\nwhen these schemes are applied to parabolic equations, severe stability limits\nappear for the time step magnitude making the explicit simulations\nprohibitively expensive. Implicit schemes remove the stability limit, but each\ntime step requires now the solution of linear (at best) or even nonlinear\nsystems of equations. However, there exists a number of tricks to overcome (or\nat least to relax) severe stability limitations of explicit schemes without\ngoing into the trouble of fully implicit ones. The purpose of this manuscript\nis just to inform the readers about these alternative techniques to extend the\nstability limits. It was not written for classical scientific publication\npurposes."
},{
    "category": "cs.MS", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.00530v1", 
    "title": "Implementation and evaluation of data-compression algorithms for   irregular-grid iterative methods on the PEZY-SC processor", 
    "arxiv-id": "1612.00530v1", 
    "author": "Jun Makino", 
    "publish": "2016-12-02T01:09:23Z", 
    "summary": "Iterative methods on irregular grids have been used widely in all areas of\ncomptational science and engineering for solving partial differential equations\nwith complex geometry. They provide the flexibility to express complex shapes\nwith relatively low computational cost. However, the direction of the evolution\nof high-performance processors in the last two decades have caused serious\ndegradation of the computational efficiency of iterative methods on irregular\ngrids, because of relatively low memory bandwidth. Data compression can in\nprinciple reduce the necessary memory memory bandwidth of iterative methods and\nthus improve the efficiency. We have implemented several data compression\nalgorithms on the PEZY-SC processor, using the matrix generated for the HPCG\nbenchmark as an example. For the SpMV (Sparse Matrix-Vector multiplication)\npart of the HPCG benchmark, the best implementation without data compression\nachieved 11.6Gflops/chip, close to the theoretical limit due to the memory\nbandwidth. Our implementation with data compression has achieved 32.4Gflops.\nThis is of course rather extreme case, since the grid used in HPCG is\ngeometrically regular and thus its compression efficiency is very high.\nHowever, in real applications, it is in many cases possible to make a large\npart of the grid to have regular geometry, in particular when the resolution is\nhigh. Note that we do not need to change the structure of the program, except\nfor the addition of the data compression/decompression subroutines. Thus, we\nbelieve the data compression will be very useful way to improve the performance\nof many applications which rely on the use of irregular grids."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.02561v4", 
    "title": "A Higher Order Isoparametric Fictitious Domain Method for Level Set   Domains", 
    "arxiv-id": "1612.02561v4", 
    "author": "Christoph Lehrenfeld", 
    "publish": "2016-12-08T08:36:43Z", 
    "summary": "We consider a new fictitious domain approach of higher order accuracy. To\nimplement Dirichlet conditions we apply the classical Nitsche method combined\nwith a facet-based stabilization (ghost penalty). Both techniques are combined\nwith a higher order isoparametric finite element space which is based on a\nspecial mesh transformation. The mesh transformation is build upon a higher\norder accurate level set representation and allows to reduce the problem of\nnumerical integration to problems on domains which are described by piecewise\nlinear level set functions. The combination of this strategy for the numerical\nintegration and the stabilized Nitsche formulation results in an accurate and\nrobust method. We introduce and analyze it and give numerical examples."
},{
    "category": "cs.CE", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.03247v1", 
    "title": "Parameter Estimation of a Nonlinear Burgers Model using Nanoindentation   and Finite Element-based Inverse Analysis", 
    "arxiv-id": "1612.03247v1", 
    "author": "Salah U. Hamim", 
    "publish": "2016-12-10T03:39:20Z", 
    "summary": "Nanoindentation involves probing a hard diamond tip into a material, where\nthe load and the displacement experienced by the tip is recorded continuously.\nThis load-displacement data is a direct function of material's innate\nstress-strain behavior. Thus, theoretically it is possible to extract\nmechanical properties of a material through nanoindentation. However, due to\nvarious nonlinearities associated with nanoindentation the process of\ninterpreting load-displacement data into material properties is difficult.\nAlthough, simple elastic behavior can be characterized easily, a method to\ncharacterize complicated material behavior such as nonlinear viscoelasticity is\nstill lacking. In this study, a nanoindentation-based material characterization\ntechnique is developed to characterize soft materials exhibiting nonlinear\nviscoelasticity. Nanoindentation experiment was modeled in finite element\nanalysis software (ABAQUS), where a nonlinear viscoelastic behavior was\nincorporated using user-defined subroutine (UMAT). The model parameters were\ncalibrated using a process called inverse analysis. In this study, a surrogate\nmodel-based approach was used for the inverse analysis. The different factors\naffecting the surrogate model performance are analyzed in order to optimize the\nperformance with respect to the computational cost."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.04464v1", 
    "title": "Frames and numerical approximation", 
    "arxiv-id": "1612.04464v1", 
    "author": "Daan Huybrechs", 
    "publish": "2016-12-14T02:49:06Z", 
    "summary": "Functions of one or more variables are usually approximated with a basis; a\ncomplete, linearly independent set of functions that spans an appropriate\nfunction space. The topic of this paper is the numerical approximation of\nfunctions using the more general notion of frames; complete systems that are\ngenerally redundant but provide stable infinite representations. While frames\nare well-known in image and signal processing, coding theory and other areas of\nmathematics, their use in numerical analysis is far less widespread. Yet, as we\nshow via example, frames are more flexible than bases, and can be constructed\neasily in a range of problems where finding orthonormal bases with desirable\nproperties is difficult or impossible.\n  A major difficulty in computing best approximations is that frames\nnecessarily lead to ill-conditioned linear systems of equations. However, we\nshow that frame approximations can in fact be computed numerically up to an\nerror of order $\\sqrt{\\epsilon}$ with a simple algorithm, where $\\epsilon$ is a\nthreshold parameter that can be chosen close to machine precision. Moreover,\nthis accuracy can be improved to order $\\epsilon$ with modifications to the\nalgorithm. Crucially, the order of convergence down to this limit is determined\nby the existence of representations of the function being approximated that are\naccurate and have small-norm coefficients. We demonstrate the existence of such\nrepresentations in all our examples. Overall, our analysis suggests that frames\nare a natural generalization of bases in which to develop numerical\napproximation. In particular, even in the presence of severe ill-conditioning,\nthe frame condition imposes sufficient mathematical structure on the redundant\nset in order to give rise to good approximations in finite precision\ncalculations."
},{
    "category": "math.OC", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.04694v3", 
    "title": "Stochastic Second-Order Optimization via von Neumann Series", 
    "arxiv-id": "1612.04694v3", 
    "author": "Mojmir Mutny", 
    "publish": "2016-12-14T15:38:37Z", 
    "summary": "A stochastic iterative algorithm approximating second-order information using\nvon Neumann series is discussed. We present an improved and simplified\nconvergence analysis, in contrast to a similar algorithm and its analysis,\nLISSA. The algorithm is primarily suitable for training large scale linear\nmodels, where the number of data points is very large. Two novel analyses, one\nshowing space independent linear convergence, and one showing conditional\nquadratic convergence are discussed. In numerical experiments, the behavior of\nthe error is similar to the second-order algorithm L-BFGS, and it performs\nbetter than a similar algorithm LISSA."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.04796v1", 
    "title": "Robust Multigrid for Cartesian Interior Penalty DG Formulations of the   Poisson Equation in 3D", 
    "arxiv-id": "1612.04796v1", 
    "author": "Joerg Stiller", 
    "publish": "2016-12-14T20:26:33Z", 
    "summary": "We present a polynomial multigrid method for the nodal interior penalty\nformulation of the Poisson equation on three-dimensional Cartesian grids. Its\nkey ingredient is a weighted overlapping Schwarz smoother operating on\nelement-centered subdomains. The MG method reaches superior convergence rates\ncorresponding to residual reductions of about two orders of magnitude within a\nsingle V(1,1) cycle. It is robust with respect to the mesh size and the ansatz\norder, at least up to ${P=32}$. Rigorous exploitation of tensor-product\nfactorization yields a computational complexity of $O(PN)$ for $N$ unknowns,\nwhereas numerical experiments indicate even linear runtime scaling. Moreover,\nby allowing adjustable subdomain overlaps and adding Krylov acceleration, the\nmethod proved feasible for anisotropic grids with element aspect ratios up to\n48."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.07578v1", 
    "title": "Construction and implementation of asymptotic expansions for   Laguerre-type orthogonal polynomials", 
    "arxiv-id": "1612.07578v1", 
    "author": "Peter Opsomer", 
    "publish": "2016-12-22T12:36:19Z", 
    "summary": "Laguerre and Laguerre-type polynomials are orthogonal polynomials on the\ninterval $[0,\\infty)$ with respect to a weight function of the form $w(x) =\nx^{\\alpha} e^{-Q(x)}, Q(x) = \\sum_{k=0}^m q_k x^k, \\alpha > -1, q_m > 0$. The\nclassical Laguerre polynomials correspond to $Q(x)=x$. The computation of\nhigher-order terms of the asymptotic expansions of these polynomials for large\ndegree becomes quite complicated, and a full description seems to be lacking in\nliterature. However, this information is implicitly available in the work of\nVanlessen, based on a non-linear steepest descent analysis of an associated\nso-called Riemann--Hilbert problem. We will extend this work and show how to\nefficiently compute an arbitrary number of higher-order terms in the asymptotic\nexpansions of Laguerre and Laguerre-type polynomials. This effort is similar to\nthe case of Jacobi and Jacobi-type polynomials in a previous paper. We supply\nan implementation with explicit expansions in four different regions of the\ncomplex plane. These expansions can also be extended to Hermite-type weights of\nthe form $\\exp(-\\sum_{k=0}^m q_k x^{2k})$ on $(-\\infty,\\infty)$, and to general\nnon-polynomial functions $Q(x)$ using contour integrals. The expansions may be\nused, e.g., to compute Gauss-Laguerre quadrature rules in a lower computational\ncomplexity than based on the recurrence relation, and with improved accuracy\nfor large degree. They are also of interest in random matrix theory."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.07838v1", 
    "title": "Convergence Rates for Greedy Kaczmarz Algorithms, and Faster Randomized   Kaczmarz Rules Using the Orthogonality Graph", 
    "arxiv-id": "1612.07838v1", 
    "author": "Alim Virani", 
    "publish": "2016-12-22T23:31:35Z", 
    "summary": "The Kaczmarz method is an iterative algorithm for solving systems of linear\nequalities and inequalities, that iteratively projects onto these constraints.\nRecently, Strohmer and Vershynin [J. Fourier Anal. Appl., 15(2):262-278, 2009]\ngave a non-asymptotic convergence rate analysis for this algorithm, spurring\nnumerous extensions and generalizations of the Kaczmarz method. Rather than the\nrandomized selection rule analyzed in that work, in this paper we instead\ndiscuss greedy and approximate greedy selection rules. We show that in some\napplications the computational costs of greedy and random selection are\ncomparable, and that in many cases greedy selection rules give faster\nconvergence rates than random selection rules. Further, we give the first\nmulti-step analysis of Kaczmarz methods for a particular greedy rule, and\npropose a provably-faster randomized selection rule for matrices with many\npairwise-orthogonal rows."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.07875v1", 
    "title": "Streaming GPU Singular Value and Dynamic Mode Decompositions", 
    "arxiv-id": "1612.07875v1", 
    "author": "Steven L. Brunton", 
    "publish": "2016-12-23T05:06:25Z", 
    "summary": "This work develops a parallelized algorithm to compute the dynamic mode\ndecomposition (DMD) on a graphics processing unit using the streaming method of\nsnapshots singular value decomposition. This allows the algorithm to operate\nefficiently on streaming data by avoiding redundant inner-products as new data\nbecomes available. In addition, it is possible to leverage the native\ncompressed format of many data streams, such as HD video and computational\nphysics codes that are represented sparsely in the Fourier domain, to massively\nreduce data transfer from CPU to GPU and to enable sparse matrix\nmultiplications. Taken together, these algorithms facilitate real-time\nstreaming DMD on high-dimensional data streams. We demonstrate the proposed\nmethod on numerous high-dimensional data sets ranging from video background\nmodeling to scientific computing applications, where DMD is becoming a mainstay\nalgorithm. The computational framework is developed as an open-source library\nwritten in C++ with CUDA, and the algorithms may be generalized to include\nother DMD advances, such as compressed sensing DMD, multi resolution DMD, or\nDMD with control. Keywords: Singular value decomposition, dynamic mode\ndecomposition, streaming computations, graphics processing unit, video\nbackground modeling, scientific computing."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1612.08686v1", 
    "title": "Differentiable monotonicity-preserving schemes for discontinuous   Galerkin methods on arbitrary meshes", 
    "arxiv-id": "1612.08686v1", 
    "author": "Alba Hierro", 
    "publish": "2016-12-27T17:17:53Z", 
    "summary": "This work is devoted to the design of interior penalty discontinuous Galerkin\n(dG) schemes that preserve maximum principles at the discrete level for the\nsteady transport and convection-diffusion problems and the respective transient\nproblems with implicit time integration. Monotonic schemes that combine\nexplicit time stepping with dG space discretization are very common, but the\ndesign of such schemes for implicit time stepping is rare, and it had only been\nattained so far for 1D problems. The proposed scheme is based on an artificial\ndiffusion that linearly depends on a shock detector that identifies the\ntroublesome areas. In order to define the new shock detector, we have\nintroduced the concept of discrete local extrema. The diffusion operator is a\ngraph-Laplacian, instead of the more common finite element discretization of\nthe Laplacian operator, which is essential to keep monotonicity on general\nmeshes and in multi-dimension. The resulting nonlinear stabilization is\nnon-smooth and nonlinear solvers can fail to converge. As a result, we propose\na smoothed (twice differentiable) version of the nonlinear stabilization, which\nallows us to use Newton with line search nonlinear solvers and dramatically\nimprove nonlinear convergence. A theoretical numerical analysis of the proposed\nschemes show that they satisfy the desired monotonicity properties. Further,\nthe resulting operator is Lipschitz continuous and there exists at least one\nsolution of the discrete problem, even in the non-smooth version. We provide a\nset of numerical results to support our findings."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.00392v1", 
    "title": "On the Computation of Complex-valued Gradients with Application to   Statistically Optimum Beamforming", 
    "arxiv-id": "1701.00392v1", 
    "author": "Reinhold Haeb-Umbach", 
    "publish": "2017-01-02T14:03:38Z", 
    "summary": "This report describes the computation of gradients by algorithmic\ndifferentiation for statistically optimum beamforming operations. Especially\nthe derivation of complex-valued functions is a key component of this approach.\nTherefore the real-valued algorithmic differentiation is extended via the\ncomplex-valued chain rule. In addition to the basic mathematic operations the\nderivative of the eigenvalue problem with complex-valued eigenvectors is one of\nthe key results of this report. The potential of this approach is shown with\nexperimental results on the CHiME-3 challenge database. There, the beamforming\ntask is used as a front-end for an ASR system. With the developed derivatives a\njoint optimization of a speech enhancement and speech recognition system w.r.t.\nthe recognition optimization criterion is possible."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.00722v1", 
    "title": "The Unum Number Format: Mathematical Foundations, Implementation and   Comparison to IEEE 754 Floating-Point Numbers", 
    "arxiv-id": "1701.00722v1", 
    "author": "Laslo Hunhold", 
    "publish": "2017-01-02T23:21:43Z", 
    "summary": "This thesis examines a modern concept for machine numbers based on interval\narithmetic called 'Unums' and compares it to IEEE 754 floating-point\narithmetic, evaluating possible uses of this format where floating-point\nnumbers are inadequate. In the course of this examination, this thesis builds\ntheoretical foundations for IEEE 754 floating-point numbers, interval\narithmetic based on the projectively extended real numbers and Unums."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.01477v1", 
    "title": "Unconstrained inverse quadratic programming problem", 
    "arxiv-id": "1701.01477v1", 
    "author": "E. G. Abramov", 
    "publish": "2017-01-04T15:48:59Z", 
    "summary": "The paper covers a formulation of the inverse quadratic programming problem\nin terms of unconstrained optimization where it is required to find the unknown\nparameters (the matrix of the quadratic form and the vector of the quasi-linear\npart of the quadratic form) provided that approximate estimates of the optimal\nsolution of the direct problem and those of the target function to be minimized\nin the form of pairs of values lying in the corresponding neighborhoods are\nonly known. The formulation of the inverse problem and its solution are based\non the least squares method. In the explicit form the inverse problem solution\nhas been derived in the form a system of linear equations. The parameters\nobtained can be used for reconstruction of the direct quadratic programming\nproblem and determination of the optimal solution and the extreme value of the\ntarget function, which were not known formerly. It is possible this approach\nopens new ways in over applications, for example, in neurocomputing and quadric\nsurfaces fitting. Simple numerical examples have been demonstrated. A scenario\nin the Octave/MATLAB programming language has been proposed for practical\nimplementation of the method."
},{
    "category": "cs.SC", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.01994v1", 
    "title": "Computing Approximate Greatest Common Right Divisors of Differential   Polynomials", 
    "arxiv-id": "1701.01994v1", 
    "author": "Erich Kaltofen", 
    "publish": "2017-01-08T18:06:21Z", 
    "summary": "Differential (Ore) type polynomials with \"approximate\" polynomial\ncoefficients are introduced. These provide an effective notion of approximate\ndifferential operators, with a strong algebraic structure. We introduce the\napproximate Greatest Common Right Divisor Problem (GCRD) of differential\npolynomials, as a non-commutative generalization of the well-studied\napproximate GCD problem.\n  Given two differential polynomials, we present an algorithm to find nearby\ndifferential polynomials with a non-trivial GCRD, where nearby is defined with\nrespect to a suitable coefficient norm. Intuitively, given two linear\ndifferential polynomials as input, the (approximate) GCRD problem corresponds\nto finding the (approximate) differential polynomial whose solution space is\nthe intersection of the solution spaces of the two inputs.\n  The approximate GCRD problem is proven to be locally well-posed. A method\nbased on the singular value decomposition of a differential Sylvester matrix is\ndeveloped to produce an initial approximation of the GCRD. With a sufficiently\ngood initial approximation, Newton iteration is shown to converge quadratically\nto an optimal solution. Finally, sufficient conditions for existence of a\nsolution to the global problem are presented along with examples demonstrating\nthat no solution exists when these conditions are not satisfied."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.02324v1", 
    "title": "An $N \\log N$ Parallel Fast Direct Solver for Kernel Matrices", 
    "arxiv-id": "1701.02324v1", 
    "author": "George Biros", 
    "publish": "2017-01-09T19:11:10Z", 
    "summary": "Kernel matrices appear in machine learning and non-parametric statistics.\nGiven $N$ points in $d$ dimensions and a kernel function that requires\n$\\mathcal{O}(d)$ work to evaluate, we present an $\\mathcal{O}(dN\\log N)$-work\nalgorithm for the approximate factorization of a regularized kernel matrix, a\ncommon computational bottleneck in the training phase of a learning task. With\nthis factorization, solving a linear system with a kernel matrix can be done\nwith $\\mathcal{O}(N\\log N)$ work. Our algorithm only requires kernel\nevaluations and does not require that the kernel matrix admits an efficient\nglobal low rank approximation. Instead our factorization only assumes low-rank\nproperties for the off-diagonal blocks under an appropriate row and column\nordering. We also present a hybrid method that, when the factorization is\nprohibitively expensive, combines a partial factorization with iterative\nmethods. As a highlight, we are able to approximately factorize a dense\n$11M\\times11M$ kernel matrix in 2 minutes on 3,072 x86 \"Haswell\" cores and a\n$4.5M\\times4.5M$ matrix in 1 minute using 4,352 \"Knights Landing\" cores."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.03240v1", 
    "title": "Error estimation for surrogate models of dynamical systems using machine   learning", 
    "arxiv-id": "1701.03240v1", 
    "author": "Louis J. Durlofsky", 
    "publish": "2017-01-12T05:42:22Z", 
    "summary": "This work proposes a machine-learning-based framework for estimating the\nerror introduced by surrogate models of parameterized dynamical systems. The\nframework applies high-dimensional regression techniques (e.g., random forests,\nLASSO) to map a large set of inexpensively-computed 'error indicators' (i.e.,\nfeatures) produced by the surrogate model at a given time instance to a\nprediction of the surrogate-model error in a quantity of interest (QoI). The\nmethodology requires a training set of parameter instances at which the\ntime-dependent surrogate-model error is computed by simulating both the\nhigh-fidelity and surrogate models. Using these training data, the method first\nperforms feature-space partitioning (via classification or clustering), and\nsubsequently constructs a 'local' regression model to predict the\ntime-instantaneous error within each identified region of feature space. We\nconsider two uses for the resulting error model: (1) as a correction to the\nsurrogate-model QoI prediction at each time instance, and (2) as a way to\nstatistically model arbitrary functions of the time-dependent surrogate-model\nerror (e.g., time-integrated errors). We apply the proposed framework to a\nnonlinear oil-water problem with time-varying well-control (bottom-hole\npressure) parameters, and consider proper orthogonal decomposition applied with\ntrajectory piecewise linearization (POD-TPWL) as the surrogate model. When the\nfirst use of the method is considered, numerical experiments demonstrate that\nthe method consistently improves accuracy in the time-instantaneous QoI\nprediction relative to the original surrogate model across a large number of\ntest cases. When the second use is considered, numerical experiments show that\nthe proposed method generates accurate, unbiased statistical predictions of the\ntime- and well-averaged errors."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.03477v1", 
    "title": "Space-time balancing domain decomposition", 
    "arxiv-id": "1701.03477v1", 
    "author": "Marc Olm", 
    "publish": "2017-01-12T19:32:09Z", 
    "summary": "In this work, we propose two-level space-time domain decomposition\npreconditioners for parabolic problems discretized using finite elements. They\nare motivated as an extension to space-time of balancing domain decomposition\nby constraints preconditioners. The key ingredients to be defined are the\nsub-assembled space and operator, the coarse degrees of freedom (DOFs) in which\nwe want to enforce continuity among subdomains at the preconditioner level, and\nthe transfer operator from the sub-assembled to the original finite element\nspace. With regard to the sub-assembled operator, a perturbation of the time\nderivative is needed to end up with a well-posed preconditioner. The set of\ncoarse DOFs includes the time average (at the space-time subdomain) of\nclassical space constraints plus new constraints between consecutive subdomains\nin time. Numerical experiments show that the proposed schemes are weakly\nscalable in time, i.e., we can efficiently exploit increasing computational\nresources to solve more time steps in the same {total elapsed} time. Further,\nthe scheme is also weakly space-time scalable, since it leads to asymptotically\nconstant iterations when solving larger problems both in space and time.\nExcellent {wall clock} time weak scalability is achieved for space-time\nparallel solvers on some thousands of cores."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.03989v1", 
    "title": "The Adaptive $s$-step Conjugate Gradient Method", 
    "arxiv-id": "1701.03989v1", 
    "author": "Erin Carson", 
    "publish": "2017-01-15T03:17:00Z", 
    "summary": "On modern large-scale parallel computers, the performance of Krylov subspace\niterative methods is limited by global synchronization. This has inspired the\ndevelopment of $s$-step Krylov subspace method variants, in which iterations\nare computed in blocks of $s$, which can reduce the number of global\nsynchronizations per iteration by a factor of $O(s)$.\n  Although the $s$-step variants are mathematically equivalent to their\nclassical counterparts, they can behave quite differently in finite precision\ndepending on the parameter $s$. If $s$ is chosen too large, the $s$-step method\ncan suffer a convergence delay and a decrease in attainable accuracy relative\nto the classical method. This makes it difficult for a potential user of such\nmethods - the $s$ value that minimizes the time per iteration may not be the\nbest $s$ for minimizing the overall time-to-solution, and further may cause an\nunacceptable decrease in accuracy.\n  Towards improving the reliability and usability of $s$-step Krylov subspace\nmethods, in this work we derive the \\emph{adaptive $s$-step CG method}, a\nvariable $s$-step CG method where in block $k$, the parameter $s_k$ is\ndetermined automatically such that a user-specified accuracy is attainable. The\nmethod for determining $s_k$ is based on a bound on growth of the residual gap\nwithin block $k$, from which we derive a constraint on the condition numbers of\nthe computed $O(s_k)$-dimensional Krylov subspace bases. The computations\nrequired for determining the block size $s_k$ can be performed without\nincreasing the number of global synchronizations per block. Our numerical\nexperiments demonstrate that the adaptive $s$-step CG method is able to attain\nup to the same accuracy as classical CG while still significantly reducing the\ntotal number of global synchronizations."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.05420v1", 
    "title": "The tensor network representation of high order cumulant and algorithm   for their calculation", 
    "arxiv-id": "1701.05420v1", 
    "author": "\u0141ukasz Pawela", 
    "publish": "2017-01-19T14:08:50Z", 
    "summary": "In this paper we introduce a novel algorithm of calculating arbitrary order\ncumulants of multidimensional data. Since the n th order cumulant can be\npresented in the form of an n-dimensional tensor, the algorithm is presented\nusing the tensor network notation. The presented algorithm exploits the\nsuper--symmetry of cumulant and moment tensors. We show, that proposed\nalgorithm highly decreases the computational complexity of cumulants\ncalculation, compared to the naive algorithm"
},{
    "category": "cs.DS", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.06446v1", 
    "title": "On updates of high order cumulant tensors", 
    "arxiv-id": "1701.06446v1", 
    "author": "Piotr Gawron", 
    "publish": "2017-01-20T11:51:35Z", 
    "summary": "High order cumulants carry information about statistics of non--normally\ndistributed multivariate data. Such cumulants are utilised in extreme events\nanalysis, small target detection or outliers detection. In this work we present\na new algorithm,for updating high order cumulant tensors of random multivariate\ndata, if new package of data is recorded. We show algebraically and\nnumerically, that the proposed algorithm is faster than a naive cumulants\nrecalculation algorithm. For investigated computer generated data our algorithm\nappears to be fasten than a naive one by 1-2 orders of magnitude. That update\nalgorithm makes the online updates of multivariate data statistics much faster,\nand can be used for the data streaming analysis.\n  Further we propose the map reduce algorithm of cumulants calculation, that is\nbased on introduced cumulants updates algorithm. This map reduce algorithm can\nbe used to collect statistics about multivariate confidential data that are\nheld by many agents, without sharing those data."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.06920v1", 
    "title": "An hp-adaptive strategy for elliptic problems", 
    "arxiv-id": "1701.06920v1", 
    "author": "Linbo Zhang", 
    "publish": "2017-01-23T17:08:14Z", 
    "summary": "In this paper a new hp-adaptive strategy for elliptic problems based on\nrefinement history is proposed, which chooses h-, p- or hp-refinement on\nindividual elements according to a posteriori error estimate, as well as\nsmoothness estimate of the solution obtained by comparing the actual and\nexpected error reduction rate. Numerical experiments show that exponential\nconvergence can be achieved with this strategy."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1701.08935v2", 
    "title": "Spectrum Slicing for Sparse Hermitian Definite Matrices Based on   Zolotarev's Functions", 
    "arxiv-id": "1701.08935v2", 
    "author": "Haizhao Yang", 
    "publish": "2017-01-31T07:46:47Z", 
    "summary": "This paper proposes an efficient method for computing selected generalized\neigenpairs of a sparse Hermitian definite matrix pencil $(A,B)$. Based on\nZolotarev's best rational function approximations of the signum function and\nconformal maps, we construct the best rational function approximation of a\nrectangular function supported on an arbitrary interval. This new best rational\nfunction approximation is applied to construct spectrum filters of $(A,B)$.\nCombining fast direct solvers and the shift-invariant GMRES, a hybrid fast\nalgorithm is proposed to apply spectral filters efficiently. Assuming that the\nsparse Hermitian matrices $A$ and $B$ are of size $N\\times N$ with $O(N)$\nnonzero entries, the computational cost for computing $O(1)$ interior\neigenpairs is bounded by that of solving a shifted linear system $(A-\\sigma\nB)x=b$. Utilizing the spectrum slicing idea, the proposed method computes the\nfull eigenvalue decomposition of a sparse Hermitian definite matrix pencil via\nsolving $O(N)$ linear systems. The efficiency and stability of the proposed\nmethod are demonstrated by numerical examples of a wide range of sparse\nmatrices. Compared with existing spectrum slicing algorithms based on contour\nintegrals, the proposed method is faster and more reliable."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1702.00108v1", 
    "title": "On the optimality and sharpness of Laguerre's lower bound on the   smallest eigenvalue of a symmetric positive definite matrix", 
    "arxiv-id": "1702.00108v1", 
    "author": "Yusaku Yamamoto", 
    "publish": "2017-02-01T03:00:08Z", 
    "summary": "Lower bounds on the smallest eigenvalue of a symmetric positive definite\nmatrices $A\\in\\mathbb{R}^{m\\times m}$ play an important role in condition\nnumber estimation and in iterative methods for singular value computation. In\nparticular, the bounds based on ${\\rm Tr}(A^{-1})$ and ${\\rm Tr}(A^{-2})$\nattract attention recently because they can be computed in $O(m)$ work when $A$\nis tridiagonal. In this paper, we focus on these bounds and investigate their\nproperties in detail. First, we consider the problem of finding the optimal\nbound that can be computed solely from ${\\rm Tr}(A^{-1})$ and ${\\rm\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\nbecomes smallest when ${\\rm Tr}(A^{-2})/\\{{\\rm Tr}(A^{-1})\\}^2$ approaches 1 or\n$\\frac{1}{m}$. These results will be useful, for example, in designing\nefficient shift strategies for singular value computation algorithms."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1702.02912v1", 
    "title": "Randomized Dynamic Mode Decomposition", 
    "arxiv-id": "1702.02912v1", 
    "author": "J. Nathan Kutz", 
    "publish": "2017-02-07T18:26:28Z", 
    "summary": "This paper presents a randomized algorithm for computing the near-optimal\nlow-rank dynamic mode decomposition (DMD). Randomized algorithms are emerging\ntechniques to compute low-rank matrix approximations. They are able to ease the\ncomputational challenges arising in the area of big data. The idea is to derive\nfrom the high-dimensional input matrix a smaller matrix, which is then used to\nefficiently compute the dynamic modes and eigenvalues. The algorithm is\npresented in a modular probabilistic framework, and the approximation quality\ncan be controlled via oversampling, and power iterations."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1702.04274v1", 
    "title": "Hybrid System Modelling and Simulation with Dirac Deltas", 
    "arxiv-id": "1702.04274v1", 
    "author": "Hans Vangheluwe", 
    "publish": "2017-02-14T16:04:20Z", 
    "summary": "For a wide variety of problems, creating detailed continuous models of\n(continuous) physical systems is, at the very least, impractical. Hybrid models\ncan abstract away short transient behaviour (thus introducing discontinuities)\nin order to simplify the study of such systems. For example, when modelling a\nbouncing ball, the bounce can be abstracted as a discontinuous change of the\nvelocity, instead of resorting to the physics of the ball (de-)compression to\nkeep the velocity signal continuous. Impulsive differential equations can be\nused to model and simulate hybrid systems such as the bouncing ball. In this\napproach, the force acted on the ball by the floor is abstracted as an\ninfinitely large function in an infinitely small interval of time, that is, an\nimpulse. Current simulators cannot handle such approximations well due to the\nlimitations of machine precision.\n  In this paper, we explore the simulation of impulsive differential equations,\nwhere impulses are first class citizens. We present two approaches for the\nsimulation of impulses: symbolic and numerical. Our contribution is a\ntheoretically founded description of the implementation of both approaches in a\nCausal Block Diagram modelling and simulation tool. Furthermore, we investigate\nthe conditions for which one approach is better than the other."
},{
    "category": "math.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1702.07199v1", 
    "title": "Convergence acceleration of alternating series", 
    "arxiv-id": "1702.07199v1", 
    "author": "Rafa\u0142 Nowak", 
    "publish": "2017-02-23T12:51:51Z", 
    "summary": "A new simple convergence acceleration method is proposed for a certain wide\nrange class of convergent alternating series. The method has some common\nfeatures with Smith's and Ford's modification of Levin's and Weniger's sequence\ntransformations, but it is computationally less expensive. The similarities and\ndifferences between all three methods are analyzed and some common theoretical\nresults are given. Numerical examples confirm a similar performance of all\nthree methods."
},{
    "category": "cs.CE", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1703.01202v1", 
    "title": "Parallel energy-stable phase field crystal simulations based on domain   decomposition methods", 
    "arxiv-id": "1703.01202v1", 
    "author": "Jizu Huang", 
    "publish": "2017-03-03T15:27:32Z", 
    "summary": "In this paper, we present a parallel numerical algorithm for solving the\nphase field crystal equation. In the algorithm, a semi-implicit finite\ndifference scheme is derived based on the discrete variational derivative\nmethod. Theoretical analysis is provided to show that the scheme is\nunconditionally energy stable and can achieve second-order accuracy in both\nspace and time. An adaptive time step strategy is adopted such that the time\nstep size can be flexibly controlled based on the dynamical evolution of the\nproblem. At each time step, a nonlinear algebraic system is constructed from\nthe discretization of the phase field crystal equation and solved by a domain\ndecomposition based, parallel Newton--Krylov--Schwarz method with improved\nboundary conditions for subdomain problems. Numerical experiments with several\ntwo and three dimensional test cases show that the proposed algorithm is\nsecond-order accurate in both space and time, energy stable with large time\nsteps, and highly scalable to over ten thousands processor cores on the Sunway\nTaihuLight supercomputer."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1703.01325v1", 
    "title": "Decoupled Block-Wise ILU(k) Preconditioner on GPU", 
    "arxiv-id": "1703.01325v1", 
    "author": "Zhangxin Chen", 
    "publish": "2017-03-03T20:08:02Z", 
    "summary": "This research investigates the implementation mechanism of block-wise ILU(k)\npreconditioner on GPU. The block-wise ILU(k) algorithm requires both the level\nk and the block size to be designed as variables. A decoupled ILU(k) algorithm\nconsists of a symbolic phase and a factorization phase. In the symbolic phase,\na ILU(k) nonzero pattern is established from the point-wise structure extracted\nfrom a block-wise matrix. In the factorization phase, the block-wise matrix\nwith a variable block size is factorized into a block lower triangular matrix\nand a block upper triangular matrix. And a further diagonal factorization is\nrequired to perform on the block upper triangular matrix for adapting a\nparallel triangular solver on GPU.We also present the numerical experiments to\nstudy the preconditioner actions on different k levels and block sizes."
},{
    "category": "cs.NA", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1703.03722v1", 
    "title": "Recovery of Sparse and Low Rank Components of Matrices Using Iterative   Method with Adaptive Thresholding", 
    "arxiv-id": "1703.03722v1", 
    "author": "Farokh Marvasti", 
    "publish": "2017-03-09T17:06:21Z", 
    "summary": "In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time."
},{
    "category": "cs.LG", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/1703.04219v1", 
    "title": "SPARTan: Scalable PARAFAC2 for Large & Sparse Data", 
    "arxiv-id": "1703.04219v1", 
    "author": "Jimeng Sun", 
    "publish": "2017-03-13T01:38:56Z", 
    "summary": "In exploratory tensor mining, a common problem is how to analyze a set of\nvariables across a set of subjects whose observations do not align naturally.\nFor example, when modeling medical features across a set of patients, the\nnumber and duration of treatments may vary widely in time, meaning there is no\nmeaningful way to align their clinical records across time points for analysis\npurposes. To handle such data, the state-of-the-art tensor model is the\nso-called PARAFAC2, which yields interpretable and robust output and can\nnaturally handle sparse data. However, its main limitation up to now has been\nthe lack of efficient algorithms that can handle large-scale datasets.\n  In this work, we fill this gap by developing a scalable method to compute the\nPARAFAC2 decomposition of large and sparse datasets, called SPARTan. Our method\nexploits special structure within PARAFAC2, leading to a novel algorithmic\nreformulation that is both fast (in absolute time) and more memory-efficient\nthan prior work. We evaluate SPARTan on both synthetic and real datasets,\nshowing 22X performance gains over the best previous implementation and also\nhandling larger problem instances for which the baseline fails. Furthermore, we\nare able to apply SPARTan to the mining of temporally-evolving phenotypes on\ndata taken from real and medically complex pediatric patients. The clinical\nmeaningfulness of the phenotypes identified in this process, as well as their\ntemporal evolution over time for several patients, have been endorsed by\nclinical experts."
},{
    "category": "cond-mat.dis-nn", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/cond-mat/0305527v2", 
    "title": "Back-propagation of accuracy", 
    "arxiv-id": "cond-mat/0305527v2", 
    "author": "D. C. Wunsch II", 
    "publish": "2003-05-22T11:22:40Z", 
    "summary": "In this paper we solve the problem: how to determine maximal allowable\nerrors, possible for signals and parameters of each element of a network\nproceeding from the condition that the vector of output signals of the network\nshould be calculated with given accuracy? \"Back-propagation of accuracy\" is\ndeveloped to solve this problem. The calculation of allowable errors for each\nelement of network by back-propagation of accuracy is surprisingly similar to a\nback-propagation of error, because it is the backward signals motion, but at\nthe same time it is very different because the new rules of signals\ntransformation in the passing back through the elements are different. The\nmethod allows us to formulate the requirements to the accuracy of calculations\nand to the realization of technical devices, if the requirements to the\naccuracy of output signals of the network are known."
},{
    "category": "cs.NE", 
    "doi": "10.1088/1361-6420/33/1/015003", 
    "link": "http://arxiv.org/pdf/cs/0102015v1", 
    "title": "Non-convex cost functionals in boosting algorithms and methods for panel   selection", 
    "arxiv-id": "cs/0102015v1", 
    "author": "Marco Visentin", 
    "publish": "2001-02-20T13:08:15Z", 
    "summary": "In this document we propose a new improvement for boosting techniques as\nproposed in Friedman '99 by the use of non-convex cost functional. The idea is\nto introduce a correlation term to better deal with forecasting of additive\ntime series. The problem is discussed in a theoretical way to prove the\nexistence of minimizing sequence, and in a numerical way to propose a new\n\"ArgMin\" algorithm. The model has been used to perform the touristic presence\nforecast for the winter season 1999/2000 in Trentino (italian Alps)."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0303004v1", 
    "title": "Reliability Conditions in Quadrature Algorithms", 
    "arxiv-id": "cs/0303004v1", 
    "author": "N. M. Plakida", 
    "publish": "2003-03-06T09:19:42Z", 
    "summary": "The detection of insufficiently resolved or ill-conditioned integrand\nstructures is critical for the reliability assessment of the quadrature rule\noutputs. We discuss a method of analysis of the profile of the integrand at the\nquadrature knots which allows inferences approaching the theoretical 100% rate\nof success, under error estimate sharpening. The proposed procedure is of the\nhighest interest for the solution of parametric integrals arising in complex\nphysical models."
},{
    "category": "cs.NA", 
    "doi": "10.1137/030602666", 
    "link": "http://arxiv.org/pdf/cs/0311011v1", 
    "title": "On an explicit finite difference method for fractional diffusion   equations", 
    "arxiv-id": "cs/0311011v1", 
    "author": "L. Acedo", 
    "publish": "2003-11-10T13:13:32Z", 
    "summary": "A numerical method to solve the fractional diffusion equation, which could\nalso be easily extended to many other fractional dynamics equations, is\nconsidered. These fractional equations have been proposed in order to describe\nanomalous transport characterized by non-Markovian kinetics and the breakdown\nof Fick's law. In this paper we combine the forward time centered space (FTCS)\nmethod, well known for the numerical integration of ordinary diffusion\nequations, with the Grunwald-Letnikov definition of the fractional derivative\noperator to obtain an explicit fractional FTCS scheme for solving the\nfractional diffusion equation. The resulting method is amenable to a stability\nanalysis a la von Neumann. We show that the analytical stability bounds are in\nexcellent agreement with numerical tests. Comparison between exact analytical\nsolutions and numerical predictions are made."
},{
    "category": "cs.MS", 
    "doi": "10.1137/030602666", 
    "link": "http://arxiv.org/pdf/cs/0401008v1", 
    "title": "Algorithm xxx: Modified Bessel functions of imaginary order and positive   argument", 
    "arxiv-id": "cs/0401008v1", 
    "author": "Nico M. Temme", 
    "publish": "2004-01-13T14:29:26Z", 
    "summary": "Fortran 77 programs for the computation of modified Bessel functions of\npurely imaginary order are presented. The codes compute the functions\n$K_{ia}(x)$, $L_{ia}(x)$ and their derivatives for real $a$ and positive $x$;\nthese functions are independent solutions of the differential equation $x^2 w''\n+x w' +(a^2 -x^2)w=0$. The code also computes exponentially scaled functions.\nThe range of computation is $(x,a)\\in (0,1500]\\times [-1500,1500]$ when scaled\nfunctions are considered and it is larger than $(0,500]\\times [-400,400]$ for\nstandard IEEE double precision arithmetic. The relative accuracy is better than\n$10^{-13}$ in the range $(0,200]\\times [-200,200]$ and close to $10^{-12}$ in\n$(0,1500]\\times [-1500,1500]$."
},{
    "category": "cs.NA", 
    "doi": "10.1137/030602666", 
    "link": "http://arxiv.org/pdf/cs/0404047v1", 
    "title": "Using matrices in post-processing phase of CFD simulations", 
    "arxiv-id": "cs/0404047v1", 
    "author": "Gianluca Argentini", 
    "publish": "2004-04-22T15:33:52Z", 
    "summary": "In this work I present a technique of construction and fast evaluation of a\nfamily of cubic polynomials for analytic smoothing and graphical rendering of\nparticles trajectories for flows in a generic geometry. The principal result of\nthe work was implementation and test of a method for interpolating 3D points by\nregular parametric curves and their fast and efficient evaluation for a good\nresolution of rendering. For the purpose I have used a parallel environment\nusing a multiprocessor cluster architecture. The efficiency of the used method\nis good, mainly reducing the number of floating-points computations by caching\nthe numerical values of some line-parameter's powers, and reducing the\nnecessity of communication among processes. This work has been developed for\nthe Research and Development Department of my company for planning advanced\ncustomized models of industrial burners."
},{
    "category": "cs.NA", 
    "doi": "10.1137/030602666", 
    "link": "http://arxiv.org/pdf/cs/0408053v1", 
    "title": "Weighted average finite difference methods for fractional diffusion   equations", 
    "arxiv-id": "cs/0408053v1", 
    "author": "Santos B. Yuste", 
    "publish": "2004-08-23T12:00:13Z", 
    "summary": "Weighted averaged finite difference methods for solving fractional diffusion\nequations are discussed and different formulae of the discretization of the\nRiemann-Liouville derivative are considered. The stability analysis of the\ndifferent numerical schemes is carried out by means of a procedure close to the\nwell-known von Neumann method of ordinary diffusion equations. The stability\nbounds are easily found and checked in some representative examples."
},{
    "category": "cs.NA", 
    "doi": "10.1137/030602666", 
    "link": "http://arxiv.org/pdf/cs/0409056v1", 
    "title": "Using sparse matrices and splines-based interpolation in computational   fluid dynamics simulations", 
    "arxiv-id": "cs/0409056v1", 
    "author": "Gianluca Argentini", 
    "publish": "2004-09-29T10:34:45Z", 
    "summary": "In this relation I present a technique of construction and fast evaluation of\na family of cubic polynomials for analytic smoothing and graphical rendering of\nparticles trajectories for flows in a generic geometry. The principal result of\nthe work was implementation and test of a method for interpolating 3D points by\nregular parametric curves and their fast and efficient evaluation for a good\nresolution of rendering. For the purpose a parallel environment using a\nmultiprocessor cluster architecture has been used. This work has been developed\nfor the Research and Development Department of my company for planning advanced\ncustomized models of industrial burners."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.953", 
    "link": "http://arxiv.org/pdf/cs/0411047v3", 
    "title": "Numerical Solutions of 2-D Steady Incompressible Driven Cavity Flow at   High Reynolds Numbers", 
    "arxiv-id": "cs/0411047v3", 
    "author": "C. Gokcol", 
    "publish": "2004-11-15T23:38:00Z", 
    "summary": "Numerical calculations of the 2-D steady incompressible driven cavity flow\nare presented. The Navier-Stokes equations in streamfunction and vorticity\nformulation are solved numerically using a fine uniform grid mesh of 601x601.\nThe steady driven cavity solutions are computed for Re<21,000 with a maximum\nabsolute residuals of the governing equations that were less than 10-10. A new\nquaternary vortex at the bottom left corner and a new tertiary vortex at the\ntop left corner of the cavity are observed in the flow field as the Reynolds\nnumber increases. Detailed results are presented and comparisons are made with\nbenchmark solutions found in the literature."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1887", 
    "link": "http://arxiv.org/pdf/cs/0411048v3", 
    "title": "Discussions on Driven Cavity Flow", 
    "arxiv-id": "cs/0411048v3", 
    "author": "E. Erturk", 
    "publish": "2004-11-15T23:45:18Z", 
    "summary": "The widely studied benchmark problem, 2-D driven cavity flow problem is\ndiscussed in details in terms of physical and mathematical and also numerical\naspects. A very brief literature survey on studies on the driven cavity flow is\ngiven. Based on the several numerical and experimental studies, the fact of the\nmatter is, above moderate Reynolds numbers physically the flow in a driven\ncavity is not two-dimensional. However there exist numerical solutions for 2-D\ndriven cavity flow at high Reynolds numbers."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0411049v2", 
    "title": "Fourth Order Compact Formulation of Navier-Stokes Equations and Driven   Cavity Flow at High Reynolds Numbers", 
    "arxiv-id": "cs/0411049v2", 
    "author": "C. Gokcol", 
    "publish": "2004-11-15T23:52:25Z", 
    "summary": "A new fourth order compact formulation for the steady 2-D incompressible\nNavier-Stokes equations is presented. The formulation is in the same form of\nthe Navier-Stokes equations such that any numerical method that solve the\nNavier-Stokes equations can also be applied to this fourth order compact\nformulation. In particular in this work the formulation is solved with an\nefficient numerical method that requires the solution of tridiagonal systems\nusing a fine grid mesh of 601x601. Using this formulation, the steady 2-D\nincompressible flow in a driven cavity is solved up to Reynolds number of\n20,000 with fourth order spatial accuracy. Detailed solutions are presented."
},{
    "category": "cs.DS", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0412107v2", 
    "title": "A Monte Carlo algorithm for efficient large matrix inversion", 
    "arxiv-id": "cs/0412107v2", 
    "author": "C. Cabrillo", 
    "publish": "2004-12-23T17:01:14Z", 
    "summary": "This paper introduces a new Monte Carlo algorithm to invert large matrices.\nIt is based on simultaneous coupled draws from two random vectors whose\ncovariance is the required inverse. It can be considered a generalization of a\npreviously reported algorithm for hermitian matrices inversion based in only\none draw. The use of two draws allows the inversion on non-hermitian matrices.\nBoth the conditions for convergence and the rate of convergence are similar to\nthe Gauss-Seidel algorithm. Results on two examples are presented, a real\nnon-symmetric matrix related to quantitative genetics and a complex\nnon-hermitian matrix relevant for physicists. Compared with other Monte Carlo\nalgorithms it reveals a large reduction of the processing time showing eight\ntimes faster processing in the examples studied."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0412120v2", 
    "title": "An estimate of accuracy for interpolant numerical solutions of a PDE   problem", 
    "arxiv-id": "cs/0412120v2", 
    "author": "Gianluca Argentini", 
    "publish": "2004-12-31T18:16:00Z", 
    "summary": "In this paper we present an estimate of accuracy for a piecewise polynomial\napproximation of a classical numerical solution to a non linear differential\nproblem. We suppose the numerical solution U is computed using a grid with a\nsmall linear step and interval time Tu, while the polynomial approximation V is\nan interpolation of the values of a numerical solution on a less fine grid and\ninterval time Tv << Tu. The estimate shows that the interpolant solution V can\nbe, under suitable hypotheses, a good approximation and in general its\ncomputational cost is much lower of the cost of the fine numerical solution. We\npresent two possible applications to linear case and periodic case."
},{
    "category": "cs.CC", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0502066v1", 
    "title": "On the Complexity of Real Functions", 
    "arxiv-id": "cs/0502066v1", 
    "author": "Mark Braverman", 
    "publish": "2005-02-15T05:24:30Z", 
    "summary": "We develop a notion of computability and complexity of functions over the\nreals, which seems to be very natural when one tries to determine just how\n\"difficult\" a certain function is. This notion can be viewed as an extension of\nboth BSS computability [Blum, Cucker, Shub, Smale 1998], and bit computability\nin the tradition of computable analysis [Weihrauch 2000] as it relies on the\nlatter but allows some discontinuities and multiple values."
},{
    "category": "cs.DC", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0509067v1", 
    "title": "Decomposing Solution Sets of Polynomial Systems: A New Parallel   Monodromy Breakup Algorithm", 
    "arxiv-id": "cs/0509067v1", 
    "author": "Jan Verschelde", 
    "publish": "2005-09-21T20:55:09Z", 
    "summary": "We consider the numerical irreducible decomposition of a positive dimensional\nsolution set of a polynomial system into irreducible factors. Path tracking\ntechniques computing loops around singularities connect points on the same\nirreducible components. The computation of a linear trace for each factor\ncertifies the decomposition. This factorization method exhibits a good\npractical performance on solution sets of relative high degrees.\n  Using the same concepts of monodromy and linear trace, we present a new\nmonodromy breakup algorithm. It shows a better performance than the old method\nwhich requires construction of permutations of witness points in order to break\nup the solution set. In contrast, the new algorithm assumes a finer approach\nallowing us to avoid tracking unnecessary homotopy paths.\n  As we designed the serial algorithm keeping in mind distributed computing, an\nadditional advantage is that its parallel version can be easily built.\nSynchronization issues resulted in a performance loss of the straightforward\nparallel version of the old algorithm. Our parallel implementation of the new\napproach bypasses these issues, therefore, exhibiting a better performance,\nespecially on solution sets of larger degree."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0510051v1", 
    "title": "Numerical resolution of some BVP using Bernstein polynomials", 
    "arxiv-id": "cs/0510051v1", 
    "author": "Gianluca Argentini", 
    "publish": "2005-10-18T09:55:42Z", 
    "summary": "In this work we present a method, based on the use of Bernstein polynomials,\nfor the numerical resolution of some boundary values problems. The computations\nhave not need of particular approximations of derivatives, such as finite\ndifferences, or particular techniques, such as finite elements. Also, the\nmethod doesn't require the use of matrices, as in resolution of linear\nalgebraic systems, nor the use of like-Newton algorithms, as in resolution of\nnon linear sets of equations. An initial equation is resolved only once, then\nthe method is based on iterated evaluations of appropriate polynomials."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0601104v2", 
    "title": "The complexity of class polynomial computation via floating point   approximations", 
    "arxiv-id": "cs/0601104v2", 
    "author": "Andreas Enge", 
    "publish": "2006-01-24T11:01:46Z", 
    "summary": "We analyse the complexity of computing class polynomials, that are an\nimportant ingredient for CM constructions of elliptic curves, via complex\nfloating point approximations of their roots. The heart of the algorithm is the\nevaluation of modular functions in several arguments. The fastest one of the\npresented approaches uses a technique devised by Dupont to evaluate modular\nfunctions by Newton iterations on an expression involving the\narithmetic-geometric mean. It runs in time $O (|D| \\log^5 |D| \\log \\log |D|) =\nO (|D|^{1 + \\epsilon}) = O (h^{2 + \\epsilon})$ for any $\\epsilon > 0$, where\n$D$ is the CM discriminant and $h$ is the degree of the class polynomial.\nAnother fast algorithm uses multipoint evaluation techniques known from\nsymbolic computation; its asymptotic complexity is worse by a factor of $\\log\n|D|$. Up to logarithmic factors, this running time matches the size of the\nconstructed polynomials. The estimate also relies on a new result concerning\nthe complexity of enumerating the class group of an imaginary-quadratic order\nand on a rigorously proven upper bound for the height of class polynomials."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0602025v1", 
    "title": "On local symbolic approximation and resolution of ODEs using Implicit   Function Theorem", 
    "arxiv-id": "cs/0602025v1", 
    "author": "Gianluca Argentini", 
    "publish": "2006-02-07T11:44:09Z", 
    "summary": "In this work the implicit function theorem is used for searching local\nsymbolic resolution of differential equations. General results of existence for\nfirst order equations are proven and some examples, one relative to cavitation\nin a fluid, are developed. These examples seem to show that local approximation\nof non linear differential equations can give useful informations about\nsymbolic form of possible solutions, and in the case a global solution is\nknown, locally the accuracy of approximation can be good."
},{
    "category": "cs.DM", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0606103v22", 
    "title": "Precision Arithmetic: A New Floating-Point Arithmetic", 
    "arxiv-id": "cs/0606103v22", 
    "author": "Chengpu Wang", 
    "publish": "2006-06-25T18:56:28Z", 
    "summary": "A new deterministic floating-point arithmetic called precision arithmetic is\ndeveloped to track precision for arithmetic calculations. It uses a novel\nrounding scheme to avoid excessive rounding error propagation of conventional\nfloating-point arithmetic. Unlike interval arithmetic, its uncertainty tracking\nis based on statistics and the central limit theorem, with a much tighter\nbounding range. Its stable rounding error distribution is approximated by a\ntruncated normal distribution. Generic standards and systematic methods for\nvalidating uncertainty-bearing arithmetics are discussed. The precision\narithmetic is found to be better than interval arithmetic in both\nuncertainty-tracking and uncertainty-bounding for normal usages.\n  The precision arithmetic is available publicly at\nhttp://precisionarithm.sourceforge.net."
},{
    "category": "cs.DS", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0607061v1", 
    "title": "On Some Peculiarities of Dynamic Switch between Component   Implementations in an Autonomic Computing System", 
    "arxiv-id": "cs/0607061v1", 
    "author": "Igor Mackarov", 
    "publish": "2006-07-12T11:09:52Z", 
    "summary": "Behavior of the delta algorithm of autonomic switch between two component\nimplementations is considered on several examples of a client-server systems\ninvolving, in particular, periodic change of intensities of requests for the\ncomponent. It is shown that in the cases of some specific combinations of\nelementary requests costs, the number of clients in the system, the number of\nrequests per unit of time, and the cost of switch between the implementations,\nthe algorithm may reveal behavior that is rather far from the desired. A\nsufficient criterion of a success of the algorithm is proposed based on the\nanalysis of the accumulated implementations costs difference as a function of\ntime. Suggestions are pointed out of practical evaluation of the algorithm\nfunctioning regarding the observations made in this paper."
},{
    "category": "cs.MS", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0609082v1", 
    "title": "Classifying extrema using intervals", 
    "arxiv-id": "cs/0609082v1", 
    "author": "Marek W. Gutowski", 
    "publish": "2006-09-14T18:32:46Z", 
    "summary": "We present a straightforward and verified method of deciding whether the\nn-dimensional point x (n>=1), such that \\nabla f(x)=0, is the local minimizer,\nmaximizer or just a saddle point of a real-valued function f.\n  The method scales linearly with dimensionality of the problem and never\nproduces false results."
},{
    "category": "cs.CC", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0611008v1", 
    "title": "Why Linear Programming cannot solve large instances of NP-complete   problems in polynomial time", 
    "arxiv-id": "cs/0611008v1", 
    "author": "Radoslaw Hofman", 
    "publish": "2006-11-02T08:40:53Z", 
    "summary": "This article discusses ability of Linear Programming models to be used as\nsolvers of NP-complete problems. Integer Linear Programming is known as\nNP-complete problem, but non-integer Linear Programming problems can be solved\nin polynomial time, what places them in P class. During past three years there\nappeared some articles using LP to solve NP-complete problems. This methods use\nlarge number of variables (O(n^9)) solving correctly almost all instances that\ncan be solved in reasonable time. Can they solve infinitively large instances?\nThis article gives answer to this question."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0611080v1", 
    "title": "A Multi-server Scheduling Framework for Resource Allocation in Wireless   Multi-carrier Networks", 
    "arxiv-id": "cs/0611080v1", 
    "author": "Ying Jun Zhang", 
    "publish": "2006-11-16T12:33:51Z", 
    "summary": "Multiuser resource allocation has recently been recognized as an effective\nmethodology for enhancing the power and spectrum efficiency in OFDM (orthogonal\nfrequency division multiplexing) systems. It is, however, not directly\napplicable to current packet-switched networks, because (i) most existing\npacket-scheduling schemes are based on a single-server model and do not serve\nmultiple users at the same time; and (ii) the conventional separate design of\nMAC (medium access control) packet scheduling and PHY (physical) resource\nallocation yields inefficient resource utilization. In this paper, we propose a\ncross-layer resource allocation algorithm based on a novel multi-server\nscheduling framework to achieve overall high system power efficiency in\npacket-switched OFDM networks. Our contribution is four fold: (i) we propose\nand analyze a MPGPS (multi-server packetized general processor sharing) service\ndiscipline that serves multiple users at the same time and facilitates\nmultiuser resource allocation; (ii) we present a MPGPS-based joint MAC-PHY\nresource allocation scheme that incorporates packet scheduling, subcarrier\nallocation, and power allocation in an integrated framework; (iii) by\ninvestigating the fundamental tradeoff between multiuser-diversity and queueing\nperformance, we present an A-MPGPS (adaptive MPGPS) service discipline that\nstrikes balance between power efficiency and queueing performance; and (iv) we\nextend MPGPS to an O-MPGPS (opportunistic MPGPS) service discipline to further\nenhance the resource utilization efficiency."
},{
    "category": "cs.MS", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0703025v1", 
    "title": "LIBOPT - An environment for testing solvers on heterogeneous collections   of problems - Version 1.0", 
    "arxiv-id": "cs/0703025v1", 
    "author": "Xavier Jonsson", 
    "publish": "2007-03-06T14:05:28Z", 
    "summary": "The Libopt environment is both a methodology and a set of tools that can be\nused for testing, comparing, and profiling solvers on problems belonging to\nvarious collections. These collections can be heterogeneous in the sense that\ntheir problems can have common features that differ from one collection to the\nother. Libopt brings a unified view on this composite world by offering, for\nexample, the possibility to run any solver on any problem compatible with it,\nusing the same Unix/Linux command. The environment also provides tools for\ncomparing the results obtained by solvers on a specified set of problems. Most\nof the scripts going with the Libopt environment have been written in Perl."
},{
    "category": "cs.CG", 
    "doi": "10.1002/fld.1061", 
    "link": "http://arxiv.org/pdf/cs/0703093v1", 
    "title": "Some problems in asymptotic convex geometry and random matrices   motivated by numerical algorithms", 
    "arxiv-id": "cs/0703093v1", 
    "author": "Roman Vershynin", 
    "publish": "2007-03-19T21:51:50Z", 
    "summary": "The simplex method in Linear Programming motivates several problems of\nasymptotic convex geometry. We discuss some conjectures and known results in\ntwo related directions -- computing the size of projections of high dimensional\npolytopes and estimating the norms of random matrices and their inverses."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.sigpro.2008.01.004", 
    "link": "http://arxiv.org/pdf/cs/0703150v2", 
    "title": "Type-II/III DCT/DST algorithms with reduced number of arithmetic   operations", 
    "arxiv-id": "cs/0703150v2", 
    "author": "Steven G. Johnson", 
    "publish": "2007-03-30T00:53:48Z", 
    "summary": "We present algorithms for the discrete cosine transform (DCT) and discrete\nsine transform (DST), of types II and III, that achieve a lower count of real\nmultiplications and additions than previously published algorithms, without\nsacrificing numerical accuracy. Asymptotically, the operation count is reduced\nfrom ~ 2N log_2 N to ~ (17/9) N log_2 N for a power-of-two transform size N.\nFurthermore, we show that a further N multiplications may be saved by a certain\nrescaling of the inputs or outputs, generalizing a well-known technique for N=8\nby Arai et al. These results are derived by considering the DCT to be a special\ncase of a DFT of length 4N, with certain symmetries, and then pruning redundant\noperations from a recent improved fast Fourier transform algorithm (based on a\nrecursive rescaling of the conjugate-pair split radix algorithm). The improved\nalgorithms for DCT-III, DST-II, and DST-III follow immediately from the\nimproved count for the DCT-II."
},{
    "category": "math.AG", 
    "doi": "10.1016/j.sigpro.2008.01.004", 
    "link": "http://arxiv.org/pdf/math/9809071v2", 
    "title": "Solving Degenerate Sparse Polynomial Systems Faster", 
    "arxiv-id": "math/9809071v2", 
    "author": "J. Maurice Rojas", 
    "publish": "1998-09-14T07:46:19Z", 
    "summary": "Consider a system F of n polynomial equations in n unknowns, over an\nalgebraically closed field of arbitrary characteristic. We present a fast\nmethod to find a point in every irreducible component of the zero set Z of F.\nOur techniques allow us to sharpen and lower prior complexity bounds for this\nproblem by fully taking into account the monomial term structure. As a\ncorollary of our development we also obtain new explicit formulae for the exact\nnumber of isolated roots of F and the intersection multiplicity of the\npositive-dimensional part of Z. Finally, we present a combinatorial\nconstruction of non-degenerate polynomial systems, with specified monomial term\nstructure and maximally many isolated roots, which may be of independent\ninterest."
},lol]