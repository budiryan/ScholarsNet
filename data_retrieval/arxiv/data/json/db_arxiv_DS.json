[{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9301115v1", 
    "title": "Context-free multilanguages", 
    "arxiv-id": "cs/9301115v1", 
    "author": "Donald E. Knuth", 
    "publish": "1991-12-01T00:00:00Z", 
    "summary": "This article is a sketch of ideas that were once intended to appear in the\nauthor's famous series, \"The Art of Computer Programming\". He generalizes the\nnotion of a context-free language from a set to a multiset of words over an\nalphabet. The idea is to keep track of the number of ways to parse a string.\nFor example, \"fruit flies like a banana\" can famously be parsed in two ways;\nanalogous examples in the setting of programming languages may yet be important\nin the future.\n  The treatment is informal but essentially rigorous."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9608105v1", 
    "title": "Shellsort with three increments", 
    "arxiv-id": "cs/9608105v1", 
    "author": "Donald E. Knuth", 
    "publish": "1996-08-22T00:00:00Z", 
    "summary": "A perturbation technique can be used to simplify and sharpen A. C. Yao's\ntheorems about the behavior of shellsort with increments $(h,g,1)$. In\nparticular, when $h=\\Theta(n^{7/15})$ and $g=\\Theta(h^{1/5})$, the average\nrunning time is $O(n^{23/15})$. The proof involves interesting properties of\nthe inversions in random permutations that have been $h$-sorted and $g$-sorted."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9801103v1", 
    "title": "Linear probing and graphs", 
    "arxiv-id": "cs/9801103v1", 
    "author": "Donald E. Knuth", 
    "publish": "1998-01-15T00:00:00Z", 
    "summary": "Mallows and Riordan showed in 1968 that labeled trees with a small number of\ninversions are related to labeled graphs that are connected and sparse. Wright\nenumerated sparse connected graphs in 1977, and Kreweras related the inversions\nof trees to the so-called ``parking problem'' in 1980. A~combination of these\nthree results leads to a surprisingly simple analysis of the behavior of\nhashing by linear probing, including higher moments of the cost of successful\nsearch."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9809012v1", 
    "title": "A Fully Polynomial Randomized Approximation Scheme for the All Terminal   Network Reliability Problem", 
    "arxiv-id": "cs/9809012v1", 
    "author": "David R. Karger", 
    "publish": "1998-09-09T02:38:56Z", 
    "summary": "The classic all-terminal network reliability problem posits a graph, each of\nwhose edges fails independently with some given probability."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9812007v1", 
    "title": "Minimum Cuts in Near-Linear Time", 
    "arxiv-id": "cs/9812007v1", 
    "author": "David R. Karger", 
    "publish": "1998-12-08T21:29:20Z", 
    "summary": "We significantly improve known time bounds for solving the minimum cut\nproblem on undirected graphs. We use a ``semi-duality'' between minimum cuts\nand maximum spanning tree packings combined with our previously developed\nrandom sampling techniques. We give a randomized algorithm that finds a minimum\ncut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We\nalso give a simpler randomized algorithm that finds all minimum cuts with high\nprobability in O(n^2 log n) time. This variant has an optimal RNC\nparallelization. Both variants improve on the previous best time bound of O(n^2\nlog^3 n). Other applications of the tree-packing approach are new, nearly tight\nbounds on the number of near minimum cuts a graph may have and a new data\nstructure for representing them in a space-efficient manner."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9812008v1", 
    "title": "Approximate Graph Coloring by Semidefinite Programming", 
    "arxiv-id": "cs/9812008v1", 
    "author": "Madhu Sudan", 
    "publish": "1998-12-08T22:03:36Z", 
    "summary": "We consider the problem of coloring k-colorable graphs with the fewest\npossible colors. We present a randomized polynomial time algorithm that colors\na 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log\nn), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any\nvertex. Besides giving the best known approximation ratio in terms of n, this\nmarks the first non-trivial approximation result as a function of the maximum\ndegree Delta. This result can be generalized to k-colorable graphs to obtain a\ncoloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}\nlog^{1/2} n) colors. Our results are inspired by the recent work of Goemans and\nWilliamson who used an algorithm for semidefinite optimization problems, which\ngeneralize linear programs, to obtain improved approximations for the MAX CUT\nand MAX 2-SAT problems. An intriguing outcome of our work is a duality\nrelationship established between the value of the optimum solution to our\nsemidefinite program and the Lovasz theta-function. We show lower bounds on the\ngap between the optimum solution of our semidefinite program and the actual\nchromatic number; by duality this also demonstrates interesting new facts about\nthe theta-function."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9903010v1", 
    "title": "A class of problems of NP to be worth to search an efficient solving   algorithm", 
    "arxiv-id": "cs/9903010v1", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "1999-03-11T19:36:05Z", 
    "summary": "We examine possibility to design an efficient solving algorithm for problems\nof the class \\np. It is introduced a classification of \\np problems by the\nproperty that a partial solution of size $k$ can be extended into a partial\nsolution of size $k+1$ in polynomial time. It is defined an unique class\nproblems to be worth to search an efficient solving algorithm. The problems,\nwhich are outside of this class, are inherently exponential. We show that the\nHamiltonian cycle problem is inherently exponential."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9906021v1", 
    "title": "Reconstructing hv-Convex Polyominoes from Orthogonal Projections", 
    "arxiv-id": "cs/9906021v1", 
    "author": "Marek Chrobak", 
    "publish": "1999-06-22T09:56:53Z", 
    "summary": "Tomography is the area of reconstructing objects from projections. Here we\nwish to reconstruct a set of cells in a two dimensional grid, given the number\nof cells in every row and column. The set is required to be an hv-convex\npolyomino, that is all its cells must be connected and the cells in every row\nand column must be consecutive. A simple, polynomial algorithm for\nreconstructing hv-convex polyominoes is provided, which is several orders of\nmagnitudes faster than the best previously known algorithm from Barcucci et al.\nIn addition, the problem of reconstructing a special class of centered\nhv-convex polyominoes is addressed. (An object is centered if it contains a row\nwhose length equals the total width of the object). It is shown that in this\ncase the reconstruction problem can be solved in linear time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TWC.2008.070194", 
    "link": "http://arxiv.org/pdf/cs/9911003v1", 
    "title": "Subgraph Isomorphism in Planar Graphs and Related Problems", 
    "arxiv-id": "cs/9911003v1", 
    "author": "David Eppstein", 
    "publish": "1999-11-09T18:58:58Z", 
    "summary": "We solve the subgraph isomorphism problem in planar graphs in linear time,\nfor any pattern of constant size. Our results are based on a technique of\npartitioning the planar graph into pieces of small tree-width, and applying\ndynamic programming within each piece. The same methods can be used to solve\nother planar graph problems including connectivity, diameter, girth, induced\nsubgraph isomorphism, and shortest paths."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/9912014v1", 
    "title": "Fast Hierarchical Clustering and Other Applications of Dynamic Closest   Pairs", 
    "arxiv-id": "cs/9912014v1", 
    "author": "David Eppstein", 
    "publish": "1999-12-22T01:42:51Z", 
    "summary": "We develop data structures for dynamic closest pair problems with arbitrary\ndistance functions, that do not necessarily come from any geometric structure\non the objects. Based on a technique previously used by the author for\nEuclidean closest pairs, we show how to insert and delete objects from an\nn-object set, maintaining the closest pair, in O(n log^2 n) time per update and\nO(n) space. With quadratic space, we can instead use a quadtree-like structure\nto achieve an optimal time bound, O(n) per update. We apply these data\nstructures to hierarchical clustering, greedy matching, and TSP heuristics, and\ndiscuss other potential applications in machine learning, Groebner bases, and\nlocal improvement algorithms for partition and placement problems. Experiments\nshow our new methods to be faster in practice than previously used heuristics."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/9912020v2", 
    "title": "Additive models in high dimensions", 
    "arxiv-id": "cs/9912020v2", 
    "author": "Vladimir Pestov", 
    "publish": "1999-12-30T07:50:11Z", 
    "summary": "We discuss some aspects of approximating functions on high-dimensional data\nsets with additive functions or ANOVA decompositions, that is, sums of\nfunctions depending on fewer variables each. It is seen that under appropriate\nsmoothness conditions, the errors of the ANOVA decompositions are of order\n$O(n^{m/2})$ for approximations using sums of functions of up to $m$ variables\nunder some mild restrictions on the (possibly dependent) predictor variables.\nSeveral simulated examples illustrate this behaviour."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/0003078v1", 
    "title": "About the finding of independent vertices of a graph", 
    "arxiv-id": "cs/0003078v1", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "2000-03-24T23:31:24Z", 
    "summary": "We examine the Maximum Independent Set Problem in an undirected graph. The\nmain result is that this problem can be considered as the solving the same\nproblem in a subclass of the weighted normal twin-orthogonal graphs. The\nproblem is formulated which is dual to the problem above. It is shown that, for\ntrivial twin-orthogonal graphs, any of its maximal independent set is also\nmaximum one."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0006046v1", 
    "title": "3-Coloring in Time O(1.3289^n)", 
    "arxiv-id": "cs/0006046v1", 
    "author": "David Eppstein", 
    "publish": "2000-06-30T22:04:04Z", 
    "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT,\n3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a\nconstraint satisfaction (CSP) formulation of these problems. 3-SAT is\nequivalent to (2,3)-CSP while the other problems above are special cases of\n(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to\n(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the\ntime bounds for solving the other problems listed above. Our techniques involve\na mixture of Davis-Putnam-style backtracking with more sophisticated matching\nand network flow based ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0007029v1", 
    "title": "Dimension-Dependent behavior in the satisfability of random k-Horn   formulae", 
    "arxiv-id": "cs/0007029v1", 
    "author": "Gabriel Istrate", 
    "publish": "2000-07-18T21:50:04Z", 
    "summary": "We determine the asymptotical satisfiability probability of a random\nat-most-k-Horn formula, via a probabilistic analysis of a simple version,\ncalled PUR, of positive unit resolution. We show that for k=k(n)->oo the\nproblem can be ``reduced'' to the case k(n)=n, that was solved in\ncs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR\nis modeled by a simple queuing chain, leading to a closed-form solution when\nk=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.\nUnder a rescaled parameter, the graphs of satisfaction probability\ncorresponding to finite values of k converge to the one for the uniform case, a\n``dimension-dependent behavior'' similar to the one found experimentally by\nKirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively\nexplained by a threshold property for the number of iterations of PUR makes on\nrandom satisfiable Horn formulas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0007043v1", 
    "title": "Min-Max Fine Heaps", 
    "arxiv-id": "cs/0007043v1", 
    "author": "M. Kaykobad", 
    "publish": "2000-07-31T00:13:17Z", 
    "summary": "In this paper we present a new data structure for double ended priority\nqueue, called min-max fine heap, which combines the techniques used in fine\nheap and traditional min-max heap. The standard operations on this proposed\nstructure are also presented, and their analysis indicates that the new\nstructure outperforms the traditional one."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0008011v1", 
    "title": "All Pairs Shortest Paths using Bridging Sets and Rectangular Matrix   Multiplication", 
    "arxiv-id": "cs/0008011v1", 
    "author": "Uri Zwick", 
    "publish": "2000-08-16T20:39:45Z", 
    "summary": "We present two new algorithms for solving the {\\em All Pairs Shortest Paths}\n(APSP) problem for weighted directed graphs. Both algorithms use fast matrix\nmultiplication algorithms.\n  The first algorithm solves the APSP problem for weighted directed graphs in\nwhich the edge weights are integers of small absolute value in $\\Ot(n^{2+\\mu})$\ntime, where $\\mu$ satisfies the equation $\\omega(1,\\mu,1)=1+2\\mu$ and\n$\\omega(1,\\mu,1)$ is the exponent of the multiplication of an $n\\times n^\\mu$\nmatrix by an $n^\\mu \\times n$ matrix. Currently, the best available bounds on\n$\\omega(1,\\mu,1)$, obtained by Coppersmith, imply that $\\mu<0.575$. The running\ntime of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on\nthe $\\Ot(n^{(3+\\omega)/2})$ time algorithm, where $\\omega=\\omega(1,1,1)<2.376$\nis the usual exponent of matrix multiplication, obtained by Alon, Galil and\nMargalit, whose running time is only known to be $O(n^{2.688})$.\n  The second algorithm solves the APSP problem {\\em almost} exactly for\ndirected graphs with {\\em arbitrary} non-negative real weights. The algorithm\nruns in $\\Ot((n^\\omega/\\eps)\\log (W/\\eps))$ time, where $\\eps>0$ is an error\nparameter and W is the largest edge weight in the graph, after the edge weights\nare scaled so that the smallest non-zero edge weight in the graph is 1. It\nreturns estimates of all the distances in the graph with a stretch of at most\n$1+\\eps$. Corresponding paths can also be found efficiently."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0009006v1", 
    "title": "Improved Algorithms for 3-Coloring, 3-Edge-Coloring, and Constraint   Satisfaction", 
    "arxiv-id": "cs/0009006v1", 
    "author": "David Eppstein", 
    "publish": "2000-09-13T20:42:55Z", 
    "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT,\n3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a\nconstraint satisfaction (CSP) formulation of these problems; 3-SAT is\nequivalent to (2,3)-CSP while the other problems above are special cases of\n(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the\ntime bounds for solving the other problems listed above. Our techniques involve\na mixture of Davis-Putnam-style backtracking with more sophisticated matching\nand network flow based ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0011047v1", 
    "title": "Dancing links", 
    "arxiv-id": "cs/0011047v1", 
    "author": "Donald E. Knuth", 
    "publish": "2000-11-15T00:00:00Z", 
    "summary": "The author presents two tricks to accelerate depth-first search algorithms\nfor a class of combinatorial puzzle problems, such as tiling a tray by a fixed\nset of polyominoes. The first trick is to implement each assumption of the\nsearch with reversible local operations on doubly linked lists. By this trick,\nevery step of the search affects the data incrementally.\n  The second trick is to add a ghost square that represents the identity of\neach polyomino. Thus puts the rule that each polyomino be used once on the same\nfooting as the rule that each square be covered once. The coding simplifies to\na more abstract form which is equivalent to 0-1 integer programming. More\nsignificantly for the total computation time, the search can naturally switch\nbetween placing a fixed polyomino or covering a fixed square at different\nstages, according to a combined heuristic.\n  Finally the author reports excellent performance for his algorithm for some\nfamiliar puzzles. These include tiling a hexagon by 19 hexiamonds and the N\nqueens problem for N up to 18."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0012002v1", 
    "title": "Random Shuffling to Reduce Disorder in Adaptive Sorting Scheme", 
    "arxiv-id": "cs/0012002v1", 
    "author": "Abdun Naser Mahmood", 
    "publish": "2000-12-02T17:47:26Z", 
    "summary": "In this paper we present a random shuffling scheme to apply with adaptive\nsorting algorithms. Adaptive sorting algorithms utilize the presortedness\npresent in a given sequence. We have probabilistically increased the amount of\npresortedness present in a sequence by using a random shuffling technique that\nrequires little computation. Theoretical analysis suggests that the proposed\nscheme can improve the performance of adaptive sorting. Experimental results\nshow that it significantly reduces the amount of disorder present in a given\nsequence and improves the execution time of adaptive sorting algorithm as well."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0111050v7", 
    "title": "Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes   Polynomial Time", 
    "arxiv-id": "cs/0111050v7", 
    "author": "Shang-Hua Teng", 
    "publish": "2001-11-19T23:37:14Z", 
    "summary": "We introduce the smoothed analysis of algorithms, which is a hybrid of the\nworst-case and average-case analysis of algorithms. In smoothed analysis, we\nmeasure the maximum over inputs of the expected performance of an algorithm\nunder small random perturbations of that input. We measure this performance in\nterms of both the input size and the magnitude of the perturbations. We show\nthat the simplex algorithm has polynomial smoothed complexity."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0112022v2", 
    "title": "Faster Algorithm of String Comparison", 
    "arxiv-id": "cs/0112022v2", 
    "author": "Sun Peng", 
    "publish": "2001-12-21T05:58:12Z", 
    "summary": "In many applications, it is necessary to determine the string similarity.\nEdit distance[WF74] approach is a classic method to determine Field Similarity.\nA well known dynamic programming algorithm [GUS97] is used to calculate edit\ndistance with the time complexity O(nm). (for worst case, average case and even\nbest case) Instead of continuing with improving the edit distance approach,\n[LL+99] adopted a brand new approach-token-based approach. Its new concept of\ntoken-base-retain the original semantic information, good time complex-O(nm)\n(for worst, average and best case) and good experimental performance make it a\nmilestone paper in this area. Further study indicates that there is still room\nfor improvement of its Field Similarity algorithm. Our paper is to introduce a\npackage of substring-based new algorithms to determine Field Similarity.\nCombined together, our new algorithms not only achieve higher accuracy but also\ngain the time complexity O(knm) (k<0.75) for worst case, O(*n) where <6 for\naverage case and O(1) for best case. Throughout the paper, we use the approach\nof comparative examples to show higher accuracy of our algorithms compared to\nthe one proposed in [LL+99]. Theoretical analysis, concrete examples and\nexperimental result show that our algorithms can significantly improve the\naccuracy and time complexity of the calculation of Field Similarity. [US97] D.\nGuseld. Algorithms on Strings, Trees and Sequences, in Computer Science and\nComputational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and\nwarehousing, In Proceedings of the 10th International Conference on Database\nand Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R.\nWagner and M. Fisher, The String to String Correction Problem, JACM 21 pages\n168-173, 1974."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0203018v1", 
    "title": "Improving Table Compression with Combinatorial Optimization", 
    "arxiv-id": "cs/0203018v1", 
    "author": "Raffaele Giancarlo", 
    "publish": "2002-03-13T19:09:41Z", 
    "summary": "We study the problem of compressing massive tables within the\npartition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which\na table is partitioned by an off-line training procedure into disjoint\nintervals of columns, each of which is compressed separately by a standard,\non-line compressor like gzip. We provide a new theory that unifies previous\nexperimental observations on partitioning and heuristic observations on column\npermutation, all of which are used to improve compression rates. Based on the\ntheory, we devise the first on-line training algorithms for table compression,\nwhich can be applied to individual files, not just continuously operating\nsources; and also a new, off-line training algorithm, based on a link to the\nasymmetric traveling salesman problem, which improves on prior work by\nrearranging columns prior to partitioning. We demonstrate these results\nexperimentally. On various test files, the on-line algorithms provide 35-55%\nimprovement over gzip with negligible slowdown; the off-line reordering\nprovides up to 20% further improvement over partitioning alone. We also show\nthat a variation of the table compression problem is MAX-SNP hard."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0204033v1", 
    "title": "Randomized selection revisited", 
    "arxiv-id": "cs/0204033v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2002-04-15T04:48:21Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select for\nfinding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This\nrectifies the analysis of Floyd and Rivest, and extends it to the case of\nnondistinct elements. Our computational results confirm that Select may be the\nbest algorithm in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1109/DCC.1997.582053", 
    "link": "http://arxiv.org/pdf/cs/0205029v1", 
    "title": "A Codebook Generation Algorithm for Document Image Compression", 
    "arxiv-id": "cs/0205029v1", 
    "author": "Neal Young", 
    "publish": "2002-05-17T23:52:11Z", 
    "summary": "Pattern-matching-based document-compression systems (e.g. for faxing) rely on\nfinding a small set of patterns that can be used to represent all of the ink in\nthe document. Finding an optimal set of patterns is NP-hard; previous\ncompression schemes have resorted to heuristics. This paper describes an\nextension of the cross-entropy approach, used previously for measuring pattern\nsimilarity, to this problem. This approach reduces the problem to a k-medians\nproblem, for which the paper gives a new algorithm with a provably good\nperformance guarantee. In comparison to previous heuristics (First Fit, with\nand without generalized Lloyd's/k-means postprocessing steps), the new\nalgorithm generates a better codebook, resulting in an overall improvement in\ncompression performance of almost 17%."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.2001.959930", 
    "link": "http://arxiv.org/pdf/cs/0205039v1", 
    "title": "Sequential and Parallel Algorithms for Mixed Packing and Covering", 
    "arxiv-id": "cs/0205039v1", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T15:12:41Z", 
    "summary": "Mixed packing and covering problems are problems that can be formulated as\nlinear programs using only non-negative coefficients. Examples include\nmulticommodity network flow, the Held-Karp lower bound on TSP, fractional\nrelaxations of set cover, bin-packing, knapsack, scheduling problems,\nminimum-weight triangulation, etc. This paper gives approximation algorithms\nfor the general class of problems. The sequential algorithm is a simple greedy\nalgorithm that can be implemented to find an epsilon-approximate solution in\nO(epsilon^-2 log m) linear-time iterations. The parallel algorithm does\ncomparable work but finishes in polylogarithmic time.\n  The results generalize previous work on pure packing and covering (the\nspecial case when the constraints are all \"less-than\" or all \"greater-than\") by\nMichael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/100794092", 
    "link": "http://arxiv.org/pdf/cs/0205048v2", 
    "title": "Huffman Coding with Letter Costs: A Linear-Time Approximation Scheme", 
    "arxiv-id": "cs/0205048v2", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T18:57:04Z", 
    "summary": "We give a polynomial-time approximation scheme for the generalization of\nHuffman Coding in which codeword letters have non-uniform costs (as in Morse\ncode, where the dash is twice as long as the dot). The algorithm computes a\n(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is\nthe input size."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0205049v1", 
    "title": "Prefix Codes: Equiprobable Words, Unequal Letter Costs", 
    "arxiv-id": "cs/0205049v1", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T19:05:55Z", 
    "summary": "Describes a near-linear-time algorithm for a variant of Huffman coding, in\nwhich the letters may have non-uniform lengths (as in Morse code), but with the\nrestriction that each word to be encoded has equal probability. [See also\n``Huffman Coding with Unequal Letter Costs'' (2002).]"
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0206033v1", 
    "title": "Algorithms for Media", 
    "arxiv-id": "cs/0206033v1", 
    "author": "Jean-Claude Falmagne", 
    "publish": "2002-06-24T06:50:52Z", 
    "summary": "Falmagne recently introduced the concept of a medium, a combinatorial object\nencompassing hyperplane arrangements, topological orderings, acyclic\norientations, and many other familiar structures. We find efficient solutions\nfor several algorithmic problems on media: finding short reset sequences,\nshortest paths, testing whether a medium has a closed orientation, and listing\nthe states of a medium given a black-box description."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207061v2", 
    "title": "Linear-Time Pointer-Machine Algorithms for Path-Evaluation Problems on   Trees and Graphs", 
    "arxiv-id": "cs/0207061v2", 
    "author": "Jeffery R. Westbrook", 
    "publish": "2002-07-15T18:47:57Z", 
    "summary": "We present algorithms that run in linear time on pointer machines for a\ncollection of problems, each of which either directly or indirectly requires\nthe evaluation of a function defined on paths in a tree. These problems\npreviously had linear-time algorithms but only for random-access machines\n(RAMs); the best pointer-machine algorithms were super-linear by an\ninverse-Ackermann-function factor. Our algorithms are also simpler, in some\ncases substantially, than the previous linear-time RAM algorithms. Our\nimprovements come primarily from three new ideas: a refined analysis of path\ncompression that gives a linear bound if the compressions favor certain nodes,\na pointer-based radix sort as a replacement for table-based methods, and a more\ncareful partitioning of a tree into easily managed parts. Our algorithms\ncompute nearest common ancestors off-line, verify and construct minimum\nspanning trees, do interval analysis on a flowgraph, find the dominators of a\nflowgraph, and build the component tree of a weighted tree."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207066v1", 
    "title": "Polynomial Time Data Reduction for Dominating Set", 
    "arxiv-id": "cs/0207066v1", 
    "author": "Rolf Niedermeier", 
    "publish": "2002-07-16T17:58:48Z", 
    "summary": "Dealing with the NP-complete Dominating Set problem on undirected graphs, we\ndemonstrate the power of data reduction by preprocessing from a theoretical as\nwell as a practical side. In particular, we prove that Dominating Set\nrestricted to planar graphs has a so-called problem kernel of linear size,\nachieved by two simple and easy to implement reduction rules. Moreover, having\nimplemented our reduction rules, first experiments indicate the impressive\npractical potential of these rules. Thus, this work seems to open up a new and\nprospective way how to cope with one of the most important problems in graph\ntheory and combinatorial optimization."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207082v1", 
    "title": "Dynamic Generators of Topologically Embedded Graphs", 
    "arxiv-id": "cs/0207082v1", 
    "author": "David Eppstein", 
    "publish": "2002-07-24T19:56:17Z", 
    "summary": "We provide a data structure for maintaining an embedding of a graph on a\nsurface (represented combinatorially by a permutation of edges around each\nvertex) and computing generators of the fundamental group of the surface, in\namortized time O(log n + log g(log log g)^3) per update on a surface of genus\ng; we can also test orientability of the surface in the same time, and maintain\nthe minimum and maximum spanning tree of the graph in time O(log n + log^4 g)\nper update. Our data structure allows edge insertion and deletion as well as\nthe dual operations; these operations may implicitly change the genus of the\nembedding surface. We apply similar ideas to improve the constant factor in a\nseparator theorem for low-genus graphs, and to find in linear time a\ntree-decomposition of low-genus low-diameter graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0209033v2", 
    "title": "Preemptive Scheduling of Equal-Length Jobs to Maximize Weighted   Throughput", 
    "arxiv-id": "cs/0209033v2", 
    "author": "Nodari Vakhania", 
    "publish": "2002-09-30T10:33:13Z", 
    "summary": "We study the problem of computing a preemptive schedule of equal-length jobs\nwith given release times, deadlines and weights. Our goal is to maximize the\nweighted throughput, which is the total weight of completed jobs. In Graham's\nnotation this problem is described as (1 | r_j;p_j=p;pmtn | sum w_j U_j). We\nprovide an O(n^4)-time algorithm for this problem, improving the previous bound\nof O(n^{10}) by Baptiste."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0210006v2", 
    "title": "Dynamic Ordered Sets with Exponential Search Trees", 
    "arxiv-id": "cs/0210006v2", 
    "author": "Mikkel Thorup", 
    "publish": "2002-10-09T04:49:56Z", 
    "summary": "We introduce exponential search trees as a novel technique for converting\nstatic polynomial space search structures for ordered sets into fully-dynamic\nlinear space data structures.\n  This leads to an optimal bound of O(sqrt(log n/loglog n)) for searching and\nupdating a dynamic set of n integer keys in linear space. Here searching an\ninteger y means finding the maximum key in the set which is smaller than or\nequal to y. This problem is equivalent to the standard text book problem of\nmaintaining an ordered set (see, e.g., Cormen, Leiserson, Rivest, and Stein:\nIntroduction to Algorithms, 2nd ed., MIT Press, 2001).\n  The best previous deterministic linear space bound was O(log n/loglog n) due\nFredman and Willard from STOC 1990. No better deterministic search bound was\nknown using polynomial space.\n  We also get the following worst-case linear space trade-offs between the\nnumber n, the word length w, and the maximal key U < 2^w: O(min{loglog n+log\nn/log w, (loglog n)(loglog U)/(logloglog U)}). These trade-offs are, however,\nnot likely to be optimal.\n  Our results are generalized to finger searching and string searching,\nproviding optimal results for both in terms of n."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0210013v2", 
    "title": "On the Sum-of-Squares Algorithm for Bin Packing", 
    "arxiv-id": "cs/0210013v2", 
    "author": "Richard R. Weber", 
    "publish": "2002-10-14T17:17:28Z", 
    "summary": "In this paper we present a theoretical analysis of the deterministic on-line\n{\\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studied\nexperimentally in \\cite{CJK99}, along with several new variants. $SS$ is\napplicable to any instance of bin packing in which the bin capacity $B$ and\nitem sizes $s(a)$ are integral (or can be scaled to be so), and runs in time\n$O(nB)$. It performs remarkably well from an average case point of view: For\nany discrete distribution in which the optimal expected waste is sublinear,\n$SS$ also has sublinear expected waste. For any discrete distribution where the\noptimal expected waste is bounded, $SS$ has expected waste at most $O(\\log n)$.\nIn addition, we discuss several interesting variants on $SS$, including a\nrandomized $O(nB\\log B)$-time on-line algorithm $SS^*$, based on $SS$, whose\nexpected behavior is essentially optimal for all discrete distributions.\nAlgorithm $SS^*$ also depends on a new linear-programming-based\npseudopolynomial-time algorithm for solving the NP-hard problem of determining,\ngiven a discrete distribution $F$, just what is the growth rate for the optimal\nexpected waste. This article is a greatly expanded version of the conference\npaper \\cite{sumsq2000}."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211001v2", 
    "title": "Fast and Simple Computation of All Longest Common Subsequences", 
    "arxiv-id": "cs/0211001v2", 
    "author": "Ronald I. Greenberg", 
    "publish": "2002-11-01T06:23:00Z", 
    "summary": "This paper shows that a simple algorithm produces the {\\em\nall-prefixes-LCSs-graph} in $O(mn)$ time for two input sequences of size $m$\nand $n$. Given any prefix $p$ of the first input sequence and any prefix $q$ of\nthe second input sequence, all longest common subsequences (LCSs) of $p$ and\n$q$ can be generated in time proportional to the output size, once the\nall-prefixes-LCSs-graph has been constructed. The problem can be solved in the\ncontext of generating all the distinct character strings that represent an LCS\nor in the context of generating all ways of embedding an LCS in the two input\nstrings."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211009v1", 
    "title": "Improved Phylogeny Comparisons: Non-Shared Edges Nearest Neighbor   Interchanges, and Subtree Transfers", 
    "arxiv-id": "cs/0211009v1", 
    "author": "Siu-Ming Yiu", 
    "publish": "2002-11-11T12:02:30Z", 
    "summary": "The number of the non-shared edges of two phylogenies is a basic measure of\nthe dissimilarity between the phylogenies. The non-shared edges are also the\nbuilding block for approximating a more sophisticated metric called the nearest\nneighbor interchange (NNI) distance. In this paper, we give the first\nsubquadratic-time algorithm for finding the non-shared edges, which are then\nused to speed up the existing approximating algorithm for the NNI distance from\n$O(n^2)$ time to $O(n \\log n)$ time. Another popular distance metric for\nphylogenies is the subtree transfer (STT) distance. Previous work on computing\nthe STT distance considered degree-3 trees only. We give an approximation\nalgorithm for the STT distance for degree-$d$ trees with arbitrary $d$ and with\ngeneralized STT operations."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211010v2", 
    "title": "Efficient Tree Layout in a Multilevel Memory Hierarchy", 
    "arxiv-id": "cs/0211010v2", 
    "author": "Mikkel Thorup", 
    "publish": "2002-11-12T03:32:02Z", 
    "summary": "We consider the problem of laying out a tree with fixed parent/child\nstructure in hierarchical memory. The goal is to minimize the expected number\nof block transfers performed during a search along a root-to-leaf path, subject\nto a given probability distribution on the leaves. This problem was previously\nconsidered by Gil and Itai, who developed optimal but slow algorithms when the\nblock-transfer size B is known. We present faster but approximate algorithms\nfor the same problem; the fastest such algorithm runs in linear time and\nproduces a solution that is within an additive constant of optimal.\n  In addition, we show how to extend any approximately optimal algorithm to the\ncache-oblivious setting in which the block-transfer size is unknown to the\nalgorithm. The query performance of the cache-oblivious layout is within a\nconstant factor of the query performance of the optimal known-block-size\nlayout. Computing the cache-oblivious layout requires only logarithmically many\ncalls to the layout algorithm for known block size; in particular, the\ncache-oblivious layout can be computed in O(N lg N) time, where N is the number\nof nodes.\n  Finally, we analyze two greedy strategies, and show that they have a\nperformance ratio between Omega(lg B / lg lg B) and O(lg B) when compared to\nthe optimal layout."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211018v2", 
    "title": "Indexing schemes for similarity search: an illustrated paradigm", 
    "arxiv-id": "cs/0211018v2", 
    "author": "Aleksandar Stojmirovic", 
    "publish": "2002-11-14T19:10:16Z", 
    "summary": "We suggest a variation of the Hellerstein--Koutsoupias--Papadimitriou\nindexability model for datasets equipped with a similarity measure, with the\naim of better understanding the structure of indexing schemes for\nsimilarity-based search and the geometry of similarity workloads. This in\nparticular provides a unified approach to a great variety of schemes used to\nindex into metric spaces and facilitates their transfer to more general\nsimilarity measures such as quasi-metrics. We discuss links between performance\nof indexing schemes and high-dimensional geometry. The concepts and results are\nillustrated on a very large concrete dataset of peptide fragments equipped with\na biologically significant similarity measure."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0212044v1", 
    "title": "Solving a \"Hard\" Problem to Approximate an \"Easy\" One: Heuristics for   Maximum Matchings and Maximum Traveling Salesman Problems", 
    "arxiv-id": "cs/0212044v1", 
    "author": "Walter Tietze", 
    "publish": "2002-12-16T09:39:16Z", 
    "summary": "We consider geometric instances of the Maximum Weighted Matching Problem\n(MWMP) and the Maximum Traveling Salesman Problem (MTSP) with up to 3,000,000\nvertices. Making use of a geometric duality relationship between MWMP, MTSP,\nand the Fermat-Weber-Problem (FWP), we develop a heuristic approach that yields\nin near-linear time solutions as well as upper bounds. Using various\ncomputational tools, we get solutions within considerably less than 1% of the\noptimum.\n  An interesting feature of our approach is that, even though an FWP is hard to\ncompute in theory and Edmonds' algorithm for maximum weighted matching yields a\npolynomial solution for the MWMP, the practical behavior is just the opposite,\nand we can solve the FWP with high accuracy in order to find a good heuristic\nsolution for the MWMP."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0301019v1", 
    "title": "Smoothed Analysis of Interior-Point Algorithms: Termination", 
    "arxiv-id": "cs/0301019v1", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-01-21T17:47:05Z", 
    "summary": "We perform a smoothed analysis of the termination phase of an interior-point\nmethod. By combining this analysis with the smoothed analysis of Renegar's\ninterior-point algorithm by Dunagan, Spielman and Teng, we show that the\nsmoothed complexity of an interior-point algorithm for linear programming is $O\n(m^{3} \\log (m/\\sigma))$. In contrast, the best known bound on the worst-case\ncomplexity of linear programming is $O (m^{3} L)$, where $L$ could be as large\nas $m$. We include an introduction to smoothed analysis and a tutorial on proof\ntechniques that have been useful in smoothed analyses."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0301021v2", 
    "title": "PHORMA: Perfectly Hashable Order Restricted Multidimensional Arrays", 
    "arxiv-id": "cs/0301021v2", 
    "author": "Silvio Melo", 
    "publish": "2003-01-21T23:55:17Z", 
    "summary": "In this paper we propose a simple and efficient data structure yielding a\nperfect hashing of quite general arrays. The data structure is named phorma,\nwhich is an acronym for perfectly hashable order restricted multidimensional\narray.\n  Keywords: Perfect hash function, Digraph, Implicit enumeration,\nNijenhuis-Wilf combinatorial family."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0302030v2", 
    "title": "The traveling salesman problem for cubic graphs", 
    "arxiv-id": "cs/0302030v2", 
    "author": "David Eppstein", 
    "publish": "2003-02-20T06:36:35Z", 
    "summary": "We show how to find a Hamiltonian cycle in a graph of degree at most three\nwith n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithm\ncan find the minimum weight Hamiltonian cycle (traveling salesman problem), in\nthe same time bound. We can also count or list all Hamiltonian cycles in a\ndegree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the traveling\nsalesman problem in graphs of degree at most four, by randomized and\ndeterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n and\nO((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specify\na set of forced edges which must be part of any generated cycle. Our cycle\nlisting algorithm shows that every degree three graph has O(2^{3n/8})\nHamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltonian\ncycles per graph."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0304005v1", 
    "title": "Quantum Computation and Lattice Problems", 
    "arxiv-id": "cs/0304005v1", 
    "author": "Oded Regev", 
    "publish": "2003-04-01T23:35:11Z", 
    "summary": "We present the first explicit connection between quantum computation and\nlattice problems. Namely, we show a solution to the Unique Shortest Vector\nProblem (SVP) under the assumption that there exists an algorithm that solves\nthe hidden subgroup problem on the dihedral group by coset sampling. Moreover,\nwe solve the hidden subgroup problem on the dihedral group by using an average\ncase subset sum routine. By combining the two results, we get a quantum\nreduction from $\\Theta(n^{2.5})$-unique-SVP to the average case subset sum\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306046v1", 
    "title": "Compact Approximation of Lattice Functions with Applications to   Large-Alphabet Text Search", 
    "arxiv-id": "cs/0306046v1", 
    "author": "Sebastiano Vigna", 
    "publish": "2003-06-11T09:13:39Z", 
    "summary": "We propose a very simple randomised data structure that stores an\napproximation from above of a lattice-valued function. Computing the function\nvalue requires a constant number of steps, and the error probability can be\nbalanced with space usage, much like in Bloom filters. The structure is\nparticularly well suited for functions that are bottom on most of their domain.\nWe then show how to use our methods to store in a compact way the bad-character\nshift function for variants of the Boyer-Moore text search algorithms. As a\nresult, we obtain practical implementations of these algorithms that can be\nused with large alphabets, such as Unicode collation elements, with a small\nsetup time. The ideas described in this paper have been implemented as free\nsoftware under the GNU General Public License within the MG4J project\n(http://mg4j.dsi.unimi.it/)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306104v1", 
    "title": "Efficient pebbling for list traversal synopses", 
    "arxiv-id": "cs/0306104v1", 
    "author": "Ely Porat", 
    "publish": "2003-06-16T21:31:36Z", 
    "summary": "We show how to support efficient back traversal in a unidirectional list,\nusing small memory and with essentially no slowdown in forward steps. Using\n$O(\\log n)$ memory for a list of size $n$, the $i$'th back-step from the\nfarthest point reached so far takes $O(\\log i)$ time in the worst case, while\nthe overhead per forward step is at most $\\epsilon$ for arbitrary small\nconstant $\\epsilon>0$. An arbitrary sequence of forward and back steps is\nallowed. A full trade-off between memory usage and time per back-step is\npresented: $k$ vs. $kn^{1/k}$ and vice versa. Our algorithms are based on a\nnovel pebbling technique which moves pebbles on a virtual binary, or $t$-ary,\ntree that can only be traversed in a pre-order fashion. The compact data\nstructures used by the pebbling algorithms, called list traversal synopses,\nextend to general directed graphs, and have other interesting applications,\nincluding memory efficient hash-chain implementation. Perhaps the most\nsurprising application is in showing that for any program, arbitrary rollback\nsteps can be efficiently supported with small overhead in memory, and marginal\noverhead in its ordinary execution. More concretely: Let $P$ be a program that\nruns for at most $T$ steps, using memory of size $M$. Then, at the cost of\nrecording the input used by the program, and increasing the memory by a factor\nof $O(\\log T)$ to $O(M \\log T)$, the program $P$ can be extended to support an\narbitrary sequence of forward execution and rollback steps: the $i$'th rollback\nstep takes $O(\\log i)$ time in the worst case, while forward steps take O(1)\ntime in the worst case, and $1+\\epsilon$ amortized time per step."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306123v1", 
    "title": "Heuristic to reduce the complexity of complete bipartite graphs to   accelerate the search for maximum weighted matchings with small error", 
    "arxiv-id": "cs/0306123v1", 
    "author": "Daniel Etzold", 
    "publish": "2003-06-23T19:37:42Z", 
    "summary": "A maximum weighted matching for bipartite graphs $G=(A \\cup B,E)$ can be\nfound by using the algorithm of Edmonds and Karp with a Fibonacci Heap and a\nmodified Dijkstra in $O(nm + n^2 \\log{n})$ time where n is the number of nodes\nand m the number of edges. For the case that $|A|=|B|$ the number of edges is\n$n^2$ and therefore the complexity is $O(n^3)$. In this paper we want to\npresent a simple heuristic method to reduce the number of edges of complete\nbipartite graphs $G=(A \\cup B,E)$ with $|A|=|B|$ such that $m = n\\log{n}$ and\ntherefore the complexity of such that $m = n\\log{n}$ and therefore the\ncomplexity of $O(n^2 \\log{n})$. The weights of all edges in G must be uniformly\ndistributed in [0,1]."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0307034v1", 
    "title": "Range Mode and Range Median Queries on Lists and Trees", 
    "arxiv-id": "cs/0307034v1", 
    "author": "Michiel Smid", 
    "publish": "2003-07-12T21:41:56Z", 
    "summary": "We consider algorithms for preprocessing labelled lists and trees so that,\nfor any two nodes u and v we can answer queries of the form: What is the mode\nor median label in the sequence of labels on the path from u to v."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0307043v1", 
    "title": "An Extension of the Lovasz Local Lemma, and its Applications to Integer   Programming", 
    "arxiv-id": "cs/0307043v1", 
    "author": "Aravind Srinivasan", 
    "publish": "2003-07-18T04:02:18Z", 
    "summary": "The Lovasz Local Lemma due to Erdos and Lovasz is a powerful tool in proving\nthe existence of rare events. We present an extension of this lemma, which\nworks well when the event to be shown to exist is a conjunction of individual\nevents, each of which asserts that a random variable does not deviate much from\nits mean. As applications, we consider two classes of NP-hard integer programs:\nminimax and covering integer programs. A key technique, randomized rounding of\nlinear relaxations, was developed by Raghavan and Thompson to derive good\napproximation algorithms for such problems. We use our extension of the Local\nLemma to prove that randomized rounding produces, with non-zero probability,\nmuch better feasible solutions than known before, if the constraint matrices of\nthese integer programs are column-sparse (e.g., routing using short paths,\nproblems on hypergraphs with small dimension/degree). This complements certain\nwell-known results from discrepancy theory. We also generalize the method of\npessimistic estimators due to Raghavan, to obtain constructive (algorithmic)\nversions of our results for covering integer programs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0308041v1", 
    "title": "Static Data Structure for Discrete Advance Bandwidth Reservations on the   Internet", 
    "arxiv-id": "cs/0308041v1", 
    "author": "Andreas Nilsson", 
    "publish": "2003-08-24T06:30:41Z", 
    "summary": "In this paper we present a discrete data structure for reservations of\nlimited resources. A reservation is defined as a tuple consisting of the time\ninterval of when the resource should be reserved, $I_R$, and the amount of the\nresource that is reserved, $B_R$, formally $R=\\{I_R,B_R\\}$.\n  The data structure is similar to a segment tree. The maximum spanning\ninterval of the data structure is fixed and defined in advance. The granularity\nand thereby the size of the intervals of the leaves is also defined in advance.\nThe data structure is built only once. Neither nodes nor leaves are ever\ninserted, deleted or moved. Hence, the running time of the operations does not\ndepend on the number of reservations previously made. The running time does not\ndepend on the size of the interval of the reservation either. Let $n$ be the\nnumber of leaves in the data structure. In the worst case, the number of\ntouched (i.e. traversed) nodes is in any operation $O(\\log n)$, hence the\nrunning time of any operation is also $O(\\log n)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0309043v1", 
    "title": "Finding approximate palindromes in strings", 
    "arxiv-id": "cs/0309043v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T13:45:48Z", 
    "summary": "We introduce a novel definition of approximate palindromes in strings, and\nprovide an algorithm to find all maximal approximate palindromes in a string\nwith up to $k$ errors. Our definition is based on the usual edit operations of\napproximate pattern matching, and the algorithm we give, for a string of size\n$n$ on a fixed alphabet, runs in $O(k^2 n)$ time. We also discuss two\nimplementation-related improvements to the algorithm, and demonstrate their\nefficacy in practice by means of both experiments and an average-case analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0310065v2", 
    "title": "Maintaining Information in Fully-Dynamic Trees with Top Trees", 
    "arxiv-id": "cs/0310065v2", 
    "author": "Mikkel Thorup", 
    "publish": "2003-10-31T18:37:47Z", 
    "summary": "We introduce top trees as a design of a new simpler interface for data\nstructures maintaining information in a fully-dynamic forest. We demonstrate\nhow easy and versatile they are to use on a host of different applications. For\nexample, we show how to maintain the diameter, center, and median of each tree\nin the forest. The forest can be updated by insertion and deletion of edges and\nby changes to vertex and edge weights. Each update is supported in O(log n)\ntime, where n is the size of the tree(s) involved in the update. Also, we show\nhow to support nearest common ancestor queries and level ancestor queries with\nrespect to arbitrary roots in O(log n) time. Finally, with marked and unmarked\nvertices, we show how to compute distances to a nearest marked vertex. The\nlater has applications to approximate nearest marked vertex in general graphs,\nand thereby to static optimization problems over shortest path metrics.\n  Technically speaking, top trees are easily implemented either with\nFrederickson's topology trees [Ambivalent Data Structures for Dynamic\n2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp.\n484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure for\nDynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, we\nclaim that the interface is simpler for many applications, and indeed our new\nbounds are quadratic improvements over previous bounds where they exist."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0311030v1", 
    "title": "Set K-Cover Algorithms for Energy Efficient Monitoring in Wireless   Sensor Networks", 
    "arxiv-id": "cs/0311030v1", 
    "author": "Serge Plotkin", 
    "publish": "2003-11-20T22:47:11Z", 
    "summary": "Wireless sensor networks (WSNs) are emerging as an effective means for\nenvironment monitoring. This paper investigates a strategy for energy efficient\nmonitoring in WSNs that partitions the sensors into covers, and then activates\nthe covers iteratively in a round-robin fashion. This approach takes advantage\nof the overlap created when many sensors monitor a single area. Our work builds\nupon previous work in \"Power Efficient Organization of Wireless Sensor\nNetworks\" by Slijepcevic and Potkonjak, where the model is first formulated. We\nhave designed three approximation algorithms for a variation of the SET K-COVER\nproblem, where the objective is to partition the sensors into covers such that\nthe number of covers that include an area, summed over all areas, is maximized.\nThe first algorithm is randomized and partitions the sensors, in expectation,\nwithin a fraction 1 - 1/e (~.63) of the optimum. We present two other\ndeterministic approximation algorithms. One is a distributed greedy algorithm\nwith a 1/2 approximation ratio and the other is a centralized greedy algorithm\nwith a 1 - 1/e approximation ratio. We show that it is NP-Complete to guarantee\nbetter than 15/16 of the optimal coverage, indicating that all three algorithms\nperform well with respect to the best approximation algorithm possible.\nSimulations indicate that in practice, the deterministic algorithms perform far\nabove their worst case bounds, consistently covering more than 72% of what is\ncovered by an optimum solution. Simulations also indicate that the increase in\nlongevity is proportional to the amount of overlap amongst the sensors. The\nalgorithms are fast, easy to use, and according to simulations, significantly\nincrease the longevity of sensor networks. The randomized algorithm in\nparticular seems quite practical."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0312054v1", 
    "title": "Partitioning schemes for quicksort and quickselect", 
    "arxiv-id": "cs/0312054v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2003-12-23T03:47:55Z", 
    "summary": "We introduce several modifications of the partitioning schemes used in\nHoare's quicksort and quickselect algorithms, including ternary schemes which\nidentify keys less or greater than the pivot. We give estimates for the numbers\nof swaps made by each scheme. Our computational experiments indicate that\nternary schemes allow quickselect to identify all keys equal to the selected\nkey at little additional cost."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0312055v1", 
    "title": "Randomized selection with quintary partitions", 
    "arxiv-id": "cs/0312055v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2003-12-23T04:12:30Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select for\nfinding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This\nrectifies the analysis of Floyd and Rivest, and extends it to the case of\nnondistinct elements. Our computational results confirm that Select may be the\nbest algorithm in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0401003v1", 
    "title": "Randomized selection with tripartitioning", 
    "arxiv-id": "cs/0401003v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2004-01-04T05:29:58Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select [Comm.\\\nACM {\\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require\nat most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average, even when equal elements\noccur. This parallels our recent analysis of another variant due to Floyd and\nRivest [Comm. ACM {\\bf 18} (1975) 165--172]. Our computational results suggest\nthat both variants perform well in practice, and may compete with other\nselection methods, such as Hoare's Find or quickselect with median-of-3 pivots."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0402005v1", 
    "title": "Improved randomized selection", 
    "arxiv-id": "cs/0402005v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2004-02-02T14:16:52Z", 
    "summary": "We show that several versions of Floyd and Rivest's improved algorithm Select\nfor finding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+O(n^{1/2}\\ln^{1/2}n)$ comparisons on average and with high\nprobability. This rectifies the analysis of Floyd and Rivest, and extends it to\nthe case of nondistinct elements. Encouraging computational results on large\nmedian-finding problems are reported."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0402045v2", 
    "title": "The Freeze-Tag Problem: How to Wake Up a Swarm of Robots", 
    "arxiv-id": "cs/0402045v2", 
    "author": "Martin Skutella", 
    "publish": "2004-02-18T20:49:02Z", 
    "summary": "An optimization problem that naturally arises in the study of swarm robotics\nis the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, by\nhaving an awakened robot move to their locations. Once a robot is awake, it can\nassist in awakening other slumbering robots.The objective is to have all robots\nawake as early as possible. While the FTP bears some resemblance to problems\nfrom areas in combinatorial optimization such as routing, broadcasting,\nscheduling, and covering, its algorithmic characteristics are surprisingly\ndifferent. We consider both scenarios on graphs and in geometric\nenvironments.In graphs, robots sleep at vertices and there is a length function\non the edges. Awake robots travel along edges, with time depending on edge\nlength. For most scenarios, we consider the offline version of the problem, in\nwhich each awake robot knows the position of all other robots. We prove that\nthe problem is NP-hard, even for the special case of star graphs. We also\nestablish hardness of approximation, showing that it is NP-hard to obtain an\napproximation factor better than 5/3, even for graphs of bounded degree.These\nlower bounds are complemented with several positive algorithmic results,\nincluding: (1) We show that the natural greedy strategy on star graphs has a\ntight worst-case performance of 7/3 and give a polynomial-time approximation\nscheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive online\nalgorithm for graphs with maximum degree D and locally bounded edge weights.\n(3) We give a PTAS, running in nearly linear time, for geometrically embedded\ninstances."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0403022v2", 
    "title": "Fast Multipoint-Evaluation of Bivariate Polynomials", 
    "arxiv-id": "cs/0403022v2", 
    "author": "Martin Ziegler", 
    "publish": "2004-03-12T14:31:43Z", 
    "summary": "We generalize univariate multipoint evaluation of polynomials of degree n at\nsublinear amortized cost per point. More precisely, it is shown how to evaluate\na bivariate polynomial p of maximum degree less than n, specified by its n^2\ncoefficients, simultaneously at n^2 given points using a total of O(n^{2.667})\narithmetic operations. In terms of the input size N being quadratic in n, this\namounts to an amortized cost of O(N^{0.334}) per point."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0404028v1", 
    "title": "The Random Buffer Tree : A Randomized Technique for I/O-efficient   Algorithms", 
    "arxiv-id": "cs/0404028v1", 
    "author": "G. Sajith", 
    "publish": "2004-04-13T13:49:11Z", 
    "summary": "In this paper, we present a probabilistic self-balancing dictionary data\nstructure for massive data sets, and prove expected amortized I/O-optimal\nbounds on the dictionary operations. We show how to use the structure as an\nI/O-optimal priority queue. The data structure, which we call as the random\nbuffer tree, abstracts the properties of the random treap and the buffer tree\nand has the same expected I/O-bounds as the buffer tree."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0404058v1", 
    "title": "Efficient coroutine generation of constrained Gray sequences", 
    "arxiv-id": "cs/0404058v1", 
    "author": "Frank Ruskey", 
    "publish": "2004-04-30T00:00:00Z", 
    "summary": "We study an interesting family of cooperating coroutines, which is able to\ngenerate all patterns of bits that satisfy certain fairly general ordering\nconstraints, changing only one bit at a time. (More precisely, the directed\ngraph of constraints is required to be cycle-free when it is regarded as an\nundirected graph.) If the coroutines are implemented carefully, they yield an\nalgorithm that needs only a bounded amount of computation per bit change,\nthereby solving an open problem in the field of combinatorial pattern\ngeneration."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0405094v1", 
    "title": "The Complexity of Maximum Matroid-Greedoid Intersection and Weighted   Greedoid Maximization", 
    "arxiv-id": "cs/0405094v1", 
    "author": "Esko Ukkonen", 
    "publish": "2004-05-25T12:09:34Z", 
    "summary": "The maximum intersection problem for a matroid and a greedoid, given by\npolynomial-time oracles, is shown $NP$-hard by expressing the satisfiability of\nboolean formulas in 3-conjunctive normal form as such an intersection. The\ncorresponding approximation problems are shown $NP$-hard for certain\napproximation performance bounds. Moreover, some natural parameterized variants\nof the problem are shown $W[P]$-hard. The results are in contrast with the\nmaximum matroid-matroid intersection which is solvable in polynomial time by an\nold result of Edmonds. We also prove that it is $NP$-hard to approximate the\nweighted greedoid maximization within $2^{n^{O(1)}}$ where $n$ is the size of\nthe domain of the greedoid.\n  A preliminary version ``The Complexity of Maximum Matroid-Greedoid\nIntersection'' appeared in Proc. FCT 2001, LNCS 2138, pp. 535--539,\nSpringer-Verlag 2001."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2005.05.008", 
    "link": "http://arxiv.org/pdf/cs/0406028v1", 
    "title": "Ramsey-type theorems for metric spaces with applications to online   problems", 
    "arxiv-id": "cs/0406028v1", 
    "author": "Manor Mendel", 
    "publish": "2004-06-16T21:56:48Z", 
    "summary": "A nearly logarithmic lower bound on the randomized competitive ratio for the\nmetrical task systems problem is presented. This implies a similar lower bound\nfor the extensively studied k-server problem. The proof is based on Ramsey-type\ntheorems for metric spaces, that state that every metric space contains a large\nsubspace which is approximately a hierarchically well-separated tree (and in\nparticular an ultrametric). These Ramsey-type theorems may be of independent\ninterest."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.002", 
    "link": "http://arxiv.org/pdf/cs/0406033v2", 
    "title": "Randomized k-server algorithms for growth-rate bounded graphs", 
    "arxiv-id": "cs/0406033v2", 
    "author": "Manor Mendel", 
    "publish": "2004-06-17T15:11:54Z", 
    "summary": "The paper referred to in the title is withdrawn."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539700376159", 
    "link": "http://arxiv.org/pdf/cs/0406034v1", 
    "title": "Better algorithms for unfair metrical task systems and applications", 
    "arxiv-id": "cs/0406034v1", 
    "author": "Manor Mendel", 
    "publish": "2004-06-17T18:49:20Z", 
    "summary": "Unfair metrical task systems are a generalization of online metrical task\nsystems. In this paper we introduce new techniques to combine algorithms for\nunfair metrical task systems and apply these techniques to obtain improved\nrandomized online algorithms for metrical task systems on arbitrary metric\nspaces."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0406036v1", 
    "title": "Online Companion Caching", 
    "arxiv-id": "cs/0406036v1", 
    "author": "Steven S. Seiden", 
    "publish": "2004-06-18T16:20:24Z", 
    "summary": "This paper is concerned with online caching algorithms for the\n(n,k)-companion cache, defined by Brehob et. al. In this model the cache is\ncomposed of two components: a k-way set-associative cache and a companion\nfully-associative cache of size n. We show that the deterministic competitive\nratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio is\nO(\\log n \\log k) and \\Omega(\\log n +\\log k)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0406045v3", 
    "title": "Online Searching with Turn Cost", 
    "arxiv-id": "cs/0406045v3", 
    "author": "Shmuel Gal", 
    "publish": "2004-06-23T16:56:53Z", 
    "summary": "We consider the problem of searching for an object on a line at an unknown\ndistance OPT from the original position of the searcher, in the presence of a\ncost of d for each time the searcher changes direction. This is a\ngeneralization of the well-studied linear-search problem. We describe a\nstrategy that is guaranteed to find the object at a cost of at most 9*OPT + 2d,\nwhich has the optimal competitive ratio 9 with respect to OPT plus the minimum\ncorresponding additive term. Our argument for upper and lower bound uses an\ninfinite linear program, which we solve by experimental solution of an infinite\nseries of approximating finite linear programs, estimating the limits, and\nsolving the resulting recurrences. We feel that this technique is interesting\nin its own right and should help solve other searching problems. In particular,\nwe consider the star search or cow-path problem with turn cost, where the\nhidden object is placed on one of m rays emanating from the original position\nof the searcher. For this problem we give a tight bound of\n(1+(2(m^m)/((m-1)^(m-1))) OPT + m ((m/(m-1))^(m-1) - 1) d. We also discuss\ntradeoff between the corresponding coefficients, and briefly consider\nrandomized strategies on the line."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0407003v1", 
    "title": "Insertion Sort is O(n log n)", 
    "arxiv-id": "cs/0407003v1", 
    "author": "Miguel Mosteiro", 
    "publish": "2004-07-01T15:50:26Z", 
    "summary": "Traditional Insertion Sort runs in O(n^2) time because each insertion takes\nO(n) time. When people run Insertion Sort in the physical world, they leave\ngaps between items to accelerate insertions. Gaps help in computers as well.\nThis paper shows that Gapped Insertion Sort has insertion times of O(log n)\nwith high probability, yielding a total running time of O(n log n) with high\nprobability."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0407023v1", 
    "title": "Efficient Hashing with Lookups in two Memory Accesses", 
    "arxiv-id": "cs/0407023v1", 
    "author": "Rina Panigrahy", 
    "publish": "2004-07-09T22:23:40Z", 
    "summary": "The study of hashing is closely related to the analysis of balls and bins. It\nis well-known that instead of using a single hash function if we randomly hash\na ball into two bins and place it in the smaller of the two, then this\ndramatically lowers the maximum load on bins. This leads to the concept of\ntwo-way hashing where the largest bucket contains $O(\\log\\log n)$ balls with\nhigh probability. The hash look up will now search in both the buckets an item\nhashes to. Since an item may be placed in one of two buckets, we could\npotentially move an item after it has been initially placed to reduce maximum\nload. with a maximum load of We show that by performing moves during inserts, a\nmaximum load of 2 can be maintained on-line, with high probability, while\nsupporting hash update operations. In fact, with $n$ buckets, even if the space\nfor two items are pre-allocated per bucket, as may be desirable in hardware\nimplementations, more than $n$ items can be stored giving a high memory\nutilization. We also analyze the trade-off between the number of moves\nperformed during inserts and the maximum load on a bucket. By performing at\nmost $h$ moves, we can maintain a maximum load of $O(\\frac{\\log \\log n}{h\n\\log(\\log\\log n/h)})$. So, even by performing one move, we achieve a better\nbound than by performing no moves at all."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1597036.1597042", 
    "link": "http://arxiv.org/pdf/cs/0407036v1", 
    "title": "All Maximal Independent Sets and Dynamic Dominance for Sparse Graphs", 
    "arxiv-id": "cs/0407036v1", 
    "author": "David Eppstein", 
    "publish": "2004-07-15T21:04:45Z", 
    "summary": "We describe algorithms, based on Avis and Fukuda's reverse search paradigm,\nfor listing all maximal independent sets in a sparse graph in polynomial time\nand delay per output. For bounded degree graphs, our algorithms take constant\ntime per set generated; for minor-closed graph families, the time is O(n) per\nset, and for more general sparse graph families we achieve subquadratic time\nper set. We also describe new data structures for maintaining a dynamic vertex\nset S in a sparse or minor-closed graph family, and querying the number of\nvertices not dominated by S; for minor-closed graph families the time per\nupdate is constant, while it is sublinear for any sparse graph family. We can\nalso maintain a dynamic vertex set in an arbitrary m-edge graph and test the\nindependence of the maintained set in time O(sqrt m) per update. We use the\ndomination data structures as part of our enumeration algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0408003v1", 
    "title": "Multi-Embedding of Metric Spaces", 
    "arxiv-id": "cs/0408003v1", 
    "author": "Manor Mendel", 
    "publish": "2004-08-02T16:42:43Z", 
    "summary": "Metric embedding has become a common technique in the design of algorithms.\nIts applicability is often dependent on how high the embedding's distortion is.\nFor example, embedding finite metric space into trees may require linear\ndistortion as a function of its size. Using probabilistic metric embeddings,\nthe bound on the distortion reduces to logarithmic in the size.\n  We make a step in the direction of bypassing the lower bound on the\ndistortion in terms of the size of the metric. We define \"multi-embeddings\" of\nmetric spaces in which a point is mapped onto a set of points, while keeping\nthe target metric of polynomial size and preserving the distortion of paths.\nThe distortion obtained with such multi-embeddings into ultrametrics is at most\nO(log Delta loglog Delta) where Delta is the aspect ratio of the metric. In\nparticular, for expander graphs, we are able to obtain constant distortion\nembeddings into trees in contrast with the Omega(log n) lower bound for all\nprevious notions of embeddings.\n  We demonstrate the algorithmic application of the new embeddings for two\noptimization problems: group Steiner tree and metrical task systems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0408040v1", 
    "title": "Hash sort: A linear time complexity multiple-dimensional sort algorithm", 
    "arxiv-id": "cs/0408040v1", 
    "author": "William F. Gilreath", 
    "publish": "2004-08-17T09:23:35Z", 
    "summary": "Sorting and hashing are two completely different concepts in computer\nscience, and appear mutually exclusive to one another. Hashing is a search\nmethod using the data as a key to map to the location within memory, and is\nused for rapid storage and retrieval. Sorting is a process of organizing data\nfrom a random permutation into an ordered arrangement, and is a common activity\nperformed frequently in a variety of applications.\n  Almost all conventional sorting algorithms work by comparison, and in doing\nso have a linearithmic greatest lower bound on the algorithmic time complexity.\nAny improvement in the theoretical time complexity of a sorting algorithm can\nresult in overall larger gains in implementation performance.. A gain in\nalgorithmic performance leads to much larger gains in speed for the application\nthat uses the sort algorithm. Such a sort algorithm needs to use an alternative\nmethod for ordering the data than comparison, to exceed the linearithmic time\ncomplexity boundary on algorithmic performance.\n  The hash sort is a general purpose non-comparison based sorting algorithm by\nhashing, which has some interesting features not found in conventional sorting\nalgorithms. The hash sort asymptotically outperforms the fastest traditional\nsorting algorithm, the quick sort. The hash sort algorithm has a linear time\ncomplexity factor -- even in the worst case. The hash sort opens an area for\nfurther work and investigation into alternative means of sorting."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0410046v1", 
    "title": "A Note on Scheduling Equal-Length Jobs to Maximize Throughput", 
    "arxiv-id": "cs/0410046v1", 
    "author": "Maciej Kurowski", 
    "publish": "2004-10-18T22:41:30Z", 
    "summary": "We study the problem of scheduling equal-length jobs with release times and\ndeadlines, where the objective is to maximize the number of completed jobs.\nPreemptions are not allowed. In Graham's notation, the problem is described as\n1|r_j;p_j=p|\\sum U_j. We give the following results: (1) We show that the often\ncited algorithm by Carlier from 1981 is not correct. (2) We give an algorithm\nfor this problem with running time O(n^5)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0410048v4", 
    "title": "Worst-Case Optimal Tree Layout in External Memory", 
    "arxiv-id": "cs/0410048v4", 
    "author": "Stefan Langerman", 
    "publish": "2004-10-19T15:17:57Z", 
    "summary": "Consider laying out a fixed-topology tree of N nodes into external memory\nwith block size B so as to minimize the worst-case number of block memory\ntransfers required to traverse a path from the root to a node of depth D. We\nprove that the optimal number of memory transfers is $$ \\cases{\n  \\displaystyle\n  \\Theta\\left( {D \\over \\lg (1{+}B)} \\right)\n  & when $D = O(\\lg N)$, \\cr\n  \\displaystyle\n  \\Theta\\left( {\\lg N \\over \\lg \\left(1{+}{B \\lg N \\over D}\\right)} \\right)\n  & when $D = \\Omega(\\lg N)$ and $D = O(B \\lg N)$, \\cr\n  \\displaystyle\n  \\Theta\\left( {D \\over B} \\right)\n  & when $D = \\Omega(B \\lg N)$.\n  } $$"
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412004v1", 
    "title": "Finding Approximate Palindromes in Strings Quickly and Simply", 
    "arxiv-id": "cs/0412004v1", 
    "author": "L. Allison", 
    "publish": "2004-12-01T17:08:55Z", 
    "summary": "Described are two algorithms to find long approximate palindromes in a\nstring, for example a DNA sequence. A simple algorithm requires O(n)-space and\nalmost always runs in $O(k.n)$-time where n is the length of the string and k\nis the number of ``errors'' allowed in the palindrome. Its worst-case\ntime-complexity is $O(n^2)$ but this does not occur with real biological\nsequences. A more complex algorithm guarantees $O(k.n)$ worst-case time\ncomplexity."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412006v1", 
    "title": "The Accelerated Euclidean Algorithm", 
    "arxiv-id": "cs/0412006v1", 
    "author": "Sidi Mohamed Sedjelmaci", 
    "publish": "2004-12-02T15:01:39Z", 
    "summary": "We present a new GCD algorithm of two integers or polynomials. The algorithm\nis iterative and its time complexity is still $O(n \\\\log^2 n ~ log \\\\log n)$\nfor $n$-bit inputs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412089v1", 
    "title": "Evolving Categories: Consistent Framework for Representation of Data and   Algorithms", 
    "arxiv-id": "cs/0412089v1", 
    "author": "Evgeny Yanenko", 
    "publish": "2004-12-17T22:58:13Z", 
    "summary": "A concept of \"evolving categories\" is suggested to build a simple, scalable,\nmathematically consistent framework for representing in uniform way both data\nand algorithms. A state machine for executing algorithms becomes clear, rich\nand powerful semantics, based on category theory, and still allows easy\nimplementation. Moreover, it gives an original insight into the nature and\nsemantics of algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412094v1", 
    "title": "Preemptive Multi-Machine Scheduling of Equal-Length Jobs to Minimize the   Average Flow Time", 
    "arxiv-id": "cs/0412094v1", 
    "author": "Francis Sourd", 
    "publish": "2004-12-20T16:15:59Z", 
    "summary": "We study the problem of preemptive scheduling of n equal-length jobs with\ngiven release times on m identical parallel machines. The objective is to\nminimize the average flow time. Recently, Brucker and Kravchenko proved that\nthe optimal schedule can be computed in polynomial time by solving a linear\nprogram with O(n^3) variables and constraints, followed by some substantial\npost-processing (where n is the number of jobs.) In this note we describe a\nsimple linear program with only O(mn) variables and constraints. Our linear\nprogram produces directly the optimal schedule and does not require any\npost-processing."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412100v1", 
    "title": "Formal Test Purposes and The Validity of Test Cases", 
    "arxiv-id": "cs/0412100v1", 
    "author": "Stephan Tobies", 
    "publish": "2004-12-22T08:53:49Z", 
    "summary": "We give a formalization of the notion of test purpose based on (suitably\nrestricted) Message Sequence Charts. We define the validity of test cases with\nrespect to such a formal test purpose and provide a simple decision procedure\nfor validity."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0501020v1", 
    "title": "Enhancing Histograms by Tree-Like Bucket Indices", 
    "arxiv-id": "cs/0501020v1", 
    "author": "Domenico Rosaci", 
    "publish": "2005-01-11T10:15:31Z", 
    "summary": "Histograms are used to summarize the contents of relations into a number of\nbuckets for the estimation of query result sizes. Several techniques (e.g.,\nMaxDiff and V-Optimal) have been proposed in the past for determining bucket\nboundaries which provide accurate estimations. However, while search strategies\nfor optimal bucket boundaries are rather sophisticated, no much attention has\nbeen paid for estimating queries inside buckets and all of the above techniques\nadopt naive methods for such an estimation. This paper focuses on the problem\nof improving the estimation inside a bucket once its boundaries have been\nfixed. The proposed technique is based on the addition, to each bucket, of\n32-bit additional information (organized into a 4-level tree index), storing\napproximate cumulative frequencies at 7 internal intervals of the bucket. Both\ntheoretical analysis and experimental results show that, among a number of\nalternative ways to organize the additional information, the 4-level tree index\nprovides the best frequency estimation inside a bucket. The index is later\nadded to two well-known histograms, MaxDiff and V-Optimal, obtaining the\nnon-obvious result that despite the spatial cost of 4LT which reduces the\nnumber of allowed buckets once the storage space has been fixed, the original\nmethods are strongly improved in terms of accuracy."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502032v1", 
    "title": "On Dynamic Range Reporting in One Dimension", 
    "arxiv-id": "cs/0502032v1", 
    "author": "Mihai Patrascu", 
    "publish": "2005-02-05T23:22:37Z", 
    "summary": "We consider the problem of maintaining a dynamic set of integers and\nanswering queries of the form: report a point (equivalently, all points) in a\ngiven interval. Range searching is a natural and fundamental variant of integer\nsearch, and can be solved using predecessor search. However, for a RAM with\nw-bit words, we show how to perform updates in O(lg w) time and answer queries\nin O(lglg w) time. The update time is identical to the van Emde Boas structure,\nbut the query time is exponentially faster. Existing lower bounds show that\nachieving our query time for predecessor search requires doubly-exponentially\nslower updates. We present some arguments supporting the conjecture that our\nsolution is optimal.\n  Our solution is based on a new and interesting recursion idea which is \"more\nextreme\" that the van Emde Boas recursion. Whereas van Emde Boas uses a simple\nrecursion (repeated halving) on each path in a trie, we use a nontrivial, van\nEmde Boas-like recursion on every such path. Despite this, our algorithm is\nquite clean when seen from the right angle. To achieve linear space for our\ndata structure, we solve a problem which is of independent interest. We develop\nthe first scheme for dynamic perfect hashing requiring sublinear space. This\ngives a dynamic Bloomier filter (an approximate storage scheme for sparse\nvectors) which uses low space. We strengthen previous lower bounds to show that\nthese results are optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502054v1", 
    "title": "Improved Tag Set Design and Multiplexing Algorithms for Universal Arrays", 
    "arxiv-id": "cs/0502054v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-02-10T20:20:53Z", 
    "summary": "In this paper we address two optimization problems arising in the design of\ngenomic assays based on universal tag arrays. First, we address the universal\narray tag set design problem. For this problem, we extend previous formulations\nto incorporate antitag-to-antitag hybridization constraints in addition to\nconstraints on antitag-to-tag hybridization specificity, establish a\nconstructive upper bound on the maximum number of tags satisfying the extended\nconstraints, and propose a simple greedy tag selection algorithm. Second, we\ngive methods for improving the multiplexing rate in large-scale genomic assays\nby combining primer selection with tag assignment. Experimental results on\nsimulated data show that this integrated optimization leads to reductions of up\nto 50% in the number of required arrays."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502065v1", 
    "title": "Highly Scalable Algorithms for Robust String Barcoding", 
    "arxiv-id": "cs/0502065v1", 
    "author": "Alex A. Shvartsman", 
    "publish": "2005-02-14T22:19:52Z", 
    "summary": "String barcoding is a recently introduced technique for genomic-based\nidentification of microorganisms. In this paper we describe the engineering of\nhighly scalable algorithms for robust string barcoding. Our methods enable\ndistinguisher selection based on whole genomic sequences of hundreds of\nmicroorganisms of up to bacterial size on a well-equipped workstation, and can\nbe easily parallelized to further extend the applicability range to thousands\nof bacterial size genomes. Experimental results on both randomly generated and\nNCBI genomic data show that whole-genome based selection results in a number of\ndistinguishers nearly matching the information theoretic lower bounds for the\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502073v1", 
    "title": "A note on the Burrows-Wheeler transformation", 
    "arxiv-id": "cs/0502073v1", 
    "author": "Dominique Perrin", 
    "publish": "2005-02-17T07:06:28Z", 
    "summary": "We relate the Burrows-Wheeler transformation with a result in combinatorics\non words known as the Gessel-Reutenauer transformation."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0503057v1", 
    "title": "Exact and Approximation Algorithms for DNA Tag Set Design", 
    "arxiv-id": "cs/0503057v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-03-23T02:36:14Z", 
    "summary": "In this paper we propose new solution methods for designing tag sets for use\nin universal DNA arrays. First, we give integer linear programming formulations\nfor two previous formalizations of the tag set design problem, and show that\nthese formulations can be solved to optimality for instance sizes of practical\ninterest by using general purpose optimization packages. Second, we note the\nbenefits of periodic tags, and establish an interesting connection between the\ntag design problem and the problem of packing the maximum number of\nvertex-disjoint directed cycles in a given graph. We show that combining a\nsimple greedy cycle packing algorithm with a previously proposed alphabetic\ntree search strategy yields an increase of over 40% in the number of tags\ncompared to previous methods."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0504023v1", 
    "title": "Correlation Clustering with a Fixed Number of Clusters", 
    "arxiv-id": "cs/0504023v1", 
    "author": "Venkatesan Guruswami", 
    "publish": "2005-04-06T22:36:03Z", 
    "summary": "We continue the investigation of problems concerning correlation clustering\nor clustering with qualitative information, which is a clustering formulation\nthat has been studied recently. The basic setup here is that we are given as\ninput a complete graph on n nodes (which correspond to nodes to be clustered)\nwhose edges are labeled + (for similar pairs of items) and - (for dissimilar\npairs of items). Thus we have only as input qualitative information on\nsimilarity and no quantitative distance measure between items. The quality of a\nclustering is measured in terms of its number of agreements, which is simply\nthe number of edges it correctly classifies, that is the sum of number of -\nedges whose endpoints it places in different clusters plus the number of +\nedges both of whose endpoints it places within the same cluster.\n  In this paper, we study the problem of finding clusterings that maximize the\nnumber of agreements, and the complementary minimization version where we seek\nclusterings that minimize the number of disagreements. We focus on the\nsituation when the number of clusters is stipulated to be a small constant k.\nOur main result is that for every k, there is a polynomial time approximation\nscheme for both maximizing agreements and minimizing disagreements. (The\nproblems are NP-hard for every k >= 2.) The main technical work is for the\nminimization version, as the PTAS for maximizing agreements follows along the\nlines of the property tester for Max k-CUT.\n  In contrast, when the number of clusters is not specified, the problem of\nminimizing disagreements was shown to be APX-hard, even though the maximization\nversion admits a PTAS."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-007-9005-x", 
    "link": "http://arxiv.org/pdf/cs/0504103v2", 
    "title": "Oblivious Medians via Online Bidding", 
    "arxiv-id": "cs/0504103v2", 
    "author": "Neal E. Young", 
    "publish": "2005-04-27T00:07:32Z", 
    "summary": "Following Mettu and Plaxton, we study online algorithms for the k-medians\nproblem. Such an algorithm must produce a nested sequence F_1\\subseteq\nF_2\\subseteq...\\subseteq F_n of sets of facilities. Mettu and Plaxton show that\nonline metric medians has a (roughly) 40-competitive deterministic\npolynomial-time algorithm. We give improved algorithms, including a\n(24+\\epsilon)-competitive deterministic polynomial-time algorithm and a\n5.44-competitive, randomized, non-polynomial-time algorithm.\n  We also consider the competitive ratio with respect to size. An algorithm is\ns-size-competitive if, for each k, the cost of F_k is at most the minimum cost\nof any set of k facilities, while the size of F_k is at most s k. We present\noptimally competitive algorithms for this problem.\n  Our proofs reduce online medians to the following online bidding problem:\nfaced with some unknown threshold T>0, an algorithm must submit ``bids'' b>0\nuntil it submits a bid as large as T. The algorithm pays the sum of its bids.\nWe describe optimally competitive algorithms for online bidding.\n  Our results on cost-competitive online medians extend to approximately metric\ndistance functions, online fractional medians, and online bicriteria\napproximation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0504104v2", 
    "title": "The reverse greedy algorithm for the metric k-median problem", 
    "arxiv-id": "cs/0504104v2", 
    "author": "Neal E. Young", 
    "publish": "2005-04-27T19:36:08Z", 
    "summary": "The Reverse Greedy algorithm (RGreedy) for the k-median problem works as\nfollows. It starts by placing facilities on all nodes. At each step, it removes\na facility to minimize the resulting total distance from the customers to the\nremaining facilities. It stops when k facilities remain. We prove that, if the\ndistance function is metric, then the approximation ratio of RGreedy is between\n?(log n/ log log n) and O(log n)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505007v1", 
    "title": "Adaptive Codes: A New Class of Non-standard Variable-length Codes", 
    "arxiv-id": "cs/0505007v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-05-02T09:40:02Z", 
    "summary": "We introduce a new class of non-standard variable-length codes, called\nadaptive codes. This class of codes associates a variable-length codeword to\nthe symbol being encoded depending on the previous symbols in the input data\nstring. An efficient algorithm for constructing adaptive codes of order one is\npresented. Then, we introduce a natural generalization of adaptive codes,\ncalled GA codes."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505009v17", 
    "title": "Human being is a living random number generator", 
    "arxiv-id": "cs/0505009v17", 
    "author": "Arindam Mitra", 
    "publish": "2005-05-03T15:42:24Z", 
    "summary": "General wisdom is, mathematical operation is needed to generate number by\nnumbers. It is pointed out that without any mathematical operation true random\nnumbers can be generated by numbers through algorithmic process. It implies\nthat human brain itself is a living true random number generator. Human brain\ncan meet the enormous human demand of true random numbers."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505027v1", 
    "title": "The Generic Multiple-Precision Floating-Point Addition With Exact   Rounding (as in the MPFR Library)", 
    "arxiv-id": "cs/0505027v1", 
    "author": "Vincent Lef\u00e8vre", 
    "publish": "2005-05-11T14:22:54Z", 
    "summary": "We study the multiple-precision addition of two positive floating-point\nnumbers in base 2, with exact rounding, as specified in the MPFR library, i.e.\nwhere each number has its own precision. We show how the best possible\ncomplexity (up to a constant factor that depends on the implementation) can be\nobtain."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505048v1", 
    "title": "Improved Combinatorial Group Testing Algorithms for Real-World Problem   Sizes", 
    "arxiv-id": "cs/0505048v1", 
    "author": "Daniel S. Hirschberg", 
    "publish": "2005-05-18T20:25:16Z", 
    "summary": "We study practically efficient methods for performing combinatorial group\ntesting. We present efficient non-adaptive and two-stage combinatorial group\ntesting algorithms, which identify the at most d items out of a given set of n\nitems that are defective, using fewer tests for all practical set sizes. For\nexample, our two-stage algorithm matches the information theoretic lower bound\nfor the number of tests in a combinatorial group testing regimen."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505061v1", 
    "title": "EAH: A New Encoder based on Adaptive Variable-length Codes", 
    "arxiv-id": "cs/0505061v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-05-24T06:53:33Z", 
    "summary": "Adaptive variable-length codes associate a variable-length codeword to the\nsymbol being encoded depending on the previous symbols in the input string.\nThis class of codes has been recently presented in [Dragos Trinca,\narXiv:cs.DS/0505007] as a new class of non-standard variable-length codes. New\nalgorithms for data compression, based on adaptive variable-length codes of\norder one and Huffman's algorithm, have been recently presented in [Dragos\nTrinca, ITCC 2004]. In this paper, we extend the work done so far by the\nfollowing contributions: first, we propose an improved generalization of these\nalgorithms, called EAHn. Second, we compute the entropy bounds for EAHn, using\nthe well-known bounds for Huffman's algorithm. Third, we discuss implementation\ndetails and give reports of experimental results obtained on some well-known\ncorpora. Finally, we describe a parallel version of EAHn using the PRAM model\nof computation."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505066v1", 
    "title": "Decision Sort and its Parallel Implementation", 
    "arxiv-id": "cs/0505066v1", 
    "author": "Udayan Khuarana", 
    "publish": "2005-05-24T15:41:27Z", 
    "summary": "In this paper, a sorting technique is presented that takes as input a data\nset whose primary key domain is known to the sorting algorithm, and works with\nan time efficiency of O(n+k), where k is the primary key domain. It is shown\nthat the algorithm has applicability over a wide range of data sets. Later, a\nparallel formulation of the same is proposed and its effectiveness is argued.\nThough this algorithm is applicable over a wide range of general data sets, it\nfinds special application (much superior to others) in places where sorting\ninformation that arrives in parts and in cases where input data is huge in\nsize."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505077v1", 
    "title": "Efficient Approximation of Convex Recolorings", 
    "arxiv-id": "cs/0505077v1", 
    "author": "Sagi Snir", 
    "publish": "2005-05-27T23:16:48Z", 
    "summary": "A coloring of a tree is convex if the vertices that pertain to any color\ninduce a connected subtree; a partial coloring (which assigns colors to some of\nthe vertices) is convex if it can be completed to a convex (total) coloring.\nConvex coloring of trees arise in areas such as phylogenetics, linguistics,\netc. eg, a perfect phylogenetic tree is one in which the states of each\ncharacter induce a convex coloring of the tree. Research on perfect phylogeny\nis usually focused on finding a tree so that few predetermined partial\ncolorings of its vertices are convex.\n  When a coloring of a tree is not convex, it is desirable to know \"how far\" it\nis from a convex one. In [19], a natural measure for this distance, called the\nrecoloring distance was defined: the minimal number of color changes at the\nvertices needed to make the coloring convex. This can be viewed as minimizing\nthe number of \"exceptional vertices\" w.r.t. to a closest convex coloring. The\nproblem was proved to be NP-hard even for colored string.\n  In this paper we continue the work of [19], and present a 2-approximation\nalgorithm of convex recoloring of strings whose running time O(cn), where c is\nthe number of colors and n is the size of the input, and an O(cn^2)-time\n3-approximation algorithm for convex recoloring of trees."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0506027v1", 
    "title": "Sorting a Low-Entropy Sequence", 
    "arxiv-id": "cs/0506027v1", 
    "author": "Travis Gagie", 
    "publish": "2005-06-08T22:15:18Z", 
    "summary": "We give the first sorting algorithm with bounds in terms of higher-order\nentropies: let $S$ be a sequence of length $m$ containing $n$ distinct elements\nand let (H_\\ell (S)) be the $\\ell$th-order empirical entropy of $S$, with\n(n^{\\ell + 1} \\log n \\in O (m)); our algorithm sorts $S$ using ((H_\\ell (S) + O\n(1)) m) comparisons."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0507014v1", 
    "title": "Isomorphism of graphs-a polynomial test", 
    "arxiv-id": "cs/0507014v1", 
    "author": "Moshe Schwartz", 
    "publish": "2005-07-06T11:35:42Z", 
    "summary": "An explicit algorithm is presented for testing whether two non-directed\ngraphs are isomorphic or not. It is shown that for a graph of n vertices, the\nnumber of n independent operations needed for the test is polynomial in n. A\nproof that the algorithm actually performs the test is presented."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508045v1", 
    "title": "Multicommodity Flow Algorithms for Buffered Global Routing", 
    "arxiv-id": "cs/0508045v1", 
    "author": "Alexander Zelikovsky", 
    "publish": "2005-08-06T12:44:09Z", 
    "summary": "In this paper we describe a new algorithm for buffered global routing\naccording to a prescribed buffer site map. Specifically, we describe a provably\ngood multi-commodity flow based algorithm that finds a global routing\nminimizing buffer and wire congestion subject to given constraints on routing\narea (wirelength and number of buffers) and sink delays. Our algorithm allows\ncomputing the tradeoff curve between routing area and wire/buffer congestion\nunder any combination of delay and capacity constraints, and simultaneously\nperforms buffer/wire sizing, as well as layer and pin assignment. Experimental\nresults show that near-optimal results are obtained with a practical runtime."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508086v1", 
    "title": "High-performance BWT-based Encoders", 
    "arxiv-id": "cs/0508086v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T05:47:00Z", 
    "summary": "In 1994, Burrows and Wheeler developed a data compression algorithm which\nperforms significantly better than Lempel-Ziv based algorithms. Since then, a\nlot of work has been done in order to improve their algorithm, which is based\non a reversible transformation of the input string, called BWT (the\nBurrows-Wheeler transformation). In this paper, we propose a compression scheme\nbased on BWT, MTF (move-to-front coding), and a version of the algorithms\npresented in [Dragos Trinca, ITCC-2004]."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508087v1", 
    "title": "Modelling the Eulerian Path Problem using a String Matching Framework", 
    "arxiv-id": "cs/0508087v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T06:08:40Z", 
    "summary": "The well-known Eulerian path problem can be solved in polynomial time (more\nexactly, there exists a linear time algorithm for this problem). In this paper,\nwe model the problem using a string matching framework, and then initiate an\nalgorithmic study on a variant of this problem, called the (2,1)-STRING-MATCH\nproblem (which is actually a generalization of the Eulerian path problem).\nThen, we present a polynomial-time algorithm for the (2,1)-STRING-MATCH\nproblem, which is the most important result of this paper. Specifically, we get\na lower bound of Omega(n), and an upper bound of O(n^{2})."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508089v1", 
    "title": "Modelling the EAH Data Compression Algorithm using Graph Theory", 
    "arxiv-id": "cs/0508089v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T19:32:39Z", 
    "summary": "Adaptive codes associate variable-length codewords to symbols being encoded\ndepending on the previous symbols in the input data string. This class of codes\nhas been introduced in [Dragos Trinca, cs.DS/0505007] as a new class of\nnon-standard variable-length codes. New algorithms for data compression, based\non adaptive codes of order one, have been presented in [Dragos Trinca,\nITCC-2004], where we have behaviorally shown that for a large class of input\ndata strings, these algorithms substantially outperform the Lempel-Ziv\nuniversal data compression algorithm. EAH has been introduced in [Dragos\nTrinca, cs.DS/0505061], as an improved generalization of these algorithms. In\nthis paper, we present a translation of the EAH algorithm into the graph\ntheory."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508090v1", 
    "title": "Translating the EAH Data Compression Algorithm into Automata Theory", 
    "arxiv-id": "cs/0508090v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T19:56:31Z", 
    "summary": "Adaptive codes have been introduced in [Dragos Trinca, cs.DS/0505007] as a\nnew class of non-standard variable-length codes. These codes associate\nvariable-length codewords to symbols being encoded depending on the previous\nsymbols in the input data string. A new data compression algorithm, called EAH,\nhas been introduced in [Dragos Trinca, cs.DS/0505061], where we have\nbehaviorally shown that for a large class of input data strings, this algorithm\nsubstantially outperforms the well-known Lempel-Ziv universal data compression\nalgorithm. In this paper, we translate the EAH encoder into automata theory."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508125v1", 
    "title": "A Sorting Algorithm Based on Calculation", 
    "arxiv-id": "cs/0508125v1", 
    "author": "De-Shun Zheng", 
    "publish": "2005-08-29T14:22:57Z", 
    "summary": "This article introduces an adaptive sorting algorithm that can relocate\nelements accurately by substituting their values into a function which we name\nit the guessing function. We focus on building this function which is the\nmapping relationship between record values and their corresponding sorted\nlocations essentially. The time complexity of this algorithm O(n),when records\ndistributed uniformly. Additionally, similar approach can be used in the\nsearching algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509026v1", 
    "title": "Sampling to estimate arbitrary subset sums", 
    "arxiv-id": "cs/0509026v1", 
    "author": "Mikkel Thorup", 
    "publish": "2005-09-09T21:47:52Z", 
    "summary": "Starting with a set of weighted items, we want to create a generic sample of\na certain size that we can later use to estimate the total weight of arbitrary\nsubsets. For this purpose, we propose priority sampling which tested on\nInternet data performed better than previous methods by orders of magnitude.\n  Priority sampling is simple to define and implement: we consider a steam of\nitems i=0,...,n-1 with weights w_i. For each item i, we generate a random\nnumber r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of\nthe k highest priority items. Let t be the (k+1)th highest priority. Each\nsampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled\nitems get weight estimate W_i=0.\n  Magically, it turns out that the weight estimates are unbiased, that is,\nE[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over\nany subset sum simply by adding the sampled weight estimates from the subset.\nAlso, we can estimate the variance of the estimates, and surpricingly, there is\nno co-variance between different weight estimates W_i and W_j.\n  We conjecture an extremely strong near-optimality; namely that for any weight\nsequence, there exists no specialized scheme for sampling k items with unbiased\nestimators that gets smaller total variance than priority sampling with k+1\nitems. Very recently Mario Szegedy has settled this conjecture."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509031v1", 
    "title": "On the Worst-case Performance of the Sum-of-Squares Algorithm for Bin   Packing", 
    "arxiv-id": "cs/0509031v1", 
    "author": "Claire Kenyon", 
    "publish": "2005-09-12T14:49:48Z", 
    "summary": "The Sum of Squares algorithm for bin packing was defined in [2] and studied\nin great detail in [1], where it was proved that its worst case performance\nratio is at most 3. In this note, we improve the asymptotic worst case bound to\n2.7777..."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509038v1", 
    "title": "Algorithms for Max Hamming Exact Satisfiability", 
    "arxiv-id": "cs/0509038v1", 
    "author": "Vilhelm Dahllof", 
    "publish": "2005-09-14T09:04:20Z", 
    "summary": "We here study Max Hamming XSAT, ie, the problem of finding two XSAT models at\nmaximum Hamming distance. By using a recent XSAT solver as an auxiliary\nfunction, an O(1.911^n) time algorithm can be constructed, where n is the\nnumber of variables. This upper time bound can be further improved to\nO(1.8348^n) by introducing a new kind of branching, more directly suited for\nfinding models at maximum Hamming distance. The techniques presented here are\nlikely to be of practical use as well as of theoretical value, proving that\nthere are non-trivial algorithms for maximum Hamming distance problems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509069v3", 
    "title": "Fast and Compact Regular Expression Matching", 
    "arxiv-id": "cs/0509069v3", 
    "author": "Martin Farach-Colton", 
    "publish": "2005-09-22T13:30:20Z", 
    "summary": "We study 4 problems in string matching, namely, regular expression matching,\napproximate regular expression matching, string edit distance, and subsequence\nindexing, on a standard word RAM model of computation that allows\nlogarithmic-sized words to be manipulated in constant time. We show how to\nimprove the space and/or remove a dependency on the alphabet size for each\nproblem using either an improved tabulation technique of an existing algorithm\nor by combining known algorithms in a new way."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0510019v2", 
    "title": "Entropy based Nearest Neighbor Search in High Dimensions", 
    "arxiv-id": "cs/0510019v2", 
    "author": "Rina Panigrahy", 
    "publish": "2005-10-07T00:55:06Z", 
    "summary": "In this paper we study the problem of finding the approximate nearest\nneighbor of a query point in the high dimensional space, focusing on the\nEuclidean space. The earlier approaches use locality-preserving hash functions\n(that tend to map nearby points to the same value) to construct several hash\ntables to ensure that the query point hashes to the same bucket as its nearest\nneighbor in at least one table. Our approach is different -- we use one (or a\nfew) hash table and hash several randomly chosen points in the neighborhood of\nthe query point showing that at least one of them will hash to the bucket\ncontaining its nearest neighbor. We show that the number of randomly chosen\npoints in the neighborhood of the query point $q$ required depends on the\nentropy of the hash value $h(p)$ of a random point $p$ at the same distance\nfrom $q$ at its nearest neighbor, given $q$ and the locality preserving hash\nfunction $h$ chosen randomly from the hash family. Precisely, we show that if\nthe entropy $I(h(p)|q,h) = M$ and $g$ is a bound on the probability that two\nfar-off points will hash to the same bucket, then we can find the approximate\nnearest neighbor in $O(n^\\rho)$ time and near linear $\\tilde O(n)$ space where\n$\\rho = M/\\log(1/g)$. Alternatively we can build a data structure of size\n$\\tilde O(n^{1/(1-\\rho)})$ to answer queries in $\\tilde O(d)$ time. By applying\nthis analysis to the locality preserving hash functions in and adjusting the\nparameters we show that the $c$ nearest neighbor can be computed in time\n$\\tilde O(n^\\rho)$ and near linear space where $\\rho \\approx 2.06/c$ as $c$\nbecomes large."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0510086v1", 
    "title": "Balanced Allocation on Graphs", 
    "arxiv-id": "cs/0510086v1", 
    "author": "R. Panigrahy", 
    "publish": "2005-10-27T21:59:21Z", 
    "summary": "In this paper, we study the two choice balls and bins process when balls are\nnot allowed to choose any two random bins, but only bins that are connected by\nan edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the\ngraph is almost regular with degree $n^\\epsilon$, where $\\epsilon$ is not too\nsmall, the previous bounds on the maximum load continue to hold. Precisely, the\nmaximum load is $\\log \\log n + O(1/\\epsilon) + O(1)$. For general\n$\\Delta$-regular graphs, we show that the maximum load is $\\log\\log n +\nO(\\frac{\\log n}{\\log (\\Delta/\\log^4 n)}) + O(1)$ and also provide an almost\nmatching lower bound of $\\log \\log n + \\frac{\\log n}{\\log (\\Delta \\log n)}$.\n  V{\\\"o}cking [Voc99] showed that the maximum bin size with $d$ choice load\nbalancing can be further improved to $O(\\log\\log n /d)$ by breaking ties to the\nleft. This requires $d$ random bin choices. We show that such bounds can be\nachieved by making only two random accesses and querying $d/2$ contiguous bins\nin each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of\n$d/2$ consecutive bins, if each ball chooses two groups at random and inserts\nthe new ball into the least-loaded bin in the lesser loaded group, then the\nmaximum load is $O(\\log\\log n/d)$ with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0511020v2", 
    "title": "Pbit and other list sorting algorithms", 
    "arxiv-id": "cs/0511020v2", 
    "author": "David S. P\u0142aneta", 
    "publish": "2005-11-04T01:52:02Z", 
    "summary": "Pbit, besides its simplicity, is definitely the fastest list sorting\nalgorithm. It considerably surpasses all already known methods. Among many\nadvantages, it is stable, linear and be made to run in place. I will compare\nPbit with algorithm described by Donald E. Knuth in the third volume of ''The\nArt of Computer Programming'' and other (QuickerSort, MergeSort) list sorting\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0511082v1", 
    "title": "Approximating Clustering of Fingerprint Vectors with Missing Values", 
    "arxiv-id": "cs/0511082v1", 
    "author": "Riccardo Dondi", 
    "publish": "2005-11-23T10:32:47Z", 
    "summary": "The problem of clustering fingerprint vectors is an interesting problem in\nComputational Biology that has been proposed in (Figureroa et al. 2004). In\nthis paper we show some improvements in closing the gaps between the known\nlower bounds and upper bounds on the approximability of some variants of the\nbiological problem. Namely we are able to prove that the problem is APX-hard\neven when each fingerprint contains only two unknown position. Moreover we have\nstudied some variants of the orginal problem, and we give two 2-approximation\nalgorithm for the IECMV and OECMV problems when the number of unknown entries\nfor each vector is at most a constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512046v3", 
    "title": "A polynomial algorithm for the k-cluster problem on interval graphs", 
    "arxiv-id": "cs/0512046v3", 
    "author": "George B. Mertzios", 
    "publish": "2005-12-11T23:13:44Z", 
    "summary": "This paper deals with the problem of finding, for a given graph and a given\nnatural number k, a subgraph of k nodes with a maximum number of edges. This\nproblem is known as the k-cluster problem and it is NP-hard on general graphs\nas well as on chordal graphs. In this paper, it is shown that the k-cluster\nproblem is solvable in polynomial time on interval graphs. In particular, we\npresent two polynomial time algorithms for the class of proper interval graphs\nand the class of general interval graphs, respectively. Both algorithms are\nbased on a matrix representation for interval graphs. In contrast to\nrepresentations used in most of the previous work, this matrix representation\ndoes not make use of the maximal cliques in the investigated graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512061v3", 
    "title": "Matching Subsequences in Trees", 
    "arxiv-id": "cs/0512061v3", 
    "author": "Inge Li Goertz", 
    "publish": "2005-12-15T10:28:04Z", 
    "summary": "Given two rooted, labeled trees $P$ and $T$ the tree path subsequence problem\nis to determine which paths in $P$ are subsequences of which paths in $T$. Here\na path begins at the root and ends at a leaf. In this paper we propose this\nproblem as a useful query primitive for XML data, and provide new algorithms\nimproving the previously best known time and space bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512081v1", 
    "title": "De Dictionariis Dynamicis Pauco Spatio Utentibus", 
    "arxiv-id": "cs/0512081v1", 
    "author": "Mihai Patrascu", 
    "publish": "2005-12-20T23:01:41Z", 
    "summary": "We develop dynamic dictionaries on the word RAM that use asymptotically\noptimal space, up to constant factors, subject to insertions and deletions, and\nsubject to supporting perfect-hashing queries and/or membership queries, each\noperation in constant time with high probability. When supporting only\nmembership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,\nwhere n and u are the sizes of the dictionary and the universe, respectively.\nPrevious dictionaries either did not achieve this space bound or had time\nbounds that were only expected and amortized. When supporting perfect-hashing\nqueries, the optimal space bound depends on the range {1,2,...,n+t} of\nhashcodes allowed as output. We prove that the optimal space bound is Theta(n\nlglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,\nand it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership\nqueries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0601084v1", 
    "title": "Randomized Fast Design of Short DNA Words", 
    "arxiv-id": "cs/0601084v1", 
    "author": "Robert Schweller", 
    "publish": "2006-01-19T00:22:56Z", 
    "summary": "We consider the problem of efficiently designing sets (codes) of equal-length\nDNA strings (words) that satisfy certain combinatorial constraints. This\nproblem has numerous motivations including DNA computing and DNA self-assembly.\nPrevious work has extended results from coding theory to obtain bounds on code\nsize for new biologically motivated constraints and has applied heuristic local\nsearch and genetic algorithm techniques for code design. This paper proposes a\nnatural optimization formulation of the DNA code design problem in which the\ngoal is to design n strings that satisfy a given set of constraints while\nminimizing the length of the strings. For multiple sets of constraints, we\nprovide high-probability algorithms that run in time polynomial in n and any\ngiven constraint parameters, and output strings of length within a constant\nfactor of the optimal. To the best of our knowledge, this work is the first to\nconsider this type of optimization problem in the context of DNA code design."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0601117v2", 
    "title": "Finding Cliques of a Graph using Prime Numbers", 
    "arxiv-id": "cs/0601117v2", 
    "author": "Prashant", 
    "publish": "2006-01-27T20:11:14Z", 
    "summary": "This paper proposes a new algorithm for solving maximal cliques for simple\nundirected graphs using the theory of prime numbers. A novel approach using\nprime numbers is used to find cliques and ends with a discussion of the\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0601127v1", 
    "title": "Truly Online Paging with Locality of Reference", 
    "arxiv-id": "cs/0601127v1", 
    "author": "Manor Mendel", 
    "publish": "2006-01-30T20:58:23Z", 
    "summary": "The competitive analysis fails to model locality of reference in the online\npaging problem. To deal with it, Borodin et. al. introduced the access graph\nmodel, which attempts to capture the locality of reference. However, the access\ngraph model has a number of troubling aspects. The access graph has to be known\nin advance to the paging algorithm and the memory required to represent the\naccess graph itself may be very large.\n  In this paper we present truly online strongly competitive paging algorithms\nin the access graph model that do not have any prior information on the access\nsequence. We present both deterministic and randomized algorithms. The\nalgorithms need only O(k log n) bits of memory, where k is the number of page\nslots available and n is the size of the virtual address space. I.e.,\nasymptotically no more memory than needed to store the virtual address\ntranslation table.\n  We also observe that our algorithms adapt themselves to temporal changes in\nthe locality of reference. We model temporal changes in the locality of\nreference by extending the access graph model to the so called extended access\ngraph model, in which many vertices of the graph can correspond to the same\nvirtual page. We define a measure for the rate of change in the locality of\nreference in G denoted by Delta(G). We then show our algorithms remain strongly\ncompetitive as long as Delta(G) >= (1+ epsilon)k, and no truly online algorithm\ncan be strongly competitive on a class of extended access graphs that includes\nall graphs G with Delta(G) >= k- o(k)."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602002v1", 
    "title": "Simulating Network Influence Algorithms Using Particle-Swarms: PageRank   and PageRank-Priors", 
    "arxiv-id": "cs/0602002v1", 
    "author": "Johan Bollen", 
    "publish": "2006-01-31T23:24:42Z", 
    "summary": "A particle-swarm is a set of indivisible processing elements that traverse a\nnetwork in order to perform a distributed function. This paper will describe a\nparticular implementation of a particle-swarm that can simulate the behavior of\nthe popular PageRank algorithm in both its {\\it global-rank} and {\\it\nrelative-rank} incarnations. PageRank is compared against the particle-swarm\nmethod on artificially generated scale-free networks of 1,000 nodes constructed\nusing a common gamma value, $\\gamma = 2.5$. The running time of the\nparticle-swarm algorithm is $O(|P|+|P|t)$ where $|P|$ is the size of the\nparticle population and $t$ is the number of particle propagation iterations.\nThe particle-swarm method is shown to be useful due to its ease of extension\nand running time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602057v1", 
    "title": "Plane Decompositions as Tools for Approximation", 
    "arxiv-id": "cs/0602057v1", 
    "author": "Christopher M. Homan", 
    "publish": "2006-02-15T19:09:39Z", 
    "summary": "Tree decompositions were developed by Robertson and Seymour. Since then\nalgorithms have been developed to solve intractable problems efficiently for\ngraphs of bounded treewidth. In this paper we extend tree decompositions to\nallow cycles to exist in the decomposition graph; we call these new\ndecompositions plane decompositions because we require that the decomposition\ngraph be planar. First, we give some background material about tree\ndecompositions and an overview of algorithms both for decompositions and for\napproximations of planar graphs. Then, we give our plane decomposition\ndefinition and an algorithm that uses this decomposition to approximate the\nsize of the maximum independent set of the underlying graph in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602073v2", 
    "title": "An O(n^{2.75}) algorithm for online topological ordering", 
    "arxiv-id": "cs/0602073v2", 
    "author": "Ulrich Meyer", 
    "publish": "2006-02-21T10:32:15Z", 
    "summary": "We present a simple algorithm which maintains the topological order of a\ndirected acyclic graph with n nodes under an online edge insertion sequence in\nO(n^{2.75}) time, independent of the number of edges m inserted. For dense\nDAGs, this is an improvement over the previous best result of O(min(m^{3/2}\nlog(n), m^{3/2} + n^2 log(n)) by Katriel and Bodlaender. We also provide an\nempirical comparison of our algorithm with other algorithms for online\ntopological sorting. Our implementation outperforms them on certain hard\ninstances while it is still competitive on random edge insertion sequences\nleading to complete DAGs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603048v1", 
    "title": "Homogeneity vs. Adjacency: generalising some graph decomposition   algorithms", 
    "arxiv-id": "cs/0603048v1", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-03-13T09:48:49Z", 
    "summary": "In this paper, a new general decomposition theory inspired from modular graph\ndecomposition is presented. Our main result shows that, within this general\ntheory, most of the nice algorithmic tools developed for modular decomposition\nare still efficient. This theory not only unifies the usual modular\ndecomposition generalisations such as modular decomposition of directed graphs\nor decomposition of 2-structures, but also star cutsets and bimodular\ndecomposition. Our general framework provides a decomposition algorithm which\nimproves the best known algorithms for bimodular decomposition."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603050v1", 
    "title": "Multiple serial episode matching", 
    "arxiv-id": "cs/0603050v1", 
    "author": "Yuri Matiyasevich", 
    "publish": "2006-03-13T11:03:34Z", 
    "summary": "In a previous paper we generalized the Knuth-Morris-Pratt (KMP) pattern\nmatching algorithm and defined a non-conventional kind of RAM, the MP--RAMs\n(RAMS equipped with extra operations), and designed an O(n) on-line algorithm\nfor solving the serial episode matching problem on MP--RAMs when there is only\none single episode. We here give two extensions of this algorithm to the case\nwhen we search for several patterns simultaneously and compare them. More\npreciseley, given $q+1$ strings (a text $t$ of length $n$ and $q$ patterns\n$m\\_1,...,m\\_q$) and a natural number $w$, the {\\em multiple serial episode\nmatching problem} consists in finding the number of size $w$ windows of text\n$t$ which contain patterns $m\\_1,...,m\\_q$ as subsequences, i.e. for each\n$m\\_i$, if $m\\_i=p\\_1,..., p\\_k$, the letters $p\\_1,..., p\\_k$ occur in the\nwindow, in the same order as in $m\\_i$, but not necessarily consecutively (they\nmay be interleaved with other letters).} The main contribution is an algorithm\nsolving this problem on-line in time $O(nq)$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603122v1", 
    "title": "Complexity of Monadic inf-datalog. Application to temporal logic", 
    "arxiv-id": "cs/0603122v1", 
    "author": "Irene Guessarian", 
    "publish": "2006-03-30T15:25:11Z", 
    "summary": "In [11] we defined Inf-Datalog and characterized the fragments of Monadic\ninf-Datalog that have the same expressive power as Modal Logic (resp. $CTL$,\nalternation-free Modal $\\mu$-calculus and Modal $\\mu$-calculus). We study here\nthe time and space complexity of evaluation of Monadic inf-Datalog programs on\nfinite models. We deduce a new unified proof that model checking has 1. linear\ndata and program complexities (both in time and space) for $CTL$ and\nalternation-free Modal $\\mu$-calculus, and 2. linear-space (data and program)\ncomplexities, linear-time program complexity and polynomial-time data\ncomplexity for $L\\mu\\_k$ (Modal $\\mu$-calculus with fixed alternation-depth at\nmost $k$).}"
},{
    "category": "cs.DS", 
    "doi": "10.1145/1644015.1644017", 
    "link": "http://arxiv.org/pdf/cs/0604037v3", 
    "title": "An O(n^3)-Time Algorithm for Tree Edit Distance", 
    "arxiv-id": "cs/0604037v3", 
    "author": "Oren Weimann", 
    "publish": "2006-04-10T00:39:11Z", 
    "summary": "The {\\em edit distance} between two ordered trees with vertex labels is the\nminimum cost of transforming one tree into the other by a sequence of\nelementary operations consisting of deleting and relabeling existing nodes, as\nwell as inserting new nodes. In this paper, we present a worst-case\n$O(n^3)$-time algorithm for this problem, improving the previous best\n$O(n^3\\log n)$-time algorithm~\\cite{Klein}. Our result requires a novel\nadaptive strategy for deciding how a dynamic program divides into subproblems\n(which is interesting in its own right), together with a deeper understanding\nof the previous algorithms for the problem. We also prove the optimality of our\nalgorithm among the family of \\emph{decomposition strategy} algorithms--which\nalso includes the previous fastest algorithms--by tightening the known lower\nbound of $\\Omega(n^2\\log^2 n)$~\\cite{Touzet} to $\\Omega(n^3)$, matching our\nalgorithm's running time. Furthermore, we obtain matching upper and lower\nbounds of $\\Theta(n m^2 (1 + \\log \\frac{n}{m}))$ when the two trees have\ndifferent sizes $m$ and~$n$, where $m < n$."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1644015.1644017", 
    "link": "http://arxiv.org/pdf/cs/0604045v1", 
    "title": "An exact algorithm for higher-dimensional orthogonal packing", 
    "arxiv-id": "cs/0604045v1", 
    "author": "Jan C. van der Veen", 
    "publish": "2006-04-11T13:55:03Z", 
    "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical\napplications, including packing, cutting, and scheduling. Combining the use of\nour data structure for characterizing feasible packings with our new classes of\nlower bounds, and other heuristics, we develop a two-level tree search\nalgorithm for solving higher-dimensional packing problems to optimality.\nComputational results are reported, including optimal solutions for all\ntwo--dimensional test problems from recent literature.\n  This is the third in a series of articles describing new approaches to\nhigher-dimensional packing; see cs.DS/0310032 and cs.DS/0402044."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0604065v3", 
    "title": "Unifying two Graph Decompositions with Modular Decomposition", 
    "arxiv-id": "cs/0604065v3", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-04-16T19:41:38Z", 
    "summary": "We introduces the umodules, a generalisation of the notion of graph module.\nThe theory we develop captures among others undirected graphs, tournaments,\ndigraphs, and $2-$structures. We show that, under some axioms, a unique\ndecomposition tree exists for umodules. Polynomial-time algorithms are provided\nfor: non-trivial umodule test, maximal umodule computation, and decomposition\ntree computation when the tree exists. Our results unify many known\ndecomposition like modular and bi-join decomposition of graphs, and a new\ndecomposition of tournaments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0604097v4", 
    "title": "Approximation algorithms for wavelet transform coding of data streams", 
    "arxiv-id": "cs/0604097v4", 
    "author": "Boulos Harb", 
    "publish": "2006-04-25T01:27:37Z", 
    "summary": "This paper addresses the problem of finding a B-term wavelet representation\nof a given discrete function $f \\in \\real^n$ whose distance from f is\nminimized. The problem is well understood when we seek to minimize the\nEuclidean distance between f and its representation. The first known algorithms\nfor finding provably approximate representations minimizing general $\\ell_p$\ndistances (including $\\ell_\\infty$) under a wide variety of compactly supported\nwavelet bases are presented in this paper. For the Haar basis, a polynomial\ntime approximation scheme is demonstrated. These algorithms are applicable in\nthe one-pass sublinear-space data stream model of computation. They generalize\nnaturally to multiple dimensions and weighted norms. A universal representation\nthat provides a provable approximation guarantee under all p-norms\nsimultaneously; and the first approximation algorithms for bit-budget versions\nof the problem, known as adaptive quantization, are also presented. Further, it\nis shown that the algorithms presented here can be used to select a basis from\na tree-structured dictionary of bases and find a B-term representation of the\ngiven function that provably approximates its best dictionary-basis\nrepresentation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0605078v1", 
    "title": "The Complexity of Mean Flow Time Scheduling Problems with Release Times", 
    "arxiv-id": "cs/0605078v1", 
    "author": "Francis Sourd", 
    "publish": "2006-05-17T22:07:17Z", 
    "summary": "We study the problem of preemptive scheduling n jobs with given release times\non m identical parallel machines. The objective is to minimize the average flow\ntime. We show that when all jobs have equal processing times then the problem\ncan be solved in polynomial time using linear programming. Our algorithm can\nalso be applied to the open-shop problem with release times and unit processing\ntimes. For the general case (when processing times are arbitrary), we show that\nthe problem is unary NP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0605126v1", 
    "title": "Power-aware scheduling for makespan and flow", 
    "arxiv-id": "cs/0605126v1", 
    "author": "David P. Bunde", 
    "publish": "2006-05-26T21:57:35Z", 
    "summary": "We consider offline scheduling algorithms that incorporate speed scaling to\naddress the bicriteria problem of minimizing energy consumption and a\nscheduling metric. For makespan, we give linear-time algorithms to compute all\nnon-dominated solutions for the general uniprocessor problem and for the\nmultiprocessor problem when every job requires the same amount of work. We also\nshow that the multiprocessor problem becomes NP-hard when jobs can require\ndifferent amounts of work.\n  For total flow, we show that the optimal flow corresponding to a particular\nenergy budget cannot be exactly computed on a machine supporting arithmetic and\nthe extraction of roots. This hardness result holds even when scheduling\nequal-work jobs on a uniprocessor. We do, however, extend previous work by\nPruhs et al. to give an arbitrarily-good approximation for scheduling\nequal-work jobs on a multiprocessor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0606042v1", 
    "title": "Enabling user-driven Checkpointing strategies in Reverse-mode Automatic   Differentiation", 
    "arxiv-id": "cs/0606042v1", 
    "author": "Mauricio Araya-Polo", 
    "publish": "2006-06-09T16:01:46Z", 
    "summary": "This paper presents a new functionality of the Automatic Differentiation (AD)\ntool Tapenade. Tapenade generates adjoint codes which are widely used for\noptimization or inverse problems. Unfortunately, for large applications the\nadjoint code demands a great deal of memory, because it needs to store a large\nset of intermediates values. To cope with that problem, Tapenade implements a\nsub-optimal version of a technique called checkpointing, which is a trade-off\nbetween storage and recomputation. Our long-term goal is to provide an optimal\ncheckpointing strategy for every code, not yet achieved by any AD tool. Towards\nthat goal, we first introduce modifications in Tapenade in order to give the\nuser the choice to select the checkpointing strategy most suitable for their\ncode. Second, we conduct experiments in real-size scientific codes in order to\ngather hints that help us to deduce an optimal checkpointing strategy. Some of\nthe experimental results show memory savings up to 35% and execution time up to\n90%."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10951-007-0038-4", 
    "link": "http://arxiv.org/pdf/cs/0606067v2", 
    "title": "Scheduling Algorithms for Procrastinators", 
    "arxiv-id": "cs/0606067v2", 
    "author": "Kostas Tsichlas", 
    "publish": "2006-06-14T16:55:44Z", 
    "summary": "This paper presents scheduling algorithms for procrastinators, where the\nspeed that a procrastinator executes a job increases as the due date\napproaches. We give optimal off-line scheduling policies for linearly\nincreasing speed functions. We then explain the computational/numerical issues\ninvolved in implementing this policy. We next explore the online setting,\nshowing that there exist adversaries that force any online scheduling policy to\nmiss due dates. This impossibility result motivates the problem of minimizing\nthe maximum interval stretch of any job; the interval stretch of a job is the\njob's flow time divided by the job's due date minus release time. We show that\nseveral common scheduling strategies, including the \"hit-the-highest-nail\"\nstrategy beloved by procrastinators, have arbitrarily large maximum interval\nstretch. Then we give the \"thrashing\" scheduling policy and show that it is a\n\\Theta(1) approximation algorithm for the maximum interval stretch."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606109v4", 
    "title": "Maximum gradient embeddings and monotone clustering", 
    "arxiv-id": "cs/0606109v4", 
    "author": "Assaf Naor", 
    "publish": "2006-06-26T19:32:29Z", 
    "summary": "Let (X,d_X) be an n-point metric space. We show that there exists a\ndistribution D over non-contractive embeddings into trees f:X-->T such that for\nevery x in X, the expectation with respect to D of the maximum over y in X of\nthe ratio d_T(f(x),f(y)) / d_X(x,y) is at most C (log n)^2, where C is a\nuniversal constant. Conversely we show that the above quadratic dependence on\nlog n cannot be improved in general. Such embeddings, which we call maximum\ngradient embeddings, yield a framework for the design of approximation\nalgorithms for a wide range of clustering problems with monotone costs,\nincluding fault-tolerant versions of k-median and facility location."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606116v1", 
    "title": "New Algorithms for Regular Expression Matching", 
    "arxiv-id": "cs/0606116v1", 
    "author": "Philip Bille", 
    "publish": "2006-06-28T10:51:39Z", 
    "summary": "In this paper we revisit the classical regular expression matching problem,\nnamely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches\none of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and\n$Q$, respectively. On a standard unit-cost RAM with word length $w \\geq \\log\nn$, we show that the problem can be solved in $O(m)$ space with the following\nrunning times: \\begin{equation*} \\begin{cases}\n  O(n\\frac{m \\log w}{w} + m \\log w) & \\text{if $m > w$} \\\\\n  O(n\\log m + m\\log m) & \\text{if $\\sqrt{w} < m \\leq w$} \\\\\n  O(\\min(n+ m^2, n\\log m + m\\log m)) & \\text{if $m \\leq \\sqrt{w}$.} \\end{cases}\n\\end{equation*} This improves the best known time bound among algorithms using\n$O(m)$ space. Whenever $w \\geq \\log^2 n$ it improves all known time bounds\nregardless of how much space is used."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606124v2", 
    "title": "Weighted hierarchical alignment of directed acyclic graph", 
    "arxiv-id": "cs/0606124v2", 
    "author": "Dmitri Maslov", 
    "publish": "2006-06-29T18:07:49Z", 
    "summary": "In some applications of matching, the structural or hierarchical properties\nof the two graphs being aligned must be maintained. The hierarchical properties\nare induced by the direction of the edges in the two directed graphs. These\nstructural relationships defined by the hierarchy in the graphs act as a\nconstraint on the alignment. In this paper, we formalize the above problem as\nthe weighted alignment between two directed acyclic graphs. We prove that this\nproblem is NP-complete, show several upper bounds for approximating the\nsolution, and finally introduce polynomial time algorithms for sub-classes of\ndirected acyclic graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607045v2", 
    "title": "Improved online hypercube packing", 
    "arxiv-id": "cs/0607045v2", 
    "author": "Yong Zhou", 
    "publish": "2006-07-11T09:43:45Z", 
    "summary": "In this paper, we study online multidimensional bin packing problem when all\nitems are hypercubes.\n  Based on the techniques in one dimensional bin packing algorithm Super\nHarmonic by Seiden, we give a framework for online hypercube packing problem\nand obtain new upper bounds of asymptotic competitive ratios.\n  For square packing, we get an upper bound of 2.1439, which is better than\n2.24437.\n  For cube packing, we also give a new upper bound 2.6852 which is better than\n2.9421 by Epstein and van Stee."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607046v2", 
    "title": "Strip Packing vs. Bin Packing", 
    "arxiv-id": "cs/0607046v2", 
    "author": "Guochuan Zhang", 
    "publish": "2006-07-11T09:58:34Z", 
    "summary": "In this paper we establish a general algorithmic framework between bin\npacking and strip packing, with which we achieve the same asymptotic bounds by\napplying bin packing algorithms to strip packing. More precisely we obtain the\nfollowing results: (1) Any offline bin packing algorithm can be applied to\nstrip packing maintaining the same asymptotic worst-case ratio. Thus using FFD\n(MFFD) as a subroutine, we get a practical (simple and fast) algorithm for\nstrip packing with an upper bound 11/9 (71/60). A simple AFPTAS for strip\npacking immediately follows. (2) A class of Harmonic-based algorithms for bin\npacking can be applied to online strip packing maintaining the same asymptotic\ncompetitive ratio. It implies online strip packing admits an upper bound of\n1.58889 on the asymptotic competitive ratio, which is very close to the lower\nbound 1.5401 and significantly improves the previously best bound of 1.6910 and\naffirmatively answers an open question posed by Csirik et. al."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607100v1", 
    "title": "New Upper Bounds on The Approximability of 3D Strip Packing", 
    "arxiv-id": "cs/0607100v1", 
    "author": "Guochuan Zhang", 
    "publish": "2006-07-22T02:06:26Z", 
    "summary": "In this paper, we study the 3D strip packing problem in which we are given a\nlist of 3-dimensional boxes and required to pack all of them into a\n3-dimensional strip with length 1 and width 1 and unlimited height to minimize\nthe height used. Our results are below: i) we give an approximation algorithm\nwith asymptotic worst-case ratio 1.69103, which improves the previous best\nbound of $2+\\epsilon$ by Jansen and Solis-Oba of SODA 2006; ii) we also present\nan asymptotic PTAS for the case in which all items have {\\em square} bases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0608079v1", 
    "title": "Algorithmic linear dimension reduction in the l_1 norm for sparse   vectors", 
    "arxiv-id": "cs/0608079v1", 
    "author": "R. Vershynin", 
    "publish": "2006-08-19T01:55:14Z", 
    "summary": "This paper develops a new method for recovering m-sparse signals that is\nsimultaneously uniform and quick. We present a reconstruction algorithm whose\nrun time, O(m log^2(m) log^2(d)), is sublinear in the length d of the signal.\nThe reconstruction error is within a logarithmic factor (in m) of the optimal\nm-term approximation error in l_1. In particular, the algorithm recovers\nm-sparse signals perfectly and noisy signals are recovered with polylogarithmic\ndistortion. Our algorithm makes O(m log^2 (d)) measurements, which is within a\nlogarithmic factor of optimal. We also present a small-space implementation of\nthe algorithm. These sketching techniques and the corresponding reconstruction\nalgorithms provide an algorithmic dimension reduction in the l_1 norm. In\nparticular, vectors of support m in dimension d can be linearly embedded into\nO(m log^2 d) dimensions with polylogarithmic distortion. We can reconstruct a\nvector from its low-dimensional sketch in time O(m log^2(m) log^2(d)).\nFurthermore, this reconstruction is stable and robust under small\nperturbations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0608124v5", 
    "title": "The Tree Inclusion Problem: In Linear Space and Faster", 
    "arxiv-id": "cs/0608124v5", 
    "author": "Inge Li Goertz", 
    "publish": "2006-08-31T12:23:37Z", 
    "summary": "Given two rooted, ordered, and labeled trees $P$ and $T$ the tree inclusion\nproblem is to determine if $P$ can be obtained from $T$ by deleting nodes in\n$T$. This problem has recently been recognized as an important query primitive\nin XML databases. Kilpel\\\"ainen and Mannila [\\emph{SIAM J. Comput. 1995}]\npresented the first polynomial time algorithm using quadratic time and space.\nSince then several improved results have been obtained for special cases when\n$P$ and $T$ have a small number of leaves or small depth. However, in the worst\ncase these algorithms still use quadratic time and space. Let $n_S$, $l_S$, and\n$d_S$ denote the number of nodes, the number of leaves, and the %maximum depth\nof a tree $S \\in \\{P, T\\}$. In this paper we show that the tree inclusion\nproblem can be solved in space $O(n_T)$ and time: O(\\min(l_Pn_T, l_Pl_T\\log\n\\log n_T + n_T, \\frac{n_Pn_T}{\\log n_T} + n_{T}\\log n_{T})). This improves or\nmatches the best known time complexities while using only linear space instead\nof quadratic. This is particularly important in practical applications, such as\nXML databases, where the space is likely to be a bottleneck."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0609032v3", 
    "title": "CR-precis: A deterministic summary structure for update data streams", 
    "arxiv-id": "cs/0609032v3", 
    "author": "Anirban Majumder", 
    "publish": "2006-09-07T19:21:01Z", 
    "summary": "We present the \\crprecis structure, that is a general-purpose, deterministic\nand sub-linear data structure for summarizing \\emph{update} data streams. The\n\\crprecis structure yields the \\emph{first deterministic sub-linear space/time\nalgorithms for update streams} for answering a variety of fundamental stream\nqueries, such as, (a) point queries, (b) range queries, (c) finding approximate\nfrequent items, (d) finding approximate quantiles, (e) finding approximate\nhierarchical heavy hitters, (f) estimating inner-products, (g) near-optimal\n$B$-bucket histograms, etc.."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0609085v2", 
    "title": "Improved Approximate String Matching and Regular Expression Matching on   Ziv-Lempel Compressed Texts", 
    "arxiv-id": "cs/0609085v2", 
    "author": "Inge Li Goertz", 
    "publish": "2006-09-15T07:36:25Z", 
    "summary": "We study the approximate string matching and regular expression matching\nproblem for the case when the text to be searched is compressed with the\nZiv-Lempel adaptive dictionary compression schemes. We present a time-space\ntrade-off that leads to algorithms improving the previously known complexities\nfor both problems. In particular, we significantly improve the space bounds,\nwhich in practical applications are likely to be a bottleneck."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610001v1", 
    "title": "Practical Entropy-Compressed Rank/Select Dictionary", 
    "arxiv-id": "cs/0610001v1", 
    "author": "Kunihiko Sadakane", 
    "publish": "2006-09-29T23:52:09Z", 
    "summary": "Rank/Select dictionaries are data structures for an ordered set $S \\subset\n\\{0,1,...,n-1\\}$ to compute $\\rank(x,S)$ (the number of elements in $S$ which\nare no greater than $x$), and $\\select(i,S)$ (the $i$-th smallest element in\n$S$), which are the fundamental components of \\emph{succinct data structures}\nof strings, trees, graphs, etc. In those data structures, however, only\nasymptotic behavior has been considered and their performance for real data is\nnot satisfactory. In this paper, we propose novel four Rank/Select\ndictionaries, esp, recrank, vcode and sdarray, each of which is small if the\nnumber of elements in $S$ is small, and indeed close to $nH_0(S)$ ($H_0(S) \\leq\n1$ is the zero-th order \\textit{empirical entropy} of $S$) in practice, and its\nquery time is superior to the previous ones. Experimental results reveal the\ncharacteristics of our data structures and also show that these data structures\nare superior to existing implementations in both size and query time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610046v5", 
    "title": "Streaming Maximum-Minimum Filter Using No More than Three Comparisons   per Element", 
    "arxiv-id": "cs/0610046v5", 
    "author": "Daniel Lemire", 
    "publish": "2006-10-09T22:09:42Z", 
    "summary": "The running maximum-minimum (max-min) filter computes the maxima and minima\nover running windows of size w. This filter has numerous applications in signal\nprocessing and time series analysis. We present an easy-to-implement online\nalgorithm requiring no more than 3 comparisons per element, in the worst case.\nComparatively, no algorithm is known to compute the running maximum (or\nminimum) filter in 1.5 comparisons per element, in the worst case. Our\nalgorithm has reduced latency and memory usage."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610119v1", 
    "title": "Approximate Convex Optimization by Online Game Playing", 
    "arxiv-id": "cs/0610119v1", 
    "author": "Elad Hazan", 
    "publish": "2006-10-19T22:10:32Z", 
    "summary": "Lagrangian relaxation and approximate optimization algorithms have received\nmuch attention in the last two decades. Typically, the running time of these\nmethods to obtain a $\\epsilon$ approximate solution is proportional to\n$\\frac{1}{\\epsilon^2}$. Recently, Bienstock and Iyengar, following Nesterov,\ngave an algorithm for fractional packing linear programs which runs in\n$\\frac{1}{\\epsilon}$ iterations. The latter algorithm requires to solve a\nconvex quadratic program every iteration - an optimization subroutine which\ndominates the theoretical running time.\n  We give an algorithm for convex programs with strictly convex constraints\nwhich runs in time proportional to $\\frac{1}{\\epsilon}$. The algorithm does NOT\nrequire to solve any quadratic program, but uses gradient steps and elementary\noperations only. Problems which have strictly convex constraints include\nmaximum entropy frequency estimation, portfolio optimization with loss risk\nconstraints, and various computational problems in signal processing.\n  As a side product, we also obtain a simpler version of Bienstock and\nIyengar's result for general linear programming, with similar running time.\n  We derive these algorithms using a new framework for deriving convex\noptimization algorithms from online game playing algorithms, which may be of\nindependent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611001v1", 
    "title": "A near-optimal fully dynamic distributed algorithm for maintaining   sparse spanners", 
    "arxiv-id": "cs/0611001v1", 
    "author": "Michael Elkin", 
    "publish": "2006-11-01T09:36:20Z", 
    "summary": "In this paper we devise an extremely efficient fully dynamic distributed\nalgorithm for maintaining sparse spanners. Our resuls also include the first\nfully dynamic centralized algorithm for the problem with non-trivial bounds for\nboth incremental and decremental update. Finally, we devise a very efficient\nstreaming algorithm for the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611019v2", 
    "title": "Algorithmic Aspects of a General Modular Decomposition Theory", 
    "arxiv-id": "cs/0611019v2", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-11-04T18:32:23Z", 
    "summary": "A new general decomposition theory inspired from modular graph decomposition\nis presented. This helps unifying modular decomposition on different\nstructures, including (but not restricted to) graphs. Moreover, even in the\ncase of graphs, the terminology ``module'' not only captures the classical\ngraph modules but also allows to handle 2-connected components, star-cutsets,\nand other vertex subsets. The main result is that most of the nice algorithmic\ntools developed for modular decomposition of graphs still apply efficiently on\nour generalisation of modules. Besides, when an essential axiom is satisfied,\nalmost all the important properties can be retrieved. For this case, an\nalgorithm given by Ehrenfeucht, Gabow, McConnell and Sullivan 1994 is\ngeneralised and yields a very efficient solution to the associated\ndecomposition problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611023v1", 
    "title": "Faster Streaming algorithms for graph spanners", 
    "arxiv-id": "cs/0611023v1", 
    "author": "Surender Baswana", 
    "publish": "2006-11-06T03:09:05Z", 
    "summary": "Given an undirected graph $G=(V,E)$ on $n$ vertices, $m$ edges, and an\ninteger $t\\ge 1$, a subgraph $(V,E_S)$, $E_S\\subseteq E$ is called a\n$t$-spanner if for any pair of vertices $u,v \\in V$, the distance between them\nin the subgraph is at most $t$ times the actual distance. We present streaming\nalgorithms for computing a $t$-spanner of essentially optimal size-stretch\ntrade offs for any undirected graph.\n  Our first algorithm is for the classical streaming model and works for\nunweighted graphs only. The algorithm performs a single pass on the stream of\nedges and requires $O(m)$ time to process the entire stream of edges. This\ndrastically improves the previous best single pass streaming algorithm for\ncomputing a $t$-spanner which requires $\\theta(mn^{\\frac{2}{t}})$ time to\nprocess the stream and computes spanner with size slightly larger than the\noptimal.\n  Our second algorithm is for {\\em StreamSort} model introduced by Aggarwal et\nal. [FOCS 2004], which is the streaming model augmented with a sorting\nprimitive. The {\\em StreamSort} model has been shown to be a more powerful and\nstill very realistic model than the streaming model for massive data sets\napplications. Our algorithm, which works of weighted graphs as well, performs\n$O(t)$ passes using $O(\\log n)$ bits of working memory only.\n  Our both the algorithms require elementary data structures."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0612037v1", 
    "title": "Least Significant Digit First Presburger Automata", 
    "arxiv-id": "cs/0612037v1", 
    "author": "J\u00e9r\u00f4me Leroux", 
    "publish": "2006-12-06T14:55:36Z", 
    "summary": "Since 1969 \\cite{C-MST69,S-SMJ77}, we know that any Presburger-definable set\n\\cite{P-PCM29} (a set of integer vectors satisfying a formula in the\nfirst-order additive theory of the integers) can be represented by a\nstate-based symmbolic representation, called in this paper Finite Digit Vector\nAutomata (FDVA). Efficient algorithms for manipulating these sets have been\nrecently developed. However, the problem of deciding if a FDVA represents such\na set, is a well-known hard problem first solved by Muchnik in 1991 with a\nquadruply-exponential time algorithm. In this paper, we show how to determine\nin polynomial time whether a FDVA represents a Presburger-definable set, and we\nprovide in this positive case a polynomial time algorithm that constructs a\nPresburger-formula that defines the same set."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0612100v1", 
    "title": "Improved results for a memory allocation problem", 
    "arxiv-id": "cs/0612100v1", 
    "author": "Rob van Stee", 
    "publish": "2006-12-20T13:39:18Z", 
    "summary": "We consider a memory allocation problem that can be modeled as a version of\nbin packing where items may be split, but each bin may contain at most two\n(parts of) items. A 3/2-approximation algorithm and an NP-hardness proof for\nthis problem was given by Chung et al. We give a simpler 3/2-approximation\nalgorithm for it which is in fact an online algorithm. This algorithm also has\ngood performance for the more general case where each bin may contain at most k\nparts of items. We show that this general case is also strongly NP-hard.\nAdditionally, we give an efficient 7/5-approximation algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0701020v2", 
    "title": "A nearly optimal and deterministic summary structure for update data   streams", 
    "arxiv-id": "cs/0701020v2", 
    "author": "Sumit Ganguly", 
    "publish": "2007-01-04T09:03:08Z", 
    "summary": "The paper has been withdrawn due to an error in Lemma 1."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0701142v1", 
    "title": "Knowledge State Algorithms: Randomization with Limited Information", 
    "arxiv-id": "cs/0701142v1", 
    "author": "R\u00fcdiger Reischuk", 
    "publish": "2007-01-23T00:54:27Z", 
    "summary": "We introduce the concept of knowledge states; many well-known algorithms can\nbe viewed as knowledge state algorithms. The knowledge state approach can be\nused to to construct competitive randomized online algorithms and study the\ntradeoff between competitiveness and memory. A knowledge state simply states\nconditional obligations of an adversary, by fixing a work function, and gives a\ndistribution for the algorithm. When a knowledge state algorithm receives a\nrequest, it then calculates one or more \"subsequent\" knowledge states, together\nwith a probability of transition to each. The algorithm then uses randomization\nto select one of those subsequents to be the new knowledge state. We apply the\nmethod to the paging problem. We present optimally competitive algorithm for\npaging for the cases where the cache sizes are k=2 and k=3. These algorithms\nuse only a very limited number of bookmarks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702029v1", 
    "title": "On the variance of subset sum estimation", 
    "arxiv-id": "cs/0702029v1", 
    "author": "Mikkel Thorup", 
    "publish": "2007-02-05T15:55:41Z", 
    "summary": "For high volume data streams and large data warehouses, sampling is used for\nefficient approximate answers to aggregate queries over selected subsets.\nMathematically, we are dealing with a set of weighted items and want to support\nqueries to arbitrary subset sums. With unit weights, we can compute subset\nsizes which together with the previous sums provide the subset averages. The\nquestion addressed here is which sampling scheme we should use to get the most\naccurate subset sum estimates.\n  We present a simple theorem on the variance of subset sum estimation and use\nit to prove variance optimality and near-optimality of subset sum estimation\nwith different known sampling schemes. This variance is measured as the average\nover all subsets of any given size. By optimal we mean there is no set of input\nweights for which any sampling scheme can have a better average variance. Such\npowerful results can never be established experimentally. The results of this\npaper are derived mathematically. For example, we show that appropriately\nweighted systematic sampling is simultaneously optimal for all subset sizes.\nMore standard schemes such as uniform sampling and\nprobability-proportional-to-size sampling with replacement can be arbitrarily\nbad.\n  Knowing the variance optimality of different sampling schemes can help\ndeciding which sampling scheme to apply in a given context."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702032v1", 
    "title": "Finding large and small dense subgraphs", 
    "arxiv-id": "cs/0702032v1", 
    "author": "Reid Andersen", 
    "publish": "2007-02-05T19:29:38Z", 
    "summary": "We consider two optimization problems related to finding dense subgraphs. The\ndensest at-least-k-subgraph problem (DalkS) is to find an induced subgraph of\nhighest average degree among all subgraphs with at least k vertices, and the\ndensest at-most-k-subgraph problem (DamkS) is defined similarly. These problems\nare related to the well-known densest k-subgraph problem (DkS), which is to\nfind the densest subgraph on exactly k vertices. We show that DalkS can be\napproximated efficiently, while DamkS is nearly as hard to approximate as the\ndensest k-subgraph problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702043v1", 
    "title": "Deciding k-colourability of $P_5$-free graphs in polynomial time", 
    "arxiv-id": "cs/0702043v1", 
    "author": "X. Shu", 
    "publish": "2007-02-07T15:29:32Z", 
    "summary": "The problem of computing the chromatic number of a $P_5$-free graph is known\nto be NP-hard. In contrast to this negative result, we show that determining\nwhether or not a $P_5$-free graph admits a $k$-colouring, for each fixed number\nof colours $k$, can be done in polynomial time. If such a colouring exists, our\nalgorithm produces it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702056v1", 
    "title": "A probabilistic analysis of a leader election algorithm", 
    "arxiv-id": "cs/0702056v1", 
    "author": "Hanene Mohamed", 
    "publish": "2007-02-09T15:16:48Z", 
    "summary": "A {\\em leader election} algorithm is an elimination process that divides\nrecursively into tow subgroups an initial group of n items, eliminates one\nsubgroup and continues the procedure until a subgroup is of size 1. In this\npaper the biased case is analyzed. We are interested in the {\\em cost} of the\nalgorithm, i.e. the number of operations needed until the algorithm stops.\nUsing a probabilistic approach, the asymptotic behavior of the algorithm is\nshown to be related to the behavior of a hitting time of two random sequences\non [0,1]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702057v2", 
    "title": "An Efficient Algorithm to Recognize Locally Equivalent Graphs in   Non-Binary Case", 
    "arxiv-id": "cs/0702057v2", 
    "author": "Salman Beigi", 
    "publish": "2007-02-09T15:42:46Z", 
    "summary": "Let $v$ be a vertex of a graph $G$. By the local complementation of $G$ at\n$v$ we mean to complement the subgraph induced by the neighbors of $v$. This\noperator can be generalized as follows. Assume that, each edge of $G$ has a\nlabel in the finite field $\\mathbf{F}_q$. Let $(g_{ij})$ be set of labels\n($g_{ij}$ is the label of edge $ij$). We define two types of operators. For the\nfirst one, let $v$ be a vertex of $G$ and $a\\in \\mathbf{F}_q$, and obtain the\ngraph with labels $g'_{ij}=g_{ij}+ag_{vi}g_{vj}$. For the second, if $0\\neq\nb\\in \\mathbf{F}_q$ the resulted graph is a graph with labels $g''_{vi}=bg_{vi}$\nand $g''_{ij}=g_{ij}$, for $i,j$ unequal to $v$. It is clear that if the field\nis binary, the operators are just local complementations that we described.\n  The problem of whether two graphs are equivalent under local complementations\nhas been studied, \\cite{bouchalg}. Here we consider the general case and\nassuming that $q$ is odd, present the first known efficient algorithm to verify\nwhether two graphs are locally equivalent or not."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702151v3", 
    "title": "Succinct Sampling on Streams", 
    "arxiv-id": "cs/0702151v3", 
    "author": "Carlo Zaniolo", 
    "publish": "2007-02-25T17:20:48Z", 
    "summary": "A streaming model is one where data items arrive over long period of time,\neither one item at a time or in bursts. Typical tasks include computing various\nstatistics over a sliding window of some fixed time-horizon. What makes the\nstreaming model interesting is that as the time progresses, old items expire\nand new ones arrive. One of the simplest and central tasks in this model is\nsampling. That is, the task of maintaining up to $k$ uniformly distributed\nitems from a current time-window as old items expire and new ones arrive. We\ncall sampling algorithms {\\bf succinct} if they use provably optimal (up to\nconstant factors) {\\bf worst-case} memory to maintain $k$ items (either with or\nwithout replacement). We stress that in many applications structures that have\n{\\em expected} succinct representation as the time progresses are not\nsufficient, as small probability events eventually happen with probability 1.\nThus, in this paper we ask the following question: are Succinct Sampling on\nStreams (or $S^3$-algorithms)possible, and if so for what models? Perhaps\nsomewhat surprisingly, we show that $S^3$-algorithms are possible for {\\em all}\nvariants of the problem mentioned above, i.e. both with and without replacement\nand both for one-at-a-time and bursty arrival models. Finally, we use $S^3$\nalgorithms to solve various problems in sliding windows model, including\nfrequency moments, counting triangles, entropy and density estimations. For\nthese problems we present \\emph{first} solutions with provable worst-case\nmemory guarantees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0703006v1", 
    "title": "XORSAT: An Efficient Algorithm for the DIMACS 32-bit Parity Problem", 
    "arxiv-id": "cs/0703006v1", 
    "author": "Jing-Chao Chen", 
    "publish": "2007-03-02T01:38:16Z", 
    "summary": "The DIMACS 32-bit parity problem is a satisfiability (SAT) problem hard to\nsolve. So far, EqSatz by Li is the only solver which can solve this problem.\nHowever, This solver is very slow. It is reported that it spent 11855 seconds\nto solve a par32-5 instance on a Maxintosh G3 300 MHz. The paper introduces a\nnew solver, XORSAT, which splits the original problem into two parts:\nstructured part and random part, and then solves separately them with WalkSAT\nand an XOR equation solver. Based our empirical observation, XORSAT is\nsurprisingly fast, which is approximately 1000 times faster than EqSatz. For a\npar32-5 instance, XORSAT took 2.9 seconds, while EqSatz took 2844 seconds on\nIntel Pentium IV 2.66GHz CPU. We believe that this method significantly\ndifferent from traditional methods is also useful beyond this domain."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0703010v2", 
    "title": "An optimal bifactor approximation algorithm for the metric uncapacitated   facility location problem", 
    "arxiv-id": "cs/0703010v2", 
    "author": "Karen Aardal", 
    "publish": "2007-03-02T14:49:57Z", 
    "summary": "We obtain a 1.5-approximation algorithm for the metric uncapacitated facility\nlocation problem (UFL), which improves on the previously best known\n1.52-approximation algorithm by Mahdian, Ye and Zhang. Note, that the\napproximability lower bound by Guha and Khuller is 1.463.\n  An algorithm is a {\\em ($\\lambda_f$,$\\lambda_c$)-approximation algorithm} if\nthe solution it produces has total cost at most $\\lambda_f \\cdot F^* +\n\\lambda_c \\cdot C^*$, where $F^*$ and $C^*$ are the facility and the connection\ncost of an optimal solution. Our new algorithm, which is a modification of the\n$(1+2/e)$-approximation algorithm of Chudak and Shmoys, is a\n(1.6774,1.3738)-approximation algorithm for the UFL problem and is the first\none that touches the approximability limit curve $(\\gamma_f, 1+2e^{-\\gamma_f})$\nestablished by Jain, Mahdian and Saberi. As a consequence, we obtain the first\noptimal approximation algorithm for instances dominated by connection costs.\nWhen combined with a (1.11,1.7764)-approximation algorithm proposed by Jain et\nal., and later analyzed by Mahdian et al., we obtain the overall approximation\nguarantee of 1.5 for the metric UFL problem. We also describe how to use our\nalgorithm to improve the approximation ratio for the 3-level version of UFL."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74839-7_9", 
    "link": "http://arxiv.org/pdf/cs/0703013v1", 
    "title": "NLC-2 graph recognition and isomorphism", 
    "arxiv-id": "cs/0703013v1", 
    "author": "Micha\u00ebl Rao", 
    "publish": "2007-03-03T06:44:57Z", 
    "summary": "NLC-width is a variant of clique-width with many application in graph\nalgorithmic. This paper is devoted to graphs of NLC-width two. After giving new\nstructural properties of the class, we propose a $O(n^2 m)$-time algorithm,\nimproving Johansson's algorithm \\cite{Johansson00}. Moreover, our alogrithm is\nsimple to understand. The above properties and algorithm allow us to propose a\nrobust $O(n^2 m)$-time isomorphism algorithm for NLC-2 graphs. As far as we\nknow, it is the first polynomial-time algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74839-7_9", 
    "link": "http://arxiv.org/pdf/cs/0703109v2", 
    "title": "Tag-Cloud Drawing: Algorithms for Cloud Visualization", 
    "arxiv-id": "cs/0703109v2", 
    "author": "Daniel Lemire", 
    "publish": "2007-03-22T14:54:48Z", 
    "summary": "Tag clouds provide an aggregate of tag-usage statistics. They are typically\nsent as in-line HTML to browsers. However, display mechanisms suited for\nordinary text are not ideal for tags, because font sizes may vary widely on a\nline. As well, the typical layout does not account for relationships that may\nbe known between tags. This paper presents models and algorithms to improve the\ndisplay of tag clouds that consist of in-line HTML, as well as algorithms that\nuse nested tables to achieve a more general 2-dimensional layout in which tag\nrelationships are considered. The first algorithms leverage prior work in\ntypesetting and rectangle packing, whereas the second group of algorithms\nleverage prior work in Electronic Design Automation. Experiments show our\nalgorithms can be efficiently implemented and perform well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.0062v1", 
    "title": "On-line Viterbi Algorithm and Its Relationship to Random Walks", 
    "arxiv-id": "0704.0062v1", 
    "author": "Tom\u00e1\u0161 Vina\u0159", 
    "publish": "2007-03-31T23:52:33Z", 
    "summary": "In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\ntwo-state HMMs suggests that the expected maximum memory used to decode\nsequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$,\nwithout a significant slow-down compared to the classical Viterbi algorithm.\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\nfor continuous data streams. We also experimentally demonstrate the performance\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\nsimulated and real DNA sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.0834v1", 
    "title": "P-adic arithmetic coding", 
    "arxiv-id": "0704.0834v1", 
    "author": "Sergey Volkov", 
    "publish": "2007-04-06T02:30:42Z", 
    "summary": "A new incremental algorithm for data compression is presented. For a sequence\nof input symbols algorithm incrementally constructs a p-adic integer number as\nan output. Decoding process starts with less significant part of a p-adic\ninteger and incrementally reconstructs a sequence of input symbols. Algorithm\nis based on certain features of p-adic numbers and p-adic norm. p-adic coding\nalgorithm may be considered as of generalization a popular compression\ntechnique - arithmetic coding algorithms. It is shown that for p = 2 the\nalgorithm works as integer variant of arithmetic coding; for a special class of\nmodels it gives exactly the same codes as Huffman's algorithm, for another\nspecial model and a specific alphabet it gives Golomb-Rice codes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.3313v3", 
    "title": "Straggler Identification in Round-Trip Data Streams via Newton's   Identities and Invertible Bloom Filters", 
    "arxiv-id": "0704.3313v3", 
    "author": "Michael T. Goodrich", 
    "publish": "2007-04-25T06:59:43Z", 
    "summary": "We introduce the straggler identification problem, in which an algorithm must\ndetermine the identities of the remaining members of a set after it has had a\nlarge number of insertion and deletion operations performed on it, and now has\nrelatively few remaining members. The goal is to do this in o(n) space, where n\nis the total number of identities. The straggler identification problem has\napplications, for example, in determining the set of unacknowledged packets in\na high-bandwidth multicast data stream. We provide a deterministic solution to\nthe straggler identification problem that uses only O(d log n) bits and is\nbased on a novel application of Newton's identities for symmetric polynomials.\nThis solution can identify any subset of d stragglers from a set of n O(log\nn)-bit identifiers, assuming that there are no false deletions of identities\nnot already in the set. Indeed, we give a lower bound argument that shows that\nany small-space deterministic solution to the straggler identification problem\ncannot be guaranteed to handle false deletions. Nevertheless, we show that\nthere is a simple randomized solution using O(d log n log(1/epsilon)) bits that\ncan maintain a multiset and solve the straggler identification problem,\ntolerating false deletions, where epsilon>0 is a user-defined parameter\nbounding the probability of an incorrect response. This randomized solution is\nbased on a new type of Bloom filter, which we call the invertible Bloom filter."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.3773v2", 
    "title": "Avoiding Rotated Bitboards with Direct Lookup", 
    "arxiv-id": "0704.3773v2", 
    "author": "Sam Tannous", 
    "publish": "2007-04-28T03:11:59Z", 
    "summary": "This paper describes an approach for obtaining direct access to the attacked\nsquares of sliding pieces without resorting to rotated bitboards. The technique\ninvolves creating four hash tables using the built in hash arrays from an\ninterpreted, high level language. The rank, file, and diagonal occupancy are\nfirst isolated by masking the desired portion of the board. The attacked\nsquares are then directly retrieved from the hash tables. Maintaining\nincrementally updated rotated bitboards becomes unnecessary as does all the\nupdating, mapping and shifting required to access the attacked squares.\nFinally, rotated bitboard move generation speed is compared with that of the\ndirect hash table lookup method."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.0204v1", 
    "title": "Using Images to create a Hierarchical Grid Spatial Index", 
    "arxiv-id": "0705.0204v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-02T05:37:32Z", 
    "summary": "This paper presents a hybrid approach to spatial indexing of two dimensional\ndata. It sheds new light on the age old problem by thinking of the traditional\nalgorithms as working with images. Inspiration is drawn from an analogous\nsituation that is found in machine and human vision. Image processing\ntechniques are used to assist in the spatial indexing of the data. A fixed grid\napproach is used and bins with too many records are sub-divided hierarchically.\nSearch queries are pre-computed for bins that do not contain any data records.\nThis has the effect of dividing the search space up into non rectangular\nregions which are based on the spatial properties of the data. The bucketing\nquad tree can be considered as an image with a resolution of two by two for\neach layer. The results show that this method performs better than the quad\ntree if there are more divisions per layer. This confirms our suspicions that\nthe algorithm works better if it gets to look at the data with higher\nresolution images. An elegant class structure is developed where the\nimplementation of concrete spatial indexes for a particular data type merely\nrelies on rendering the data onto an image."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1025v2", 
    "title": "Recognizing Partial Cubes in Quadratic Time", 
    "arxiv-id": "0705.1025v2", 
    "author": "David Eppstein", 
    "publish": "2007-05-08T17:59:08Z", 
    "summary": "We show how to test whether a graph with n vertices and m edges is a partial\ncube, and if so how to find a distance-preserving embedding of the graph into a\nhypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time\nsolutions."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1521v2", 
    "title": "A note on module-composed graphs", 
    "arxiv-id": "0705.1521v2", 
    "author": "Frank Gurski", 
    "publish": "2007-05-10T18:08:22Z", 
    "summary": "In this paper we consider module-composed graphs, i.e. graphs which can be\ndefined by a sequence of one-vertex insertions v_1,...,v_n, such that the\nneighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of\nthe graph defined by vertices v_1,..., v_{i-1}.\n  We show that module-composed graphs are HHDS-free and thus homogeneously\norderable, weakly chordal, and perfect. Every bipartite distance hereditary\ngraph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is\nmodule-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide\nwhether a given graph G is module-composed and construct a corresponding\nmodule-sequence.\n  For the case of bipartite graphs, module-composed graphs are exactly distance\nhereditary graphs, which implies simple linear time algorithms for their\nrecognition and construction of a corresponding module-sequence."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1750v6", 
    "title": "A Tighter Analysis of Setcover Greedy Algorithm for Test Set", 
    "arxiv-id": "0705.1750v6", 
    "author": "Peng Cui", 
    "publish": "2007-05-12T04:18:36Z", 
    "summary": "Setcover greedy algorithm is a natural approximation algorithm for test set\nproblem. This paper gives a precise and tighter analysis of performance\nguarantee of this algorithm. The author improves the performance guarantee\n$2\\ln n$ which derives from set cover problem to $1.1354\\ln n$ by applying the\npotential function technique. In addition, the author gives a nontrivial lower\nbound $1.0004609\\ln n$ of performance guarantee of this algorithm. This lower\nbound, together with the matching bound of information content heuristic,\nconfirms the fact information content heuristic is slightly better than\nsetcover greedy algorithm in worst case."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1970v1", 
    "title": "A Closed-Form Method for LRU Replacement under Generalized Power-Law   Demand", 
    "arxiv-id": "0705.1970v1", 
    "author": "Nikolaos Laoutaris", 
    "publish": "2007-05-14T16:04:48Z", 
    "summary": "We consider the well known \\emph{Least Recently Used} (LRU) replacement\nalgorithm and analyze it under the independent reference model and generalized\npower-law demand. For this extensive family of demand distributions we derive a\nclosed-form expression for the per object steady-state hit ratio. To the best\nof our knowledge, this is the first analytic derivation of the per object hit\nratio of LRU that can be obtained in constant time without requiring laborious\nnumeric computations or simulation. Since most applications of replacement\nalgorithms include (at least) some scenarios under i.i.d. requests, our method\nhas substantial practical value, especially when having to analyze multiple\ncaches, where existing numeric methods and simulation become too time\nconsuming."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1986v1", 
    "title": "On the Hopcroft's minimization algorithm", 
    "arxiv-id": "0705.1986v1", 
    "author": "Andrei Paun", 
    "publish": "2007-05-14T17:15:53Z", 
    "summary": "We show that the absolute worst case time complexity for Hopcroft's\nminimization algorithm applied to unary languages is reached only for de Bruijn\nwords. A previous paper by Berstel and Carton gave the example of de Bruijn\nwords as a language that requires O(n log n) steps by carefully choosing the\nsplitting sets and processing these sets in a FIFO mode. We refine the previous\nresult by showing that the Berstel/Carton example is actually the absolute\nworst case time complexity in the case of unary languages. We also show that a\nLIFO implementation will not achieve the same worst time complexity for the\ncase of unary languages. Lastly, we show that the same result is valid also for\nthe cover automata and a modification of the Hopcroft's algorithm, modification\nused in minimization of cover automata."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.4171v1", 
    "title": "Grover search algorithm", 
    "arxiv-id": "0705.4171v1", 
    "author": "Eva Borbely", 
    "publish": "2007-05-29T09:42:46Z", 
    "summary": "A quantum algorithm is a set of instructions for a quantum computer, however,\nunlike algorithms in classical computer science their results cannot be\nguaranteed. A quantum system can undergo two types of operation, measurement\nand quantum state transformation, operations themselves must be unitary\n(reversible). Most quantum algorithms involve a series of quantum state\ntransformations followed by a measurement. Currently very few quantum\nalgorithms are known and no general design methodology exists for their\nconstruction."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.0046v1", 
    "title": "Symmetry Partition Sort", 
    "arxiv-id": "0706.0046v1", 
    "author": "Jing-Chao Chen", 
    "publish": "2007-06-01T01:47:06Z", 
    "summary": "In this paper, we propose a useful replacement for quicksort-style utility\nfunctions. The replacement is called Symmetry Partition Sort, which has\nessentially the same principle as Proportion Extend Sort. The maximal\ndifference between them is that the new algorithm always places already\npartially sorted inputs (used as a basis for the proportional extension) on\nboth ends when entering the partition routine. This is advantageous to speeding\nup the partition routine. The library function based on the new algorithm is\nmore attractive than Psort which is a library function introduced in 2004. Its\nimplementation mechanism is simple. The source code is clearer. The speed is\nfaster, with O(n log n) performance guarantee. Both the robustness and\nadaptivity are better. As a library function, it is competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.1084v1", 
    "title": "Sublinear Algorithms for Approximating String Compressibility", 
    "arxiv-id": "0706.1084v1", 
    "author": "Adam Smith", 
    "publish": "2007-06-08T02:58:28Z", 
    "summary": "We raise the question of approximating the compressibility of a string with\nrespect to a fixed compression scheme, in sublinear time. We study this\nquestion in detail for two popular lossless compression schemes: run-length\nencoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for\napproximating compressibility with respect to both schemes. We also give\nseveral lower bounds that show that our algorithms for both schemes cannot be\nimproved significantly.\n  Our investigation of LZ yields results whose interest goes beyond the initial\nquestions we set out to study. In particular, we prove combinatorial structural\nlemmas that relate the compressibility of a string with respect to Lempel-Ziv\nto the number of distinct short substrings contained in it. In addition, we\nshow that approximating the compressibility with respect to LZ is related to\napproximating the support size of a distribution."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.3565v2", 
    "title": "Experimental Algorithm for the Maximum Independent Set Problem", 
    "arxiv-id": "0706.3565v2", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "2007-06-25T06:45:49Z", 
    "summary": "We develop an experimental algorithm for the exact solving of the maximum\nindependent set problem. The algorithm consecutively finds the maximal\nindependent sets of vertices in an arbitrary undirected graph such that the\nnext such set contains more elements than the preceding one. For this purpose,\nwe use a technique, developed by Ford and Fulkerson for the finite partially\nordered sets, in particular, their method for partition of a poset into the\nminimum number of chains with finding the maximum antichain. In the process of\nsolving, a special digraph is constructed, and a conjecture is formulated\nconcerning properties of such digraph. This allows to offer of the solution\nalgorithm. Its theoretical estimation of running time equals to is $O(n^{8})$,\nwhere $n$ is the number of graph vertices. The offered algorithm was tested by\na program on random graphs. The testing the confirms correctness of the\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.4107v1", 
    "title": "Radix Sorting With No Extra Space", 
    "arxiv-id": "0706.4107v1", 
    "author": "Mihai Patrascu", 
    "publish": "2007-06-27T22:04:40Z", 
    "summary": "It is well known that n integers in the range [1,n^c] can be sorted in O(n)\ntime in the RAM model using radix sorting. More generally, integers in any\nrange [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these\nalgorithms use O(n) words of extra memory. Is this necessary?\n  We present a simple, stable, integer sorting algorithm for words of size\nO(log n), which works in O(n) time and uses only O(1) words of extra memory on\na RAM model. This is the integer sorting case most useful in practice. We\nextend this result with same bounds to the case when the keys are read-only,\nwhich is of theoretical interest. Another interesting question is the case of\narbitrary c. Here we present a black-box transformation from any RAM sorting\nalgorithm to a sorting algorithm which uses only O(1) extra space and has the\nsame running time. This settles the complexity of in-place sorting in terms of\nthe complexity of sorting."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.0546v1", 
    "title": "Weighted Popular Matchings", 
    "arxiv-id": "0707.0546v1", 
    "author": "Juli\u00e1n Mestre", 
    "publish": "2007-07-04T06:55:43Z", 
    "summary": "We study the problem of assigning jobs to applicants. Each applicant has a\nweight and provides a preference list ranking a subset of the jobs. A matching\nM is popular if there is no other matching M' such that the weight of the\napplicants who prefer M' over M exceeds the weight of those who prefer M over\nM'. This paper gives efficient algorithms to find a popular matching if one\nexists."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.0648v1", 
    "title": "Dial a Ride from k-forest", 
    "arxiv-id": "0707.0648v1", 
    "author": "R. Ravi", 
    "publish": "2007-07-04T16:08:40Z", 
    "summary": "The k-forest problem is a common generalization of both the k-MST and the\ndense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices\n$V$, with $m$ demand pairs $\\subseteq V \\times V$ and a ``target'' $k\\le m$,\nthe goal is to find a minimum cost subgraph that connects at least $k$ demand\npairs. In this paper, we give an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\})$-approximation\nalgorithm for $k$-forest, improving on the previous best ratio of\n$O(n^{2/3}\\log n)$ by Segev & Segev.\n  We then apply our algorithm for k-forest to obtain approximation algorithms\nfor several Dial-a-Ride problems. The basic Dial-a-Ride problem is the\nfollowing: given an $n$ point metric space with $m$ objects each with its own\nsource and destination, and a vehicle capable of carrying at most $k$ objects\nat any time, find the minimum length tour that uses this vehicle to move each\nobject from its source to destination. We prove that an $\\alpha$-approximation\nalgorithm for the $k$-forest problem implies an\n$O(\\alpha\\cdot\\log^2n)$-approximation algorithm for Dial-a-Ride. Using our\nresults for $k$-forest, we get an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\}\\cdot\\log^2 n)$-\napproximation algorithm for Dial-a-Ride. The only previous result known for\nDial-a-Ride was an $O(\\sqrt{k}\\log n)$-approximation by Charikar &\nRaghavachari; our results give a different proof of a similar approximation\nguarantee--in fact, when the vehicle capacity $k$ is large, we give a slight\nimprovement on their results."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.1051v1", 
    "title": "Noisy Sorting Without Resampling", 
    "arxiv-id": "0707.1051v1", 
    "author": "Elchanan Mossel", 
    "publish": "2007-07-06T21:30:24Z", 
    "summary": "In this paper we study noisy sorting without re-sampling. In this problem\nthere is an unknown order $a_{\\pi(1)} < ... < a_{\\pi(n)}$ where $\\pi$ is a\npermutation on $n$ elements. The input is the status of $n \\choose 2$ queries\nof the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least\n$1/2+\\ga$ if $\\pi(i) > \\pi(j)$ for all pairs $i \\neq j$, where $\\ga > 0$ is a\nconstant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that\nthe errors are independent. Given the status of the queries the goal is to find\nthe maximum likelihood order. In other words, the goal is find a permutation\n$\\sigma$ that minimizes the number of pairs $\\sigma(i) > \\sigma(j)$ where\n$q(\\sigma(i),\\sigma(j)) = -$. The problem so defined is the feedback arc set\nproblem on distributions of inputs, each of which is a tournament obtained as a\nnoisy perturbations of a linear order. Note that when $\\ga < 1/2$ and $n$ is\nlarge, it is impossible to recover the original order $\\pi$.\n  It is known that the weighted feedback are set problem on tournaments is\nNP-hard in general. Here we present an algorithm of running time\n$n^{O(\\gamma^{-4})}$ and sampling complexity $O_{\\gamma}(n \\log n)$ that with\nhigh probability solves the noisy sorting without re-sampling problem. We also\nshow that if $a_{\\sigma(1)},a_{\\sigma(2)},...,a_{\\sigma(n)}$ is an optimal\nsolution of the problem then it is ``close'' to the original order. More\nformally, with high probability it holds that $\\sum_i |\\sigma(i) - \\pi(i)| =\n\\Theta(n)$ and $\\max_i |\\sigma(i) - \\pi(i)| = \\Theta(\\log n)$.\n  Our results are of interest in applications to ranking, such as ranking in\nsports, or ranking of search items based on comparisons by experts."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.1714v1", 
    "title": "Sampling Algorithms and Coresets for Lp Regression", 
    "arxiv-id": "0707.1714v1", 
    "author": "Michael W. Mahoney", 
    "publish": "2007-07-11T22:04:18Z", 
    "summary": "The Lp regression problem takes as input a matrix $A \\in \\Real^{n \\times d}$,\na vector $b \\in \\Real^n$, and a number $p \\in [1,\\infty)$, and it returns as\noutput a number ${\\cal Z}$ and a vector $x_{opt} \\in \\Real^d$ such that ${\\cal\nZ} = \\min_{x \\in \\Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we\nconstruct coresets and obtain an efficient two-stage sampling-based\napproximation algorithm for the very overconstrained ($n \\gg d$) version of\nthis classical problem, for all $p \\in [1, \\infty)$. The first stage of our\nalgorithm non-uniformly samples $\\hat{r}_1 = O(36^p d^{\\max\\{p/2+1, p\\}+1})$\nrows of $A$ and the corresponding elements of $b$, and then it solves the Lp\nregression problem on the sample; we prove this is an 8-approximation. The\nsecond stage of our algorithm uses the output of the first stage to resample\n$\\hat{r}_1/\\epsilon^2$ constraints, and then it solves the Lp regression\nproblem on the new sample; we prove this is a $(1+\\epsilon)$-approximation. Our\nalgorithm unifies, improves upon, and extends the existing algorithms for\nspecial cases of Lp regression, namely $p = 1,2$. In course of proving our\nresult, we develop two concepts--well-conditioned bases and subspace-preserving\nsampling--that are of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.2160v1", 
    "title": "Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture", 
    "arxiv-id": "0707.2160v1", 
    "author": "Seth Pettie", 
    "publish": "2007-07-14T16:38:08Z", 
    "summary": "We introduce a new technique to bound the asymptotic performance of splay\ntrees. The basic idea is to transcribe, in an indirect fashion, the rotations\nperformed by the splay tree as a Davenport-Schinzel sequence S, none of whose\nsubsequences are isomorphic to fixed forbidden subsequence. We direct this\ntechnique towards Tarjan's deque conjecture and prove that n deque operations\nrequire O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of\napplications of the inverse-Ackermann function mapping n to a constant. We are\noptimistic that this approach could be directed towards other open conjectures\non splay trees such as the traversal and split conjectures."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.0600v1", 
    "title": "Complementary algorithms for graphs and percolation", 
    "arxiv-id": "0708.0600v1", 
    "author": "Michael J. Lee", 
    "publish": "2007-08-04T02:56:13Z", 
    "summary": "A pair of complementary algorithms are presented. One of the pair is a fast\nmethod for connecting graphs with an edge. The other is a fast method for\nremoving edges from a graph. Both algorithms employ the same tree based graph\nrepresentation and so, in concert, can arbitrarily modify any graph. Since the\nclusters of a percolation model may be described as simple connected graphs, an\nefficient Monte Carlo scheme can be constructed that uses the algorithms to\nsweep the occupation probability back and forth between two turning points.\nThis approach concentrates computational sampling time within a region of\ninterest. A high precision value of pc = 0.59274603(9) was thus obtained, by\nMersenne twister, for the two dimensional square site percolation threshold."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.2936v1", 
    "title": "Priority Queue Based on Multilevel Prefix Tree", 
    "arxiv-id": "0708.2936v1", 
    "author": "David S. Planeta", 
    "publish": "2007-08-21T22:59:49Z", 
    "summary": "Tree structures are very often used data structures. Among ordered types of\ntrees there are many variants whose basic operations such as insert, delete,\nsearch, delete-min are characterized by logarithmic time complexity. In the\narticle I am going to present the structure whose time complexity for each of\nthe above operations is $O(\\frac{M}{K} + K)$, where M is the size of data type\nand K is constant properly matching the size of data type. Properly matched K\nwill make the structure function as a very effective Priority Queue. The\nstructure size linearly depends on the number and size of elements. PTrie is a\nclever combination of the idea of prefix tree -- Trie, structure of logarithmic\ntime complexity for insert and remove operations, doubly linked list and\nqueues."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.3408v1", 
    "title": "Linear Time Algorithms Based on Multilevel Prefix Tree for Finding   Shortest Path with Positive Weights and Minimum Spanning Tree in a Networks", 
    "arxiv-id": "0708.3408v1", 
    "author": "David S. Planeta", 
    "publish": "2007-08-24T21:58:29Z", 
    "summary": "In this paper I present general outlook on questions relevant to the basic\ngraph algorithms; Finding the Shortest Path with Positive Weights and Minimum\nSpanning Tree. I will show so far known solution set of basic graph problems\nand present my own. My solutions to graph problems are characterized by their\nlinear worst-case time complexity. It should be noticed that the algorithms\nwhich compute the Shortest Path and Minimum Spanning Tree problems not only\nanalyze the weight of arcs (which is the main and often the only criterion of\nsolution hitherto known algorithms) but also in case of identical path weights\nthey select this path which walks through as few vertices as possible. I have\npresented algorithms which use priority queue based on multilevel prefix tree\n-- PTrie. PTrie is a clever combination of the idea of prefix tree -- Trie, the\nstructure of logarithmic time complexity for insert and remove operations,\ndoubly linked list and queues. In C++ I will implement linear worst-case time\nalgorithm computing the Single-Destination Shortest-Paths problem and I will\nexplain its usage."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.3696v1", 
    "title": "Relative-Error CUR Matrix Decompositions", 
    "arxiv-id": "0708.3696v1", 
    "author": "S. Muthukrishnan", 
    "publish": "2007-08-27T23:34:50Z", 
    "summary": "Many data analysis applications deal with large matrices and involve\napproximating the matrix using a small number of ``components.'' Typically,\nthese components are linear combinations of the rows and columns of the matrix,\nand are thus difficult to interpret in terms of the original features of the\ninput data. In this paper, we propose and study matrix approximations that are\nexplicitly expressed in terms of a small number of columns and/or rows of the\ndata matrix, and thereby more amenable to interpretation in terms of the\noriginal data. Our main algorithmic results are two randomized algorithms which\ntake as input an $m \\times n$ matrix $A$ and a rank parameter $k$. In our first\nalgorithm, $C$ is chosen, and we let $A'=CC^+A$, where $C^+$ is the\nMoore-Penrose generalized inverse of $C$. In our second algorithm $C$, $U$, $R$\nare chosen, and we let $A'=CUR$. ($C$ and $R$ are matrices that consist of\nactual columns and rows, respectively, of $A$, and $U$ is a generalized inverse\nof their intersection.) For each algorithm, we show that with probability at\nleast $1-\\delta$: $$ ||A-A'||_F \\leq (1+\\epsilon) ||A-A_k||_F, $$ where $A_k$\nis the ``best'' rank-$k$ approximation provided by truncating the singular\nvalue decomposition (SVD) of $A$. The number of columns of $C$ and rows of $R$\nis a low-degree polynomial in $k$, $1/\\epsilon$, and $\\log(1/\\delta)$. Our two\nalgorithms are the first polynomial time algorithms for such low-rank matrix\napproximations that come with relative-error guarantees; previously, in some\ncases, it was not even known whether such matrix decompositions exist. Both of\nour algorithms are simple, they take time of the order needed to approximately\ncompute the top $k$ singular vectors of $A$, and they use a novel, intuitive\nsampling method called ``subspace sampling.''"
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.4288v1", 
    "title": "Pattern Matching in Trees and Strings", 
    "arxiv-id": "0708.4288v1", 
    "author": "Philip Bille", 
    "publish": "2007-08-31T08:07:32Z", 
    "summary": "We study the design of efficient algorithms for combinatorial pattern\nmatching. More concretely, we study algorithms for tree matching, string\nmatching, and string matching in compressed texts."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0709.0624v1", 
    "title": "On Faster Integer Calculations using Non-Arithmetic Primitives", 
    "arxiv-id": "0709.0624v1", 
    "author": "Martin Ziegler", 
    "publish": "2007-09-05T11:34:54Z", 
    "summary": "The unit cost model is both convenient and largely realistic for describing\ninteger decision algorithms over (+,*). Additional operations like division\nwith remainder or bitwise conjunction, although equally supported by computing\nhardware, may lead to a considerable drop in complexity. We show a variety of\nconcrete problems to benefit from such NON-arithmetic primitives by presenting\nand analyzing corresponding fast algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.0083v1", 
    "title": "Sorting and Selection with Random Costs", 
    "arxiv-id": "0710.0083v1", 
    "author": "Andrew McGregor", 
    "publish": "2007-09-29T18:10:28Z", 
    "summary": "There is a growing body of work on sorting and selection in models other than\nthe unit-cost comparison model. This work is the first treatment of a natural\nstochastic variant of the problem where the cost of comparing two elements is a\nrandom variable. Each cost is chosen independently and is known to the\nalgorithm. In particular we consider the following three models: each cost is\nchosen uniformly in the range $[0,1]$, each cost is 0 with some probability $p$\nand 1 otherwise, or each cost is 1 with probability $p$ and infinite otherwise.\nWe present lower and upper bounds (optimal in most cases) for these problems.\nWe obtain our upper bounds by carefully designing algorithms to ensure that the\ncosts incurred at various stages are independent and using properties of random\npartial orders when appropriate."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.1435v4", 
    "title": "Faster Least Squares Approximation", 
    "arxiv-id": "0710.1435v4", 
    "author": "Tamas Sarlos", 
    "publish": "2007-10-07T17:37:37Z", 
    "summary": "Least squares approximation is a technique to find an approximate solution to\na system of linear equations that has no exact solution. In a typical setting,\none lets $n$ be the number of constraints and $d$ be the number of variables,\nwith $n \\gg d$. Then, existing exact methods find a solution vector in\n$O(nd^2)$ time. We present two randomized algorithms that provide very accurate\nrelative-error approximations to the optimal value and the solution vector of a\nleast squares approximation problem more rapidly than existing exact\nalgorithms. Both of our algorithms preprocess the data with the Randomized\nHadamard Transform. One then uniformly randomly samples constraints and solves\nthe smaller problem on those constraints, and the other performs a sparse\nrandom projection and solves the smaller problem on those projected\ncoordinates. In both cases, solving the smaller problem provides relative-error\napproximations, and, if $n$ is sufficiently larger than $d$, the approximate\nsolution can be computed in $O(nd \\log d)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.2532v1", 
    "title": "Sleeping on the Job: Energy-Efficient Broadcast for Radio Networks", 
    "arxiv-id": "0710.2532v1", 
    "author": "Maxwell Young", 
    "publish": "2007-10-12T19:56:45Z", 
    "summary": "We address the problem of minimizing power consumption when performing\nreliable broadcast on a radio network under the following popular model. Each\nnode in the network is located on a point in a two dimensional grid, and\nwhenever a node sends a message, all awake nodes within distance r receive the\nmessage. In the broadcast problem, some node wants to successfully send a\nmessage to all other nodes in the network even when up to a 1/2 fraction of the\nnodes within every neighborhood can be deleted by an adversary. The set of\ndeleted nodes is carefully chosen by the adversary to foil our algorithm and\nmoreover, the set of deleted nodes may change periodically. This models\nworst-case behavior due to mobile nodes, static nodes losing power or simply\nsome points in the grid being unoccupied. A trivial solution requires each node\nin the network to be awake roughly 1/2 the time, and a trivial lower bound\nshows that each node must be awake for at least a 1/n fraction of the time. Our\nfirst result is an algorithm that requires each node to be awake for only a\n1/sqrt(n) fraction of the time in expectation. Our algorithm achieves this\nwhile ensuring correctness with probability 1, and keeping optimal values for\nother resource costs such as latency and number of messages sent. We give a\nlower-bound that shows that this reduction in power consumption is\nasymptotically optimal when latency and number of messages sent must be\noptimal. If we can increase the latency and messages sent by only a log*n\nfactor we give a Las Vegas algorithm that requires each node to be awake for\nonly a (log*n)/n expected fraction of the time; we give a lower-bound showing\nthat this second algorithm is near optimal. Finally, we show how to ensure\nenergy-efficient broadcast in the presence of Byzantine faults."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.4410v1", 
    "title": "A Multi-level Blocking Distinct Degree Factorization Algorithm", 
    "arxiv-id": "0710.4410v1", 
    "author": "Paul Zimmermann", 
    "publish": "2007-10-24T09:18:33Z", 
    "summary": "We give a new algorithm for performing the distinct-degree factorization of a\npolynomial P(x) over GF(2), using a multi-level blocking strategy. The coarsest\nlevel of blocking replaces GCD computations by multiplications, as suggested by\nPollard (1975), von zur Gathen and Shoup (1992), and others. The novelty of our\napproach is that a finer level of blocking replaces multiplications by\nsquarings, which speeds up the computation in GF(2)[x]/P(x) of certain interval\npolynomials when P(x) is sparse. As an application we give a fast algorithm to\nsearch for all irreducible trinomials x^r + x^s + 1 of degree r over GF(2),\nwhile producing a certificate that can be checked in less time than the full\nsearch. Naive algorithms cost O(r^2) per trinomial, thus O(r^3) to search over\nall trinomials of given degree r. Under a plausible assumption about the\ndistribution of factors of trinomials, the new algorithm has complexity O(r^2\n(log r)^{3/2}(log log r)^{1/2}) for the search over all trinomials of degree r.\nOur implementation achieves a speedup of greater than a factor of 560 over the\nnaive algorithm in the case r = 24036583 (a Mersenne exponent). Using our\nprogram, we have found two new primitive trinomials of degree 24036583 over\nGF(2) (the previous record degree was 6972593)."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.0251v1", 
    "title": "Faster Algorithms for Online Topological Ordering", 
    "arxiv-id": "0711.0251v1", 
    "author": "Rogers Mathew", 
    "publish": "2007-11-02T06:42:43Z", 
    "summary": "We present two algorithms for maintaining the topological order of a directed\nacyclic graph with n vertices, under an online edge insertion sequence of m\nedges. Efficient algorithms for online topological ordering have many\napplications, including online cycle detection, which is to discover the first\nedge that introduces a cycle under an arbitrary sequence of edge insertions in\na directed graph. In this paper we present efficient algorithms for the online\ntopological ordering problem.\n  We first present a simple algorithm with running time O(n^{5/2}) for the\nonline topological ordering problem. This is the current fastest algorithm for\nthis problem on dense graphs, i.e., when m > n^{5/3}. We then present an\nalgorithm with running time O((m + nlog n)\\sqrt{m}); this is more efficient for\nsparse graphs. Our results yield an improved upper bound of O(min(n^{5/2}, (m +\nnlog n)sqrt{m})) for the online topological ordering problem."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.1682v1", 
    "title": "Data Structures for Mergeable Trees", 
    "arxiv-id": "0711.1682v1", 
    "author": "Renato F. Werneck", 
    "publish": "2007-11-11T21:28:20Z", 
    "summary": "Motivated by an application in computational topology, we consider a novel\nvariant of the problem of efficiently maintaining dynamic rooted trees. This\nvariant requires merging two paths in a single operation. In contrast to the\nstandard problem, in which only one tree arc changes at a time, a single merge\noperation can change many arcs. In spite of this, we develop a data structure\nthat supports merges on an n-node forest in O(log^2 n) amortized time and all\nother standard tree operations in O(log n) time (amortized, worst-case, or\nrandomized depending on the underlying data structure). For the special case\nthat occurs in the motivating application, in which arbitrary arc deletions\n(cuts) are not allowed, we give a data structure with an O(log n) time bound\nper operation. This is asymptotically optimal under certain assumptions. For\nthe even-more special case in which both cuts and parent queries are\ndisallowed, we give an alternative O(log n)-time solution that uses standard\ndynamic trees as a black box. This solution also applies to the motivating\napplication. Our methods use previous work on dynamic trees in various ways,\nbut the analysis of each algorithm requires novel ideas. We also investigate\nlower bounds for the problem under various assumptions."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2157v3", 
    "title": "On Approximating Multi-Criteria TSP", 
    "arxiv-id": "0711.2157v3", 
    "author": "Bodo Manthey", 
    "publish": "2007-11-14T10:53:49Z", 
    "summary": "We present approximation algorithms for almost all variants of the\nmulti-criteria traveling salesman problem (TSP).\n  First, we devise randomized approximation algorithms for multi-criteria\nmaximum traveling salesman problems (Max-TSP). For multi-criteria Max-STSP,\nwhere the edge weights have to be symmetric, we devise an algorithm with an\napproximation ratio of 2/3 - eps. For multi-criteria Max-ATSP, where the edge\nweights may be asymmetric, we present an algorithm with a ratio of 1/2 - eps.\nOur algorithms work for any fixed number k of objectives. Furthermore, we\npresent a deterministic algorithm for bi-criteria Max-STSP that achieves an\napproximation ratio of 7/27.\n  Finally, we present a randomized approximation algorithm for the asymmetric\nmulti-criteria minimum TSP with triangle inequality Min-ATSP. This algorithm\nachieves a ratio of log n + eps."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2399v3", 
    "title": "Minimum-weight double-tree shortcutting for Metric TSP: Bounding the   approximation ratio", 
    "arxiv-id": "0711.2399v3", 
    "author": "Alexander Tiskin", 
    "publish": "2007-11-15T13:19:01Z", 
    "summary": "The Metric Traveling Salesman Problem (TSP) is a classical NP-hard\noptimization problem. The double-tree shortcutting method for Metric TSP yields\nan exponentially-sized space of TSP tours, each of which approximates the\noptimal solution within at most a factor of 2. We consider the problem of\nfinding among these tours the one that gives the closest approximation, i.e.\\\nthe \\emph{minimum-weight double-tree shortcutting}. Previously, we gave an\nefficient algorithm for this problem, and carried out its experimental\nanalysis. In this paper, we address the related question of the worst-case\napproximation ratio for the minimum-weight double-tree shortcutting method. In\nparticular, we give lower bounds on the approximation ratio in some specific\nmetric spaces: the ratio of 2 in the discrete shortest path metric, 1.622 in\nthe planar Euclidean metric, and 1.666 in the planar Minkowski metric. The\nfirst of these lower bounds is tight; we conjecture that the other two bounds\nare also tight, and in particular that the minimum-weight double-tree method\nprovides a 1.622-approximation for planar Euclidean TSP."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2710v2", 
    "title": "Finding a Feasible Flow in a Strongly Connected Network", 
    "arxiv-id": "0711.2710v2", 
    "author": "Robert E. Tarjan", 
    "publish": "2007-11-17T01:59:53Z", 
    "summary": "We consider the problem of finding a feasible single-commodity flow in a\nstrongly connected network with fixed supplies and demands, provided that the\nsum of supplies equals the sum of demands and the minimum arc capacity is at\nleast this sum. A fast algorithm for this problem improves the worst-case time\nbound of the Goldberg-Rao maximum flow method by a constant factor. Erlebach\nand Hagerup gave an linear-time feasible flow algorithm. We give an arguably\nsimpler one."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.3250v1", 
    "title": "Improved Fully Dynamic Reachability Algorithm for Directed Graph", 
    "arxiv-id": "0711.3250v1", 
    "author": "Venkata Seshu Kumar Kurapati", 
    "publish": "2007-11-21T03:22:12Z", 
    "summary": "We propose a fully dynamic algorithm for maintaining reachability information\nin directed graphs. The proposed deterministic dynamic algorithm has an update\ntime of $O((ins*n^{2}) + (del * (m+n*log(n))))$ where $m$ is the current number\nof edges, $n$ is the number of vertices in the graph, $ins$ is the number of\nedge insertions and $del$ is the number of edge deletions. Each query can be\nanswered in O(1) time after each update. The proposed algorithm combines\nexisting fully dynamic reachability algorithm with well known witness counting\ntechnique to improve efficiency of maintaining reachability information when\nedges are deleted. The proposed algorithm improves by a factor of\n$O(\\frac{n^2}{m+n*log(n)})$ for edge deletion over the best existing fully\ndynamic algorithm for maintaining reachability information."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.3861v5", 
    "title": "Approximation Algorithms for Restless Bandit Problems", 
    "arxiv-id": "0711.3861v5", 
    "author": "Peng Shi", 
    "publish": "2007-11-25T18:01:35Z", 
    "summary": "The restless bandit problem is one of the most well-studied generalizations\nof the celebrated stochastic multi-armed bandit problem in decision theory. In\nits ultimate generality, the restless bandit problem is known to be PSPACE-Hard\nto approximate to any non-trivial factor, and little progress has been made\ndespite its importance in modeling activity allocation under uncertainty.\n  We consider a special case that we call Feedback MAB, where the reward\nobtained by playing each of n independent arms varies according to an\nunderlying on/off Markov process whose exact state is only revealed when the\narm is played. The goal is to design a policy for playing the arms in order to\nmaximize the infinite horizon time average expected reward. This problem is\nalso an instance of a Partially Observable Markov Decision Process (POMDP), and\nis widely studied in wireless scheduling and unmanned aerial vehicle (UAV)\nrouting. Unlike the stochastic MAB problem, the Feedback MAB problem does not\nadmit to greedy index-based optimal policies.\n  We develop a novel and general duality-based algorithmic technique that\nyields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy\nto this problem. We then define a general sub-class of restless bandit problems\nthat we term Monotone bandits, for which our policy is a 2-approximation. Our\ntechnique is robust enough to handle generalizations of these problems to\nincorporate various side-constraints such as blocking plays and switching\ncosts. This technique is also of independent interest for other restless bandit\nproblems. By presenting the first (and efficient) O(1) approximations for\nnon-trivial instances of restless bandits as well as of POMDPs, our work\ninitiates the study of approximation algorithms in both these contexts."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.4573v1", 
    "title": "A Note On Computing Set Overlap Classes", 
    "arxiv-id": "0711.4573v1", 
    "author": "Micha\u00ebl Rao", 
    "publish": "2007-11-28T20:07:46Z", 
    "summary": "Let ${\\cal V}$ be a finite set of $n$ elements and ${\\cal F}=\\{X_1,X_2, >...,\nX_m\\}$ a family of $m$ subsets of ${\\cal V}.$ Two sets $X_i$ and $X_j$ of\n${\\cal F}$ overlap if $X_i \\cap X_j \\neq \\emptyset,$ $X_j \\setminus X_i \\neq\n\\emptyset,$ and $X_i \\setminus X_j \\neq \\emptyset.$ Two sets $X,Y\\in {\\cal F}$\nare in the same overlap class if there is a series $X=X_1,X_2, ..., X_k=Y$ of\nsets of ${\\cal F}$ in which each $X_iX_{i+1}$ overlaps. In this note, we focus\non efficiently identifying all overlap classes in $O(n+\\sum_{i=1}^m |X_i|)$\ntime. We thus revisit the clever algorithm of Dahlhaus of which we give a clear\npresentation and that we simplify to make it practical and implementable in its\nreal worst case complexity. An useful variant of Dahlhaus's approach is also\nexplained."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.4825v1", 
    "title": "Approximation Algorithms for Orienteering with Time Windows", 
    "arxiv-id": "0711.4825v1", 
    "author": "Nitish Korula", 
    "publish": "2007-11-29T21:10:48Z", 
    "summary": "Orienteering is the following optimization problem: given an edge-weighted\ngraph (directed or undirected), two nodes s,t and a time limit T, find an s-t\nwalk of total length at most T that maximizes the number of distinct nodes\nvisited by the walk. One obtains a generalization, namely orienteering with\ntime-windows (also referred to as TSP with time-windows), if each node v has a\nspecified time-window [R(v), D(v)] and a node v is counted as visited by the\nwalk only if v is visited during its time-window. For the time-window problem,\nan O(\\log \\opt) approximation can be achieved even for directed graphs if the\nalgorithm is allowed quasi-polynomial time. However, the best known polynomial\ntime approximation ratios are O(\\log^2 \\opt) for undirected graphs and O(\\log^4\n\\opt) in directed graphs. In this paper we make some progress towards closing\nthis discrepancy, and in the process obtain improved approximation ratios in\nseveral natural settings. Let L(v) = D(v) - R(v) denote the length of the\ntime-window for v and let \\lmax = \\max_v L(v) and \\lmin = \\min_v L(v). Our\nresults are given below with \\alpha denoting the known approximation ratio for\norienteering (without time-windows). Currently \\alpha = (2+\\eps) for undirected\ngraphs and \\alpha = O(\\log^2 \\opt) in directed graphs.\n  1. An O(\\alpha \\log \\lmax) approximation when R(v) and D(v) are integer\nvalued for each v.\n  2. An O(\\alpha \\max{\\log \\opt, \\log \\frac{\\lmax}{\\lmin}}) approximation.\n  3. An O(\\alpha \\log \\frac{\\lmax}{\\lmin}) approximation when no start and end\npoints are specified.\n  In particular, if \\frac{\\lmax}{\\lmin} is poly-bounded, we obtain an O(\\log n)\napproximation for the time-window problem in undirected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.2629v2", 
    "title": "Approximation Algorithms for the Highway Problem under the Coupon Model", 
    "arxiv-id": "0712.2629v2", 
    "author": "Kouhei Tomita", 
    "publish": "2007-12-17T04:47:38Z", 
    "summary": "When a store sells items to customers, the store wishes to determine the\nprices of the items to maximize its profit. Intuitively, if the store sells the\nitems with low (resp. high) prices, the customers buy more (resp. less) items,\nwhich provides less profit to the store. So it would be hard for the store to\ndecide the prices of items. Assume that the store has a set V of n items and\nthere is a set E of m customers who wish to buy those items, and also assume\nthat each item i \\in V has the production cost d_i and each customer e_j \\in E\nhas the valuation v_j on the bundle e_j \\subseteq V of items. When the store\nsells an item i \\in V at the price r_i, the profit for the item i is\np_i=r_i-d_i. The goal of the store is to decide the price of each item to\nmaximize its total profit. In most of the previous works, the item pricing\nproblem was considered under the assumption that p_i \\geq 0 for each i \\in V,\nhowever, Balcan, et al. [In Proc. of WINE, LNCS 4858, 2007] introduced the\nnotion of loss-leader, and showed that the seller can get more total profit in\nthe case that p_i < 0 is allowed than in the case that p_i < 0 is not allowed.\nIn this paper, we consider the line and the cycle highway problem, and show\napproximation algorithms for the line and/or cycle highway problem for which\nthe smallest valuation is s and the largest valuation is \\ell or all valuations\nare identical."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3360v1", 
    "title": "Compressed Text Indexes:From Theory to Practice!", 
    "arxiv-id": "0712.3360v1", 
    "author": "Rossano Venturini", 
    "publish": "2007-12-20T10:42:54Z", 
    "summary": "A compressed full-text self-index represents a text in a compressed form and\nstill answers queries efficiently. This technology represents a breakthrough\nover the text indexing techniques of the previous decade, whose indexes\nrequired several times the size of the text. Although it is relatively new,\nthis technology has matured up to a point where theoretical research is giving\nway to practical developments. Nonetheless this requires significant\nprogramming skills, a deep engineering effort, and a strong algorithmic\nbackground to dig into the research results. To date only isolated\nimplementations and focused comparisons of compressed indexes have been\nreported, and they missed a common API, which prevented their re-use or\ndeployment within other applications.\n  The goal of this paper is to fill this gap. First, we present the existing\nimplementations of compressed indexes from a practitioner's point of view.\nSecond, we introduce the Pizza&Chili site, which offers tuned implementations\nand a standardized API for the most successful compressed full-text\nself-indexes, together with effective testbeds and scripts for their automatic\nvalidation and test. Third, we show the results of our extensive experiments on\nthese codes with the aim of demonstrating the practical relevance of this novel\nand exciting technology."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3568v1", 
    "title": "A Partition-Based Relaxation For Steiner Trees", 
    "arxiv-id": "0712.3568v1", 
    "author": "Kunlun Tan", 
    "publish": "2007-12-20T21:06:35Z", 
    "summary": "The Steiner tree problem is a classical NP-hard optimization problem with a\nwide range of practical applications. In an instance of this problem, we are\ngiven an undirected graph G=(V,E), a set of terminals R, and non-negative costs\nc_e for all edges e in E. Any tree that contains all terminals is called a\nSteiner tree; the goal is to find a minimum-cost Steiner tree. The nodes V R\nare called Steiner nodes.\n  The best approximation algorithm known for the Steiner tree problem is due to\nRobins and Zelikovsky (SIAM J. Discrete Math, 2005); their greedy algorithm\nachieves a performance guarantee of 1+(ln 3)/2 ~ 1.55. The best known linear\n(LP)-based algorithm, on the other hand, is due to Goemans and Bertsimas (Math.\nProgramming, 1993) and achieves an approximation ratio of 2-2/|R|. In this\npaper we establish a link between greedy and LP-based approaches by showing\nthat Robins and Zelikovsky's algorithm has a natural primal-dual interpretation\nwith respect to a novel partition-based linear programming relaxation. We also\nexhibit surprising connections between the new formulation and existing LPs and\nwe show that the new LP is stronger than the bidirected cut formulation.\n  An instance is b-quasi-bipartite if each connected component of G R has at\nmost b vertices. We show that Robins' and Zelikovsky's algorithm has an\napproximation ratio better than 1+(ln 3)/2 for such instances, and we prove\nthat the integrality gap of our LP is between 8/7 and (2b+1)/(b+1)."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3858v1", 
    "title": "Bottleneck flows in networks", 
    "arxiv-id": "0712.3858v1", 
    "author": "Ruonan Zhang", 
    "publish": "2007-12-22T13:49:45Z", 
    "summary": "The bottleneck network flow problem (BNFP) is a generalization of several\nwell-studied bottleneck problems such as the bottleneck transportation problem\n(BTP), bottleneck assignment problem (BAP), bottleneck path problem (BPP), and\nso on. In this paper we provide a review of important results on this topic and\nits various special cases. We observe that the BNFP can be solved as a sequence\nof $O(\\log n)$ maximum flow problems. However, special augmenting path based\nalgorithms for the maximum flow problem can be modified to obtain algorithms\nfor the BNFP with the property that these variations and the corresponding\nmaximum flow algorithms have identical worst case time complexity. On unit\ncapacity network we show that BNFP can be solved in $O(\\min \\{{m(n\\log\nn)}^{{2/3}}, m^{{3/2}}\\sqrt{\\log n}\\})$. This improves the best available\nalgorithm by a factor of $\\sqrt{\\log n}$. On unit capacity simple graphs, we\nshow that BNFP can be solved in $O(m \\sqrt {n \\log n})$ time. As a consequence\nwe have an $O(m \\sqrt {n \\log n})$ algorithm for the BTP with unit arc\ncapacities."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3876v5", 
    "title": "Explicit Non-Adaptive Combinatorial Group Testing Schemes", 
    "arxiv-id": "0712.3876v5", 
    "author": "Amir Rothschild", 
    "publish": "2007-12-22T21:04:34Z", 
    "summary": "Group testing is a long studied problem in combinatorics: A small set of $r$\nill people should be identified out of the whole ($n$ people) by using only\nqueries (tests) of the form \"Does set X contain an ill human?\". In this paper\nwe provide an explicit construction of a testing scheme which is better\n(smaller) than any known explicit construction. This scheme has $\\bigT{\\min[r^2\n\\ln n,n]}$ tests which is as many as the best non-explicit schemes have. In our\nconstruction we use a fact that may have a value by its own right: Linear\nerror-correction codes with parameters $[m,k,\\delta m]_q$ meeting the\nGilbert-Varshamov bound may be constructed quite efficiently, in $\\bigT{q^km}$\ntime."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0801.1987v2", 
    "title": "A Nearly Linear-Time PTAS for Explicit Fractional Packing and Covering   Linear Programs", 
    "arxiv-id": "0801.1987v2", 
    "author": "Neal E. Young", 
    "publish": "2008-01-13T22:04:49Z", 
    "summary": "We give an approximation algorithm for packing and covering linear programs\n(linear programs with non-negative coefficients). Given a constraint matrix\nwith n non-zeros, r rows, and c columns, the algorithm computes feasible primal\nand dual solutions whose costs are within a factor of 1+eps of the optimal cost\nin time O((r+c)log(n)/eps^2 + n)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0801.4238v1", 
    "title": "Algorithms for Temperature-Aware Task Scheduling in Microprocessor   Systems", 
    "arxiv-id": "0801.4238v1", 
    "author": "Julien Robert", 
    "publish": "2008-01-28T10:47:42Z", 
    "summary": "We study scheduling problems motivated by recently developed techniques for\nmicroprocessor thermal management at the operating systems level. The general\nscenario can be described as follows. The microprocessor's temperature is\ncontrolled by the hardware thermal management system that continuously monitors\nthe chip temperature and automatically reduces the processor's speed as soon as\nthe thermal threshold is exceeded. Some tasks are more CPU-intensive than other\nand thus generate more heat during execution. The cooling system operates\nnon-stop, reducing (at an exponential rate) the deviation of the processor's\ntemperature from the ambient temperature. As a result, the processor's\ntemperature, and thus the performance as well, depends on the order of the task\nexecution. Given a variety of possible underlying architectures, models for\ncooling and for hardware thermal management, as well as types of tasks, this\nscenario gives rise to a plethora of interesting and never studied scheduling\nproblems.\n  We focus on scheduling real-time jobs in a simplified model for cooling and\nthermal management. A collection of unit-length jobs is given, each job\nspecified by its release time, deadline and heat contribution. If, at some time\nstep, the temperature of the system is t and the processor executes a job with\nheat contribution h, then the temperature at the next step is (t+h)/2. The\ntemperature cannot exceed the given thermal threshold T. The objective is to\nmaximize the throughput, that is, the number of tasks that meet their\ndeadlines. We prove that, in the offline case, computing the optimum schedule\nis NP-hard, even if all jobs are released at the same time. In the online case,\nwe show a 2-competitive deterministic algorithm and a matching lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.0017v1", 
    "title": "Improved Deterministic Length Reduction", 
    "arxiv-id": "0802.0017v1", 
    "author": "Amir Rothschild", 
    "publish": "2008-01-31T21:59:33Z", 
    "summary": "This paper presents a new technique for deterministic length reduction. This\ntechnique improves the running time of the algorithm presented in \\cite{LR07}\nfor performing fast convolution in sparse data. While the regular fast\nconvolution of vectors $V_1,V_2$ whose sizes are $N_1,N_2$ respectively, takes\n$O(N_1 \\log N_2)$ using FFT, using the new technique for length reduction, the\nalgorithm proposed in \\cite{LR07} performs the convolution in $O(n_1 \\log^3\nn_1)$, where $n_1$ is the number of non-zero values in $V_1$. The algorithm\nassumes that $V_1$ is given in advance, and $V_2$ is given in running time. The\nnovel technique presented in this paper improves the convolution time to $O(n_1\n\\log^2 n_1)$ {\\sl deterministically}, which equals the best running time given\nachieved by a {\\sl randomized} algorithm.\n  The preprocessing time of the new technique remains the same as the\npreprocessing time of \\cite{LR07}, which is $O(n_1^2)$. This assumes and deals\nthe case where $N_1$ is polynomial in $n_1$. In the case where $N_1$ is\nexponential in $n_1$, a reduction to a polynomial case can be used. In this\npaper we also improve the preprocessing time of this reduction from $O(n_1^4)$\nto $O(n_1^3{\\rm polylog}(n_1))$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1059v1", 
    "title": "Average-Case Analysis of Online Topological Ordering", 
    "arxiv-id": "0802.1059v1", 
    "author": "Tobias Friedrich", 
    "publish": "2008-02-07T20:27:17Z", 
    "summary": "Many applications like pointer analysis and incremental compilation require\nmaintaining a topological ordering of the nodes of a directed acyclic graph\n(DAG) under dynamic updates. All known algorithms for this problem are either\nonly analyzed for worst-case insertion sequences or only evaluated\nexperimentally on random DAGs. We present the first average-case analysis of\nonline topological ordering algorithms. We prove an expected runtime of O(n^2\npolylog(n)) under insertion of the edges of a complete DAG in a random order\nfor the algorithms of Alpern et al. (SODA, 1990), Katriel and Bodlaender (TALG,\n2006), and Pearce and Kelly (JEA, 2006). This is much less than the best known\nworst-case bound O(n^{2.75}) for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1427v1", 
    "title": "Approximating General Metric Distances Between a Pattern and a Text", 
    "arxiv-id": "0802.1427v1", 
    "author": "Ely Porat", 
    "publish": "2008-02-11T12:36:31Z", 
    "summary": "Let $T=t_0 ... t_{n-1}$ be a text and $P = p_0 ... p_{m-1}$ a pattern taken\nfrom some finite alphabet set $\\Sigma$, and let $\\dist$ be a metric on\n$\\Sigma$. We consider the problem of calculating the sum of distances between\nthe symbols of $P$ and the symbols of substrings of $T$ of length $m$ for all\npossible offsets. We present an $\\epsilon$-approximation algorithm for this\nproblem which runs in time $O(\\frac{1}{\\epsilon^2}n\\cdot\n\\mathrm{polylog}(n,\\abs{\\Sigma}))$"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1471v2", 
    "title": "Error-Correcting Data Structures", 
    "arxiv-id": "0802.1471v2", 
    "author": "Ronald de Wolf", 
    "publish": "2008-02-11T16:35:49Z", 
    "summary": "We study data structures in the presence of adversarial noise. We want to\nencode a given object in a succinct data structure that enables us to\nefficiently answer specific queries about the object, even if the data\nstructure has been corrupted by a constant fraction of errors. This new model\nis the common generalization of (static) data structures and locally decodable\nerror-correcting codes. The main issue is the tradeoff between the space used\nby the data structure and the time (number of probes) needed to answer a query\nabout the encoded object. We prove a number of upper and lower bounds on\nvarious natural error-correcting data structure problems. In particular, we\nshow that the optimal length of error-correcting data structures for the\nMembership problem (where we want to store subsets of size s from a universe of\nsize n) is closely related to the optimal length of locally decodable codes for\ns-bit strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1685v2", 
    "title": "Generalized Whac-a-Mole", 
    "arxiv-id": "0802.1685v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2008-02-12T18:41:46Z", 
    "summary": "We consider online competitive algorithms for the problem of collecting\nweighted items from a dynamic set S, when items are added to or deleted from S\nover time. The objective is to maximize the total weight of collected items. We\nstudy the general version, as well as variants with various restrictions,\nincluding the following: the uniform case, when all items have the same weight,\nthe decremental sets, when all items are present at the beginning and only\ndeletion operations are allowed, and dynamic queues, where the dynamic set is\nordered and only its prefixes can be deleted (with no restriction on\ninsertions). The dynamic queue case is a generalization of bounded-delay packet\nscheduling (also referred to as buffer management). We present several upper\nand lower bounds on the competitive ratio for these variants."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1722v1", 
    "title": "Parameterized Algorithms for Partial Cover Problems", 
    "arxiv-id": "0802.1722v1", 
    "author": "Saket Saurabh", 
    "publish": "2008-02-12T21:19:40Z", 
    "summary": "Covering problems are fundamental classical problems in optimization,\ncomputer science and complexity theory. Typically an input to these problems is\na family of sets over a finite universe and the goal is to cover the elements\nof the universe with as few sets of the family as possible.\n  The variations of covering problems include well known problems like Set\nCover, Vertex Cover, Dominating Set and Facility Location to name a few.\nRecently there has been a lot of study on partial covering problems, a natural\ngeneralization of covering problems. Here, the goal is not to cover all the\nelements but to cover the specified number of elements with the minimum number\nof sets.\n  In this paper we study partial covering problems in graphs in the realm of\nparameterized complexity. Classical (non-partial) version of all these problems\nhave been intensively studied in planar graphs and in graphs excluding a fixed\ngraph $H$ as a minor. However, the techniques developed for parameterized\nversion of non-partial covering problems cannot be applied directly to their\npartial counterparts. The approach we use, to show that various partial\ncovering problems are fixed parameter tractable on planar graphs, graphs of\nbounded local treewidth and graph excluding some graph as a minor, is quite\ndifferent from previously known techniques. The main idea behind our approach\nis the concept of implicit branching. We find implicit branching technique to\nbe interesting on its own and believe that it can be used for some other\nproblems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2184v1", 
    "title": "Set Covering Problems with General Objective Functions", 
    "arxiv-id": "0802.2184v1", 
    "author": "Christophe Dumeunier", 
    "publish": "2008-02-15T11:56:28Z", 
    "summary": "We introduce a parameterized version of set cover that generalizes several\npreviously studied problems. Given a ground set V and a collection of subsets\nS_i of V, a feasible solution is a partition of V such that each subset of the\npartition is included in one of the S_i. The problem involves maximizing the\nmean subset size of the partition, where the mean is the generalized mean of\nparameter p, taken over the elements. For p=-1, the problem is equivalent to\nthe classical minimum set cover problem. For p=0, it is equivalent to the\nminimum entropy set cover problem, introduced by Halperin and Karp. For p=1,\nthe problem includes the maximum-edge clique partition problem as a special\ncase. We prove that the greedy algorithm simultaneously approximates the\nproblem within a factor of (p+1)^1/p for any p in R^+, and that this is the\nbest possible unless P=NP. These results both generalize and simplify previous\nresults for special cases. We also consider the corresponding graph coloring\nproblem, and prove several tractability and inapproximability results. Finally,\nwe consider a further generalization of the set cover problem in which we aim\nat minimizing the sum of some concave function of the part sizes. As an\napplication, we derive an approximation ratio for a Rent-or-Buy set cover\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2528v1", 
    "title": "Min-Cost 2-Connected Subgraphs With k Terminals", 
    "arxiv-id": "0802.2528v1", 
    "author": "Nitish Korula", 
    "publish": "2008-02-18T18:34:28Z", 
    "summary": "In the k-2VC problem, we are given an undirected graph G with edge costs and\nan integer k; the goal is to find a minimum-cost 2-vertex-connected subgraph of\nG containing at least k vertices. A slightly more general version is obtained\nif the input also specifies a subset S \\subseteq V of terminals and the goal is\nto find a subgraph containing at least k terminals. Closely related to the\nk-2VC problem, and in fact a special case of it, is the k-2EC problem, in which\nthe goal is to find a minimum-cost 2-edge-connected subgraph containing k\nvertices. The k-2EC problem was introduced by Lau et al., who also gave a\npoly-logarithmic approximation for it. No previous approximation algorithm was\nknown for the more general k-2VC problem. We describe an O(\\log n \\log k)\napproximation for the k-2VC problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2827v1", 
    "title": "Design by Measure and Conquer, A Faster Exact Algorithm for Dominating   Set", 
    "arxiv-id": "0802.2827v1", 
    "author": "Hans L. Bodlaender", 
    "publish": "2008-02-20T14:05:58Z", 
    "summary": "The measure and conquer approach has proven to be a powerful tool to analyse\nexact algorithms for combinatorial problems, like Dominating Set and\nIndependent Set. In this paper, we propose to use measure and conquer also as a\ntool in the design of algorithms. In an iterative process, we can obtain a\nseries of branch and reduce algorithms. A mathematical analysis of an algorithm\nin the series with measure and conquer results in a quasiconvex programming\nproblem. The solution by computer to this problem not only gives a bound on the\nrunning time, but also can give a new reduction rule, thus giving a new,\npossibly faster algorithm. This makes design by measure and conquer a form of\ncomputer aided algorithm design. When we apply the methodology to a Set Cover\nmodelling of the Dominating Set problem, we obtain the currently fastest known\nexact algorithms for Dominating Set: an algorithm that uses $O(1.5134^n)$ time\nand polynomial space, and an algorithm that uses $O(1.5063^n)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2832v1", 
    "title": "Rent, Lease or Buy: Randomized Algorithms for Multislope Ski Rental", 
    "arxiv-id": "0802.2832v1", 
    "author": "Dror Rawitz", 
    "publish": "2008-02-20T14:13:19Z", 
    "summary": "In the Multislope Ski Rental problem, the user needs a certain resource for\nsome unknown period of time. To use the resource, the user must subscribe to\none of several options, each of which consists of a one-time setup cost\n(``buying price''), and cost proportional to the duration of the usage\n(``rental rate''). The larger the price, the smaller the rent. The actual usage\ntime is determined by an adversary, and the goal of an algorithm is to minimize\nthe cost by choosing the best option at any point in time. Multislope Ski\nRental is a natural generalization of the classical Ski Rental problem (where\nthe only options are pure rent and pure buy), which is one of the fundamental\nproblems of online computation. The Multislope Ski Rental problem is an\nabstraction of many problems where online decisions cannot be modeled by just\ntwo options, e.g., power management in systems which can be shut down in parts.\nIn this paper we study randomized algorithms for Multislope Ski Rental. Our\nresults include the best possible online randomized strategy for any additive\ninstance, where the cost of switching from one option to another is the\ndifference in their buying prices; and an algorithm that produces an\n$e$-competitive randomized strategy for any (non-additive) instance."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2847v1", 
    "title": "On Dynamic Breadth-First Search in External-Memory", 
    "arxiv-id": "0802.2847v1", 
    "author": "Ulrich Meyer", 
    "publish": "2008-02-20T14:21:21Z", 
    "summary": "We provide the first non-trivial result on dynamic breadth-first search (BFS)\nin external-memory: For general sparse undirected graphs of initially $n$ nodes\nand O(n) edges and monotone update sequences of either $\\Theta(n)$ edge\ninsertions or $\\Theta(n)$ edge deletions, we prove an amortized\nhigh-probability bound of $O(n/B^{2/3}+\\sort(n)\\cdot \\log B)$ I/Os per update.\nIn contrast, the currently best approach for static BFS on sparse undirected\ngraphs requires $\\Omega(n/B^{1/2}+\\sort(n))$ I/Os."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2851v1", 
    "title": "An Improved Randomized Truthful Mechanism for Scheduling Unrelated   Machines", 
    "arxiv-id": "0802.2851v1", 
    "author": "Changyuan Yu", 
    "publish": "2008-02-20T14:22:30Z", 
    "summary": "We study the scheduling problem on unrelated machines in the mechanism design\nsetting. This problem was proposed and studied in the seminal paper (Nisan and\nRonen 1999), where they gave a 1.75-approximation randomized truthful mechanism\nfor the case of two machines. We improve this result by a 1.6737-approximation\nrandomized truthful mechanism. We also generalize our result to a\n$0.8368m$-approximation mechanism for task scheduling with $m$ machines, which\nimprove the previous best upper bound of $0.875m(Mu'alem and Schapira 2007)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2852v1", 
    "title": "Tight Bounds for Blind Search on the Integers", 
    "arxiv-id": "0802.2852v1", 
    "author": "Philipp Woelfel", 
    "publish": "2008-02-20T14:22:33Z", 
    "summary": "We analyze a simple random process in which a token is moved in the interval\n$A=\\{0,...,n\\$: Fix a probability distribution $\\mu$ over $\\{1,...,n\\$.\nInitially, the token is placed in a random position in $A$. In round $t$, a\nrandom value $d$ is chosen according to $\\mu$. If the token is in position\n$a\\geq d$, then it is moved to position $a-d$. Otherwise it stays put. Let $T$\nbe the number of rounds until the token reaches position 0. We show tight\nbounds for the expectation of $T$ for the optimal distribution $\\mu$. More\nprecisely, we show that $\\min_\\mu\\{E_\\mu(T)\\=\\Theta((\\log n)^2)$. For the\nproof, a novel potential function argument is introduced. The research is\nmotivated by the problem of approximating the minimum of a continuous function\nover $[0,1]$ with a ``blind'' optimization strategy."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2855v1", 
    "title": "Computing Minimum Spanning Trees with Uncertainty", 
    "arxiv-id": "0802.2855v1", 
    "author": "Rajeev Raman", 
    "publish": "2008-02-20T14:24:10Z", 
    "summary": "We consider the minimum spanning tree problem in a setting where information\nabout the edge weights of the given graph is uncertain. Initially, for each\nedge $e$ of the graph only a set $A_e$, called an uncertainty area, that\ncontains the actual edge weight $w_e$ is known. The algorithm can `update' $e$\nto obtain the edge weight $w_e \\in A_e$. The task is to output the edge set of\na minimum spanning tree after a minimum number of updates. An algorithm is\n$k$-update competitive if it makes at most $k$ times as many updates as the\noptimum. We present a 2-update competitive algorithm if all areas $A_e$ are\nopen or trivial, which is the best possible among deterministic algorithms. The\ncondition on the areas $A_e$ is to exclude degenerate inputs for which no\nconstant update competitive algorithm can exist. Next, we consider a setting\nwhere the vertices of the graph correspond to points in Euclidean space and the\nweight of an edge is equal to the distance of its endpoints. The location of\neach point is initially given as an uncertainty area, and an update reveals the\nexact location of the point. We give a general relation between the edge\nuncertainty and the vertex uncertainty versions of a problem and use it to\nderive a 4-update competitive algorithm for the minimum spanning tree problem\nin the vertex uncertainty model. Again, we show that this is best possible\namong deterministic algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2864v1", 
    "title": "On Geometric Spanners of Euclidean and Unit Disk Graphs", 
    "arxiv-id": "0802.2864v1", 
    "author": "Ljubomir Perkovic", 
    "publish": "2008-02-20T14:36:52Z", 
    "summary": "We consider the problem of constructing bounded-degree planar geometric\nspanners of Euclidean and unit-disk graphs. It is well known that the Delaunay\nsubgraph is a planar geometric spanner with stretch factor $C_{del\\approx\n2.42$; however, its degree may not be bounded. Our first result is a very\nsimple linear time algorithm for constructing a subgraph of the Delaunay graph\nwith stretch factor $\\rho =1+2\\pi(k\\cos{\\frac{\\pi{k)^{-1$ and degree bounded by\n$k$, for any integer parameter $k\\geq 14$. This result immediately implies an\nalgorithm for constructing a planar geometric spanner of a Euclidean graph with\nstretch factor $\\rho \\cdot C_{del$ and degree bounded by $k$, for any integer\nparameter $k\\geq 14$. Moreover, the resulting spanner contains a Euclidean\nMinimum Spanning Tree (EMST) as a subgraph. Our second contribution lies in\ndeveloping the structural results necessary to transfer our analysis and\nalgorithm from Euclidean graphs to unit disk graphs, the usual model for\nwireless ad-hoc networks. We obtain a very simple distributed, {\\em\nstrictly-localized algorithm that, given a unit disk graph embedded in the\nplane, constructs a geometric spanner with the above stretch factor and degree\nbound, and also containing an EMST as a subgraph. The obtained results\ndramatically improve the previous results in all aspects, as shown in the\npaper."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2867v1", 
    "title": "Fixed Parameter Polynomial Time Algorithms for Maximum Agreement and   Compatible Supertrees", 
    "arxiv-id": "0802.2867v1", 
    "author": "Wing-Kin Sung", 
    "publish": "2008-02-20T14:38:47Z", 
    "summary": "Consider a set of labels $L$ and a set of trees ${\\mathcal T} = \\{{\\mathcal\nT}^{(1), {\\mathcal T}^{(2), ..., {\\mathcal T}^{(k) \\$ where each tree\n${\\mathcal T}^{(i)$ is distinctly leaf-labeled by some subset of $L$. One\nfundamental problem is to find the biggest tree (denoted as supertree) to\nrepresent $\\mathcal T}$ which minimizes the disagreements with the trees in\n${\\mathcal T}$ under certain criteria. This problem finds applications in\nphylogenetics, database, and data mining. In this paper, we focus on two\nparticular supertree problems, namely, the maximum agreement supertree problem\n(MASP) and the maximum compatible supertree problem (MCSP). These two problems\nare known to be NP-hard for $k \\geq 3$. This paper gives the first polynomial\ntime algorithms for both MASP and MCSP when both $k$ and the maximum degree $D$\nof the trees are constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.0248v1", 
    "title": "Networks become navigable as nodes move and forget", 
    "arxiv-id": "0803.0248v1", 
    "author": "Emmanuelle Lebhar", 
    "publish": "2008-03-03T14:44:08Z", 
    "summary": "We propose a dynamical process for network evolution, aiming at explaining\nthe emergence of the small world phenomenon, i.e., the statistical observation\nthat any pair of individuals are linked by a short chain of acquaintances\ncomputable by a simple decentralized routing algorithm, known as greedy\nrouting. Previously proposed dynamical processes enabled to demonstrate\nexperimentally (by simulations) that the small world phenomenon can emerge from\nlocal dynamics. However, the analysis of greedy routing using the probability\ndistributions arising from these dynamics is quite complex because of mutual\ndependencies. In contrast, our process enables complete formal analysis. It is\nbased on the combination of two simple processes: a random walk process, and an\nharmonic forgetting process. Both processes reflect natural behaviors of the\nindividuals, viewed as nodes in the network of inter-individual acquaintances.\nWe prove that, in k-dimensional lattices, the combination of these two\nprocesses generates long-range links mutually independently distributed as a\nk-harmonic distribution. We analyze the performances of greedy routing at the\nstationary regime of our process, and prove that the expected number of steps\nfor routing from any source to any target in any multidimensional lattice is a\npolylogarithmic function of the distance between the two nodes in the lattice.\nUp to our knowledge, these results are the first formal proof that navigability\nin small worlds can emerge from a dynamical process for network evolution. Our\ndynamical process can find practical applications to the design of spatial\ngossip and resource location protocols."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.0473v2", 
    "title": "Stream sampling for variance-optimal estimation of subset sums", 
    "arxiv-id": "0803.0473v2", 
    "author": "Mikkel Thorup", 
    "publish": "2008-03-04T15:12:24Z", 
    "summary": "From a high volume stream of weighted items, we want to maintain a generic\nsample of a certain limited size $k$ that we can later use to estimate the\ntotal weight of arbitrary subsets. This is the classic context of on-line\nreservoir sampling, thinking of the generic sample as a reservoir. We present\nan efficient reservoir sampling scheme, $\\varoptk$, that dominates all previous\nschemes in terms of estimation quality.\n  $\\varoptk$ provides {\\em variance optimal unbiased estimation of subset\nsums}. More precisely, if we have seen $n$ items of the stream, then for {\\em\nany} subset size $m$, our scheme based on $k$ samples minimizes the average\nvariance over all subsets of size $m$. In fact, the optimality is against any\noff-line scheme with $k$ samples tailored for the concrete set of items seen.\nIn addition to optimal average variance, our scheme provides tighter worst-case\nbounds on the variance of {\\em particular} subsets than previously possible. It\nis efficient, handling each new item of the stream in $O(\\log k)$ time.\nFinally, it is particularly well suited for combination of samples from\ndifferent streams in a distributed setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.0792v1", 
    "title": "Incremental Topological Ordering and Strong Component Maintenance", 
    "arxiv-id": "0803.0792v1", 
    "author": "Robert E. Tarjan", 
    "publish": "2008-03-06T05:11:18Z", 
    "summary": "We present an on-line algorithm for maintaining a topological order of a\ndirected acyclic graph as arcs are added, and detecting a cycle when one is\ncreated. Our algorithm takes O(m^{1/2}) amortized time per arc, where m is the\ntotal number of arcs. For sparse graphs, this bound improves the best previous\nbound by a logarithmic factor and is tight to within a constant factor for a\nnatural class of algorithms that includes all the existing ones. Our main\ninsight is that the bidirectional search method of previous algorithms does not\nrequire an ordered search, but can be more general. This allows us to avoid the\nuse of heaps (priority queues) entirely. Instead, the deterministic version of\nour algorithm uses (approximate) median-finding. The randomized version of our\nalgorithm avoids this complication, making it very simple. We extend our\ntopological ordering algorithm to give the first detailed algorithm for\nmaintaining the strong components of a directed graph, and a topological order\nof these components, as arcs are added. This extension also has an amortized\ntime bound of O(m^{1/2}) per arc."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.0929v4", 
    "title": "Graph Sparsification by Effective Resistances", 
    "arxiv-id": "0803.0929v4", 
    "author": "Nikhil Srivastava", 
    "publish": "2008-03-06T18:03:06Z", 
    "summary": "We present a nearly-linear time algorithm that produces high-quality\nsparsifiers of weighted graphs. Given as input a weighted graph $G=(V,E,w)$ and\na parameter $\\epsilon>0$, we produce a weighted subgraph\n$H=(V,\\tilde{E},\\tilde{w})$ of $G$ such that $|\\tilde{E}|=O(n\\log\nn/\\epsilon^2)$ and for all vectors $x\\in\\R^V$ $(1-\\epsilon)\\sum_{uv\\in\nE}(x(u)-x(v))^2w_{uv}\\le \\sum_{uv\\in\\tilde{E}}(x(u)-x(v))^2\\tilde{w}_{uv} \\le\n(1+\\epsilon)\\sum_{uv\\in E}(x(u)-x(v))^2w_{uv}. (*)$\n  This improves upon the sparsifiers constructed by Spielman and Teng, which\nhad $O(n\\log^c n)$ edges for some large constant $c$, and upon those of\nBencz\\'ur and Karger, which only satisfied (*) for $x\\in\\{0,1\\}^V$.\n  A key ingredient in our algorithm is a subroutine of independent interest: a\nnearly-linear time algorithm that builds a data structure from which we can\nquery the approximate effective resistance between any two vertices in a graph\nin $O(\\log n)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.1321v2", 
    "title": "Treewidth computation and extremal combinatorics", 
    "arxiv-id": "0803.1321v2", 
    "author": "Yngve Villanger", 
    "publish": "2008-03-09T20:54:58Z", 
    "summary": "For a given graph G and integers b,f >= 0, let S be a subset of vertices of G\nof size b+1 such that the subgraph of G induced by S is connected and S can be\nseparated from other vertices of G by removing f vertices. We prove that every\ngraph on n vertices contains at most n\\binom{b+f}{b} such vertex subsets. This\nresult from extremal combinatorics appears to be very useful in the design of\nseveral enumeration and exact algorithms. In particular, we use it to provide\nalgorithms that for a given n-vertex graph G - compute the treewidth of G in\ntime O(1.7549^n) by making use of exponential space and in time O(2.6151^n) and\npolynomial space; - decide in time O(({\\frac{2n+k+1}{3})^{k+1}\\cdot kn^6}) if\nthe treewidth of G is at most k; - list all minimal separators of G in time\nO(1.6181^n) and all potential maximal cliques of G in time O(1.7549^n). This\nsignificantly improves previous algorithms for these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.2615v1", 
    "title": "Rapport de recherche sur le probl\u00e8me du plus court chemin contraint", 
    "arxiv-id": "0803.2615v1", 
    "author": "Anass Nagih", 
    "publish": "2008-03-18T12:37:36Z", 
    "summary": "This article provides an overview of the performance and the theoretical\ncomplexity of approximate and exact methods for various versions of the\nshortest path problem. The proposed study aims to improve the resolution of a\nmore general covering problem within a column generation scheme in which the\nshortest path problem is the sub-problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0803.2842v1", 
    "title": "Admission Control to Minimize Rejections and Online Set Cover with   Repetitions", 
    "arxiv-id": "0803.2842v1", 
    "author": "Shai Gutner", 
    "publish": "2008-03-19T16:53:42Z", 
    "summary": "We study the admission control problem in general networks. Communication\nrequests arrive over time, and the online algorithm accepts or rejects each\nrequest while maintaining the capacity limitations of the network. The\nadmission control problem has been usually analyzed as a benefit problem, where\nthe goal is to devise an online algorithm that accepts the maximum number of\nrequests possible. The problem with this objective function is that even\nalgorithms with optimal competitive ratios may reject almost all of the\nrequests, when it would have been possible to reject only a few. This could be\ninappropriate for settings in which rejections are intended to be rare events.\n  In this paper, we consider preemptive online algorithms whose goal is to\nminimize the number of rejected requests. Each request arrives together with\nthe path it should be routed on. We show an $O(\\log^2 (mc))$-competitive\nrandomized algorithm for the weighted case, where $m$ is the number of edges in\nthe graph and $c$ is the maximum edge capacity. For the unweighted case, we\ngive an $O(\\log m \\log c)$-competitive randomized algorithm. This settles an\nopen question of Blum, Kalai and Kleinberg raised in \\cite{BlKaKl01}. We note\nthat allowing preemption and handling requests with given paths are essential\nfor avoiding trivial lower bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0804.0149v2", 
    "title": "From Random Graph to Small World by Wandering", 
    "arxiv-id": "0804.0149v2", 
    "author": "Fabien Mathieu", 
    "publish": "2008-04-01T11:59:43Z", 
    "summary": "Numerous studies show that most known real-world complex networks share\nsimilar properties in their connectivity and degree distribution. They are\ncalled small worlds. This article gives a method to turn random graphs into\nSmall World graphs by the dint of random walks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0804.0277v1", 
    "title": "Mapping Semantic Networks to Undirected Networks", 
    "arxiv-id": "0804.0277v1", 
    "author": "Marko A. Rodriguez", 
    "publish": "2008-04-02T01:19:55Z", 
    "summary": "There exists an injective, information-preserving function that maps a\nsemantic network (i.e a directed labeled network) to a directed network (i.e. a\ndirected unlabeled network). The edge label in the semantic network is\nrepresented as a topological feature of the directed network. Also, there\nexists an injective function that maps a directed network to an undirected\nnetwork (i.e. an undirected unlabeled network). The edge directionality in the\ndirected network is represented as a topological feature of the undirected\nnetwork. Through function composition, there exists an injective function that\nmaps a semantic network to an undirected network. Thus, aside from space\nconstraints, the semantic network construct does not have any modeling\nfunctionality that is not possible with either a directed or undirected network\nrepresentation. Two proofs of this idea will be presented. The first is a proof\nof the aforementioned function composition concept. The second is a simpler\nproof involving an undirected binary encoding of a semantic network."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0722v3", 
    "title": "A Memetic Algorithm for the Generalized Traveling Salesman Problem", 
    "arxiv-id": "0804.0722v3", 
    "author": "Daniel Karapetyan", 
    "publish": "2008-04-04T13:21:40Z", 
    "summary": "The generalized traveling salesman problem (GTSP) is an extension of the\nwell-known traveling salesman problem. In GTSP, we are given a partition of\ncities into groups and we are required to find a minimum length tour that\nincludes exactly one city from each group. The recent studies on this subject\nconsider different variations of a memetic algorithm approach to the GTSP. The\naim of this paper is to present a new memetic algorithm for GTSP with a\npowerful local search procedure. The experiments show that the proposed\nalgorithm clearly outperforms all of the known heuristics with respect to both\nsolution quality and running time. While the other memetic algorithms were\ndesigned only for the symmetric GTSP, our algorithm can solve both symmetric\nand asymmetric instances."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0735v2", 
    "title": "Generalized Traveling Salesman Problem Reduction Algorithms", 
    "arxiv-id": "0804.0735v2", 
    "author": "Daniel Karapetyan", 
    "publish": "2008-04-04T13:36:19Z", 
    "summary": "The generalized traveling salesman problem (GTSP) is an extension of the\nwell-known traveling salesman problem. In GTSP, we are given a partition of\ncities into groups and we are required to find a minimum length tour that\nincludes exactly one city from each group. The aim of this paper is to present\na problem reduction algorithm that deletes redundant vertices and edges,\npreserving the optimal solution. The algorithm's running time is O(N^3) in the\nworst case, but it is significantly faster in practice. The algorithm has\nreduced the problem size by 15-20% on average in our experiments and this has\ndecreased the solution time by 10-60% for each of the considered solvers."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0936v1", 
    "title": "Cache-Oblivious Selection in Sorted X+Y Matrices", 
    "arxiv-id": "0804.0936v1", 
    "author": "Shripad Thite", 
    "publish": "2008-04-06T22:31:04Z", 
    "summary": "Let X[0..n-1] and Y[0..m-1] be two sorted arrays, and define the mxn matrix A\nby A[j][i]=X[i]+Y[j]. Frederickson and Johnson gave an efficient algorithm for\nselecting the k-th smallest element from A. We show how to make this algorithm\nIO-efficient. Our cache-oblivious algorithm performs O((m+n)/B) IOs, where B is\nthe block size of memory transfers."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0940v1", 
    "title": "Optimum Binary Search Trees on the Hierarchical Memory Model", 
    "arxiv-id": "0804.0940v1", 
    "author": "Shripad Thite", 
    "publish": "2008-04-07T00:06:08Z", 
    "summary": "The Hierarchical Memory Model (HMM) of computation is similar to the standard\nRandom Access Machine (RAM) model except that the HMM has a non-uniform memory\norganized in a hierarchy of levels numbered 1 through h. The cost of accessing\na memory location increases with the level number, and accesses to memory\nlocations belonging to the same level cost the same. Formally, the cost of a\nsingle access to the memory location at address a is given by m(a), where m: N\n-> N is the memory cost function, and the h distinct values of m model the\ndifferent levels of the memory hierarchy.\n  We study the problem of constructing and storing a binary search tree (BST)\nof minimum cost, over a set of keys, with probabilities for successful and\nunsuccessful searches, on the HMM with an arbitrary number of memory levels,\nand for the special case h=2.\n  While the problem of constructing optimum binary search trees has been well\nstudied for the standard RAM model, the additional parameter m for the HMM\nincreases the combinatorial complexity of the problem. We present two dynamic\nprogramming algorithms to construct optimum BSTs bottom-up. These algorithms\nrun efficiently under some natural assumptions about the memory hierarchy. We\nalso give an efficient algorithm to construct a BST that is close to optimum,\nby modifying a well-known linear-time approximation algorithm for the RAM\nmodel. We conjecture that the problem of constructing an optimum BST for the\nHMM with an arbitrary memory cost function m is NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.1170v1", 
    "title": "Approximating L1-distances between mixture distributions using random   projections", 
    "arxiv-id": "0804.1170v1", 
    "author": "Daniel Stefankovic", 
    "publish": "2008-04-08T02:11:13Z", 
    "summary": "We consider the problem of computing L1-distances between every pair\nofcprobability densities from a given family. We point out that the technique\nof Cauchy random projections (Indyk'06) in this context turns into stochastic\nintegrals with respect to Cauchy motion.\n  For piecewise-linear densities these integrals can be sampled from if one can\nsample from the stochastic integral of the function x->(1,x). We give an\nexplicit density function for this stochastic integral and present an efficient\nsampling algorithm. As a consequence we obtain an efficient algorithm to\napproximate the L1-distances with a small relative error.\n  For piecewise-polynomial densities we show how to approximately sample from\nthe distributions resulting from the stochastic integrals. This also results in\nan efficient algorithm to approximate the L1-distances, although our inability\nto get exact samples worsens the dependence on the parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.3902v1", 
    "title": "Minimum-energy broadcast in random-grid ad-hoc networks: approximation   and distributed algorithms", 
    "arxiv-id": "0804.3902v1", 
    "author": "Riccardo Silvestri", 
    "publish": "2008-04-24T11:17:57Z", 
    "summary": "The Min Energy broadcast problem consists in assigning transmission ranges to\nthe nodes of an ad-hoc network in order to guarantee a directed spanning tree\nfrom a given source node and, at the same time, to minimize the energy\nconsumption (i.e. the energy cost) yielded by the range assignment. Min energy\nbroadcast is known to be NP-hard.\n  We consider random-grid networks where nodes are chosen independently at\nrandom from the $n$ points of a $\\sqrt n \\times \\sqrt n$ square grid in the\nplane. The probability of the existence of a node at a given point of the grid\ndoes depend on that point, that is, the probability distribution can be\nnon-uniform.\n  By using information-theoretic arguments, we prove a lower bound\n$(1-\\epsilon) \\frac n{\\pi}$ on the energy cost of any feasible solution for\nthis problem. Then, we provide an efficient solution of energy cost not larger\nthan $1.1204 \\frac n{\\pi}$.\n  Finally, we present a fully-distributed protocol that constructs a broadcast\nrange assignment of energy cost not larger than $8n$,thus still yielding\nconstant approximation. The energy load is well balanced and, at the same time,\nthe work complexity (i.e. the energy due to all message transmissions of the\nprotocol) is asymptotically optimal. The completion time of the protocol is\nonly an $O(\\log n)$ factor slower than the optimum. The approximation quality\nof our distributed solution is also experimentally evaluated.\n  All bounds hold with probability at least $1-1/n^{\\Theta(1)}$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.3947v1", 
    "title": "Time Dependent Contraction Hierarchies -- Basic Algorithmic Ideas", 
    "arxiv-id": "0804.3947v1", 
    "author": "Peter Sanders", 
    "publish": "2008-04-24T15:24:08Z", 
    "summary": "Contraction hierarchies are a simple hierarchical routing technique that has\nproved extremely efficient for static road networks. We explain how to\ngeneralize them to networks with time-dependent edge weights. This is the first\nhierarchical speedup technique for time-dependent routing that allows\nbidirectional query algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.4138v1", 
    "title": "Sketching and Streaming Entropy via Approximation Theory", 
    "arxiv-id": "0804.4138v1", 
    "author": "Krzysztof Onak", 
    "publish": "2008-04-25T16:04:20Z", 
    "summary": "We conclude a sequence of work by giving near-optimal sketching and streaming\nalgorithms for estimating Shannon entropy in the most general streaming model,\nwith arbitrary insertions and deletions. This improves on prior results that\nobtain suboptimal space bounds in the general model, and near-optimal bounds in\nthe insertion-only model without sketching. Our high-level approach is simple:\nwe give algorithms to estimate Renyi and Tsallis entropy, and use them to\nextrapolate an estimate of Shannon entropy. The accuracy of our estimates is\nproven using approximation theory arguments and extremal properties of\nChebyshev polynomials, a technique which may be useful for other problems. Our\nwork also yields the best-known and near-optimal additive approximations for\nentropy, and hence also for conditional entropy and mutual information."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0804.4819v2", 
    "title": "The Minimum Backlog Problem", 
    "arxiv-id": "0804.4819v2", 
    "author": "Jukka Suomela", 
    "publish": "2008-04-30T13:13:12Z", 
    "summary": "We study the minimum backlog problem (MBP). This online problem arises, e.g.,\nin the context of sensor networks. We focus on two main variants of MBP.\n  The discrete MBP is a 2-person game played on a graph $G=(V,E)$. The player\nis initially located at a vertex of the graph. In each time step, the adversary\npours a total of one unit of water into cups that are located on the vertices\nof the graph, arbitrarily distributing the water among the cups. The player\nthen moves from her current vertex to an adjacent vertex and empties the cup at\nthat vertex. The player's objective is to minimize the backlog, i.e., the\nmaximum amount of water in any cup at any time.\n  The geometric MBP is a continuous-time version of the MBP: the cups are\npoints in the two-dimensional plane, the adversary pours water continuously at\na constant rate, and the player moves in the plane with unit speed. Again, the\nplayer's objective is to minimize the backlog.\n  We show that the competitive ratio of any algorithm for the MBP has a lower\nbound of $\\Omega(D)$, where $D$ is the diameter of the graph (for the discrete\nMBP) or the diameter of the point set (for the geometric MBP). Therefore we\nfocus on determining a strategy for the player that guarantees a uniform upper\nbound on the absolute value of the backlog.\n  For the absolute value of the backlog there is a trivial lower bound of\n$\\Omega(D)$, and the deamortization analysis of Dietz and Sleator gives an\nupper bound of $O(D\\log N)$ for $N$ cups. Our main result is a tight upper\nbound for the geometric MBP: we show that there is a strategy for the player\nthat guarantees a backlog of $O(D)$, independently of the number of cups."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1071v3", 
    "title": "Submodular approximation: sampling-based algorithms and lower bounds", 
    "arxiv-id": "0805.1071v3", 
    "author": "Lisa Fleischer", 
    "publish": "2008-05-07T21:37:18Z", 
    "summary": "We introduce several generalizations of classical computer science problems\nobtained by replacing simpler objective functions with general submodular\nfunctions. The new problems include submodular load balancing, which\ngeneralizes load balancing or minimum-makespan scheduling, submodular sparsest\ncut and submodular balanced cut, which generalize their respective graph cut\nproblems, as well as submodular function minimization with a cardinality lower\nbound. We establish upper and lower bounds for the approximability of these\nproblems with a polynomial number of queries to a function-value oracle. The\napproximation guarantees for most of our algorithms are of the order of\nsqrt(n/ln n). We show that this is the inherent difficulty of the problems by\nproving matching lower bounds. We also give an improved lower bound for the\nproblem of approximately learning a monotone submodular function. In addition,\nwe present an algorithm for approximately learning submodular functions with\nspecial structure, whose guarantee is close to the lower bound. Although quite\nrestrictive, the class of functions with this structure includes the ones that\nare used for lower bounds both by us and in previous work. This demonstrates\nthat if there are significantly stronger lower bounds for this problem, they\nrely on more general submodular functions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1598v1", 
    "title": "A Simple In-Place Algorithm for In-Shuffle", 
    "arxiv-id": "0805.1598v1", 
    "author": "Peiyush Jain", 
    "publish": "2008-05-12T09:28:18Z", 
    "summary": "The paper presents a simple, linear time, in-place algorithm for performing a\n2-way in-shuffle which can be used with little modification for certain other\nk-way shuffles."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1661v2", 
    "title": "NAPX: A Polynomial Time Approximation Scheme for the Noah's Ark Problem", 
    "arxiv-id": "0805.1661v2", 
    "author": "N. Zeh", 
    "publish": "2008-05-12T15:04:26Z", 
    "summary": "The Noah's Ark Problem (NAP) is an NP-Hard optimization problem with\nrelevance to ecological conservation management. It asks to maximize the\nphylogenetic diversity (PD) of a set of taxa given a fixed budget, where each\ntaxon is associated with a cost of conservation and a probability of\nextinction. NAP has received renewed interest with the rise in availability of\ngenetic sequence data, allowing PD to be used as a practical measure of\nbiodiversity. However, only simplified instances of the problem, where one or\nmore parameters are fixed as constants, have as of yet been addressed in the\nliterature. We present NAPX, the first algorithm for the general version of NAP\nthat returns a $1 - \\epsilon$ approximation of the optimal solution. It runs in\n$O(\\frac{n B^2 h^2 \\log^2n}{\\log^2(1 - \\epsilon)})$ time where $n$ is the\nnumber of species, and $B$ is the total budget and $h$ is the height of the\ninput tree. We also provide improved bounds for its expected running time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.2630v2", 
    "title": "Sequential Design of Experiments via Linear Programming", 
    "arxiv-id": "0805.2630v2", 
    "author": "Kamesh Munagala", 
    "publish": "2008-05-17T22:48:22Z", 
    "summary": "The celebrated multi-armed bandit problem in decision theory models the basic\ntrade-off between exploration, or learning about the state of a system, and\nexploitation, or utilizing the system. In this paper we study the variant of\nthe multi-armed bandit problem where the exploration phase involves costly\nexperiments and occurs before the exploitation phase; and where each play of an\narm during the exploration phase updates a prior belief about the arm. The\nproblem of finding an inexpensive exploration strategy to optimize a certain\nexploitation objective is NP-Hard even when a single play reveals all\ninformation about an arm, and all exploration steps cost the same.\n  We provide the first polynomial time constant-factor approximation algorithm\nfor this class of problems. We show that this framework also generalizes\nseveral problems of interest studied in the context of data acquisition in\nsensor networks. Our analyses also extends to switching and setup costs, and to\nconcave utility objectives.\n  Our solution approach is via a novel linear program rounding technique based\non stochastic packing. In addition to yielding exploration policies whose\nperformance is within a small constant factor of the adaptive optimal policy, a\nnice feature of this approach is that the resulting policies explore the arms\nsequentially without revisiting any arm. Sequentiality is a well-studied\nconcept in decision theory, and is very desirable in domains where multiple\nexplorations can be conducted in parallel, for instance, in the sensor network\ncontext."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.2646v1", 
    "title": "Small Approximate Pareto Sets for Bi-objective Shortest Paths and Other   Problems", 
    "arxiv-id": "0805.2646v1", 
    "author": "Mihalis Yannakakis", 
    "publish": "2008-05-17T06:10:19Z", 
    "summary": "We investigate the problem of computing a minimum set of solutions that\napproximates within a specified accuracy $\\epsilon$ the Pareto curve of a\nmultiobjective optimization problem. We show that for a broad class of\nbi-objective problems (containing many important widely studied problems such\nas shortest paths, spanning tree, and many others), we can compute in\npolynomial time an $\\epsilon$-Pareto set that contains at most twice as many\nsolutions as the minimum such set. Furthermore we show that the factor of 2 is\ntight for these problems, i.e., it is NP-hard to do better. We present upper\nand lower bounds for three or more objectives, as well as for the dual problem\nof computing a specified number $k$ of solutions which provide a good\napproximation to the Pareto curve."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0806.0983v2", 
    "title": "A Comparison of Performance Measures for Online Algorithms", 
    "arxiv-id": "0806.0983v2", 
    "author": "Kim S. Larsen", 
    "publish": "2008-06-05T14:50:08Z", 
    "summary": "This paper provides a systematic study of several proposed measures for\nonline algorithms in the context of a specific problem, namely, the two server\nproblem on three colinear points. Even though the problem is simple, it\nencapsulates a core challenge in online algorithms which is to balance\ngreediness and adaptability. We examine Competitive Analysis, the Max/Max\nRatio, the Random Order Ratio, Bijective Analysis and Relative Worst Order\nAnalysis, and determine how these measures compare the Greedy Algorithm, Double\nCoverage, and Lazy Double Coverage, commonly studied algorithms in the context\nof server problems. We find that by the Max/Max Ratio and Bijective Analysis,\nGreedy is the best of the three algorithms. Under the other measures, Double\nCoverage and Lazy Double Coverage are better, though Relative Worst Order\nAnalysis indicates that Greedy is sometimes better. Only Bijective Analysis and\nRelative Worst Order Analysis indicate that Lazy Double Coverage is better than\nDouble Coverage. Our results also provide the first proof of optimality of an\nalgorithm under Relative Worst Order Analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0806.1722v1", 
    "title": "Fast Arithmetics Using Chinese Remaindering", 
    "arxiv-id": "0806.1722v1", 
    "author": "Guangwu Xu", 
    "publish": "2008-06-10T18:21:09Z", 
    "summary": "In this paper, some issues concerning the Chinese remaindering representation\nare discussed. Some new converting methods, including an efficient\nprobabilistic algorithm based on a recent result of von zur Gathen and\nShparlinski \\cite{Gathen-Shparlinski}, are described. An efficient refinement\nof the NC$^1$ division algorithm of Chiu, Davida and Litow\n\\cite{Chiu-Davida-Litow} is given, where the number of moduli is reduced by a\nfactor of $\\log n$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0806.1948v1", 
    "title": "Tight Bounds for Hashing Block Sources", 
    "arxiv-id": "0806.1948v1", 
    "author": "Salil Vadhan", 
    "publish": "2008-06-11T19:54:14Z", 
    "summary": "It is known that if a 2-universal hash function $H$ is applied to elements of\na {\\em block source} $(X_1,...,X_T)$, where each item $X_i$ has enough\nmin-entropy conditioned on the previous items, then the output distribution\n$(H,H(X_1),...,H(X_T))$ will be ``close'' to the uniform distribution. We\nprovide improved bounds on how much min-entropy per item is required for this\nto hold, both when we ask that the output be close to uniform in statistical\ndistance and when we only ask that it be statistically close to a distribution\nwith small collision probability. In both cases, we reduce the dependence of\nthe min-entropy on the number $T$ of items from $2\\log T$ in previous work to\n$\\log T$, which we show to be optimal. This leads to corresponding improvements\nto the recent results of Mitzenmacher and Vadhan (SODA `08) on the analysis of\nhashing-based algorithms and data structures when the data items come from a\nblock source."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0806.1978v5", 
    "title": "Max Cut and the Smallest Eigenvalue", 
    "arxiv-id": "0806.1978v5", 
    "author": "Luca Trevisan", 
    "publish": "2008-06-12T17:51:02Z", 
    "summary": "We describe a new approximation algorithm for Max Cut. Our algorithm runs in\n$\\tilde O(n^2)$ time, where $n$ is the number of vertices, and achieves an\napproximation ratio of $.531$. On instances in which an optimal solution cuts a\n$1-\\epsilon$ fraction of edges, our algorithm finds a solution that cuts a\n$1-4\\sqrt{\\epsilon} + 8\\epsilon-o(1)$ fraction of edges.\n  Our main result is a variant of spectral partitioning, which can be\nimplemented in nearly linear time. Given a graph in which the Max Cut optimum\nis a $1-\\epsilon$ fraction of edges, our spectral partitioning algorithm finds\na set $S$ of vertices and a bipartition $L,R=S-L$ of $S$ such that at least a\n$1-O(\\sqrt \\epsilon)$ fraction of the edges incident on $S$ have one endpoint\nin $L$ and one endpoint in $R$. (This can be seen as an analog of Cheeger's\ninequality for the smallest eigenvalue of the adjacency matrix of a graph.)\nIterating this procedure yields the approximation results stated above.\n  A different, more complicated, variant of spectral partitioning leads to an\n$\\tilde O(n^3)$ time algorithm that cuts $1/2 + e^{-\\Omega(1/\\eps)}$ fraction\nof edges in graphs in which the optimum is $1/2 + \\epsilon$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0806.3258v6", 
    "title": "Local Search Heuristics For The Multidimensional Assignment Problem", 
    "arxiv-id": "0806.3258v6", 
    "author": "Daniel Karapetyan", 
    "publish": "2008-06-19T18:31:51Z", 
    "summary": "The Multidimensional Assignment Problem (MAP) (abbreviated s-AP in the case\nof s dimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s also\nhave a large number of applications. We consider several known neighborhoods,\ngeneralize them and propose some new ones. The heuristics are evaluated both\ntheoretically and experimentally and dominating algorithms are selected. We\nalso demonstrate a combination of two neighborhoods may yield a heuristics\nwhich is superior to both of its components."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0806.3668v1", 
    "title": "Approximating Multi-Criteria Max-TSP", 
    "arxiv-id": "0806.3668v1", 
    "author": "Oliver Putz", 
    "publish": "2008-06-23T12:28:10Z", 
    "summary": "We present randomized approximation algorithms for multi-criteria Max-TSP.\nFor Max-STSP with k > 1 objective functions, we obtain an approximation ratio\nof $1/k - \\eps$ for arbitrarily small $\\eps > 0$. For Max-ATSP with k objective\nfunctions, we obtain an approximation ratio of $1/(k+1) - \\eps$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0806.4790v5", 
    "title": "AMS Without 4-Wise Independence on Product Domains", 
    "arxiv-id": "0806.4790v5", 
    "author": "Rafail Ostrovsky", 
    "publish": "2008-06-29T21:34:28Z", 
    "summary": "In their seminal work, Alon, Matias, and Szegedy introduced several sketching\ntechniques, including showing that 4-wise independence is sufficient to obtain\ngood approximations of the second frequency moment. In this work, we show that\ntheir sketching technique can be extended to product domains $[n]^k$ by using\nthe product of 4-wise independent functions on $[n]$. Our work extends that of\nIndyk and McGregor, who showed the result for $k = 2$. Their primary motivation\nwas the problem of identifying correlations in data streams. In their model, a\nstream of pairs $(i,j) \\in [n]^2$ arrive, giving a joint distribution $(X,Y)$,\nand they find approximation algorithms for how close the joint distribution is\nto the product of the marginal distributions under various metrics, which\nnaturally corresponds to how close $X$ and $Y$ are to being independent. By\nusing our technique, we obtain a new result for the problem of approximating\nthe $\\ell_2$ distance between the joint distribution and the product of the\nmarginal distributions for $k$-ary vectors, instead of just pairs, in a single\npass. Our analysis gives a randomized algorithm that is a $(1 \\pm \\epsilon)$\napproximation (with probability $1-\\delta$) that requires space logarithmic in\n$n$ and $m$ and proportional to $3^k$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.0928v1", 
    "title": "Bloomier Filters: A second look", 
    "arxiv-id": "0807.0928v1", 
    "author": "Kumar Chellapilla", 
    "publish": "2008-07-06T19:45:37Z", 
    "summary": "A Bloom filter is a space efficient structure for storing static sets, where\nthe space efficiency is gained at the expense of a small probability of\nfalse-positives. A Bloomier filter generalizes a Bloom filter to compactly\nstore a function with a static support. In this article we give a simple\nconstruction of a Bloomier filter. The construction is linear in space and\nrequires constant time to evaluate. The creation of our Bloomier filter takes\nlinear time which is faster than the existing construction. We show how one can\nimprove the space utilization further at the cost of increasing the time for\ncreating the data structure."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.1139v1", 
    "title": "Algorithms for Secretary Problems on Graphs and Hypergraphs", 
    "arxiv-id": "0807.1139v1", 
    "author": "Martin Pal", 
    "publish": "2008-07-07T22:51:46Z", 
    "summary": "We examine several online matching problems, with applications to Internet\nadvertising reservation systems. Consider an edge-weighted bipartite graph G,\nwith partite sets L, R. We develop an 8-competitive algorithm for the following\nsecretary problem: Initially given R, and the size of L, the algorithm receives\nthe vertices of L sequentially, in a random order. When a vertex l \\in L is\nseen, all edges incident to l are revealed, together with their weights. The\nalgorithm must immediately either match l to an available vertex of R, or\ndecide that l will remain unmatched.\n  Dimitrov and Plaxton show a 16-competitive algorithm for the transversal\nmatroid secretary problem, which is the special case with weights on vertices,\nnot edges. (Equivalently, one may assume that for each l \\in L, the weights on\nall edges incident to l are identical.) We use a similar algorithm, but\nsimplify and improve the analysis to obtain a better competitive ratio for the\nmore general problem. Perhaps of more interest is the fact that our analysis is\neasily extended to obtain competitive algorithms for similar problems, such as\nto find disjoint sets of edges in hypergraphs where edges arrive online. We\nalso introduce secretary problems with adversarially chosen groups. Finally, we\ngive a 2e-competitive algorithm for the secretary problem on graphic matroids,\nwhere, with edges appearing online, the goal is to find a maximum-weight\nacyclic subgraph of a given graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.1891v1", 
    "title": "Online Scheduling to Minimize the Maximum Delay Factor", 
    "arxiv-id": "0807.1891v1", 
    "author": "Benjamin Moseley", 
    "publish": "2008-07-11T17:00:07Z", 
    "summary": "In this paper two scheduling models are addressed. First is the standard\nmodel (unicast) where requests (or jobs) are independent. The other is the\nbroadcast model where broadcasting a page can satisfy multiple outstanding\nrequests for that page. We consider online scheduling of requests when they\nhave deadlines. Unlike previous models, which mainly consider the objective of\nmaximizing throughput while respecting deadlines, here we focus on scheduling\nall the given requests with the goal of minimizing the maximum {\\em delay\nfactor}.We prove strong lower bounds on the achievable competitive ratios for\ndelay factor scheduling even with unit-time requests.For the unicast model we\ngive algorithms that are $(1 + \\eps)$-speed $O({1 \\over \\eps})$-competitive in\nboth the single machine and multiple machine settings. In the broadcast model\nwe give an algorithm for similar-sized pages that is $(2+ \\eps)$-speed $O({1\n\\over \\eps^2})$-competitive. For arbitrary page sizes we give an algorithm that\nis $(4+\\eps)$-speed $O({1 \\over \\eps^2})$-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.2694v4", 
    "title": "Algorithms for Scheduling Weighted Packets with Deadlines in a Bounded   Queue", 
    "arxiv-id": "0807.2694v4", 
    "author": "Fei Li", 
    "publish": "2008-07-17T04:58:30Z", 
    "summary": "Motivated by the Quality-of-Service (QoS) buffer management problem, we\nconsider online scheduling of packets with hard deadlines in a finite capacity\nqueue. At any time, a queue can store at most $b \\in \\mathbb Z^+$ packets.\nPackets arrive over time. Each packet is associated with a non-negative value\nand an integer deadline. In each time step, only one packet is allowed to be\nsent. Our objective is to maximize the total value gained by the packets sent\nby their deadlines in an online manner. Due to the Internet traffic's chaotic\ncharacteristics, no stochastic assumptions are made on the packet input\nsequences. This model is called a {\\em finite-queue model}.\n  We use competitive analysis to measure an online algorithm's performance\nversus an unrealizable optimal offline algorithm who constructs the worst\npossible input based on the knowledge of the online algorithm. For the\nfinite-queue model, we first present a deterministic 3-competitive memoryless\nonline algorithm. Then, we give a randomized ($\\phi^2 = ((1 + \\sqrt{5}) / 2)^2\n\\approx 2.618$)-competitive memoryless online algorithm.\n  The algorithmic framework and its theoretical analysis include several\ninteresting features. First, our algorithms use (possibly) modified\ncharacteristics of packets; these characteristics may not be same as those\nspecified in the input sequence. Second, our analysis method is different from\nthe classical potential function approach."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.4368v1", 
    "title": "Improved Algorithms for Approximate String Matching (Extended Abstract)", 
    "arxiv-id": "0807.4368v1", 
    "author": "Georgios Papamichail", 
    "publish": "2008-07-28T07:17:57Z", 
    "summary": "The problem of approximate string matching is important in many different\nareas such as computational biology, text processing and pattern recognition. A\ngreat effort has been made to design efficient algorithms addressing several\nvariants of the problem, including comparison of two strings, approximate\npattern identification in a string or calculation of the longest common\nsubsequence that two strings share.\n  We designed an output sensitive algorithm solving the edit distance problem\nbetween two strings of lengths n and m respectively in time\nO((s-|n-m|)min(m,n,s)+m+n) and linear space, where s is the edit distance\nbetween the two strings. This worst-case time bound sets the quadratic factor\nof the algorithm independent of the longest string length and improves existing\ntheoretical bounds for this problem. The implementation of our algorithm excels\nalso in practice, especially in cases where the two strings compared differ\nsignificantly in length. Source code of our algorithm is available at\nhttp://www.cs.miami.edu/\\~dimitris/edit_distance"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0807.5111v2", 
    "title": "Finding Dense Subgraphs in G(n,1/2)", 
    "arxiv-id": "0807.5111v2", 
    "author": "Ravi Kannan", 
    "publish": "2008-07-31T17:12:25Z", 
    "summary": "Finding the largest clique is a notoriously hard problem, even on random\ngraphs. It is known that the clique number of a random graph G(n,1/2) is almost\nsurely either k or k+1, where k = 2log n - 2log(log n) - 1. However, a simple\ngreedy algorithm finds a clique of size only (1+o(1))log n, with high\nprobability, and finding larger cliques -- that of size even (1+ epsilon)log n\n-- in randomized polynomial time has been a long-standing open problem. In this\npaper, we study the following generalization: given a random graph G(n,1/2),\nfind the largest subgraph with edge density at least (1-delta). We show that a\nsimple modification of the greedy algorithm finds a subset of 2log n vertices\nwhose induced subgraph has edge density at least 0.951, with high probability.\nTo complement this, we show that almost surely there is no subset of 2.784log n\nvertices whose induced subgraph has edge density 0.951 or more."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.0760v1", 
    "title": "Declarative Combinatorics: Boolean Functions, Circuit Synthesis and BDDs   in Haskell", 
    "arxiv-id": "0808.0760v1", 
    "author": "Paul Tarau", 
    "publish": "2008-08-06T01:41:51Z", 
    "summary": "We describe Haskell implementations of interesting combinatorial generation\nalgorithms with focus on boolean functions and logic circuit representations.\n  First, a complete exact combinational logic circuit synthesizer is described\nas a combination of catamorphisms and anamorphisms.\n  Using pairing and unpairing functions on natural number representations of\ntruth tables, we derive an encoding for Binary Decision Diagrams (BDDs) with\nthe unique property that its boolean evaluation faithfully mimics its\nstructural conversion to a a natural number through recursive application of a\nmatching pairing function.\n  We then use this result to derive ranking and unranking functions for BDDs\nand reduced BDDs.\n  Finally, a generalization of the encoding techniques to Multi-Terminal BDDs\nis provided.\n  The paper is organized as a self-contained literate Haskell program,\navailable at http://logic.csci.unt.edu/tarau/research/2008/fBDD.zip .\n  Keywords: exact combinational logic synthesis, binary decision diagrams,\nencodings of boolean functions, pairing/unpairing functions, ranking/unranking\nfunctions for BDDs and MTBDDs, declarative combinatorics in Haskell"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.1108v2", 
    "title": "Cache oblivious storage and access heuristics for blocked matrix-matrix   multiplication", 
    "arxiv-id": "0808.1108v2", 
    "author": "Matt Challacombe", 
    "publish": "2008-08-07T22:49:29Z", 
    "summary": "We investigate effects of ordering in blocked matrix--matrix multiplication.\nWe find that submatrices do not have to be stored contiguously in memory to\nachieve near optimal performance. Instead it is the choice of execution order\nof the submatrix multiplications that leads to a speedup of up to four times\nfor small block sizes. This is in contrast to results for single matrix\nelements showing that contiguous memory allocation quickly becomes irrelevant\nas the blocksize increases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.1246v2", 
    "title": "Minimum Dissatisfaction Personnel Scheduling", 
    "arxiv-id": "0808.1246v2", 
    "author": "Angela Andreica", 
    "publish": "2008-08-08T17:15:34Z", 
    "summary": "In this paper we consider two problems regarding the scheduling of available\npersonnel in order to perform a given quantity of work, which can be\narbitrarily decomposed into a sequence of activities. We are interested in\nschedules which minimize the overall dissatisfaction, where each employee's\ndissatisfaction is modeled as a time-dependent linear function. For the two\nsituations considered we provide a detailed mathematical analysis, as well as\nefficient algorithms for determining optimal schedules."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.1766v1", 
    "title": "The Optimal Quantile Estimator for Compressed Counting", 
    "arxiv-id": "0808.1766v1", 
    "author": "Ping Li", 
    "publish": "2008-08-13T01:38:45Z", 
    "summary": "Compressed Counting (CC) was recently proposed for very efficiently computing\nthe (approximate) $\\alpha$th frequency moments of data streams, where $0<\\alpha\n<= 2$. Several estimators were reported including the geometric mean estimator,\nthe harmonic mean estimator, the optimal power estimator, etc. The geometric\nmean estimator is particularly interesting for theoretical purposes. For\nexample, when $\\alpha -> 1$, the complexity of CC (using the geometric mean\nestimator) is $O(1/\\epsilon)$, breaking the well-known large-deviation bound\n$O(1/\\epsilon^2)$. The case $\\alpha\\approx 1$ has important applications, for\nexample, computing entropy of data streams.\n  For practical purposes, this study proposes the optimal quantile estimator.\nCompared with previous estimators, this estimator is computationally more\nefficient and is also more accurate when $\\alpha> 1$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.1771v2", 
    "title": "A Very Efficient Scheme for Estimating Entropy of Data Streams Using   Compressed Counting", 
    "arxiv-id": "0808.1771v2", 
    "author": "Ping Li", 
    "publish": "2008-08-13T03:05:33Z", 
    "summary": "Compressed Counting (CC)} was recently proposed for approximating the\n$\\alpha$th frequency moments of data streams, for $0<\\alpha \\leq 2$. Under the\nrelaxed strict-Turnstile model, CC dramatically improves the standard algorithm\nbased on symmetric stable random projections}, especially as $\\alpha\\to 1$. A\ndirect application of CC is to estimate the entropy, which is an important\nsummary statistic in Web/network measurement and often serves a crucial\n\"feature\" for data mining. The R\\'enyi entropy and the Tsallis entropy are\nfunctions of the $\\alpha$th frequency moments; and both approach the Shannon\nentropy as $\\alpha\\to 1$. A recent theoretical work suggested using the\n$\\alpha$th frequency moment to approximate the Shannon entropy with\n$\\alpha=1+\\delta$ and very small $|\\delta|$ (e.g., $<10^{-4}$).\n  In this study, we experiment using CC to estimate frequency moments, R\\'enyi\nentropy, Tsallis entropy, and Shannon entropy, on real Web crawl data. We\ndemonstrate the variance-bias trade-off in estimating Shannon entropy and\nprovide practical recommendations. In particular, our experiments enable us to\ndraw some important conclusions:\n  (1) As $\\alpha\\to 1$, CC dramatically improves {\\em symmetric stable random\nprojections} in estimating frequency moments, R\\'enyi entropy, Tsallis entropy,\nand Shannon entropy. The improvements appear to approach \"infinity.\"\n  (2) Using {\\em symmetric stable random projections} and $\\alpha = 1+\\delta$\nwith very small $|\\delta|$ does not provide a practical algorithm because the\nrequired sample size is enormous."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.2222v1", 
    "title": "Better Bounds for Frequency Moments in Random-Order Streams", 
    "arxiv-id": "0808.2222v1", 
    "author": "Rina Panigrahy", 
    "publish": "2008-08-16T00:43:14Z", 
    "summary": "Estimating frequency moments of data streams is a very well studied problem\nand tight bounds are known on the amount of space that is necessary and\nsufficient when the stream is adversarially ordered. Recently, motivated by\nvarious practical considerations and applications in learning and statistics,\nthere has been growing interest into studying streams that are randomly\nordered. In the paper we improve the previous lower bounds on the space\nrequired to estimate the frequency moments of a randomly ordered streams."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0808.3197v1", 
    "title": "On the Monotonicity of Work Function in k-Server Conjecture", 
    "arxiv-id": "0808.3197v1", 
    "author": "Ming-Zhe Chen", 
    "publish": "2008-08-24T15:37:35Z", 
    "summary": "This paper presents a mistake in work function algorithm of k-server\nconjecture. That is, the monotonicity of the work function is not always true."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.0460v1", 
    "title": "Stochastic Combinatorial Optimization under Probabilistic Constraints", 
    "arxiv-id": "0809.0460v1", 
    "author": "Yinyu Ye", 
    "publish": "2008-09-02T16:07:42Z", 
    "summary": "In this paper, we present approximation algorithms for combinatorial\noptimization problems under probabilistic constraints. Specifically, we focus\non stochastic variants of two important combinatorial optimization problems:\nthe k-center problem and the set cover problem, with uncertainty characterized\nby a probability distribution over set of points or elements to be covered. We\nconsider these problems under adaptive and non-adaptive settings, and present\nefficient approximation algorithms for the case when underlying distribution is\na product distribution. In contrast to the expected cost model prevalent in\nstochastic optimization literature, our problem definitions support\nrestrictions on the probability distributions of the total costs, via\nincorporating constraints that bound the probability with which the incurred\ncosts may exceed a given threshold."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.1715v1", 
    "title": "Improved Smoothed Analysis of the k-Means Method", 
    "arxiv-id": "0809.1715v1", 
    "author": "Heiko R\u00f6glin", 
    "publish": "2008-09-10T07:00:38Z", 
    "summary": "The k-means method is a widely used clustering algorithm. One of its\ndistinguished features is its speed in practice. Its worst-case running-time,\nhowever, is exponential, leaving a gap between practical and theoretical\nperformance. Arthur and Vassilvitskii (FOCS 2006) aimed at closing this gap,\nand they proved a bound of $\\poly(n^k, \\sigma^{-1})$ on the smoothed\nrunning-time of the k-means method, where n is the number of data points and\n$\\sigma$ is the standard deviation of the Gaussian perturbation. This bound,\nthough better than the worst-case bound, is still much larger than the\nrunning-time observed in practice.\n  We improve the smoothed analysis of the k-means method by showing two upper\nbounds on the expected running-time of k-means. First, we prove that the\nexpected running-time is bounded by a polynomial in $n^{\\sqrt k}$ and\n$\\sigma^{-1}$. Second, we prove an upper bound of $k^{kd} \\cdot \\poly(n,\n\\sigma^{-1})$, where d is the dimension of the data space. The polynomial is\nindependent of k and d, and we obtain a polynomial bound for the expected\nrunning-time for $k, d \\in O(\\sqrt{\\log n/\\log \\log n})$.\n  Finally, we show that k-means runs in smoothed polynomial time for\none-dimensional instances."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.1895v1", 
    "title": "Thinking Twice about Second-Price Ad Auctions", 
    "arxiv-id": "0809.1895v1", 
    "author": "C. Thach Nguyen", 
    "publish": "2008-09-10T23:33:47Z", 
    "summary": "Recent work has addressed the algorithmic problem of allocating advertisement\nspace for keywords in sponsored search auctions so as to maximize revenue, most\nof which assume that pricing is done via a first-price auction. This does not\nrealistically model the Generalized Second Price (GSP) auction used in\npractice, in which bidders pay the next-highest bid for keywords that they are\nallocated. Towards the goal of more realistically modeling these auctions, we\nintroduce the Second-Price Ad Auctions problem, in which bidders' payments are\ndetermined by the GSP mechanism. We show that the complexity of the\nSecond-Price Ad Auctions problem is quite different than that of the more\nstudied First-Price Ad Auctions problem. First, unlike the first-price variant,\nfor which small constant-factor approximations are known, it is NP-hard to\napproximate the Second-Price Ad Auctions problem to any non-trivial factor,\neven when the bids are small compared to the budgets. Second, this discrepancy\nextends even to the 0-1 special case that we call the Second-Price Matching\nproblem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no\ndeterministic algorithm achieving a non-trivial competitive ratio and no\nrandomized algorithm achieving a competitive ratio better than 2. This\ncontrasts with the results for the analogous special case in the first-price\nmodel, the standard bipartite matching problem, which is solvable in polynomial\ntime and which has deterministic and randomized online algorithms achieving\nbetter competitive ratios. On the positive side, we provide a 2-approximation\nfor offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.\nThe latter result makes use of a new generalization of a result on the\nperformance of the \"Ranking\" algorithm for online bipartite matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.1902v2", 
    "title": "Fast C-K-R Partitions of Sparse Graphs", 
    "arxiv-id": "0809.1902v2", 
    "author": "Chaya Schwob", 
    "publish": "2008-09-11T02:10:29Z", 
    "summary": "We present fast algorithms for constructing probabilistic embeddings and\napproximate distance oracles in sparse graphs. The main ingredient is a fast\nalgorithm for sampling the probabilistic partitions of Calinescu, Karloff, and\nRabani in sparse graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.1906v2", 
    "title": "Betweenness Centrality : Algorithms and Lower Bounds", 
    "arxiv-id": "0809.1906v2", 
    "author": "Shiva Kintali", 
    "publish": "2008-09-11T02:49:07Z", 
    "summary": "One of the most fundamental problems in large scale network analysis is to\ndetermine the importance of a particular node in a network. Betweenness\ncentrality is the most widely used metric to measure the importance of a node\nin a network. In this paper, we present a randomized parallel algorithm and an\nalgebraic method for computing betweenness centrality of all nodes in a\nnetwork. We prove that any path-comparison based algorithm cannot compute\nbetweenness in less than O(nm) time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.2097v1", 
    "title": "Algorithms for Locating Constrained Optimal Intervals", 
    "arxiv-id": "0809.2097v1", 
    "author": "Kun-Mao Chao", 
    "publish": "2008-09-11T20:12:08Z", 
    "summary": "In this work, we obtain the following new results.\n  1. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number\npairs, where $s_i>0$ for all $i$, and a number $L_h$, we propose an O(n)-time\nalgorithm for finding an index interval $[i,j]$ that maximizes\n$\\frac{\\sum_{k=i}^{j} h_k}{\\sum_{k=i}^{j} s_k}$ subject to $\\sum_{k=i}^{j} h_k\n\\geq L_h$.\n  2. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number\npairs, where $s_i=1$ for all $i$, and an integer $L_s$ with $1\\leq L_s\\leq n$,\nwe propose an $O(n\\frac{T(L_s^{1/2})}{L_s^{1/2}})$-time algorithm for finding\nan index interval $[i,j]$ that maximizes $\\frac{\\sum_{k=i}^{j}\nh_k}{\\sqrt{\\sum_{k=i}^{j} s_k}}$ subject to $\\sum_{k=i}^{j} s_k \\geq L_s$,\nwhere $T(n')$ is the time required to solve the all-pairs shortest paths\nproblem on a graph of $n'$ nodes. By the latest result of Chan \\cite{Chan},\n$T(n')=O(n'^3 \\frac{(\\log\\log n')^3}{(\\log n')^2})$, so our algorithm runs in\nsubquadratic time $O(nL_s\\frac{(\\log\\log L_s)^3}{(\\log L_s)^2})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.2554v1", 
    "title": "Simpler Analyses of Local Search Algorithms for Facility Location", 
    "arxiv-id": "0809.2554v1", 
    "author": "Kanat Tangwongsan", 
    "publish": "2008-09-15T15:42:47Z", 
    "summary": "We study local search algorithms for metric instances of facility location\nproblems: the uncapacitated facility location problem (UFL), as well as\nuncapacitated versions of the $k$-median, $k$-center and $k$-means problems.\nAll these problems admit natural local search heuristics: for example, in the\nUFL problem the natural moves are to open a new facility, close an existing\nfacility, and to swap a closed facility for an open one; in $k$-medians, we are\nallowed only swap moves. The local-search algorithm for $k$-median was analyzed\nby Arya et al. (SIAM J. Comput. 33(3):544-562, 2004), who used a clever\n``coupling'' argument to show that local optima had cost at most constant times\nthe global optimum. They also used this argument to show that the local search\nalgorithm for UFL was 3-approximation; their techniques have since been applied\nto other facility location problems.\n  In this paper, we give a proof of the $k$-median result which avoids this\ncoupling argument. These arguments can be used in other settings where the Arya\net al. arguments have been used. We also show that for the problem of opening\n$k$ facilities $F$ to minimize the objective function $\\Phi_p(F) = \\big(\\sum_{j\n\\in V} d(j, F)^p\\big)^{1/p}$, the natural swap-based local-search algorithm is\na $\\Theta(p)$-approximation. This implies constant-factor approximations for\n$k$-medians (when $p=1$), and $k$-means (when $p = 2$), and an $O(\\log\nn)$-approximation algorithm for the $k$-center problem (which is essentially $p\n= \\log n$)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.2970v1", 
    "title": "Single source shortest paths in $H$-minor free graphs", 
    "arxiv-id": "0809.2970v1", 
    "author": "Raphael Yuster", 
    "publish": "2008-09-17T17:51:09Z", 
    "summary": "We present an algorithm for the Single Source Shortest Paths (SSSP) problem\nin \\emph{$H$-minor free} graphs. For every fixed $H$, if $G$ is a graph with\n$n$ vertices having integer edge lengths and $s$ is a designated source vertex\nof $G$, the algorithm runs in $\\tilde{O}(n^{\\sqrt{11.5}-2} \\log L) \\le\nO(n^{1.392} \\log L)$ time, where $L$ is the absolute value of the smallest edge\nlength. The algorithm computes shortest paths and the distances from $s$ to all\nvertices of the graph, or else provides a certificate that $G$ is not $H$-minor\nfree. Our result improves an earlier $O(n^{1.5} \\log L)$ time algorithm for\nthis problem, which follows from a general SSSP algorithm of Goldberg."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.3527v2", 
    "title": "Inferring Company Structure from Limited Available Information", 
    "arxiv-id": "0809.3527v2", 
    "author": "Romulus Andreica", 
    "publish": "2008-09-20T20:05:21Z", 
    "summary": "In this paper we present several algorithmic techniques for inferring the\nstructure of a company when only a limited amount of information is available.\nWe consider problems with two types of inputs: the number of pairs of employees\nwith a given property and restricted information about the hierarchical\nstructure of the company. We provide dynamic programming and greedy algorithms\nfor these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0809.3528v2", 
    "title": "Locating Restricted Facilities on Binary Maps", 
    "arxiv-id": "0809.3528v2", 
    "author": "Madalina Ecaterina Andreica", 
    "publish": "2008-09-20T20:06:34Z", 
    "summary": "In this paper we consider several facility location problems with\napplications to cost and social welfare optimization, when the area map is\nencoded as a binary (0,1) mxn matrix. We present algorithmic solutions for all\nthe problems. Some cases are too particular to be used in practical situations,\nbut they are at least a starting point for more generic solutions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.0075v1", 
    "title": "Acerca del Algoritmo de Dijkstra", 
    "arxiv-id": "0810.0075v1", 
    "author": "Alvaro Salas", 
    "publish": "2008-10-01T04:55:11Z", 
    "summary": "In this paper we prove the correctness of Dijkstra's algorithm. We also\ndiscuss it and at the end we show an application."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.0264v1", 
    "title": "A Fast Generic Sequence Matching Algorithm", 
    "arxiv-id": "0810.0264v1", 
    "author": "Gor V. Nishanov", 
    "publish": "2008-10-01T19:54:51Z", 
    "summary": "A string matching -- and more generally, sequence matching -- algorithm is\npresented that has a linear worst-case computing time bound, a low worst-case\nbound on the number of comparisons (2n), and sublinear average-case behavior\nthat is better than that of the fastest versions of the Boyer-Moore algorithm.\nThe algorithm retains its efficiency advantages in a wide variety of sequence\nmatching problems of practical interest, including traditional string matching;\nlarge-alphabet problems (as in Unicode strings); and small-alphabet,\nlong-pattern problems (as in DNA searches). Since it is expressed as a generic\nalgorithm for searching in sequences over an arbitrary type T, it is well\nsuited for use in generic software libraries such as the C++ Standard Template\nLibrary. The algorithm was obtained by adding to the Knuth-Morris-Pratt\nalgorithm one of the pattern-shifting techniques from the Boyer-Moore\nalgorithm, with provision for use of hashing in this technique. In situations\nin which a hash function or random access to the sequences is not available,\nthe algorithm falls back to an optimized version of the Knuth-Morris-Pratt\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.0558v2", 
    "title": "The Ratio Index for Budgeted Learning, with Applications", 
    "arxiv-id": "0810.0558v2", 
    "author": "Brad Null", 
    "publish": "2008-10-03T01:37:45Z", 
    "summary": "In the budgeted learning problem, we are allowed to experiment on a set of\nalternatives (given a fixed experimentation budget) with the goal of picking a\nsingle alternative with the largest possible expected payoff. Approximation\nalgorithms for this problem were developed by Guha and Munagala by rounding a\nlinear program that couples the various alternatives together. In this paper we\npresent an index for this problem, which we call the ratio index, which also\nguarantees a constant factor approximation. Index-based policies have the\nadvantage that a single number (i.e. the index) can be computed for each\nalternative irrespective of all other alternatives, and the alternative with\nthe highest index is experimented upon. This is analogous to the famous Gittins\nindex for the discounted multi-armed bandit problem.\n  The ratio index has several interesting structural properties. First, we show\nthat it can be computed in strongly polynomial time. Second, we show that with\nthe appropriate discount factor, the Gittins index and our ratio index are\nconstant factor approximations of each other, and hence the Gittins index also\ngives a constant factor approximation to the budgeted learning problem.\nFinally, we show that the ratio index can be used to create an index-based\npolicy that achieves an O(1)-approximation for the finite horizon version of\nthe multi-armed bandit problem. Moreover, the policy does not require any\nknowledge of the horizon (whereas we compare its performance against an optimal\nstrategy that is aware of the horizon). This yields the following surprising\nresult: there is an index-based policy that achieves an O(1)-approximation for\nthe multi-armed bandit problem, oblivious to the underlying discount factor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.0674v1", 
    "title": "Packing multiway cuts in capacitated graphs", 
    "arxiv-id": "0810.0674v1", 
    "author": "Shuchi Chawla", 
    "publish": "2008-10-03T16:13:00Z", 
    "summary": "We consider the following \"multiway cut packing\" problem in undirected\ngraphs: we are given a graph G=(V,E) and k commodities, each corresponding to a\nset of terminals located at different vertices in the graph; our goal is to\nproduce a collection of cuts {E_1,...,E_k} such that E_i is a multiway cut for\ncommodity i and the maximum load on any edge is minimized. The load on an edge\nis defined to be the number of cuts in the solution crossing the edge. In the\ncapacitated version of the problem the goal is to minimize the maximum relative\nload on any edge--the ratio of the edge's load to its capacity. Multiway cut\npacking arises in the context of graph labeling problems where we are given a\npartial labeling of a set of items and a neighborhood structure over them, and,\ninformally, the goal is to complete the labeling in the most consistent way.\nThis problem was introduced by Rabani, Schulman, and Swamy (SODA'08), who\ndeveloped an O(log n/log log n) approximation for it in general graphs, as well\nas an improved O(log^2 k) approximation in trees. Here n is the number of nodes\nin the graph. We present the first constant factor approximation for this\nproblem in arbitrary undirected graphs. Our approach is based on the\nobservation that every instance of the problem admits a near-optimal laminar\nsolution (that is, one in which no pair of cuts cross each other)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.0906v2", 
    "title": "A linear time algorithm for L(2,1)-labeling of trees", 
    "arxiv-id": "0810.0906v2", 
    "author": "Yushi Uno", 
    "publish": "2008-10-06T08:21:17Z", 
    "summary": "An L(2,1)-labeling of a graph $G$ is an assignment $f$ from the vertex set\n$V(G)$ to the set of nonnegative integers such that $|f(x)-f(y)|\\ge 2$ if $x$\nand $y$ are adjacent and $|f(x)-f(y)|\\ge 1$ if $x$ and $y$ are at distance 2,\nfor all $x$ and $y$ in $V(G)$. A $k$-L(2,1)-labeling is an assignment\n$f:V(G)\\to\\{0,..., k\\}$, and the L(2,1)-labeling problem asks the minimum $k$,\nwhich we denote by $\\lambda(G)$, among all possible assignments. It is known\nthat this problem is NP-hard even for graphs of treewidth 2, and tree is one of\na very few classes for which the problem is polynomially solvable. The running\ntime of the best known algorithm for trees had been $\\mO(\\Delta^{4.5} n)$ for\nmore than a decade, however, an $\\mO(n^{1.75})$-time algorithm has been\nproposed recently, which substantially improved the previous one, where\n$\\Delta$ is the maximum degree of $T$ and $n=|V(T)|$. In this paper, we finally\nestablish a linear time algorithm for L(2,1)-labeling of trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.3438v1", 
    "title": "Efficient Algorithms and Routing Protocols for Handling Transient Single   Node Failures", 
    "arxiv-id": "0810.3438v1", 
    "author": "Teofilo F Gonzalez", 
    "publish": "2008-10-19T22:57:53Z", 
    "summary": "Single node failures represent more than 85% of all node failures in the\ntoday's large communication networks such as the Internet. Also, these node\nfailures are usually transient. Consequently, having the routing paths globally\nrecomputed does not pay off since the failed nodes recover fairly quickly, and\nthe recomputed routing paths need to be discarded. Instead, we develop\nalgorithms and protocols for dealing with such transient single node failures\nby suppressing the failure (instead of advertising it across the network), and\nrouting messages to the destination via alternate paths that do not use the\nfailed node. We compare our solution to that of Ref. [11] wherein the authors\nhave presented a \"Failure Insensitive Routing\" protocol as a proactive recovery\nscheme for handling transient node failures. We show that our algorithms are\nfaster by an order of magnitude while our paths are equally good. We show via\nsimulation results that our paths are usually within 15% of the optimal for\nrandomly generated graph with 100-1000 nodes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.4812v2", 
    "title": "A constructive proof of the Lovasz Local Lemma", 
    "arxiv-id": "0810.4812v2", 
    "author": "Robin A. Moser", 
    "publish": "2008-10-27T14:02:48Z", 
    "summary": "The Lovasz Local Lemma [EL75] is a powerful tool to prove the existence of\ncombinatorial objects meeting a prescribed collection of criteria. The\ntechnique can directly be applied to the satisfiability problem, yielding that\na k-CNF formula in which each clause has common variables with at most 2^(k-2)\nother clauses is always satisfiable. All hitherto known proofs of the Local\nLemma are non-constructive and do thus not provide a recipe as to how a\nsatisfying assignment to such a formula can be efficiently found. In his\nbreakthrough paper [Bec91], Beck demonstrated that if the neighbourhood of each\nclause be restricted to O(2^(k/48)), a polynomial time algorithm for the search\nproblem exists. Alon simplified and randomized his procedure and improved the\nbound to O(2^(k/8)) [Alo91]. Srinivasan presented in [Sri08] a variant that\nachieves a bound of essentially O(2^(k/4)). In [Mos08], we improved this to\nO(2^(k/2)). In the present paper, we give a randomized algorithm that finds a\nsatisfying assignment to every k-CNF formula in which each clause has a\nneighbourhood of at most the asymptotic optimum of 2^(k-5)-1 other clauses and\nthat runs in expected time polynomial in the size of the formula, irrespective\nof k. If k is considered a constant, we can also give a deterministic variant.\nIn contrast to all previous approaches, our analysis does not anymore invoke\nthe standard non-constructive versions of the Local Lemma and can therefore be\nconsidered an alternative, constructive proof of it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.4934v1", 
    "title": "Exponential-Time Approximation of Hard Problems", 
    "arxiv-id": "0810.4934v1", 
    "author": "Mateusz Wykurz", 
    "publish": "2008-10-27T20:18:00Z", 
    "summary": "We study optimization problems that are neither approximable in polynomial\ntime (at least with a constant factor) nor fixed parameter tractable, under\nwidely believed complexity assumptions. Specifically, we focus on Maximum\nIndependent Set, Vertex Coloring, Set Cover, and Bandwidth.\n  In recent years, many researchers design exact exponential-time algorithms\nfor these and other hard problems. The goal is getting the time complexity\nstill of order $O(c^n)$, but with the constant $c$ as small as possible. In\nthis work we extend this line of research and we investigate whether the\nconstant $c$ can be made even smaller when one allows constant factor\napproximation. In fact, we describe a kind of approximation schemes --\ntrade-offs between approximation factor and the time complexity.\n  We study two natural approaches. The first approach consists of designing a\nbacktracking algorithm with a small search tree. We present one result of that\nkind: a $(4r-1)$-approximation of Bandwidth in time $O^*(2^{n/r})$, for any\npositive integer $r$.\n  The second approach uses general transformations from exponential-time exact\nalgorithms to approximations that are faster but still exponential-time. For\nexample, we show that for any reduction rate $r$, one can transform any\n$O^*(c^n)$-time algorithm for Set Cover into a $(1+\\ln r)$-approximation\nalgorithm running in time $O^*(c^{n/r})$. We believe that results of that kind\nextend the applicability of exact algorithms for NP-hard problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.5263v1", 
    "title": "Lower bounds for distributed markov chain problems", 
    "arxiv-id": "0810.5263v1", 
    "author": "Andy Twigg", 
    "publish": "2008-10-29T12:52:59Z", 
    "summary": "We study the worst-case communication complexity of distributed algorithms\ncomputing a path problem based on stationary distributions of random walks in a\nnetwork $G$ with the caveat that $G$ is also the communication network. The\nproblem is a natural generalization of shortest path lengths to expected path\nlengths, and represents a model used in many practical applications such as\npagerank and eigentrust as well as other problems involving Markov chains\ndefined by networks.\n  For the problem of computing a single stationary probability, we prove an\n$\\Omega(n^2 \\log n)$ bits lower bound; the trivial centralized algorithm costs\n$O(n^3)$ bits and no known algorithm beats this. We also prove lower bounds for\nthe related problems of approximately computing the stationary probabilities,\ncomputing only the ranking of the nodes, and computing the node with maximal\nrank. As a corollary, we obtain lower bounds for labelling schemes for the\nhitting time between two nodes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10732-010-9133-3", 
    "link": "http://arxiv.org/pdf/0810.5477v1", 
    "title": "Worst-case time decremental connectivity and k-edge witness", 
    "arxiv-id": "0810.5477v1", 
    "author": "Andrew Twigg", 
    "publish": "2008-10-30T12:15:33Z", 
    "summary": "We give a simple algorithm for decremental graph connectivity that handles\nedge deletions in worst-case time $O(k \\log n)$ and connectivity queries in\n$O(\\log k)$, where $k$ is the number of edges deleted so far, and uses\nworst-case space $O(m^2)$. We use this to give an algorithm for $k$-edge\nwitness (``does the removal of a given set of $k$ edges disconnect two vertices\n$u,v$?'') with worst-case time $O(k^2 \\log n)$ and space $O(k^2 n^2)$. For $k =\no(\\sqrt{n})$ these improve the worst-case $O(\\sqrt{n})$ bound for deletion due\nto Eppstein et al. We also give a decremental connectivity algorithm using\n$O(n^2 \\log n / \\log \\log n)$ space, whose time complexity depends on the\ntoughness and independence number of the input graph. Finally, we show how to\nconstruct a distributed data structure for \\kvw by giving a labeling scheme.\nThis is the first data structure for \\kvw that can efficiently distributed\nwithout just giving each vertex a copy of the whole structure. Its complexity\ndepends on being able to construct a linear layout with good properties."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.2572v2", 
    "title": "An Efficient Algorithm for Partial Order Production", 
    "arxiv-id": "0811.2572v2", 
    "author": "J. Ian Munro", 
    "publish": "2008-11-17T16:23:45Z", 
    "summary": "We consider the problem of partial order production: arrange the elements of\nan unknown totally ordered set T into a target partially ordered set S, by\ncomparing a minimum number of pairs in T. Special cases include sorting by\ncomparisons, selection, multiple selection, and heap construction.\n  We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the\nworst case. Here, n denotes the size of the ground sets, and ITLB denotes a\nnatural information-theoretic lower bound on the number of comparisons needed\nto produce the target partial order.\n  Our approach is to replace the target partial order by a weak order (that is,\na partial order with a layered structure) extending it, without increasing the\ninformation theoretic lower bound too much. We then solve the problem by\napplying an efficient multiple selection algorithm. The overall complexity of\nour algorithm is polynomial. This answers a question of Yao (SIAM J. Comput.\n18, 1989).\n  We base our analysis on the entropy of the target partial order, a quantity\nthat can be efficiently computed and provides a good estimate of the\ninformation-theoretic lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3062v1", 
    "title": "Dynamic External Hashing: The Limit of Buffering", 
    "arxiv-id": "0811.3062v1", 
    "author": "Qin Zhang", 
    "publish": "2008-11-19T08:11:14Z", 
    "summary": "Hash tables are one of the most fundamental data structures in computer\nscience, in both theory and practice. They are especially useful in external\nmemory, where their query performance approaches the ideal cost of just one\ndisk access. Knuth gave an elegant analysis showing that with some simple\ncollision resolution strategies such as linear probing or chaining, the\nexpected average number of disk I/Os of a lookup is merely $1+1/2^{\\Omega(b)}$,\nwhere each I/O can read a disk block containing $b$ items. Inserting a new item\ninto the hash table also costs $1+1/2^{\\Omega(b)}$ I/Os, which is again almost\nthe best one can do if the hash table is entirely stored on disk. However, this\nassumption is unrealistic since any algorithm operating on an external hash\ntable must have some internal memory (at least $\\Omega(1)$ blocks) to work\nwith. The availability of a small internal memory buffer can dramatically\nreduce the amortized insertion cost to $o(1)$ I/Os for many external memory\ndata structures. In this paper we study the inherent query-insertion tradeoff\nof external hash tables in the presence of a memory buffer. In particular, we\nshow that for any constant $c>1$, if the query cost is targeted at\n$1+O(1/b^{c})$ I/Os, then it is not possible to support insertions in less than\n$1-O(1/b^{\\frac{c-1}{4}})$ I/Os amortized, which means that the memory buffer\nis essentially useless. While if the query cost is relaxed to $1+O(1/b^{c})$\nI/Os for any constant $c<1$, there is a simple dynamic hash table with $o(1)$\ninsertion cost. These results also answer the open question recently posed by\nJensen and Pagh."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3448v2", 
    "title": "Binar Sort: A Linear Generalized Sorting Algorithm", 
    "arxiv-id": "0811.3448v2", 
    "author": "William F. Gilreath", 
    "publish": "2008-11-21T01:38:09Z", 
    "summary": "Sorting is a common and ubiquitous activity for computers. It is not\nsurprising that there exist a plethora of sorting algorithms. For all the\nsorting algorithms, it is an accepted performance limit that sorting algorithms\nare linearithmic or O(N lg N). The linearithmic lower bound in performance\nstems from the fact that the sorting algorithms use the ordering property of\nthe data. The sorting algorithm uses comparison by the ordering property to\narrange the data elements from an initial permutation into a sorted\npermutation.\n  Linear O(N) sorting algorithms exist, but use a priori knowledge of the data\nto use a specific property of the data and thus have greater performance. In\ncontrast, the linearithmic sorting algorithms are generalized by using a\nuniversal property of data-comparison, but have a linearithmic performance\nlower bound. The trade-off in sorting algorithms is generality for performance\nby the chosen property used to sort the data elements.\n  A general-purpose, linear sorting algorithm in the context of the trade-off\nof performance for generality at first consideration seems implausible. But,\nthere is an implicit assumption that only the ordering property is universal.\nBut, as will be discussed and examined, it is not the only universal property\nfor data elements. The binar sort is a general-purpose sorting algorithm that\nuses this other universal property to sort linearly."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3449v1", 
    "title": "Binar Shuffle Algorithm: Shuffling Bit by Bit", 
    "arxiv-id": "0811.3449v1", 
    "author": "William F. Gilreath", 
    "publish": "2008-11-21T01:45:50Z", 
    "summary": "Frequently, randomly organized data is needed to avoid an anomalous operation\nof other algorithms and computational processes. An analogy is that a deck of\ncards is ordered within the pack, but before a game of poker or solitaire the\ndeck is shuffled to create a random permutation. Shuffling is used to assure\nthat an aggregate of data elements for a sequence S is randomly arranged, but\navoids an ordered or partially ordered permutation.\n  Shuffling is the process of arranging data elements into a random\npermutation. The sequence S as an aggregation of N data elements, there are N!\npossible permutations. For the large number of possible permutations, two of\nthe possible permutations are for a sorted or ordered placement of data\nelements--both an ascending and descending sorted permutation. Shuffling must\navoid inadvertently creating either an ascending or descending permutation.\n  Shuffling is frequently coupled to another algorithmic function --\npseudo-random number generation. The efficiency and quality of the shuffle is\ndirectly dependent upon the random number generation algorithm utilized. A more\neffective and efficient method of shuffling is to use parameterization to\nconfigure the shuffle, and to shuffle into sub-arrays by utilizing the encoding\nof the data elements. The binar shuffle algorithm uses the encoding of the data\nelements and parameterization to avoid any direct coupling to a random number\ngeneration algorithm, but still remain a linear O(N) shuffle algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3490v2", 
    "title": "Faster Approximate String Matching for Short Patterns", 
    "arxiv-id": "0811.3490v2", 
    "author": "Philip Bille", 
    "publish": "2008-11-21T08:52:59Z", 
    "summary": "We study the classical approximate string matching problem, that is, given\nstrings $P$ and $Q$ and an error threshold $k$, find all ending positions of\nsubstrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$\nhave lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with\nword size $w \\geq \\log n$ we present an algorithm using time $$ O(nk \\cdot\n\\min(\\frac{\\log^2 m}{\\log n},\\frac{\\log^2 m\\log w}{w}) + n) $$ When $P$ is\nshort, namely, $m = 2^{o(\\sqrt{\\log n})}$ or $m = 2^{o(\\sqrt{w/\\log w})}$ this\nimproves the previously best known time bounds for the problem. The result is\nachieved using a novel implementation of the Landau-Vishkin algorithm based on\ntabulation and word-level parallelism."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3602v1", 
    "title": "Low-Memory Adaptive Prefix Coding", 
    "arxiv-id": "0811.3602v1", 
    "author": "Yakov Nekrich", 
    "publish": "2008-11-21T18:23:00Z", 
    "summary": "In this paper we study the adaptive prefix coding problem in cases where the\nsize of the input alphabet is large. We present an online prefix coding\nalgorithm that uses $O(\\sigma^{1 / \\lambda + \\epsilon}) $ bits of space for any\nconstants $\\eps>0$, $\\lambda>1$, and encodes the string of symbols in $O(\\log\n\\log \\sigma)$ time per symbol \\emph{in the worst case}, where $\\sigma$ is the\nsize of the alphabet. The upper bound on the encoding length is $\\lambda n H\n(s) +(\\lambda \\ln 2 + 2 + \\epsilon) n + O (\\sigma^{1 / \\lambda} \\log^2 \\sigma)$\nbits."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3779v1", 
    "title": "Finding Sparse Cuts Locally Using Evolving Sets", 
    "arxiv-id": "0811.3779v1", 
    "author": "Yuval Peres", 
    "publish": "2008-11-23T22:39:38Z", 
    "summary": "A {\\em local graph partitioning algorithm} finds a set of vertices with small\nconductance (i.e. a sparse cut) by adaptively exploring part of a large graph\n$G$, starting from a specified vertex. For the algorithm to be local, its\ncomplexity must be bounded in terms of the size of the set that it outputs,\nwith at most a weak dependence on the number $n$ of vertices in $G$. Previous\nlocal partitioning algorithms find sparse cuts using random walks and\npersonalized PageRank. In this paper, we introduce a randomized local\npartitioning algorithm that finds a sparse cut by simulating the {\\em\nvolume-biased evolving set process}, which is a Markov chain on sets of\nvertices. We prove that for any set of vertices $A$ that has conductance at\nmost $\\phi$, for at least half of the starting vertices in $A$ our algorithm\nwill output (with probability at least half), a set of conductance\n$O(\\phi^{1/2} \\log^{1/2} n)$. We prove that for a given run of the algorithm,\nthe expected ratio between its computational complexity and the volume of the\nset that it outputs is $O(\\phi^{-1/2} polylog(n))$. In comparison, the best\nprevious local partitioning algorithm, due to Andersen, Chung, and Lang, has\nthe same approximation guarantee, but a larger ratio of $O(\\phi^{-1}\npolylog(n))$ between the complexity and output volume. Using our local\npartitioning algorithm as a subroutine, we construct a fast algorithm for\nfinding balanced cuts. Given a fixed value of $\\phi$, the resulting algorithm\nhas complexity $O((m+n\\phi^{-1/2}) polylog(n))$ and returns a cut with\nconductance $O(\\phi^{1/2} \\log^{1/2} n)$ and volume at least $v_{\\phi}/2$,\nwhere $v_{\\phi}$ is the largest volume of any set with conductance at most\n$\\phi$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.0146v4", 
    "title": "Lower Bounds on Performance of Metric Tree Indexing Schemes for Exact   Similarity Search in High Dimensions", 
    "arxiv-id": "0812.0146v4", 
    "author": "Vladimir Pestov", 
    "publish": "2008-11-30T15:17:22Z", 
    "summary": "Within a mathematically rigorous model, we analyse the curse of\ndimensionality for deterministic exact similarity search in the context of\npopular indexing schemes: metric trees. The datasets $X$ are sampled randomly\nfrom a domain $\\Omega$, equipped with a distance, $\\rho$, and an underlying\nprobability distribution, $\\mu$. While performing an asymptotic analysis, we\nsend the intrinsic dimension $d$ of $\\Omega$ to infinity, and assume that the\nsize of a dataset, $n$, grows superpolynomially yet subexponentially in $d$.\nExact similarity search refers to finding the nearest neighbour in the dataset\n$X$ to a query point $\\omega\\in\\Omega$, where the query points are subject to\nthe same probability distribution $\\mu$ as datapoints. Let $\\mathscr F$ denote\na class of all 1-Lipschitz functions on $\\Omega$ that can be used as decision\nfunctions in constructing a hierarchical metric tree indexing scheme. Suppose\nthe VC dimension of the class of all sets $\\{\\omega\\colon f(\\omega)\\geq a\\}$,\n$a\\in\\R$ is $o(n^{1/4}/\\log^2n)$. (In view of a 1995 result of Goldberg and\nJerrum, even a stronger complexity assumption $d^{O(1)}$ is reasonable.) We\ndeduce the $\\Omega(n^{1/4})$ lower bound on the expected average case\nperformance of hierarchical metric-tree based indexing schemes for exact\nsimilarity search in $(\\Omega,X)$. In paricular, this bound is superpolynomial\nin $d$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.0209v1", 
    "title": "Optimal Tracking of Distributed Heavy Hitters and Quantiles", 
    "arxiv-id": "0812.0209v1", 
    "author": "Qin Zhang", 
    "publish": "2008-12-01T03:51:12Z", 
    "summary": "We consider the the problem of tracking heavy hitters and quantiles in the\ndistributed streaming model. The heavy hitters and quantiles are two important\nstatistics for characterizing a data distribution. Let $A$ be a multiset of\nelements, drawn from the universe $U=\\{1,...,u\\}$. For a given $0 \\le \\phi \\le\n1$, the $\\phi$-heavy hitters are those elements of $A$ whose frequency in $A$\nis at least $\\phi |A|$; the $\\phi$-quantile of $A$ is an element $x$ of $U$\nsuch that at most $\\phi|A|$ elements of $A$ are smaller than $A$ and at most\n$(1-\\phi)|A|$ elements of $A$ are greater than $x$. Suppose the elements of $A$\nare received at $k$ remote {\\em sites} over time, and each of the sites has a\ntwo-way communication channel to a designated {\\em coordinator}, whose goal is\nto track the set of $\\phi$-heavy hitters and the $\\phi$-quantile of $A$\napproximately at all times with minimum communication. We give tracking\nalgorithms with worst-case communication cost $O(k/\\eps \\cdot \\log n)$ for both\nproblems, where $n$ is the total number of items in $A$, and $\\eps$ is the\napproximation error. This substantially improves upon the previous known\nalgorithms. We also give matching lower bounds on the communication costs for\nboth problems, showing that our algorithms are optimal. We also consider a more\ngeneral version of the problem where we simultaneously track the\n$\\phi$-quantiles for all $0 \\le \\phi \\le 1$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.1012v3", 
    "title": "Adaptive Uncertainty Resolution in Bayesian Combinatorial Optimization   Problems", 
    "arxiv-id": "0812.1012v3", 
    "author": "Kamesh Munagala", 
    "publish": "2008-12-04T19:48:16Z", 
    "summary": "In several applications such as databases, planning, and sensor networks,\nparameters such as selectivity, load, or sensed values are known only with some\nassociated uncertainty. The performance of such a system (as captured by some\nobjective function over the parameters) is significantly improved if some of\nthese parameters can be probed or observed. In a resource constrained\nsituation, deciding which parameters to observe in order to optimize system\nperformance itself becomes an interesting and important optimization problem.\nThis general problem is the focus of this paper.\n  One of the most important considerations in this framework is whether\nadaptivity is required for the observations. Adaptive observations introduce\nblocking or sequential operations in the system whereas non-adaptive\nobservations can be performed in parallel. One of the important questions in\nthis regard is to characterize the benefit of adaptivity for probes and\nobservation.\n  We present general techniques for designing constant factor approximations to\nthe optimal observation schemes for several widely used scheduling and metric\nobjective functions. We show a unifying technique that relates this\noptimization problem to the outlier version of the corresponding deterministic\noptimization. By making this connection, our technique shows constant factor\nupper bounds for the benefit of adaptivity of the observation schemes. We show\nthat while probing yields significant improvement in the objective function,\nbeing adaptive about the probing is not beneficial beyond constant factors."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74061-2_12", 
    "link": "http://arxiv.org/pdf/0812.2011v1", 
    "title": "Accelerated Data-Flow Analysis", 
    "arxiv-id": "0812.2011v1", 
    "author": "Gregoire Sutre", 
    "publish": "2008-12-10T20:08:08Z", 
    "summary": "Acceleration in symbolic verification consists in computing the exact effect\nof some control-flow loops in order to speed up the iterative fix-point\ncomputation of reachable states. Even if no termination guarantee is provided\nin theory, successful results were obtained in practice by different tools\nimplementing this framework. In this paper, the acceleration framework is\nextended to data-flow analysis. Compared to a classical\nwidening/narrowing-based abstract interpretation, the loss of precision is\ncontrolled here by the choice of the abstract domain and does not depend on the\nway the abstract value is computed. Our approach is geared towards precision,\nbut we don't loose efficiency on the way. Indeed, we provide a cubic-time\nacceleration-based algorithm for solving interval constraints with full\nmultiplication."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2014v1", 
    "title": "Convex Hull of Arithmetic Automata", 
    "arxiv-id": "0812.2014v1", 
    "author": "J\u00e9r\u00f4me Leroux", 
    "publish": "2008-12-10T20:33:27Z", 
    "summary": "Arithmetic automata recognize infinite words of digits denoting\ndecompositions of real and integer vectors. These automata are known expressive\nand efficient enough to represent the whole set of solutions of complex linear\nconstraints combining both integral and real variables. In this paper, the\nclosed convex hull of arithmetic automata is proved rational polyhedral.\nMoreover an algorithm computing the linear constraints defining these convex\nset is provided. Such an algorithm is useful for effectively extracting\ngeometrical properties of the whole set of solutions of complex constraints\nsymbolically represented by arithmetic automata."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2599v1", 
    "title": "Learning Low Rank Matrices from O(n) Entries", 
    "arxiv-id": "0812.2599v1", 
    "author": "Sewoong Oh", 
    "publish": "2008-12-14T18:30:44Z", 
    "summary": "How many random entries of an n by m, rank r matrix are necessary to\nreconstruct the matrix within an accuracy d? We address this question in the\ncase of a random matrix with bounded rank, whereby the observed entries are\nchosen uniformly at random. We prove that, for any d>0, C(r,d)n observations\nare sufficient. Finally we discuss the question of reconstructing the matrix\nefficiently, and demonstrate through extensive simulations that this task can\nbe accomplished in nPoly(log n) operations, for small rank."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2775v3", 
    "title": "Optimal Succinctness for Range Minimum Queries", 
    "arxiv-id": "0812.2775v3", 
    "author": "Johannes Fischer", 
    "publish": "2008-12-15T12:03:31Z", 
    "summary": "For a static array A of n ordered objects, a range minimum query asks for the\nposition of the minimum between two specified array indices. We show how to\npreprocess A into a scheme of size 2n+o(n) bits that allows to answer range\nminimum queries on A in constant time. This space is asymptotically optimal in\nthe important setting where access to A is not permitted after the\npreprocessing step. Our scheme can be computed in linear time, using only n +\no(n) additional bits at construction time. In interesting by-product is that we\nalso improve on LCA-computation in BPS- or DFUDS-encoded trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2851v2", 
    "title": "The Violation Heap: A Relaxed Fibonacci-Like Heap", 
    "arxiv-id": "0812.2851v2", 
    "author": "Amr Elmasry", 
    "publish": "2008-12-15T16:16:58Z", 
    "summary": "We give a priority queue that achieves the same amortized bounds as Fibonacci\nheaps. Namely, find-min requires O(1) worst-case time, insert, meld and\ndecrease-key require O(1) amortized time, and delete-min requires $O(\\log n)$\namortized time. Our structure is simple and promises an efficient practical\nbehavior when compared to other known Fibonacci-like heaps. The main idea\nbehind our construction is to propagate rank updates instead of performing\ncascaded cuts following a decrease-key operation, allowing for a relaxed\nstructure."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2868v2", 
    "title": "Minimax Trees in Linear Time", 
    "arxiv-id": "0812.2868v2", 
    "author": "Travis Gagie", 
    "publish": "2008-12-15T17:15:51Z", 
    "summary": "A minimax tree is similar to a Huffman tree except that, instead of\nminimizing the weighted average of the leaves' depths, it minimizes the maximum\nof any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees\nand gave a Huffman-like, $\\Oh{n \\log n}$-time algorithm for building them.\nDrmota and Szpankowski (2002) gave another $\\Oh{n \\log n}$-time algorithm,\nwhich checks the Kraft Inequality in each step of a binary search. In this\npaper we show how Drmota and Szpankowski's algorithm can be made to run in\nlinear time on a word RAM with (\\Omega (\\log n))-bit words. We also discuss how\nour solution applies to problems in data compression, group testing and circuit\ndesign."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.3702v1", 
    "title": "Algorithmic and Statistical Challenges in Modern Large-Scale Data   Analysis are the Focus of MMDS 2008", 
    "arxiv-id": "0812.3702v1", 
    "author": "Gunnar E. Carlsson", 
    "publish": "2008-12-19T03:53:03Z", 
    "summary": "The 2008 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2008),\nsponsored by the NSF, DARPA, LinkedIn, and Yahoo!, was held at Stanford\nUniversity, June 25--28. The goals of MMDS 2008 were (1) to explore novel\ntechniques for modeling and analyzing massive, high-dimensional, and\nnonlinearly-structured scientific and internet data sets; and (2) to bring\ntogether computer scientists, statisticians, mathematicians, and data analysis\npractitioners to promote cross-fertilization of ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4293v2", 
    "title": "An Improved Approximation Algorithm for the Column Subset Selection   Problem", 
    "arxiv-id": "0812.4293v2", 
    "author": "Petros Drineas", 
    "publish": "2008-12-22T21:16:55Z", 
    "summary": "We consider the problem of selecting the best subset of exactly $k$ columns\nfrom an $m \\times n$ matrix $A$. We present and analyze a novel two-stage\nalgorithm that runs in $O(\\min\\{mn^2,m^2n\\})$ time and returns as output an $m\n\\times k$ matrix $C$ consisting of exactly $k$ columns of $A$. In the first\n(randomized) stage, the algorithm randomly selects $\\Theta(k \\log k)$ columns\naccording to a judiciously-chosen probability distribution that depends on\ninformation in the top-$k$ right singular subspace of $A$. In the second\n(deterministic) stage, the algorithm applies a deterministic column-selection\nprocedure to select and return exactly $k$ columns from the set of columns\nselected in the first stage. Let $C$ be the $m \\times k$ matrix containing\nthose $k$ columns, let $P_C$ denote the projection matrix onto the span of\nthose columns, and let $A_k$ denote the best rank-$k$ approximation to the\nmatrix $A$. Then, we prove that, with probability at least 0.8, $$ \\FNorm{A -\nP_CA} \\leq \\Theta(k \\log^{1/2} k) \\FNorm{A-A_k}. $$ This Frobenius norm bound\nis only a factor of $\\sqrt{k \\log k}$ worse than the best previously existing\nexistential result and is roughly $O(\\sqrt{k!})$ better than the best previous\nalgorithmic result for the Frobenius norm version of this Column Subset\nSelection Problem (CSSP). We also prove that, with probability at least 0.8, $$\n\\TNorm{A - P_CA} \\leq \\Theta(k \\log^{1/2} k)\\TNorm{A-A_k} +\n\\Theta(k^{3/4}\\log^{1/4}k)\\FNorm{A-A_k}. $$ This spectral norm bound is not\ndirectly comparable to the best previously existing bounds for the spectral\nnorm version of this CSSP. Our bound depends on $\\FNorm{A-A_k}$, whereas\nprevious results depend on $\\sqrt{n-k}\\TNorm{A-A_k}$; if these two quantities\nare comparable, then our bound is asymptotically worse by a $(k \\log k)^{1/4}$\nfactor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4442v1", 
    "title": "An $O(k^{3} log n)$-Approximation Algorithm for Vertex-Connectivity   Survivable Network Design", 
    "arxiv-id": "0812.4442v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2008-12-23T19:04:25Z", 
    "summary": "In the Survivable Network Design problem (SNDP), we are given an undirected\ngraph $G(V,E)$ with costs on edges, along with a connectivity requirement\n$r(u,v)$ for each pair $u,v$ of vertices. The goal is to find a minimum-cost\nsubset $E^*$ of edges, that satisfies the given set of pairwise connectivity\nrequirements. In the edge-connectivity version we need to ensure that there are\n$r(u,v)$ edge-disjoint paths for every pair $u, v$ of vertices, while in the\nvertex-connectivity version the paths are required to be vertex-disjoint. The\nedge-connectivity version of SNDP is known to have a 2-approximation. However,\nno non-trivial approximation algorithm has been known so far for the vertex\nversion of SNDP, except for special cases of the problem. We present an\nextremely simple algorithm to achieve an $O(k^3 \\log n)$-approximation for this\nproblem, where $k$ denotes the maximum connectivity requirement, and $n$\ndenotes the number of vertices. We also give a simple proof of the recently\ndiscovered $O(k^2 \\log n)$-approximation result for the single-source version\nof vertex-connectivity SNDP. We note that in both cases, our analysis in fact\nyields slightly better guarantees in that the $\\log n$ term in the\napproximation guarantee can be replaced with a $\\log \\tau$ term where $\\tau$\ndenotes the number of distinct vertices that participate in one or more pairs\nwith a positive connectivity requirement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4547v2", 
    "title": "Random Projections for the Nonnegative Least-Squares Problem", 
    "arxiv-id": "0812.4547v2", 
    "author": "Petros Drineas", 
    "publish": "2008-12-24T16:43:22Z", 
    "summary": "Constrained least-squares regression problems, such as the Nonnegative Least\nSquares (NNLS) problem, where the variables are restricted to take only\nnonnegative values, often arise in applications. Motivated by the recent\ndevelopment of the fast Johnson-Lindestrauss transform, we present a fast\nrandom projection type approximation algorithm for the NNLS problem. Our\nalgorithm employs a randomized Hadamard transform to construct a much smaller\nNNLS problem and solves this smaller problem using a standard NNLS solver. We\nprove that our approach finds a nonnegative solution vector that, with high\nprobability, is close to the optimum nonnegative solution in a relative error\napproximation sense. We experimentally evaluate our approach on a large\ncollection of term-document data and verify that it does offer considerable\nspeedups without a significant loss in accuracy. Our analysis is based on a\nnovel random projection type result that might be of independent interest. In\nparticular, given a tall and thin matrix $\\Phi \\in \\mathbb{R}^{n \\times d}$ ($n\n\\gg d$) and a vector $y \\in \\mathbb{R}^d$, we prove that the Euclidean length\nof $\\Phi y$ can be estimated very accurately by the Euclidean length of\n$\\tilde{\\Phi}y$, where $\\tilde{\\Phi}$ consists of a small subset of\n(appropriately rescaled) rows of $\\Phi$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4919v1", 
    "title": "Obtaining a Planar Graph by Vertex Deletion", 
    "arxiv-id": "0812.4919v1", 
    "author": "Ildik\u00f3 Schlotter", 
    "publish": "2008-12-29T14:57:14Z", 
    "summary": "In the k-Apex problem the task is to find at most k vertices whose deletion\nmakes the given graph planar. The graphs for which there exists a solution form\na minor closed class of graphs, hence by the deep results of Robertson and\nSeymour, there is an O(n^3) time algorithm for every fixed value of k. However,\nthe proof is extremely complicated and the constants hidden by the big-O\nnotation are huge. Here we give a much simpler algorithm for this problem with\nquadratic running time, by iteratively reducing the input graph and then\napplying techniques for graphs of bounded treewidth."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0205v1", 
    "title": "On Allocating Goods to Maximize Fairness", 
    "arxiv-id": "0901.0205v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2009-01-02T01:24:26Z", 
    "summary": "Given a set of $m$ agents and a set of $n$ items, where agent $A$ has utility\n$u_{A,i}$ for item $i$, our goal is to allocate items to agents to maximize\nfairness. Specifically, the utility of an agent is the sum of its utilities for\nitems it receives, and we seek to maximize the minimum utility of any agent.\nWhile this problem has received much attention recently, its approximability\nhas not been well-understood thus far: the best known approximation algorithm\nachieves an $\\tilde{O}(\\sqrt{m})$-approximation, and in contrast, the best\nknown hardness of approximation stands at 2.\n  Our main result is an approximation algorithm that achieves an\n$\\tilde{O}(n^{\\eps})$ approximation for any $\\eps=\\Omega(\\log\\log n/\\log n)$ in\ntime $n^{O(1/\\eps)}$. In particular, we obtain poly-logarithmic approximation\nin quasi-polynomial time, and for any constant $\\eps > 0$, we obtain\n$O(n^{\\eps})$ approximation in polynomial time. An interesting aspect of our\nalgorithm is that we use as a building block a linear program whose integrality\ngap is $\\Omega(\\sqrt m)$. We bypass this obstacle by iteratively using the\nsolutions produced by the LP to construct new instances with significantly\nsmaller integrality gaps, eventually obtaining the desired approximation.\n  We also investigate the special case of the problem, where every item has a\nnon-zero utility for at most two agents. We show that even in this restricted\nsetting the problem is hard to approximate upto any factor better tha 2, and\nshow a factor $(2+\\eps)$-approximation algorithm running in time\n$poly(n,1/\\eps)$ for any $\\eps>0$. This special case can be cast as a graph\nedge orientation problem, and our algorithm can be viewed as a generalization\nof Eulerian orientations to weighted graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0501v2", 
    "title": "Interprocedural Dataflow Analysis over Weight Domains with Infinite   Descending Chains", 
    "arxiv-id": "0901.0501v2", 
    "author": "Stefan Kiefer", 
    "publish": "2009-01-05T16:47:21Z", 
    "summary": "We study generalized fixed-point equations over idempotent semirings and\nprovide an efficient algorithm for the detection whether a sequence of Kleene's\niterations stabilizes after a finite number of steps. Previously known\napproaches considered only bounded semirings where there are no infinite\ndescending chains. The main novelty of our work is that we deal with semirings\nwithout the boundedness restriction. Our study is motivated by several\napplications from interprocedural dataflow analysis. We demonstrate how the\nreachability problem for weighted pushdown automata can be reduced to solving\nequations in the framework mentioned above and we describe a few applications\nto demonstrate its usability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0930v2", 
    "title": "An \u03a9(n log n) lower bound for computing the sum of even-ranked   elements", 
    "arxiv-id": "0901.0930v2", 
    "author": "Jan Tusch", 
    "publish": "2009-01-07T21:55:59Z", 
    "summary": "Given a sequence A of 2n real numbers, the Even-Rank-Sum problem asks for the\nsum of the n values that are at the even positions in the sorted order of the\nelements in A. We prove that, in the algebraic computation-tree model, this\nproblem has time complexity \\Theta(n log n). This solves an open problem posed\nby Michael Shamos at the Canadian Conference on Computational Geometry in 2008."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.1761v1", 
    "title": "Towards Optimal Range Medians", 
    "arxiv-id": "0901.1761v1", 
    "author": "Peter Sanders", 
    "publish": "2009-01-13T14:46:50Z", 
    "summary": "We consider the following problem: given an unsorted array of $n$ elements,\nand a sequence of intervals in the array, compute the median in each of the\nsubarrays defined by the intervals. We describe a simple algorithm which uses\nO(n) space and needs $O(n\\log k + k\\log n)$ time to answer the first $k$\nqueries. This improves previous algorithms by a logarithmic factor and matches\na lower bound for $k=O(n)$.\n  Since the algorithm decomposes the range of element values rather than the\narray, it has natural generalizations to higher dimensional problems -- it\nreduces a range median query to a logarithmic number of range counting queries."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2645v2", 
    "title": "On some simplicial elimination schemes for chordal graphs", 
    "arxiv-id": "0901.2645v2", 
    "author": "Vincent Limouzy", 
    "publish": "2009-01-17T15:23:29Z", 
    "summary": "We present here some results on particular elimination schemes for chordal\ngraphs, namely we show that for any chordal graph we can construct in linear\ntime a simplicial elimination scheme starting with a pending maximal clique\nattached via a minimal separator maximal (resp. minimal) under inclusion among\nall minimal separators."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2897v2", 
    "title": "Online validation of the pi and pi' failure functions", 
    "arxiv-id": "0901.2897v2", 
    "author": "Lukasz Jez", 
    "publish": "2009-01-19T20:12:21Z", 
    "summary": "Let pi_w denote the failure function of the Morris-Pratt algorithm for a word\nw. In this paper we study the following problem: given an integer array\nA[1..n], is there a word w over arbitrary alphabet such that A[i]=pi_w[i] for\nall i? Moreover, what is the minimum required cardinality of the alphabet? We\ngive a real time linear algorithm for this problem in the unit-cost RAM model\nwith \\Theta(log n) bits word size. Our algorithm returns a word w over minimal\nalphabet such that pi_w = A as well and uses just o(n) words of memory. Then we\nconsider function pi' instead of pi and give an online O(n log n) algorithm for\nthis case. This is the first polynomial algorithm for online version of this\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2900v1", 
    "title": "An O(log(n)) Fully Dynamic Algorithm for Maximum matching in a tree", 
    "arxiv-id": "0901.2900v1", 
    "author": "Ankit Sharma", 
    "publish": "2009-01-19T17:30:28Z", 
    "summary": "In this paper, we have developed a fully-dynamic algorithm for maintaining\ncardinality of maximum-matching in a tree using the construction of top-trees.\nThe time complexities are as follows:\n  1. Initialization Time: $O(n(log(n)))$ to build the Top-tree. 2. Update Time:\n$O(log(n))$ 3. Query Time: O(1) to query the cardinality of maximum-matching\nand $O(log(n))$ to find if a particular edge is matched."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.4002v1", 
    "title": "Max Edge Coloring of Trees", 
    "arxiv-id": "0901.4002v1", 
    "author": "Vangelis Th. Paschos", 
    "publish": "2009-01-26T13:27:34Z", 
    "summary": "We study the weighted generalization of the edge coloring problem where the\nweight of each color class (matching) equals to the weight of its heaviest edge\nand the goal is to minimize the sum of the colors' weights. We present a\n3/2-approximation algorithm for trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.4201v1", 
    "title": "Peer to Peer Optimistic Collaborative Editing on XML-like trees", 
    "arxiv-id": "0901.4201v1", 
    "author": "St\u00e9phane Martin", 
    "publish": "2009-01-27T09:09:52Z", 
    "summary": "Collaborative editing consists in editing a common document shared by several\nindependent sites. This may give rise to conficts when two different users\nperform simultaneous uncompatible operations. Centralized systems solve this\nproblem by using locks that prevent some modifications to occur and leave the\nresolution of confict to users. On the contrary, peer to peer (P2P) editing\ndoesn't allow locks and the optimistic approach uses a Integration\nTransformation IT that reconciliates the conficting operations and ensures\nconvergence (all copies are identical on each site). Two properties TP1 and\nTP2, relating the set of allowed operations Op and the transformation IT, have\nbeen shown to ensure the correctness of the process. The choice of the set Op\nis crucial to define an integration operation that satisfies TP1 and TP2. Many\nexisting algorithms don't satisfy these properties and are indeed incorrect\ni.e. convergence is not guaranteed. No algorithm enjoying both properties is\nknown for strings and little work has been done for XML trees in a pure P2P\nframework (that doesn't use time-stamps for instance). We focus on editing\nunranked unordered labeled trees, so-called XML-like trees that are considered\nfor instance in the Harmony pro ject. We show that no transformation satisfying\nTP1 and TP2 can exist for a first set of operations but we show that TP1 and\nTP2 hold for a richer set of operations. We show how to combine our approach\nwith any convergent editing process on strings (not necessarily based on\nintegration transformation) to get a convergent process."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.0140v2", 
    "title": "Graph Sparsification in the Semi-streaming Model", 
    "arxiv-id": "0902.0140v2", 
    "author": "Sudipto Guha", 
    "publish": "2009-02-01T16:50:53Z", 
    "summary": "Analyzing massive data sets has been one of the key motivations for studying\nstreaming algorithms. In recent years, there has been significant progress in\nanalysing distributions in a streaming setting, but the progress on graph\nproblems has been limited. A main reason for this has been the existence of\nlinear space lower bounds for even simple problems such as determining the\nconnectedness of a graph. However, in many new scenarios that arise from social\nand other interaction networks, the number of vertices is significantly less\nthan the number of edges. This has led to the formulation of the semi-streaming\nmodel where we assume that the space is (near) linear in the number of vertices\n(but not necessarily the edges), and the edges appear in an arbitrary (and\npossibly adversarial) order.\n  In this paper we focus on graph sparsification, which is one of the major\nbuilding blocks in a variety of graph algorithms. There has been a long history\nof (non-streaming) sampling algorithms that provide sparse graph approximations\nand it a natural question to ask if the sparsification can be achieved using a\nsmall space, and in addition using a single pass over the data? The question is\ninteresting from the standpoint of both theory and practice and we answer the\nquestion in the affirmative, by providing a one pass\n$\\tilde{O}(n/\\epsilon^{2})$ space algorithm that produces a sparsification that\napproximates each cut to a $(1+\\epsilon)$ factor. We also show that $\\Omega(n\n\\log \\frac1\\epsilon)$ space is necessary for a one pass streaming algorithm to\napproximate the min-cut, improving upon the $\\Omega(n)$ lower bound that arises\nfrom lower bounds for testing connectivity."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1038v1", 
    "title": "Compressed Representations of Permutations, and Applications", 
    "arxiv-id": "0902.1038v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2009-02-06T09:48:32Z", 
    "summary": "We explore various techniques to compress a permutation $\\pi$ over n\nintegers, taking advantage of ordered subsequences in $\\pi$, while supporting\nits application $\\pi$(i) and the application of its inverse $\\pi^{-1}(i)$ in\nsmall time. Our compression schemes yield several interesting byproducts, in\nmany cases matching, improving or extending the best existing results on\napplications such as the encoding of a permutation in order to support iterated\napplications $\\pi^k(i)$ of it, of integer functions, and of inverted lists and\nsuffix arrays."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1260v1", 
    "title": "Nonclairvoyant Speed Scaling for Flow and Energy", 
    "arxiv-id": "0902.1260v1", 
    "author": "Kirk Pruhs", 
    "publish": "2009-02-07T18:02:32Z", 
    "summary": "We study online nonclairvoyant speed scaling to minimize total flow time plus\nenergy. We first consider the traditional model where the power function is P\n(s) = s\\^\\propto. We give a nonclairvoyant algorithm that is shown to be\nO(\\propto\\^3)-competitive. We then show an \\Omega(\\propto\\^(1/3-\\epsilon))\nlower bound on the competitive ratio of any nonclairvoyant algorithm. We also\nshow that there are power functions for which no nonclairvoyant algorithm can\nbe O(1)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1378v1", 
    "title": "On the Additive Constant of the k-server Work Function Algorithm", 
    "arxiv-id": "0902.1378v1", 
    "author": "Adi Rosen", 
    "publish": "2009-02-09T07:47:19Z", 
    "summary": "We consider the Work Function Algorithm for the k-server problem. We show\nthat if the Work Function Algorithm is c-competitive, then it is also strictly\n(2c)-competitive. As a consequence of [Koutsoupias and Papadimitriou, JACM\n1995] this also shows that the Work Function Algorithm is strictly\n(4k-2)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1604v1", 
    "title": "A Comparison of Techniques for Sampling Web Pages", 
    "arxiv-id": "0902.1604v1", 
    "author": "Markus Kinzler", 
    "publish": "2009-02-10T08:44:14Z", 
    "summary": "As the World Wide Web is growing rapidly, it is getting increasingly\nchallenging to gather representative information about it. Instead of crawling\nthe web exhaustively one has to resort to other techniques like sampling to\ndetermine the properties of the web. A uniform random sample of the web would\nbe useful to determine the percentage of web pages in a specific language, on a\ntopic or in a top level domain. Unfortunately, no approach has been shown to\nsample the web pages in an unbiased way. Three promising web sampling\nalgorithms are based on random walks. They each have been evaluated\nindividually, but making a comparison on different data sets is not possible.\nWe directly compare these algorithms in this paper. We performed three random\nwalks on the web under the same conditions and analyzed their outcomes in\ndetail. We discuss the strengths and the weaknesses of each algorithm and\npropose improvements based on experimental results."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1605v1", 
    "title": "Lower Bounds for Multi-Pass Processing of Multiple Data Streams", 
    "arxiv-id": "0902.1605v1", 
    "author": "Nicole Schweikardt", 
    "publish": "2009-02-10T08:46:27Z", 
    "summary": "This paper gives a brief overview of computation models for data stream\nprocessing, and it introduces a new model for multi-pass processing of multiple\nstreams, the so-called mp2s-automata. Two algorithms for solving the set\ndisjointness problem wi th these automata are presented. The main technical\ncontribution of this paper is the proof of a lower bound on the size of memory\nand the number of heads that are required for solvin g the set disjointness\nproblem with mp2s-automata."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.1693v4", 
    "title": "Fast Evaluation of Interlace Polynomials on Graphs of Bounded Treewidth", 
    "arxiv-id": "0902.1693v4", 
    "author": "Christian Hoffmann", 
    "publish": "2009-02-10T16:41:35Z", 
    "summary": "We consider the multivariate interlace polynomial introduced by Courcelle\n(2008), which generalizes several interlace polynomials defined by Arratia,\nBollobas, and Sorkin (2004) and by Aigner and van der Holst (2004). We present\nan algorithm to evaluate the multivariate interlace polynomial of a graph with\nn vertices given a tree decomposition of the graph of width k. The best\npreviously known result (Courcelle 2008) employs a general logical framework\nand leads to an algorithm with running time f(k)*n, where f(k) is doubly\nexponential in k. Analyzing the GF(2)-rank of adjacency matrices in the context\nof tree decompositions, we give a faster and more direct algorithm. Our\nalgorithm uses 2^{3k^2+O(k)}*n arithmetic operations and can be efficiently\nimplemented in parallel."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.1792v3", 
    "title": "Correlation Robust Stochastic Optimization", 
    "arxiv-id": "0902.1792v3", 
    "author": "Yinyu Ye", 
    "publish": "2009-02-11T01:51:42Z", 
    "summary": "We consider a robust model proposed by Scarf, 1958, for stochastic\noptimization when only the marginal probabilities of (binary) random variables\nare given, and the correlation between the random variables is unknown. In the\nrobust model, the objective is to minimize expected cost against worst possible\njoint distribution with those marginals. We introduce the concept of\ncorrelation gap to compare this model to the stochastic optimization model that\nignores correlations and minimizes expected cost under independent Bernoulli\ndistribution. We identify a class of functions, using concepts of summable cost\nsharing schemes from game theory, for which the correlation gap is well-bounded\nand the robust model can be approximated closely by the independent\ndistribution model. As a result, we derive efficient approximation factors for\nmany popular cost functions, like submodular functions, facility location, and\nSteiner tree. As a byproduct, our analysis also yields some new results in the\nareas of social welfare maximization and existence of Walrasian equilibria,\nwhich may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2209v3", 
    "title": "Online Scheduling of Bounded Length Jobs to Maximize Throughput", 
    "arxiv-id": "0902.2209v3", 
    "author": "Nguyen Kim Thang", 
    "publish": "2009-02-12T21:24:57Z", 
    "summary": "We consider an online scheduling problem, motivated by the issues present at\nthe joints of networks using ATM and TCP/IP. Namely, IP packets have to broken\ndown to small ATM cells and sent out before their deadlines, but cells\ncorresponding to different packets can be interwoven. More formally, we\nconsider the online scheduling problem with preemptions, where each job j is\nrevealed at release time r_j, has processing time p_j, deadline d_j and weight\nw_j. A preempted job can be resumed at any time. The goal is to maximize the\ntotal weight of all jobs completed on time. Our main result are as follows: we\nprove that if all jobs have processing time exactly k, the deterministic\ncompetitive ratio is between 2.598 and 5, and when the processing times are at\nmost k, the deterministic competitive ratio is Theta(k/log k)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2648v1", 
    "title": "More Haste, Less Waste: Lowering the Redundancy in Fully Indexable   Dictionaries", 
    "arxiv-id": "0902.2648v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2009-02-16T10:14:08Z", 
    "summary": "We consider the problem of representing, in a compressed format, a bit-vector\n$S$ of $m$ bits with $n$ 1s, supporting the following operations, where $b \\in\n\\{0, 1 \\}$: $rank_b(S,i)$ returns the number of occurrences of bit $b$ in the\nprefix $S[1..i]$; $select_b(S,i)$ returns the position of the $i$th occurrence\nof bit $b$ in $S$. Such a data structure is called \\emph{fully indexable\ndictionary (FID)} [Raman et al.,2007], and is at least as powerful as\npredecessor data structures. Our focus is on space-efficient FIDs on the\n\\textsc{ram} model with word size $\\Theta(\\lg m)$ and constant time for all\noperations, so that the time cost is independent of the input size. Given the\nbitstring $S$ to be encoded, having length $m$ and containing $n$ ones, the\nminimal amount of information that needs to be stored is $B(n,m) = \\lceil \\log\n{{m}\\choose{n}} \\rceil$. The state of the art in building a FID for $S$ is\ngiven in [Patrascu,2008] using $B(m,n)+O(m / ((\\log m/ t) ^t)) + O(m^{3/4}) $\nbits, to support the operations in $O(t)$ time. Here, we propose a parametric\ndata structure exhibiting a time/space trade-off such that, for any real\nconstants $0 < \\delta \\leq 1/2$, $0 < \\eps \\leq 1$, and integer $s > 0$, it\nuses \\[ B(n,m) + O(n^{1+\\delta} + n (\\frac{m}{n^s})^\\eps) \\] bits and performs\nall the operations in time $O(s\\delta^{-1} + \\eps^{-1})$. The improvement is\ntwofold: our redundancy can be lowered parametrically and, fixing $s = O(1)$,\nwe get a constant-time FID whose space is $B(n,m) + O(m^\\eps/\\poly{n})$ bits,\nfor sufficiently large $m$. This is a significant improvement compared to the\nprevious bounds for the general case."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2795v1", 
    "title": "A Graph Reduction Step Preserving Element-Connectivity and Applications", 
    "arxiv-id": "0902.2795v1", 
    "author": "Nitish Korula", 
    "publish": "2009-02-16T21:29:26Z", 
    "summary": "Given an undirected graph G=(V,E) and subset of terminals T \\subseteq V, the\nelement-connectivity of two terminals u,v \\in T is the maximum number of u-v\npaths that are pairwise disjoint in both edges and non-terminals V \\setminus T\n(the paths need not be disjoint in terminals). Element-connectivity is more\ngeneral than edge-connectivity and less general than vertex-connectivity. Hind\nand Oellermann gave a graph reduction step that preserves the global\nelement-connectivity of the graph. We show that this step also preserves local\nconnectivity, that is, all the pairwise element-connectivities of the\nterminals. We give two applications of this reduction step to connectivity and\nnetwork design problems:\n  1. Given a graph G and disjoint terminal sets T_1, T_2, ..., T_m, we seek a\nmaximum number of element-disjoint Steiner forests where each forest connects\neach T_i. We prove that if each T_i is k-element-connected then there exist\n\\Omega(\\frac{k}{\\log h \\log m}) element-disjoint Steiner forests, where h =\n|\\bigcup_i T_i|. If G is planar (or more generally, has fixed genus), we show\nthat there exist \\Omega(k) Steiner forests. Our proofs are constructive, giving\npoly-time algorithms to find these forests; these are the first non-trivial\nalgorithms for packing element-disjoint Steiner Forests.\n  2. We give a very short and intuitive proof of a spider-decomposition theorem\nof Chuzhoy and Khanna in the context of the single-sink k-vertex-connectivity\nproblem; this yields a simple and alternative analysis of an O(k \\log n)\napproximation.\n  Our results highlight the effectiveness of the element-connectivity reduction\nstep; we believe it will find more applications in the future."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.3121v1", 
    "title": "Parallel machine scheduling with precedence constraints and setup times", 
    "arxiv-id": "0902.3121v1", 
    "author": "Pierre Lopez", 
    "publish": "2009-02-18T14:00:16Z", 
    "summary": "This paper presents different methods for solving parallel machine scheduling\nproblems with precedence constraints and setup times between the jobs. Limited\ndiscrepancy search methods mixed with local search principles, dominance\nconditions and specific lower bounds are proposed. The proposed methods are\nevaluated on a set of randomly generated instances and compared with previous\nresults from the literature and those obtained with an efficient commercial\nsolver. We conclude that our propositions are quite competitive and our results\neven outperform other approaches in most cases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0116v1", 
    "title": "Heaps Simplified", 
    "arxiv-id": "0903.0116v1", 
    "author": "Robert E. Tarjan", 
    "publish": "2009-03-01T00:19:10Z", 
    "summary": "The heap is a basic data structure used in a wide variety of applications,\nincluding shortest path and minimum spanning tree algorithms. In this paper we\nexplore the design space of comparison-based, amortized-efficient heap\nimplementations. From a consideration of dynamic single-elimination\ntournaments, we obtain the binomial queue, a classical heap implementation, in\na simple and natural way. We give four equivalent ways of representing heaps\narising from tournaments, and we obtain two new variants of binomial queues, a\none-tree version and a one-pass version. We extend the one-pass version to\nsupport key decrease operations, obtaining the {\\em rank-pairing heap}, or {\\em\nrp-heap}. Rank-pairing heaps combine the performance guarantees of Fibonacci\nheaps with simplicity approaching that of pairing heaps. Like pairing heaps,\nrank-pairing heaps consist of trees of arbitrary structure, but these trees are\ncombined by rank, not by list position, and rank changes, but not structural\nchanges, cascade during key decrease operations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0136v2", 
    "title": "A polynomial graph extension procedure for improving graph isomorphism   algorithms", 
    "arxiv-id": "0903.0136v2", 
    "author": "Daniel Cosmin Porumbel", 
    "publish": "2009-03-01T12:54:57Z", 
    "summary": "We present in this short note a polynomial graph extension procedure that can\nbe used to improve any graph isomorphism algorithm. This construction\npropagates new constraints from the isomorphism constraints of the input graphs\n(denoted by $G(V,E)$ and $G'(V',E')$). Thus, information from the edge\nstructures of $G$ and $G'$ is \"hashed\" into the weighted edges of the extended\ngraphs. A bijective mapping is an isomorphism of the initial graphs if and only\nif it is an isomorphism of the extended graphs. As such, the construction\nenables the identification of pair of vertices $i\\in V$ and $i'\\in V'$ that can\nnot be mapped by any isomorphism $h^*:V \\to V'$ (e.g. if the extended edges of\n$i$ and $i'$ are different). A forbidding matrix $F$, that encodes all pairs of\nincompatible mappings $(i,i')$, is constructed in order to be used by a\ndifferent algorithm. Moreover, tests on numerous graph classes show that the\nmatrix $F$ might leave only one compatible element for each $i \\in V$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0197v1", 
    "title": "Rotation Distance is Fixed-Parameter Tractable", 
    "arxiv-id": "0903.0197v1", 
    "author": "Katherine St. John", 
    "publish": "2009-03-02T01:36:50Z", 
    "summary": "Rotation distance between trees measures the number of simple operations it\ntakes to transform one tree into another. There are no known polynomial-time\nalgorithms for computing rotation distance. In the case of ordered rooted\ntrees, we show that the rotation distance between two ordered trees is\nfixed-parameter tractable, in the parameter, k, the rotation distance. The\nproof relies on the kernalization of the initial trees to trees with size\nbounded by 7k."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0199v2", 
    "title": "A Linear-Time Approximation Algorithm for Rotation Distance", 
    "arxiv-id": "0903.0199v2", 
    "author": "Katherine St. John", 
    "publish": "2009-03-02T01:40:14Z", 
    "summary": "Rotation distance between rooted binary trees measures the number of simple\noperations it takes to transform one tree into another. There are no known\npolynomial-time algorithms for computing rotation distance. We give an\nefficient, linear-time approximation algorithm, which estimates the rotation\ndistance, within a provable factor of 2, between ordered rooted binary trees. ."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0367v1", 
    "title": "How to Play Unique Games on Expanders", 
    "arxiv-id": "0903.0367v1", 
    "author": "Yury Makarychev", 
    "publish": "2009-03-02T20:40:53Z", 
    "summary": "In this note we improve a recent result by Arora, Khot, Kolla, Steurer,\nTulsiani, and Vishnoi on solving the Unique Games problem on expanders.\n  Given a $(1-\\varepsilon)$-satisfiable instance of Unique Games with the\nconstraint graph $G$, our algorithm finds an assignment satisfying at least a\n$1- C \\varepsilon/h_G$ fraction of all constraints if $\\varepsilon < c\n\\lambda_G$ where $h_G$ is the edge expansion of $G$, $\\lambda_G$ is the second\nsmallest eigenvalue of the Laplacian of $G$, and $C$ and $c$ are some absolute\nconstants."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0391v1", 
    "title": "De-amortized Cuckoo Hashing: Provable Worst-Case Performance and   Experimental Results", 
    "arxiv-id": "0903.0391v1", 
    "author": "Gil Segev", 
    "publish": "2009-03-02T21:21:47Z", 
    "summary": "Cuckoo hashing is a highly practical dynamic dictionary: it provides\namortized constant insertion time, worst case constant deletion time and lookup\ntime, and good memory utilization. However, with a noticeable probability\nduring the insertion of n elements some insertion requires \\Omega(log n) time.\nWhereas such an amortized guarantee may be suitable for some applications, in\nother applications (such as high-performance routing) this is highly\nundesirable.\n  Recently, Kirsch and Mitzenmacher (Allerton '07) proposed a de-amortization\nof cuckoo hashing using various queueing techniques that preserve its\nattractive properties. Kirsch and Mitzenmacher demonstrated a significant\nimprovement to the worst case performance of cuckoo hashing via experimental\nresults, but they left open the problem of constructing a scheme with provable\nproperties.\n  In this work we follow Kirsch and Mitzenmacher and present a de-amortization\nof cuckoo hashing that provably guarantees constant worst case operations.\nSpecifically, for any sequence of polynomially many operations, with\noverwhelming probability over the randomness of the initialization phase, each\noperation is performed in constant time. Our theoretical analysis and\nexperimental results indicate that the scheme is highly efficient, and provides\na practical alternative to the only other known dynamic dictionary with such\nworst case guarantees, due to Dietzfelbinger and Meyer auf der Heide (ICALP\n'90)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.0938v1", 
    "title": "Algorithm for Finding $k$-Vertex Out-trees and its Application to   $k$-Internal Out-branching Problem", 
    "arxiv-id": "0903.0938v1", 
    "author": "Anders Yeo", 
    "publish": "2009-03-05T16:15:16Z", 
    "summary": "An out-tree $T$ is an oriented tree with only one vertex of in-degree zero. A\nvertex $x$ of $T$ is internal if its out-degree is positive. We design\nrandomized and deterministic algorithms for deciding whether an input digraph\ncontains a given out-tree with $k$ vertices. The algorithms are of runtime\n$O^*(5.704^k)$ and $O^*(5.704^{k(1+o(1))})$, respectively. We apply the\ndeterministic algorithm to obtain a deterministic algorithm of runtime\n$O^*(c^k)$, where $c$ is a constant, for deciding whether an input digraph\ncontains a spanning out-tree with at least $k$ internal vertices. This answers\nin affirmative a question of Gutin, Razgon and Kim (Proc. AAIM'08)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0903.4130v2", 
    "title": "Pairing Heaps with Costless Meld", 
    "arxiv-id": "0903.4130v2", 
    "author": "Amr Elmasry", 
    "publish": "2009-03-24T16:49:57Z", 
    "summary": "Improving the structure and analysis in \\cite{elm0}, we give a variation of\nthe pairing heaps that has amortized zero cost per meld (compared to an $O(\\log\n\\log{n})$ in \\cite{elm0}) and the same amortized bounds for all other\noperations. More precisely, the new pairing heap requires: no cost per meld,\nO(1) per find-min and insert, $O(\\log{n})$ per delete-min, and $O(\\log\\log{n})$\nper decrease-key. These bounds are the best known for any self-adjusting heap,\nand match the lower bound proved by Fredman for a family of such heaps.\nMoreover, the changes we have done make our structure even simpler than that in\n\\cite{elm0}."
},{
    "category": "cs.DS", 
    "doi": "10.1109/DCC.2009.50", 
    "link": "http://arxiv.org/pdf/0903.4251v1", 
    "title": "On the Use of Suffix Arrays for Memory-Efficient Lempel-Ziv Data   Compression", 
    "arxiv-id": "0903.4251v1", 
    "author": "Mario Figueiredo", 
    "publish": "2009-03-25T19:25:24Z", 
    "summary": "Much research has been devoted to optimizing algorithms of the Lempel-Ziv\n(LZ) 77 family, both in terms of speed and memory requirements. Binary search\ntrees and suffix trees (ST) are data structures that have been often used for\nthis purpose, as they allow fast searches at the expense of memory usage.\n  In recent years, there has been interest on suffix arrays (SA), due to their\nsimplicity and low memory requirements. One key issue is that an SA can solve\nthe sub-string problem almost as efficiently as an ST, using less memory. This\npaper proposes two new SA-based algorithms for LZ encoding, which require no\nmodifications on the decoder side. Experimental results on standard benchmarks\nshow that our algorithms, though not faster, use 3 to 5 times less memory than\nthe ST counterparts. Another important feature of our SA-based algorithms is\nthat the amount of memory is independent of the text to search, thus the memory\nthat has to be allocated can be defined a priori. These features of low and\npredictable memory requirements are of the utmost importance in several\nscenarios, such as embedded systems, where memory is at a premium and speed is\nnot critical. Finally, we point out that the new algorithms are general, in the\nsense that they are adequate for applications other than LZ compression, such\nas text retrieval and forward/backward sub-string search."
},{
    "category": "cs.DS", 
    "doi": "10.1109/DCC.2009.50", 
    "link": "http://arxiv.org/pdf/0903.4521v3", 
    "title": "Solving Dominating Set in Larger Classes of Graphs: FPT Algorithms and   Polynomial Kernels", 
    "arxiv-id": "0903.4521v3", 
    "author": "Somnath Sikdar", 
    "publish": "2009-03-26T06:23:58Z", 
    "summary": "We show that the k-Dominating Set problem is fixed parameter tractable (FPT)\nand has a polynomial kernel for any class of graphs that exclude K_{i,j} as a\nsubgraph, for any fixed i, j >= 1. This strictly includes every class of graphs\nfor which this problem has been previously shown to have FPT algorithms and/or\npolynomial kernels. In particular, our result implies that the problem\nrestricted to bounded- degenerate graphs has a polynomial kernel, solving an\nopen problem posed by Alon and Gutner."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0903.4726v6", 
    "title": "Range Quantile Queries: Another Virtue of Wavelet Trees", 
    "arxiv-id": "0903.4726v6", 
    "author": "Andrew Turpin", 
    "publish": "2009-03-27T02:29:01Z", 
    "summary": "We show how to use a balanced wavelet tree as a data structure that stores a\nlist of numbers and supports efficient {\\em range quantile queries}. A range\nquantile query takes a rank and the endpoints of a sublist and returns the\nnumber with that rank in that sublist. For example, if the rank is half the\nsublist's length, then the query returns the sublist's median. We also show how\nthese queries can be used to support space-efficient {\\em coloured range\nreporting} and {\\em document listing}."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.0292v1", 
    "title": "Sublinear Time Algorithms for Earth Mover's Distance", 
    "arxiv-id": "0904.0292v1", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2009-04-02T02:17:43Z", 
    "summary": "We study the problem of estimating the Earth Mover's Distance (EMD) between\nprobability distributions when given access only to samples. We give closeness\ntesters and additive-error estimators over domains in $[0, \\Delta]^d$, with\nsample complexities independent of domain size - permitting the testability\neven of continuous distributions over infinite domains. Instead, our algorithms\ndepend on other parameters, such as the diameter of the domain space, which may\nbe significantly smaller. We also prove lower bounds showing the dependencies\non these parameters to be essentially optimal. Additionally, we consider\nwhether natural classes of distributions exist for which there are algorithms\nwith better dependence on the dimension, and show that for highly clusterable\ndata, this is indeed the case. Lastly, we consider a variant of the EMD,\ndefined over tree metrics instead of the usual L1 metric, and give optimal\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.0352v2", 
    "title": "Incremental Deployment of Network Monitors Based on Group Betweenness   Centrality", 
    "arxiv-id": "0904.0352v2", 
    "author": "Polina Zilberman", 
    "publish": "2009-04-02T09:32:51Z", 
    "summary": "In many applications we are required to increase the deployment of a\ndistributed monitoring system on an evolving network. In this paper we present\na new method for finding candidate locations for additional deployment in the\nnetwork. This method is based on the Group Betweenness Centrality (GBC) measure\nthat is used to estimate the influence of a group of nodes over the information\nflow in the network. The new method assists in finding the location of k\nadditional monitors in the evolving network, such that the portion of\nadditional traffic covered is at least (1-1/e) of the optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.1002v1", 
    "title": "A Program to Determine the Exact Competitive Ratio of List s-Batching   with Unit Jobs", 
    "arxiv-id": "0904.1002v1", 
    "author": "John Noga", 
    "publish": "2009-04-06T20:08:48Z", 
    "summary": "We consider the online list s-batch problem, where all the jobs have\nprocessing time 1 and we seek to minimize the sum of the completion times of\nthe jobs. We give a Java program which is used to verify that the\ncompetitiveness of this problem is 619/583."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.1705v1", 
    "title": "Bounded Max-Colorings of Graphs", 
    "arxiv-id": "0904.1705v1", 
    "author": "Ioannis Milis", 
    "publish": "2009-04-10T15:36:02Z", 
    "summary": "In a bounded max-coloring of a vertex/edge weighted graph, each color class\nis of cardinality at most $b$ and of weight equal to the weight of the heaviest\nvertex/edge in this class. The bounded max-vertex/edge-coloring problems ask\nfor such a coloring minimizing the sum of all color classes' weights.\n  In this paper we present complexity results and approximation algorithms for\nthose problems on general graphs, bipartite graphs and trees. We first show\nthat both problems are polynomial for trees, when the number of colors is\nfixed, and $H_b$ approximable for general graphs, when the bound $b$ is fixed.\nFor the bounded max-vertex-coloring problem, we show a 17/11-approximation\nalgorithm for bipartite graphs, a PTAS for trees as well as for bipartite\ngraphs when $b$ is fixed. For unit weights, we show that the known 4/3 lower\nbound for bipartite graphs is tight by providing a simple 4/3 approximation\nalgorithm. For the bounded max-edge-coloring problem, we prove approximation\nfactors of $3-2/\\sqrt{2b}$, for general graphs, $\\min\\{e, 3-2/\\sqrt{b}\\}$, for\nbipartite graphs, and 2, for trees. Furthermore, we show that this problem is\nNP-complete even for trees. This is the first complexity result for\nmax-coloring problems on trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.2027v1", 
    "title": "A Near-Optimal Algorithm for L1-Difference", 
    "arxiv-id": "0904.2027v1", 
    "author": "David P. Woodruff", 
    "publish": "2009-04-13T22:54:26Z", 
    "summary": "We give the first L_1-sketching algorithm for integer vectors which produces\nnearly optimal sized sketches in nearly linear time. This answers the first\nopen problem in the list of open problems from the 2006 IITK Workshop on\nAlgorithms for Data Streams. Specifically, suppose Alice receives a vector x in\n{-M,...,M}^n and Bob receives y in {-M,...,M}^n, and the two parties share\nrandomness. Each party must output a short sketch of their vector such that a\nthird party can later quickly recover a (1 +/- eps)-approximation to ||x-y||_1\nwith 2/3 probability given only the sketches. We give a sketching algorithm\nwhich produces O(eps^{-2}log(1/eps)log(nM))-bit sketches in O(n*log^2(nM))\ntime, independent of eps. The previous best known sketching algorithm for L_1\nis due to [Feigenbaum et al., SICOMP 2002], which achieved the optimal sketch\nlength of O(eps^{-2}log(nM)) bits but had a running time of O(n*log(nM)/eps^2).\nNotice that our running time is near-linear for every eps, whereas for\nsufficiently small values of eps, the running time of the previous algorithm\ncan be as large as quadratic. Like their algorithm, our sketching procedure\nalso yields a small-space, one-pass streaming algorithm which works even if the\nentries of x,y are given in arbitrary order."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.2129v1", 
    "title": "Crossing-Optimal Acyclic HP-Completion for Outerplanar st-Digraphs", 
    "arxiv-id": "0904.2129v1", 
    "author": "Antonios Symvonis", 
    "publish": "2009-04-14T14:29:56Z", 
    "summary": "Given an embedded planar acyclic digraph G, we define the problem of acyclic\nhamiltonian path completion with crossing minimization (Acyclic-HPCCM) to be\nthe problem of determining a hamiltonian path completion set of edges such\nthat, when these edges are embedded on G, they create the smallest possible\nnumber of edge crossings and turn G to a hamiltonian acyclic digraph. Our\nresults include: 1. We provide a characterization under which a planar\nst-digraph G is hamiltonian. 2. For an outerplanar st-digraph G, we define the\nst-polygon decomposition of G and, based on its properties, we develop a\nlinear-time algorithm that solves the Acyclic-HPCCM problem. 3. For the class\nof planar st-digraphs, we establish an equivalence between the Acyclic-HPCCM\nproblem and the problem of determining an upward 2-page topological book\nembedding with minimum number of spine crossings. We infer (based on this\nequivalence) for the class of outerplanar st-digraphs an upward topological\n2-page book embedding with minimum number of spine crossings. To the best of\nour knowledge, it is the first time that edge-crossing minimization is studied\nin conjunction with the acyclic hamiltonian completion problem and the first\ntime that an optimal algorithm with respect to spine crossing minimization is\npresented for upward topological book embeddings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.2576v1", 
    "title": "PTAS for k-tour cover problem on the plane for moderately large values   of k", 
    "arxiv-id": "0904.2576v1", 
    "author": "Andrzej Lingas", 
    "publish": "2009-04-16T20:12:33Z", 
    "summary": "Let P be a set of n points in the Euclidean plane and let O be the origin\npoint in the plane. In the k-tour cover problem (called frequently the\ncapacitated vehicle routing problem), the goal is to minimize the total length\nof tours that cover all points in P, such that each tour starts and ends in O\nand covers at most k points from P.\n  The k-tour cover problem is known to be NP-hard. It is also known to admit\nconstant factor approximation algorithms for all values of k and even a\npolynomial-time approximation scheme (PTAS) for small values of k, i.e.,\nk=O(log n / log log n).\n  We significantly enlarge the set of values of k for which a PTAS is provable.\nWe present a new PTAS for all values of k <= 2^{log^{\\delta}n}, where \\delta =\n\\delta(\\epsilon). The main technical result proved in the paper is a novel\nreduction of the k-tour cover problem with a set of n points to a small set of\ninstances of the problem, each with O((k/\\epsilon)^O(1)) points."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.2728v1", 
    "title": "Fast Computation of Empirically Tight Bounds for the Diameter of Massive   Graphs", 
    "arxiv-id": "0904.2728v1", 
    "author": "Michel Habib", 
    "publish": "2009-04-17T15:44:49Z", 
    "summary": "The diameter of a graph is among its most basic parameters. Since a few\nyears, it moreover became a key issue to compute it for massive graphs in the\ncontext of complex network analysis. However, known algorithms, including the\nones producing approximate values, have too high a time and/or space complexity\nto be used in such cases. We propose here a new approach relying on very simple\nand fast algorithms that compute (upper and lower) bounds for the diameter. We\nshow empirically that, on various real-world cases representative of complex\nnetworks studied in the literature, the obtained bounds are very tight (and\neven equal in some cases). This leads to rigorous and very accurate estimations\nof the actual diameter in cases which were previously untractable in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03784-9_1", 
    "link": "http://arxiv.org/pdf/0904.3062v2", 
    "title": "Approximate counting with a floating-point counter", 
    "arxiv-id": "0904.3062v2", 
    "author": "Miklos Csuros", 
    "publish": "2009-04-20T15:53:33Z", 
    "summary": "Memory becomes a limiting factor in contemporary applications, such as\nanalyses of the Webgraph and molecular sequences, when many objects need to be\ncounted simultaneously. Robert Morris [Communications of the ACM, 21:840--842,\n1978] proposed a probabilistic technique for approximate counting that is\nextremely space-efficient. The basic idea is to increment a counter containing\nthe value $X$ with probability $2^{-X}$. As a result, the counter contains an\napproximation of $\\lg n$ after $n$ probabilistic updates stored in $\\lg\\lg n$\nbits. Here we revisit the original idea of Morris, and introduce a binary\nfloating-point counter that uses a $d$-bit significand in conjunction with a\nbinary exponent. The counter yields a simple formula for an unbiased estimation\nof $n$ with a standard deviation of about $0.6\\cdot n2^{-d/2}$, and uses\n$d+\\lg\\lg n$ bits.\n  We analyze the floating-point counter's performance in a general framework\nthat applies to any probabilistic counter, and derive practical formulas to\nassess its accuracy."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3741v1", 
    "title": "The h-Index of a Graph and its Application to Dynamic Subgraph   Statistics", 
    "arxiv-id": "0904.3741v1", 
    "author": "Emma S. Spiro", 
    "publish": "2009-04-23T18:37:36Z", 
    "summary": "We describe a data structure that maintains the number of triangles in a\ndynamic undirected graph, subject to insertions and deletions of edges and of\ndegree-zero vertices. More generally it can be used to maintain the number of\ncopies of each possible three-vertex subgraph in time O(h) per update, where h\nis the h-index of the graph, the maximum number such that the graph contains\n$h$ vertices of degree at least h. We also show how to maintain the h-index\nitself, and a collection of h high-degree vertices in the graph, in constant\ntime per update. Our data structure has applications in social network analysis\nusing the exponential random graph model (ERGM); its bound of O(h) time per\nedge is never worse than the Theta(sqrt m) time per edge necessary to list all\ntriangles in a static graph, and is strictly better for graphs obeying a power\nlaw degree distribution. In order to better understand the behavior of the\nh-index statistic and its implications for the performance of our algorithms,\nwe also study the behavior of the h-index on a set of 136 real-world networks."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3756v3", 
    "title": "On the Approximability of Geometric and Geographic Generalization and   the Min-Max Bin Covering Problem", 
    "arxiv-id": "0904.3756v3", 
    "author": "George S. Lueker", 
    "publish": "2009-04-23T21:06:58Z", 
    "summary": "We study the problem of abstracting a table of data about individuals so that\nno selection query can identify fewer than k individuals. We show that it is\nimpossible to achieve arbitrarily good polynomial-time approximations for a\nnumber of natural variations of the generalization technique, unless P = NP,\neven when the table has only a single quasi-identifying attribute that\nrepresents a geographic or unordered attribute:\n  Zip-codes: nodes of a planar graph generalized into connected subgraphs\n  GPS coordinates: points in R2 generalized into non-overlapping rectangles\n  Unordered data: text labels that can be grouped arbitrarily. In addition to\nimpossibility results, we provide approximation algorithms for these difficult\nsingle-attribute generalization problems, which, of course, apply to\nmultiple-attribute instances with one that is quasi-identifying. We show\ntheoretically and experimentally that our approximation algorithms can come\nreasonably close to optimal solutions. Incidentally, the generalization problem\nfor unordered data can be viewed as a novel type of bin packing\nproblem--min-max bin covering--which may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3898v2", 
    "title": "On Smoothed Analysis of Quicksort and Hoare's Find", 
    "arxiv-id": "0904.3898v2", 
    "author": "Nima Zeini Jahromi", 
    "publish": "2009-04-24T16:12:40Z", 
    "summary": "We provide a smoothed analysis of Hoare's find algorithm and we revisit the\nsmoothed analysis of quicksort.\n  Hoare's find algorithm - often called quickselect - is an easy-to-implement\nalgorithm for finding the k-th smallest element of a sequence. While the\nworst-case number of comparisons that Hoare's find needs is quadratic, the\naverage-case number is linear. We analyze what happens between these two\nextremes by providing a smoothed analysis of the algorithm in terms of two\ndifferent perturbation models: additive noise and partial permutations.\n  Moreover, we provide lower bounds for the smoothed number of comparisons of\nquicksort and Hoare's find for the median-of-three pivot rule, which usually\nyields faster algorithms than always selecting the first element: The pivot is\nthe median of the first, middle, and last element of the sequence. We show that\nmedian-of-three does not yield a significant improvement over the classic rule:\nthe lower bounds for the classic rule carry over to median-of-three."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.4061v1", 
    "title": "Approximation Algorithms for Key Management in Secure Multicast", 
    "arxiv-id": "0904.4061v1", 
    "author": "Feng Zhu", 
    "publish": "2009-04-27T15:28:58Z", 
    "summary": "Many data dissemination and publish-subscribe systems that guarantee the\nprivacy and authenticity of the participants rely on symmetric key\ncryptography. An important problem in such a system is to maintain the shared\ngroup key as the group membership changes. We consider the problem of\ndetermining a key hierarchy that minimizes the average communication cost of an\nupdate, given update frequencies of the group members and an edge-weighted\nundirected graph that captures routing costs. We first present a\npolynomial-time approximation scheme for minimizing the average number of\nmulticast messages needed for an update. We next show that when routing costs\nare considered, the problem is NP-hard even when the underlying routing network\nis a tree network or even when every group member has the same update\nfrequency. Our main result is a polynomial time constant-factor approximation\nalgorithm for the general case where the routing network is an arbitrary\nweighted graph and group members have nonuniform update frequencies."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.0283v1", 
    "title": "Optimal Embedding Into Star Metrics", 
    "arxiv-id": "0905.0283v1", 
    "author": "Kevin A. Wortman", 
    "publish": "2009-05-03T19:21:52Z", 
    "summary": "We present an O(n^3 log^2 n)-time algorithm for the following problem: given\na finite metric space X, create a star-topology network with the points of X as\nits leaves, such that the distances in the star are at least as large as in X,\nwith minimum dilation. As part of our algorithm, we solve in the same time\nbound the parametric negative cycle detection problem: given a directed graph\nwith edge weights that are increasing linear functions of a parameter lambda,\nfind the smallest value of lambda such that the graph contains no\nnegative-weight cycles."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.0768v5", 
    "title": "Fully-Functional Static and Dynamic Succinct Trees", 
    "arxiv-id": "0905.0768v5", 
    "author": "Kunihiko Sadakane", 
    "publish": "2009-05-06T07:56:38Z", 
    "summary": "We propose new succinct representations of ordinal trees, which have been\nstudied extensively. It is known that any $n$-node static tree can be\nrepresented in $2n + o(n)$ bits and a number of operations on the tree can be\nsupported in constant time under the word-RAM model. However the data\nstructures are complicated and difficult to dynamize. We propose a simple and\nflexible data structure, called the range min-max tree, that reduces the large\nnumber of relevant tree operations considered in the literature to a few\nprimitives that are carried out in constant time on sufficiently small trees.\nThe result is extended to trees of arbitrary size, achieving $2n + O(n\n/\\polylog(n))$ bits of space. The redundancy is significantly lower than any\nprevious proposal. Our data structure builds on the range min-max tree to\nachieve $2n+O(n/\\log n)$ bits of space and $O(\\log n)$ time for all the\noperations. We also propose an improved data structure using $2n+O(n\\log\\log\nn/\\log n)$ bits and improving the time to the optimal $O(\\log n/\\log \\log n)$\nfor most operations. Furthermore, we support sophisticated operations that\nallow attaching and detaching whole subtrees, in time $\\Order(\\log^{1+\\epsilon}\nn / \\log\\log n)$. Our techniques are of independent interest. One allows\nrepresenting dynamic bitmaps and sequences supporting rank/select and indels,\nwithin zero-order entropy bounds and optimal time $O(\\log n / \\log\\log n)$ for\nall operations on bitmaps and polylog-sized alphabets, and $O(\\log n \\log\n\\sigma / (\\log\\log n)^2)$ on larger alphabet sizes $\\sigma$. This improves upon\nthe best existing bounds for entropy-bounded storage of dynamic sequences,\ncompressed full-text self-indexes, and compressed-space construction of the\nBurrows-Wheeler transform."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2028v1", 
    "title": "Quantum Algorithms of Bio-molecular Solutions for the Clique Problem on   a Quantum Computer", 
    "arxiv-id": "0905.2028v1", 
    "author": "Lai Chin Lu", 
    "publish": "2009-05-13T07:38:48Z", 
    "summary": "In this paper, it is demonstrated that the DNA-based algorithm [Ho et al.\n2005] for solving an instance of the clique problem to any a graph G = (V, E)\nwith n vertices and p edges and its complementary graph G1 = (V, E1) with n\nvertices and m = (((n*(n-1))/2)-p) edges can be implemented by Hadamard gates,\nNOT gates, CNOT gates, CCNOT gates, Grover's operators, and quantum\nmeasurements on a quantum computer. It is also demonstrated that if Grovers\nalgorithm is employed to accomplish the readout step in the DNA-based\nalgorithm, the quantum implementation of the DNA-based algorithm is equivalent\nto the oracle work (in the language of Grover's algorithm), that is, the target\nstate labeling preceding Grover,s searching steps. It is shown that one oracle\nwork can be completed with O((2 * n) * (n + 1) * (n + 2) / 3) NOT gates, one\nCNOT gate and O((4 * m) + (((2 * n) * (n + 1) * (n + 14)) / 6)) CCNOT gates.\nThis is to say that for the quantum implementation of the DNA-based algorithm\n[Ho et al. 2005] a faster labeling of the target state is attained, which also\nimplies a speedy solution to an instance of the clique problem."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2141v1", 
    "title": "Curse of Dimensionality in the Application of Pivot-based Indexes to the   Similarity Search Problem", 
    "arxiv-id": "0905.2141v1", 
    "author": "Ilya Volnyansky", 
    "publish": "2009-05-13T16:24:21Z", 
    "summary": "In this work we study the validity of the so-called curse of dimensionality\nfor indexing of databases for similarity search. We perform an asymptotic\nanalysis, with a test model based on a sequence of metric spaces $(\\Omega_d)$\nfrom which we pick datasets $X_d$ in an i.i.d. fashion. We call the subscript\n$d$ the dimension of the space $\\Omega_d$ (e.g. for $\\mathbb{R}^d$ the\ndimension is just the usual one) and we allow the size of the dataset $n=n_d$\nto be such that $d$ is superlogarithmic but subpolynomial in $n$.\n  We study the asymptotic performance of pivot-based indexing schemes where the\nnumber of pivots is $o(n/d)$. We pick the relatively simple cost model of\nsimilarity search where we count each distance calculation as a single\ncomputation and disregard the rest.\n  We demonstrate that if the spaces $\\Omega_d$ exhibit the (fairly common)\nconcentration of measure phenomenon the performance of similarity search using\nsuch indexes is asymptotically linear in $n$. That is for large enough $d$ the\ndifference between using such an index and performing a search without an index\nat all is negligeable. Thus we confirm the curse of dimensionality in this\nsetting."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2213v3", 
    "title": "Outlining an elegant solver for 3-SAT", 
    "arxiv-id": "0905.2213v3", 
    "author": "Eduardo Hwang", 
    "publish": "2009-05-13T22:23:07Z", 
    "summary": "The purpose of this article is to incite clever ways to attack problems. It\nadvocates in favor of more elegant algorithms, in place of brute force (albeit\nits very well crafted) usages."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.3107v1", 
    "title": "Fast and Compact Prefix Codes", 
    "arxiv-id": "0905.3107v1", 
    "author": "Yakov Nekrich", 
    "publish": "2009-05-19T14:19:08Z", 
    "summary": "It is well-known that, given a probability distribution over $n$ characters,\nin the worst case it takes (\\Theta (n \\log n)) bits to store a prefix code with\nminimum expected codeword length. However, in this paper we first show that,\nfor any $0<\\epsilon<1/2$ with (1 / \\epsilon = \\Oh{\\polylog{n}}), it takes\n$\\Oh{n \\log \\log (1 / \\epsilon)}$ bits to store a prefix code with expected\ncodeword length within $\\epsilon$ of the minimum. We then show that, for any\nconstant (c > 1), it takes $\\Oh{n^{1 / c} \\log n}$ bits to store a prefix code\nwith expected codeword length at most $c$ times the minimum. In both cases, our\ndata structures allow us to encode and decode any character in $\\Oh{1}$ time."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.4068v4", 
    "title": "A 4/3-competitive randomized algorithm for online scheduling of packets   with agreeable deadlines", 
    "arxiv-id": "0905.4068v4", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2009-05-25T19:43:24Z", 
    "summary": "In 2005 Li et al. gave a phi-competitive deterministic online algorithm for\nscheduling of packets with agreeable deadlines with a very interesting\nanalysis. This is known to be optimal due to a lower bound by Hajek. We claim\nthat the algorithm by Li et al. can be slightly simplified, while retaining its\ncompetitive ratio. Then we introduce randomness to the modified algorithm and\nargue that the competitive ratio against oblivious adversary is at most 4/3.\nNote that this still leaves a gap between the best known lower bound of 5/4 by\nChin et al. for randomised algorithms against oblivious adversary."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.4444v1", 
    "title": "Approximation Algorithms for the Traveling Repairman and Speeding   Deliveryman Problems", 
    "arxiv-id": "0905.4444v1", 
    "author": "Barry Wittman", 
    "publish": "2009-05-27T15:05:28Z", 
    "summary": "Constant-factor, polynomial-time approximation algorithms are presented for\ntwo variations of the traveling salesman problem with time windows. In the\nfirst variation, the traveling repairman problem, the goal is to find a tour\nthat visits the maximum possible number of locations during their time windows.\nIn the second variation, the speeding deliveryman problem, the goal is to find\na tour that uses the minimum possible speedup to visit all locations during\ntheir time windows. For both variations, the time windows are of unit length,\nand the distance metric is based on a weighted, undirected graph. Algorithms\nwith improved approximation ratios are given for the case when the input is\ndefined on a tree rather than a general graph. The algorithms are also extended\nto handle time windows whose lengths fall in any bounded range."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2010.12.011", 
    "link": "http://arxiv.org/pdf/0905.4930v2", 
    "title": "Improved Approximation Algorithms for Segment Minimization in Intensity   Modulated Radiation Therapy", 
    "arxiv-id": "0905.4930v2", 
    "author": "Maxwell Young", 
    "publish": "2009-05-29T17:56:06Z", 
    "summary": "he segment minimization problem consists of finding the smallest set of\ninteger matrices that sum to a given intensity matrix, such that each summand\nhas only one non-zero value, and the non-zeroes in each row are consecutive.\nThis has direct applications in intensity-modulated radiation therapy, an\neffective form of cancer treatment. We develop three approximation algorithms\nfor matrices with arbitrarily many rows. Our first two algorithms improve the\napproximation factor from the previous best of $1+\\log_2 h $ to (roughly) $3/2\n\\cdot (1+\\log_3 h)$ and $11/6\\cdot(1+\\log_4{h})$, respectively, where $h$ is\nthe largest entry in the intensity matrix. We illustrate the limitations of the\nspecific approach used to obtain these two algorithms by proving a lower bound\nof $\\frac{(2b-2)}{b}\\cdot\\log_b{h} + \\frac{1}{b}$ on the approximation\nguarantee. Our third algorithm improves the approximation factor from $2 \\cdot\n(\\log D+1)$ to $24/13 \\cdot (\\log D+1)$, where $D$ is (roughly) the largest\ndifference between consecutive elements of a row of the intensity matrix.\nFinally, experimentation with these algorithms shows that they perform well\nwith respect to the optimum and outperform other approximation algorithms on\n77% of the 122 test cases we consider, which include both real world and\nsynthetic data."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2010.12.011", 
    "link": "http://arxiv.org/pdf/0906.0328v1", 
    "title": "Rivisiting Token/Bucket Algorithms in New Applications", 
    "arxiv-id": "0906.0328v1", 
    "author": "Andrea Pasquinucci", 
    "publish": "2009-06-01T18:06:45Z", 
    "summary": "We consider a somehow peculiar Token/Bucket problem which at first sight\nlooks confusing and difficult to solve. The winning approach to solve the\nproblem consists in going back to the simple and traditional methods to solve\ncomputer science problems like the one taught to us by Knuth. Somehow the main\ntrick is to be able to specify clearly what needs to be achieved, and then the\nsolution, even if complex, appears almost by itself."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SISAP.2009.9", 
    "link": "http://arxiv.org/pdf/0906.0391v2", 
    "title": "Curse of Dimensionality in Pivot-based Indexes", 
    "arxiv-id": "0906.0391v2", 
    "author": "Vladimir Pestov", 
    "publish": "2009-06-02T00:41:46Z", 
    "summary": "We offer a theoretical validation of the curse of dimensionality in the\npivot-based indexing of datasets for similarity search, by proving, in the\nframework of statistical learning, that in high dimensions no pivot-based\nindexing scheme can essentially outperform the linear scan.\n  A study of the asymptotic performance of pivot-based indexing schemes is\nperformed on a sequence of datasets modeled as samples $X_d$ picked in i.i.d.\nfashion from metric spaces $\\Omega_d$. We allow the size of the dataset $n=n_d$\nto be such that $d$, the ``dimension'', is superlogarithmic but subpolynomial\nin $n$. The number of pivots is allowed to grow as $o(n/d)$. We pick the least\nrestrictive cost model of similarity search where we count each distance\ncalculation as a single computation and disregard the rest.\n  We demonstrate that if the intrinsic dimension of the spaces $\\Omega_d$ in\nthe sense of concentration of measure phenomenon is $O(d)$, then the\nperformance of similarity search pivot-based indexes is asymptotically linear\nin $n$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03751-1_12", 
    "link": "http://arxiv.org/pdf/0906.0862v1", 
    "title": "A Memetic Algorithm for the Multidimensional Assignment Problem", 
    "arxiv-id": "0906.0862v1", 
    "author": "Daniel Karapetyan", 
    "publish": "2009-06-04T09:44:59Z", 
    "summary": "The Multidimensional Assignment Problem (MAP or s-AP in the case of s\ndimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we propose a memetic algorithm for\nMAP that is a combination of a genetic algorithm with a local search procedure.\nThe main contribution of the paper is an idea of dynamically adjusted\ngeneration size, that yields an outstanding flexibility of the algorithm to\nperform well for both small and large fixed running times. The results of\ncomputational experiments for several instance families show that the proposed\nalgorithm produces solutions of very high quality in a reasonable time and\noutperforms the state-of-the art 3-AP memetic algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03751-1_12", 
    "link": "http://arxiv.org/pdf/0906.1341v1", 
    "title": "Efficient Algorithms for Several Constrained Activity Scheduling   Problems in the Time and Space Domains", 
    "arxiv-id": "0906.1341v1", 
    "author": "Angela Andreica", 
    "publish": "2009-06-07T09:35:01Z", 
    "summary": "In this paper we consider several constrained activity scheduling problems in\nthe time and space domains, like finding activity orderings which optimize the\nvalues of several objective functions (time scheduling) or finding optimal\nlocations where certain types of activities will take place (space scheduling).\nWe present novel, efficient algorithmic solutions for all the considered\nproblems, based on the dynamic programming and greedy techniques. In each case\nwe compute exact, optimal solutions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2020v1", 
    "title": "Scheduling with Outliers", 
    "arxiv-id": "0906.2020v1", 
    "author": "Danny Segev", 
    "publish": "2009-06-10T21:22:22Z", 
    "summary": "In classical scheduling problems, we are given jobs and machines, and have to\nschedule all the jobs to minimize some objective function. What if each job has\na specified profit, and we are no longer required to process all jobs -- we can\nschedule any subset of jobs whose total profit is at least a (hard) target\nprofit requirement, while still approximately minimizing the objective\nfunction?\n  We refer to this class of problems as scheduling with outliers. This model\nwas initiated by Charikar and Khuller (SODA'06) on the minimum max-response\ntime in broadcast scheduling. We consider three other well-studied scheduling\nobjectives: the generalized assignment problem, average weighted completion\ntime, and average flow time, and provide LP-based approximation algorithms for\nthem. For the minimum average flow time problem on identical machines, we give\na logarithmic approximation algorithm for the case of unit profits based on\nrounding an LP relaxation; we also show a matching integrality gap. For the\naverage weighted completion time problem on unrelated machines, we give a\nconstant factor approximation. The algorithm is based on randomized rounding of\nthe time-indexed LP relaxation strengthened by the knapsack-cover inequalities.\nFor the generalized assignment problem with outliers, we give a simple\nreduction to GAP without outliers to obtain an algorithm whose makespan is\nwithin 3 times the optimum makespan, and whose cost is at most (1 + \\epsilon)\ntimes the optimal cost."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2048v1", 
    "title": "Minimizing Maximum Response Time and Delay Factor in Broadcast   Scheduling", 
    "arxiv-id": "0906.2048v1", 
    "author": "Benjamin Moseley", 
    "publish": "2009-06-11T07:24:52Z", 
    "summary": "We consider online algorithms for pull-based broadcast scheduling. In this\nsetting there are n pages of information at a server and requests for pages\narrive online. When the server serves (broadcasts) a page p, all outstanding\nrequests for that page are satisfied. We study two related metrics, namely\nmaximum response time (waiting time) and maximum delay-factor and their\nweighted versions. We obtain the following results in the worst-case online\ncompetitive model.\n  - We show that FIFO (first-in first-out) is 2-competitive even when the page\nsizes are different. Previously this was known only for unit-sized pages [10]\nvia a delicate argument. Our proof differs from [10] and is perhaps more\nintuitive.\n  - We give an online algorithm for maximum delay-factor that is\nO(1/eps^2)-competitive with (1+\\eps)-speed for unit-sized pages and with\n(2+\\eps)-speed for different sized pages. This improves on the algorithm in\n[12] which required (2+\\eps)-speed and (4+\\eps)-speed respectively. In addition\nwe show that the algorithm and analysis can be extended to obtain the same\nresults for maximum weighted response time and delay factor.\n  - We show that a natural greedy algorithm modeled after LWF\n(Longest-Wait-First) is not O(1)-competitive for maximum delay factor with any\nconstant speed even in the setting of standard scheduling with unit-sized jobs.\nThis complements our upper bound and demonstrates the importance of the\ntradeoff made in our algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2395v1", 
    "title": "Longest Wait First for Broadcast Scheduling", 
    "arxiv-id": "0906.2395v1", 
    "author": "Benjamin Moseley", 
    "publish": "2009-06-12T18:27:07Z", 
    "summary": "We consider online algorithms for broadcast scheduling. In the pull-based\nbroadcast model there are $n$ unit-sized pages of information at a server and\nrequests arrive online for pages. When the server transmits a page $p$, all\noutstanding requests for that page are satisfied. The longest-wait-first} (LWF)\nalgorithm is a natural algorithm that has been shown to have good empirical\nperformance. In this paper we make two main contributions to the analysis of\nLWF and broadcast scheduling. \\begin{itemize} \\item We give an intuitive and\neasy to understand analysis of LWF which shows that it is\n$O(1/\\eps^2)$-competitive for average flow-time with $(4+\\eps)$ speed. Using a\nmore involved analysis, we show that LWF is $O(1/\\eps^3)$-competitive for\naverage flow-time with $(3.4+\\epsilon)$ speed. \\item We show that a natural\nextension of LWF is O(1)-speed O(1)-competitive for more general objective\nfunctions such as average delay-factor and $L_k$ norms of delay-factor (for\nfixed $k$). \\end{itemize}"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2448v1", 
    "title": "The Limit of Convexity Based Isoperimetry: Sampling Harmonic-Concave   Functions", 
    "arxiv-id": "0906.2448v1", 
    "author": "Santosh Vempala", 
    "publish": "2009-06-13T05:15:53Z", 
    "summary": "Logconcave functions represent the current frontier of efficient algorithms\nfor sampling, optimization and integration in R^n. Efficient sampling\nalgorithms to sample according to a probability density (to which the other two\nproblems can be reduced) relies on good isoperimetry which is known to hold for\narbitrary logconcave densities. In this paper, we extend this frontier in two\nways: first, we characterize convexity-like conditions that imply good\nisoperimetry, i.e., what condition on function values along every line\nguarantees good isoperimetry? The answer turns out to be the set of\n(1/(n-1))-harmonic concave functions in R^n; we also prove that this is the\nbest possible characterization along every line, of functions having good\nisoperimetry. Next, we give the first efficient algorithm for sampling\naccording to such functions with complexity depending on a smoothness\nparameter. Further, noting that the multivariate Cauchy density is an important\ndistribution in this class, we exploit certain properties of the Cauchy density\nto give an efficient sampling algorithm based on random walks with a mixing\ntime that matches the current best bounds known for sampling logconcave\nfunctions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2671v1", 
    "title": "Oscillations and Random Perturbations of a FitzHugh-Nagumo System", 
    "arxiv-id": "0906.2671v1", 
    "author": "Mich\u00e8le Thieullen", 
    "publish": "2009-06-15T12:29:32Z", 
    "summary": "We consider a stochastic perturbation of a FitzHugh-Nagumo system. We show\nthat it is possible to generate oscillations for values of parameters which do\nnot allow oscillations for the deterministic system. We also study the\nappearance of a new equilibrium point and new bifurcation parameters due to the\nnoisy component."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2960v1", 
    "title": "Empirical evaluation of construction heuristics for the multidimensional   assignment problem", 
    "arxiv-id": "0906.2960v1", 
    "author": "Boris Goldengorin", 
    "publish": "2009-06-16T15:47:17Z", 
    "summary": "The multidimensional assignment problem (MAP) (abbreviated s-AP in the case\nof s dimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we consider four fast construction\nheuristics for MAP. One of the heuristics is new. A modification of the\nheuristics is proposed to optimize the access to slow computer memory. The\nresults of computational experiments for several instance families are provided\nand discussed."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.3056v1", 
    "title": "Approximating Scheduling Machines with Capacity Constraints", 
    "arxiv-id": "0906.3056v1", 
    "author": "Jing Liu", 
    "publish": "2009-06-17T02:01:26Z", 
    "summary": "In the Scheduling Machines with Capacity Constraints problem, we are given k\nidentical machines, each of which can process at most m_i jobs. M jobs are also\ngiven, where job j has a non-negative processing time length t_j >= 0. The task\nis to find a schedule such that the makespan is minimized and the capacity\nconstraints are met. In this paper, we present a 3-approximation algorithm\nusing an extension of Iterative Rounding Method introduced by Jain. To the best\nof the authors' knowledge, this is the first attempt to apply Iterative\nRounding Method to scheduling problem with capacity constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5050v1", 
    "title": "AFPTAS results for common variants of bin packing: A new method to   handle the small items", 
    "arxiv-id": "0906.5050v1", 
    "author": "Asaf Levin", 
    "publish": "2009-06-27T08:39:18Z", 
    "summary": "We consider two well-known natural variants of bin packing, and show that\nthese packing problems admit asymptotic fully polynomial time approximation\nschemes (AFPTAS). In bin packing problems, a set of one-dimensional items of\nsize at most 1 is to be assigned (packed) to subsets of sum at most 1 (bins).\nIt has been known for a while that the most basic problem admits an AFPTAS. In\nthis paper, we develop methods that allow to extend this result to other\nvariants of bin packing. Specifically, the problems which we study in this\npaper, for which we design asymptotic fully polynomial time approximation\nschemes, are the following. The first problem is \"Bin packing with cardinality\nconstraints\", where a parameter k is given, such that a bin may contain up to k\nitems. The goal is to minimize the number of bins used. The second problem is\n\"Bin packing with rejection\", where every item has a rejection penalty\nassociated with it. An item needs to be either packed to a bin or rejected, and\nthe goal is to minimize the number of used bins plus the total rejection\npenalty of unpacked items. This resolves the complexity of two important\nvariants of the bin packing problem. Our approximation schemes use a novel\nmethod for packing the small items. This new method is the core of the improved\nrunning times of our schemes over the running times of the previous results,\nwhich are only asymptotic polynomial time approximation schemes (APTAS)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5051v1", 
    "title": "Bin packing with general cost structures", 
    "arxiv-id": "0906.5051v1", 
    "author": "Asaf Levin", 
    "publish": "2009-06-27T19:48:10Z", 
    "summary": "Following the work of Anily et al., we consider a variant of bin packing,\ncalled \"bin packing with general cost structures\" (GCBP) and design an\nasymptotic fully polynomial time approximation scheme (AFPTAS) for this\nproblem. In the classic bin packing problem, a set of one-dimensional items is\nto be assigned to subsets of total size at most 1, that is, to be packed into\nunit sized bins. However, in GCBP, the cost of a bin is not 1 as in classic bin\npacking, but it is a non-decreasing and concave function of the number of items\npacked in it, where the cost of an empty bin is zero. The construction of the\nAFPTAS requires novel techniques for dealing with small items, which are\ndeveloped in this work. In addition, we develop a fast approximation algorithm\nwhich acts identically for all non-decreasing and concave functions, and has an\nasymptotic approximation ratio of 1.5 for all functions simultaneously."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5062v2", 
    "title": "Geometrical Interpretation of the Master Theorem for Divide-and-conquer   Recurrences", 
    "arxiv-id": "0906.5062v2", 
    "author": "Simant Dube", 
    "publish": "2009-06-29T04:43:17Z", 
    "summary": "We provide geometrical interpretation of the Master Theorem to solve\ndivide-and-conquer recurrences. We show how different cases of the recurrences\ncorrespond to different kinds of fractal images. Fractal dimension and\nHausdorff measure are shown to be closely related to the solution of such\nrecurrences."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5070v1", 
    "title": "Integrating Genetic Algorithm, Tabu Search Approach for Job Shop   Scheduling", 
    "arxiv-id": "0906.5070v1", 
    "author": "Dr. P. Balasubramanie", 
    "publish": "2009-06-27T11:16:30Z", 
    "summary": "This paper presents a new algorithm based on integrating Genetic Algorithms\nand Tabu Search methods to solve the Job Shop Scheduling problem. The idea of\nthe proposed algorithm is derived from Genetic Algorithms. Most of the\nscheduling problems require either exponential time or space to generate an\noptimal answer. Job Shop scheduling (JSS) is the general scheduling problem and\nit is a NP-complete problem, but it is difficult to find the optimal solution.\nThis paper applies Genetic Algorithms and Tabu Search for Job Shop Scheduling\nproblem and compares the results obtained by each. With the implementation of\nour approach the JSS problems reaches optimal solution and minimize the\nmakespan."
},{
    "category": "cs.DS", 
    "doi": "10.1137/100801901", 
    "link": "http://arxiv.org/pdf/0907.0305v1", 
    "title": "Improved approximation guarantees for weighted matching in the   semi-streaming model", 
    "arxiv-id": "0907.0305v1", 
    "author": "Danny Segev", 
    "publish": "2009-07-02T08:11:22Z", 
    "summary": "We study the maximum weight matching problem in the semi-streaming model, and\nimprove on the currently best one-pass algorithm due to Zelke (Proc. of\nSTACS2008, pages 669-680) by devising a deterministic approach whose\nperformance guarantee is 4.91+epsilon. In addition, we study preemptive online\nalgorithms, a sub-class of one-pass algorithms where we are only allowed to\nmaintain a feasible matching in memory at any point in time. All known results\nprior to Zelke's belong to this sub-class. We provide a lower bound of 4.967 on\nthe competitive ratio of any such deterministic algorithm, and hence show that\nfuture improvements will have to store in memory a set of edges which is not\nnecessarily a feasible matching."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0718v2", 
    "title": "A Randomized Sublinear Time Parallel GCD Algorithm for the EREW PRAM", 
    "arxiv-id": "0907.0718v2", 
    "author": "Jonathan P. Sorenson", 
    "publish": "2009-07-03T21:12:38Z", 
    "summary": "We present a randomized parallel algorithm that computes the greatest common\ndivisor of two integers of n bits in length with probability 1-o(1) that takes\nO(n loglog n / log n) expected time using n^{6+\\epsilon} processors on the EREW\nPRAM parallel model of computation. We believe this to be the first randomized\nsublinear time algorithm on the EREW PRAM for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0726v2", 
    "title": "Asymmetric Traveling Salesman Path and Directed Latency Problems", 
    "arxiv-id": "0907.0726v2", 
    "author": "Zoya Svitkina", 
    "publish": "2009-07-03T22:42:23Z", 
    "summary": "We study integrality gaps and approximability of two closely related problems\non directed graphs. Given a set V of n nodes in an underlying asymmetric metric\nand two specified nodes s and t, both problems ask to find an s-t path visiting\nall other nodes. In the asymmetric traveling salesman path problem (ATSPP), the\nobjective is to minimize the total cost of this path. In the directed latency\nproblem, the objective is to minimize the sum of distances on this path from s\nto each node. Both of these problems are NP-hard. The best known approximation\nalgorithms for ATSPP had ratio O(log n) until the very recent result that\nimproves it to O(log n/ log log n). However, only a bound of O(sqrt(n)) for the\nintegrality gap of its linear programming relaxation has been known. For\ndirected latency, the best previously known approximation algorithm has a\nguarantee of O(n^(1/2+eps)), for any constant eps > 0. We present a new\nalgorithm for the ATSPP problem that has an approximation ratio of O(log n),\nbut whose analysis also bounds the integrality gap of the standard LP\nrelaxation of ATSPP by the same factor. This solves an open problem posed by\nChekuri and Pal [2007]. We then pursue a deeper study of this linear program\nand its variations, which leads to an algorithm for the k-person ATSPP (where k\ns-t paths of minimum total length are sought) and an O(log n)-approximation for\nthe directed latency problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0741v1", 
    "title": "Tight Bounds for Online Stable Sorting", 
    "arxiv-id": "0907.0741v1", 
    "author": "Yakov Nekrich", 
    "publish": "2009-07-04T06:18:38Z", 
    "summary": "Although many authors have considered how many ternary comparisons it takes\nto sort a multiset $S$ of size $n$, the best known upper and lower bounds still\ndiffer by a term linear in $n$. In this paper we restrict our attention to\nonline stable sorting and prove upper and lower bounds that are within (o (n))\nnot only of each other but also of the best known upper bound for offline\nsorting. Specifically, we first prove that if the number of distinct elements\n(\\sigma = o (n / \\log n)), then ((H + 1) n + o (n)) comparisons are sufficient,\nwhere $H$ is the entropy of the distribution of the elements in $S$. We then\ngive a simple proof that ((H + 1) n - o (n)) comparisons are necessary in the\nworst case."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1295v1", 
    "title": "Online Sorting via Searching and Selection", 
    "arxiv-id": "0907.1295v1", 
    "author": "Jonathan P. Sorenson", 
    "publish": "2009-07-07T20:37:19Z", 
    "summary": "In this paper, we present a framework based on a simple data structure and\nparameterized algorithms for the problems of finding items in an unsorted list\nof linearly ordered items based on their rank (selection) or value (search). As\na side-effect of answering these online selection and search queries, we\nprogressively sort the list. Our algorithms are based on Hoare's Quickselect,\nand are parameterized based on the pivot selection method.\n  For example, if we choose the pivot as the last item in a subinterval, our\nframework yields algorithms that will answer q<=n unique selection and/or\nsearch queries in a total of O(n log q) average time. After q=\\Omega(n) queries\nthe list is sorted. Each repeated selection query takes constant time, and each\nrepeated search query takes O(log n) time. The two query types can be\ninterleaved freely. By plugging different pivot selection methods into our\nframework, these results can, for example, become randomized expected time or\ndeterministic worst-case time. Our methods are easy to implement, and we show\nthey perform well in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1369v1", 
    "title": "Towards an $O(\\sqrt[3]{\\log n})$-Approximation Algorithm for {\\sc   Balanced Separator}", 
    "arxiv-id": "0907.1369v1", 
    "author": "Manjish Pal", 
    "publish": "2009-07-08T09:32:02Z", 
    "summary": "The {\\sc $c$-Balanced Separator} problem is a graph-partitioning problem in\nwhich given a graph $G$, one aims to find a cut of minimum size such that both\nthe sides of the cut have at least $cn$ vertices. In this paper, we present new\ndirections of progress in the {\\sc $c$-Balanced Separator} problem. More\nspecifically, we propose a new family of mathematical programs, which depends\nupon a parameter $\\epsilon > 0$, and extend the seminal work of\nArora-Rao-Vazirani ({\\sf ARV}) \\cite{ARV} to show that the polynomial time\nsolvability of the proposed family of programs implies an improvement in the\napproximation factor to $O(\\log^{{1/3} + \\epsilon} n)$ from the best-known\nfactor of $O(\\sqrt{\\log n})$ due to {\\sf ARV}. In fact, for $\\epsilon = 1/3$,\nthe program we get is the SDP proposed by {\\sf ARV}. For $\\epsilon < 1/3$, this\nfamily of programs is not convex but one can transform them into so called\n\\emph{\\textbf{concave programs}} in which one optimizes a concave function over\na convex feasible set. The properties of concave programs allows one to apply\ntechniques due to Hoffman \\cite{H81} or Tuy \\emph{et al} \\cite{TTT85} to solve\nsuch problems with arbitrary accuracy. But the problem of finding of a method\nto solve these programs that converges in polynomial time still remains open.\nOur result, although conditional, introduces a new family of programs which is\nmore powerful than semi-definite programming in the context of approximation\nalgorithms and hence it will of interest to investigate this family both in the\ndirection of designing efficient algorithms and proving hardness results."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1840v1", 
    "title": "A PTAS for the Minimum Consensus Clustering Problem with a Fixed Number   of Clusters", 
    "arxiv-id": "0907.1840v1", 
    "author": "Riccardo Dondi", 
    "publish": "2009-07-10T15:16:43Z", 
    "summary": "The Consensus Clustering problem has been introduced as an effective way to\nanalyze the results of different microarray experiments. The problem consists\nof looking for a partition that best summarizes a set of input partitions (each\ncorresponding to a different microarray experiment) under a simple and\nintuitive cost function. The problem admits polynomial time algorithms on two\ninput partitions, but is APX-hard on three input partitions. We investigate the\nrestriction of Consensus Clustering when the output partition is required to\ncontain at most k sets, giving a polynomial time approximation scheme (PTAS)\nwhile proving the NP-hardness of this restriction."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2050v2", 
    "title": "Randomised Buffer Management with Bounded Delay against Adaptive   Adversary", 
    "arxiv-id": "0907.2050v2", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2009-07-12T16:11:45Z", 
    "summary": "We give a new analysis of the RMix algorithm by Chin et al. for the Buffer\nManagement with Bounded Delay problem (or online scheduling of unit jobs to\nmaximise weighted throughput). Unlike the original proof of\ne/(e-1)-competitiveness, the new one holds even in adaptive-online adversary\nmodel. In fact, the proof works also for a slightly more general problem\nstudied by Bie{\\'n}kowski et al."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2071v1", 
    "title": "Layered Working-Set Trees", 
    "arxiv-id": "0907.2071v1", 
    "author": "John Howat", 
    "publish": "2009-07-13T18:11:05Z", 
    "summary": "The working-set bound [Sleator and Tarjan, J. ACM, 1985] roughly states that\nsearching for an element is fast if the element was accessed recently. Binary\nsearch trees, such as splay trees, can achieve this property in the amortized\nsense, while data structures that are not binary search trees are known to have\nthis property in the worst case. We close this gap and present a binary search\ntree called a layered working-set tree that guarantees the working-set property\nin the worst case. The unified bound [Badoiu et al., TCS, 2007] roughly states\nthat searching for an element is fast if it is near (in terms of rank distance)\nto a recently accessed element. We show how layered working-set trees can be\nused to achieve the unified bound to within a small additive term in the\namortized sense while maintaining in the worst case an access time that is both\nlogarithmic and within a small multiplicative factor of the working-set bound."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2741v1", 
    "title": "Bounded Delay Packet Scheduling in a Bounded Buffer", 
    "arxiv-id": "0907.2741v1", 
    "author": "Stanley P. Y. Fung", 
    "publish": "2009-07-16T04:05:05Z", 
    "summary": "We study the problem of buffer management in QoS-enabled network switches in\nthe bounded delay model where each packet is associated with a weight and a\ndeadline. We consider the more realistic situation where the network switch has\na finite buffer size. A 9.82-competitive algorithm is known for the case of\nmultiple buffers (Azar and Levy, SWAT'06). Recently, for the case of a single\nbuffer, a 3-competitive deterministic algorithm and a 2.618-competitive\nrandomized algorithm was known (Li, INFOCOM'09). In this paper we give a simple\ndeterministic 2-competitive algorithm for the case of a single buffer."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.3135v2", 
    "title": "Fast Searching in Packed Strings", 
    "arxiv-id": "0907.3135v2", 
    "author": "Philip Bille", 
    "publish": "2009-07-17T19:29:13Z", 
    "summary": "Given strings $P$ and $Q$ the (exact) string matching problem is to find all\npositions of substrings in $Q$ matching $P$. The classical Knuth-Morris-Pratt\nalgorithm [SIAM J. Comput., 1977] solves the string matching problem in linear\ntime which is optimal if we can only read one character at the time. However,\nmost strings are stored in a computer in a packed representation with several\ncharacters in a single word, giving us the opportunity to read multiple\ncharacters simultaneously. In this paper we study the worst-case complexity of\nstring matching on strings given in packed representation. Let $m \\leq n$ be\nthe lengths $P$ and $Q$, respectively, and let $\\sigma$ denote the size of the\nalphabet. On a standard unit-cost word-RAM with logarithmic word size we\npresent an algorithm using time $$ O\\left(\\frac{n}{\\log_\\sigma n} + m +\n\\occ\\right). $$ Here $\\occ$ is the number of occurrences of $P$ in $Q$. For $m\n= o(n)$ this improves the $O(n)$ bound of the Knuth-Morris-Pratt algorithm.\nFurthermore, if $m = O(n/\\log_\\sigma n)$ our algorithm is optimal since any\nalgorithm must spend at least $\\Omega(\\frac{(n+m)\\log\n  \\sigma}{\\log n} + \\occ) = \\Omega(\\frac{n}{\\log_\\sigma n} + \\occ)$ time to\nread the input and report all occurrences. The result is obtained by a novel\nautomaton construction based on the Knuth-Morris-Pratt algorithm combined with\na new compact representation of subautomata allowing an optimal\ntabulation-based simulation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5269v1", 
    "title": "Fast Detour Computation for Ride Sharing", 
    "arxiv-id": "0907.5269v1", 
    "author": "Lars Volker", 
    "publish": "2009-07-30T11:31:05Z", 
    "summary": "Todays ride sharing services still mimic a better billboard. They list the\noffers and allow to search for the source and target city, sometimes enriched\nwith radial search. So finding a connection between big cities is quite easy.\nThese places are on a list of designated origin and distination points. But\nwhen you want to go from a small town to another small town, even when they are\nnext to a freeway, you run into problems. You can't find offers that would or\ncould pass by the town easily with little or no detour. We solve this\ninteresting problem by presenting a fast algorithm that computes the offers\nwith the smallest detours w.r.t. a request. Our experiments show that the\nproblem is efficiently solvable in times suitable for a web service\nimplementation. For realistic database size we achieve lookup times of about\n5ms and a matching rate of 90% instead of just 70% for the simple matching\nalgorithms used today."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5372v1", 
    "title": "Speedup in the Traveling Repairman Problem with Unit Time Windows", 
    "arxiv-id": "0907.5372v1", 
    "author": "Barry Wittman", 
    "publish": "2009-07-30T16:30:15Z", 
    "summary": "The input to the unrooted traveling repairman problem is an undirected metric\ngraph and a subset of nodes, each of which has a time window of unit length.\nGiven that a repairman can start at any location, the goal is to plan a route\nthat visits as many nodes as possible during their respective time windows. A\npolynomial-time bicriteria approximation algorithm is presented for this\nproblem, gaining an increased fraction of repairman visits for increased\nspeedup of repairman motion. For speedup $s$, we find a $6\\gamma/(s +\n1)$-approximation for $s$ in the range $1 \\leq s \\leq 2$ and a\n$4\\gamma/s$-approximation for $s$ in the range $2 \\leq s \\leq 4$, where $\\gamma\n= 1$ on tree-shaped networks and $\\gamma = 2 + \\epsilon$ on general metric\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5474v1", 
    "title": "Optimal Angular Resolution for Face-Symmetric Drawings", 
    "arxiv-id": "0907.5474v1", 
    "author": "Kevin A. Wortman", 
    "publish": "2009-07-31T06:27:19Z", 
    "summary": "Let G be a graph that may be drawn in the plane in such a way that all\ninternal faces are centrally symmetric convex polygons. We show how to find a\ndrawing of this type that maximizes the angular resolution of the drawing, the\nminimum angle between any two incident edges, in polynomial time, by reducing\nthe problem to one of finding parametric shortest paths in an auxiliary graph.\nThe running time is at most O(t^3), where t is a parameter of the input graph\nthat is at most O(n) but is more typically proportional to n^.5."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0239v1", 
    "title": "On Bijective Variants of the Burrows-Wheeler Transform", 
    "arxiv-id": "0908.0239v1", 
    "author": "Manfred Kufleitner", 
    "publish": "2009-08-03T13:10:48Z", 
    "summary": "The sort transform (ST) is a modification of the Burrows-Wheeler transform\n(BWT). Both transformations map an arbitrary word of length n to a pair\nconsisting of a word of length n and an index between 1 and n. The BWT sorts\nall rotation conjugates of the input word, whereas the ST of order k only uses\nthe first k letters for sorting all such conjugates. If two conjugates start\nwith the same prefix of length k, then the indices of the rotations are used\nfor tie-breaking. Both transforms output the sequence of the last letters of\nthe sorted list and the index of the input within the sorted list. In this\npaper, we discuss a bijective variant of the BWT (due to Scott), proving its\ncorrectness and relations to other results due to Gessel and Reutenauer (1993)\nand Crochemore, Desarmenien, and Perrin (2005). Further, we present a novel\nbijective variant of the ST."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0350v1", 
    "title": "Region growing for multi-route cuts", 
    "arxiv-id": "0908.0350v1", 
    "author": "Shuchi Chawla", 
    "publish": "2009-08-03T21:21:06Z", 
    "summary": "We study a number of multi-route cut problems: given a graph G=(V,E) and\nconnectivity thresholds k_(u,v) on pairs of nodes, the goal is to find a\nminimum cost set of edges or vertices the removal of which reduces the\nconnectivity between every pair (u,v) to strictly below its given threshold.\nThese problems arise in the context of reliability in communication networks;\nThey are natural generalizations of traditional minimum cut problems where the\nthresholds are either 1 (we want to completely separate the pair) or infinity\n(we don't care about the connectivity for the pair). We provide the first\nnon-trivial approximations to a number of variants of the problem including for\nboth node-disjoint and edge-disjoint connectivity thresholds. A main\ncontribution of our work is an extension of the region growing technique for\napproximating minimum multicuts to the multi-route setting. When the\nconnectivity thresholds are either 2 or infinity (the \"2-route cut\" case), we\nobtain polylogarithmic approximations while satisfying the thresholds exactly.\nFor arbitrary connectivity thresholds this approach leads to bicriteria\napproximations where we approximately satisfy the thresholds and approximately\nminimize the cost. We present a number of different algorithms achieving\ndifferent cost-connectivity tradeoffs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0375v1", 
    "title": "Deterministic Algorithms for the Lovasz Local Lemma", 
    "arxiv-id": "0908.0375v1", 
    "author": "Bernhard Haeupler", 
    "publish": "2009-08-04T02:13:54Z", 
    "summary": "The Lovasz Local Lemma (LLL) is a powerful result in probability theory that\nstates that the probability that none of a set of bad events happens is nonzero\nif the probability of each event is small compared to the number of events that\ndepend on it. It is often used in combination with the probabilistic method for\nnon-constructive existence proofs. A prominent application is to k-CNF\nformulas, where LLL implies that, if every clause in the formula shares\nvariables with at most d <= 2^k/e other clauses then such a formula has a\nsatisfying assignment. Recently, a randomized algorithm to efficiently\nconstruct a satisfying assignment was given by Moser. Subsequently Moser and\nTardos gave a randomized algorithm to construct the structures guaranteed by\nthe LLL in a very general algorithmic framework. We address the main problem\nleft open by Moser and Tardos of derandomizing these algorithms efficiently.\nSpecifically, for a k-CNF formula with m clauses and d <= 2^{k/(1+\\eps)}/e for\nsome \\eps\\in (0,1), we give an algorithm that finds a satisfying assignment in\ntime \\tilde{O}(m^{2(1+1/\\eps)}). This improves upon the deterministic\nalgorithms of Moser and of Moser-Tardos with running time m^{\\Omega(k^2)} which\nis superpolynomial for k=\\omega(1) and upon other previous algorithms which\nwork only for d\\leq 2^{k/16}/e. Our algorithm works efficiently for a general\nversion of LLL under the algorithmic framework of Moser and Tardos, and is also\nparallelizable, i.e., has polylogarithmic running time using polynomially many\nprocessors."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.1379v1", 
    "title": "Breaking the Multicommodity Flow Barrier for sqrt(log(n))-Approximations   to Sparsest Cut", 
    "arxiv-id": "0908.1379v1", 
    "author": "Jonah Sherman", 
    "publish": "2009-08-10T19:57:32Z", 
    "summary": "This paper ties the line of work on algorithms that find an\nO(sqrt(log(n)))-approximation to the sparsest cut together with the line of\nwork on algorithms that run in sub-quadratic time by using only\nsingle-commodity flows. We present an algorithm that simultaneously achieves\nboth goals, finding an O(sqrt(log(n)/eps))-approximation using O(n^eps log^O(1)\nn) max-flows. The core of the algorithm is a stronger, algorithmic version of\nArora et al.'s structure theorem, where we show that matching-chaining argument\nat the heart of their proof can be viewed as an algorithm that finds good\naugmenting paths in certain geometric multicommodity flow networks. By using\nthat specialized algorithm in place of a black-box solver, we are able to solve\nthose instances much more efficiently. We also show the cut-matching game\nframework can not achieve an approximation any better than Omega(log(n)/log\nlog(n)) without re-routing flow."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.1528v2", 
    "title": "Contraction of Timetable Networks with Realistic Transfers", 
    "arxiv-id": "0908.1528v2", 
    "author": "Robert Geisberger", 
    "publish": "2009-08-11T16:03:46Z", 
    "summary": "We successfully contract timetable networks with realistic transfer times.\nContraction gradually removes nodes from the graph and adds shortcuts to\npreserve shortest paths. This reduces query times to 1 ms with preprocessing\ntimes around 6 minutes on all tested instances. We achieve this by an improved\ncontraction algorithm and by using a station graph model. Every node in our\ngraph has a one-to-one correspondence to a station and every edge has an\nassigned collection of connections. Our graph model does not need parallel\nedges. The query algorithm does not compute a single earliest arrival time at a\nstation but a set of arriving connections that allow best transfer\nopportunities."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.2256v3", 
    "title": "On k-Column Sparse Packing Programs", 
    "arxiv-id": "0908.2256v3", 
    "author": "Aravind Srinivasan", 
    "publish": "2009-08-16T17:55:03Z", 
    "summary": "We consider the class of packing integer programs (PIPs) that are column\nsparse, i.e. there is a specified upper bound k on the number of constraints\nthat each variable appears in. We give an (ek+o(k))-approximation algorithm for\nk-column sparse PIPs, improving on recent results of $k^2\\cdot 2^k$ and\n$O(k^2)$. We also show that the integrality gap of our linear programming\nrelaxation is at least 2k-1; it is known that k-column sparse PIPs are\n$\\Omega(k/ \\log k)$-hard to approximate. We also extend our result (at the loss\nof a small constant factor) to the more general case of maximizing a submodular\nobjective over k-column sparse packing constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.2834v1", 
    "title": "On Revenue Maximization in Second-Price Ad Auctions", 
    "arxiv-id": "0908.2834v1", 
    "author": "C. Thach Nguyen", 
    "publish": "2009-08-19T23:46:02Z", 
    "summary": "Most recent papers addressing the algorithmic problem of allocating\nadvertisement space for keywords in sponsored search auctions assume that\npricing is done via a first-price auction, which does not realistically model\nthe Generalized Second Price (GSP) auction used in practice. Towards the goal\nof more realistically modeling these auctions, we introduce the Second-Price Ad\nAuctions problem, in which bidders' payments are determined by the GSP\nmechanism. We show that the complexity of the Second-Price Ad Auctions problem\nis quite different than that of the more studied First-Price Ad Auctions\nproblem. First, unlike the first-price variant, for which small constant-factor\napproximations are known, it is NP-hard to approximate the Second-Price Ad\nAuctions problem to any non-trivial factor. Second, this discrepancy extends\neven to the 0-1 special case that we call the Second-Price Matching problem\n(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no\ndeterministic algorithm achieving a non-trivial competitive ratio and no\nrandomized algorithm achieving a competitive ratio better than 2. This stands\nin contrast to the results for the analogous special case in the first-price\nmodel, the standard bipartite matching problem, which is solvable in polynomial\ntime and which has deterministic and randomized online algorithms achieving\nbetter competitive ratios. On the positive side, we provide a 2-approximation\nfor offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.\nThe latter result makes use of a new generalization of a classic result on the\nperformance of the \"Ranking\" algorithm for online bipartite matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.3089v1", 
    "title": "Data structure for representing a graph: combination of linked list and   hash table", 
    "arxiv-id": "0908.3089v1", 
    "author": "Maxim A. Kolosovskiy", 
    "publish": "2009-08-21T10:12:31Z", 
    "summary": "In this article we discuss a data structure, which combines advantages of two\ndifferent ways for representing graphs: adjacency matrix and collection of\nadjacency lists. This data structure can fast add and search edges (advantages\nof adjacency matrix), use linear amount of memory, let to obtain adjacency list\nfor certain vertex (advantages of collection of adjacency lists). Basic\nknowledge of linked lists and hash tables is required to understand this\narticle. The article contains examples of implementation on Java."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.3505v2", 
    "title": "Polynomial Time Algorithms for Minimum Energy Scheduling", 
    "arxiv-id": "0908.3505v2", 
    "author": "Christoph Durr", 
    "publish": "2009-08-24T21:44:45Z", 
    "summary": "The aim of power management policies is to reduce the amount of energy\nconsumed by computer systems while maintaining satisfactory level of\nperformance. One common method for saving energy is to simply suspend the\nsystem during the idle times. No energy is consumed in the suspend mode.\nHowever, the process of waking up the system itself requires a certain fixed\namount of energy, and thus suspending the system is beneficial only if the idle\ntime is long enough to compensate for this additional energy expenditure. In\nthe specific problem studied in the paper, we have a set of jobs with release\ntimes and deadlines that need to be executed on a single processor. Preemptions\nare allowed. The processor requires energy L to be woken up and, when it is on,\nit uses one unit of energy per one unit of time. It has been an open problem\nwhether a schedule minimizing the overall energy consumption can be computed in\npolynomial time. We solve this problem in positive, by providing an O(n^5)-time\nalgorithm. In addition we provide an O(n^4)-time algorithm for computing the\nminimum energy schedule when all jobs have unit length."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.0095v2", 
    "title": "Online Algorithms for Self-Organizing Sequential Search - A Survey", 
    "arxiv-id": "0909.0095v2", 
    "author": "N. S. Narayanaswamy", 
    "publish": "2009-09-01T05:46:32Z", 
    "summary": "The main objective of this survey is to present the important theoretical and\nexperimental results contributed till date in the area of online algorithms for\nthe self organizing sequential search problem, also popularly known as the List\nUpdate Problem(LUP) in a chronological way. The survey includes competitiveness\nresults of deterministic and randomized online algorithms and complexity\nresults of optimal off line algorithms for the list update problem. We also\npresent the results associated with list update with look ahead, list update\nwith locality of reference and other variants of the list update problem. We\ninvestigate research issues, explore scope of future work associated with each\nissue so that future researchers can find it useful to work on."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.0941v1", 
    "title": "A Randomized Rounding Algorithm for the Asymmetric Traveling Salesman   Problem", 
    "arxiv-id": "0909.0941v1", 
    "author": "Mohit Singh", 
    "publish": "2009-09-04T19:49:22Z", 
    "summary": "We present an algorithm for the asymmetric traveling salesman problem on\ninstances which satisfy the triangle inequality. Like several existing\nalgorithms, it achieves approximation ratio O(log n). Unlike previous\nalgorithms, it uses randomized rounding."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.1030v1", 
    "title": "The Euler Path to Static Level-Ancestors", 
    "arxiv-id": "0909.1030v1", 
    "author": "Amir M. Ben-Amram", 
    "publish": "2009-09-05T14:15:29Z", 
    "summary": "Suppose that a rooted tree T is given for preprocessing. The Level-Ancestor\nProblem is to answer quickly queries of the following form. Given a vertex v\nand an integer i > 0, find the i-th vertex on the path from the root to v.\nAlgorithms that achieve a linear time bound for preprocessing and a constant\ntime bound for a query have been published by Dietz (1991), Alstrup and Holm\n(2000), and Bender and Farach (2002). The first two algorithms address dynamic\nversions of the problem; the last addresses the static version only and is the\nsimplest so far. The purpose of this note is to expose another simple\nalgorithm, derived from a complicated PRAM algorithm by Berkman and Vishkin\n(1990,1994). We further show some easy extensions of its functionality, adding\nqueries for descendants and level successors as well as ancestors, extensions\nfor which the formerly known algorithms are less suitable."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.1037v3", 
    "title": "Randomized Shellsort: A Simple Oblivious Sorting Algorithm", 
    "arxiv-id": "0909.1037v3", 
    "author": "Michael T. Goodrich", 
    "publish": "2009-09-05T15:40:42Z", 
    "summary": "In this paper, we describe randomized Shellsort--a simple, randomized,\ndata-oblivious version of the Shellsort algorithm that always runs in O(n log\nn) time and, as we show, succeeds in sorting any given input permutation with\nvery high probability. Thus, randomized Shellsort is simultaneously simple,\ntime-optimal, and data-oblivious. Taken together, these properties imply\napplications in the design of new efficient privacy-preserving computations\nbased on the secure multi-party computation (SMC) paradigm. In addition, by a\ntrivial conversion of this Monte Carlo algorithm to its Las Vegas equivalent,\none gets the first version of Shellsort with a running time that is provably\nO(n log n) with very high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.1870v1", 
    "title": "Paired approximation problems and incompatible inapproximabilities", 
    "arxiv-id": "0909.1870v1", 
    "author": "David Eppstein", 
    "publish": "2009-09-10T05:23:43Z", 
    "summary": "This paper considers pairs of optimization problems that are defined from a\nsingle input and for which it is desired to find a good approximation to either\none of the problems. In many instances, it is possible to efficiently find an\napproximation of this type that is better than known inapproximability lower\nbounds for either of the two individual optimization problems forming the pair.\nIn particular, we find either a $(1+\\epsilon)$-approximation to $(1,2)$-TSP or\na $1/\\epsilon$-approximation to maximum independent set, from a given graph, in\nlinear time. We show a similar paired approximation result for finding either a\ncoloring or a long path. However, no such tradeoff exists in some other cases:\nfor set cover and hitting set problems defined from a single set family, and\nfor clique and independent set problems on the same graph, it is not possible\nto find an approximation when both problems are combined that is better than\nthe best approximation for either problem on its own."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.2547v1", 
    "title": "Simple implementation of deletion from open-address hash table", 
    "arxiv-id": "0909.2547v1", 
    "author": "Maxim Kolosovskiy", 
    "publish": "2009-09-14T13:36:58Z", 
    "summary": "Deletion from open-address hash table is not so easy as deletion from chained\nhash table, because in open-address table we can't simply mark a slot\ncontaining deleted key as empty. Search for keys may become incorrect. The\nclassical method to implement deletion is to mark slots in hash table by three\nvalues: \"free\", \"busy\", \"deleted\". That method is easy to implement, but there\nare some disadvantages. In this article we consider alternative method of\ndeletion keys, where we avoid using the mark \"deleted\". The article contains\nthe implementation of the method in Java."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.2704v1", 
    "title": "Memory Consistency Conditions for Self-Assembly Programming", 
    "arxiv-id": "0909.2704v1", 
    "author": "Aaron Sterling", 
    "publish": "2009-09-15T19:45:22Z", 
    "summary": "Perhaps the two most significant theoretical questions about the programming\nof self-assembling agents are: (1) necessary and sufficient conditions to\nproduce a unique terminal assembly, and (2) error correction. We address both\nquestions, by reducing two well-studied models of tile assembly to models of\ndistributed shared memory (DSM), in order to obtain results from the memory\nconsistency systems induced by tile assembly systems when simulated in the DSM\nsetting. The Abstract Tile Assembly Model (aTAM) can be simulated by a DSM\nsystem that obeys causal consistency, and the locally deterministic tile\nassembly systems in the aTAM correspond exactly to the concurrent-write free\nprograms that simulate tile assembly in such a model. Thus, the detection of\nthe failure of local determinism (which had formerly been an open problem)\nreduces to the detection of data races in simulating programs. Further, the\nKinetic Tile Assembly Model can be simulated by a DSM system that obeys GWO, a\nmemory consistency condition defined by Steinke and Nutt. (To our knowledge,\nthis is the first natural example of a DSM system that obeys GWO, but no\nstronger consistency condition.) We combine these results with the observation\nthat self-assembly algorithms are local algorithms, and there exists a fast\nconversion of deterministic local algorithms into deterministic\nself-stabilizing algorithms. This provides an \"immediate\" generalization of a\ntheorem by Soloveichik et al. about the existence of tile assembly systems that\nsimultaneously perform two forms of self-stabilization: proofreading and\nself-healing. Our reductions and proof techniques can be extended to the\nprogramming of self-assembling agents in a variety of media, not just DNA\ntiles, and not just two-dimensional surfaces."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.3180v1", 
    "title": "FPT Algorithms for Connected Feedback Vertex Set", 
    "arxiv-id": "0909.3180v1", 
    "author": "Somnath Sikdar", 
    "publish": "2009-09-17T10:37:31Z", 
    "summary": "We study the recently introduced Connected Feedback Vertex Set (CFVS) problem\nfrom the view-point of parameterized algorithms. CFVS is the connected variant\nof the classical Feedback Vertex Set problem and is defined as follows: given a\ngraph G=(V,E) and an integer k, decide whether there exists a subset F of V, of\nsize at most k, such that G[V F] is a forest and G[F] is connected. We show\nthat Connected Feedback Vertex Set can be solved in time $O(2^{O(k)}n^{O(1)})$\non general graphs and in time $O(2^{O(\\sqrt{k}\\log k)}n^{O(1)})$ on graphs\nexcluding a fixed graph H as a minor. Our result on general undirected graphs\nuses as subroutine, a parameterized algorithm for Group Steiner Tree, a well\nstudied variant of Steiner Tree. We find the algorithm for Group Steiner Tree\nof independent interest and believe that it will be useful for obtaining\nparameterized algorithms for other connectivity problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.3637v2", 
    "title": "Packet Scheduling in a Size-Bounded Buffer", 
    "arxiv-id": "0909.3637v2", 
    "author": "Fei Li", 
    "publish": "2009-09-20T15:20:17Z", 
    "summary": "We consider algorithms to schedule packets with values and deadlines in a\nsize-bounded buffer. At any time, the buffer can store at most B packets.\nPackets arrive over time. Each packet has a non-negative value and an integer\ndeadline. In each time step, at most one packet can be sent. Packets can be\ndropped at any time before they are sent. The objective is to maximize the\ntotal value gained by delivering packets no later than their respective\ndeadlines. This model generalizes the well-studied bounded-delay model (Hajek.\nCISS 2001. Kesselman et al. STOC 2001). We first provide an optimal offline\nalgorithm for this model. Then we present an alternative proof of the\n2-competitive deterministic online algorithm (Fung. arXiv July 2009). We also\nprove that the lower bound of competitive ratio of a family of (deterministic\nand randomized) algorithms is 2 - 1 / B."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.3696v2", 
    "title": "Efficient and Error-Correcting Data Structures for Membership and   Polynomial Evaluation", 
    "arxiv-id": "0909.3696v2", 
    "author": "Ronald de Wolf", 
    "publish": "2009-09-21T07:16:22Z", 
    "summary": "We construct efficient data structures that are resilient against a constant\nfraction of adversarial noise. Our model requires that the decoder answers most\nqueries correctly with high probability and for the remaining queries, the\ndecoder with high probability either answers correctly or declares \"don't\nknow.\" Furthermore, if there is no noise on the data structure, it answers all\nqueries correctly with high probability. Our model is the common generalization\nof a model proposed recently by de Wolf and the notion of \"relaxed locally\ndecodable codes\" developed in the PCP literature.\n  We measure the efficiency of a data structure in terms of its length,\nmeasured by the number of bits in its representation, and query-answering time,\nmeasured by the number of bit-probes to the (possibly corrupted)\nrepresentation. In this work, we study two data structure problems: membership\nand polynomial evaluation. We show that these two problems have constructions\nthat are simultaneously efficient and error-correcting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.4224v1", 
    "title": "Breaking the 2^n-Barrier for Irredundance: A Parameterized Route to   Solving Exact Puzzles", 
    "arxiv-id": "0909.4224v1", 
    "author": "Dieter Kratsch Alexander Langer Mathieu Liedloff Daniel Raible Peter Rossmanith", 
    "publish": "2009-09-23T15:01:24Z", 
    "summary": "The lower and the upper irredundance numbers of a graph $G$, denoted $ir(G)$\nand $IR(G)$ respectively, are conceptually linked to domination and\nindependence numbers and have numerous relations to other graph parameters. It\nis a long-standing open question whether determining these numbers for a graph\n$G$ on $n$ vertices admits exact algorithms running in time less than the\ntrivial $\\Omega(2^n)$ enumeration barrier. We solve these open problems by\ndevising parameterized algorithms for the dual of the natural parameterizations\nof the problems with running times faster than $O^*(4^{k})$. For example, we\npresent an algorithm running in time $O^*(3.069^{k})$ for determining whether\n$IR(G)$ is at least $n-k$. Although the corresponding problem has been known to\nbe in FPT by kernelization techniques, this paper offers the first\nparameterized algorithms with an exponential dependency on the parameter in the\nrunning time. Additionally, our work also appears to be the first example of a\nparameterized approach leading to a solution to a problem in exponential time\nalgorithmics where the natural interpretation as an exact exponential-time\nalgorithm fails."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.4341v1", 
    "title": "Lightweight Data Indexing and Compression in External Memory", 
    "arxiv-id": "0909.4341v1", 
    "author": "Giovanni Manzini", 
    "publish": "2009-09-24T18:23:20Z", 
    "summary": "In this paper we describe algorithms for computing the BWT and for building\n(compressed) indexes in external memory. The innovative feature of our\nalgorithms is that they are lightweight in the sense that, for an input of size\n$n$, they use only ${n}$ bits of disk working space while all previous\napproaches use $\\Th{n \\log n}$ bits of disk working space. Moreover, our\nalgorithms access disk data only via sequential scans, thus they take full\nadvantage of modern disk features that make sequential disk accesses much\nfaster than random accesses.\n  We also present a scan-based algorithm for inverting the BWT that uses\n$\\Th{n}$ bits of working space, and a lightweight {\\em internal-memory}\nalgorithm for computing the BWT which is the fastest in the literature when the\navailable working space is $\\os{n}$ bits.\n  Finally, we prove {\\em lower} bounds on the complexity of computing and\ninverting the BWT via sequential scans in terms of the classic product:\ninternal-memory space $\\times$ number of passes over the disk data."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.4369v1", 
    "title": "Exploration of Periodically Varying Graphs", 
    "arxiv-id": "0909.4369v1", 
    "author": "Nicola Santoro", 
    "publish": "2009-09-24T06:01:25Z", 
    "summary": "We study the computability and complexity of the exploration problem in a\nclass of highly dynamic graphs: periodically varying (PV) graphs, where the\nedges exist only at some (unknown) times defined by the periodic movements of\ncarriers. These graphs naturally model highly dynamic infrastructure-less\nnetworks such as public transports with fixed timetables, low earth orbiting\n(LEO) satellite systems, security guards' tours, etc. We establish necessary\nconditions for the problem to be solved. We also derive lower bounds on the\namount of time required in general, as well as for the PV graphs defined by\nrestricted classes of carriers movements: simple routes, and circular routes.\nWe then prove that the limitations on computability and complexity we have\nestablished are indeed tight. In fact we prove that all necessary conditions\nare also sufficient and all lower bounds on costs are tight. We do so\nconstructively presenting two worst case optimal solution algorithms, one for\nanonymous systems, and one for those with distinct nodes ids. An added benefit\nis that the algorithms are rather simple."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.4893v2", 
    "title": "Range Non-Overlapping Indexing", 
    "arxiv-id": "0909.4893v2", 
    "author": "Ely Porat", 
    "publish": "2009-09-26T20:24:19Z", 
    "summary": "We study the non-overlapping indexing problem: Given a text T, preprocess it\nso that you can answer queries of the form: given a pattern P, report the\nmaximal set of non-overlapping occurrences of P in T. A generalization of this\nproblem is the range non-overlapping indexing where in addition we are given\ntwo indexes i,j to report the maximal set of non-overlapping occurrences\nbetween these two indexes. We suggest new solutions for these problems. For the\nnon-overlapping problem our solution uses O(n) space with query time of O(m +\nocc_{NO}). For the range non-overlapping problem we propose a solution with\nO(n\\log^\\epsilon n) space for some 0<\\epsilon<1 and O(m + \\log\\log n +\nocc_{ij,NO}) query time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.4969v1", 
    "title": "MACH: Fast Randomized Tensor Decompositions", 
    "arxiv-id": "0909.4969v1", 
    "author": "Charalampos E. Tsourakakis", 
    "publish": "2009-09-27T22:36:17Z", 
    "summary": "Tensors naturally model many real world processes which generate multi-aspect\ndata. Such processes appear in many different research disciplines, e.g,\nchemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor\ndecompositions such as the Tucker decomposition are used to analyze\nmulti-aspect data and extract latent factors, which capture the multilinear\ndata structure. Such decompositions are powerful mining tools, for extracting\npatterns from large data volumes. However, most frequently used algorithms for\nsuch decompositions involve the computationally expensive Singular Value\nDecomposition.\n  In this paper we propose MACH, a new sampling algorithm to compute such\ndecompositions. Our method is of significant practical value for tensor\nstreams, such as environmental monitoring systems, IP traffic matrices over\ntime, where large amounts of data are accumulated and the analysis is\ncomputationally intensive but also in \"post-mortem\" data analysis cases where\nthe tensor does not fit in the available memory. We provide the theoretical\nanalysis of our proposed method, and verify its efficacy in monitoring system\napplications."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.5146v3", 
    "title": "Fast Set Intersection and Two Patterns Matching", 
    "arxiv-id": "0909.5146v3", 
    "author": "Ely Porat", 
    "publish": "2009-09-28T17:13:56Z", 
    "summary": "In this paper we present a new problem, the fast set intersection problem,\nwhich is to preprocess a collection of sets in order to efficiently report the\nintersection of any two sets in the collection. In addition we suggest new\nsolutions for the two-dimensional substring indexing problem and the document\nlisting problem for two patterns by reduction to the fast set intersection\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0909.5278v2", 
    "title": "Finding Induced Subgraphs via Minimal Triangulations", 
    "arxiv-id": "0909.5278v2", 
    "author": "Yngve Villanger", 
    "publish": "2009-09-29T07:13:39Z", 
    "summary": "Potential maximal cliques and minimal separators are combinatorial objects\nwhich were introduced and studied in the realm of minimal triangulations\nproblems including Minimum Fill-in and Treewidth. We discover unexpected\napplications of these notions to the field of moderate exponential algorithms.\nIn particular, we show that given an n-vertex graph G together with its set of\npotential maximal cliques Pi_G, and an integer t, it is possible in time |Pi_G|\n* n^(O(t)) to find a maximum induced subgraph of treewidth t in G; and for a\ngiven graph F of treewidth t, to decide if G contains an induced subgraph\nisomorphic to F. Combined with an improved algorithm enumerating all potential\nmaximal cliques in time O(1.734601^n), this yields that both problems are\nsolvable in time 1.734601^n * n^(O(t))."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0460v3", 
    "title": "Exact Covers via Determinants", 
    "arxiv-id": "0910.0460v3", 
    "author": "Andreas Bj\u00f6rklund", 
    "publish": "2009-10-02T19:36:39Z", 
    "summary": "Given a k-uniform hypergraph on n vertices, partitioned in k equal parts such\nthat every hyperedge includes one vertex from each part, the k-dimensional\nmatching problem asks whether there is a disjoint collection of the hyperedges\nwhich covers all vertices. We show it can be solved by a randomized polynomial\nspace algorithm in time O*(2^(n(k-2)/k)). The O*() notation hides factors\npolynomial in n and k.\n  When we drop the partition constraint and permit arbitrary hyperedges of\ncardinality k, we obtain the exact cover by k-sets problem. We show it can be\nsolved by a randomized polynomial space algorithm in time O*(c_k^n), where\nc_3=1.496, c_4=1.642, c_5=1.721, and provide a general bound for larger k.\n  Both results substantially improve on the previous best algorithms for these\nproblems, especially for small k, and follow from the new observation that\nLovasz' perfect matching detection via determinants (1979) admits an embedding\nin the recently proposed inclusion-exclusion counting scheme for set covers,\ndespite its inability to count the perfect matchings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0504v2", 
    "title": "Improved Analysis of a Max Cut Algorithm Based on Spectral Partitioning", 
    "arxiv-id": "0910.0504v2", 
    "author": "Jos\u00e9 Soto", 
    "publish": "2009-10-05T19:58:59Z", 
    "summary": "Trevisan [SICOMP 2012] presented an algorithm for Max-Cut based on spectral\npartitioning techniques. This is the first algorithm for Max-Cut with an\napproximation guarantee strictly larger than 1/2 that is not based on\nsemidefinite programming. Trevisan showed that its approximation ratio is of at\nleast 0.531. In this paper we improve this bound up to 0.614247. We also define\nand extend this result for the more general Maximum Colored Cut problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0553v1", 
    "title": "Combining Approximation Algorithms for the Prize-Collecting TSP", 
    "arxiv-id": "0910.0553v1", 
    "author": "Michel X. Goemans", 
    "publish": "2009-10-03T15:48:44Z", 
    "summary": "We present a 1.91457-approximation algorithm for the prize-collecting\ntravelling salesman problem. This is obtained by combining a randomized variant\nof a rounding algorithm of Bienstock et al. and a primal-dual algorithm of\nGoemans and Williamson."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.0832v1", 
    "title": "Optimal deterministic ring exploration with oblivious asynchronous   robots", 
    "arxiv-id": "0910.0832v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2009-10-05T19:08:49Z", 
    "summary": "We consider the problem of exploring an anonymous unoriented ring of size $n$\nby $k$ identical, oblivious, asynchronous mobile robots, that are unable to\ncommunicate, yet have the ability to sense their environment and take decisions\nbased on their local view. Previous works in this weak scenario prove that $k$\nmust not divide $n$ for a deterministic solution to exist. Also, it is known\nthat the minimum number of robots (either deterministic or probabilistic) to\nexplore a ring of size $n$ is 4. An upper bound of 17 robots holds in the\ndeterministic case while 4 probabilistic robots are sufficient. In this paper,\nwe close the complexity gap in the deterministic setting, by proving that no\ndeterministic exploration is feasible with less than five robots whenever the\nsize of the ring is even, and that five robots are sufficient for any $n$ that\nis coprime with five. Our protocol completes exploration in O(n) robot moves,\nwhich is also optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.1969v1", 
    "title": "A Generalized Recursive Algorithm for Binary Multiplication based on   Vedic Mathematics", 
    "arxiv-id": "0910.1969v1", 
    "author": "Ashish Joglekar", 
    "publish": "2009-10-11T04:57:22Z", 
    "summary": "A generalized algorithm for multiplication is proposed through recursive\napplication of the Nikhilam Sutra from Vedic Mathematics, operating in radix -\n2 number system environment suitable for digital platforms. Statistical\nanalysis has been carried out based on the number of recursions profile as a\nfunction of the smaller multiplicand. The proposed algorithm is efficient for\nsmaller multiplicands as well, unlike most of the asymptotically fast\nalgorithms. Further, a basic block schematic of Hardware Implementation of our\nalgorithm is suggested to exploit parallelism and speed up the implementation\nof the algorithm in a multiprocessor environment."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.3123v2", 
    "title": "Wee LCP", 
    "arxiv-id": "0910.3123v2", 
    "author": "Johannes Fischer", 
    "publish": "2009-10-16T13:50:12Z", 
    "summary": "We prove that longest common prefix (LCP) information can be stored in much\nless space than previously known. More precisely, we show that in the presence\nof the text and the suffix array, o(n) additional bits are sufficient to answer\nLCP-queries asymptotically in the same time that is needed to retrieve an entry\nfrom the suffix array. This yields the smallest compressed suffix tree with\nsub-logarithmic navigation time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.3243v1", 
    "title": "Testing Distribution Identity Efficiently", 
    "arxiv-id": "0910.3243v1", 
    "author": "Krzysztof Onak", 
    "publish": "2009-10-16T22:26:02Z", 
    "summary": "We consider the problem of testing distribution identity. Given a sequence of\nindependent samples from an unknown distribution on a domain of size n, the\ngoal is to check if the unknown distribution approximately equals a known\ndistribution on the same domain. While Batu, Fortnow, Fischer, Kumar,\nRubinfeld, and White (FOCS 2001) proved that the sample complexity of the\nproblem is O~(sqrt(n) * poly(1/epsilon)), the running time of their tester is\nmuch higher: O(n) + O~(sqrt(n) * poly(1/epsilon)). We modify their tester to\nachieve a running time of O~(sqrt(n) * poly(1/epsilon))."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9424-y", 
    "link": "http://arxiv.org/pdf/0910.3503v2", 
    "title": "Searching a bitstream in linear time for the longest substring of any   given density", 
    "arxiv-id": "0910.3503v2", 
    "author": "Benjamin A. Burton", 
    "publish": "2009-10-19T10:06:34Z", 
    "summary": "Given an arbitrary bitstream, we consider the problem of finding the longest\nsubstring whose ratio of ones to zeroes equals a given value. The central\nresult of this paper is an algorithm that solves this problem in linear time.\nThe method involves (i) reformulating the problem as a constrained walk through\na sparse matrix, and then (ii) developing a data structure for this sparse\nmatrix that allows us to perform each step of the walk in amortised constant\ntime. We also give a linear time algorithm to find the longest substring whose\nratio of ones to zeroes is bounded below by a given value. Both problems have\npractical relevance to cryptography and bioinformatics."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0910.5599v1", 
    "title": "Vector Bin Packing with Multiple-Choice", 
    "arxiv-id": "0910.5599v1", 
    "author": "Dror Rawitz", 
    "publish": "2009-10-29T10:00:37Z", 
    "summary": "We consider a variant of bin packing called multiple-choice vector bin\npacking. In this problem we are given a set of items, where each item can be\nselected in one of several $D$-dimensional incarnations. We are also given $T$\nbin types, each with its own cost and $D$-dimensional size. Our goal is to pack\nthe items in a set of bins of minimum overall cost. The problem is motivated by\nscheduling in networks with guaranteed quality of service (QoS), but due to its\ngeneral formulation it has many other applications as well. We present an\napproximation algorithm that is guaranteed to produce a solution whose cost is\nabout $\\ln D$ times the optimum. For the running time to be polynomial we\nrequire $D=O(1)$ and $T=O(\\log n)$. This extends previous results for vector\nbin packing, in which each item has a single incarnation and there is only one\nbin type. To obtain our result we also present a PTAS for the multiple-choice\nversion of multidimensional knapsack, where we are given only one bin and the\ngoal is to pack a maximum weight set of (incarnations of) items in that bin."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0910.5744v1", 
    "title": "Exact algorithms for OWA-optimization in multiobjective spanning tree   problems", 
    "arxiv-id": "0910.5744v1", 
    "author": "Olivier Spanjaard", 
    "publish": "2009-10-29T23:17:24Z", 
    "summary": "This paper deals with the multiobjective version of the optimal spanning tree\nproblem. More precisely, we are interested in determining the optimal spanning\ntree according to an Ordered Weighted Average (OWA) of its objective values. We\nfirst show that the problem is weakly NP-hard. In the case where the weights of\nthe OWA are strictly decreasing, we then propose a mixed integer programming\nformulation, and provide dedicated optimality conditions yielding an important\nreduction of the size of the program. Next, we present two bounds that can be\nused to prune subspaces of solutions either in a shaving phase or in a branch\nand bound procedure. The validity of these bounds does not depend on specific\nproperties of the weights (apart from non-negativity). All these exact\nresolution algorithms are compared on the basis of numerical experiments,\naccording to their respective validity scopes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0174v1", 
    "title": "A O(E) Time Shortest Path Algorithm For Non Negative Weighted Undirected   Graphs", 
    "arxiv-id": "0911.0174v1", 
    "author": "Rehan Akbar", 
    "publish": "2009-11-02T20:24:05Z", 
    "summary": "In most of the shortest path problems like vehicle routing problems and\nnetwork routing problems, we only need an efficient path between two points\nsource and destination, and it is not necessary to calculate the shortest path\nfrom source to all other nodes. This paper concentrates on this very idea and\npresents an algorithm for calculating shortest path for (i) nonnegative\nweighted undirected graphs (ii) unweighted undirected graphs. The algorithm\ncompletes its execution in O(E) for all graphs except few in which longer path\n(in terms of number of edges) from source to some node makes it best selection\nfor that node. The main advantage of the algorithms is its simplicity and it\ndoes not need complex data structures for implementations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0397v1", 
    "title": "Algorithm as Defining Dynamic Systems", 
    "arxiv-id": "0911.0397v1", 
    "author": "Hong Pyo Ha", 
    "publish": "2009-11-02T20:01:19Z", 
    "summary": "This paper proposes a new view to algorithms, Algorithms as defining dynamic\nsystems. This view extends the traditional, deterministic view that an\nalgorithm is a step by step procedure with nondeterminism. As a dynamic system\ncan be designed by a set of its defining laws, it is also desirable to design\nan algorithm by a (possibly nondeterministic) set of defining laws. This\nobservation requires some changes to algorithm development. We propose a two\nstep approach, the first step is to design an algorithm via a set of defining\nlaws of dynamic system. The second step is to translate these laws (written in\na natural language) into a formal language such as linear logic."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0577v2", 
    "title": "Fast Arc-Annotated Subsequence Matching in Linear Space", 
    "arxiv-id": "0911.0577v2", 
    "author": "Inge Li Goertz", 
    "publish": "2009-11-03T13:35:41Z", 
    "summary": "An arc-annotated string is a string of characters, called bases, augmented\nwith a set of pairs, called arcs, each connecting two bases. Given\narc-annotated strings $P$ and $Q$ the arc-preserving subsequence problem is to\ndetermine if $P$ can be obtained from $Q$ by deleting bases from $Q$. Whenever\na base is deleted any arc with an endpoint in that base is also deleted.\nArc-annotated strings where the arcs are ``nested'' are a natural model of RNA\nmolecules that captures both the primary and secondary structure of these. The\narc-preserving subsequence problem for nested arc-annotated strings is basic\nprimitive for investigating the function of RNA molecules. Gramm et al. [ACM\nTrans. Algorithms 2006] gave an algorithm for this problem using $O(nm)$ time\nand space, where $m$ and $n$ are the lengths of $P$ and $Q$, respectively. In\nthis paper we present a new algorithm using $O(nm)$ time and $O(n + m)$ space,\nthereby matching the previous time bound while significantly reducing the space\nfrom a quadratic term to linear. This is essential to process large RNA\nmolecules where the space is likely to be a bottleneck. To obtain our result we\nintroduce several novel ideas which may be of independent interest for related\nproblems on arc-annotated strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.1626v2", 
    "title": "Fast Approximation in Subspaces by Doubling Metric Decomposition", 
    "arxiv-id": "0911.1626v2", 
    "author": "Piotr Sankowski", 
    "publish": "2009-11-09T10:29:49Z", 
    "summary": "In this paper we propose and study a new complexity model for approximation\nalgorithms. The main motivation are practical problems over large data sets\nthat need to be solved many times for different scenarios, e.g., many multicast\ntrees that need to be constructed for different groups of users. In our model\nwe allow a preprocessing phase, when some information of the input graph\n$G=(V,E)$ is stored in a limited size data structure. Next, the data structure\nenables processing queries of the form ``solve problem A for an input\n$S\\subseteq V$''. We consider problems like {\\sc Steiner Forest}, {\\sc Facility\nLocation}, {\\sc $k$-Median}, {\\sc $k$-Center} and {\\sc TSP} in the case when\nthe graph induces a doubling metric. Our main results are data structures of\nnear-linear size that are able to answer queries in time close to linear in\n$|S|$. This improves over typical worst case reuniting time of approximation\nalgorithms in the classical setting which is $\\Omega(|E|)$ independently of the\nquery size. In most cases, our approximation guarantees are arbitrarily close\nto those in the classical setting. Additionally, we present the first fully\ndynamic algorithm for the Steiner tree problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.1765v1", 
    "title": "GEDI: Scalable Algorithms for Genotype Error Detection and Imputation", 
    "arxiv-id": "0911.1765v1", 
    "author": "Bogdan Pasaniuc", 
    "publish": "2009-11-09T23:35:41Z", 
    "summary": "Genome-wide association studies generate very large datasets that require\nscalable analysis algorithms. In this report we describe the GEDI software\npackage, which implements efficient algorithms for performing several common\ntasks in the analysis of population genotype data, including genotype error\ndetection and correction, imputation of both randomly missing and untyped\ngenotypes, and genotype phasing. Experimental results show that GEDI achieves\nhigh accuracy with a runtime scaling linearly with the number of markers and\nsamples. The open source C++ code of GEDI, released under the GNU General\nPublic License, is available for download at\nhttp://dna.engr.uconn.edu/software/GEDI/"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.2924v2", 
    "title": "Synthesizing Minimal Tile Sets for Patterned DNA Self-Assembly", 
    "arxiv-id": "0911.2924v2", 
    "author": "Pekka Orponen", 
    "publish": "2009-11-16T07:43:41Z", 
    "summary": "The Pattern self-Assembly Tile set Synthesis (PATS) problem is to determine a\nset of coloured tiles that self-assemble to implement a given rectangular\ncolour pattern. We give an exhaustive branch-and-bound algorithm to find tile\nsets of minimum cardinality for the PATS problem. Our algorithm makes use of a\nsearch tree in the lattice of partitions of the ambient rectangular grid, and\nan efficient bounding function to prune this search tree. Empirical data on the\nperformance of the algorithm shows that it compares favourably to previously\npresented heuristic solutions to the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_6", 
    "link": "http://arxiv.org/pdf/0911.3355v1", 
    "title": "A Minimal Periods Algorithm with Applications", 
    "arxiv-id": "0911.3355v1", 
    "author": "Zhi Xu", 
    "publish": "2009-11-17T17:34:23Z", 
    "summary": "Kosaraju in ``Computation of squares in a string'' briefly described a\nlinear-time algorithm for computing the minimal squares starting at each\nposition in a word. Using the same construction of suffix trees, we generalize\nhis result and describe in detail how to compute in O(k|w|)-time the minimal\nk-th power, with period of length larger than s, starting at each position in a\nword w for arbitrary exponent $k\\geq2$ and integer $s\\geq0$. We provide the\ncomplete proof of correctness of the algorithm, which is somehow not completely\nclear in Kosaraju's original paper. The algorithm can be used as a sub-routine\nto detect certain types of pseudo-patterns in words, which is our original\nintention to study the generalization."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_6", 
    "link": "http://arxiv.org/pdf/0911.4384v3", 
    "title": "Ptolemaic Indexing", 
    "arxiv-id": "0911.4384v3", 
    "author": "Magnus Lie Hetland", 
    "publish": "2009-11-23T12:16:15Z", 
    "summary": "This paper discusses a new family of bounds for use in similarity search,\nrelated to those used in metric indexing, but based on Ptolemy's inequality,\nrather than the metric axioms. Ptolemy's inequality holds for the well-known\nEuclidean distance, but is also shown here to hold for quadratic form metrics\nin general, with Mahalanobis distance as an important special case. The\ninequality is examined empirically on both synthetic and real-world data sets\nand is also found to hold approximately, with a very low degree of error, for\nimportant distances such as the angular pseudometric and several Lp norms.\nIndexing experiments demonstrate a highly increased filtering power compared to\nexisting, triangular methods. It is also shown that combining the Ptolemaic and\ntriangular filtering can lead to better results than using either approach on\nits own."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.4544v1", 
    "title": "Improved Algorithm for Degree Bounded Survivable Network Design Problem", 
    "arxiv-id": "0911.4544v1", 
    "author": "Nisheeth Vishnoi", 
    "publish": "2009-11-24T05:27:19Z", 
    "summary": "We consider the Degree-Bounded Survivable Network Design Problem: the\nobjective is to find a minimum cost subgraph satisfying the given connectivity\nrequirements as well as the degree bounds on the vertices. If we denote the\nupper bound on the degree of a vertex v by b(v), then we present an algorithm\nthat finds a solution whose cost is at most twice the cost of the optimal\nsolution while the degree of a degree constrained vertex v is at most 2b(v) +\n2. This improves upon the results of Lau and Singh and that of Lau, Naor,\nSalavatipour and Singh."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.4981v4", 
    "title": "Efficient Fully-Compressed Sequence Representations", 
    "arxiv-id": "0911.4981v4", 
    "author": "Yakov Nekrich", 
    "publish": "2009-11-25T23:31:23Z", 
    "summary": "We present a data structure that stores a sequence $s[1..n]$ over alphabet\n$[1..\\sigma]$ in $n\\Ho(s) + o(n)(\\Ho(s){+}1)$ bits, where $\\Ho(s)$ is the\nzero-order entropy of $s$. This structure supports the queries \\access, \\rank\\\nand \\select, which are fundamental building blocks for many other compressed\ndata structures, in worst-case time $\\Oh{\\lg\\lg\\sigma}$ and average time\n$\\Oh{\\lg \\Ho(s)}$. The worst-case complexity matches the best previous results,\nyet these had been achieved with data structures using $n\\Ho(s)+o(n\\lg\\sigma)$\nbits. On highly compressible sequences the $o(n\\lg\\sigma)$ bits of the\nredundancy may be significant compared to the the $n\\Ho(s)$ bits that encode\nthe data. Our representation, instead, compresses the redundancy as well.\nMoreover, our average-case complexity is unprecedented. Our technique is based\non partitioning the alphabet into characters of similar frequency. The\nsubsequence corresponding to each group can then be encoded using fast\nuncompressed representations without harming the overall compression ratios,\neven in the redundancy. The result also improves upon the best current\ncompressed representations of several other data structures. For example, we\nachieve $(i)$ compressed redundancy, retaining the best time complexities, for\nthe smallest existing full-text self-indexes; $(ii)$ compressed permutations\n$\\pi$ with times for $\\pi()$ and $\\pii()$ improved to loglogarithmic; and\n$(iii)$ the first compressed representation of dynamic collections of disjoint\nsets. We also point out various applications to inverted indexes, suffix\narrays, binary relations, and data compressors. ..."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5031v2", 
    "title": "An $O(n^2)$ Algorithm for Computing Longest Common Cyclic Subsequence", 
    "arxiv-id": "0911.5031v2", 
    "author": "M. Sohel Rahman", 
    "publish": "2009-11-26T08:49:48Z", 
    "summary": "The {\\em longest common subsequence (LCS)} problem is a classic and\nwell-studied problem in computer science. LCS is a central problem in\nstringology and finds broad applications in text compression, error-detecting\ncodes and biological sequence comparison. However, in numerous contexts, words\nrepresent cyclic sequences of symbols and LCS must be generalized to consider\nall circular shifts of the strings. This occurs especially in computational\nbiology when genetic material is sequenced form circular DNA or RNA molecules.\nThis initiates the problem of {\\em longest common cyclic subsequence (LCCS)}\nwhich finds the longest subsequence between all circular shifts of two strings.\nIn this paper, we give an $O(n^2)$ algorithm for solving LCCS problem where $n$\nis the number of symbols in the strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5094v1", 
    "title": "Faster FAST(Feedback Arc Set in Tournaments)", 
    "arxiv-id": "0911.5094v1", 
    "author": "Uriel Feige", 
    "publish": "2009-11-26T14:43:31Z", 
    "summary": "We present an algorithm that finds a feedback arc set of size $k$ in a\ntournament in time $n^{O(1)}2^{O(\\sqrt{k})}$. This is asymptotically faster\nthan the running time of previously known algorithms for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5143v1", 
    "title": "Approximation Schemes for Steiner Forest on Planar Graphs and Graphs of   Bounded Treewidth", 
    "arxiv-id": "0911.5143v1", 
    "author": "D\u00e1niel Marx", 
    "publish": "2009-11-26T19:19:53Z", 
    "summary": "We give the first polynomial-time approximation scheme (PTAS) for the Steiner\nforest problem on planar graphs and, more generally, on graphs of bounded\ngenus. As a first step, we show how to build a Steiner forest spanner for such\ngraphs. The crux of the process is a clustering procedure called\nprize-collecting clustering that breaks down the input instance into separate\nsubinstances which are easier to handle; moreover, the terminals in different\nsubinstances are far from each other. Each subinstance has a relatively\ninexpensive Steiner tree connecting all its terminals, and the subinstances can\nbe solved (almost) separately. Another building block is a PTAS for Steiner\nforest on graphs of bounded treewidth. Surprisingly, Steiner forest is NP-hard\neven on graphs of treewidth 3. Therefore, our PTAS for bounded treewidth graph\nneeds a nontrivial combination of approximation arguments and dynamic\nprogramming on the tree decomposition. We further show that Steiner forest can\nbe solved in polynomial time for series-parallel graphs (graphs of treewidth at\nmost two) by a novel combination of dynamic programming and minimum cut\ncomputations, completing our thorough complexity study of Steiner forest in the\nrange of bounded treewidth graphs, planar graphs, and bounded genus graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0287v3", 
    "title": "Tight Thresholds for Cuckoo Hashing via XORSAT", 
    "arxiv-id": "0912.0287v3", 
    "author": "Michael Rink", 
    "publish": "2009-12-01T22:23:48Z", 
    "summary": "We settle the question of tight thresholds for offline cuckoo hashing. The\nproblem can be stated as follows: we have n keys to be hashed into m buckets\neach capable of holding a single key. Each key has k >= 3 (distinct) associated\nbuckets chosen uniformly at random and independently of the choices of other\nkeys. A hash table can be constructed successfully if each key can be placed\ninto one of its buckets. We seek thresholds alpha_k such that, as n goes to\ninfinity, if n/m <= alpha for some alpha < alpha_k then a hash table can be\nconstructed successfully with high probability, and if n/m >= alpha for some\nalpha > alpha_k a hash table cannot be constructed successfully with high\nprobability. Here we are considering the offline version of the problem, where\nall keys and hash values are given, so the problem is equivalent to previous\nmodels of multiple-choice hashing. We find the thresholds for all values of k >\n2 by showing that they are in fact the same as the previously known thresholds\nfor the random k-XORSAT problem. We then extend these results to the setting\nwhere keys can have differing number of choices, and provide evidence in the\nform of an algorithm for a conjecture extending this result to cuckoo hash\ntables that store multiple keys in a bucket."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0322v4", 
    "title": "Submodular Functions: Extensions, Distributions, and Algorithms. A   Survey", 
    "arxiv-id": "0912.0322v4", 
    "author": "Shaddin Dughmi", 
    "publish": "2009-12-02T02:02:27Z", 
    "summary": "Submodularity is a fundamental phenomenon in combinatorial optimization.\nSubmodular functions occur in a variety of combinatorial settings such as\ncoverage problems, cut problems, welfare maximization, and many more.\nTherefore, a lot of work has been concerned with maximizing or minimizing a\nsubmodular function, often subject to combinatorial constraints. Many of these\nalgorithmic results exhibit a common structure. Namely, the function is\nextended to a continuous, usually non-linear, function on a convex domain.\nThen, this relaxation is solved, and the fractional solution rounded to yield\nan integral solution. Often, the continuous extension has a natural\ninterpretation in terms of distributions on subsets of the ground set. This\ninterpretation is often crucial to the results and their analysis. The purpose\nof this survey is to highlight this connection between extensions,\ndistributions, relaxations, and optimization in the context of submodular\nfunctions. We also present the first constant factor approximation algorithm\nfor minimizing symmetric submodular functions subject to a cardinality\nconstraint."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0681v3", 
    "title": "A Local Spectral Method for Graphs: with Applications to Improving Graph   Partitions and Exploring Data Graphs Locally", 
    "arxiv-id": "0912.0681v3", 
    "author": "Nisheeth K. Vishnoi", 
    "publish": "2009-12-03T15:20:59Z", 
    "summary": "The second eigenvalue of the Laplacian matrix and its associated eigenvector\nare fundamental features of an undirected graph, and as such they have found\nwidespread use in scientific computing, machine learning, and data analysis. In\nmany applications, however, graphs that arise have several \\emph{local} regions\nof interest, and the second eigenvector will typically fail to provide\ninformation fine-tuned to each local region. In this paper, we introduce a\nlocally-biased analogue of the second eigenvector, and we demonstrate its\nusefulness at highlighting local properties of data graphs in a semi-supervised\nmanner. To do so, we first view the second eigenvector as the solution to a\nconstrained optimization problem, and we incorporate the local information as\nan additional constraint; we then characterize the optimal solution to this new\nproblem and show that it can be interpreted as a generalization of a\nPersonalized PageRank vector; and finally, as a consequence, we show that the\nsolution can be computed in nearly-linear time. In addition, we show that this\nlocally-biased vector can be used to compute an approximation to the best\npartition \\emph{near} an input seed set in a manner analogous to the way in\nwhich the second eigenvector of the Laplacian can be used to obtain an\napproximation to the best partition in the entire input graph. Such a primitive\nis useful for identifying and refining clusters locally, as it allows us to\nfocus on a local region of interest in a semi-supervised manner. Finally, we\nprovide a detailed empirical evaluation of our method by showing how it can\napplied to finding locally-biased sparse cuts around an input vertex seed set\nin social and information networks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0850v4", 
    "title": "Grammar-Based Compression in a Streaming Model", 
    "arxiv-id": "0912.0850v4", 
    "author": "Pawel Gawrychowski", 
    "publish": "2009-12-04T13:17:22Z", 
    "summary": "We show that, given a string $s$ of length $n$, with constant memory and\nlogarithmic passes over a constant number of streams we can build a\ncontext-free grammar that generates $s$ and only $s$ and whose size is within\nan $\\Oh{\\min (g \\log g, \\sqrt{n \\log g})}$-factor of the minimum $g$. This\nstands in contrast to our previous result that, with polylogarithmic memory and\npolylogarithmic passes over a single stream, we cannot build such a grammar\nwhose size is within any polynomial of $g$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0975v1", 
    "title": "An expected-case sub-cubic solution to the all-pairs shortest path   problem in R", 
    "arxiv-id": "0912.0975v1", 
    "author": "Tib\u00e9rio S. Caetano", 
    "publish": "2009-12-05T03:31:07Z", 
    "summary": "It has been shown by Alon et al. that the so-called 'all-pairs shortest-path'\nproblem can be solved in O((MV)^2.688 * log^3(V)) for graphs with V vertices,\nwith integer distances bounded by M. We solve the more general problem for\ngraphs in R (assuming no negative cycles), with expected-case running time\nO(V^2.5 * log(V)). While our result appears to violate the Omega(V^3)\nrequirement of \"Funny Matrix Multiplication\" (due to Kerr), we find that it has\na sub-cubic expected time solution subject to reasonable conditions on the data\ndistribution. The expected time solution arises when certain sub-problems are\nuncorrelated, though we can do better/worse than the expected-case under\npositive/negative correlation (respectively). Whether we observe\npositive/negative correlation depends on the statistics of the graph in\nquestion. In practice, our algorithm is significantly faster than\nFloyd-Warshall, even for dense graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.1045v3", 
    "title": "Thresholded Covering Algorithms for Robust and Max-Min Optimization", 
    "arxiv-id": "0912.1045v3", 
    "author": "R. Ravi", 
    "publish": "2009-12-05T19:04:49Z", 
    "summary": "The general problem of robust optimization is this: one of several possible\nscenarios will appear tomorrow, but things are more expensive tomorrow than\nthey are today. What should you anticipatorily buy today, so that the\nworst-case cost (summed over both days) is minimized? Feige et al. and\nKhandekar et al. considered the k-robust model where the possible outcomes\ntomorrow are given by all demand-subsets of size k, and gave algorithms for the\nset cover problem, and the Steiner tree and facility location problems in this\nmodel, respectively.\n  In this paper, we give the following simple and intuitive template for\nk-robust problems: \"having built some anticipatory solution, if there exists a\nsingle demand whose augmentation cost is larger than some threshold, augment\nthe anticipatory solution to cover this demand as well, and repeat\". In this\npaper we show that this template gives us improved approximation algorithms for\nk-robust Steiner tree and set cover, and the first approximation algorithms for\nk-robust Steiner forest, minimum-cut and multicut. All our approximation ratios\n(except for multicut) are almost best possible.\n  As a by-product of our techniques, we also get algorithms for max-min\nproblems of the form: \"given a covering problem instance, which k of the\nelements are costliest to cover?\"."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.1137v1", 
    "title": "Euclidean Prize-collecting Steiner Forest", 
    "arxiv-id": "0912.1137v1", 
    "author": "MohammadTaghi Hajiaghayi", 
    "publish": "2009-12-06T21:15:49Z", 
    "summary": "In this paper, we consider Steiner forest and its generalizations,\nprize-collecting Steiner forest and k-Steiner forest, when the vertices of the\ninput graph are points in the Euclidean plane and the lengths are Euclidean\ndistances. First, we present a simpler analysis of the polynomial-time\napproximation scheme (PTAS) of Borradaile et al. [12] for the Euclidean Steiner\nforest problem. This is done by proving a new structural property and modifying\nthe dynamic programming by adding a new piece of information to each dynamic\nprogramming state. Next we develop a PTAS for a well-motivated case, i.e., the\nmultiplicative case, of prize-collecting and budgeted Steiner forest. The ideas\nused in the algorithm may have applications in design of a broad class of\nbicriteria PTASs. At the end, we demonstrate why PTASs for these problems can\nbe hard in the general Euclidean case (and thus for PTASs we cannot go beyond\nthe multiplicative case)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.2269v4", 
    "title": "Computing a Discrete Logarithm in O(n^3)", 
    "arxiv-id": "0912.2269v4", 
    "author": "Charles Sauerbier", 
    "publish": "2009-12-11T16:07:20Z", 
    "summary": "This paper presents a means with time complexity of at worst O(n^3) to\ncompute the discrete logarithm on cyclic finite groups of integers modulo p.\nThe algorithm makes use of reduction of the problem to that of finding the\nconcurrent zeros of two periodic functions in the real numbers. The problem is\ntreated as an analog to a form of analog rotor-code computed cipher."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.2813v2", 
    "title": "Combining Partial Order Alignment and Progressive Near-Optimal Alignment", 
    "arxiv-id": "0912.2813v2", 
    "author": "Dai Tri Man Le", 
    "publish": "2009-12-15T07:21:33Z", 
    "summary": "In this paper, I proposed to utilize partial-order alignment technique as a\nheuristic method to cope with the state-space explosion problem in progressive\nnear-optimal alignment. The key idea of my approach is a formal treatment of\nprogressive partial order alignment based on the graph product construction."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.3188v2", 
    "title": "Robust Fault Tolerant uncapacitated facility location", 
    "arxiv-id": "0912.3188v2", 
    "author": "David Peleg", 
    "publish": "2009-12-16T16:46:55Z", 
    "summary": "In the uncapacitated facility location problem, given a graph, a set of\ndemands and opening costs, it is required to find a set of facilities R, so as\nto minimize the sum of the cost of opening the facilities in R and the cost of\nassigning all node demands to open facilities. This paper concerns the robust\nfault-tolerant version of the uncapacitated facility location problem (RFTFL).\nIn this problem, one or more facilities might fail, and each demand should be\nsupplied by the closest open facility that did not fail. It is required to find\na set of facilities R, so as to minimize the sum of the cost of opening the\nfacilities in R and the cost of assigning all node demands to open facilities\nthat did not fail, after the failure of up to \\alpha facilities. We present a\npolynomial time algorithm that yields a 6.5-approximation for this problem with\nat most one failure and a 1.5 + 7.5\\alpha-approximation for the problem with at\nmost \\alpha > 1 failures. We also show that the RFTFL problem is NP-hard even\non trees, and even in the case of a single failure."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.3963v1", 
    "title": "Fast Fraction-Integer Method for Computing Multiplicative Inverse", 
    "arxiv-id": "0912.3963v1", 
    "author": "Nidal F. Shilbayeh", 
    "publish": "2009-12-20T02:32:42Z", 
    "summary": "Multiplicative inverse is a crucial operation in public key cryptography, and\nbeen widely used in cryptography. Public key cryptography has given rise to\nsuch a need, in which we need to generate a related public and private pair of\nnumbers, each of which is the inverse of the other. The basic method to find\nmultiplicative inverses is Extended-Euclidean method. In this paper we will\npropose a new algorithm for computing the inverse, based on continues subtract\nfraction from integer and divide by fraction to obtain integer that will be\nused to compute the inverse d. The authors claim that the proposed method more\nefficient and faster than the existed methods."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.4569v2", 
    "title": "Continuous Monitoring of Distributed Data Streams over a Time-based   Sliding Window", 
    "arxiv-id": "0912.4569v2", 
    "author": "Hing-Fung Ting", 
    "publish": "2009-12-23T09:56:50Z", 
    "summary": "The past decade has witnessed many interesting algorithms for maintaining\nstatistics over a data stream. This paper initiates a theoretical study of\nalgorithms for monitoring distributed data streams over a time-based sliding\nwindow (which contains a variable number of items and possibly out-of-order\nitems). The concern is how to minimize the communication between individual\nstreams and the root, while allowing the root, at any time, to be able to\nreport the global statistics of all streams within a given error bound. This\npaper presents communication-efficient algorithms for three classical\nstatistics, namely, basic counting, frequent items and quantiles. The\nworst-case communication cost over a window is $O(\\frac{k} {\\epsilon} \\log\n\\frac{\\epsilon N}{k})$ bits for basic counting and $O(\\frac{k}{\\epsilon} \\log\n\\frac{N}{k})$ words for the remainings, where $k$ is the number of distributed\ndata streams, $N$ is the total number of items in the streams that arrive or\nexpire in the window, and $\\epsilon < 1$ is the desired error bound. Matching\nand nearly matching lower bounds are also obtained."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.4798v1", 
    "title": "Demand-Supply Optimization with Risk Management for a Multi-Connection   Water Reservoir Network", 
    "arxiv-id": "0912.4798v1", 
    "author": "Thavivongse Sriburi", 
    "publish": "2009-12-24T06:04:15Z", 
    "summary": "In this paper, we propose a framework to solve a demand-supply optimization\nproblem of long-term water resource allocation on a multi-connection reservoir\nnetwork which, in two aspects, is different to the problem considered in\nprevious works. First, while all previous works consider a problem where each\nreservoir can transfer water to only one fixed reservoir, we consider a\nmulti-connection network being constructed in Thailand in which each reservoir\ncan transfer water to many reservoirs in one period of time. Second, a\ndemand-supply plan considered here is static, in contrast to a dynamic policy\nconsidered in previous works. Moreover, in order to efficiently develop a\nlong-term static plan, a severe loss (a risk) is taken into account, i.e. a\nrisk occurs if the real amount of water stored in each reservoir in each time\nperiod is less than what planned by the optimizer. The multi-connection\nfunction and the risk make the problem rather complex such that traditional\nstochastic dynamic programming and deterministic/heuristic approaches are\ninappropriate. Our framework is based on a novel convex programming formulation\nin which stochastic information can be naturally taken into account and an\noptimal solution is guaranteed to be found efficiently. Extensive experimental\nresults show promising results of the framework."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.5424v3", 
    "title": "Backyard Cuckoo Hashing: Constant Worst-Case Operations with a Succinct   Representation", 
    "arxiv-id": "0912.5424v3", 
    "author": "Gil Segev", 
    "publish": "2009-12-30T07:55:20Z", 
    "summary": "The performance of a dynamic dictionary is measured mainly by its update\ntime, lookup time, and space consumption. In terms of update time and lookup\ntime there are known constructions that guarantee constant-time operations in\nthe worst case with high probability, and in terms of space consumption there\nare known constructions that use essentially optimal space. However, although\nthe first analysis of a dynamic dictionary dates back more than 45 years ago\n(when Knuth analyzed linear probing in 1963), the trade-off between these\naspects of performance is still not completely understood. In this paper we\nsettle two fundamental open problems: - We construct the first dynamic\ndictionary that enjoys the best of both worlds: it stores n elements using\n(1+epsilon)n memory words, and guarantees constant-time operations in the worst\ncase with high probability. Specifically, for any epsilon = \\Omega((\\log\\log n\n/ \\log n)^{1/2} ) and for any sequence of polynomially many operations, with\nhigh probability over the randomness of the initialization phase, all\noperations are performed in constant time which is independent of epsilon. The\nconstruction is a two-level variant of cuckoo hashing, augmented with a\n\"backyard\" that handles a large fraction of the elements, together with a\nde-amortized perfect hashing scheme for eliminating the dependency on epsilon.\n- We present a variant of the above construction that uses only (1+o(1))B bits,\nwhere B is the information-theoretic lower bound for representing a set of size\nn taken from a universe of size u, and guarantees constant-time operations in\nthe worst case with high probability, as before. This problem was open even in\nthe amortized setting. One of the main ingredients of our construction is a\npermutation-based variant of cuckoo hashing, which significantly improves the\nspace consumption of cuckoo hashing when dealing with a rather small universe."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.5473v1", 
    "title": "A Variable Depth Sequential Search Heuristic for the Quadratic   Assignment Problem", 
    "arxiv-id": "0912.5473v1", 
    "author": "Gerald Paul", 
    "publish": "2009-12-30T14:42:30Z", 
    "summary": "We develop a variable depth search heuristic for the quadratic assignment\nproblem. The heuristic is based on sequential changes in assignments analogous\nto the Lin-Kernighan sequential edge moves for the traveling salesman problem.\nWe treat unstructured problem instances of sizes 60 to 400. When the heuristic\nis used in conjunction with robust tabu search, we measure performance\nimprovements of up to a factor of 15 compared to the use of robust tabu alone.\nThe performance improvement increases as the problem size increases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0639v1", 
    "title": "Optimal Exploration of Terrains with Obstacles", 
    "arxiv-id": "1001.0639v1", 
    "author": "Andrzej Pelc", 
    "publish": "2010-01-05T07:29:11Z", 
    "summary": "A mobile robot represented by a point moving in the plane has to explore an\nunknown terrain with obstacles. Both the terrain and the obstacles are modeled\nas arbitrary polygons. We consider two scenarios: the unlimited vision, when\nthe robot situated at a point p of the terrain explores (sees) all points q of\nthe terrain for which the segment pq belongs to the terrain, and the limited\nvision, when we require additionally that the distance between p and q be at\nmost 1. All points of the terrain (except obstacles) have to be explored and\nthe performance of an exploration algorithm is measured by the length of the\ntrajectory of the robot. For unlimited vision we show an exploration algorithm\nwith complexity O(P + D?k), where P is the total perimeter of the terrain\n(including perimeters of obstacles), D is the diameter of the convex hull of\nthe terrain, and k is the number of obstacles. We do not assume knowledge of\nthese parameters. We also prove a matching lower bound showing that the above\ncomplexity is optimal, even if the terrain is known to the robot. For limited\nvision we show exploration algorithms with complexity O(P + A + ?Ak), where A\nis the area of the terrain (excluding obstacles). Our algorithms work either\nfor arbitrary terrains, if one of the parameters A or k is known, or for c-fat\nterrains, where c is any constant (unknown to the robot) and no additional\nknowledge is assumed. (A terrain T with obstacles is c-fat if R/r ? c, where R\nis the radius of the smallest disc containing T and r is the radius of the\nlargest disc contained in T .) We also prove a matching lower bound ?(P + A +\n?Ak) on the complexity of exploration for limited vision, even if the terrain\nis known to the robot."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0824v2", 
    "title": "Approximate Shortest Paths Avoiding a Failed Vertex: Optimal Size Data   Structures for Unweighted Graph", 
    "arxiv-id": "1001.0824v2", 
    "author": "Neelesh Khanna Surender Baswana", 
    "publish": "2010-01-06T07:02:30Z", 
    "summary": "Let $G=(V,E)$ be any undirected graph on $V$ vertices and $E$ edges. A path\n$\\textbf{P}$ between any two vertices $u,v\\in V$ is said to be $t$-approximate\nshortest path if its length is at most $t$ times the length of the shortest\npath between $u$ and $v$. We consider the problem of building a compact data\nstructure for a given graph $G$ which is capable of answering the following\nquery for any $u,v,z\\in V$ and $t>1$:\n  Report $t$-approximate shortest path between $u$ and $v$ when vertex $z$\nfails\n  We present data structures for the single source as well all-pairs versions\nof this problem. Our data structures guarantee optimal query time. Most\nimpressive feature of our data structures is that their size {\\em nearly} match\nthe size of their best static counterparts."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0920v2", 
    "title": "Online Correlation Clustering", 
    "arxiv-id": "1001.0920v2", 
    "author": "Warren Schudy", 
    "publish": "2010-01-06T15:54:38Z", 
    "summary": "We study the online clustering problem where data items arrive in an online\nfashion. The algorithm maintains a clustering of data items into similarity\nclasses. Upon arrival of v, the relation between v and previously arrived items\nis revealed, so that for each u we are told whether v is similar to u. The\nalgorithm can create a new cluster for v and merge existing clusters.\n  When the objective is to minimize disagreements between the clustering and\nthe input, we prove that a natural greedy algorithm is O(n)-competitive, and\nthis is optimal.\n  When the objective is to maximize agreements between the clustering and the\ninput, we prove that the greedy algorithm is .5-competitive; that no online\nalgorithm can be better than .834-competitive; we prove that it is possible to\nget better than 1/2, by exhibiting a randomized algorithm with competitive\nratio .5+c for a small positive fixed constant c."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0961v3", 
    "title": "Computing a Frobenius Coin Problem decision problem in O(n^2)", 
    "arxiv-id": "1001.0961v3", 
    "author": "Charles Sauerbier", 
    "publish": "2010-01-06T20:38:17Z", 
    "summary": "Expanding on recent results of another an algorithm is presented that\nprovides solution to the Frobenius Coin Problem in worst case O(n^2) in the\nmagnitude of the largest denomination."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.1565v3", 
    "title": "Random Access to Grammar Compressed Strings", 
    "arxiv-id": "1001.1565v3", 
    "author": "Oren Weimann", 
    "publish": "2010-01-11T20:29:41Z", 
    "summary": "Grammar based compression, where one replaces a long string by a small\ncontext-free grammar that generates the string, is a simple and powerful\nparadigm that captures many popular compression schemes. In this paper, we\npresent a novel grammar representation that allows efficient random access to\nany character or substring without decompressing the string.\n  Let $S$ be a string of length $N$ compressed into a context-free grammar\n$\\mathcal{S}$ of size $n$. We present two representations of $\\mathcal{S}$\nachieving $O(\\log N)$ random access time, and either $O(n\\cdot \\alpha_k(n))$\nconstruction time and space on the pointer machine model, or $O(n)$\nconstruction time and space on the RAM. Here, $\\alpha_k(n)$ is the inverse of\nthe $k^{th}$ row of Ackermann's function. Our representations also efficiently\nsupport decompression of any substring in $S$: we can decompress any substring\nof length $m$ in the same complexity as a single random access query and\nadditional $O(m)$ time. Combining these results with fast algorithms for\nuncompressed approximate string matching leads to several efficient algorithms\nfor approximate string matching on grammar-compressed strings without\ndecompression. For instance, we can find all approximate occurrences of a\npattern $P$ with at most $k$ errors in time $O(n(\\min\\{|P|k, k^4 + |P|\\} + \\log\nN) + occ)$, where $occ$ is the number of occurrences of $P$ in $S$. Finally, we\ngeneralize our results to navigation and other operations on grammar-compressed\nordered trees.\n  All of the above bounds significantly improve the currently best known\nresults. To achieve these bounds, we introduce several new techniques and data\nstructures of independent interest, including a predecessor data structure, two\n\"biased\" weighted ancestor data structures, and a compact representation of\nheavy paths in grammars."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.1819v2", 
    "title": "Genealogical Information Search by Using Parent Bidirectional Breadth   Algorithm and Rule Based Relationship", 
    "arxiv-id": "1001.1819v2", 
    "author": "Phayung Meesad", 
    "publish": "2010-01-12T08:48:17Z", 
    "summary": "Genealogical information is the best histories resources for culture study\nand cultural heritage. The genealogical research generally presents family\ninformation and depict tree diagram. This paper presents Parent Bidirectional\nBreadth Algorithm (PBBA) to find consanguine relationship between two persons.\nIn addition, the paper utilizes rules based system in order to identify\nconsanguine relationship. The study reveals that PBBA is fast to solve the\ngenealogical information search problem and the Rule Based Relationship\nprovides more benefits in blood relationship identification."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_21", 
    "link": "http://arxiv.org/pdf/1001.2101v3", 
    "title": "Sampled Longest Common Prefix Array", 
    "arxiv-id": "1001.2101v3", 
    "author": "Jouni Sir\u00e9n", 
    "publish": "2010-01-13T09:18:24Z", 
    "summary": "When augmented with the longest common prefix (LCP) array and some other\nstructures, the suffix array can solve many string processing problems in\noptimal time and space. A compressed representation of the LCP array is also\none of the main building blocks in many compressed suffix tree proposals. In\nthis paper, we describe a new compressed LCP representation: the sampled LCP\narray. We show that when used with a compressed suffix array (CSA), the sampled\nLCP array often offers better time/space trade-offs than the existing\nalternatives. We also show how to construct the compressed representations of\nthe LCP array directly from a CSA."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.2860v2", 
    "title": "Succinct Dictionary Matching With No Slowdown", 
    "arxiv-id": "1001.2860v2", 
    "author": "Djamal Belazzougui", 
    "publish": "2010-01-16T22:10:57Z", 
    "summary": "The problem of dictionary matching is a classical problem in string matching:\ngiven a set S of d strings of total length n characters over an (not\nnecessarily constant) alphabet of size sigma, build a data structure so that we\ncan match in a any text T all occurrences of strings belonging to S. The\nclassical solution for this problem is the Aho-Corasick automaton which finds\nall occ occurrences in a text T in time O(|T| + occ) using a data structure\nthat occupies O(m log m) bits of space where m <= n + 1 is the number of states\nin the automaton. In this paper we show that the Aho-Corasick automaton can be\nrepresented in just m(log sigma + O(1)) + O(d log(n/d)) bits of space while\nstill maintaining the ability to answer to queries in O(|T| + occ) time. To the\nbest of our knowledge, the currently fastest succinct data structure for the\ndictionary matching problem uses space O(n log sigma) while answering queries\nin O(|T|log log n + occ) time. In this paper we also show how the space\noccupancy can be reduced to m(H0 + O(1)) + O(d log(n/d)) where H0 is the\nempirical entropy of the characters appearing in the trie representation of the\nset S, provided that sigma < m^epsilon for any constant 0 < epsilon < 1. The\nquery time remains unchanged."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.2891v1", 
    "title": "Detecting High Log-Densities -- an O(n^1/4) Approximation for Densest   k-Subgraph", 
    "arxiv-id": "1001.2891v1", 
    "author": "Aravindan Vijayaraghavan", 
    "publish": "2010-01-17T13:31:39Z", 
    "summary": "In the Densest k-Subgraph problem, given a graph G and a parameter k, one\nneeds to find a subgraph of G induced on k vertices that contains the largest\nnumber of edges. There is a significant gap between the best known upper and\nlower bounds for this problem. It is NP-hard, and does not have a PTAS unless\nNP has subexponential time algorithms. On the other hand, the current best\nknown algorithm of Feige, Kortsarz and Peleg, gives an approximation ratio of\nn^(1/3-epsilon) for some specific epsilon > 0 (estimated at around 1/60).\n  We present an algorithm that for every epsilon > 0 approximates the Densest\nk-Subgraph problem within a ratio of n^(1/4+epsilon) in time n^O(1/epsilon). In\nparticular, our algorithm achieves an approximation ratio of O(n^1/4) in time\nn^O(log n). Our algorithm is inspired by studying an average-case version of\nthe problem where the goal is to distinguish random graphs from graphs with\nplanted dense subgraphs. The approximation ratio we achieve for the general\ncase matches the distinguishing ratio we obtain for this planted problem.\n  At a high level, our algorithms involve cleverly counting appropriately\ndefined trees of constant size in G, and using these counts to identify the\nvertices of the dense subgraph. Our algorithm is based on the following\nprinciple. We say that a graph G(V,E) has log-density alpha if its average\ndegree is Theta(|V|^alpha). The algorithmic core of our result is a family of\nalgorithms that output k-subgraphs of nontrivial density whenever the\nlog-density of the densest k-subgraph is larger than the log-density of the\nhost graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3044v2", 
    "title": "Dynamic sharing of a multiple access channel", 
    "arxiv-id": "1001.3044v2", 
    "author": "Dariusz R. Kowalski", 
    "publish": "2010-01-18T12:51:04Z", 
    "summary": "In this paper we consider the mutual exclusion problem on a multiple access\nchannel. Mutual exclusion is one of the fundamental problems in distributed\ncomputing. In the classic version of this problem, n processes perform a\nconcurrent program which occasionally triggers some of them to use shared\nresources, such as memory, communication channel, device, etc. The goal is to\ndesign a distributed algorithm to control entries and exits to/from the shared\nresource in such a way that in any time there is at most one process accessing\nit. We consider both the classic and a slightly weaker version of mutual\nexclusion, called ep-mutual-exclusion, where for each period of a process\nstaying in the critical section the probability that there is some other\nprocess in the critical section is at most ep. We show that there are channel\nsettings, where the classic mutual exclusion is not feasible even for\nrandomized algorithms, while ep-mutual-exclusion is. In more relaxed channel\nsettings, we prove an exponential gap between the makespan complexity of the\nclassic mutual exclusion problem and its weaker ep-exclusion version. We also\nshow how to guarantee fairness of mutual exclusion algorithms, i.e., that each\nprocess that wants to enter the critical section will eventually succeed."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3493v1", 
    "title": "Posynomial Geometric Programming Problems with Multiple Parameters", 
    "arxiv-id": "1001.3493v1", 
    "author": "K. K. Biswal", 
    "publish": "2010-01-20T08:00:12Z", 
    "summary": "Geometric programming problem is a powerful tool for solving some special\ntype non-linear programming problems. It has a wide range of applications in\noptimization and engineering for solving some complex optimization problems.\nMany applications of geometric programming are on engineering design problems\nwhere parameters are estimated using geometric programming. When the parameters\nin the problems are imprecise, the calculated objective value should be\nimprecise as well. In this paper we have developed a method to solve geometric\nprogramming problems where the exponent of the variables in the objective\nfunction, cost coefficients and right hand side are multiple parameters. The\nequivalent mathematical programming problems are formulated to find their\ncorresponding value of the objective function based on the duality theorem. By\napplying a variable separable technique the multi-choice mathematical\nprogramming problem is transformed into multiple one level geometric\nprogramming problem which produces multiple objective values that helps\nengineers to handle more realistic engineering design problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3713v1", 
    "title": "On Fast Algorithm for Computing Even-Length DCT", 
    "arxiv-id": "1001.3713v1", 
    "author": "Yuriy A. Reznik", 
    "publish": "2010-01-21T03:13:44Z", 
    "summary": "We study recursive algorithm for computing DCT of lengths $N=q 2^m$ ($m,q \\in\n\\mathbb{N}$, $q$ is odd) due to C.W.Kok. We show that this algorithm has the\nsame multiplicative complexity as theoretically achievable by the prime factor\ndecomposition, when $m \\leqslant 2$. We also show that C.W.Kok's factorization\nallows a simple conversion to a scaled form. We analyze complexity of such a\nscaled factorization, and show that for some lengths it achieves lower\nmultiplicative complexity than one of known prime factor-based scaled\ntransforms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.4420v3", 
    "title": "The Complexity of Flood Filling Games", 
    "arxiv-id": "1001.4420v3", 
    "author": "Benjamin Sach", 
    "publish": "2010-01-25T13:40:57Z", 
    "summary": "We study the complexity of the popular one player combinatorial game known as\nFlood-It. In this game the player is given an n by n board of tiles where each\ntile is allocated one of c colours. The goal is to make the colours of all\ntiles equal via the shortest possible sequence of flooding operations. In the\nstandard version, a flooding operation consists of the player choosing a colour\nk, which then changes the colour of all the tiles in the monochromatic region\nconnected to the top left tile to k. After this operation has been performed,\nneighbouring regions which are already of the chosen colour k will then also\nbecome connected, thereby extending the monochromatic region of the board. We\nshow that finding the minimum number of flooding operations is NP-hard for c>=3\nand that this even holds when the player can perform flooding operations from\nany position on the board. However, we show that this \"free\" variant is in P\nfor c=2. We also prove that for an unbounded number of colours, Flood-It\nremains NP-hard for boards of height at least 3, but is in P for boards of\nheight 2. Next we show how a c-1 approximation and a randomised 2c/3\napproximation algorithm can be derived, and that no polynomial time constant\nfactor, independent of c, approximation algorithm exists unless P=NP. We then\ninvestigate how many moves are required for the \"most demanding\" n by n boards\n(those requiring the most moves) and show that the number grows as fast as\nTheta(n*c^0.5). Finally, we consider boards where the colours of the tiles are\nchosen at random and show that for c>=2, the number of moves required to flood\nthe whole board is Omega(n) with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.5076v2", 
    "title": "Online Stochastic Packing Applied to Display Ad Allocation", 
    "arxiv-id": "1001.5076v2", 
    "author": "Cliff Stein", 
    "publish": "2010-01-28T00:51:03Z", 
    "summary": "Inspired by online ad allocation, we study online stochastic packing linear\nprograms from theoretical and practical standpoints. We first present a\nnear-optimal online algorithm for a general class of packing linear programs\nwhich model various online resource allocation problems including online\nvariants of routing, ad allocations, generalized assignment, and combinatorial\nauctions. As our main theoretical result, we prove that a simple primal-dual\ntraining-based algorithm achieves a (1 - o(1))-approximation guarantee in the\nrandom order stochastic model. This is a significant improvement over\nlogarithmic or constant-factor approximations for the adversarial variants of\nthe same problems (e.g. factor 1 - 1/e for online ad allocation, and \\log m for\nonline routing). We then focus on the online display ad allocation problem and\nstudy the efficiency and fairness of various training-based and online\nallocation algorithms on data sets collected from real-life display ad\nallocation system. Our experimental evaluation confirms the effectiveness of\ntraining-based primal-dual algorithms on real data sets, and also indicate an\nintrinsic trade-off between fairness and efficiency."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.0046v3", 
    "title": "Multi-dimensional Boltzmann Sampling of Languages", 
    "arxiv-id": "1002.0046v3", 
    "author": "Yann Ponty", 
    "publish": "2010-01-30T06:24:13Z", 
    "summary": "This paper addresses the uniform random generation of words from a\ncontext-free language (over an alphabet of size $k$), while constraining every\nletter to a targeted frequency of occurrence. Our approach consists in a\nmultidimensional extension of Boltzmann samplers \\cite{Duchon2004}. We show\nthat, under mostly \\emph{strong-connectivity} hypotheses, our samplers return a\nword of size in $[(1-\\varepsilon)n, (1+\\varepsilon)n]$ and exact frequency in\n$\\mathcal{O}(n^{1+k/2})$ expected time. Moreover, if we accept tolerance\nintervals of width in $\\Omega(\\sqrt{n})$ for the number of occurrences of each\nletters, our samplers perform an approximate-size generation of words in\nexpected $\\mathcal{O}(n)$ time. We illustrate these techniques on the\ngeneration of Tetris tessellations with uniform statistics in the different\ntypes of tetraminoes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.0874v1", 
    "title": "MADMX: A Novel Strategy for Maximal Dense Motif Extraction", 
    "arxiv-id": "1002.0874v1", 
    "author": "Fabio Vandin", 
    "publish": "2010-02-04T01:20:12Z", 
    "summary": "We develop, analyze and experiment with a new tool, called MADMX, which\nextracts frequent motifs, possibly including don't care characters, from\nbiological sequences. We introduce density, a simple and flexible measure for\nbounding the number of don't cares in a motif, defined as the ratio of solid\n(i.e., different from don't care) characters to the total length of the motif.\nBy extracting only maximal dense motifs, MADMX reduces the output size and\nimproves performance, while enhancing the quality of the discoveries. The\nefficiency of our approach relies on a newly defined combining operation,\ndubbed fusion, which allows for the construction of maximal dense motifs in a\nbottom-up fashion, while avoiding the generation of nonmaximal ones. We provide\nexperimental evidence of the efficiency and the quality of the motifs returned\nby MADMX"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.1021v1", 
    "title": "Heuristic Contraction Hierarchies with Approximation Guarantee", 
    "arxiv-id": "1002.1021v1", 
    "author": "Robert Geisberger", 
    "publish": "2010-02-04T15:27:41Z", 
    "summary": "We present a new heuristic point-to-point routing algorithm based on\ncontraction hierarchies (CH). Given an epsilon >= 0, we can prove that the\nlength of the path computed by our algorithm is at most (1+epsilon) times the\nlength of the optimal (shortest) path. CH is based on node contraction:\nremoving nodes from a network and adding shortcut edges to preserve shortest\npath distances. Our algorithm tries to avoid shortcuts even when a replacement\npath is epsilon times longer."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.1292v2", 
    "title": "Mod/Resc Parsimony Inference", 
    "arxiv-id": "1002.1292v2", 
    "author": "Marie-France Sagot", 
    "publish": "2010-02-05T18:15:15Z", 
    "summary": "We address in this paper a new computational biology problem that aims at\nunderstanding a mechanism that could potentially be used to genetically\nmanipulate natural insect populations infected by inherited, intra-cellular\nparasitic bacteria. In this problem, that we denote by \\textsc{Mod/Resc\nParsimony Inference}, we are given a boolean matrix and the goal is to find two\nother boolean matrices with a minimum number of columns such that an\nappropriately defined operation on these matrices gives back the input. We show\nthat this is formally equivalent to the \\textsc{Bipartite Biclique Edge Cover}\nproblem and derive some complexity results for our problem using this\nequivalence. We provide a new, fixed-parameter tractability approach for\nsolving both that slightly improves upon a previously published algorithm for\nthe \\textsc{Bipartite Biclique Edge Cover}. Finally, we present experimental\nresults where we applied some of our techniques to a real-life data set."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.2147v1", 
    "title": "Optimization with More than One Budget", 
    "arxiv-id": "1002.2147v1", 
    "author": "Rico Zenklusen", 
    "publish": "2010-02-10T17:48:15Z", 
    "summary": "A natural way to deal with multiple, partially conflicting objectives is\nturning all the objectives but one into budget constraints. Some classical\npolynomial-time optimization problems, such as spanning tree and forest,\nshortest path, (perfect) matching, independent set (basis) in a matroid or in\nthe intersection of two matroids, become NP-hard even with one budget\nconstraint. Still, for most of these problems deterministic and randomized\npolynomial-time approximation schemes are known. In the case of two or more\nbudgets, typically only multi-criteria approximation schemes are available,\nwhich return slightly infeasible solutions. Not much is known however for the\ncase of strict budget constraints: filling this gap is the main goal of this\npaper.\n  We show that shortest path, perfect matching, and spanning tree (and hence\nmatroid basis and matroid intersection basis) are inapproximable already with\ntwo budget constraints. For the remaining problems, whose set of solutions\nforms an independence system, we present deterministic and randomized\npolynomial-time approximation schemes for a constant number k of budget\nconstraints. Our results are based on a variety of techniques:\n  1. We present a simple and powerful mechanism to transform multi-criteria\napproximation schemes into pure approximation schemes.\n  2. We show that points in low dimensional faces of any matroid polytope are\nalmost integral, an interesting result on its own. This gives a deterministic\napproximation scheme for k-budgeted matroid independent set.\n  3. We present a deterministic approximation scheme for 2-budgeted matching.\nThe backbone of this result is a purely topological property of curves in R^2."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4005v1", 
    "title": "Improved NSGA-II Based on a Novel Ranking Scheme", 
    "arxiv-id": "1002.4005v1", 
    "author": "A. Kandasamy", 
    "publish": "2010-02-21T19:43:45Z", 
    "summary": "Non-dominated Sorting Genetic Algorithm (NSGA) has established itself as a\nbenchmark algorithm for Multiobjective Optimization. The determination of\npareto-optimal solutions is the key to its success. However the basic algorithm\nsuffers from a high order of complexity, which renders it less useful for\npractical applications. Among the variants of NSGA, several attempts have been\nmade to reduce the complexity. Though successful in reducing the runtime\ncomplexity, there is scope for further improvements, especially considering\nthat the populations involved are frequently of large size. We propose a\nvariant which reduces the run-time complexity using the simple principle of\nspace-time trade-off. The improved algorithm is applied to the problem of\nclassifying types of leukemia based on microarray data. Results of comparative\ntests are presented showing that the improved algorithm performs well on large\npopulations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4034v1", 
    "title": "On constant factor approximation for earth mover distance over doubling   metrics", 
    "arxiv-id": "1002.4034v1", 
    "author": "Shi Li", 
    "publish": "2010-02-22T01:26:19Z", 
    "summary": "Given a metric space $(X,d_X)$, the earth mover distance between two\ndistributions over $X$ is defined as the minimum cost of a bipartite matching\nbetween the two distributions. The doubling dimension of a metric $(X, d_X)$ is\nthe smallest value $\\alpha$ such that every ball in $X$ can be covered by\n$2^\\alpha$ ball of half the radius. We study efficient algorithms for\napproximating earth mover distance over metrics with bounded doubling\ndimension.\n  Given a metric $(X, d_X)$, with $|X| = n$, we can use $\\tilde O(n^2)$\npreprocessing time to create a data structure of size $\\tilde O(n^{1 + \\e})$,\nsuch that subsequently queried EMDs can be $O(\\alpha_X/\\e)$-approximated in\n$\\tilde O(n)$ time.\n  We also show a weaker form of sketching scheme, which we call \"encoding\nscheme\". Given $(X, d_X)$, by using $\\tilde O(n^2)$ preprocessing time, every\nsubsequent distribution $\\mu$ over $X$ can be encoded into $F(\\mu)$ in $\\tilde\nO(n^{1 + \\e})$ time. Given $F(\\mu)$ and $F(\\nu)$, the EMD between $\\mu$ and\n$\\nu$ can be $O(\\alpha_X/\\e)$-approximated in $\\tilde O(n^\\e)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4330v1", 
    "title": "Defining and Computing Alternative Routes in Road Networks", 
    "arxiv-id": "1002.4330v1", 
    "author": "Roland Bader", 
    "publish": "2010-02-23T13:43:34Z", 
    "summary": "Every human likes choices. But today's fast route planning algorithms usually\ncompute just a single route between source and target. There are beginnings to\ncompute alternative routes, but this topic has not been studied thoroughly.\nOften, the aspect of meaningful alternative routes is neglected from a human\npoint of view. We fill in this gap by suggesting mathematical definitions for\nsuch routes. As a second contribution we propose heuristics to compute them, as\nthis is NP-hard in general."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.5034v2", 
    "title": "Threshold rules for online sample selection", 
    "arxiv-id": "1002.5034v2", 
    "author": "Seeun Umboh", 
    "publish": "2010-02-26T17:52:28Z", 
    "summary": "We consider the following sample selection problem. We observe in an online\nfashion a sequence of samples, each endowed by a quality. Our goal is to either\nselect or reject each sample, so as to maximize the aggregate quality of the\nsubsample selected so far. There is a natural trade-off here between the rate\nof selection and the aggregate quality of the subsample. We show that for a\nnumber of such problems extremely simple and oblivious \"threshold rules\" for\nselection achieve optimal tradeoffs between rate of selection and aggregate\nquality in a probabilistic sense. In some cases we show that the same threshold\nrule is optimal for a large class of quality distributions and is thus\noblivious in a strong sense."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.0139v1", 
    "title": "An O(loglog n)-Competitive Binary Search Tree with Optimal Worst-Case   Access Times", 
    "arxiv-id": "1003.0139v1", 
    "author": "Rolf Fagerberg", 
    "publish": "2010-02-27T23:39:35Z", 
    "summary": "We present the zipper tree, an $O(\\log \\log n)$-competitive online binary\nsearch tree that performs each access in $O(\\log n)$ worst-case time. This\nshows that for binary search trees, optimal worst-case access time and\nnear-optimal amortized access time can be guaranteed simultaneously."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.0722v2", 
    "title": "Approximation Algorithms for Optimal Decision Trees and Adaptive TSP   Problems", 
    "arxiv-id": "1003.0722v2", 
    "author": "R. Ravi", 
    "publish": "2010-03-03T03:58:29Z", 
    "summary": "We consider the problem of constructing optimal decision trees: given a\ncollection of tests which can disambiguate between a set of $m$ possible\ndiseases, each test having a cost, and the a-priori likelihood of the patient\nhaving any particular disease, what is a good adaptive strategy to perform\nthese tests to minimize the expected cost to identify the disease? We settle\nthe approximability of this problem by giving a tight $O(\\log m)$-approximation\nalgorithm. We also consider a more substantial generalization, the Adaptive TSP\nproblem. Given an underlying metric space, a random subset $S$ of cities is\ndrawn from a known distribution, but $S$ is initially unknown to us--we get\ninformation about whether any city is in $S$ only when we visit the city in\nquestion. What is a good adaptive way of visiting all the cities in the random\nsubset $S$ while minimizing the expected distance traveled? For this problem,\nwe give the first poly-logarithmic approximation, and show that this algorithm\nis best possible unless we can improve the approximation guarantees for the\nwell-known group Steiner tree problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.1260v1", 
    "title": "Cleaning Interval Graphs", 
    "arxiv-id": "1003.1260v1", 
    "author": "Ildik\u00f3 Schlotter", 
    "publish": "2010-03-05T13:26:55Z", 
    "summary": "We investigate a special case of the Induced Subgraph Isomorphism problem,\nwhere both input graphs are interval graphs. We show the NP-hardness of this\nproblem, and we prove fixed-parameter tractability of the problem with\nnon-standard parameterization, where the parameter is the difference\n|V(G)|-|V(H)|, with G and H being the larger and the smaller input graph,\nrespectively. Intuitively, we can interpret this problem as \"cleaning\" the\ngraph G, regarded as a pattern containing extra vertices indicating errors, in\norder to obtain the graph H representing the original pattern. We also prove\nW[1]-hardness for the standard parameterization where the parameter is |V(H)|."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_27", 
    "link": "http://arxiv.org/pdf/1003.1507v1", 
    "title": "On Column-restricted and Priority Covering Integer Programs", 
    "arxiv-id": "1003.1507v1", 
    "author": "Jochen Koenemann", 
    "publish": "2010-03-07T18:08:12Z", 
    "summary": "In a column-restricted covering integer program (CCIP), all the non-zero\nentries of any column of the constraint matrix are equal. Such programs capture\ncapacitated versions of covering problems. In this paper, we study the\napproximability of CCIPs, in particular, their relation to the integrality gaps\nof the underlying 0,1-CIP.\n  If the underlying 0,1-CIP has an integrality gap O(gamma), and assuming that\nthe integrality gap of the priority version of the 0,1-CIP is O(omega), we give\na factor O(gamma + omega) approximation algorithm for the CCIP. Priority\nversions of 0,1-CIPs (PCIPs) naturally capture quality of service type\nconstraints in a covering problem.\n  We investigate priority versions of the line (PLC) and the (rooted) tree\ncover (PTC) problems. Apart from being natural objects to study, these problems\nfall in a class of fundamental geometric covering problems. We bound the\nintegrality of certain classes of this PCIP by a constant. Algorithmically, we\ngive a polytime exact algorithm for PLC, show that the PTC problem is APX-hard,\nand give a factor 2-approximation algorithm for it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_27", 
    "link": "http://arxiv.org/pdf/1003.2958v3", 
    "title": "Approaching optimality for solving SDD systems", 
    "arxiv-id": "1003.2958v3", 
    "author": "Richard Peng", 
    "publish": "2010-03-15T16:37:51Z", 
    "summary": "We present an algorithm that on input of an $n$-vertex $m$-edge weighted\ngraph $G$ and a value $k$, produces an {\\em incremental sparsifier} $\\hat{G}$\nwith $n-1 + m/k$ edges, such that the condition number of $G$ with $\\hat{G}$ is\nbounded above by $\\tilde{O}(k\\log^2 n)$, with probability $1-p$. The algorithm\nruns in time\n  $$\\tilde{O}((m \\log{n} + n\\log^2{n})\\log(1/p)).$$\n  As a result, we obtain an algorithm that on input of an $n\\times n$ symmetric\ndiagonally dominant matrix $A$ with $m$ non-zero entries and a vector $b$,\ncomputes a vector ${x}$ satisfying $||{x}-A^{+}b||_A<\\epsilon ||A^{+}b||_A $,\nin expected time\n  $$\\tilde{O}(m\\log^2{n}\\log(1/\\epsilon)).$$\n  The solver is based on repeated applications of the incremental sparsifier\nthat produces a chain of graphs which is then used as input to a recursive\npreconditioned Chebyshev iteration."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_9", 
    "link": "http://arxiv.org/pdf/1003.2977v2", 
    "title": "On Generalizations of Network Design Problems with Degree Bounds", 
    "arxiv-id": "1003.2977v2", 
    "author": "Britta Peis", 
    "publish": "2010-03-15T17:49:56Z", 
    "summary": "Iterative rounding and relaxation have arguably become the method of choice\nin dealing with unconstrained and constrained network design problems. In this\npaper we extend the scope of the iterative relaxation method in two directions:\n(1) by handling more complex degree constraints in the minimum spanning tree\nproblem (namely, laminar crossing spanning tree), and (2) by incorporating\n`degree bounds' in other combinatorial optimization problems such as matroid\nintersection and lattice polyhedra. We give new or improved approximation\nalgorithms, hardness results, and integrality gaps for these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_9", 
    "link": "http://arxiv.org/pdf/1003.3418v1", 
    "title": "Exponential Lower Bounds For Policy Iteration", 
    "arxiv-id": "1003.3418v1", 
    "author": "John Fearnley", 
    "publish": "2010-03-17T17:48:58Z", 
    "summary": "We study policy iteration for infinite-horizon Markov decision processes. It\nhas recently been shown policy iteration style algorithms have exponential\nlower bounds in a two player game setting. We extend these lower bounds to\nMarkov decision processes with the total reward and average-reward optimality\ncriteria."
},{
    "category": "cs.DS", 
    "doi": "10.1162/EVCO_a_00026", 
    "link": "http://arxiv.org/pdf/1003.4314v1", 
    "title": "A New Approach to Population Sizing for Memetic Algorithms: A Case Study   for the Multidimensional Assignment Problem", 
    "arxiv-id": "1003.4314v1", 
    "author": "Gregory Gutin", 
    "publish": "2010-03-23T00:27:13Z", 
    "summary": "Memetic Algorithms are known to be a powerful technique in solving hard\noptimization problems. To design a memetic algorithm one needs to make a host\nof decisions; selecting a population size is one of the most important among\nthem. Most algorithms in the literature fix the population size to a certain\nconstant value. This reduces the algorithm's quality since the optimal\npopulation size varies for different instances, local search procedures and\nrunning times. In this paper we propose an adjustable population size. It is\ncalculated as a function of the running time of the whole algorithm and the\naverage running time of the local search for the given instance. Note that in\nmany applications the running time of a heuristic should be limited and\ntherefore we use this limit as a parameter of the algorithm. The average\nrunning time of the local search procedure is obtained during the algorithm's\nrun. Some coefficients which are independent with respect to the instance or\nthe local search are to be tuned before the algorithm run; we provide a\nprocedure to find these coefficients. The proposed approach was used to develop\na memetic algorithm for the Multidimensional Assignment Problem (MAP or s-AP in\nthe case of s dimensions) which is an extension of the well-known assignment\nproblem. MAP is NP-hard and has a host of applications. We show that using\nadjustable population size makes the algorithm flexible to perform well for\ninstances of very different sizes and types and for different running times and\nlocal searches. This allows us to select the most efficient local search for\nevery instance type. The results of computational experiments for several\ninstance families and sizes prove that the proposed algorithm performs\nefficiently for a wide range of the running times and clearly outperforms the\nstate-of-the art 3-AP memetic algorithm being given the same time."
},{
    "category": "cs.DS", 
    "doi": "10.1162/EVCO_a_00026", 
    "link": "http://arxiv.org/pdf/1003.4366v1", 
    "title": "Graph Iterators: Decoupling Graph Structures from Algorithms", 
    "arxiv-id": "1003.4366v1", 
    "author": "Marco Nissen", 
    "publish": "2010-03-23T10:23:07Z", 
    "summary": "I will present a way to implement graph algorithms which is different from\ntraditional methods. This work was motivated by the belief that some ideas from\nsoftware engineering should be applied to graph algorithms. Re-usability of\nsoftware is an important and difficult problem in general, and this is\nparticularly true for graph algorithms. The scientific literature demonstrates\nplenty of applications of graph algorithms as subroutines for other algorithms.\nMoreover, many practical problems from various domains may be modeled as graph\nproblems and hence solved by means of graph algorithms. Chapter 2 introduces\nsome data structures that will be used in 5 basic graph algorithms in chapter\n3. Chapter 4 discusses an implementation of a maximum cardinality matching\nalgorithm for general graphs. Chapter 5 explains some techniques in C++, which\nare useful to implement the data structures and algorithms in an efficient way.\nFinally chapter 6 contains some concluding remarks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5330v2", 
    "title": "Lin-Kernighan Heuristic Adaptations for the Generalized Traveling   Salesman Problem", 
    "arxiv-id": "1003.5330v2", 
    "author": "Gregory Gutin", 
    "publish": "2010-03-27T22:46:05Z", 
    "summary": "The Lin-Kernighan heuristic is known to be one of the most successful\nheuristics for the Traveling Salesman Problem (TSP). It has also proven its\nefficiency in application to some other problems. In this paper we discuss\npossible adaptations of TSP heuristics for the Generalized Traveling Salesman\nProblem (GTSP) and focus on the case of the Lin-Kernighan algorithm. At first,\nwe provide an easy-to-understand description of the original Lin-Kernighan\nheuristic. Then we propose several adaptations, both trivial and complicated.\nFinally, we conduct a fair competition between all the variations of the\nLin-Kernighan adaptation and some other GTSP heuristics. It appears that our\nadaptation of the Lin-Kernighan algorithm for the GTSP reproduces the success\nof the original heuristic. Different variations of our adaptation outperform\nall other heuristics in a wide range of trade-offs between solution quality and\nrunning time, making Lin-Kernighan the state-of-the-art GTSP local search."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5474v2", 
    "title": "Angle Tree: Nearest Neighbor Search in High Dimensions with Low   Intrinsic Dimensionality", 
    "arxiv-id": "1003.5474v2", 
    "author": "Sanjay Chawla", 
    "publish": "2010-03-29T09:24:31Z", 
    "summary": "We propose an extension of tree-based space-partitioning indexing structures\nfor data with low intrinsic dimensionality embedded in a high dimensional\nspace. We call this extension an Angle Tree. Our extension can be applied to\nboth classical kd-trees as well as the more recent rp-trees. The key idea of\nour approach is to store the angle (the \"dihedral angle\") between the data\nregion (which is a low dimensional manifold) and the random hyperplane that\nsplits the region (the \"splitter\"). We show that the dihedral angle can be used\nto obtain a tight lower bound on the distance between the query point and any\npoint on the opposite side of the splitter. This in turn can be used to\nefficiently prune the search space. We introduce a novel randomized strategy to\nefficiently calculate the dihedral angle with a high degree of accuracy.\nExperiments and analysis on real and synthetic data sets shows that the Angle\nTree is the most efficient known indexing structure for nearest neighbor\nqueries in terms of preprocessing and space usage while achieving high accuracy\nand fast search time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5907v2", 
    "title": "Faster Approximation Schemes for Fractional Multicommodity Flow Problems   via Dynamic Graph Algorithms", 
    "arxiv-id": "1003.5907v2", 
    "author": "Aleksander Madry", 
    "publish": "2010-03-30T19:53:12Z", 
    "summary": "We combine the work of Garg and Konemann, and Fleischer with ideas from\ndynamic graph algorithms to obtain faster (1-eps)-approximation schemes for\nvarious versions of the multicommodity flow problem. In particular, if eps is\nmoderately small and the size of every number used in the input instance is\npolynomially bounded, the running times of our algorithms match - up to\npoly-logarithmic factors and some provably optimal terms - the Omega(mn)\nflow-decomposition barrier for single-commodity flow."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0351v1", 
    "title": "An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems", 
    "arxiv-id": "1004.0351v1", 
    "author": "S. S. Iyengar", 
    "publish": "2010-04-02T15:51:33Z", 
    "summary": "We consider the problem of constructing a single spanning tree for the\nsingle-source buy-at-bulk network design problem for doubling-dimension graphs.\nWe compute a spanning tree to route a set of demands (or data) along a graph to\nor from a designated root node. The demands could be aggregated at (or\nsymmetrically distributed to) intermediate nodes where the fusion-cost is\nspecified by a non-negative concave function $f$. We describe a novel approach\nfor developing an oblivious spanning tree in the sense that it is independent\nof the number of data sources (or demands) and cost function at intermediate\nnodes. To our knowledge, this is the first paper to propose a single spanning\ntree solution to this problem (as opposed to multiple overlay trees). There has\nbeen no prior work where the tree is oblivious to both the fusion cost function\nand the set of sources (demands). We present a deterministic, polynomial-time\nalgorithm for constructing a spanning tree in low doubling graphs that\nguarantees $\\log^{3}D\\cdot\\log n$-approximation over the optimal cost, where\n$D$ is the diameter of the graph and $n$ the total number of nodes. With\nconstant fusion-cost function our spanning tree gives a $O(\\log^3\nD)$-approximation for every Steiner tree to the root."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0403v1", 
    "title": "CONCISE: Compressed 'n' Composable Integer Set", 
    "arxiv-id": "1004.0403v1", 
    "author": "Roberto Di Pietro", 
    "publish": "2010-04-03T00:07:00Z", 
    "summary": "Bit arrays, or bitmaps, are used to significantly speed up set operations in\nseveral areas, such as data warehousing, information retrieval, and data\nmining, to cite a few. However, bitmaps usually use a large storage space, thus\nrequiring compression. Nevertheless, there is a space-time tradeoff among\ncompression schemes. The Word Aligned Hybrid (WAH) bitmap compression trades\nsome space to allow for bitwise operations without first decompressing bitmaps.\nWAH has been recognized as the most efficient scheme in terms of computation\ntime. In this paper we present CONCISE (Compressed 'n' Composable Integer Set),\na new scheme that enjoys significatively better performances than those of WAH.\nIn particular, when compared to WAH, our algorithm is able to reduce the\nrequired memory up to 50%, by having similar or better performance in terms of\ncomputation time. Further, we show that CONCISE can be efficiently used to\nmanipulate bitmaps representing sets of integral numbers in lieu of well-known\ndata structures such as arrays, lists, hashtables, and self-balancing binary\nsearch trees. Extensive experiments over synthetic data show the effectiveness\nof our approach."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0424v2", 
    "title": "Restricted Common Superstring and Restricted Common Supersequence", 
    "arxiv-id": "1004.0424v2", 
    "author": "Alexandru Popa", 
    "publish": "2010-04-03T07:14:53Z", 
    "summary": "The {\\em shortest common superstring} and the {\\em shortest common\nsupersequence} are two well studied problems having a wide range of\napplications. In this paper we consider both problems with resource\nconstraints, denoted as the Restricted Common Superstring (shortly\n\\textit{RCSstr}) problem and the Restricted Common Supersequence (shortly\n\\textit{RCSseq}). In the \\textit{RCSstr} (\\textit{RCSseq}) problem we are given\na set $S$ of $n$ strings, $s_1$, $s_2$, $\\ldots$, $s_n$, and a multiset $t =\n\\{t_1, t_2, \\dots, t_m\\}$, and the goal is to find a permutation $\\pi : \\{1,\n\\dots, m\\} \\to \\{1, \\dots, m\\}$ to maximize the number of strings in $S$ that\nare substrings (subsequences) of $\\pi(t) = t_{\\pi(1)}t_{\\pi(2)}...t_{\\pi(m)}$\n(we call this ordering of the multiset, $\\pi(t)$, a permutation of $t$). We\nfirst show that in its most general setting the \\textit{RCSstr} problem is {\\em\nNP-complete} and hard to approximate within a factor of $n^{1-\\epsilon}$, for\nany $\\epsilon > 0$, unless P = NP. Afterwards, we present two separate\nreductions to show that the \\textit{RCSstr} problem remains NP-Hard even in the\ncase where the elements of $t$ are drawn from a binary alphabet or for the case\nwhere all input strings are of length two. We then present some approximation\nresults for several variants of the \\textit{RCSstr} problem. In the second part\nof this paper, we turn to the \\textit{RCSseq} problem, where we present some\nhardness results, tight lower bounds and approximation algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0995v1", 
    "title": "Strong Fault-Tolerance for Self-Assembly with Fuzzy Temperature", 
    "arxiv-id": "1004.0995v1", 
    "author": "Scott M. Summers", 
    "publish": "2010-04-07T02:21:21Z", 
    "summary": "We consider the problem of fault-tolerance in nanoscale algorithmic\nself-assembly. We employ a variant of Winfree's abstract Tile Assembly Model\n(aTAM), the two-handed aTAM, in which square \"tiles\" -- a model of molecules\nconstructed from DNA for the purpose of engineering self-assembled\nnanostructures -- aggregate according to specific binding sites of varying\nstrengths, and in which large aggregations of tiles may attach to each other,\nin contrast to the seeded aTAM, in which tiles aggregate one at a time to a\nsingle specially-designated \"seed\" assembly. We focus on a major cause of\nerrors in tile-based self-assembly: that of unintended growth due to \"weak\"\nstrength-1 bonds, which if allowed to persist, may be stabilized by subsequent\nattachment of neighboring tiles in the sense that at least energy 2 is now\nrequired to break apart the resulting assembly; i.e., the errant assembly is\nstable at temperature 2. We study a common self-assembly benchmark problem,\nthat of assembling an n x n square using O(log n) unique tile types, under the\ntwo-handed model of self-assembly. Our main result achieves a much stronger\nnotion of fault-tolerance than those achieved previously. Arbitrary strength-1\ngrowth is allowed (i.e., the temperature is \"fuzzy\" and may drift from 2 to 1\nfor arbitrarily long); however, any assembly that grows sufficiently to become\nstable at temperature 2 is guaranteed to assemble at temperature 2 into the\ncorrect final assembly of an n x n square. In other words, errors due to\ninsufficient attachment, which is the cause of errors studied in earlier papers\non fault-tolerance, are prevented absolutely in our main construction, rather\nthan only with high probability and for sufficiently small structures, as in\nprevious fault-tolerance studies."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.1194v1", 
    "title": "Unified Compression-Based Acceleration of Edit-Distance Computation", 
    "arxiv-id": "1004.1194v1", 
    "author": "Oren Weimann", 
    "publish": "2010-04-07T21:24:27Z", 
    "summary": "The edit distance problem is a classical fundamental problem in computer\nscience in general, and in combinatorial pattern matching in particular. The\nstandard dynamic programming solution for this problem computes the\nedit-distance between a pair of strings of total length O(N) in O(N^2) time. To\nthis date, this quadratic upper-bound has never been substantially improved for\ngeneral strings. However, there are known techniques for breaking this bound in\ncase the strings are known to compress well under a particular compression\nscheme. The basic idea is to first compress the strings, and then to compute\nthe edit distance between the compressed strings. As it turns out, practically\nall known o(N^2) edit-distance algorithms work, in some sense, under the same\nparadigm described above. It is therefore natural to ask whether there is a\nsingle edit-distance algorithm that works for strings which are compressed\nunder any compression scheme. A rephrasing of this question is to ask whether a\nsingle algorithm can exploit the compressibility properties of strings under\nany compression method, even if each string is compressed using a different\ncompression. In this paper we set out to answer this question by using straight\nline programs. These provide a generic platform for representing many popular\ncompression schemes including the LZ-family, Run-Length Encoding, Byte-Pair\nEncoding, and dictionary methods. For two strings of total length N having\nstraight-line program representations of total size n, we present an algorithm\nrunning in O(nN log(N/n)) time for computing the edit-distance of these two\nstrings under any rational scoring function, and an O(n^{2/3}N^{4/3}) time\nalgorithm for arbitrary scoring functions. Our new result, while providing a\nsigni cant speed up for highly compressible strings, does not surpass the\nquadratic time bound even in the worst case scenario."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.1208v1", 
    "title": "A Deterministic Algorithm for the Vertex Connectivity Survivable Network   Design Problem", 
    "arxiv-id": "1004.1208v1", 
    "author": "Pushkar Tripathi", 
    "publish": "2010-04-07T23:29:43Z", 
    "summary": "In the vertex connectivity survivable network design problem we are given an\nundirected graph G = (V,E) and connectivity requirement r(u,v) for each pair of\nvertices u,v. We are also given a cost function on the set of edges. Our goal\nis to find the minimum cost subset of edges such that for every pair (u,v) of\nvertices we have r(u,v) vertex disjoint paths in the graph induced by the\nchosen edges. Recently, Chuzhoy and Khanna presented a randomized algorithm\nthat achieves a factor of O(k^3 log n) for this problem where k is the maximum\nconnectivity requirement. In this paper we derandomize their algorithm to get a\ndeterministic O(k^3 log n) factor algorithm. Another problem of interest is the\nsingle source version of the problem, where there is a special vertex s and all\nnon-zero connectivity requirements must involve s. We also give a deterministic\nO(k^2 log n) algorithm for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1672v2", 
    "title": "On Feedback Vertex Set: New Measure and New Structures", 
    "arxiv-id": "1004.1672v2", 
    "author": "Yang Liu", 
    "publish": "2010-04-10T03:05:53Z", 
    "summary": "We present a new parameterized algorithm for the {feedback vertex set}\nproblem ({\\sc fvs}) on undirected graphs. We approach the problem by\nconsidering a variation of it, the {disjoint feedback vertex set} problem ({\\sc\ndisjoint-fvs}), which finds a feedback vertex set of size $k$ that has no\noverlap with a given feedback vertex set $F$ of the graph $G$. We develop an\nimproved kernelization algorithm for {\\sc disjoint-fvs} and show that {\\sc\ndisjoint-fvs} can be solved in polynomial time when all vertices in $G\n\\setminus F$ have degrees upper bounded by three. We then propose a new\nbranch-and-search process on {\\sc disjoint-fvs}, and introduce a new\nbranch-and-search measure. The process effectively reduces a given graph to a\ngraph on which {\\sc disjoint-fvs} becomes polynomial-time solvable, and the new\nmeasure more accurately evaluates the efficiency of the process. These\nalgorithmic and combinatorial studies enable us to develop an\n$O^*(3.83^k)$-time parameterized algorithm for the general {\\sc fvs} problem,\nimproving all previous algorithms for the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1808v6", 
    "title": "Polynomial Time Algorithm for Graph Isomorphism Testing", 
    "arxiv-id": "1004.1808v6", 
    "author": "Michael I. Trofimov", 
    "publish": "2010-04-11T13:41:39Z", 
    "summary": "This article deals with new polynomial time algorithm for graph isomorphism\ntesting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1823v1", 
    "title": "Clustering with Spectral Norm and the k-means Algorithm", 
    "arxiv-id": "1004.1823v1", 
    "author": "Ravindran Kannan", 
    "publish": "2010-04-11T17:36:33Z", 
    "summary": "There has been much progress on efficient algorithms for clustering data\npoints generated by a mixture of $k$ probability distributions under the\nassumption that the means of the distributions are well-separated, i.e., the\ndistance between the means of any two distributions is at least $\\Omega(k)$\nstandard deviations. These results generally make heavy use of the generative\nmodel and particular properties of the distributions. In this paper, we show\nthat a simple clustering algorithm works without assuming any generative\n(probabilistic) model. Our only assumption is what we call a \"proximity\ncondition\": the projection of any data point onto the line joining its cluster\ncenter to any other cluster center is $\\Omega(k)$ standard deviations closer to\nits own center than the other center. Here the notion of standard deviations is\nbased on the spectral norm of the matrix whose rows represent the difference\nbetween a point and the mean of the cluster to which it belongs. We show that\nin the generative models studied, our proximity condition is satisfied and so\nwe are able to derive most known results for generative models as corollaries\nof our main result. We also prove some new results for generative models -\ne.g., we can cluster all but a small fraction of points only assuming a bound\non the variance. Our algorithm relies on the well known $k$-means algorithm,\nand along the way, we prove a result of independent interest -- that the\n$k$-means algorithm converges to the \"true centers\" even in the presence of\nspurious points provided the initial (estimated) centers are close enough to\nthe corresponding actual centers and all but a small fraction of the points\nsatisfy the proximity condition. Finally, we present a new technique for\nboosting the ratio of inter-center separation to standard deviation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9505-6", 
    "link": "http://arxiv.org/pdf/1004.2033v1", 
    "title": "Feasibility Analysis of Sporadic Real-Time Multiprocessor Task Systems", 
    "arxiv-id": "1004.2033v1", 
    "author": "Alberto Marchetti-Spaccamela", 
    "publish": "2010-04-12T19:43:20Z", 
    "summary": "We give the first algorithm for testing the feasibility of a system of\nsporadic real-time tasks on a set of identical processors, solving one major\nopen problem in the area of multiprocessor real-time scheduling [S.K. Baruah\nand K. Pruhs, Journal of Scheduling, 2009]. We also investigate the related\nnotion of schedulability and a notion that we call online feasibility. Finally,\nwe show that discrete-time schedules are as powerful as continuous-time\nschedules, which answers another open question in the above mentioned survey."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.2393v3", 
    "title": "On the Continuous CNN Problem", 
    "arxiv-id": "1004.2393v3", 
    "author": "Nick Gravin", 
    "publish": "2010-04-14T13:34:03Z", 
    "summary": "In the (discrete) CNN problem, online requests appear as points in\n$\\mathbb{R}^2$. Each request must be served before the next one is revealed. We\nhave a server that can serve a request simply by aligning either its $x$ or $y$\ncoordinate with the request. The goal of the online algorithm is to minimize\nthe total $L_1$ distance traveled by the server to serve all the requests. The\nbest known competitive ratio for the discrete version is 879 (due to Sitters\nand Stougie).\n  We study the continuous version, in which, the request can move continuously\nin $\\mathbb{R}^2$ and the server must continuously serve the request. A simple\nadversarial argument shows that the lower bound on the competitive ratio of any\nonline algorithm for the continuous CNN problem is 3. Our main contribution is\nan online algorithm with competitive ratio $3+2 \\sqrt{3} \\approx 6.464$. Our\nanalysis is tight. The continuous version generalizes the discrete orthogonal\nCNN problem, in which every request must be $x$ or $y$ aligned with the\nprevious request. Therefore, Our result improves upon the previous best\ncompetitive ratio of 9 (due to Iwama and Yonezawa)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.2968v2", 
    "title": "Clustering with diversity", 
    "arxiv-id": "1004.2968v2", 
    "author": "Qin Zhang", 
    "publish": "2010-04-17T16:30:08Z", 
    "summary": "We consider the {\\em clustering with diversity} problem: given a set of\ncolored points in a metric space, partition them into clusters such that each\ncluster has at least $\\ell$ points, all of which have distinct colors.\n  We give a 2-approximation to this problem for any $\\ell$ when the objective\nis to minimize the maximum radius of any cluster. We show that the\napproximation ratio is optimal unless $\\mathbf{P=NP}$, by providing a matching\nlower bound. Several extensions to our algorithm have also been developed for\nhandling outliers. This problem is mainly motivated by applications in\nprivacy-preserving data publication."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3205v2", 
    "title": "Differential Privacy and the Fat-Shattering Dimension of Linear Queries", 
    "arxiv-id": "1004.3205v2", 
    "author": "Aaron Roth", 
    "publish": "2010-04-19T14:19:56Z", 
    "summary": "In this paper, we consider the task of answering linear queries under the\nconstraint of differential privacy. This is a general and well-studied class of\nqueries that captures other commonly studied classes, including predicate\nqueries and histogram queries. We show that the accuracy to which a set of\nlinear queries can be answered is closely related to its fat-shattering\ndimension, a property that characterizes the learnability of real-valued\nfunctions in the agnostic-learning setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3363v5", 
    "title": "Faster Algorithms for Semi-Matching Problems", 
    "arxiv-id": "1004.3363v5", 
    "author": "Danupon Nanongkai", 
    "publish": "2010-04-20T07:06:38Z", 
    "summary": "We consider the problem of finding \\textit{semi-matching} in bipartite graphs\nwhich is also extensively studied under various names in the scheduling\nliterature. We give faster algorithms for both weighted and unweighted case.\n  For the weighted case, we give an $O(nm\\log n)$-time algorithm, where $n$ is\nthe number of vertices and $m$ is the number of edges, by exploiting the\ngeometric structure of the problem. This improves the classical $O(n^3)$\nalgorithms by Horn [Operations Research 1973] and Bruno, Coffman and Sethi\n[Communications of the ACM 1974].\n  For the unweighted case, the bound could be improved even further. We give a\nsimple divide-and-conquer algorithm which runs in $O(\\sqrt{n}m\\log n)$ time,\nimproving two previous $O(nm)$-time algorithms by Abraham [MSc thesis,\nUniversity of Glasgow 2003] and Harvey, Ladner, Lov\\'asz and Tamir [WADS 2003\nand Journal of Algorithms 2006]. We also extend this algorithm to solve the\n\\textit{Balance Edge Cover} problem in $O(\\sqrt{n}m\\log n)$ time, improving the\nprevious $O(nm)$-time algorithm by Harada, Ono, Sadakane and Yamashita [ISAAC\n2008]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3702v30", 
    "title": "A Polynomial time Algorithm for Hamilton Cycle and Its detailed proof", 
    "arxiv-id": "1004.3702v30", 
    "author": "Lizhi Du", 
    "publish": "2010-04-12T04:39:27Z", 
    "summary": "Popular algorithms to find a Hamilton circle in an undirected graph are\ngenerally based on the Rotation-Extension method developed by Posa. However,\ndue to the deficiencies of Posa's method, such algorithms are only efficient\nfor graphs that are either very dense or sparse but regular. This article\nintroduces a method called \"Enlarged Rotation-Extension\" that modifies and\nextends Posa's method, overcoming its deficiencies. Based on this technique,\nour algorithm is polynomial and we give a detailed proof for it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4024v1", 
    "title": "n-Level Graph Partitioning", 
    "arxiv-id": "1004.4024v1", 
    "author": "Peter Sanders", 
    "publish": "2010-04-22T22:59:53Z", 
    "summary": "We present a multi-level graph partitioning algorithm based on the extreme\nidea to contract only a single edge on each level of the hierarchy. This\nobviates the need for a matching algorithm and promises very good partitioning\nquality since there are very few changes between two levels. Using an efficient\ndata structure and new flexible ways to break local search improvements early,\nwe obtain an algorithm that scales to large inputs and produces the best known\npartitioning results for many inputs. For example, in Walshaw's well known\nbenchmark tables we achieve 155 improvements dominating the entries for large\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4057v1", 
    "title": "Efficient volume sampling for row/column subset selection", 
    "arxiv-id": "1004.4057v1", 
    "author": "Luis Rademacher", 
    "publish": "2010-04-23T06:53:25Z", 
    "summary": "We give efficient algorithms for volume sampling, i.e., for picking\n$k$-subsets of the rows of any given matrix with probabilities proportional to\nthe squared volumes of the simplices defined by them and the origin (or the\nsquared volumes of the parallelepipeds defined by these subsets of rows). This\nsolves an open problem from the monograph on spectral algorithms by Kannan and\nVempala. Our first algorithm for volume sampling $k$-subsets of rows from an\n$m$-by-$n$ matrix runs in $O(kmn^{\\omega} \\log n)$ arithmetic operations and a\nsecond variant of it for $(1+\\epsilon)$-approximate volume sampling runs in\n$O(mn \\log m \\cdot k^{2}/\\epsilon^{2} + m \\log^{\\omega} m \\cdot\nk^{2\\omega+1}/\\epsilon^{2\\omega} \\cdot \\log(k \\epsilon^{-1} \\log m))$\narithmetic operations, which is almost linear in the size of the input (i.e.,\nthe number of entries) for small $k$. Our efficient volume sampling algorithms\nimply several interesting results for low-rank matrix approximation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4080v1", 
    "title": "A General Framework for Graph Sparsification", 
    "arxiv-id": "1004.4080v1", 
    "author": "Debmalya Panigrahi", 
    "publish": "2010-04-23T09:36:13Z", 
    "summary": "Given a weighted graph $G$ and an error parameter $\\epsilon > 0$, the {\\em\ngraph sparsification} problem requires sampling edges in $G$ and giving the\nsampled edges appropriate weights to obtain a sparse graph $G_{\\epsilon}$\n(containing O(n\\log n) edges in expectation) with the following property: the\nweight of every cut in $G_{\\epsilon}$ is within a factor of $(1\\pm \\epsilon)$\nof the weight of the corresponding cut in $G$. We provide a generic framework\nthat sets out sufficient conditions for any particular sampling scheme to\nresult in good sparsifiers, and obtain a set of results by simple\ninstantiations of this framework. The results we obtain include the following:\n(1) We improve the time complexity of graph sparsification from O(m\\log^3 n) to\nO(m + n\\log^4 n) for graphs with polynomial edge weights. (2) We improve the\ntime complexity of graph sparsification from O(m\\log^3 n) to O(m\\log^2 n) for\ngraphs with arbitrary edge weights. (3) If the size of the sparsifier is\nallowed to be O(n\\log^2 n/\\epsilon^2) instead of O(n\\log n/\\epsilon^2), we\nimprove the time complexity of sparsification to O(m) for graphs with\npolynomial edge weights. (4) We show that sampling using standard\nconnectivities results in good sparsifiers, thus resolving an open question of\nBenczur and Karger. As a corollary, we give a simple proof of (a slightly\nweaker version of) a result due to Spielman and Srivastava showing that\nsampling using effective resistances produces good sparsifiers. (5) We give a\nsimple proof showing that sampling using strong connectivities results in good\nsparsifiers, a result obtained previously using a more involved proof by\nBenczur and Karger. A key ingredient of our proofs is a generalization of\nbounds on the number of small cuts in an undirected graph due to Karger; this\ngeneralization might be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4240v1", 
    "title": "A Sparse Johnson--Lindenstrauss Transform", 
    "arxiv-id": "1004.4240v1", 
    "author": "Tam\u00e1s Sarl\u00f3s", 
    "publish": "2010-04-23T23:57:17Z", 
    "summary": "Dimension reduction is a key algorithmic tool with many applications\nincluding nearest-neighbor search, compressed sensing and linear algebra in the\nstreaming model. In this work we obtain a {\\em sparse} version of the\nfundamental tool in dimension reduction --- the Johnson--Lindenstrauss\ntransform. Using hashing and local densification, we construct a sparse\nprojection matrix with just $\\tilde{O}(\\frac{1}{\\epsilon})$ non-zero entries\nper column. We also show a matching lower bound on the sparsity for a large\nclass of projection matrices. Our bounds are somewhat surprising, given the\nknown lower bounds of $\\Omega(\\frac{1}{\\epsilon^2})$ both on the number of rows\nof any projection matrix and on the sparsity of projection matrices generated\nby natural constructions.\n  Using this, we achieve an $\\tilde{O}(\\frac{1}{\\epsilon})$ update time per\nnon-zero element for a $(1\\pm\\epsilon)$-approximate projection, thereby\nsubstantially outperforming the $\\tilde{O}(\\frac{1}{\\epsilon^2})$ update time\nrequired by prior approaches. A variant of our method offers the same\nguarantees for sparse vectors, yet its $\\tilde{O}(d)$ worst case running time\nmatches the best approach of Ailon and Liberty."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.4420v1", 
    "title": "Optimal Data Placement on Networks With Constant Number of Clients", 
    "arxiv-id": "1004.4420v1", 
    "author": "Vassilis Zissimopoulos", 
    "publish": "2010-04-26T07:41:35Z", 
    "summary": "We introduce optimal algorithms for the problems of data placement (DP) and\npage placement (PP) in networks with a constant number of clients each of which\nhas limited storage availability and issues requests for data objects. The\nobjective for both problems is to efficiently utilize each client's storage\n(deciding where to place replicas of objects) so that the total incurred access\nand installation cost over all clients is minimized. In the PP problem an extra\nconstraint on the maximum number of clients served by a single client must be\nsatisfied. Our algorithms solve both problems optimally when all objects have\nuniform lengths. When objects lengths are non-uniform we also find the optimal\nsolution, albeit a small, asymptotically tight violation of each client's\nstorage size by $\\epsilon$lmax where lmax is the maximum length of the objects\nand $\\epsilon$ some arbitrarily small positive constant. We make no assumption\non the underlying topology of the network (metric, ultrametric etc.), thus\nobtaining the first non-trivial results for non-metric data placement problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.4915v1", 
    "title": "Graph Sparsification via Refinement Sampling", 
    "arxiv-id": "1004.4915v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2010-04-27T21:05:00Z", 
    "summary": "A graph G'(V,E') is an \\eps-sparsification of G for some \\eps>0, if every\n(weighted) cut in G' is within (1\\pm \\eps) of the corresponding cut in G. A\ncelebrated result of Benczur and Karger shows that for every undirected graph\nG, an \\eps-sparsification with O(n\\log n/\\e^2) edges can be constructed in\nO(m\\log^2n) time. Applications to modern massive data sets often constrain\nalgorithms to use computation models that restrict random access to the input.\nThe semi-streaming model, in which the algorithm is constrained to use \\tilde\nO(n) space, has been shown to be a good abstraction for analyzing graph\nalgorithms in applications to large data sets. Recently, a semi-streaming\nalgorithm for graph sparsification was presented by Anh and Guha; the total\nrunning time of their implementation is \\Omega(mn), too large for applications\nwhere both space and time are important. In this paper, we introduce a new\ntechnique for graph sparsification, namely refinement sampling, that gives an\n\\tilde{O}(m) time semi-streaming algorithm for graph sparsification.\n  Specifically, we show that refinement sampling can be used to design a\none-pass streaming algorithm for sparsification that takes O(\\log\\log n) time\nper edge, uses O(\\log^2 n) space per node, and outputs an \\eps-sparsifier with\nO(n\\log^3 n/\\eps^2) edges. At a slightly increased space and time complexity,\nwe can reduce the sparsifier size to O(n \\log n/\\e^2) edges matching the\nBenczur-Karger result, while improving upon the Benczur-Karger runtime for\nm=\\omega(n\\log^3 n). Finally, we show that an \\eps-sparsifier with O(n \\log\nn/\\eps^2) edges can be constructed in two passes over the data and O(m) time\nwhenever m =\\Omega(n^{1+\\delta}) for some constant \\delta>0. As a by-product of\nour approach, we also obtain an O(m\\log\\log n+n \\log n) time streaming\nalgorithm to compute a sparse k-connectivity certificate of a graph."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5010v2", 
    "title": "The stubborn problem is stubborn no more (a polynomial algorithm for   3-compatible colouring and the stubborn list partition problem)", 
    "arxiv-id": "1004.5010v2", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2010-04-28T12:20:52Z", 
    "summary": "One of the driving problems in the CSP area is the Dichotomy Conjecture,\nformulated in 1993 by Feder and Vardi [STOC'93], stating that for any fixed\nrelational structure G the Constraint Satisfaction Problem CSP(G) is either\nNP--complete or polynomial time solvable. A large amount of research has gone\ninto checking various specific cases of this conjecture. One such variant which\nattracted a lot of attention in the recent years is the LIST MATRIX PARTITION\nproblem. In 2004 Cameron et al. [SODA'04] classified almost all LIST MATRIX\nPARTITION variants for matrices of size at most four. The only case which\nresisted the classification became known as the STUBBORN PROBLEM. In this paper\nwe show a result which enables us to finish the classification - thus solving a\nproblem which resisted attacks for the last six years.\n  Our approach is based on a combinatorial problem known to be at least as hard\nas the STUBBORN PROBLEM - the 3-COMPATIBLE COLOURING problem. In this problem\nwe are given a complete graph with each edge assigned one of 3 possible colours\nand we want to assign one of those 3 colours to each vertex in such a way that\nno edge has the same colour as both of its endpoints. The tractability of the\n3-COMPATIBLE COLOURING problem has been open for several years and the best\nknown algorithm prior to this paper is due to Feder et al. [SODA'05] - a\nquasipolynomial algorithm with a n^O(log n / log log n) time complexity. In\nthis paper we present a polynomial-time algorithm for the 3-COMPATIBLE\nCOLOURING problem and consequently we prove a dichotomy for the k-COMPATIBLE\nCOLOURING problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5012v1", 
    "title": "Bandwidth and Distortion Revisited", 
    "arxiv-id": "1004.5012v1", 
    "author": "Marcin Pilipczuk", 
    "publish": "2010-04-28T12:29:28Z", 
    "summary": "In this paper we merge recent developments on exact algorithms for finding an\nordering of vertices of a given graph that minimizes bandwidth (the BANDWIDTH\nproblem) and for finding an embedding of a given graph into a line that\nminimizes distortion (the DISTORTION problem). For both problems we develop\nalgorithms that work in O(9.363^n) time and polynomial space. For BANDWIDTH,\nthis improves O^*(10^n) algorithm by Feige and Kilian from 2000, for DISTORTION\nthis is the first polynomial space exact algorithm that works in O(c^n) time we\nare aware of. As a byproduct, we enhance the O(5^{n+o(n)})-time and\nO^*(2^n)-space algorithm for DISTORTION by Fomin et al. to an algorithm working\nin O(4.383^n) time and space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5186v1", 
    "title": "Multiscale approach for the network compression-friendly ordering", 
    "arxiv-id": "1004.5186v1", 
    "author": "Boris Temkin", 
    "publish": "2010-04-29T05:31:40Z", 
    "summary": "We present a fast multiscale approach for the network minimum logarithmic\narrangement problem. This type of arrangement plays an important role in a\nnetwork compression and fast node/link access operations. The algorithm is of\nlinear complexity and exhibits good scalability which makes it practical and\nattractive for using on large-scale instances. Its effectiveness is\ndemonstrated on a large set of real-life networks. These networks with\ncorresponding best-known minimization results are suggested as an open\nbenchmark for a research community to evaluate new methods for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5600v1", 
    "title": "On the (Im)possibility of Preserving Utility and Privacy in Personalized   Social Recommendations", 
    "arxiv-id": "1004.5600v1", 
    "author": "Atish Das Sarma", 
    "publish": "2010-04-30T19:42:14Z", 
    "summary": "With the recent surge of social networks like Facebook, new forms of\nrecommendations have become possible -- personalized recommendations of ads,\ncontent, and even new social and product connections based on one's social\ninteractions. In this paper, we study whether \"social recommendations\", or\nrecommendations that utilize a user's social network, can be made without\ndisclosing sensitive links between users. More precisely, we quantify the loss\nin utility when existing recommendation algorithms are modified to satisfy a\nstrong notion of privacy called differential privacy. We propose lower bounds\non the minimum loss in utility for any recommendation algorithm that is\ndifferentially private. We also propose two recommendation algorithms that\nsatisfy differential privacy, analyze their performance in comparison to the\nlower bound, both analytically and experimentally, and show that good private\nsocial recommendations are feasible only for a few users in the social network\nor for a lenient setting of privacy parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.0239v1", 
    "title": "On Finding Frequent Patterns in Directed Acyclic Graphs", 
    "arxiv-id": "1005.0239v1", 
    "author": "Rasmus Pagh", 
    "publish": "2010-05-03T09:23:01Z", 
    "summary": "Given a directed acyclic graph with labeled vertices, we consider the problem\nof finding the most common label sequences (\"traces\") among all paths in the\ngraph (of some maximum length m). Since the number of paths can be huge, we\npropose novel algorithms whose time complexity depends only on the size of the\ngraph, and on the relative frequency epsilon of the most frequent traces. In\naddition, we apply techniques from streaming algorithms to achieve space usage\nthat depends only on epsilon, and not on the number of distinct traces. The\nabstract problem considered models a variety of tasks concerning finding\nfrequent patterns in event sequences. Our motivation comes from working with a\ndata set of 2 million RFID readings from baggage trolleys at Copenhagen\nAirport. The question of finding frequent passenger movement patterns is mapped\nto the above problem. We report on experimental findings for this data set."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.0352v1", 
    "title": "The Deletable Bloom filter: A new member of the Bloom family", 
    "arxiv-id": "1005.0352v1", 
    "author": "Mauricio F. Magalhaes", 
    "publish": "2010-05-03T17:20:29Z", 
    "summary": "We introduce the Deletable Bloom filter (DlBF) as a new spin on the popular\ndata structure based on compactly encoding the information of where collisions\nhappen when inserting elements. The DlBF design enables false-negative-free\ndeletions at a fraction of the cost in memory consumption, which turns to be\nappealing for certain probabilistic filter applications."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.0513v1", 
    "title": "Maximum flow is approximable by deterministic constant-time algorithm in   sparse networks", 
    "arxiv-id": "1005.0513v1", 
    "author": "Endre Cs\u00f3ka", 
    "publish": "2010-05-04T13:05:16Z", 
    "summary": "We show a deterministic constant-time parallel algorithm for finding an\nalmost maximum flow in multisource-multitarget networks with bounded degrees\nand bounded edge capacities. As a consequence, we show that the value of the\nmaximum flow over the number of nodes is a testable parameter on these\nnetworks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.0670v1", 
    "title": "A Linear-time Algorithm for Sparsification of Unweighted Graphs", 
    "arxiv-id": "1005.0670v1", 
    "author": "Debmalya Panigrahi", 
    "publish": "2010-05-05T04:50:29Z", 
    "summary": "Given an undirected graph $G$ and an error parameter $\\epsilon > 0$, the {\\em\ngraph sparsification} problem requires sampling edges in $G$ and giving the\nsampled edges appropriate weights to obtain a sparse graph $G_{\\epsilon}$ with\nthe following property: the weight of every cut in $G_{\\epsilon}$ is within a\nfactor of $(1\\pm \\epsilon)$ of the weight of the corresponding cut in $G$. If\n$G$ is unweighted, an $O(m\\log n)$-time algorithm for constructing\n$G_{\\epsilon}$ with $O(n\\log n/\\epsilon^2)$ edges in expectation, and an\n$O(m)$-time algorithm for constructing $G_{\\epsilon}$ with $O(n\\log^2\nn/\\epsilon^2)$ edges in expectation have recently been developed\n(Hariharan-Panigrahi, 2010). In this paper, we improve these results by giving\nan $O(m)$-time algorithm for constructing $G_{\\epsilon}$ with $O(n\\log\nn/\\epsilon^2)$ edges in expectation, for unweighted graphs. Our algorithm is\noptimal in terms of its time complexity; further, no efficient algorithm is\nknown for constructing a sparser $G_{\\epsilon}$. Our algorithm is Monte-Carlo,\ni.e. it produces the correct output with high probability, as are all efficient\ngraph sparsification algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.0809v1", 
    "title": "On Estimating the First Frequency Moment of Data Streams", 
    "arxiv-id": "1005.0809v1", 
    "author": "Purushottam Kar", 
    "publish": "2010-05-05T18:52:00Z", 
    "summary": "Estimating the first moment of a data stream defined as $F_1 = \\sum_{i \\in\n\\{1, 2, \\ldots, n\\}} \\abs{f_i}$ to within $1 \\pm \\epsilon$-relative error with\nhigh probability is a basic and influential problem in data stream processing.\nA tight space bound of $O(\\epsilon^{-2} \\log (mM))$ is known from the work of\n[Kane-Nelson-Woodruff-SODA10]. However, all known algorithms for this problem\nrequire per-update stream processing time of $\\Omega(\\epsilon^{-2})$, with the\nonly exception being the algorithm of [Ganguly-Cormode-RANDOM07] that requires\nper-update processing time of $O(\\log^2(mM)(\\log n))$ albeit with sub-optimal\nspace $O(\\epsilon^{-3}\\log^2(mM))$. In this paper, we present an algorithm for\nestimating $F_1$ that achieves near-optimality in both space and update\nprocessing time. The space requirement is $O(\\epsilon^{-2}(\\log n + (\\log\n\\epsilon^{-1})\\log(mM)))$ and the per-update processing time is $O( (\\log\nn)\\log (\\epsilon^{-1}))$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.1053v1", 
    "title": "Round-Trip Voronoi Diagrams and Doubling Density in Geographic Networks", 
    "arxiv-id": "1005.1053v1", 
    "author": "Thomas D. Dickerson", 
    "publish": "2010-05-06T18:40:36Z", 
    "summary": "The round-trip distance function on a geographic network (such as a road\nnetwork, flight network, or utility distribution grid) defines the \"distance\"\nfrom a single vertex to a pair of vertices as the minimum length tour visiting\nall three vertices and ending at the starting vertex. Given a geographic\nnetwork and a subset of its vertices called \"sites\" (for example a road network\nwith a list of grocery stores), a two-site round-trip Voronoi diagram labels\neach vertex in the network with the pair of sites that minimizes the round-trip\ndistance from that vertex. Alternatively, given a geographic network and two\nsets of sites of different types (for example grocery stores and coffee shops),\na two-color round-trip Voronoi diagram labels each vertex with the pair of\nsites of different types minimizing the round-trip distance. In this paper, we\nprove several new properties of two-site and two-color round-trip Voronoi\ndiagrams in a geographic network, including a relationship between the\n\"doubling density\" of sites and an upper bound on the number of non-empty\nVoronoi regions. We show how those lemmas can be used in new algorithms\nasymptotically more efficient than previous known algorithms when the networks\nhave reasonable distribution properties related to doubling density, and we\nprovide experimental data suggesting that road networks with standard\npoint-of-interest sites have these properties."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1005.1122v2", 
    "title": "Estimating small frequency moments of data stream: a characteristic   function approach", 
    "arxiv-id": "1005.1122v2", 
    "author": "Purushottam Kar", 
    "publish": "2010-05-07T02:36:29Z", 
    "summary": "A data stream is viewed as a sequence of $M$ updates of the form\n$(\\text{index},i,v)$ to an $n$-dimensional integer frequency vector $f$, where\nthe update changes $f_i$ to $f_i + v$, and $v$ is an integer and assumed to be\nin $\\{-m, ..., m\\}$. The $p$th frequency moment $F_p$ is defined as\n$\\sum_{i=1}^n \\abs{f_i}^p$. We consider the problem of estimating $F_p$ to\nwithin a multiplicative approximation factor of $1\\pm \\epsilon$, for $p \\in\n[0,2]$. Several estimators have been proposed for this problem, including\nIndyk's median estimator \\cite{indy:focs00}, Li's geometric means estimator\n\\cite{pinglib:2006}, an \\Hss-based estimator \\cite{gc:random07}. The first two\nestimators require space $\\tilde{O}(\\epsilon^{-2})$, where the $\\tilde{O}$\nnotation hides polylogarithmic factors in $\\epsilon^{-1}, m, n$ and $M$.\nRecently, Kane, Nelson and Woodruff in \\cite{knw:soda10} present a\nspace-optimal and novel estimator, called the log-cosine estimator. In this\npaper, we present an elementary analysis of the log-cosine estimator in a\nstand-alone setting. The analysis in \\cite{knw:soda10} is more complicated."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.12.006", 
    "link": "http://arxiv.org/pdf/1005.2211v1", 
    "title": "Arboricity, h-Index, and Dynamic Algorithms", 
    "arxiv-id": "1005.2211v1", 
    "author": "Jayme L. Szwarcfiter", 
    "publish": "2010-05-12T21:33:10Z", 
    "summary": "In this paper we present a modification of a technique by Chiba and Nishizeki\n[Chiba and Nishizeki: Arboricity and Subgraph Listing Algorithms, SIAM J.\nComput. 14(1), pp. 210--223 (1985)]. Based on it, we design a data structure\nsuitable for dynamic graph algorithms. We employ the data structure to\nformulate new algorithms for several problems, including counting subgraphs of\nfour vertices, recognition of diamond-free graphs, cop-win graphs and strongly\nchordal graphs, among others. We improve the time complexity for graphs with\nlow arboricity or h-index."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_32", 
    "link": "http://arxiv.org/pdf/1005.2616v1", 
    "title": "Chains-into-Bins Processes", 
    "arxiv-id": "1005.2616v1", 
    "author": "Colin Cooper", 
    "publish": "2010-05-14T19:47:37Z", 
    "summary": "The study of {\\em balls-into-bins processes} or {\\em occupancy problems} has\na long history. These processes can be used to translate realistic problems\ninto mathematical ones in a natural way. In general, the goal of a\nballs-into-bins process is to allocate a set of independent objects (tasks,\njobs, balls) to a set of resources (servers, bins, urns) and, thereby, to\nminimize the maximum load. In this paper, we analyze the maximum load for the\n{\\em chains-into-bins} problem, which is defined as follows. There are $n$\nbins, and $m$ objects to be allocated. Each object consists of balls connected\ninto a chain of length $\\ell$, so that there are $m \\ell$ balls in total. We\nassume the chains cannot be broken, and that the balls in one chain have to be\nallocated to $\\ell$ consecutive bins. We allow each chain $d$ independent and\nuniformly random bin choices for its starting position. The chain is allocated\nusing the rule that the maximum load of any bin receiving a ball of that chain\nis minimized. We show that, for $d \\ge 2$ and $m\\cdot\\ell=O(n)$, the maximum\nload is $((\\ln \\ln m)/\\ln d) +O(1)$ with probability $1-\\tilde O(1/m^{d-1})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_32", 
    "link": "http://arxiv.org/pdf/1005.3473v1", 
    "title": "Efficient Algorithms and Data Structures for Massive Data Sets", 
    "arxiv-id": "1005.3473v1", 
    "author": "Alka", 
    "publish": "2010-05-19T15:32:19Z", 
    "summary": "For many algorithmic problems, traditional algorithms that optimise on the\nnumber of instructions executed prove expensive on I/Os. Novel and very\ndifferent design techniques, when applied to these problems, can produce\nalgorithms that are I/O efficient. This thesis adds to the growing chorus of\nsuch results. The computational models we use are the external memory model and\nthe W-Stream model.\n  On the external memory model, we obtain the following results. (1) An I/O\nefficient algorithm for computing minimum spanning trees of graphs that\nimproves on the performance of the best known algorithm. (2) The first external\nmemory version of soft heap, an approximate meldable priority queue. (3) Hard\nheap, the first meldable external memory priority queue that matches the\namortised I/O performance of the known external memory priority queues, while\nallowing a meld operation at the same amortised cost. (4) I/O efficient exact,\napproximate and randomised algorithms for the minimum cut problem, which has\nnot been explored before on the external memory model. (5) Some lower and upper\nbounds on I/Os for interval graphs.\n  On the W-Stream model, we obtain the following results. (1) Algorithms for\nvarious tree problems and list ranking that match the performance of the best\nknown algorithms and are easier to implement than them. (2) Pass efficient\nalgorithms for sorting, and the maximal independent set problems, that improve\non the best known algorithms. (3) Pass efficient algorithms for the graphs\nproblems of finding vertex-colouring, approximate single source shortest paths,\nmaximal matching, and approximate weighted vertex cover. (4) Lower bounds on\npasses for list ranking and maximal matching.\n  We propose two variants of the W-Stream model, and design algorithms for the\nmaximal independent set, vertex-colouring, and planar graph single source\nshortest paths problems on those models."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_32", 
    "link": "http://arxiv.org/pdf/1005.3835v3", 
    "title": "A Better Memoryless Online Algorithm for FIFO Buffering Packets with Two   Values", 
    "arxiv-id": "1005.3835v3", 
    "author": "Fei Li", 
    "publish": "2010-05-20T20:34:14Z", 
    "summary": "We consider scheduling packets with values in a capacity-bounded buffer in an\nonline setting. In this model, there is a buffer with limited capacity $B$. At\nany time, the buffer cannot accommodate more than $B$ packets. Packets arrive\nover time. Each packet is associated with a non-negative value. Packets leave\nthe buffer only because they are either sent or dropped. Those packets that\nhave left the buffer will not be reconsidered for delivery any more. In each\ntime step, at most one packet in the buffer can be sent. The order in which the\npackets are sent should comply with the order of their arrival time. The\nobjective is to maximize the total value of the packets sent in an online\nmanner. In this paper, we study a variant of this FIFO buffering model in which\na packet's value is either 1 or $\\alpha > 1$. We present a deterministic\nmemoryless 1.304-competitive algorithm. This algorithm has the same competitive\nratio as the one presented in (Lotker and Patt-Shamir. PODC 2002, Computer\nNetworks 2003). However, our algorithm is simpler and does not employ any\nmarking bits. The idea used in our algorithm is novel and different from all\nprevious approaches applied for the general model and its variants. We do not\nproactively preempt one packet when a new packet arrives. Instead, we may\npreempt more than one 1-value packet when the buffer contains sufficiently many\n$\\alpha$-value packets."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_32", 
    "link": "http://arxiv.org/pdf/1005.4033v1", 
    "title": "Polylogarithmic Approximation for Edit Distance and the Asymmetric Query   Complexity", 
    "arxiv-id": "1005.4033v1", 
    "author": "Krzysztof Onak", 
    "publish": "2010-05-21T18:01:27Z", 
    "summary": "We present a near-linear time algorithm that approximates the edit distance\nbetween two strings within a polylogarithmic factor; specifically, for strings\nof length n and every fixed epsilon>0, it can compute a (log n)^O(1/epsilon)\napproximation in n^(1+epsilon) time. This is an exponential improvement over\nthe previously known factor, 2^(O (sqrt(log n))), with a comparable running\ntime (Ostrovsky and Rabani J.ACM 2007; Andoni and Onak STOC 2009). Previously,\nno efficient polylogarithmic approximation algorithm was known for any\ncomputational task involving edit distance (e.g., nearest neighbor search or\nsketching).\n  This result arises naturally in the study of a new asymmetric query model. In\nthis model, the input consists of two strings x and y, and an algorithm can\naccess y in an unrestricted manner, while being charged for querying every\nsymbol of x. Indeed, we obtain our main result by designing an algorithm that\nmakes a small number of queries in this model. We then provide a\nnearly-matching lower bound on the number of queries.\n  Our lower bound is the first to expose hardness of edit distance stemming\nfrom the input strings being \"repetitive\", which means that many of their\nsubstrings are approximately identical. Consequently, our lower bound provides\nthe first rigorous separation between edit distance and Ulam distance, which is\nedit distance on non-repetitive strings, such as permutations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17458-2_32", 
    "link": "http://arxiv.org/pdf/1005.4394v1", 
    "title": "Scheduling Packets with Values and Deadlines in Size-bounded Buffers", 
    "arxiv-id": "1005.4394v1", 
    "author": "Fei Li", 
    "publish": "2010-05-24T18:28:25Z", 
    "summary": "Motivated by providing quality-of-service differentiated services in the\nInternet, we consider buffer management algorithms for network switches. We\nstudy a multi-buffer model. A network switch consists of multiple size-bounded\nbuffers such that at any time, the number of packets residing in each\nindividual buffer cannot exceed its capacity. Packets arrive at the network\nswitch over time; they have values, deadlines, and designated buffers. In each\ntime step, at most one pending packet is allowed to be sent and this packet can\nbe from any buffer. The objective is to maximize the total value of the packets\nsent by their respective deadlines. A 9.82-competitive online algorithm has\nbeen provided for this model (Azar and Levy. SWAT 2006), but no offline\nalgorithms have been known yet. In this paper, We study the offline setting of\nthe multi-buffer model. Our contributions include a few optimal offline\nalgorithms for some variants of the model. Each variant has its unique and\ninteresting algorithmic feature. These offline algorithms help us understand\nthe model better in designing online algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17458-2_32", 
    "link": "http://arxiv.org/pdf/1005.4652v2", 
    "title": "Succinct Representations of Dynamic Strings", 
    "arxiv-id": "1005.4652v2", 
    "author": "J. Ian Munro", 
    "publish": "2010-05-25T18:27:47Z", 
    "summary": "The rank and select operations over a string of length n from an alphabet of\nsize $\\sigma$ have been used widely in the design of succinct data structures.\nIn many applications, the string itself need be maintained dynamically,\nallowing characters of the string to be inserted and deleted. Under the word\nRAM model with word size $w=\\Omega(\\lg n)$, we design a succinct representation\nof dynamic strings using $nH_0 + o(n)\\lg\\sigma + O(w)$ bits to support rank,\nselect, insert and delete in $O(\\frac{\\lg n}{\\lg\\lg n}(\\frac{\\lg \\sigma}{\\lg\\lg\nn}+1))$ time. When the alphabet size is small, i.e. when $\\sigma = O(\\polylog\n(n))$, including the case in which the string is a bit vector, these operations\nare supported in $O(\\frac{\\lg n}{\\lg\\lg n})$ time. Our data structures are more\nefficient than previous results on the same problem, and we have applied them\nto improve results on the design and construction of space-efficient text\nindexes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17458-2_32", 
    "link": "http://arxiv.org/pdf/1005.5513v1", 
    "title": "Almost Optimal Unrestricted Fast Johnson-Lindenstrauss Transform", 
    "arxiv-id": "1005.5513v1", 
    "author": "Edo Liberty", 
    "publish": "2010-05-30T11:05:26Z", 
    "summary": "The problems of random projections and sparse reconstruction have much in\ncommon and individually received much attention. Surprisingly, until now they\nprogressed in parallel and remained mostly separate. Here, we employ new tools\nfrom probability in Banach spaces that were successfully used in the context of\nsparse reconstruction to advance on an open problem in random pojection. In\nparticular, we generalize and use an intricate result by Rudelson and Vershynin\nfor sparse reconstruction which uses Dudley's theorem for bounding Gaussian\nprocesses. Our main result states that any set of $N = \\exp(\\tilde{O}(n))$ real\nvectors in $n$ dimensional space can be linearly mapped to a space of dimension\n$k=O(\\log N\\polylog(n))$, while (1) preserving the pairwise distances among the\nvectors to within any constant distortion and (2) being able to apply the\ntransformation in time $O(n\\log n)$ on each vector. This improves on the best\nknown $N = \\exp(\\tilde{O}(n^{1/2}))$ achieved by Ailon and Liberty and $N =\n\\exp(\\tilde{O}(n^{1/3}))$ by Ailon and Chazelle.\n  The dependence in the distortion constant however is believed to be\nsuboptimal and subject to further investigation. For constant distortion, this\nsettles the open question posed by these authors up to a $\\polylog(n)$ factor\nwhile considerably simplifying their constructions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2012.01.011", 
    "link": "http://arxiv.org/pdf/1005.5525v4", 
    "title": "Efficient Local Search Algorithms for Known and New Neighborhoods for   the Generalized Traveling Salesman Problem", 
    "arxiv-id": "1005.5525v4", 
    "author": "Gregory Gutin", 
    "publish": "2010-05-30T13:13:09Z", 
    "summary": "The Generalized Traveling Salesman Problem (GTSP) is a well-known\ncombinatorial optimization problem with a host of applications. It is an\nextension of the Traveling Salesman Problem (TSP) where the set of cities is\npartitioned into so-called clusters, and the salesman has to visit every\ncluster exactly once.\n  While the GTSP is a very important combinatorial optimization problem and is\nwell studied in many aspects, the local search algorithms used in the\nliterature are mostly basic adaptations of simple TSP heuristics. Hence, a\nthorough and deep research of the neighborhoods and local search algorithms\nspecific to the GTSP is required.\n  We formalize the procedure of adaptation of a TSP neighborhood for the GTSP\nand classify all other existing and some new GTSP neighborhoods. For every\nneighborhood, we provide efficient exploration algorithms that are often\nsignificantly faster than the ones known from the literature. Finally, we\ncompare different local search implementations empirically."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.0407v2", 
    "title": "A Note on Element-wise Matrix Sparsification via a Matrix-valued   Bernstein Inequality", 
    "arxiv-id": "1006.0407v2", 
    "author": "Anastasios Zouzias", 
    "publish": "2010-06-02T14:39:17Z", 
    "summary": "Given an n x n matrix A, we present a simple, element-wise sparsification\nalgorithm that zeroes out all sufficiently small elements of A and then retains\nsome of the remaining elements with probabilities proportional to the square of\ntheir magnitudes. We analyze the approximation accuracy of the proposed\nalgorithm using a recent, elegant non-commutative Bernstein inequality, and\ncompare our bounds with all existing (to the best of our knowledge)\nelement-wise matrix sparsification algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.0809v2", 
    "title": "Tight and simple Web graph compression", 
    "arxiv-id": "1006.0809v2", 
    "author": "Wojciech Bieniecki", 
    "publish": "2010-06-04T08:41:22Z", 
    "summary": "Analysing Web graphs has applications in determining page ranks, fighting Web\nspam, detecting communities and mirror sites, and more. This study is however\nhampered by the necessity of storing a major part of huge graphs in the\nexternal memory, which prevents efficient random access to edge (hyperlink)\nlists. A number of algorithms involving compression techniques have thus been\npresented, to represent Web graphs succinctly but also providing random access.\nThose techniques are usually based on differential encodings of the adjacency\nlists, finding repeating nodes or node regions in the successive lists, more\ngeneral grammar-based transformations or 2-dimensional representations of the\nbinary matrix of the graph. In this paper we present two Web graph compression\nalgorithms. The first can be seen as engineering of the Boldi and Vigna (2004)\nmethod. We extend the notion of similarity between link lists, and use a more\ncompact encoding of residuals. The algorithm works on blocks of varying size\n(in the number of input lines) and sacrifices access time for better\ncompression ratio, achieving more succinct graph representation than other\nalgorithms reported in the literature. The second algorithm works on blocks of\nthe same size, in the number of input lines, and its key mechanism is merging\nthe block into a single ordered list. This method achieves much more attractive\nspace-time tradeoffs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1104v1", 
    "title": "Systolic Array Technique for Determining Common Approximate Substrings", 
    "arxiv-id": "1006.1104v1", 
    "author": "Kenneth B. Kent", 
    "publish": "2010-06-06T14:11:59Z", 
    "summary": "A technique using a systolic array structure is proposed for solving the\ncommon approximate substring (CAS) problem. This approach extends the technique\nintroduced in earlier work from the computation of the edit-distance between\ntwo strings to the more encompassing CAS problem. A comparison to existing work\nis given, and the technique presented is validated and analyzed based on\nsimulations."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1117v1", 
    "title": "On the hardness of distance oracle for sparse graph", 
    "arxiv-id": "1006.1117v1", 
    "author": "Ely Porat", 
    "publish": "2010-06-06T17:03:09Z", 
    "summary": "In this paper we show that set-intersection is harder than distance oracle on\nsparse graphs. Given a collection of total size n which consists of m sets\ndrawn from universe U, the set-intersection problem is to build a data\nstructure which can answer whether two sets have any intersection. A distance\noracle is a data structure which can answer distance queries on a given graph.\nWe show that if one can build distance oracle for sparse graph G=(V,E), which\nrequires s(|V|,|E|) space and answers a (2-\\epsilon,c)-approximate distance\nquery in time t(|V|,|E|) where (2-\\epsilon) is a multiplicative error and c is\na constant additive error, then, set-intersection can be solved in t(m+|U|,n)\ntime using s(m+|U|,n) space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1231v3", 
    "title": "On the Insertion Time of Cuckoo Hashing", 
    "arxiv-id": "1006.1231v3", 
    "author": "Angelika Steger", 
    "publish": "2010-06-07T11:15:01Z", 
    "summary": "Cuckoo hashing is an efficient technique for creating large hash tables with\nhigh space utilization and guaranteed constant access times. There, each item\ncan be placed in a location given by any one out of k different hash functions.\nIn this paper we investigate further the random walk heuristic for inserting in\nan online fashion new items into the hash table. Provided that k > 2 and that\nthe number of items in the table is below (but arbitrarily close) to the\ntheoretically achievable load threshold, we show a polylogarithmic bound for\nthe maximum insertion time that holds with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.1431v1", 
    "title": "Algebraic characterisation of one-way patterns", 
    "arxiv-id": "1006.1431v1", 
    "author": "Elham Kashefi", 
    "publish": "2010-06-08T01:16:41Z", 
    "summary": "We give a complete structural characterisation of the map the positive branch\nof a one-way pattern implements. We start with the representation of the\npositive branch in terms of the phase map decomposition, which is then further\nanalysed to obtain the primary structure of the matrix M, representing the\nphase map decomposition in the computational basis. Using this approach we\nobtain some preliminary results on the connection between the columns structure\nof a given unitary and the angles of measurements in a pattern that implements\nit. We believe this work is a step forward towards a full characterisation of\nthose unitaries with an efficient one-way model implementation."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.1990v2", 
    "title": "Minimizing a sum of submodular functions", 
    "arxiv-id": "1006.1990v2", 
    "author": "Vladimir Kolmogorov", 
    "publish": "2010-06-10T10:18:36Z", 
    "summary": "We consider the problem of minimizing a function represented as a sum of\nsubmodular terms. We assume each term allows an efficient computation of {\\em\nexchange capacities}. This holds, for example, for terms depending on a small\nnumber of variables, or for certain cardinality-dependent terms.\n  A naive application of submodular minimization algorithms would not exploit\nthe existence of specialized exchange capacity subroutines for individual\nterms. To overcome this, we cast the problem as a {\\em submodular flow} (SF)\nproblem in an auxiliary graph, and show that applying most existing SF\nalgorithms would rely only on these subroutines.\n  We then explore in more detail Iwata's capacity scaling approach for\nsubmodular flows (Math. Programming, 76(2):299--308, 1997). In particular, we\nshow how to improve its complexity in the case when the function contains\ncardinality-dependent terms."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.2361v1", 
    "title": "Constructions from Dots and Lines", 
    "arxiv-id": "1006.2361v1", 
    "author": "Peter Neubauer", 
    "publish": "2010-06-11T18:16:10Z", 
    "summary": "A graph is a data structure composed of dots (i.e. vertices) and lines (i.e.\nedges). The dots and lines of a graph can be organized into intricate\narrangements. The ability for a graph to denote objects and their relationships\nto one another allow for a surprisingly large number of things to be modeled as\na graph. From the dependencies that link software packages to the wood beams\nthat provide the framing to a house, most anything has a corresponding graph\nrepresentation. However, just because it is possible to represent something as\na graph does not necessarily mean that its graph representation will be useful.\nIf a modeler can leverage the plethora of tools and algorithms that store and\nprocess graphs, then such a mapping is worthwhile. This article explores the\nworld of graphs in computing and exposes situations in which graphical models\nare beneficial."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.3368v2", 
    "title": "Optimal Constant-Time Approximation Algorithms and (Unconditional)   Inapproximability Results for Every Bounded-Degree CSP", 
    "arxiv-id": "1006.3368v2", 
    "author": "Yuichi Yoshida", 
    "publish": "2010-06-17T04:38:53Z", 
    "summary": "Raghavendra (STOC 2008) gave an elegant and surprising result: if Khot's\nUnique Games Conjecture (STOC 2002) is true, then for every constraint\nsatisfaction problem (CSP), the best approximation ratio is attained by a\ncertain simple semidefinite programming and a rounding scheme for it. In this\npaper, we show that similar results hold for constant-time approximation\nalgorithms in the bounded-degree model. Specifically, we present the\nfollowings: (i) For every CSP, we construct an oracle that serves an access, in\nconstant time, to a nearly optimal solution to a basic LP relaxation of the\nCSP. (ii) Using the oracle, we give a constant-time rounding scheme that\nachieves an approximation ratio coincident with the integrality gap of the\nbasic LP. (iii) Finally, we give a generic conversion from integrality gaps of\nbasic LPs to hardness results. All of those results are \\textit{unconditional}.\nTherefore, for every bounded-degree CSP, we give the best constant-time\napproximation algorithm among all. A CSP instance is called $\\epsilon$-far from\nsatisfiability if we must remove at least an $\\epsilon$-fraction of constraints\nto make it satisfiable. A CSP is called testable if there is a constant-time\nalgorithm that distinguishes satisfiable instances from $\\epsilon$-far\ninstances with probability at least $2/3$. Using the results above, we also\nderive, under a technical assumption, an equivalent condition under which a CSP\nis testable in the bounded-degree model."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3541v1", 
    "title": "Complexity dichotomy on partial grid recognition", 
    "arxiv-id": "1006.3541v1", 
    "author": "Celina M. H. de Figueiredo", 
    "publish": "2010-06-17T18:30:09Z", 
    "summary": "Deciding whether a graph can be embedded in a grid using only unit-length\nedges is NP-complete, even when restricted to binary trees. However, it is not\ndifficult to devise a number of graph classes for which the problem is\npolynomial, even trivial. A natural step, outstanding thus far, was to provide\na broad classification of graphs that make for polynomial or NP-complete\ninstances. We provide such a classification based on the set of allowed vertex\ndegrees in the input graphs, yielding a full dichotomy on the complexity of the\nproblem. As byproducts, the previous NP-completeness result for binary trees\nwas strengthened to strictly binary trees, and the three-dimensional version of\nthe problem was for the first time proven to be NP-complete. Our results were\nmade possible by introducing the concepts of consistent orientations and robust\ngadgets, and by showing how the former allows NP-completeness proofs by local\nreplacement even in the absence of the latter."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3715v1", 
    "title": "Should Static Search Trees Ever Be Unbalanced?", 
    "arxiv-id": "1006.3715v1", 
    "author": "Karim Dou\u00efeb", 
    "publish": "2010-06-18T15:08:28Z", 
    "summary": "In this paper we study the question of whether or not a static search tree\nshould ever be unbalanced. We present several methods to restructure an\nunbalanced k-ary search tree $T$ into a new tree $R$ that preserves many of the\nproperties of $T$ while having a height of $\\log_k n +1$ which is one unit off\nof the optimal height. More specifically, we show that it is possible to ensure\nthat the depth of the elements in $R$ is no more than their depth in $T$ plus\nat most $\\log_k \\log_k n +2$. At the same time it is possible to guarantee that\nthe average access time $P(R)$ in tree $R$ is no more than the average access\ntime $P(T)$ in tree $T$ plus $O(\\log_k P(T))$. This suggests that for most\napplications, a balanced tree is always a better option than an unbalanced one\nsince the balanced tree has similar average access time and much better worst\ncase access time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3968v1", 
    "title": "Practical Range Aggregation, Selection and Set Maintenance Techniques", 
    "arxiv-id": "1006.3968v1", 
    "author": "Nicolae Tapus", 
    "publish": "2010-06-20T23:23:07Z", 
    "summary": "In this paper we present several new and very practical methods and\ntechniques for range aggregation and selection problems in multidimensional\ndata structures and other types of sets of values. We also present some new\nextensions and applications for some fundamental set maintenance problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3970v2", 
    "title": "Approximating Sparsest Cut in Graphs of Bounded Treewidth", 
    "arxiv-id": "1006.3970v2", 
    "author": "Prasad Raghavendra", 
    "publish": "2010-06-21T00:15:00Z", 
    "summary": "We give the first constant-factor approximation algorithm for Sparsest Cut\nwith general demands in bounded treewidth graphs. In contrast to previous\nalgorithms, which rely on the flow-cut gap and/or metric embeddings, our\napproach exploits the Sherali-Adams hierarchy of linear programming\nrelaxations."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3993v1", 
    "title": "An Algorithm to List All the Fixed-Point Free Involutions on a Finite   Set", 
    "arxiv-id": "1006.3993v1", 
    "author": "Cyril Prissette", 
    "publish": "2010-06-21T06:38:46Z", 
    "summary": "An involution on a finite set is a bijection such as I(I(e))=e for all the\nelement of the set. A fixed-point free involution on a finite set is an\ninvolution such as I(e)=e for none element of the set. In this article, the\nfixed-point free involutions are represented as partitions of the set and some\nproperties linked to this representation are exhibited. Then an optimal\nalgorithm to list all the fixed-point free involutions is presented. Its\nsoundness relies on the representation of the fixed-point free involutions as\npartitions. Finally, an implementation of the algorithm is proposed, with an\neffective data representation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.4093v1", 
    "title": "Dynamic Range Reporting in External Memory", 
    "arxiv-id": "1006.4093v1", 
    "author": "Yakov Nekrich", 
    "publish": "2010-06-21T15:26:40Z", 
    "summary": "In this paper we describe a dynamic external memory data structure that\nsupports range reporting queries in three dimensions in $O(\\log_B^2 N +\n\\frac{k}{B})$ I/O operations, where $k$ is the number of points in the answer\nand $B$ is the block size. This is the first dynamic data structure that\nanswers three-dimensional range reporting queries in $\\log_B^{O(1)} N +\nO(\\frac{k}{B})$ I/Os."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4136v1", 
    "title": "Competitive Boolean Function Evaluation: Beyond Monotonicity, and the   Symmetric Case", 
    "arxiv-id": "1006.4136v1", 
    "author": "Martin Milanic", 
    "publish": "2010-06-21T19:07:52Z", 
    "summary": "We study the extremal competitive ratio of Boolean function evaluation. We\nprovide the first non-trivial lower and upper bounds for classes of Boolean\nfunctions which are not included in the class of monotone Boolean functions.\nFor the particular case of symmetric functions our bounds are matching and we\nexactly characterize the best possible competitiveness achievable by a\ndeterministic algorithm. Our upper bound is obtained by a simple polynomial\ntime algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4357v1", 
    "title": "Prize-Collecting Steiner Tree and Forest in Planar Graphs", 
    "arxiv-id": "1006.4357v1", 
    "author": "Nitish Korula", 
    "publish": "2010-06-22T19:53:58Z", 
    "summary": "We obtain polynomial-time approximation-preserving reductions (up to a factor\nof 1 + \\epsilon) from the prize-collecting Steiner tree and prize-collecting\nSteiner forest problems in planar graphs to the corresponding problems in\ngraphs of bounded treewidth. We also give an exact algorithm for the\nprize-collecting Steiner tree problem that runs in polynomial time for graphs\nof bounded treewidth. This, combined with our reductions, yields a PTAS for the\nprize-collecting Steiner tree problem in planar graphs and generalizes the PTAS\nof Borradaile, Klein and Mathieu for the Steiner tree problem in planar graphs.\nOur results build upon the ideas of Borradaile, Klein and Mathieu and the work\nof Bateni, Hajiaghayi and Marx on a PTAS for the Steiner forest problem in\nplanar graphs. Our main technical result is on the properties of primal-dual\nalgorithms for Steiner tree and forest problems in general graphs when they are\nrun with scaled up penalties."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4536v1", 
    "title": "Vertex Sparsifiers and Abstract Rounding Algorithms", 
    "arxiv-id": "1006.4536v1", 
    "author": "Ankur Moitra", 
    "publish": "2010-06-23T14:44:07Z", 
    "summary": "The notion of vertex sparsification is introduced in \\cite{M}, where it was\nshown that for any graph $G = (V, E)$ and a subset of $k$ terminals $K \\subset\nV$, there is a polynomial time algorithm to construct a graph $H = (K, E_H)$ on\njust the terminal set so that simultaneously for all cuts $(A, K-A)$, the value\nof the minimum cut in $G$ separating $A$ from $K -A$ is approximately the same\nas the value of the corresponding cut in $H$.\n  We give the first super-constant lower bounds for how well a cut-sparsifier\n$H$ can simultaneously approximate all minimum cuts in $G$. We prove a lower\nbound of $\\Omega(\\log^{1/4} k)$ -- this is polynomially-related to the known\nupper bound of $O(\\log k/\\log \\log k)$. This is an exponential improvement on\nthe $\\Omega(\\log \\log k)$ bound given in \\cite{LM} which in fact was for a\nstronger vertex sparsification guarantee, and did not apply to cut sparsifiers.\n  Despite this negative result, we show that for many natural problems, we do\nnot need to incur a multiplicative penalty for our reduction. We obtain optimal\n$O(\\log k)$-competitive Steiner oblivious routing schemes, which generalize the\nresults in \\cite{R}. We also demonstrate that for a wide range of graph packing\nproblems (which includes maximum concurrent flow, maximum multiflow and\nmulticast routing, among others, as a special case), the integrality gap of the\nlinear program is always at most $O(\\log k)$ times the integrality gap\nrestricted to trees. This result helps to explain the ubiquity of the $O(\\log\nk)$ guarantees for such problems.\n  Lastly, we use our ideas to give an efficient construction for\nvertex-sparsifiers that match the current best existential results -- this was\npreviously open. Our algorithm makes novel use of Earth-mover constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1137/130908440", 
    "link": "http://arxiv.org/pdf/1006.4586v3", 
    "title": "Vertex Sparsifiers: New Results from Old Techniques", 
    "arxiv-id": "1006.4586v3", 
    "author": "Kunal Talwar", 
    "publish": "2010-06-23T16:43:47Z", 
    "summary": "Given a capacitated graph $G = (V,E)$ and a set of terminals $K \\subseteq V$,\nhow should we produce a graph $H$ only on the terminals $K$ so that every\n(multicommodity) flow between the terminals in $G$ could be supported in $H$\nwith low congestion, and vice versa? (Such a graph $H$ is called a\nflow-sparsifier for $G$.) What if we want $H$ to be a \"simple\" graph? What if\nwe allow $H$ to be a convex combination of simple graphs?\n  Improving on results of Moitra [FOCS 2009] and Leighton and Moitra [STOC\n2010], we give efficient algorithms for constructing: (a) a flow-sparsifier $H$\nthat maintains congestion up to a factor of $O(\\log k/\\log \\log k)$, where $k =\n|K|$, (b) a convex combination of trees over the terminals $K$ that maintains\ncongestion up to a factor of $O(\\log k)$, and (c) for a planar graph $G$, a\nconvex combination of planar graphs that maintains congestion up to a constant\nfactor. This requires us to give a new algorithm for the 0-extension problem,\nthe first one in which the preimages of each terminal are connected in $G$.\nMoreover, this result extends to minor-closed families of graphs.\n  Our improved bounds immediately imply improved approximation guarantees for\nseveral terminal-based cut and ordering problems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/130908440", 
    "link": "http://arxiv.org/pdf/1006.4607v2", 
    "title": "Metric Extension Operators, Vertex Sparsifiers and Lipschitz   Extendability", 
    "arxiv-id": "1006.4607v2", 
    "author": "Yury Makarychev", 
    "publish": "2010-06-23T18:43:33Z", 
    "summary": "We study vertex cut and flow sparsifiers that were recently introduced by\nMoitra, and Leighton and Moitra. We improve and generalize their results. We\ngive a new polynomial-time algorithm for constructing O(log k / log log k) cut\nand flow sparsifiers, matching the best existential upper bound on the quality\nof a sparsifier, and improving the previous algorithmic upper bound of O(log^2\nk / log log k). We show that flow sparsifiers can be obtained from linear\noperators approximating minimum metric extensions. We introduce the notion of\n(linear) metric extension operators, prove that they exist, and give an exact\npolynomial-time algorithm for finding optimal operators.\n  We then establish a direct connection between flow and cut sparsifiers and\nLipschitz extendability of maps in Banach spaces, a notion studied in\nfunctional analysis since 1930s. Using this connection, we prove a lower bound\nof Omega(sqrt{log k/log log k}) for flow sparsifiers and a lower bound of\nOmega(sqrt{log k}/log log k) for cut sparsifiers. We show that if a certain\nopen question posed by Ball in 1992 has a positive answer, then there exist\n\\tilde O(sqrt{log k}) cut sparsifiers. On the other hand, any lower bound on\ncut sparsifiers better than \\tilde Omega(sqrt{log k}) would imply a negative\nanswer to this question."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17458-2_16", 
    "link": "http://arxiv.org/pdf/1006.4828v1", 
    "title": "An Efficient Algorithm For Chinese Postman Walk on Bi-directed de Bruijn   Graphs", 
    "arxiv-id": "1006.4828v1", 
    "author": "Hieu Dinh", 
    "publish": "2010-06-24T16:35:47Z", 
    "summary": "Sequence assembly from short reads is an important problem in biology. It is\nknown that solving the sequence assembly problem exactly on a bi-directed de\nBruijn graph or a string graph is intractable. However finding a Shortest\nDouble stranded DNA string (SDDNA) containing all the k-long words in the reads\nseems to be a good heuristic to get close to the original genome. This problem\nis equivalent to finding a cyclic Chinese Postman (CP) walk on the underlying\nun-weighted bi-directed de Bruijn graph built from the reads. The Chinese\nPostman walk Problem (CPP) is solved by reducing it to a general bi-directed\nflow on this graph which runs in O(|E|2 log2(|V |)) time. In this paper we show\nthat the cyclic CPP on bi-directed graphs can be solved without reducing it to\nbi-directed flow. We present a ?(p(|V | + |E|) log(|V |) + (dmaxp)3) time\nalgorithm to solve the cyclic CPP on a weighted bi-directed de Bruijn graph,\nwhere p = max{|{v|din(v) - dout(v) > 0}|, |{v|din(v) - dout(v) < 0}|} and dmax\n= max{|din(v) - dout(v)}. Our algorithm performs asymptotically better than the\nbidirected flow algorithm when the number of imbalanced nodes p is much less\nthan the nodes in the bi-directed graph. From our experimental results on\nvarious datasets, we have noticed that the value of p/|V | lies between 0.08%\nand 0.13% with 95% probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1006.5235v1", 
    "title": "Mining Top-K Frequent Itemsets Through Progressive Sampling", 
    "arxiv-id": "1006.5235v1", 
    "author": "Fabio Vandin", 
    "publish": "2010-06-27T20:38:39Z", 
    "summary": "We study the use of sampling for efficiently mining the top-K frequent\nitemsets of cardinality at most w. To this purpose, we define an approximation\nto the top-K frequent itemsets to be a family of itemsets which includes\n(resp., excludes) all very frequent (resp., very infrequent) itemsets, together\nwith an estimate of these itemsets' frequencies with a bounded error. Our first\nresult is an upper bound on the sample size which guarantees that the top-K\nfrequent itemsets mined from a random sample of that size approximate the\nactual top-K frequent itemsets, with probability larger than a specified value.\nWe show that the upper bound is asymptotically tight when w is constant. Our\nmain algorithmic contribution is a progressive sampling approach, combined with\nsuitable stopping conditions, which on appropriate inputs is able to extract\napproximate top-K frequent itemsets from samples whose sizes are smaller than\nthe general upper bound. In order to test the stopping conditions, this\napproach maintains the frequency of all itemsets encountered, which is\npractical only for small w. However, we show how this problem can be mitigated\nby using a variation of Bloom filters. A number of experiments conducted on\nboth synthetic and real bench- mark datasets show that using samples\nsubstantially smaller than the original dataset (i.e., of size defined by the\nupper bound or reached through the progressive sampling approach) enable to\napproximate the actual top-K frequent itemsets with accuracy much higher than\nwhat analytically proved."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.0501v1", 
    "title": "Quasi-Random Rumor Spreading: Reducing Randomness Can Be Costly", 
    "arxiv-id": "1008.0501v1", 
    "author": "Mahmoud Fouz", 
    "publish": "2010-08-03T09:59:29Z", 
    "summary": "We give a time-randomness tradeoff for the quasi-random rumor spreading\nprotocol proposed by Doerr, Friedrich and Sauerwald [SODA 2008] on complete\ngraphs. In this protocol, the goal is to spread a piece of information\noriginating from one vertex throughout the network. Each vertex is assumed to\nhave a (cyclic) list of its neighbors. Once a vertex is informed by one of its\nneighbors, it chooses a position in its list uniformly at random and then\ninforms its neighbors starting from that position and proceeding in order of\nthe list. Angelopoulos, Doerr, Huber and Panagiotou [Electron.~J.~Combin.~2009]\nshowed that after $(1+o(1))(\\log_2 n + \\ln n)$ rounds, the rumor will have been\nbroadcasted to all nodes with probability $1 - o(1)$.\n  We study the broadcast time when the amount of randomness available at each\nnode is reduced in natural way. In particular, we prove that if each node can\nonly make its initial random selection from every $\\ell$-th node on its list,\nthen there exists lists such that $(1-\\varepsilon) (\\log_2 n + \\ln n - \\log_2\n\\ell - \\ln \\ell)+\\ell-1$ steps are needed to inform every vertex with\nprobability at least $1-O\\bigl(\\exp\\bigl(-\\frac{n^\\varepsilon}{2\\ln\nn}\\bigr)\\bigr)$. This shows that a further reduction of the amount of\nrandomness used in a simple quasi-random protocol comes at a loss of\nefficiency."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.0541v1", 
    "title": "Determinant Sums for Undirected Hamiltonicity", 
    "arxiv-id": "1008.0541v1", 
    "author": "Andreas Bj\u00f6rklund", 
    "publish": "2010-08-03T13:10:49Z", 
    "summary": "We present a Monte Carlo algorithm for Hamiltonicity detection in an\n$n$-vertex undirected graph running in $O^*(1.657^{n})$ time. To the best of\nour knowledge, this is the first superpolynomial improvement on the worst case\nruntime for the problem since the $O^*(2^n)$ bound established for TSP almost\nfifty years ago (Bellman 1962, Held and Karp 1962). It answers in part the\nfirst open problem in Woeginger's 2003 survey on exact algorithms for NP-hard\nproblems.\n  For bipartite graphs, we improve the bound to $O^*(1.414^{n})$ time. Both the\nbipartite and the general algorithm can be implemented to use space polynomial\nin $n$.\n  We combine several recently resurrected ideas to get the results. Our main\ntechnical contribution is a new reduction inspired by the algebraic sieving\nmethod for $k$-Path (Koutis ICALP 2008, Williams IPL 2009). We introduce the\nLabeled Cycle Cover Sum in which we are set to count weighted arc labeled cycle\ncovers over a finite field of characteristic two. We reduce Hamiltonicity to\nLabeled Cycle Cover Sum and apply the determinant summation technique for Exact\nSet Covers (Bj\\\"orklund STACS 2010) to evaluate it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.0587v1", 
    "title": "Row Sampling for Matrix Algorithms via a Non-Commutative Bernstein Bound", 
    "arxiv-id": "1008.0587v1", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2010-08-03T16:28:10Z", 
    "summary": "We focus the use of \\emph{row sampling} for approximating matrix algorithms.\nWe give applications to matrix multipication; sparse matrix reconstruction;\nand, \\math{\\ell_2} regression. For a matrix \\math{\\matA\\in\\R^{m\\times d}} which\nrepresents \\math{m} points in \\math{d\\ll m} dimensions, all of these tasks can\nbe achieved in \\math{O(md^2)} via the singular value decomposition (SVD). For\nappropriate row-sampling probabilities (which typically depend on the norms of\nthe rows of the \\math{m\\times d} left singular matrix of \\math{\\matA} (the\n\\emph{leverage scores}), we give row-sampling algorithms with linear (up to\npolylog factors) dependence on the stable rank of \\math{\\matA}. This result is\nachieved through the application of non-commutative Bernstein bounds.\n  We then give, to our knowledge, the first algorithms for computing\napproximations to the appropriate row-sampling probabilities without going\nthrough the SVD of \\math{\\matA}. Thus, these are the first \\math{o(md^2)}\nalgorithms for row-sampling based approximations to the matrix algorithms which\nuse leverage scores as the sampling probabilities. The techniques we use to\napproximate sampling according to the leverage scores uses some powerful recent\nresults in the theory of random projections for embedding, and may be of some\nindependent interest. We confess that one may perform all these matrix tasks\nmore efficiently using these same random projection methods, however the\nresulting algorithms are in terms of a small number of linear combinations of\nall the rows. In many applications, the actual rows of \\math{\\matA} have some\nphysical meaning and so methods based on a small number of the actual rows are\nof interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.1480v1", 
    "title": "Fast, precise and dynamic distance queries", 
    "arxiv-id": "1008.1480v1", 
    "author": "Liam Roditty", 
    "publish": "2010-08-09T10:21:33Z", 
    "summary": "We present an approximate distance oracle for a point set S with n points and\ndoubling dimension {\\lambda}. For every {\\epsilon}>0, the oracle supports\n(1+{\\epsilon})-approximate distance queries in (universal) constant time,\noccupies space [{\\epsilon}^{-O({\\lambda})} + 2^{O({\\lambda} log {\\lambda})}]n,\nand can be constructed in [2^{O({\\lambda})} log3 n + {\\epsilon}^{-O({\\lambda})}\n+ 2^{O({\\lambda} log {\\lambda})}]n expected time. This improves upon the best\npreviously known constructions, presented by Har-Peled and Mendel. Furthermore,\nthe oracle can be made fully dynamic with expected O(1) query time and only\n2^{O({\\lambda})} log n + {\\epsilon}^{-O({\\lambda})} + 2^{O({\\lambda} log\n{\\lambda})} update time. This is the first fully dynamic\n(1+{\\epsilon})-distance oracle."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.1687v1", 
    "title": "A Deterministic Polynomial-time Approximation Scheme for Counting   Knapsack Solutions", 
    "arxiv-id": "1008.1687v1", 
    "author": "Eric Vigoda", 
    "publish": "2010-08-10T10:54:31Z", 
    "summary": "Given n elements with nonnegative integer weights w1,..., wn and an integer\ncapacity C, we consider the counting version of the classic knapsack problem:\nfind the number of distinct subsets whose weights add up to at most the given\ncapacity. We give a deterministic algorithm that estimates the number of\nsolutions to within relative error 1+-eps in time polynomial in n and 1/eps\n(fully polynomial approximation scheme). More precisely, our algorithm takes\ntime O(n^3 (1/eps) log (n/eps)). Our algorithm is based on dynamic programming.\nPreviously, randomized polynomial time approximation schemes were known first\nby Morris and Sinclair via Markov chain Monte Carlo techniques, and\nsubsequently by Dyer via dynamic programming and rejection sampling."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.1975v4", 
    "title": "Fast Approximation Algorithms for Cut-based Problems in Undirected   Graphs", 
    "arxiv-id": "1008.1975v4", 
    "author": "Aleksander Madry", 
    "publish": "2010-08-11T19:35:38Z", 
    "summary": "We present a general method of designing fast approximation algorithms for\ncut-based minimization problems in undirected graphs. In particular, we develop\na technique that given any such problem that can be approximated quickly on\ntrees, allows approximating it almost as quickly on general graphs while only\nlosing a poly-logarithmic factor in the approximation guarantee.\n  To illustrate the applicability of our paradigm, we focus our attention on\nthe undirected sparsest cut problem with general demands and the balanced\nseparator problem. By a simple use of our framework, we obtain poly-logarithmic\napproximation algorithms for these problems that run in time close to linear.\n  The main tool behind our result is an efficient procedure that decomposes\ngeneral graphs into simpler ones while approximately preserving the cut-flow\nstructure. This decomposition is inspired by the cut-based graph decomposition\nof R\\\"acke that was developed in the context of oblivious routing schemes, as\nwell as, by the construction of the ultrasparsifiers due to Spielman and Teng\nthat was employed to preconditioning symmetric diagonally-dominant matrices."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.3091v1", 
    "title": "2-FREE-FLOOD-IT is polynomial", 
    "arxiv-id": "1008.3091v1", 
    "author": "Aurelie Lagoutte", 
    "publish": "2010-08-18T13:56:12Z", 
    "summary": "We study a discrete diffusion process introduced in some combinatorial games\ncalled FLOODIT and MADVIRUS that can be played online and whose computational\ncomplexity has been recently studied by Arthur et al (FUN'2010). The flooding\ndynamics used in those games can be defined for any colored graph. It has been\nshown in a first report (in french, hal-00509488 on HAL archive) that studying\nthis dynamics directly on general graph is a valuable approach to understand\nits specificities and extract uncluttered key patterns or algorithms that can\nbe applied with success to particular cases like the square grid of FLOODIT or\nthe hexagonal grid of MADVIRUS, and many other classes of graphs. This report\nis the translation from french to english of the section in the french report\nshowing that the variant of the problem called 2-FREE-FLOOD-IT can be solved\nwith a polynomial algorithm, answering a question raised in the previous study\nof FLOODIT by Arthur et al."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.3216v1", 
    "title": "An approximation algorithm for the total cover problem", 
    "arxiv-id": "1008.3216v1", 
    "author": "Pooya Hatami", 
    "publish": "2010-08-19T04:51:15Z", 
    "summary": "We introduce a $2$-approximation algorithm for the minimum total covering\nnumber problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.3503v1", 
    "title": "Maximum Betweenness Centrality: Approximability and Tractable Cases", 
    "arxiv-id": "1008.3503v1", 
    "author": "Joachim Spoerhase", 
    "publish": "2010-08-20T13:52:33Z", 
    "summary": "The Maximum Betweenness Centrality problem (MBC) can be defined as follows.\nGiven a graph find a $k$-element node set $C$ that maximizes the probability of\ndetecting communication between a pair of nodes $s$ and $t$ chosen uniformly at\nrandom. It is assumed that the communication between $s$ and $t$ is realized\nalong a shortest $s$--$t$ path which is, again, selected uniformly at random.\nThe communication is detected if the communication path contains a node of $C$.\nRecently, Dolev et al. (2009) showed that MBC is NP-hard and gave a\n$(1-1/e)$-approximation using a greedy approach. We provide a reduction of MBC\nto Maximum Coverage that simplifies the analysis of the algorithm of Dolev et\nal. considerably. Our reduction allows us to obtain a new algorithm with the\nsame approximation ratio for a (generalized) budgeted version of MBC. We\nprovide tight examples showing that the analyses of both algorithms are best\npossible. Moreover, we prove that MBC is APX-complete and provide an exact\npolynomial-time algorithm for MBC on tree graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1008.3672v5", 
    "title": "Prediction strategies without loss", 
    "arxiv-id": "1008.3672v5", 
    "author": "Rina Panigrahy", 
    "publish": "2010-08-22T01:09:56Z", 
    "summary": "Consider a sequence of bits where we are trying to predict the next bit from\nthe previous bits. Assume we are allowed to say 'predict 0' or 'predict 1', and\nour payoff is +1 if the prediction is correct and -1 otherwise. We will say\nthat at each point in time the loss of an algorithm is the number of wrong\npredictions minus the number of right predictions so far. In this paper we are\ninterested in algorithms that have essentially zero (expected) loss over any\nstring at any point in time and yet have small regret with respect to always\npredicting 0 or always predicting 1. For a sequence of length $T$ our algorithm\nhas regret $14\\epsilon T $ and loss $2\\sqrt{T}e^{-\\epsilon^2 T} $ in\nexpectation for all strings. We show that the tradeoff between loss and regret\nis optimal up to constant factors.\n  Our techniques extend to the general setting of $N$ experts, where the\nrelated problem of trading off regret to the best expert for regret to the\n`special' expert has been studied by Even-Dar et al. (COLT'07). We obtain\nessentially zero loss with respect to the special expert and optimal\nloss/regret tradeoff, improving upon the results of Even-Dar et al and settling\nthe main question left open in their paper.\n  The strong loss bounds of the algorithm have some surprising consequences. A\nsimple iterative application of our algorithm gives essentially optimal regret\nbounds at multiple time scales, bounds with respect to $k$-shifting optima as\nwell as regret bounds with respect to higher norms of the input sequence."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_8", 
    "link": "http://arxiv.org/pdf/1008.4250v3", 
    "title": "Cluster Editing: Kernelization based on Edge Cuts", 
    "arxiv-id": "1008.4250v3", 
    "author": "Jianer Chen", 
    "publish": "2010-08-25T10:44:39Z", 
    "summary": "Kernelization algorithms for the {\\sc cluster editing} problem have been a\npopular topic in the recent research in parameterized computation. Thus far\nmost kernelization algorithms for this problem are based on the concept of {\\it\ncritical cliques}. In this paper, we present new observations and new\ntechniques for the study of kernelization algorithms for the {\\sc cluster\nediting} problem. Our techniques are based on the study of the relationship\nbetween {\\sc cluster editing} and graph edge-cuts. As an application, we\npresent an ${\\cal O}(n^2)$-time algorithm that constructs a $2k$ kernel for the\n{\\it weighted} version of the {\\sc cluster editing} problem. Our result meets\nthe best kernel size for the unweighted version for the {\\sc cluster editing}\nproblem, and significantly improves the previous best kernel of quadratic size\nfor the weighted version of the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_8", 
    "link": "http://arxiv.org/pdf/1008.4889v1", 
    "title": "The Geometry of Scheduling", 
    "arxiv-id": "1008.4889v1", 
    "author": "Kirk Pruhs", 
    "publish": "2010-08-28T20:00:24Z", 
    "summary": "We consider the following general scheduling problem: The input consists of n\njobs, each with an arbitrary release time, size, and a monotone function\nspecifying the cost incurred when the job is completed at a particular time.\nThe objective is to find a preemptive schedule of minimum aggregate cost. This\nproblem formulation is general enough to include many natural scheduling\nobjectives, such as weighted flow, weighted tardiness, and sum of flow squared.\nOur main result is a randomized polynomial-time algorithm with an approximation\nratio O(log log nP), where P is the maximum job size. We also give an O(1)\napproximation in the special case when all jobs have identical release times.\nThe main idea is to reduce this scheduling problem to a particular geometric\nset-cover problem which is then solved using the local ratio technique and\nVaradarajan's quasi-uniform sampling technique. This general algorithmic\napproach improves the best known approximation ratios by at least an\nexponential factor (and much more in some cases) for essentially all of the\nnontrivial common special cases of this problem. Our geometric interpretation\nof scheduling may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_8", 
    "link": "http://arxiv.org/pdf/1008.5356v1", 
    "title": "When LP is the Cure for Your Matching Woes: Improved Bounds for   Stochastic Matchings", 
    "arxiv-id": "1008.5356v1", 
    "author": "Atri Rudra", 
    "publish": "2010-08-31T16:36:56Z", 
    "summary": "Consider a random graph model where each possible edge $e$ is present\nindependently with some probability $p_e$. Given these probabilities, we want\nto build a large/heavy matching in the randomly generated graph. However, the\nonly way we can find out whether an edge is present or not is to query it, and\nif the edge is indeed present in the graph, we are forced to add it to our\nmatching. Further, each vertex $i$ is allowed to be queried at most $t_i$\ntimes. How should we adaptively query the edges to maximize the expected weight\nof the matching? We consider several matching problems in this general\nframework (some of which arise in kidney exchanges and online dating, and\nothers arise in modeling online advertisements); we give LP-rounding based\nconstant-factor approximation algorithms for these problems. Our main results\nare the following:\n  We give a 4 approximation for weighted stochastic matching on general graphs,\nand a 3 approximation on bipartite graphs. This answers an open question from\n[Chen etal ICALP 09]. Combining our LP-rounding algorithm with the natural\ngreedy algorithm, we give an improved 3.46 approximation for unweighted\nstochastic matching on general graphs.\n  We introduce a generalization of the stochastic online matching problem\n[Feldman etal FOCS 09] that also models preference-uncertainty and timeouts of\nbuyers, and give a constant factor approximation algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.knosys.2010.05.009", 
    "link": "http://arxiv.org/pdf/1009.0670v1", 
    "title": "Grammar-Based Geodesics in Semantic Networks", 
    "arxiv-id": "1009.0670v1", 
    "author": "Jennifer H. Watkins", 
    "publish": "2010-09-03T13:49:47Z", 
    "summary": "A geodesic is the shortest path between two vertices in a connected network.\nThe geodesic is the kernel of various network metrics including radius,\ndiameter, eccentricity, closeness, and betweenness. These metrics are the\nfoundation of much network research and thus, have been studied extensively in\nthe domain of single-relational networks (both in their directed and undirected\nforms). However, geodesics for single-relational networks do not translate\ndirectly to multi-relational, or semantic networks, where vertices are\nconnected to one another by any number of edge labels. Here, a more\nsophisticated method for calculating a geodesic is necessary. This article\npresents a technique for calculating geodesics in semantic networks with a\nfocus on semantic networks represented according to the Resource Description\nFramework (RDF). In this framework, a discrete \"walker\" utilizes an abstract\npath description called a grammar to determine which paths to include in its\ngeodesic calculation. The grammar-based model forms a general framework for\nstudying geodesic metrics in semantic networks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0783v1", 
    "title": "Extended h-Index Parameterized Data Structures for Computing Dynamic   Subgraph Statistics", 
    "arxiv-id": "1009.0783v1", 
    "author": "Lowell Trott", 
    "publish": "2010-09-03T22:58:10Z", 
    "summary": "We present techniques for maintaining subgraph frequencies in a dynamic\ngraph, using data structures that are parameterized in terms of h, the h-index\nof the graph. Our methods extend previous results of Eppstein and Spiro for\nmaintaining statistics for undirected subgraphs of size three to directed\nsubgraphs and to subgraphs of size four. For the directed case, we provide a\ndata structure to maintain counts for all 3-vertex induced subgraphs in O(h)\namortized time per update. For the undirected case, we maintain the counts of\nsize-four subgraphs in O(h^2) amortized time per update. These extensions\nenable a number of new applications in Bioinformatics and Social Networking\nresearch."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0806v1", 
    "title": "A Quartic Kernel for Pathwidth-One Vertex Deletion", 
    "arxiv-id": "1009.0806v1", 
    "author": "Yngve Villanger", 
    "publish": "2010-09-04T05:39:41Z", 
    "summary": "The pathwidth of a graph is a measure of how path-like the graph is. Given a\ngraph G and an integer k, the problem of finding whether there exist at most k\nvertices in G whose deletion results in a graph of pathwidth at most one is NP-\ncomplete. We initiate the study of the parameterized complexity of this\nproblem, parameterized by k. We show that the problem has a quartic\nvertex-kernel: We show that, given an input instance (G = (V, E), k); |V| = n,\nwe can construct, in polynomial time, an instance (G', k') such that (i) (G, k)\nis a YES instance if and only if (G', k') is a YES instance, (ii) G' has\nO(k^{4}) vertices, and (iii) k' \\leq k. We also give a fixed parameter\ntractable (FPT) algorithm for the problem that runs in O(7^{k} k \\cdot n^{2})\ntime."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0909v2", 
    "title": "Comparing Pedigree Graphs", 
    "arxiv-id": "1009.0909v2", 
    "author": "Richard M. Karp", 
    "publish": "2010-09-05T12:02:26Z", 
    "summary": "Pedigree graphs, or family trees, are typically constructed by an expensive\nprocess of examining genealogical records to determine which pairs of\nindividuals are parent and child. New methods to automate this process take as\ninput genetic data from a set of extant individuals and reconstruct ancestral\nindividuals. There is a great need to evaluate the quality of these methods by\ncomparing the estimated pedigree to the true pedigree.\n  In this paper, we consider two main pedigree comparison problems. The first\nis the pedigree isomorphism problem, for which we present a linear-time\nalgorithm for leaf-labeled pedigrees. The second is the pedigree edit distance\nproblem, for which we present 1) several algorithms that are fast and exact in\nvarious special cases, and 2) a general, randomized heuristic algorithm.\n  In the negative direction, we first prove that the pedigree isomorphism\nproblem is as hard as the general graph isomorphism problem, and that the\nsub-pedigree isomorphism problem is NP-hard. We then show that the pedigree\nedit distance problem is APX-hard in general and NP-hard on leaf-labeled\npedigrees.\n  We use simulated pedigrees to compare our edit-distance algorithms to each\nother as well as to a branch-and-bound algorithm that always finds an optimal\nsolution."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.1381v1", 
    "title": "A Branch-and-Reduce Algorithm for Finding a Minimum Independent   Dominating Set", 
    "arxiv-id": "1009.1381v1", 
    "author": "Mathieu Liedloff", 
    "publish": "2010-09-07T19:57:23Z", 
    "summary": "An independent dominating set D of a graph G = (V,E) is a subset of vertices\nsuch that every vertex in V \\ D has at least one neighbor in D and D is an\nindependent set, i.e. no two vertices of D are adjacent in G. Finding a minimum\nindependent dominating set in a graph is an NP-hard problem. Whereas it is hard\nto cope with this problem using parameterized and approximation algorithms,\nthere is a simple exact O(1.4423^n)-time algorithm solving the problem by\nenumerating all maximal independent sets. In this paper we improve the latter\nresult, providing the first non trivial algorithm computing a minimum\nindependent dominating set of a graph in time O(1.3569^n). Furthermore, we give\na lower bound of \\Omega(1.3247^n) on the worst-case running time of this\nalgorithm, showing that the running time analysis is almost tight."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.1697v1", 
    "title": "One method of storing information", 
    "arxiv-id": "1009.1697v1", 
    "author": "Oleg Titov", 
    "publish": "2010-09-09T07:41:22Z", 
    "summary": "Formulate the problem as follows. Split a file into n pieces so that it can\nbe restored without any m parts (1<=m<=n). Such problems are called problems\nsecret sharing. There exists a set of methods for solving such problems, but\nthey all require a fairly large number of calculations applied to the problem\nposed above. The proposed method does not require calculations, and requires\nonly the operations of the division of the file into equal (nearly equal) parts\nand gluing them in a certain order in one or more files."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2109v2", 
    "title": "Maximizing the Total Resolution of Graphs", 
    "arxiv-id": "1009.2109v2", 
    "author": "Antonios Symvonis", 
    "publish": "2010-09-10T21:46:39Z", 
    "summary": "A major factor affecting the readability of a graph drawing is its\nresolution. In the graph drawing literature, the resolution of a drawing is\neither measured based on the angles formed by consecutive edges incident to a\ncommon node (angular resolution) or by the angles formed at edge crossings\n(crossing resolution). In this paper, we evaluate both by introducing the\nnotion of \"total resolution\", that is, the minimum of the angular and crossing\nresolution. To the best of our knowledge, this is the first time where the\nproblem of maximizing the total resolution of a drawing is studied.\n  The main contribution of the paper consists of drawings of asymptotically\noptimal total resolution for complete graphs (circular drawings) and for\ncomplete bipartite graphs (2-layered drawings). In addition, we present and\nexperimentally evaluate a force-directed based algorithm that constructs\ndrawings of large total resolution."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2322v1", 
    "title": "Deterministic Online Call Control in Cellular Networks and Triangle-Free   Cellular Networks", 
    "arxiv-id": "1009.2322v1", 
    "author": "Yong Zhang", 
    "publish": "2010-09-13T08:36:19Z", 
    "summary": "Wireless Communication Networks based on Frequency Division Multiplexing (FDM\nin short) plays an important role in the field of communications, in which each\nrequest can be satisfied by assigning a frequency. To avoid interference, each\nassigned frequency must be different to the neighboring assigned frequencies.\nSince frequency is a scarce resource, the main problem in wireless networks is\nhow to fully utilize the given bandwidth of frequencies. In this paper, we\nconsider the online call control problem. Given a fixed bandwidth of\nfrequencies and a sequence of communication requests arrive over time, each\nrequest must be either satisfied immediately after its arrival by assigning an\navailable frequency, or rejected. The objective of call control problem is to\nmaximize the number of accepted requests. We study the asymptotic performance\nof this problem, i.e., the number of requests in the sequence and the bandwidth\nof frequencies are very large. In this paper, we give a 7/3-competitive\nalgorithm for call control problem in cellular network, improving the previous\n2.5-competitive result. Moreover, we investigate the triangle-free cellular\nnetwork, propose a 9/4-competitive algorithm and prove that the lower bound of\ncompetitive ratio is at least 5/3."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2452v1", 
    "title": "Facility Location with Client Latencies: Linear-Programming based   Techniques for Minimum-Latency Problems", 
    "arxiv-id": "1009.2452v1", 
    "author": "Chaitanya Swamy", 
    "publish": "2010-09-13T17:04:34Z", 
    "summary": "We introduce a problem that is a common generalization of the uncapacitated\nfacility location and minimum latency (ML) problems, where facilities need to\nbe opened to serve clients and also need to be sequentially activated before\nthey can provide service. Formally, we are given a set \\F of n facilities with\nfacility-opening costs {f_i}, a set of m clients, and connection costs {c_{ij}}\nspecifying the cost of assigning a client j to a facility i, a root node r\ndenoting the depot, and a time metric d on \\F\\cup{r}. Our goal is to open a\nsubset F of facilities, find a path P starting at r and spanning F to activate\nthe open facilities, and connect each client j to a facility \\phi(j)\\in F, so\nas to minimize \\sum_{i\\in F}f_i +\\sum_{clients j}(c_{\\phi(j),j}+t_j), where t_j\nis the time taken to reach \\phi(j) along path P. We call this the minimum\nlatency uncapacitated facility location (MLUFL) problem.\n  Our main result is an O(\\log n\\max{\\log n,\\log m})-approximation for MLUFL.\nWe also show that any improvement in this approximation guarantee, implies an\nimprovement in the (current-best) approximation factor for group Steiner tree.\nWe obtain constant approximations for two natural special cases of the problem:\n(a) related MLUFL (metric connection costs that are a scalar multiple of the\ntime metric); (b) metric uniform MLUFL (metric connection costs, unform\ntime-metric). Our LP-based methods are versatile and easily adapted to yield\napproximation guarantees for MLUFL in various more general settings, such as\n(i) when the latency-cost of a client is a function of the delay faced by the\nfacility to which it is connected; and (ii) the k-route version, where k\nvehicles are routed in parallel to activate the open facilities. Our LP-based\nunderstanding of MLUFL also offers some LP-based insights into ML, which we\nbelieve is a promising direction for obtaining improvements for ML."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2591v1", 
    "title": "Popularity at Minimum Cost", 
    "arxiv-id": "1009.2591v1", 
    "author": "Prajakta Nimbhorkar", 
    "publish": "2010-09-14T08:31:31Z", 
    "summary": "We consider an extension of the {\\em popular matching} problem in this paper.\nThe input to the popular matching problem is a bipartite graph G = (A U B,E),\nwhere A is a set of people, B is a set of items, and each person a belonging to\nA ranks a subset of items in an order of preference, with ties allowed. The\npopular matching problem seeks to compute a matching M* between people and\nitems such that there is no matching M where more people are happier with M\nthan with M*. Such a matching M* is called a popular matching. However, there\nare simple instances where no popular matching exists.\n  Here we consider the following natural extension to the above problem:\nassociated with each item b belonging to B is a non-negative price cost(b),\nthat is, for any item b, new copies of b can be added to the input graph by\npaying an amount of cost(b) per copy. When G does not admit a popular matching,\nthe problem is to \"augment\" G at minimum cost such that the new graph admits a\npopular matching. We show that this problem is NP-hard; in fact, it is NP-hard\nto approximate it within a factor of sqrt{n1}/2, where n1 is the number of\npeople. This problem has a simple polynomial time algorithm when each person\nhas a preference list of length at most 2. However, if we consider the problem\nof \"constructing\" a graph at minimum cost that admits a popular matching that\nmatches all people, then even with preference lists of length 2, the problem\nbecomes NP-hard. On the other hand, when the number of copies of each item is\n\"fixed\", we show that the problem of computing a minimum cost popular matching\nor deciding that no popular matching exists can be solved in O(mn1) time, where\nm is the number of edges."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3502v1", 
    "title": "Simultaneous Interval Graphs", 
    "arxiv-id": "1009.3502v1", 
    "author": "Anna Lubiw", 
    "publish": "2010-09-17T20:25:56Z", 
    "summary": "In a recent paper, we introduced the simultaneous representation problem\n(defined for any graph class C) and studied the problem for chordal,\ncomparability and permutation graphs. For interval graphs, the problem is\ndefined as follows. Two interval graphs G_1 and G_2, sharing some vertices I\n(and the corresponding induced edges), are said to be `simultaneous interval\ngraphs' if there exist interval representations R_1 and R_2 of G_1 and G_2,\nsuch that any vertex of I is mapped to the same interval in both R_1 and R_2.\nEquivalently, G_1 and G_2 are simultaneous interval graphs if there exist edges\nE' between G_1-I and G_2-I such that G_1 \\cup G_2 \\cup E' is an interval graph.\n  Simultaneous representation problems are related to simultaneous planar\nembeddings, and have applications in any situation where it is desirable to\nconsistently represent two related graphs, for example: interval graphs\ncapturing overlaps of DNA fragments of two similar organisms; or graphs\nconnected in time, where one is an updated version of the other.\n  In this paper we give an O(n^2*logn) time algorithm for recognizing\nsimultaneous interval graphs,where n = |G_1 \\cup G_2|. This result complements\nthe polynomial time algorithms for recognizing probe interval graphs and\nprovides an efficient algorithm for the interval graph sandwich problem for the\nspecial case where the set of optional edges induce a complete bipartite graph."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3594v2", 
    "title": "Center-based Clustering under Perturbation Stability", 
    "arxiv-id": "1009.3594v2", 
    "author": "Or Sheffet", 
    "publish": "2010-09-18T22:57:43Z", 
    "summary": "Clustering under most popular objective functions is NP-hard, even to\napproximate well, and so unlikely to be efficiently solvable in the worst case.\nRecently, Bilu and Linial \\cite{Bilu09} suggested an approach aimed at\nbypassing this computational barrier by using properties of instances one might\nhope to hold in practice. In particular, they argue that instances in practice\nshould be stable to small perturbations in the metric space and give an\nefficient algorithm for clustering instances of the Max-Cut problem that are\nstable to perturbations of size $O(n^{1/2})$. In addition, they conjecture that\ninstances stable to as little as O(1) perturbations should be solvable in\npolynomial time. In this paper we prove that this conjecture is true for any\ncenter-based clustering objective (such as $k$-median, $k$-means, and\n$k$-center). Specifically, we show we can efficiently find the optimal\nclustering assuming only stability to factor-3 perturbations of the underlying\nmetric in spaces without Steiner points, and stability to factor $2+\\sqrt{3}$\nperturbations for general metrics. In particular, we show for such instances\nthat the popular Single-Linkage algorithm combined with dynamic programming\nwill find the optimal clustering. We also present NP-hardness results under a\nweaker but related condition."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3809v2", 
    "title": "One, Two, Three and N Dimensional String Search Algorithms", 
    "arxiv-id": "1009.3809v2", 
    "author": "Ramesh C. Bagadi", 
    "publish": "2010-09-20T13:11:52Z", 
    "summary": "In this research endeavor, some Sequence Alignment Algorithms are detailed\nthat are useful for finding or comparing 1 dimensional (1-D), 2 dimensional\n(2-D), 3 dimensional (3-D) sequences in or against a parent or mother database\nwhich is 1 dimensional (1-D), 2 dimensional (2-D), 3 dimensional (3-D)\nsequence. Inner Product [1], [2] based schemes are used to lay down such\nalgorithms. Also,in this research, a Sequence Alignment Algorithms is detailed\nthat is useful for finding or comparing an N-Dimensional (N-D) sequence in or\nagainst a parent or mother database which N-Dimensional (N-D) sequence. Inner\nProduct [1], [2] based schemes are used to lay down such an algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4214v2", 
    "title": "A Versatile Algorithm to Generate Various Combinatorial Structures", 
    "arxiv-id": "1009.4214v2", 
    "author": "Rama B", 
    "publish": "2010-09-21T20:59:51Z", 
    "summary": "Algorithms to generate various combinatorial structures find tremendous\nimportance in computer science. In this paper, we begin by reviewing an\nalgorithm proposed by Rohl that generates all unique permutations of a list of\nelements which possibly contains repetitions, taking some or all of the\nelements at a time, in any imposed order. The algorithm uses an auxiliary array\nthat maintains the number of occurrences of each unique element in the input\nlist. We provide a proof of correctness of the algorithm. We then show how one\ncan efficiently generate other combinatorial structures like combinations,\nsubsets, n-Parenthesizations, derangements and integer partitions &\ncompositions with minor changes to the same algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4355v1", 
    "title": "The Sorting Buffer Problem is NP-hard", 
    "arxiv-id": "1009.4355v1", 
    "author": "Rene Sitters", 
    "publish": "2010-09-22T13:14:51Z", 
    "summary": "We consider the offline sorting buffer problem. The input is a sequence of\nitems of different types. All items must be processed one by one by a server.\nThe server is equipped with a random-access buffer of limited capacity which\ncan be used to rearrange items. The problem is to design a scheduling strategy\nthat decides upon the order in which items from the buffer are sent to the\nserver. Each type change incurs unit cost, and thus, the cost minimizing\nobjective is to minimize the total number of type changes for serving the\nentire sequence. This problem is motivated by various applications in\nmanufacturing processes and computer science, and it has attracted significant\nattention in the last few years. The main focus has been on online competitive\nalgorithms. Surprisingly little is known on the basic offline problem. In this\npaper, we show that the sorting buffer problem with uniform cost is NP-hard\nand, thus, close one of the most fundamental questions for the offline problem.\nOn the positive side, we give an O(1)-approximation algorithm when the\nscheduler is given a buffer only slightly larger than double the original size.\nWe also give a dynamic programming algorithm for the special case of buffer\nsize two that solves the problem exactly in linear time, improving on the\nstandard DP which runs in cubic time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4517v2", 
    "title": "Testing Simultaneous Planarity when the Common Graph is 2-Connected", 
    "arxiv-id": "1009.4517v2", 
    "author": "Anna Lubiw", 
    "publish": "2010-09-23T04:24:51Z", 
    "summary": "Two planar graphs G1 and G2 sharing some vertices and edges are\n`simultaneously planar' if they have planar drawings such that a shared vertex\n[edge] is represented by the same point [curve] in both drawings. It is an open\nproblem whether simultaneous planarity can be tested efficiently. We give a\nlinear-time algorithm to test simultaneous planarity when the two graphs share\na 2-connected subgraph. Our algorithm extends to the case of k planar graphs\nwhere each vertex [edge] is either common to all graphs or belongs to exactly\none of them."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4529v1", 
    "title": "A PTAS for Scheduling with Tree Assignment Restrictions", 
    "arxiv-id": "1009.4529v1", 
    "author": "Ulrich M. Schwarz", 
    "publish": "2010-09-23T07:12:49Z", 
    "summary": "Scheduling with assignment restrictions is an important special case of\nscheduling unrelated machines which has attracted much attention in the recent\npast. While a lower bound on approximability of 3/2 is known for its most\ngeneral setting, subclasses of the problem admit polynomial-time approximation\nschemes. This note provides a PTAS for tree-like hierarchical structures,\nimproving on a recent 4/3-approximation by Huo and Leung."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4830v3", 
    "title": "Improving PPSZ for 3-SAT using Critical Variables", 
    "arxiv-id": "1009.4830v3", 
    "author": "Dominik Scheder", 
    "publish": "2010-09-24T13:06:34Z", 
    "summary": "A critical variable of a satisfiable CNF formula is a variable that has the\nsame value in all satisfying assignments. Using a simple case distinction on\nthe fraction of critical variables of a CNF formula, we improve the running\ntime for 3-SAT from O(1.32216^n) by Rolf [2006] to O(1.32153^n). Using a\ndifferent approach, Iwama et al. [2010] very recently achieved a running time\nof O(1.32113^n). Our method nicely combines with theirs, yielding the currently\nfastest known algorithm with running time O(1.32065^n). We also improve the\nbound for 4-SAT from O(1.47390^n) [Iwama, Tamaki 2004] to O(1.46928^n), where\nO(1.46981^n) can be obtained using the methods of [Iwama, Tamaki 2004] and\n[Rolf 2006]."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4880v1", 
    "title": "An Efficient Implementation of the Robust Tabu Search Heuristic for   Sparse Quadratic Assignment Problems", 
    "arxiv-id": "1009.4880v1", 
    "author": "Gerald Paul", 
    "publish": "2010-09-24T16:05:29Z", 
    "summary": "We propose and develop an efficient implementation of the robust tabu search\nheuristic for sparse quadratic assignment problems. The traditional\nimplementation of the heuristic applicable to all quadratic assignment problems\nis of O(N^2) complexity per iteration for problems of size N. Using multiple\npriority queues to determine the next best move instead of scanning all\npossible moves, and using adjacency lists to minimize the operations needed to\ndetermine the cost of moves, we reduce the asymptotic complexity per iteration\nto O(N log N ). For practical sized problems, the complexity is O(N)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.5143v2", 
    "title": "FAST: Kernelization based on Graph Modular Decomposition", 
    "arxiv-id": "1009.5143v2", 
    "author": "Jianer Chen", 
    "publish": "2010-09-27T02:29:59Z", 
    "summary": "Kernelization algorithms, usually a preprocessing step before other more\ntraditional algorithms, are very special in the sense that they return\n(reduced) instances, instead of final results. This characteristic excludes the\nfreedom of applying a kernelization algorithm for the weighted version of a\nproblem to its unweighted instances. Thus with only very few special cases,\nkernelization algorithms have to be studied separately for weigthed and\nunweighted versions of a single problem. {\\sc feedback arc set on tournament}\nis currently a very popular problem in recent research of parameterized, as\nwell as approximation computation, and its wide applications in many areas make\nit appear in all top conferences. The theory of graph modular decompositions is\na general approach in the study of graph structures, which only had its\nsurfaces touched in previous work on kernelization algorithms of {\\sc feedback\narc set on tournament}. In this paper, we study further properties of graph\nmodular decompositions and apply them to obtain the first linear kernel for the\nunweighted {\\sc feedback arc set on tournament} problem, which only admits\nlinear kernel in its weighted version, while quadratic kernel for the\nunweighted."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.5168v2", 
    "title": "Efficient Clustering with Limited Distance Information", 
    "arxiv-id": "1009.5168v2", 
    "author": "Yu Xia", 
    "publish": "2010-09-27T06:29:35Z", 
    "summary": "Given a point set S and an unknown metric d on S, we study the problem of\nefficiently partitioning S into k clusters while querying few distances between\nthe points. In our model we assume that we have access to one versus all\nqueries that given a point s in S return the distances between s and all other\npoints. We show that given a natural assumption about the structure of the\ninstance, we can efficiently find an accurate clustering using only O(k)\ndistance queries. Our algorithm uses an active selection strategy to choose a\nsmall set of points that we call landmarks, and considers only the distances\nbetween landmarks and other points to produce a clustering. We use our\nalgorithm to cluster proteins by sequence similarity. This setting nicely fits\nour model because we can use a fast sequence database search program to query a\nsequence against an entire dataset. We conduct an empirical study that shows\nthat even though we query a small fraction of the distances between the points,\nwe produce clusterings that are close to a desired clustering given by manual\nclassification."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5227v1", 
    "title": "The Straight-Line RAC Drawing Problem is NP-Hard", 
    "arxiv-id": "1009.5227v1", 
    "author": "Antonios Symvonis", 
    "publish": "2010-09-27T11:33:39Z", 
    "summary": "Recent cognitive experiments have shown that the negative impact of an edge\ncrossing on the human understanding of a graph drawing, tends to be eliminated\nin the case where the crossing angles are greater than 70 degrees. This\nmotivated the study of RAC drawings, in which every pair of crossing edges\nintersects at right angle. In this work, we demonstrate a class of graphs with\nunique RAC combinatorial embedding and we employ members of this class in order\nto show that it is NP-hard to decide whether a graph admits a straight-line RAC\ndrawing."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5538v1", 
    "title": "Priority Queues with Multiple Time Fingers", 
    "arxiv-id": "1009.5538v1", 
    "author": "John Iacono", 
    "publish": "2010-09-28T11:48:57Z", 
    "summary": "A priority queue is presented that supports the operations insert and\nfind-min in worst-case constant time, and delete and delete-min on element x in\nworst-case O(lg(min{w_x, q_x}+2)) time, where w_x (respectively q_x) is the\nnumber of elements inserted after x (respectively before x) and are still\npresent at the time of the deletion of x. Our priority queue then has both the\nworking-set and the queueish properties, and more strongly it satisfies these\nproperties in the worst-case sense. We also define a new distribution-sensitive\nproperty---the time-finger property, which encapsulates and generalizes both\nthe working-set and queueish properties, and present a priority queue that\nsatisfies this property.\n  In addition, we prove a strong implication that the working-set property is\nequivalent to the unified bound (which is the minimum per operation among the\nstatic finger, static optimality, and the working-set bounds). This latter\nresult is of tremendous interest by itself as it had gone unnoticed since the\nintroduction of such bounds by Sleater and Tarjan [JACM 1985]. Accordingly, our\npriority queue satisfies other distribution-sensitive properties as the static\nfinger, static optimality, and the unified bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5787v1", 
    "title": "List Factoring and Relative Worst Order Analysis", 
    "arxiv-id": "1009.5787v1", 
    "author": "Kim S. Larsen", 
    "publish": "2010-09-29T07:17:38Z", 
    "summary": "Relative worst order analysis is a supplement or alternative to competitive\nanalysis which has been shown to give results more in accordance with observed\nbehavior of online algorithms for a range of different online problems. The\ncontribution of this paper is twofold. First, it adds the static list accessing\nproblem to the collection of online problems where relative worst order\nanalysis gives better results. Second, and maybe more interesting, it adds the\nnon-trivial supplementary proof technique of list factoring to the theoretical\ntoolbox for relative worst order analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5791v1", 
    "title": "Fast Pseudo-Random Fingerprints", 
    "arxiv-id": "1009.5791v1", 
    "author": "Ely Porat", 
    "publish": "2010-09-29T07:32:27Z", 
    "summary": "We propose a method to exponentially speed up computation of various\nfingerprints, such as the ones used to compute similarity and rarity in massive\ndata sets. Rather then maintaining the full stream of $b$ items of a universe\n$[u]$, such methods only maintain a concise fingerprint of the stream, and\nperform computations using the fingerprints. The computations are done\napproximately, and the required fingerprint size $k$ depends on the desired\naccuracy $\\epsilon$ and confidence $\\delta$. Our technique maintains a single\nbit per hash function, rather than a single integer, thus requiring a\nfingerprint of length $k = O(\\frac{\\ln \\frac{1}{\\delta}}{\\epsilon^2})$ bits,\nrather than $O(\\log u \\cdot \\frac{\\ln \\frac{1}{\\delta}}{\\epsilon^2})$ bits\nrequired by previous approaches. The main advantage of the fingerprints we\npropose is that rather than computing the fingerprint of a stream of $b$ items\nin time of $O(b \\cdot k)$, we can compute it in time $O(b \\log k)$. Thus this\nallows an exponential speedup for the fingerprint construction, or\nalternatively allows achieving a much higher accuracy while preserving\ncomputation time. Our methods rely on a specific family of pseudo-random hashes\nfor which we can quickly locate hashes resulting in small values."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5863v1", 
    "title": "LRM-Trees: Compressed Indices, Adaptive Sorting, and Compressed   Permutations", 
    "arxiv-id": "1009.5863v1", 
    "author": "Johannes Fischer", 
    "publish": "2010-09-29T12:28:52Z", 
    "summary": "LRM-Trees are an elegant way to partition a sequence of values into sorted\nconsecutive blocks, and to express the relative position of the first element\nof each block within a previous block. They were used to encode ordinal trees\nand to index integer arrays in order to support range minimum queries on them.\nWe describe how they yield many other convenient results in a variety of areas,\nfrom data structures to algorithms: some compressed succinct indices for range\nminimum queries; a new adaptive sorting algorithm; and a compressed succinct\ndata structure for permutations supporting direct and indirect application in\ntime all the shortest as the permutation is compressible."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0141v1", 
    "title": "L1 Projections with Box Constraints", 
    "arxiv-id": "1010.0141v1", 
    "author": "Jing Xiao", 
    "publish": "2010-09-30T18:38:11Z", 
    "summary": "We study the L1 minimization problem with additional box constraints. We\nmotivate the problem with two different views of optimality considerations. We\nlook into imposing such constraints in projected gradient techniques and\npropose a worst case linear time algorithm to perform such projections. We\ndemonstrate the merits and effectiveness of our algorithms on synthetic as well\nas real experiments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0157v1", 
    "title": "Comparative Performance of Tabu Search and Simulated Annealing   Heuristics for the Quadratic Assignment Problem", 
    "arxiv-id": "1010.0157v1", 
    "author": "Gerald Paul", 
    "publish": "2010-10-01T13:29:22Z", 
    "summary": "For almost two decades the question of whether tabu search (TS) or simulated\nannealing (SA) performs better for the quadratic assignment problem has been\nunresolved. To answer this question satisfactorily, we compare performance at\nvarious values of targeted solution quality, running each heuristic at its\noptimal number of iterations for each target. We find that for a number of\nvaried problem instances, SA performs better for higher quality targets while\nTS performs better for lower quality targets."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0401v1", 
    "title": "Oblivious Buy-at-Bulk in Planar Graphs", 
    "arxiv-id": "1010.0401v1", 
    "author": "S. S. Iyengar", 
    "publish": "2010-10-03T13:43:00Z", 
    "summary": "In the oblivious buy-at-bulk network design problem in a graph, the task is\nto compute a fixed set of paths for every pair of source-destinations in the\ngraph, such that any set of demands can be routed along these paths. The\ndemands could be aggregated at intermediate edges where the fusion-cost is\nspecified by a canonical (non-negative concave) function $f$. We give a novel\nalgorithm for planar graphs which is oblivious with respect to the demands, and\nis also oblivious with respect to the fusion function $f$. The algorithm is\ndeterministic and computes the fixed set of paths in polynomial time, and\nguarantees a $O(\\log n)$ approximation ratio for any set of demands and any\ncanonical fusion function $f$, where $n$ is the number of nodes. The algorithm\nis asymptotically optimal, since it is known that this problem cannot be\napproximated with better than $\\Omega(\\log n)$ ratio. To our knowledge, this is\nthe first tight analysis for planar graphs, and improves the approximation\nratio by a factor of $\\log n$ with respect to previously known results."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0406v1", 
    "title": "Oblivious Algorithms for the Maximum Directed Cut Problem", 
    "arxiv-id": "1010.0406v1", 
    "author": "Shlomo Jozeph", 
    "publish": "2010-10-03T14:05:40Z", 
    "summary": "This paper introduces a special family of randomized algorithms for Max DICUT\nthat we call oblivious algorithms. Let the bias of a vertex be the ratio\nbetween the total weight of its outgoing edges and the total weight of all its\nedges. An oblivious algorithm selects at random in which side of the cut to\nplace a vertex v, with probability that only depends on the bias of v,\nindependently of other vertices. The reader may observe that the algorithm that\nignores the bias and chooses each side with probability 1/2 has an\napproximation ratio of 1/4, whereas no oblivious algorithm can have an\napproximation ratio better than 1/2 (with an even directed cycle serving as a\nnegative example). We attempt to characterize the best approximation ratio\nachievable by oblivious algorithms, and present results that are nearly tight.\nThe paper also discusses natural extensions of the notion of oblivious\nalgorithms, and extensions to the more general problem of Max 2-AND."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0809v1", 
    "title": "Engineering Time-dependent One-To-All Computation", 
    "arxiv-id": "1010.0809v1", 
    "author": "Robert Geisberger", 
    "publish": "2010-10-05T09:20:50Z", 
    "summary": "Very recently a new algorithm to the nonnegative single-source shortest path\nproblem on road networks has been discovered. It is very cache-efficient, but\nonly on static road networks. We show how to augment it to the time-dependent\nscenario. The advantage if the new approach is that it settles nodes, even for\na profile query, by scanning all downward edges. We improve the scanning of the\ndownward edges with techniques developed for time-dependent many-to-many\ncomputations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.1047v1", 
    "title": "Cut-Matching Games on Directed Graphs", 
    "arxiv-id": "1010.1047v1", 
    "author": "Anand Louis", 
    "publish": "2010-10-06T02:14:09Z", 
    "summary": "We give O(log^2 n)-approximation algorithm based on the cut-matching\nframework of [10, 13, 14] for computing the sparsest cut on directed graphs.\nOur algorithm uses only O(log^2 n) single commodity max-flow computations and\nthus breaks the multicommodity-flow barrier for computing the sparsest cut on\ndirected graphs"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.2833v1", 
    "title": "Improved Complexity Bound of Vertex Cover for Low degree Graph", 
    "arxiv-id": "1010.2833v1", 
    "author": "Weiwei Cao", 
    "publish": "2010-10-14T06:16:46Z", 
    "summary": "In this paper, we use a new method to decrease the parameterized complexity\nbound for finding the minimum vertex cover of connected max-degree-3 undirected\ngraphs. The key operation of this method is reduction of the size of a\nparticular subset of edges which we introduce in this paper and is called as\n\"real-cycle\" subset. Using \"real-cycle\" reductions alone we compute a\ncomplexity bound $O(1.15855^k)$ where $k$ is size of the optimal vertex cover.\nCombined with other techniques, the complexity bound can be further improved to\nbe $O(1.1504^k)$. This is currently the best complexity bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.2885v1", 
    "title": "Improved approximations for robust mincut and shortest path", 
    "arxiv-id": "1010.2885v1", 
    "author": "Mikko Sysikaski", 
    "publish": "2010-10-14T11:52:33Z", 
    "summary": "In two-stage robust optimization the solution to a problem is built in two\nstages: In the first stage a partial, not necessarily feasible, solution is\nexhibited. Then the adversary chooses the \"worst\" scenario from a predefined\nset of scenarios. In the second stage, the first-stage solution is extended to\nbecome feasible for the chosen scenario. The costs at the second stage are\nlarger than at the first one, and the objective is to minimize the total cost\npaid in the two stages.\n  We give a 2-approximation algorithm for the robust mincut problem and a\n({\\gamma}+2)-approximation for the robust shortest path problem, where {\\gamma}\nis the approximation ratio for the Steiner tree. This improves the factors\n(1+\\sqrt2) and 2({\\gamma}+2) from [Golovin, Goyal and Ravi. Pay today for a\nrainy day: Improved approximation algorithms for demand-robust min-cut and\nshortest path problems. STACS 2006]. In addition, our solution for robust\nshortest path is simpler and more efficient than the earlier ones; this is\nachieved by a more direct algorithm and analysis, not using some of the\nstandard demand-robust optimization techniques."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.3633v3", 
    "title": "Fixed-parameter tractability of multicut parameterized by the size of   the cutset", 
    "arxiv-id": "1010.3633v3", 
    "author": "Igor Razgon", 
    "publish": "2010-10-18T15:42:32Z", 
    "summary": "Given an undirected graph $G$, a collection $\\{(s_1,t_1),..., (s_k,t_k)\\}$ of\npairs of vertices, and an integer $p$, the Edge Multicut problem ask if there\nis a set $S$ of at most $p$ edges such that the removal of $S$ disconnects\nevery $s_i$ from the corresponding $t_i$. Vertex Multicut is the analogous\nproblem where $S$ is a set of at most $p$ vertices. Our main result is that\nboth problems can be solved in time $2^{O(p^3)}... n^{O(1)}$, i.e.,\nfixed-parameter tractable parameterized by the size $p$ of the cutset in the\nsolution. By contrast, it is unlikely that an algorithm with running time of\nthe form $f(p)... n^{O(1)}$ exists for the directed version of the problem, as\nwe show it to be W[1]-hard parameterized by the size of the cutset."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2010.08.010", 
    "link": "http://arxiv.org/pdf/1010.3685v1", 
    "title": "The set of realizations of a max-plus linear sequence is semi-polyhedral", 
    "arxiv-id": "1010.3685v1", 
    "author": "Natacha Portier", 
    "publish": "2010-10-18T19:09:11Z", 
    "summary": "We show that the set of realizations of a given dimension of a max-plus\nlinear sequence is a finite union of polyhedral sets, which can be computed\nfrom any realization of the sequence. This yields an (expensive) algorithm to\nsolve the max-plus minimal realization problem. These results are derived from\ngeneral facts on rational expressions over idempotent commutative semirings: we\nshow more generally that the set of values of the coefficients of a commutative\nrational expression in one letter that yield a given max-plus linear sequence\nis a semi-algebraic set in the max-plus sense. In particular, it is a finite\nunion of polyhedral sets."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2010.08.010", 
    "link": "http://arxiv.org/pdf/1010.4108v1", 
    "title": "Towards an SDP-based Approach to Spectral Methods: A Nearly-Linear-Time   Algorithm for Graph Partitioning and Decomposition", 
    "arxiv-id": "1010.4108v1", 
    "author": "Nisheeth K. Vishnoi", 
    "publish": "2010-10-20T06:37:28Z", 
    "summary": "In this paper, we consider the following graph partitioning problem: The\ninput is an undirected graph $G=(V,E),$ a balance parameter $b \\in (0,1/2]$ and\na target conductance value $\\gamma \\in (0,1).$ The output is a cut which, if\nnon-empty, is of conductance at most $O(f),$ for some function $f(G, \\gamma),$\nand which is either balanced or well correlated with all cuts of conductance at\nmost $\\gamma.$ Spielman and Teng gave an $\\tilde{O}(|E|/\\gamma^{2})$-time\nalgorithm for $f= \\sqrt{\\gamma \\log^{3}|V|}$ and used it to decompose graphs\ninto a collection of near-expanders. We present a new spectral algorithm for\nthis problem which runs in time $\\tilde{O}(|E|/\\gamma)$ for $f=\\sqrt{\\gamma}.$\nOur result yields the first nearly-linear time algorithm for the classic\nBalanced Separator problem that achieves the asymptotically optimal\napproximation guarantee for spectral methods. Our method has the advantage of\nbeing conceptually simple and relies on a primal-dual semidefinite-programming\nSDP approach. We first consider a natural SDP relaxation for the Balanced\nSeparator problem. While it is easy to obtain from this SDP a certificate of\nthe fact that the graph has no balanced cut of conductance less than $\\gamma,$\nsomewhat surprisingly, we can obtain a certificate for the stronger correlation\ncondition. This is achieved via a novel separation oracle for our SDP and by\nappealing to Arora and Kale's framework to bound the running time. Our result\ncontains technical ingredients that may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.4110v2", 
    "title": "Energy-Efficient Multiprocessor Scheduling for Flow Time and Makespan", 
    "arxiv-id": "1010.4110v2", 
    "author": "Rui Fan", 
    "publish": "2010-10-20T06:48:46Z", 
    "summary": "We consider energy-efficient scheduling on multiprocessors, where the speed\nof each processor can be individually scaled, and a processor consumes power\n$s^{\\alpha}$ when running at speed $s$, for $\\alpha>1$. A scheduling algorithm\nneeds to decide at any time both processor allocations and processor speeds for\na set of parallel jobs with time-varying parallelism. The objective is to\nminimize the sum of the total energy consumption and certain performance\nmetric, which in this paper includes total flow time and makespan. For both\nobjectives, we present instantaneous parallelism clairvoyant (IP-clairvoyant)\nalgorithms that are aware of the instantaneous parallelism of the jobs at any\ntime but not their future characteristics, such as remaining parallelism and\nwork. For total flow time plus energy, we present an $O(1)$-competitive\nalgorithm, which significantly improves upon the best known non-clairvoyant\nalgorithm and is the first constant competitive result on multiprocessor speed\nscaling for parallel jobs. In the case of makespan plus energy, which is\nconsidered for the first time in the literature, we present an\n$O(\\ln^{1-1/\\alpha}P)$-competitive algorithm, where $P$ is the total number of\nprocessors. We show that this algorithm is asymptotically optimal by providing\na matching lower bound. In addition, we also study non-clairvoyant scheduling\nfor total flow time plus energy, and present an algorithm that achieves $O(\\ln\nP)$-competitive for jobs with arbitrary release time and\n$O(\\ln^{1/\\alpha}P)$-competitive for jobs with identical release time. Finally,\nwe prove an $\\Omega(\\ln^{1/\\alpha}P)$ lower bound on the competitive ratio of\nany non-clairvoyant algorithm, matching the upper bound of our algorithm for\njobs with identical release time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.4925v1", 
    "title": "Property Testing via Set-Theoretic Operations", 
    "arxiv-id": "1010.4925v1", 
    "author": "Ning Xie", 
    "publish": "2010-10-24T04:02:45Z", 
    "summary": "Given two testable properties $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$, under\nwhat conditions are the union, intersection or set-difference of these two\nproperties also testable? We initiate a systematic study of these basic\nset-theoretic operations in the context of property testing. As an application,\nwe give a conceptually different proof that linearity is testable, albeit with\nmuch worse query complexity. Furthermore, for the problem of testing\ndisjunction of linear functions, which was previously known to be one-sided\ntestable with a super-polynomial query complexity, we give an improved analysis\nand show it has query complexity $O(1/\\eps^2)$, where $\\eps$ is the distance\nparameter."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5197v3", 
    "title": "Multicut is FPT", 
    "arxiv-id": "1010.5197v3", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2010-10-25T17:18:00Z", 
    "summary": "Let $G=(V,E)$ be a graph on $n$ vertices and $R$ be a set of pairs of\nvertices in $V$ called \\emph{requests}. A \\emph{multicut} is a subset $F$ of\n$E$ such that every request $xy$ of $R$ is cut by $F$, \\i.e. every $xy$-path of\n$G$ intersects $F$. We show that there exists an $O(f(k)n^c)$ algorithm which\ndecides if there exists a multicut of size at most $k$. In other words, the\n\\M{} problem parameterized by the solution size $k$ is Fixed-Parameter\nTractable. The proof extends to vertex multicuts."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5717v1", 
    "title": "PPZ For More Than Two Truth Values - An Algorithm for Constraint   Satisfaction Problems", 
    "arxiv-id": "1010.5717v1", 
    "author": "Dominik Scheder", 
    "publish": "2010-10-27T15:38:24Z", 
    "summary": "We analyze the so-called ppz algorithm for (d,k)-CSP problems for general\nvalues of d (number of values a variable can take) and k (number of literals\nper constraint). To analyze its success probability, we prove a correlation\ninequality for submodular functions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5890v1", 
    "title": "Python for education: the exact cover problem", 
    "arxiv-id": "1010.5890v1", 
    "author": "Andrzej Kapanowski", 
    "publish": "2010-10-28T08:53:26Z", 
    "summary": "Python implementation of Algorithm X by Knuth is presented. Algorithm X finds\nall solutions to the exact cover problem. The exemplary results for\npentominoes, Latin squares and Sudoku are given."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1010.5937v1", 
    "title": "Upward Point-Set Embeddability", 
    "arxiv-id": "1010.5937v1", 
    "author": "Antonios Symvonis", 
    "publish": "2010-10-28T12:19:53Z", 
    "summary": "We study the problem of Upward Point-Set Embeddability, that is the problem\nof deciding whether a given upward planar digraph $D$ has an upward planar\nembedding into a point set $S$. We show that any switch tree admits an upward\nplanar straight-line embedding into any convex point set. For the class of\n$k$-switch trees, that is a generalization of switch trees (according to this\ndefinition a switch tree is a $1$-switch tree), we show that not every\n$k$-switch tree admits an upward planar straight-line embedding into any convex\npoint set, for any $k \\geq 2$. Finally we show that the problem of Upward\nPoint-Set Embeddability is NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1010.5974v1", 
    "title": "Feedback Vertex Set in Mixed Graphs", 
    "arxiv-id": "1010.5974v1", 
    "author": "Daniel Lokshtanov", 
    "publish": "2010-10-28T14:04:50Z", 
    "summary": "A mixed graph is a graph with both directed and undirected edges. We present\nan algorithm for deciding whether a given mixed graph on $n$ vertices contains\na feedback vertex set (FVS) of size at most $k$, in time $2^{O(k)}k! O(n^4)$.\nThis is the first fixed parameter tractable algorithm for FVS that applies to\nboth directed and undirected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.0108v11", 
    "title": "An Active Learning Algorithm for Ranking from Pairwise Preferences with   an Almost Optimal Query Complexity", 
    "arxiv-id": "1011.0108v11", 
    "author": "Nir Ailon", 
    "publish": "2010-10-30T21:47:19Z", 
    "summary": "We study the problem of learning to rank from pairwise preferences, and solve\na long-standing open problem that has led to development of many heuristics but\nno provable results for our particular problem. Given a set $V$ of $n$\nelements, we wish to linearly order them given pairwise preference labels. A\npairwise preference label is obtained as a response, typically from a human, to\nthe question \"which if preferred, u or v?$ for two elements $u,v\\in V$. We\nassume possible non-transitivity paradoxes which may arise naturally due to\nhuman mistakes or irrationality. The goal is to linearly order the elements\nfrom the most preferred to the least preferred, while disagreeing with as few\npairwise preference labels as possible. Our performance is measured by two\nparameters: The loss and the query complexity (number of pairwise preference\nlabels we obtain). This is a typical learning problem, with the exception that\nthe space from which the pairwise preferences is drawn is finite, consisting of\n${n\\choose 2}$ possibilities only. We present an active learning algorithm for\nthis problem, with query bounds significantly beating general (non active)\nbounds for the same error guarantee, while almost achieving the information\ntheoretical lower bound. Our main construct is a decomposition of the input\ns.t. (i) each block incurs high loss at optimum, and (ii) the optimal solution\nrespecting the decomposition is not much worse than the true opt. The\ndecomposition is done by adapting a recent result by Kenyon and Schudy for a\nrelated combinatorial optimization problem to the query efficient setting. We\nthus settle an open problem posed by learning-to-rank theoreticians and\npractitioners: What is a provably correct way to sample preference labels? To\nfurther show the power and practicality of our solution, we show how to use it\nin concert with an SVM relaxation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.0531v1", 
    "title": "A better tester for bipartiteness?", 
    "arxiv-id": "1011.0531v1", 
    "author": "Fan Li", 
    "publish": "2010-11-02T07:48:15Z", 
    "summary": "Alon and Krivelevich (SIAM J. Discrete Math. 15(2): 211-227 (2002)) show that\nif a graph is {\\epsilon}-far from bipartite, then the subgraph induced by a\nrandom subset of O(1/{\\epsilon}) vertices is bipartite with high probability.\nWe conjecture that the induced subgraph is {\\Omega}~({\\epsilon})-far from\nbipartite with high probability. Gonen and Ron (RANDOM 2007) proved this\nconjecture in the case when the degrees of all vertices are at most\nO({\\epsilon}n). We give a more general proof that works for any d-regular (or\nalmost d-regular) graph for arbitrary degree d. Assuming this conjecture, we\nprove that bipartiteness is testable with one-sided error in time\nO(1/{\\epsilon}^c), where c is a constant strictly smaller than two, improving\nupon the tester of Alon and Krivelevich. As it is known that non-adaptive\ntesters for bipartiteness require {\\Omega}(1/{\\epsilon}^2) queries (Bogdanov\nand Trevisan, CCC 2004), our result shows, assuming the conjecture, that\nadaptivity helps in testing bipartiteness."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1168v2", 
    "title": "Santa Claus Schedules Jobs on Unrelated Machines", 
    "arxiv-id": "1011.1168v2", 
    "author": "Ola Svensson", 
    "publish": "2010-11-04T14:22:11Z", 
    "summary": "One of the classic results in scheduling theory is the 2-approximation\nalgorithm by Lenstra, Shmoys, and Tardos for the problem of scheduling jobs to\nminimize makespan on unrelated machines, i.e., job j requires time p_{ij} if\nprocessed on machine i. More than two decades after its introduction it is\nstill the algorithm of choice even in the restricted model where processing\ntimes are of the form p_{ij} in {p_j, \\infty}. This problem, also known as the\nrestricted assignment problem, is NP-hard to approximate within a factor less\nthan 1.5 which is also the best known lower bound for the general version.\n  Our main result is a polynomial time algorithm that estimates the optimal\nmakespan of the restricted assignment problem within a factor 33/17 + \\epsilon\n\\approx 1.9412 + \\epsilon, where \\epsilon > 0 is an arbitrarily small constant.\nThe result is obtained by upper bounding the integrality gap of a certain\nstrong linear program, known as configuration LP, that was previously\nsuccessfully used for the related Santa Claus problem. Similar to the strongest\nanalysis for that problem our proof is based on a local search algorithm that\nwill eventually find a schedule of the mentioned approximation guarantee, but\nis not known to converge in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1202v1", 
    "title": "Hardness and Approximation of The Asynchronous Border Minimization   Problem", 
    "arxiv-id": "1011.1202v1", 
    "author": "Fencol C. C. Yung", 
    "publish": "2010-11-04T16:25:26Z", 
    "summary": "We study a combinatorial problem arising from microarrays synthesis. The\nsynthesis is done by a light-directed chemical process. The objective is to\nminimize unintended illumination that may contaminate the quality of\nexperiments. Unintended illumination is measured by a notion called border\nlength and the problem is called Border Minimization Problem (BMP). The\nobjective of the BMP is to place a set of probe sequences in the array and find\nan embedding (deposition of nucleotides/residues to the array cells) such that\nthe sum of border length is minimized. A variant of the problem, called P-BMP,\nis that the placement is given and the concern is simply to find the embedding.\nApproximation algorithms have been previously proposed for the problem but it\nis unknown whether the problem is NP-hard or not. In this paper, we give a\nthorough study of different variations of BMP by giving NP-hardness proofs and\nimproved approximation algorithms. We show that P-BMP, 1D-BMP, and BMP are all\nNP-hard. Contrast with the previous result that 1D-P-BMP is polynomial time\nsolvable, the interesting implications include (i) the array dimension (1D or\n2D) differentiates the complexity of P-BMP; (ii) for 1D array, whether\nplacement is given differentiates the complexity of BMP; (iii) BMP is NP-hard\nregardless of the dimension of the array. Another contribution of the paper is\nimproving the approximation for BMP from $O(n^{1/2} \\log^2 n)$ to $O(n^{1/4}\n\\log^2 n)$, where $n$ is the total number of sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1337v1", 
    "title": "Optimal Binary Search Trees with Near Minimal Height", 
    "arxiv-id": "1011.1337v1", 
    "author": "Peter Becker", 
    "publish": "2010-11-05T08:16:42Z", 
    "summary": "Suppose we have n keys, n access probabilities for the keys, and n+1 access\nprobabilities for the gaps between the keys. Let h_min(n) be the minimal height\nof a binary search tree for n keys. We consider the problem to construct an\noptimal binary search tree with near minimal height, i.e.\\ with height h <=\nh_min(n) + Delta for some fixed Delta. It is shown, that for any fixed Delta\noptimal binary search trees with near minimal height can be constructed in time\nO(n^2). This is as fast as in the unrestricted case.\n  So far, the best known algorithms for the construction of height-restricted\noptimal binary search trees have running time O(L n^2), whereby L is the\nmaximal permitted height. Compared to these algorithms our algorithm is at\nleast faster by a factor of log n, because L is lower bounded by log n."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1708v2", 
    "title": "CRAM: Compressed Random Access Memory", 
    "arxiv-id": "1011.1708v2", 
    "author": "Wing-Kin Sung", 
    "publish": "2010-11-08T04:10:42Z", 
    "summary": "We present a new data structure called the \\emph{Compressed Random Access\nMemory} (CRAM) that can store a dynamic string $T$ of characters, e.g.,\nrepresenting the memory of a computer, in compressed form while achieving\nasymptotically almost-optimal bounds (in terms of empirical entropy) on the\ncompression ratio. It allows short substrings of $T$ to be decompressed and\nretrieved efficiently and, significantly, characters at arbitrary positions of\n$T$ to be modified quickly during execution \\emph{without decompressing the\nentire string}. This can be regarded as a new type of data compression that can\nupdate a compressed file directly. Moreover, at the cost of slightly increasing\nthe time spent per operation, the CRAM can be extended to also support\ninsertions and deletions. Our key observation that the empirical entropy of a\nstring does not change much after a small change to the string, as well as our\nsimple yet efficient method for maintaining an array of variable-length blocks\nunder length modifications, may be useful for many other applications as well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1827v1", 
    "title": "Finding topological subgraphs is fixed-parameter tractable", 
    "arxiv-id": "1011.1827v1", 
    "author": "Paul Wollan", 
    "publish": "2010-11-08T15:23:20Z", 
    "summary": "We show that for every fixed undirected graph $H$, there is a $O(|V(G)|^3)$\ntime algorithm that tests, given a graph $G$, if $G$ contains $H$ as a\ntopological subgraph (that is, a subdivision of $H$ is subgraph of $G$). This\nshows that topological subgraph testing is fixed-parameter tractable, resolving\na longstanding open question of Downey and Fellows from 1992. As a corollary,\nfor every $H$ we obtain an $O(|V(G)|^3)$ time algorithm that tests if there is\nan immersion of $H$ into a given graph $G$. This answers another open question\nraised by Downey and Fellows in 1992."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2187v1", 
    "title": "Online Scheduling on Identical Machines using SRPT", 
    "arxiv-id": "1011.2187v1", 
    "author": "Benjamin Moseley", 
    "publish": "2010-11-09T20:18:56Z", 
    "summary": "Due to its optimality on a single machine for the problem of minimizing\naverage flow time, Shortest-Remaining-Processing-Time (\\srpt) appears to be the\nmost natural algorithm to consider for the problem of minimizing average flow\ntime on multiple identical machines. It is known that $\\srpt$ achieves the best\npossible competitive ratio on multiple machines up to a constant factor. Using\nresource augmentation, $\\srpt$ is known to achieve total flow time at most that\nof the optimal solution when given machines of speed $2- \\frac{1}{m}$. Further,\nit is known that $\\srpt$'s competitive ratio improves as the speed increases;\n$\\srpt$ is $s$-speed $\\frac{1}{s}$-competitive when $s \\geq 2- \\frac{1}{m}$.\n  However, a gap has persisted in our understanding of $\\srpt$. Before this\nwork, the performance of $\\srpt$ was not known when $\\srpt$ is given\n$(1+\\eps)$-speed when $0 < \\eps < 1-\\frac{1}{m}$, even though it has been\nthought that $\\srpt$ is $(1+\\eps)$-speed $O(1)$-competitive for over a decade.\nResolving this question was suggested in Open Problem 2.9 from the survey\n\"Online Scheduling\" by Pruhs, Sgall, and Torng \\cite{PruhsST}, and we answer\nthe question in this paper. We show that $\\srpt$ is \\emph{scalable} on $m$\nidentical machines. That is, we show $\\srpt$ is $(1+\\eps)$-speed\n$O(\\frac{1}{\\eps})$-competitive for $\\eps >0$. We complement this by showing\nthat $\\srpt$ is $(1+\\eps)$-speed $O(\\frac{1}{\\eps^2})$-competitive for the\nobjective of minimizing the $\\ell_k$-norms of flow time on $m$ identical\nmachines. Both of our results rely on new potential functions that capture the\nstructure of \\srpt. Our results, combined with previous work, show that $\\srpt$\nis the best possible online algorithm in essentially every aspect when\nmigration is permissible."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2249v1", 
    "title": "Pareto Optimal Solutions for Smoothed Analysts", 
    "arxiv-id": "1011.2249v1", 
    "author": "Ryan O'Donnell", 
    "publish": "2010-11-10T01:31:44Z", 
    "summary": "Consider an optimization problem with $n$ binary variables and $d+1$ linear\nobjective functions. Each valid solution $x \\in \\{0,1\\}^n$ gives rise to an\nobjective vector in $\\R^{d+1}$, and one often wants to enumerate the Pareto\noptima among them. In the worst case there may be exponentially many Pareto\noptima; however, it was recently shown that in (a generalization of) the\nsmoothed analysis framework, the expected number is polynomial in $n$.\nUnfortunately, the bound obtained had a rather bad dependence on $d$; roughly\n$n^{d^d}$. In this paper we show a significantly improved bound of $n^{2d}$.\n  Our proof is based on analyzing two algorithms. The first algorithm, on input\na Pareto optimal $x$, outputs a \"testimony\" containing clues about $x$'s\nobjective vector, $x$'s coordinates, and the region of space $B$ in which $x$'s\nobjective vector lies. The second algorithm can be regarded as a {\\em\nspeculative} execution of the first -- it can uniquely reconstruct $x$ from the\ntestimony's clues and just \\emph{some} of the probability space's outcomes. The\nremainder of the probability space's outcomes are just enough to bound the\nprobability that $x$'s objective vector falls into the region $B$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2480v1", 
    "title": "Spin-the-bottle Sort and Annealing Sort: Oblivious Sorting via   Round-robin Random Comparisons", 
    "arxiv-id": "1011.2480v1", 
    "author": "Michael T. Goodrich", 
    "publish": "2010-11-10T20:22:42Z", 
    "summary": "We study sorting algorithms based on randomized round-robin comparisons.\nSpecifically, we study Spin-the-bottle sort, where comparisons are\nunrestricted, and Annealing sort, where comparisons are restricted to a\ndistance bounded by a \\emph{temperature} parameter. Both algorithms are simple,\nrandomized, data-oblivious sorting algorithms, which are useful in\nprivacy-preserving computations, but, as we show, Annealing sort is much more\nefficient. We show that there is an input permutation that causes\nSpin-the-bottle sort to require $\\Omega(n^2\\log n)$ expected time in order to\nsucceed, and that in $O(n^2\\log n)$ time this algorithm succeeds with high\nprobability for any input. We also show there is an implementation of Annealing\nsort that runs in $O(n\\log n)$ time and succeeds with very high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2571v1", 
    "title": "Recursive Sketching For Frequency Moments", 
    "arxiv-id": "1011.2571v1", 
    "author": "Rafail Ostrovsky", 
    "publish": "2010-11-11T06:07:18Z", 
    "summary": "In a ground-breaking paper, Indyk and Woodruff (STOC 05) showed how to\ncompute $F_k$ (for $k>2$) in space complexity $O(\\mbox{\\em poly-log}(n,m)\\cdot\nn^{1-\\frac2k})$, which is optimal up to (large) poly-logarithmic factors in $n$\nand $m$, where $m$ is the length of the stream and $n$ is the upper bound on\nthe number of distinct elements in a stream. The best known lower bound for\nlarge moments is $\\Omega(\\log(n)n^{1-\\frac2k})$. A follow-up work of\nBhuvanagiri, Ganguly, Kesh and Saha (SODA 2006) reduced the poly-logarithmic\nfactors of Indyk and Woodruff to $O(\\log^2(m)\\cdot (\\log n+ \\log m)\\cdot\nn^{1-{2\\over k}})$. Further reduction of poly-log factors has been an elusive\ngoal since 2006, when Indyk and Woodruff method seemed to hit a natural\n\"barrier.\" Using our simple recursive sketch, we provide a different yet simple\napproach to obtain a $O(\\log(m)\\log(nm)\\cdot (\\log\\log n)^4\\cdot n^{1-{2\\over\nk}})$ algorithm for constant $\\epsilon$ (our bound is, in fact, somewhat\nstronger, where the $(\\log\\log n)$ term can be replaced by any constant number\nof $\\log $ iterations instead of just two or three, thus approaching $log^*n$.\nOur bound also works for non-constant $\\epsilon$ (for details see the body of\nthe paper). Further, our algorithm requires only $4$-wise independence, in\ncontrast to existing methods that use pseudo-random generators for computing\nlarge frequency moments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2590v1", 
    "title": "Rademacher Chaos, Random Eulerian Graphs and The Sparse   Johnson-Lindenstrauss Transform", 
    "arxiv-id": "1011.2590v1", 
    "author": "Yuval Rabani", 
    "publish": "2010-11-11T08:13:15Z", 
    "summary": "The celebrated dimension reduction lemma of Johnson and Lindenstrauss has\nnumerous computational and other applications. Due to its application in\npractice, speeding up the computation of a Johnson-Lindenstrauss style\ndimension reduction is an important question. Recently, Dasgupta, Kumar, and\nSarlos (STOC 2010) constructed such a transform that uses a sparse matrix. This\nis motivated by the desire to speed up the computation when applied to sparse\ninput vectors, a scenario that comes up in applications. The sparsity of their\nconstruction was further improved by Kane and Nelson (ArXiv 2010).\n  We improve the previous bound on the number of non-zero entries per column of\nKane and Nelson from $O(1/\\epsilon \\log(1/\\delta)\\log(k/\\delta))$ (where the\ntarget dimension is $k$, the distortion is $1\\pm \\epsilon$, and the failure\nprobability is $\\delta$) to $$ O\\left({1\\over\\epsilon}\n\\left({\\log(1/\\delta)\\log\\log\\log(1/\\delta) \\over\n\\log\\log(1/\\delta)}\\right)^2\\right). $$\n  We also improve the amount of randomness needed to generate the matrix. Our\nresults are obtained by connecting the moments of an order 2 Rademacher chaos\nto the combinatorial properties of random Eulerian multigraphs. Estimating the\nchance that a random multigraph is composed of a given number of node-disjoint\nEulerian components leads to a new tail bound on the chaos. Our estimates may\nbe of independent interest, and as this part of the argument is decoupled from\nthe analysis of the coefficients of the chaos, we believe that our methods can\nbe useful in the analysis of other chaoses."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2843v2", 
    "title": "Improved Minimum Cuts and Maximum Flows in Undirected Planar Graphs", 
    "arxiv-id": "1011.2843v2", 
    "author": "Piotr Sankowski", 
    "publish": "2010-11-12T08:45:01Z", 
    "summary": "In this paper we study minimum cut and maximum flow problems on planar\ngraphs, both in static and in dynamic settings. First, we present an algorithm\nthat given an undirected planar graph computes the minimum cut between any two\ngiven vertices in O(n log log n) time. Second, we show how to achieve the same\nO(n log log n) bound for the problem of computing maximum flows in undirected\nplanar graphs. To the best of our knowledge, these are the first algorithms for\nthose two problems that break the O(n log n) barrier, which has been standing\nfor more than 25 years. Third, we present a fully dynamic algorithm that is\nable to maintain information about minimum cuts and maximum flows in a plane\ngraph (i.e., a planar graph with a fixed embedding): our algorithm is able to\ninsert edges, delete edges and answer min-cut and max-flow queries between any\npair of vertices in O(n^(2/3) log^3 n) time per operation. This result is based\non a new dynamic shortest path algorithm for planar graphs which may be of\nindependent interest. We remark that this is the first known non-trivial\nalgorithm for min-cut and max-flow problems in a dynamic setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3441v2", 
    "title": "Worst case efficient single and multiple string matching in the Word-RAM   model", 
    "arxiv-id": "1011.3441v2", 
    "author": "Djamal Belazzougui", 
    "publish": "2010-11-15T16:37:43Z", 
    "summary": "In this paper, we explore worst-case solutions for the problems of single and\nmultiple matching on strings in the word RAM model with word length w. In the\nfirst problem, we have to build a data structure based on a pattern p of length\nm over an alphabet of size sigma such that we can answer to the following\nquery: given a text T of length n, where each character is encoded using\nlog(sigma) bits return the positions of all the occurrences of p in T (in the\nfollowing we refer by occ to the number of reported occurrences). For the\nmulti-pattern matching problem we have a set S of d patterns of total length m\nand a query on a text T consists in finding all positions of all occurrences in\nT of the patterns in S. As each character of the text is encoded using log\nsigma bits and we can read w bits in constant time in the RAM model, we assume\nthat we can read up to (w/log sigma) consecutive characters of the text in one\ntime step. This implies that the fastest possible query time for both problems\nis O((n(log sigma/w)+occ). In this paper we present several different results\nfor both problems which come close to that best possible query time. We first\npresent two different linear space data structures for the first and second\nproblem: the first one answers to single pattern matching queries in time\nO(n(1/m+log sigma/w)+occ) while the second one answers to multiple pattern\nmatching queries to O(n((log d+log y+log log d)/y+log sigma/w)+occ) where y is\nthe length of the shortest pattern in the case of multiple pattern-matching. We\nthen show how a simple application of the four russian technique permits to get\ndata structures with query times independent of the length of the shortest\npattern (the length of the only pattern in case of single string matching) at\nthe expense of using more space."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3480v1", 
    "title": "Counting Colours in Compressed Strings", 
    "arxiv-id": "1011.3480v1", 
    "author": "Juha K\u00e4rkk\u00e4inen", 
    "publish": "2010-11-15T19:30:19Z", 
    "summary": "Suppose we are asked to preprocess a string \\(s [1..n]\\) such that later,\ngiven a substring's endpoints, we can quickly count how many distinct\ncharacters it contains. In this paper we give a data structure for this problem\nthat takes \\(n H_0 (s) + \\Oh{n} + \\oh{n H_0 (s)}\\) bits, where \\(H_0 (s)\\) is\nthe 0th-order empirical entropy of $s$, and answers queries in $\\Oh{\\log^{1 +\n\\epsilon} n}$ time for any constant \\(\\epsilon > 0\\). We also show how our data\nstructure can be made partially dynamic."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3491v2", 
    "title": "Pattern Kits", 
    "arxiv-id": "1011.3491v2", 
    "author": "Leena Salmela", 
    "publish": "2010-11-15T20:21:33Z", 
    "summary": "Suppose we have just performed searches in a self-index for two patterns $A$\nand $B$ and now we want to search for their concatenation \\A B); how can we\nbest make use of our previous computations? In this paper we consider this\nproblem and, more generally, how we can store a dynamic library of patterns\nthat we can easily manipulate in interesting ways. We give a space- and\ntime-efficient data structure for this problem that is compatible with many of\nthe best self-indexes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3701v2", 
    "title": "Directed Spanners via Flow-Based Linear Programs", 
    "arxiv-id": "1011.3701v2", 
    "author": "Robert Krauthgamer", 
    "publish": "2010-11-16T14:14:17Z", 
    "summary": "We examine directed spanners through flow-based linear programming\nrelaxations. We design an $\\~O(n^{2/3})$-approximation algorithm for the\ndirected $k$-spanner problem that works for all $k\\geq 1$, which is the first\nsublinear approximation for arbitrary edge-lengths. Even in the more restricted\nsetting of unit edge-lengths, our algorithm improves over the previous\n$\\~O(n^{1-1/k})$ approximation of Bhattacharyya et al. when $k\\ge 4$. For the\nspecial case of $k=3$ we design a different algorithm achieving an\n$\\~O(\\sqrt{n})$-approximation, improving the previous $\\~O(n^{2/3})$. Both of\nour algorithms easily extend to the fault-tolerant setting, which has recently\nattracted attention but not from an approximation viewpoint. We also prove a\nnearly matching integrality gap of $\\Omega(n^{\\frac13 - \\epsilon})$ for any\nconstant $\\epsilon > 0$.\n  A virtue of all our algorithms is that they are relatively simple.\nTechnically, we introduce a new yet natural flow-based relaxation, and show how\nto approximately solve it even when its size is not polynomial. The main\nchallenge is to design a rounding scheme that \"coordinates\" the choices of\nflow-paths between the many demand pairs while using few edges overall. We\nachieve this, roughly speaking, by randomization at the level of vertices."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3770v2", 
    "title": "Optimal Lower Bounds for Universal and Differentially Private Steiner   Tree and TSP", 
    "arxiv-id": "1011.3770v2", 
    "author": "Sanjeev Khanna", 
    "publish": "2010-11-16T17:49:27Z", 
    "summary": "Given a metric space on n points, an {\\alpha}-approximate universal algorithm\nfor the Steiner tree problem outputs a distribution over rooted spanning trees\nsuch that for any subset X of vertices containing the root, the expected cost\nof the induced subtree is within an {\\alpha} factor of the optimal Steiner tree\ncost for X. An {\\alpha}-approximate differentially private algorithm for the\nSteiner tree problem takes as input a subset X of vertices, and outputs a tree\ndistribution that induces a solution within an {\\alpha} factor of the optimal\nas before, and satisfies the additional property that for any set X' that\ndiffers in a single vertex from X, the tree distributions for X and X' are\n\"close\" to each other. Universal and differentially private algorithms for TSP\nare defined similarly. An {\\alpha}-approximate universal algorithm for the\nSteiner tree problem or TSP is also an {\\alpha}-approximate differentially\nprivate algorithm. It is known that both problems admit O(logn)-approximate\nuniversal algorithms, and hence O(log n)-approximate differentially private\nalgorithms as well. We prove an {\\Omega}(logn) lower bound on the approximation\nratio achievable for the universal Steiner tree problem and the universal TSP,\nmatching the known upper bounds. Our lower bound for the Steiner tree problem\nholds even when the algorithm is allowed to output a more general solution of a\ndistribution on paths to the root."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3944v2", 
    "title": "Non-Orthodox Combinatorial Models Based on Discordant Structures", 
    "arxiv-id": "1011.3944v2", 
    "author": "V. F. Romanov", 
    "publish": "2010-11-17T11:19:31Z", 
    "summary": "This paper introduces a novel method for compact representation of sets of\nn-dimensional binary sequences in a form of compact triplets structures (CTS),\nsupposing both logic and arithmetic interpretations of data. Suitable\nillustration of CTS application is the unique graph-combinatorial model for the\nclassic intractable 3-Satisfiability problem and a polynomial algorithm for the\nmodel synthesis. The method used for Boolean formulas analysis and\nclassification by means of the model is defined as a bijective mapping\nprinciple for sets of components of discordant structures to a basic set. The\nstatistic computer-aided experiment showed efficiency of the algorithm in a\nlarge scale of problem dimension parameters, including those that make\nenumeration procedures of no use. The formulated principle expands resources of\nconstructive approach to investigation of intractable problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.4465v1", 
    "title": "Compressed Transmission of Route Descriptions", 
    "arxiv-id": "1011.4465v1", 
    "author": "Peter Sanders", 
    "publish": "2010-11-19T16:36:30Z", 
    "summary": "We present two methods to compress the description of a route in a road\nnetwork, i.e., of a path in a directed graph. The first method represents a\npath by a sequence of via edges. The subpaths between the via edges have to be\nunique shortest paths. Instead of via edges also via nodes can be used, though\nthis requires some simple preprocessing. The second method uses contraction\nhierarchies to replace subpaths of the original path by shortcuts. The two\nmethods can be combined with each other. Also, we propose the application to\nmobile server based routing: We compute the route on a server which has access\nto the latest information about congestions for example. Then we transmit the\ncomputed route to the car using some mobile radio communication. There, we\napply the compression to save costs and transmission time. If the compression\nworks well, we can transmit routes even when the bandwidth is low. Although we\nhave not evaluated our ideas with realistic data yet, they are quite promising."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.5200v2", 
    "title": "The Power of Simple Tabulation Hashing", 
    "arxiv-id": "1011.5200v2", 
    "author": "Mikkel Thorup", 
    "publish": "2010-11-23T19:18:30Z", 
    "summary": "Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions used to yield the desired theoretical guarantees are often neither\nsimple nor practical. Here we show that the simplest possible tabulation\nhashing provides unexpectedly strong guarantees.\n  The scheme itself dates back to Carter and Wegman (STOC'77). Keys are viewed\nas consisting of c characters. We initialize c tables T_1, ..., T_c mapping\ncharacters to random hash codes. A key x=(x_1, ..., x_q) is hashed to T_1[x_1]\nxor ... xor T_c[x_c].\n  While this scheme is not even 4-independent, we show that it provides many of\nthe guarantees that are normally obtained via higher independence, e.g.,\nChernoff-type concentration, min-wise hashing for estimating set intersection,\nand cuckoo hashing."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.6181v2", 
    "title": "Computing the diameter polynomially faster than APSP", 
    "arxiv-id": "1011.6181v2", 
    "author": "Raphael Yuster", 
    "publish": "2010-11-29T10:26:27Z", 
    "summary": "We present a new randomized algorithm for computing the diameter of a\nweighted directed graph. The algorithm runs in\n$\\Ot(M^{\\w/(\\w+1)}n^{(\\w^2+3)/(\\w+1)})$ time, where $\\w < 2.376$ is the\nexponent of fast matrix multiplication, $n$ is the number of vertices of the\ngraph, and the edge weights are integers in $\\{-M,...,0,...,M\\}$. For bounded\ninteger weights the running time is $O(n^{2.561})$ and if $\\w=2+o(1)$ it is\n$\\Ot(n^{7/3})$. This is the first algorithm that computes the diameter of an\ninteger weighted directed graph polynomially faster than any known All-Pairs\nShortest Paths (APSP) algorithm. For bounded integer weights, the fastest\nalgorithm for APSP runs in $O(n^{2.575})$ time for the present value of $\\w$\nand runs in $\\Ot(n^{2.5})$ time if $\\w=2+o(1)$.\n  For directed graphs with {\\em positive} integer weights in $\\{1,...,M\\}$ we\nobtain a deterministic algorithm that computes the diameter in $\\Ot(Mn^\\w)$\ntime. This extends a simple $\\Ot(n^\\w)$ algorithm for computing the diameter of\nan {\\em unweighted} directed graph to the positive integer weighted setting and\nis the first algorithm in this setting whose time complexity matches that of\nthe fastest known Diameter algorithm for {\\em undirected} graphs.\n  The diameter algorithms are consequences of a more general result. We\nconstruct algorithms that for any given integer $d$, report all ordered pairs\nof vertices having distance {\\em at most} $d$. The diameter can therefore be\ncomputed using binary search for the smallest $d$ for which all pairs are\nreported."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1012.0032v1", 
    "title": "Testing of sequences by simulation", 
    "arxiv-id": "1012.0032v1", 
    "author": "Bal\u00e1zs Nov\u00e1k", 
    "publish": "2010-11-30T21:41:24Z", 
    "summary": "Let $\\xi$ be a random integer vector, having uniform distribution\n\\[\\mathbf{P} \\{\\xi = (i_1,i_2,...,i_n) = 1/n^n \\} \\ \\hbox{for} \\ 1 \\leq\ni_1,i_2,...,i_n\\leq n.\\] A realization $(i_1,i_2,...,i_n)$ of $\\xi$ is called\n\\textit{good}, if its elements are different. We present algorithms\n\\textsc{Linear}, \\textsc{Backward}, \\textsc{Forward}, \\textsc{Tree},\n\\textsc{Garbage}, \\textsc{Bucket} which decide whether a given realization is\ngood. We analyse the number of comparisons and running time of these algorithms\nusing simulation gathering data on all possible inputs for small values of $n$\nand generating random inputs for large values of $n$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1012.0058v1", 
    "title": "Modelling dynamic programming problems by generalized d-graphs", 
    "arxiv-id": "1012.0058v1", 
    "author": "Zolt\u00e1n K\u00e1tai", 
    "publish": "2010-11-30T22:59:46Z", 
    "summary": "In this paper we introduce the concept of generalized d-graph (admitting\ncycles) as special dependency-graphs for modelling dynamic programming (DP)\nproblems. We describe the d-graph versions of three famous single-source\nshortest algorithms (The algorithm based on the topological order of the\nvertices, Dijkstra algorithm and Bellman-Ford algorithm), which can be viewed\nas general DP strategies in the case of three different class of optimization\nproblems. The new modelling method also makes possible to classify DP problems\nand the corresponding DP strategies in term of graph theory."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1012.0256v2", 
    "title": "Weighted Random Sampling over Data Streams", 
    "arxiv-id": "1012.0256v2", 
    "author": "Pavlos S. Efraimidis", 
    "publish": "2010-12-01T17:48:48Z", 
    "summary": "In this work, we present a comprehensive treatment of weighted random\nsampling (WRS) over data streams. More precisely, we examine two natural\ninterpretations of the item weights, describe an existing algorithm for each\ncase ([2, 4]), discuss sampling with and without replacement and show\nadaptations of the algorithms for several WRS problems and evolving data\nstreams."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1012.0259v1", 
    "title": "(\u03b1, \u03b2) Fibonacci Search", 
    "arxiv-id": "1012.0259v1", 
    "author": "Pavlos S. Efraimidis", 
    "publish": "2010-12-01T18:00:01Z", 
    "summary": "Knuth [12, Page 417] states that \"the (program of the) Fibonaccian search\ntechnique looks very mysterious at first glance\" and that \"it seems to work by\nmagic\". In this work, we show that there is even more magic in Fibonaccian (or\nelse Fibonacci) search. We present a generalized Fibonacci procedure that\nfollows perfectly the implicit optimal decision tree for search problems where\nthe cost of each comparison depends on its outcome."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.0280v1", 
    "title": "String Matching with Inversions and Translocations in Linear Average   Time (Most of the Time)", 
    "arxiv-id": "1012.0280v1", 
    "author": "Emanuele Giaquinta", 
    "publish": "2010-12-01T19:53:05Z", 
    "summary": "We present an efficient algorithm for finding all approximate occurrences of\na given pattern $p$ of length $m$ in a text $t$ of length $n$ allowing for\ntranslocations of equal length adjacent factors and inversions of factors. The\nalgorithm is based on an efficient filtering method and has an\n$\\bigO(nm\\max(\\alpha, \\beta))$-time complexity in the worst case and\n$\\bigO(\\max(\\alpha, \\beta))$-space complexity, where $\\alpha$ and $\\beta$ are\nrespectively the maximum length of the factors involved in any translocation\nand inversion. Moreover we show that under the assumptions of equiprobability\nand independence of characters our algorithm has a $\\bigO(n)$ average time\ncomplexity, whenever $\\sigma = \\Omega(\\log m / \\log\\log^{1-\\epsilon} m)$, where\n$\\epsilon > 0$ and $\\sigma$ is the dimension of the alphabet. Experiments show\nthat the new proposed algorithm achieves very good results in practical cases."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.1129v1", 
    "title": "Weighted random generation of context-free languages: Analysis of   collisions in random urn occupancy models", 
    "arxiv-id": "1012.1129v1", 
    "author": "Yann Ponty", 
    "publish": "2010-12-06T11:09:44Z", 
    "summary": "The present work analyzes the redundancy of sets of combinatorial objects\nproduced by a weighted random generation algorithm proposed by Denise et al.\nThis scheme associates weights to the terminals symbols of a weighted\ncontext-free grammar, extends this weight definition multiplicatively on words,\nand draws words of length $n$ with probability proportional their weight. We\ninvestigate the level of redundancy within a sample of $k$ word, the proportion\nof the total probability covered by $k$ words (coverage), the time (number of\ngenerations) of the first collision, and the time of the full collection. For\nthese four questions, we use an analytic urn analogy to derive asymptotic\nestimates and/or polynomially computable exact forms. We illustrate these tools\nby an analysis of an RNA secondary structure statistical sampling algorithm\nintroduced by Ding et al."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.1163v1", 
    "title": "Lower Bounds for the Smoothed Number of Pareto optimal Solutions", 
    "arxiv-id": "1012.1163v1", 
    "author": "Heiko Roeglin", 
    "publish": "2010-12-06T13:14:36Z", 
    "summary": "In 2009, Roeglin and Teng showed that the smoothed number of Pareto optimal\nsolutions of linear multi-criteria optimization problems is polynomially\nbounded in the number $n$ of variables and the maximum density $\\phi$ of the\nsemi-random input model for any fixed number of objective functions. Their\nbound is, however, not very practical because the exponents grow exponentially\nin the number $d+1$ of objective functions. In a recent breakthrough, Moitra\nand O'Donnell improved this bound significantly to $O(n^{2d} \\phi^{d(d+1)/2})$.\n  An \"intriguing problem\", which Moitra and O'Donnell formulate in their paper,\nis how much further this bound can be improved. The previous lower bounds do\nnot exclude the possibility of a polynomial upper bound whose degree does not\ndepend on $d$. In this paper we resolve this question by constructing a class\nof instances with $\\Omega ((n \\phi)^{(d-\\log{d}) \\cdot (1-\\Theta{1/\\phi})})$\nPareto optimal solutions in expectation. For the bi-criteria case we present a\nhigher lower bound of $\\Omega (n^2 \\phi^{1 - \\Theta{1/\\phi}})$, which almost\nmatches the known upper bound of $O(n^2 \\phi)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.1338v1", 
    "title": "On Tuning the Bad-Character Rule: the Worst-Character Rule", 
    "arxiv-id": "1012.1338v1", 
    "author": "Simone Faro", 
    "publish": "2010-12-06T21:16:37Z", 
    "summary": "In this note we present the worst-character rule, an efficient variation of\nthe bad-character heuristic for the exact string matching problem, firstly\nintroduced in the well-known Boyer-Moore algorithm. Our proposed rule selects a\nposition relative to the current shift which yields the largest average\nadvancement, according to the characters distribution in the text. Experimental\nresults show that the worst-character rule achieves very good results\nespecially in the case of long patterns or small alphabets in random texts and\nin the case of texts in natural languages."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.1850v1", 
    "title": "Capacitated Vehicle Routing with Non-Uniform Speeds", 
    "arxiv-id": "1012.1850v1", 
    "author": "R. Ravi", 
    "publish": "2010-12-08T20:55:06Z", 
    "summary": "The capacitated vehicle routing problem (CVRP) involves distributing\n(identical) items from a depot to a set of demand locations, using a single\ncapacitated vehicle. We study a generalization of this problem to the setting\nof multiple vehicles having non-uniform speeds (that we call Heterogenous\nCVRP), and present a constant-factor approximation algorithm.\n  The technical heart of our result lies in achieving a constant approximation\nto the following TSP variant (called Heterogenous TSP). Given a metric denoting\ndistances between vertices, a depot r containing k vehicles with possibly\ndifferent speeds, the goal is to find a tour for each vehicle (starting and\nending at r), so that every vertex is covered in some tour and the maximum\ncompletion time is minimized. This problem is precisely Heterogenous CVRP when\nvehicles are uncapacitated.\n  The presence of non-uniform speeds introduces difficulties for employing\nstandard tour-splitting techniques. In order to get a better understanding of\nthis technique in our context, we appeal to ideas from the 2-approximation for\nscheduling in parallel machine of Lenstra et al.. This motivates the\nintroduction of a new approximate MST construction called Level-Prim, which is\nrelated to Light Approximate Shortest-path Trees. The last component of our\nalgorithm involves partitioning the Level-Prim tree and matching the resulting\nparts to vehicles. This decomposition is more subtle than usual since now we\nneed to enforce correlation between the size of the parts and their distances\nto the depot."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.1886v2", 
    "title": "Sublinear Time, Measurement-Optimal, Sparse Recovery For All", 
    "arxiv-id": "1012.1886v2", 
    "author": "Martin J. Strauss", 
    "publish": "2010-12-08T22:39:52Z", 
    "summary": "An approximate sparse recovery system in ell_1 norm formally consists of\nparameters N, k, epsilon an m-by-N measurement matrix, Phi, and a decoding\nalgorithm, D. Given a vector, x, where x_k denotes the optimal k-term\napproximation to x, the system approximates x by hat_x = D(Phi.x), which must\nsatisfy\n  ||hat_x - x||_1 <= (1+epsilon)||x - x_k||_1.\n  Among the goals in designing such systems are minimizing m and the runtime of\nD. We consider the \"forall\" model, in which a single matrix Phi is used for all\nsignals x.\n  All previous algorithms that use the optimal number m=O(k log(N/k)) of\nmeasurements require superlinear time Omega(N log(N/k)). In this paper, we give\nthe first algorithm for this problem that uses the optimum number of\nmeasurements (up to a constant factor) and runs in sublinear time o(N) when\nk=o(N), assuming access to a data structure requiring space and preprocessing\nO(N)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.2202v2", 
    "title": "Transforming a random graph drawing into a Lombardi drawing", 
    "arxiv-id": "1012.2202v2", 
    "author": "Nicolaos Matsakis", 
    "publish": "2010-12-10T08:31:21Z", 
    "summary": "The visualization of any graph plays important role in various aspects, such\nas graph drawing software. Complex systems (like large databases or networks)\nthat have a graph structure should be properly visualized in order to avoid\nobfuscation. One way to provide an aesthetic improvement to a graph\nvisualization is to apply a force-directed drawing algorithm to it. This\nmethod, that emerged in the 60's views graphs as spring systems that exert\nforces (repulsive or attractive) to the nodes.\n  A Lombardi drawing of a graph is a drawing where the edges are drawn as\ncircular arcs (straight edges are considered circular arcs of infinite radius)\nwith perfect angular resolution. This means, that consecutive edges around a\nvertex are equally spaced around it. In other words, each angle between the\ntangents of two consecutive edges is equal to $2\\pi/d$ where d is the degree of\nthat specific vertex. The requirement of using circular edges in graphs when we\nwant to provide perfect angular resolution is necessary, since even cycle\ngraphs cannot be drawn with straight edges when perfect angular resolution is\nneeded.\n  In this survey, we provide an algorithm that takes as input a random drawing\nof a graph and provides its Lombardi drawing, giving a proper visualization of\nthe graph."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.2547v1", 
    "title": "The Exact String Matching Problem: a Comprehensive Experimental   Evaluation", 
    "arxiv-id": "1012.2547v1", 
    "author": "Thierry Lecroq", 
    "publish": "2010-12-12T14:47:00Z", 
    "summary": "This paper addresses the online exact string matching problem which consists\nin finding all occurrences of a given pattern p in a text t. It is an\nextensively studied problem in computer science, mainly due to its direct\napplications to such diverse areas as text, image and signal processing, speech\nanalysis and recognition, data compression, information retrieval,\ncomputational biology and chemistry. Since 1970 more than 80 string matching\nalgorithms have been proposed, and more than 50% of them in the last ten years.\nIn this note we present a comprehensive list of all string matching algorithms\nand present experimental results in order to compare them from a practical\npoint of view. From our experimental evaluation it turns out that the\nperformance of the algorithms are quite different for different alphabet sizes\nand pattern length."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.2573v1", 
    "title": "Approximating Vertex Cover in Dense Hypergraphs", 
    "arxiv-id": "1012.2573v1", 
    "author": "Claus Viehmann", 
    "publish": "2010-12-12T18:47:05Z", 
    "summary": "We consider the minimum vertex cover problem in hypergraphs in which every\nhyperedge has size k (also known as minimum hitting set problem, or minimum set\ncover with element frequency k). Simple algorithms exist that provide\nk-approximations, and this is believed to be the best possible approximation\nachievable in polynomial time. We show how to exploit density and regularity\nproperties of the input hypergraph to break this barrier. In particular, we\nprovide a randomized polynomial-time algorithm with approximation factor k/(1\n+(k-1)d/(k Delta)), where d and Delta are the average and maximum degree,\nrespectively, and Delta must be Omega(n^{k-1}/log n). The proposed algorithm\ngeneralizes the recursive sampling technique of Imamura and Iwama (SODA'05) for\nvertex cover in dense graphs. As a corollary, we obtain an approximation factor\nk/(2-1/k) for subdense regular hypergraphs, which is shown to be the best\npossible under the unique games conjecture."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.02.015", 
    "link": "http://arxiv.org/pdf/1012.3011v1", 
    "title": "An Improved Algorithm for Bipartite Correlation Clustering", 
    "arxiv-id": "1012.3011v1", 
    "author": "Edo Liberty", 
    "publish": "2010-12-14T12:58:59Z", 
    "summary": "Bipartite Correlation clustering is the problem of generating a set of\ndisjoint bi-cliques on a set of nodes while minimizing the symmetric difference\nto a bipartite input graph. The number or size of the output clusters is not\nconstrained in any way. The best known approximation algorithm for this problem\ngives a factor of 11. This result and all previous ones involve solving large\nlinear or semi-definite programs which become prohibitive even for modestly\nsized tasks. In this paper we present an improved factor 4 approximation\nalgorithm to this problem using a simple combinatorial algorithm which does not\nrequire solving large convex programs. The analysis extends a method developed\nby Ailon, Charikar and Alantha in 2008, where a randomized pivoting algorithm\nwas analyzed for obtaining a 3-approximation algorithm for Correlation\nClustering, which is the same problem on graphs (not bipartite). The analysis\nfor Correlation Clustering there required defining events for structures\ncontaining 3 vertices and using the probability of these events to produce a\nfeasible solution to a dual of a certain natural LP bounding the optimal cost.\nIt is tempting here to use sets of 4 vertices, which are the smallest\nstructures for which contradictions arise for Bipartite Correlation Clustering.\nThis simple idea, however, appears to be evasive. We show that, by modifying\nthe LP, we can analyze algorithms which take into consideration subgraph\nstructures of unbounded size. We believe our techniques are interesting in\ntheir own right, and may be used for other problems as well."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.3130v3", 
    "title": "How to Catch L_2-Heavy-Hitters on Sliding Windows", 
    "arxiv-id": "1012.3130v3", 
    "author": "Rafail Ostrovsky", 
    "publish": "2010-12-14T18:52:58Z", 
    "summary": "Finding heavy-elements (heavy-hitters) in streaming data is one of the\ncentral, and well-understood tasks. Despite the importance of this problem,\nwhen considering the sliding windows model of streaming (where elements\neventually expire) the problem of finding L_2-heavy elements has remained\ncompletely open despite multiple papers and considerable success in finding\nL_1-heavy elements.\n  In this paper, we develop the first poly-logarithmic-memory algorithm for\nfinding L_2-heavy elements in sliding window model. Since L_2 heavy elements\nplay a central role for many fundamental streaming problems (such as frequency\nmoments), we believe our method would be extremely useful for many\nsliding-windows algorithms and applications. For example, our technique allows\nus not only to find L_2-heavy elements, but also heavy elements with respect to\nany L_p for 0<p<2 on sliding windows. Thus, our paper completely resolves the\nquestion of finding L_p-heavy elements for sliding windows with\npoly-logarithmic memory for all values of p since it is well known that for p>2\nthis task is impossible.\n  Our method may have other applications as well. We demonstrate a broader\napplicability of our novel yet simple method on two additional examples: we\nshow how to obtain a sliding window approximation of other properties such as\nthe similarity of two streams, or the fraction of elements that appear exactly\na specified number of times within the window (the rarity problem). In these\ntwo illustrative examples of our method, we replace the current expected memory\nbounds with worst case bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.3932v1", 
    "title": "Balanced Interval Coloring", 
    "arxiv-id": "1012.3932v1", 
    "author": "Alexander Souza", 
    "publish": "2010-12-17T17:26:31Z", 
    "summary": "We consider the discrepancy problem of coloring $n$ intervals with $k$ colors\nsuch that at each point on the line, the maximal difference between the number\nof intervals of any two colors is minimal. Somewhat surprisingly, a coloring\nwith maximal difference at most one always exists. Furthermore, we give an\nalgorithm with running time $O(n \\log n + kn \\log k)$ for its construction.\nThis is in particular interesting because many known results for discrepancy\nproblems are non-constructive. This problem naturally models a load balancing\nscenario, where $n$ tasks with given start- and endtimes have to be distributed\namong $k$ servers. Our results imply that this can be done ideally balanced.\n  When generalizing to $d$-dimensional boxes (instead of intervals), a solution\nwith difference at most one is not always possible. We show that for any $d \\ge\n2$ and any $k \\ge 2$ it is NP-complete to decide if such a solution exists,\nwhich implies also NP-hardness of the respective minimization problem.\n  In an online scenario, where intervals arrive over time and the color has to\nbe decided upon arrival, the maximal difference in the size of color classes\ncan become arbitrarily high for any online algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.4231v1", 
    "title": "Computation in Large-Scale Scientific and Internet Data Applications is   a Focus of MMDS 2010", 
    "arxiv-id": "1012.4231v1", 
    "author": "Michael W. Mahoney", 
    "publish": "2010-12-20T03:07:53Z", 
    "summary": "The 2010 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2010) was\nheld at Stanford University, June 15--18. The goals of MMDS 2010 were (1) to\nexplore novel techniques for modeling and analyzing massive, high-dimensional,\nand nonlinearly-structured scientific and Internet data sets; and (2) to bring\ntogether computer scientists, statisticians, applied mathematicians, and data\nanalysis practitioners to promote cross-fertilization of ideas. MMDS 2010\nfollowed on the heels of two previous MMDS workshops. The first, MMDS 2006,\naddressed the complementary perspectives brought by the numerical linear\nalgebra and theoretical computer science communities to matrix algorithms in\nmodern informatics applications; and the second, MMDS 2008, explored more\ngenerally fundamental algorithmic and statistical challenges in modern\nlarge-scale data analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.4263v1", 
    "title": "Lightweight LCP-Array Construction in Linear Time", 
    "arxiv-id": "1012.4263v1", 
    "author": "Enno Ohlebusch", 
    "publish": "2010-12-20T09:08:05Z", 
    "summary": "The suffix tree is a very important data structure in string processing, but\nit suffers from a huge space consumption. In large-scale applications,\ncompressed suffix trees (CSTs) are therefore used instead. A CST consists of\nthree (compressed) components: the suffix array, the LCP-array, and data\nstructures for simulating navigational operations on the suffix tree. The\nLCP-array stores the lengths of the longest common prefixes of\nlexicographically adjacent suffixes, and it can be computed in linear time. In\nthis paper, we present new LCP-array construction algorithms that are fast and\nvery space efficient. In practice, our algorithms outperform the currently best\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.4763v2", 
    "title": "A simple and practical algorithm for differentially private data release", 
    "arxiv-id": "1012.4763v2", 
    "author": "Frank McSherry", 
    "publish": "2010-12-21T18:41:19Z", 
    "summary": "We present new theoretical results on differentially private data release\nuseful with respect to any target class of counting queries, coupled with\nexperimental results on a variety of real world data sets.\n  Specifically, we study a simple combination of the multiplicative weights\napproach of [Hardt and Rothblum, 2010] with the exponential mechanism of\n[McSherry and Talwar, 2007]. The multiplicative weights framework allows us to\nmaintain and improve a distribution approximating a given data set with respect\nto a set of counting queries. We use the exponential mechanism to select those\nqueries most incorrectly tracked by the current distribution. Combing the two,\nwe quickly approach a distribution that agrees with the data set on the given\nset of queries up to small error.\n  The resulting algorithm and its analysis is simple, but nevertheless improves\nupon previous work in terms of both error and running time. We also empirically\ndemonstrate the practicality of our approach on several data sets commonly used\nin the statistical community for contingency table release."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.4938v1", 
    "title": "Join-Reachability Problems in Directed Graphs", 
    "arxiv-id": "1012.4938v1", 
    "author": "Leonidas Palios", 
    "publish": "2010-12-22T11:01:40Z", 
    "summary": "For a given collection G of directed graphs we define the join-reachability\ngraph of G, denoted by J(G), as the directed graph that, for any pair of\nvertices a and b, contains a path from a to b if and only if such a path exists\nin all graphs of G. Our goal is to compute an efficient representation of J(G).\nIn particular, we consider two versions of this problem. In the explicit\nversion we wish to construct the smallest join-reachability graph for G. In the\nimplicit version we wish to build an efficient data structure (in terms of\nspace and query time) such that we can report fast the set of vertices that\nreach a query vertex in all graphs of G. This problem is related to the\nwell-studied reachability problem and is motivated by emerging applications of\ngraph-structured databases and graph algorithms. We consider the construction\nof join-reachability structures for two graphs and develop techniques that can\nbe applied to both the explicit and the implicit problem. First we present\noptimal and near-optimal structures for paths and trees. Then, based on these\nresults, we provide efficient structures for planar graphs and general directed\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.4962v2", 
    "title": "Robust and MaxMin Optimization under Matroid and Knapsack Uncertainty   Sets", 
    "arxiv-id": "1012.4962v2", 
    "author": "R. Ravi", 
    "publish": "2010-12-22T13:05:51Z", 
    "summary": "Consider the following problem: given a set system (U,I) and an edge-weighted\ngraph G = (U, E) on the same universe U, find the set A in I such that the\nSteiner tree cost with terminals A is as large as possible: \"which set in I is\nthe most difficult to connect up?\" This is an example of a max-min problem:\nfind the set A in I such that the value of some minimization (covering) problem\nis as large as possible.\n  In this paper, we show that for certain covering problems which admit good\ndeterministic online algorithms, we can give good algorithms for max-min\noptimization when the set system I is given by a p-system or q-knapsacks or\nboth. This result is similar to results for constrained maximization of\nsubmodular functions. Although many natural covering problems are not even\napproximately submodular, we show that one can use properties of the online\nalgorithm as a surrogate for submodularity.\n  Moreover, we give stronger connections between max-min optimization and\ntwo-stage robust optimization, and hence give improved algorithms for robust\nversions of various covering problems, for cases where the uncertainty sets are\ngiven by p-systems and q-knapsacks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.5024v1", 
    "title": "Shortest Paths with Pairwise-Distinct Edge Labels: Finding Biochemical   Pathways in Metabolic Networks", 
    "arxiv-id": "1012.5024v1", 
    "author": "Michael Stelzer", 
    "publish": "2010-12-22T16:16:11Z", 
    "summary": "A problem studied in Systems Biology is how to find shortest paths in\nmetabolic networks. Unfortunately, simple (i.e., graph theoretic) shortest\npaths do not properly reflect biochemical facts. An approach to overcome this\nissue is to use edge labels and search for paths with distinct labels.\n  In this paper, we show that such biologically feasible shortest paths are\nhard to compute. Moreover, we present solutions to find such paths in networks\nin reasonable time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.5330v2", 
    "title": "No-Break Dynamic Defragmentation of Reconfigurable Devices", 
    "arxiv-id": "1012.5330v2", 
    "author": "Juergen Teich", 
    "publish": "2010-12-24T01:16:08Z", 
    "summary": "We propose a new method for defragmenting the module layout of a\nreconfigurable device, enabled by a novel approach for dealing with\ncommunication needs between relocated modules and with inhomogeneities found in\ncommonly used FPGAs. Our method is based on dynamic relocation of module\npositions during runtime, with only very little reconfiguration overhead; the\nobjective is to maximize the length of contiguous free space that is available\nfor new modules. We describe a number of algorithmic aspects of good\ndefragmentation, and present an optimization method based on tabu search.\nExperimental results indicate that we can improve the quality of module layout\nby roughly 50 % over static layout. Among other benefits, this improvement\navoids unnecessary rejections of modules"
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1012.5911v2", 
    "title": "Near approximation of maximum weight matching through efficient weight   reduction", 
    "arxiv-id": "1012.5911v2", 
    "author": "Cui Di", 
    "publish": "2010-12-29T11:22:31Z", 
    "summary": "Let G be an edge-weighted hypergraph on n vertices, m edges of size \\le s,\nwhere the edges have real weights in an interval [1,W]. We show that if we can\napproximate a maximum weight matching in G within factor alpha in time T(n,m,W)\nthen we can find a matching of weight at least (alpha-epsilon) times the\nmaximum weight of a matching in G in time (epsilon^{-1})^{O(1)}max_{1\\le q \\le\nO(epsilon \\frac {log {\\frac n {epsilon}}} {log epsilon^{-1}})}\nmax_{m_1+...m_q=m}\nsum_1^qT(min{n,sm_j},m_{j},(epsilon^{-1})^{O(epsilon^{-1})}). In particular, if\nwe combine our result with the recent (1-\\epsilon)-approximation algorithm for\nmaximum weight matching in graphs due to Duan and Pettie whose time complexity\nhas a poly-logarithmic dependence on W then we obtain a\n(1-\\epsilon)-approximation algorithm for maximum weight matching in graphs\nrunning in time (epsilon^{-1})^{O(1)}(m+n)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.0021v1", 
    "title": "Popular b-matchings", 
    "arxiv-id": "1101.0021v1", 
    "author": "Katarzyna Paluch", 
    "publish": "2010-12-29T23:32:39Z", 
    "summary": "Suppose that each member of a set of agents has a preference list of a subset\nof houses, possibly involving ties and each agent and house has their capacity\ndenoting the maximum number of correspondingly agents/houses that can be\nmatched to him/her/it. We want to find a matching $M$, for which there is no\nother matching $M'$ such that more agents prefer $M'$ to $M$ than $M$ to $M'$.\n(What it means that an agent prefers one matching to the other is explained in\nthe paper.) Popular matchings have been studied quite extensively, especially\nin the one-to-one setting. We provide a characterization of popular b-matchings\nfor two defintions of popularity, show some $NP$-hardness results and for\ncertain versions describe polynomial algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.0056v1", 
    "title": "An Algorithm to Reduce the Time Complexity of Earliest Deadline First   Scheduling Algorithm in Real-Time System", 
    "arxiv-id": "1101.0056v1", 
    "author": "Jagbeer Singh", 
    "publish": "2010-12-30T08:50:49Z", 
    "summary": "In this paper I have study to Reduce the time Complexity of Earliest Deadline\nFirst (EDF), a global scheduling scheme for Earliest Deadline First in Real\nTime System tasks on a Multiprocessors system. Several admission control\nalgorithms for Earliest Deadline First (EDF) are presented, both for hard and\nsoft real-time tasks. The average performance of these admission control\nalgorithms is compared with the performance of known partitioning schemes. I\nhave applied some modification to the global Earliest Deadline First (EDF)\nalgorithms to decrease the number of task migration and also to add\npredictability to its behavior. The Aim of this work is to provide a\nsensitivity analysis for task deadline context of multiprocessor system by\nusing a new approach of EFDF (Earliest Feasible Deadline First) algorithm. In\norder to decrease the number of migrations we prevent a job from moving one\nprocessor to another processor if it is among the m higher priority jobs.\nTherefore, a job will continue its execution on the same processor if possible\n(processor affinity). The result of these comparisons outlines some situations\nwhere one scheme is preferable over the other. Partitioning schemes are better\nsuited for hard real-time systems, while a global scheme is preferable for soft\nreal-time systems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.0080v3", 
    "title": "A Searchable Compressed Edit-Sensitive Parsing", 
    "arxiv-id": "1101.0080v3", 
    "author": "Hiroshi Sakamoto", 
    "publish": "2010-12-30T13:04:56Z", 
    "summary": "Practical data structures for the edit-sensitive parsing (ESP) are proposed.\nGiven a string S, its ESP tree is equivalent to a context-free grammar G\ngenerating just S, which is represented by a DAG. Using the succinct data\nstructures for trees and permutations, G is decomposed to two LOUDS bit strings\nand single array in (1+\\epsilon)n\\log n+4n+o(n) bits for any 0<\\epsilon <1 and\nthe number n of variables in G. The time to count occurrences of P in S is in\nO(\\frac{1}{\\epsilon}(m\\log n+occ_c(\\log m\\log u)), whereas m = |P|, u = |S|,\nand occ_c is the number of occurrences of a maximal common subtree in ESPs of P\nand S. The efficiency of the proposed index is evaluated by the experiments\nconducted on several benchmarks complying with the other compressed indexes."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.1256v1", 
    "title": "Non-clairvoyant Scheduling Games", 
    "arxiv-id": "1101.1256v1", 
    "author": "Nguyen Kim Thang", 
    "publish": "2011-01-06T17:05:50Z", 
    "summary": "In a scheduling game, each player owns a job and chooses a machine to execute\nit. While the social cost is the maximal load over all machines (makespan), the\ncost (disutility) of each player is the completion time of its own job. In the\ngame, players may follow selfish strategies to optimize their cost and\ntherefore their behaviors do not necessarily lead the game to an equilibrium.\nEven in the case there is an equilibrium, its makespan might be much larger\nthan the social optimum, and this inefficiency is measured by the price of\nanarchy -- the worst ratio between the makespan of an equilibrium and the\noptimum. Coordination mechanisms aim to reduce the price of anarchy by\ndesigning scheduling policies that specify how jobs assigned to a same machine\nare to be scheduled. Typically these policies define the schedule according to\nthe processing times as announced by the jobs. One could wonder if there are\npolicies that do not require this knowledge, and still provide a good price of\nanarchy. This would make the processing times be private information and avoid\nthe problem of truthfulness. In this paper we study these so-called\nnon-clairvoyant policies. In particular, we study the RANDOM policy that\nschedules the jobs in a random order without preemption, and the EQUI policy\nthat schedules the jobs in parallel using time-multiplexing, assigning each job\nan equal fraction of CPU time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.1941v1", 
    "title": "On Parsimonious Explanations for 2-D Tree- and Linearly-Ordered Data", 
    "arxiv-id": "1101.1941v1", 
    "author": "Yuval Rabani", 
    "publish": "2011-01-10T20:15:03Z", 
    "summary": "This paper studies the \"explanation problem\" for tree- and linearly-ordered\narray data, a problem motivated by database applications and recently solved\nfor the one-dimensional tree-ordered case. In this paper, one is given a matrix\nA whose rows and columns have semantics: special subsets of the rows and\nspecial subsets of the columns are meaningful, others are not. A submatrix in A\nis said to be meaningful if and only if it is the cross product of a meaningful\nrow subset and a meaningful column subset, in which case we call it an \"allowed\nrectangle.\" The goal is to \"explain\" A as a sparse sum of weighted allowed\nrectangles. Specifically, we wish to find as few weighted allowed rectangles as\npossible such that, for all i,j, a_{ij} equals the sum of the weights of all\nrectangles which include cell (i,j).\n  In this paper we consider the natural cases in which the matrix dimensions\nare tree-ordered or linearly-ordered. In the tree-ordered case, we are given a\nrooted tree T1 whose leaves are the rows of A and another, T2, whose leaves are\nthe columns. Nodes of the trees correspond in an obvious way to the sets of\ntheir leaf descendants. In the linearly-ordered case, a set of rows or columns\nis meaningful if and only if it is contiguous.\n  For tree-ordered data, we prove the explanation problem NP-Hard and give a\nrandomized 2-approximation algorithm for it. For linearly-ordered data, we\nprove the explanation problem NP-Hard and give a 2.56-approximation algorithm.\nTo our knowledge, these are the first results for the problem of sparsely and\nexactly representing matrices by weighted rectangles."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.2973v5", 
    "title": "Maximizing Non-monotone Submodular Set Functions Subject to Different   Constraints: Combined Algorithms", 
    "arxiv-id": "1101.2973v5", 
    "author": "MohammadAli Safari", 
    "publish": "2011-01-15T11:30:05Z", 
    "summary": "We study the problem of maximizing constrained non-monotone submodular\nfunctions and provide approximation algorithms that improve existing algorithms\nin terms of either the approximation factor or simplicity. Our algorithms\ncombine existing local search and greedy based algorithms. Different\nconstraints that we study are exact cardinality and multiple knapsack\nconstraints. For the multiple-knapsack constraints we achieve a\n$(0.25-2\\epsilon)$-factor algorithm.\n  We also show, as our main contribution, how to use the continuous greedy\nprocess for non-monotone functions and, as a result, obtain a $0.13$-factor\napproximation algorithm for maximization over any solvable down-monotone\npolytope. The continuous greedy process has been previously used for maximizing\nsmooth monotone submodular function over a down-monotone polytope\n\\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,\nsuch as maximizing a non-negative submodular function subject to a matroid\nconstraint and/or multiple knapsack constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.3182v2", 
    "title": "Multi-Stage Improved Route Planning Approach: theoretical foundations", 
    "arxiv-id": "1101.3182v2", 
    "author": "Ondrej Moris", 
    "publish": "2011-01-17T11:04:58Z", 
    "summary": "A new approach to the static route planning problem, based on a multi-staging\nconcept and a \\emph{scope} notion, is presented. The main goal (besides implied\nefficiency of planning) of our approach is to address---with a solid\ntheoretical foundation---the following two practically motivated aspects: a\n\\emph{route comfort} and a very \\emph{limited storage} space of a small\nnavigation device, which both do not seem to be among the chief objectives of\nmany other studies. We show how our novel idea can tackle both these seemingly\nunrelated aspects at once, and may also contribute to other established route\nplanning approaches with which ours can be naturally combined. We provide a\ntheoretical proof that our approach efficiently computes exact optimal routes\nwithin this concept, as well as we demonstrate with experimental results on\npublicly available road networks of the US the good practical performance of\nthe solution."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.3396v1", 
    "title": "Multi-objective Optimization For The Dynamic Multi-Pickup and Delivery   Problem with Time Windows", 
    "arxiv-id": "1101.3396v1", 
    "author": "Mekki Ksouri", 
    "publish": "2011-01-18T08:14:47Z", 
    "summary": "The PDPTW is an optimization vehicles routing problem which must meet\nrequests for transport between suppliers and customers satisfying precedence,\ncapacity and time constraints. We present, in this paper, a genetic algorithm\nfor multi-objective optimization of a dynamic multi pickup and delivery problem\nwith time windows (Dynamic m-PDPTW). We propose a brief literature review of\nthe PDPTW, present our approach based on Pareto dominance method and lower\nbounds, to give a satisfying solution to the Dynamic m-PDPTW minimizing the\ncompromise between total travel cost and total tardiness time. Computational\nresults indicate that the proposed algorithm gives good results with a total\ntardiness equal to zero with a tolerable cost."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.3448v1", 
    "title": "Inducing the LCP-Array", 
    "arxiv-id": "1101.3448v1", 
    "author": "Johannes Fischer", 
    "publish": "2011-01-18T12:58:02Z", 
    "summary": "We show how to modify the linear-time construction algorithm for suffix\narrays based on induced sorting (Nong et al., DCC'09) such that it computes the\narray of longest common prefixes (LCP-array) as well. Practical tests show that\nthis outperforms recent LCP-array construction algorithms (Gog and Ohlebusch,\nALENEX'11)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.3953v1", 
    "title": "Two Multivehicle Routing Problems with Unit-Time Windows", 
    "arxiv-id": "1101.3953v1", 
    "author": "Barry Wittman", 
    "publish": "2011-01-20T16:39:00Z", 
    "summary": "Two multivehicle routing problems are considered in the framework that a\nvisit to a location must take place during a specific time window in order to\nbe counted and all time windows are the same length. In the first problem, the\ngoal is to visit as many locations as possible using a fixed number of\nvehicles. In the second, the goal is to visit all locations using the smallest\nnumber of vehicles possible. For the first problem, we present an approximation\nalgorithm whose output path collects a reward within a constant factor of\noptimal for any fixed number of vehicles. For the second problem, our algorithm\nfinds a 6-approximation to the problem on a tree metric, whenever a single\nvehicle could visit all locations during their time windows."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.3960v1", 
    "title": "Speedup in the Traveling Repairman Problem with Constrained Time Windows", 
    "arxiv-id": "1101.3960v1", 
    "author": "Barry Wittman", 
    "publish": "2011-01-20T16:51:55Z", 
    "summary": "A bicriteria approximation algorithm is presented for the unrooted traveling\nrepairman problem, realizing increased profit in return for increased speedup\nof repairman motion. The algorithm generalizes previous results from the case\nin which all time windows are the same length to the case in which their\nlengths can range between l and 2. This analysis can extend to any range of\ntime window lengths, following our earlier techniques. This relationship\nbetween repairman profit and speedup is applicable over a range of values that\nis dependent on the cost of putting the input in an especially desirable form,\ninvolving what are called \"trimmed windows.\" For time windows with lengths\nbetween 1 and 2, the range of values for speedup $s$ for which our analysis\nholds is $1 \\leq s \\leq 6$. In this range, we establish an approximation ratio\nthat is constant for any specific value of $s$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.4065v1", 
    "title": "Self-Index Based on LZ77", 
    "arxiv-id": "1101.4065v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-01-21T02:43:16Z", 
    "summary": "We introduce the first self-index based on the Lempel-Ziv 1977 compression\nformat (LZ77). It is particularly competitive for highly repetitive text\ncollections such as sequence databases of genomes of related species, software\nrepositories, versioned document collections, and temporal text databases. Such\ncollections are extremely compressible but classical self-indexes fail to\ncapture that source of compressibility. Our self-index takes in practice a few\ntimes the space of the text compressed with LZ77 (as little as 2.6 times),\nextracts 1--2 million characters of the text per second, and finds patterns at\na rate of 10--50 microseconds per occurrence. It is smaller (up to one half)\nthan the best current self-index for repetitive collections, and faster in many\ncases."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.4068v1", 
    "title": "Linear-Space Data Structures for Range Mode Query in Arrays", 
    "arxiv-id": "1101.4068v1", 
    "author": "Jason Morrison", 
    "publish": "2011-01-21T03:23:57Z", 
    "summary": "A mode of a multiset $S$ is an element $a \\in S$ of maximum multiplicity;\nthat is, $a$ occurs at least as frequently as any other element in $S$. Given a\nlist $A[1:n]$ of $n$ items, we consider the problem of constructing a data\nstructure that efficiently answers range mode queries on $A$. Each query\nconsists of an input pair of indices $(i, j)$ for which a mode of $A[i:j]$ must\nbe returned. We present an $O(n^{2-2\\epsilon})$-space static data structure\nthat supports range mode queries in $O(n^\\epsilon)$ time in the worst case, for\nany fixed $\\epsilon \\in [0,1/2]$. When $\\epsilon = 1/2$, this corresponds to\nthe first linear-space data structure to guarantee $O(\\sqrt{n})$ query time. We\nthen describe three additional linear-space data structures that provide\n$O(k)$, $O(m)$, and $O(|j-i|)$ query time, respectively, where $k$ denotes the\nnumber of distinct elements in $A$ and $m$ denotes the frequency of the mode of\n$A$. Finally, we examine generalizing our data structures to higher dimensions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.4446v1", 
    "title": "High-Confidence Predictions under Adversarial Uncertainty", 
    "arxiv-id": "1101.4446v1", 
    "author": "Andrew Drucker", 
    "publish": "2011-01-24T05:35:05Z", 
    "summary": "We study the setting in which the bits of an unknown infinite binary sequence\nx are revealed sequentially to an observer. We show that very limited\nassumptions about x allow one to make successful predictions about unseen bits\nof x. First, we study the problem of successfully predicting a single 0 from\namong the bits of x. In our model we have only one chance to make a prediction,\nbut may do so at a time of our choosing. We describe and motivate this as the\nproblem of a frog who wants to cross a road safely.\n  Letting N_t denote the number of 1s among the first t bits of x, we say that\nx is \"eps-weakly sparse\" if lim inf (N_t/t) <= eps. Our main result is a\nrandomized algorithm that, given any eps-weakly sparse sequence x, predicts a 0\nof x with success probability as close as desired to 1 - \\eps. Thus we can\nperform this task with essentially the same success probability as under the\nmuch stronger assumption that each bit of x takes the value 1 independently\nwith probability eps. We apply this result to show how to successfully predict\na bit (0 or 1) under a broad class of possible assumptions on the sequence x.\nThe assumptions are stated in terms of the behavior of a finite automaton M\nreading the bits of x.\n  We also propose and solve a variant of the well-studied \"ignorant\nforecasting\" problem. For every eps > 0, we give a randomized forecasting\nalgorithm S_eps that, given sequential access to a binary sequence x, makes a\nprediction of the form: \"A p fraction of the next N bits will be 1s.\" (The\nalgorithm gets to choose p, N, and the time of the prediction.) For any fixed\nsequence x, the forecast fraction p is accurate to within +-eps with\nprobability 1 - eps."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.4491v3", 
    "title": "Conflict Packing: an unifying technique to obtain polynomial kernels for   editing problems on dense instances", 
    "arxiv-id": "1101.4491v3", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2011-01-24T10:42:46Z", 
    "summary": "We develop a technique that we call Conflict Packing in the context of\nkernelization, obtaining (and improving) several polynomial kernels for editing\nproblems on dense instances. We apply this technique on several well-studied\nproblems: Feedback Arc Set in (Bipartite) Tournaments, Dense Rooted Triplet\nInconsistency and Betweenness in Tournaments. For the former, one is given a\n(bipartite) tournament $T = (V,A)$ and seeks a set of at most $k$ arcs whose\nreversal in $T$ results in an acyclic (bipartite) tournament. While a linear\nvertex-kernel is already known for the first problem, using the Conflict\nPacking allows us to find a so-called safe partition, the central tool of the\nkernelization algorithm in, with simpler arguments. For the case of bipartite\ntournaments, the same technique allows us to obtain a quadratic vertex-kernel.\nAgain, such a kernel was already known to exist, using the concept of so-called\nbimodules. We believe however that providing an unifying technique to cope with\nsuch problems is interesting. Regarding Dense Rooted Triplet Inconsistency, one\nis given a set of vertices $V$ and a dense collection $\\mathcal{R}$ of rooted\nbinary trees over three vertices of $V$ and seeks a rooted tree over $V$\ncontaining all but at most $k$ triplets from $\\mathcal{R}$. As a main\nconsequence of our technique, we prove that the Dense Rooted Triplet\nInconsistency problem admits a linear vertex-kernel. This result improves the\nbest known bound of $O(k^2)$ vertices for this problem. Finally, we use this\ntechnique to obtain a linear vertex-kernel for Betweenness in Tournaments,\nwhere one is given a set of vertices $V$ and a dense collection $\\mathcal{R}$\nof so-called betweenness triplets and seeks a linear ordering of the vertices\ncontaining all but at most $k$ triplets from $\\mathcal{R}$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.5407v1", 
    "title": "Maintaining Arrays of Contiguous Objects", 
    "arxiv-id": "1101.5407v1", 
    "author": "Nils Schweer", 
    "publish": "2011-01-27T22:48:51Z", 
    "summary": "In this paper we consider methods for dynamically storing a set of different\nobjects (\"modules\") in a physical array. Each module requires one free\ncontiguous subinterval in order to be placed. Items are inserted or removed,\nresulting in a fragmented layout that makes it harder to insert further\nmodules. It is possible to relocate modules, one at a time, to another free\nsubinterval that is contiguous and does not overlap with the current location\nof the module. These constraints clearly distinguish our problem from classical\nmemory allocation. We present a number of algorithmic results, including a\nbound of Theta(n^2) on physical sorting if there is a sufficiently large free\nspace and sum up NP-hardness results for arbitrary initial layouts. For online\nscenarios in which modules arrive one at a time, we present a method that\nrequires O(1) moves per insertion or deletion and amortized cost O(m_i log M)\nper insertion or deletion, where m_i is the module's size, M is the size of the\nlargest module and costs for moves are linear in the size of a module."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.06.008", 
    "link": "http://arxiv.org/pdf/1101.5506v1", 
    "title": "Compressed String Dictionaries", 
    "arxiv-id": "1101.5506v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-01-28T11:08:46Z", 
    "summary": "The problem of storing a set of strings --- a string dictionary --- in\ncompact form appears naturally in many cases. While classically it has\nrepresented a small part of the whole data to be processed (e.g., for Natural\nLanguage processing or for indexing text collections), more recent applications\nin Web engines, Web mining, RDF graphs, Internet routing, Bioinformatics, and\nmany others, make use of very large string dictionaries, whose size is a\nsignificant fraction of the whole data. Thus novel approaches to compress them\nefficiently are necessary. In this paper we experimentally compare time and\nspace performance of some existing alternatives, as well as new ones we\npropose. We show that space reductions of up to 20% of the original size of the\nstrings is possible while supporting fast dictionary searches."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.06.010", 
    "link": "http://arxiv.org/pdf/1101.5518v3", 
    "title": "The complexity of Free-Flood-It on 2xn boards", 
    "arxiv-id": "1101.5518v3", 
    "author": "Alexander Scott", 
    "publish": "2011-01-28T11:51:32Z", 
    "summary": "We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Our main result is that\ncomputing the length of an optimal sequence is fixed parameter tractable (with\nthe number of colours present as a parameter) when restricted to rectangular\n2xn boards. We also show that, when the number of colours is unbounded, the\nproblem remains NP-hard on such boards. This resolves a question of Clifford,\nJalsenius, Montanaro and Sach (2010)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.06.010", 
    "link": "http://arxiv.org/pdf/1101.5586v1", 
    "title": "A 4/3-approximation for TSP on cubic 3-edge-connected graphs", 
    "arxiv-id": "1101.5586v1", 
    "author": "Swati Gupta", 
    "publish": "2011-01-28T17:45:05Z", 
    "summary": "We provide a polynomial time 4/3 approximation algorithm for TSP on metrics\narising from the metric completion of cubic 3-edge connected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1101.5876v3", 
    "title": "The complexity of flood-filling games on graphs", 
    "arxiv-id": "1101.5876v3", 
    "author": "Alexander Scott", 
    "publish": "2011-01-31T08:42:14Z", 
    "summary": "We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Although computing the\nminimum number of moves required to flood an arbitrary graph is known to be\nNP-hard, we demonstrate a polynomial time algorithm to compute the minimum\nnumber of moves required to link each pair of vertices. We apply this result to\ncompute in polynomial time the minimum number of moves required to flood a\npath, and an additive approximation to this quantity for an arbitrary k x n\nboard, coloured with a bounded number of colours, for any fixed k. On the other\nhand, we show that, for k>=3, determining the minimum number of moves required\nto flood a k x n board coloured with at least four colours remains NP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1101.5944v2", 
    "title": "Random Walk on Directed Dynamic Graphs", 
    "arxiv-id": "1101.5944v2", 
    "author": "Luis Rodrigues", 
    "publish": "2011-01-31T13:01:30Z", 
    "summary": "Dynamic graphs have emerged as an appropriate model to capture the changing\nnature of many modern networks, such as peer-to-peer overlays and mobile ad hoc\nnetworks. Most of the recent research on dynamic networks has only addressed\nthe undirected dynamic graph model. However, realistic networks such as the\nones identified above are directed. In this paper we present early work in\naddressing the properties of directed dynamic graphs. In particular, we explore\nthe problem of random walk in such graphs. We assume the existence of an\noblivious adversary that makes arbitrary changes in every communication round.\nWe explore the problem of covering the dynamic graph, that even in the static\ncase can be exponential, and we establish an upper bound O(d_max n^3 log^2 n)\nof the cover time for balanced dynamic graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0041v2", 
    "title": "Consecutive Ones Property and PQ-Trees for Multisets: Hardness of   Counting Their Orderings", 
    "arxiv-id": "1102.0041v2", 
    "author": "Noemi Scutell\u00e0", 
    "publish": "2011-01-31T23:59:40Z", 
    "summary": "A binary matrix satisfies the consecutive ones property (COP) if its columns\ncan be permuted such that the ones in each row of the resulting matrix are\nconsecutive. Equivalently, a family of sets F = {Q_1,..,Q_m}, where Q_i is\nsubset of R for some universe R, satisfies the COP if the symbols in R can be\npermuted such that the elements of each set Q_i occur consecutively, as a\ncontiguous segment of the permutation of R's symbols. We consider the COP\nversion on multisets and prove that counting its solutions is difficult\n(#P-complete). We prove completeness results also for counting the frontiers of\nPQ-trees, which are typically used for testing the COP on sets, thus showing\nthat a polynomial algorithm is unlikely to exist when dealing with multisets.\nWe use a combinatorial approach based on parsimonious reductions from the\nHamiltonian path problem, showing that the decisional version of our problems\nis therefore NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0395v1", 
    "title": "Combined Data Structure for Previous- and Next-Smaller-Values", 
    "arxiv-id": "1102.0395v1", 
    "author": "Johannes Fischer", 
    "publish": "2011-02-02T10:20:58Z", 
    "summary": "Let $A$ be a static array storing $n$ elements from a totally ordered set. We\npresent a data structure of optimal size at most $n\\log_2(3+2\\sqrt{2})+o(n)$\nbits that allows us to answer the following queries on $A$ in constant time,\nwithout accessing $A$: (1) previous smaller value queries, where given an index\n$i$, we wish to find the first index to the left of $i$ where $A$ is strictly\nsmaller than at $i$, and (2) next smaller value queries, which search to the\nright of $i$. As an additional bonus, our data structure also allows to answer\na third kind of query: given indices $i<j$, find the position of the minimum in\n$A[i..j]$. Our data structure has direct consequences for the space-efficient\nstorage of suffix trees."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0471v2", 
    "title": "Matrix method for the multi salesmen problem (TSP) with several vehicles", 
    "arxiv-id": "1102.0471v2", 
    "author": "Elena Ishkova", 
    "publish": "2011-02-01T11:56:13Z", 
    "summary": "In this paper discussed procedure of separation of the original problem with\nseveral vehicles to a number of simpler problems with one vehicle which based\non the matrix approach."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0588v2", 
    "title": "Adaptive Population Models for Offspring Populations and Parallel   Evolutionary Algorithms", 
    "arxiv-id": "1102.0588v2", 
    "author": "Dirk Sudholt", 
    "publish": "2011-02-02T23:39:57Z", 
    "summary": "We present two adaptive schemes for dynamically choosing the number of\nparallel instances in parallel evolutionary algorithms. This includes the\nchoice of the offspring population size in a (1+$\\lambda$) EA as a special\ncase. Our schemes are parameterless and they work in a black-box setting where\nno knowledge on the problem is available. Both schemes double the number of\ninstances in case a generation ends without finding an improvement. In a\nsuccessful generation, the first scheme resets the system to one instance,\nwhile the second scheme halves the number of instances. Both schemes provide\nnear-optimal speed-ups in terms of the parallel time. We give upper bounds for\nthe asymptotic sequential time (i.e., the total number of function evaluations)\nthat are not larger than upper bounds for a corresponding non-parallel\nalgorithm derived by the fitness-level method."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1124v3", 
    "title": "A Polynomial Time Algorithm for a Special Case of Linear Integer   Programming", 
    "arxiv-id": "1102.1124v3", 
    "author": "Yahya Tabesh", 
    "publish": "2011-02-06T04:53:31Z", 
    "summary": "This article has been withdrawn."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1273v1", 
    "title": "One to Rule Them All: a General Randomized Algorithm for Buffer   Management with Bounded Delay", 
    "arxiv-id": "1102.1273v1", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2011-02-07T11:09:34Z", 
    "summary": "We give a memoryless scale-invariant randomized algorithm for the Buffer\nManagement with Bounded Delay problem that is e/(e-1)-competitive against an\nadaptive adversary, together with better performance guarantees for many\nrestricted variants, including the s-bounded instances. In particular, our\nalgorithm attains the optimum competitive ratio of 4/3 on 2-bounded instances.\n  Both the algorithm and its analysis are applicable to a more general problem,\ncalled Collecting Items, in which only the relative order between packets'\ndeadlines is known. Our algorithm is the optimal randomized memoryless\nalgorithm against adaptive adversary for that problem in a strong sense.\n  While some of provided upper bounds were already known, in general, they were\nattained by several different algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1472v1", 
    "title": "Algorithms for Implicit Hitting Set Problems", 
    "arxiv-id": "1102.1472v1", 
    "author": "Santosh Vempala", 
    "publish": "2011-02-07T23:40:58Z", 
    "summary": "A hitting set for a collection of sets is a set that has a non-empty\nintersection with each set in the collection; the hitting set problem is to\nfind a hitting set of minimum cardinality. Motivated by instances of the\nhitting set problem where the number of sets to be hit is large, we introduce\nthe notion of implicit hitting set problems. In an implicit hitting set problem\nthe collection of sets to be hit is typically too large to list explicitly;\ninstead, an oracle is provided which, given a set H, either determines that H\nis a hitting set or returns a set that H does not hit. We show a number of\nexamples of classic implicit hitting set problems, and give a generic algorithm\nfor solving such problems optimally. The main contribution of this paper is to\nshow that this framework is valuable in developing approximation algorithms. We\nillustrate this methodology by presenting a simple on-line algorithm for the\nminimum feedback vertex set problem on random graphs. In particular our\nalgorithm gives a feedback vertex set of size n-(1/p)\\log{np}(1-o(1)) with\nprobability at least 3/4 for the random graph G_{n,p} (the smallest feedback\nvertex set is of size n-(2/p)\\log{np}(1+o(1))). We also consider a planted\nmodel for the feedback vertex set in directed random graphs. Here we show that\na hitting set for a polynomial-sized subset of cycles is a hitting set for the\nplanted random graph and this allows us to exactly recover the planted feedback\nvertex set."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.1746v1", 
    "title": "Algorithms for Jumbled Pattern Matching in Strings", 
    "arxiv-id": "1102.1746v1", 
    "author": "Zsuzsanna Lipt\u00e1k", 
    "publish": "2011-02-08T23:11:17Z", 
    "summary": "The Parikh vector p(s) of a string s is defined as the vector of\nmultiplicities of the characters. Parikh vector q occurs in s if s has a\nsubstring t with p(t)=q. We present two novel algorithms for searching for a\nquery q in a text s. One solves the decision problem over a binary text in\nconstant time, using a linear size index of the text. The second algorithm, for\na general finite alphabet, finds all occurrences of a given Parikh vector q and\nhas sub-linear expected time complexity; we present two variants, which both\nuse a linear size index of the text."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3393v1", 
    "title": "Better Bounds for Incremental Frequency Allocation in Bipartite Graphs", 
    "arxiv-id": "1102.3393v1", 
    "author": "Ji\u0159\u00ed Sgall", 
    "publish": "2011-02-16T18:32:07Z", 
    "summary": "We study frequency allocation in wireless networks. A wireless network is\nmodeled by an undirected graph, with vertices corresponding to cells. In each\nvertex we have a certain number of requests, and each of those requests must be\nassigned a different frequency. Edges represent conflicts between cells,\nmeaning that frequencies in adjacent vertices must be different as well. The\nobjective is to minimize the total number of used frequencies.\n  The offline version of the problem is known to be NP-hard. In the incremental\nversion, requests for frequencies arrive over time and the algorithm is\nrequired to assign a frequency to a request as soon as it arrives. Competitive\nincremental algorithms have been studied for several classes of graphs. For\npaths, the optimal (asymptotic) ratio is known to be 4/3, while for\nhexagonal-cell graphs it is between 1.5 and 1.9126. For k-colorable graphs, the\nratio of (k+1)/2 can be achieved.\n  In this paper, we prove nearly tight bounds on the asymptotic competitive\nratio for bipartite graphs, showing that it is between 1.428 and 1.433. This\nimproves the previous lower bound of 4/3 and upper bound of 1.5. Our proofs are\nbased on reducing the incremental problem to a purely combinatorial\n(equivalent) problem of constructing set families with certain intersection\nproperties."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3491v1", 
    "title": "A simple PTAS for Weighted Matroid Matching on Strongly Base Orderable   Matroids", 
    "arxiv-id": "1102.3491v1", 
    "author": "Jos\u00e9 A. Soto", 
    "publish": "2011-02-17T04:22:13Z", 
    "summary": "We give a simple polynomial time approximation scheme for the weighted\nmatroid matching problem on strongly base orderable matroids. We also show that\neven the unweighted version of this problem is NP-complete and not in\noracle-coNP."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3537v1", 
    "title": "Even Better Framework for min-wise Based Algorithms", 
    "arxiv-id": "1102.3537v1", 
    "author": "Ariel Shiftan", 
    "publish": "2011-02-17T09:43:17Z", 
    "summary": "In a recent paper from SODA11 \\cite{kminwise} the authors introduced a\ngeneral framework for exponential time improvement of \\minwise based algorithms\nby defining and constructing almost \\kmin independent family of hash functions.\nHere we take it a step forward and reduce the space and the independent needed\nfor representing the functions, by defining and constructing a \\dkmin\nindependent family of hash functions. Surprisingly, for most cases only 8-wise\nindependent is needed for exponential time and space improvement. Moreover, we\nbypass the $O(\\log{\\frac{1}{\\epsilon}})$ independent lower bound for\napproximately \\minwise functions \\cite{patrascu10kwise-lb}, as we use\nalternative definition. In addition, as the independent's degree is a small\nconstant it can be implemented efficiently.\n  Informally, under this definition, all subsets of size $d$ of any fixed set\n$X$ have an equal probability to have hash values among the minimal $k$ values\nin $X$, where the probability is over the random choice of hash function from\nthe family. This property measures the randomness of the family, as choosing a\ntruly random function, obviously, satisfies the definition for $d=k=|X|$. We\ndefine and give an efficient time and space construction of approximately\n\\dkmin independent family of hash functions. The degree of independent required\nis optimal, i.e. only $O(d)$ for $2 \\le d < k=O(\\frac{d}{\\epsilon^2})$, where\n$\\epsilon \\in (0,1)$ is the desired error bound. This construction can be used\nto improve many \\minwise based algorithms, such as\n\\cite{sizeEstimationFramework,Datar02estimatingrarity,NearDuplicate,SimilaritySearch,DBLP:conf/podc/CohenK07},\nas will be discussed here. To our knowledge such definitions, for hash\nfunctions, were never studied and no construction was given before."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3643v2", 
    "title": "A Constant Factor Approximation Algorithm for Unsplittable Flow on Paths", 
    "arxiv-id": "1102.3643v2", 
    "author": "Andreas Wiese", 
    "publish": "2011-02-17T17:43:08Z", 
    "summary": "In the unsplittable flow problem on a path, we are given a capacitated path\n$P$ and $n$ tasks, each task having a demand, a profit, and start and end\nvertices. The goal is to compute a maximum profit set of tasks, such that for\neach edge $e$ of $P$, the total demand of selected tasks that use $e$ does not\nexceed the capacity of $e$. This is a well-studied problem that has been\nstudied under alternative names, such as resource allocation, bandwidth\nallocation, resource constrained scheduling, temporal knapsack and interval\npacking.\n  We present a polynomial time constant-factor approximation algorithm for this\nproblem. This improves on the previous best known approximation ratio of\n$O(\\log n)$. The approximation ratio of our algorithm is $7+\\epsilon$ for any\n$\\epsilon>0$.\n  We introduce several novel algorithmic techniques, which might be of\nindependent interest: a framework which reduces the problem to instances with a\nbounded range of capacities, and a new geometrically inspired dynamic program\nwhich solves a special case of the maximum weight independent set of rectangles\nproblem to optimality. In the setting of resource augmentation, wherein the\ncapacities can be slightly violated, we give a $(2+\\epsilon)$-approximation\nalgorithm. In addition, we show that the problem is strongly NP-hard even if\nall edge capacities are equal and all demands are either~1,~2, or~3."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3749v1", 
    "title": "Approximation Algorithms for Correlated Knapsacks and Non-Martingale   Bandits", 
    "arxiv-id": "1102.3749v1", 
    "author": "R. Ravi", 
    "publish": "2011-02-18T04:05:21Z", 
    "summary": "In the stochastic knapsack problem, we are given a knapsack of size B, and a\nset of jobs whose sizes and rewards are drawn from a known probability\ndistribution. However, we know the actual size and reward only when the job\ncompletes. How should we schedule jobs to maximize the expected total reward?\nWe know O(1)-approximations when we assume that (i) rewards and sizes are\nindependent random variables, and (ii) we cannot prematurely cancel jobs. What\ncan we say when either or both of these assumptions are changed?\n  The stochastic knapsack problem is of interest in its own right, but\ntechniques developed for it are applicable to other stochastic packing\nproblems. Indeed, ideas for this problem have been useful for budgeted learning\nproblems, where one is given several arms which evolve in a specified\nstochastic fashion with each pull, and the goal is to pull the arms a total of\nB times to maximize the reward obtained. Much recent work on this problem focus\non the case when the evolution of the arms follows a martingale, i.e., when the\nexpected reward from the future is the same as the reward at the current state.\nWhat can we say when the rewards do not form a martingale?\n  In this paper, we give constant-factor approximation algorithms for the\nstochastic knapsack problem with correlations and/or cancellations, and also\nfor budgeted learning problems where the martingale condition is not satisfied.\nIndeed, we can show that previously proposed LP relaxations have large\nintegrality gaps. We propose new time-indexed LP relaxations, and convert the\nfractional solutions into distributions over strategies, and then use the LP\nvalues and the time ordering information from these strategies to devise a\nrandomized adaptive scheduling algorithm. We hope our LP formulation and\ndecomposition methods may provide a new way to address other correlated bandit\nproblems with more general contexts."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3813v2", 
    "title": "Efficient Algorithms for Dualizing Large-Scale Hypergraphs", 
    "arxiv-id": "1102.3813v2", 
    "author": "Takeaki Uno", 
    "publish": "2011-02-18T11:51:28Z", 
    "summary": "A hypergraph ${\\cal F}$ is a set family defined on vertex set $V$. The dual\nof ${\\cal F}$ is the set of minimal subsets $H$ of $V$ such that $F\\cap H \\ne\n\\emptyset$ for any $F\\in {\\cal F}$. The computation of the dual is equivalent\nto many problems, such as minimal hitting set enumeration of a subset family,\nminimal set cover enumeration, and the enumeration of hypergraph transversals.\nAlthough many algorithms have been proposed for solving the problem, to the\nbest of our knowledge, none of them can work on large-scale input with a large\nnumber of output minimal hitting sets. This paper focuses on developing time-\nand space-efficient algorithms for solving the problem. We propose two new\nalgorithms with new search methods, new pruning methods, and fast techniques\nfor the minimality check. The computational experiments show that our\nalgorithms are quite fast even for large-scale input for which existing\nalgorithms do not terminate in a practical time."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4480v3", 
    "title": "LGM: Mining Frequent Subgraphs from Linear Graphs", 
    "arxiv-id": "1102.4480v3", 
    "author": "Koji Tsuda", 
    "publish": "2011-02-22T12:20:18Z", 
    "summary": "A linear graph is a graph whose vertices are totally ordered. Biological and\nlinguistic sequences with interactions among symbols are naturally represented\nas linear graphs. Examples include protein contact maps, RNA secondary\nstructures and predicate-argument structures. Our algorithm, linear graph miner\n(LGM), leverages the vertex order for efficient enumeration of frequent\nsubgraphs. Based on the reverse search principle, the pattern space is\nsystematically traversed without expensive duplication checking. Disconnected\nsubgraph patterns are particularly important in linear graphs due to their\nsequential nature. Unlike conventional graph mining algorithms detecting\nconnected patterns only, LGM can detect disconnected patterns as well. The\nutility and efficiency of LGM are demonstrated in experiments on protein\ncontact maps."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4523v1", 
    "title": "On Dynamic Optimality for Binary Search Trees", 
    "arxiv-id": "1102.4523v1", 
    "author": "Manoj Gupta", 
    "publish": "2011-02-22T14:47:19Z", 
    "summary": "Does there exist O(1)-competitive (self-adjusting) binary search tree (BST)\nalgorithms? This is a well-studied problem. A simple offline BST algorithm\nGreedyFuture was proposed independently by Lucas and Munro, and they\nconjectured it to be O(1)-competitive. Recently, Demaine et al. gave a\ngeometric view of the BST problem. This view allowed them to give an online\nalgorithm GreedyArb with the same cost as GreedyFuture. However, no\no(n)-competitive ratio was known for GreedyArb. In this paper we make progress\ntowards proving O(1)-competitive ratio for GreedyArb by showing that it is\nO(\\log n)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4842v4", 
    "title": "A nearly-mlogn time solver for SDD linear systems", 
    "arxiv-id": "1102.4842v4", 
    "author": "Richard Peng", 
    "publish": "2011-02-23T20:53:03Z", 
    "summary": "We present an improved algorithm for solving symmetrically diagonally\ndominant linear systems. On input of an $n\\times n$ symmetric diagonally\ndominant matrix $A$ with $m$ non-zero entries and a vector $b$ such that\n$A\\bar{x} = b$ for some (unknown) vector $\\bar{x}$, our algorithm computes a\nvector $x$ such that $||{x}-\\bar{x}||_A < \\epsilon ||\\bar{x}||_A $\n{$||\\cdot||_A$ denotes the A-norm} in time $${\\tilde O}(m\\log n \\log\n(1/\\epsilon)).$$\n  The solver utilizes in a standard way a `preconditioning' chain of\nprogressively sparser graphs. To claim the faster running time we make a\ntwo-fold improvement in the algorithm for constructing the chain. The new chain\nexploits previously unknown properties of the graph sparsification algorithm\ngiven in [Koutis,Miller,Peng, FOCS 2010], allowing for stronger preconditioning\nproperties. We also present an algorithm of independent interest that\nconstructs nearly-tight low-stretch spanning trees in time\n$\\tilde{O}(m\\log{n})$, a factor of $O(\\log{n})$ faster than the algorithm in\n[Abraham,Bartal,Neiman, FOCS 2008]. This speedup directly reflects on the\nconstruction time of the preconditioning chain."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4884v3", 
    "title": "Upper Bounds for Maximally Greedy Binary Search Trees", 
    "arxiv-id": "1102.4884v3", 
    "author": "Kyle Fox", 
    "publish": "2011-02-24T00:19:46Z", 
    "summary": "At SODA 2009, Demaine et al. presented a novel connection between binary\nsearch trees (BSTs) and subsets of points on the plane. This connection was\nindependently discovered by Derryberry et al. As part of their results, Demaine\net al. considered GreedyFuture, an offline BST algorithm that greedily\nrearranges the search path to minimize the cost of future searches. They showed\nthat GreedyFuture is actually an online algorithm in their geometric view, and\nthat there is a way to turn GreedyFuture into an online BST algorithm with only\na constant factor increase in total search cost. Demaine et al. conjectured\nthis algorithm was dynamically optimal, but no upper bounds were given in their\npaper. We prove the first non-trivial upper bounds for the cost of search\noperations using GreedyFuture including giving an access lemma similar to that\nfound in Sleator and Tarjan's classic paper on splay trees."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5105v1", 
    "title": "Approximation Algorithms for Union and Intersection Covering Problems", 
    "arxiv-id": "1102.5105v1", 
    "author": "Piotr Sankowski", 
    "publish": "2011-02-24T21:33:20Z", 
    "summary": "In a classical covering problem, we are given a set of requests that we need\nto satisfy (fully or partially), by buying a subset of items at minimum cost.\nFor example, in the k-MST problem we want to find the cheapest tree spanning at\nleast k nodes of an edge-weighted graph. Here nodes and edges represent\nrequests and items, respectively.\n  In this paper, we initiate the study of a new family of multi-layer covering\nproblems. Each such problem consists of a collection of h distinct instances of\na standard covering problem (layers), with the constraint that all layers share\nthe same set of requests. We identify two main subfamilies of these problems: -\nin a union multi-layer problem, a request is satisfied if it is satisfied in at\nleast one layer; - in an intersection multi-layer problem, a request is\nsatisfied if it is satisfied in all layers. To see some natural applications,\nconsider both generalizations of k-MST. Union k-MST can model a problem where\nwe are asked to connect a set of users to at least one of two communication\nnetworks, e.g., a wireless and a wired network. On the other hand, intersection\nk-MST can formalize the problem of connecting a subset of users to both\nelectricity and water.\n  We present a number of hardness and approximation results for union and\nintersection versions of several standard optimization problems: MST, Steiner\ntree, set cover, facility location, TSP, and their partial covering variants."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5146v1", 
    "title": "Structure-Aware Sampling: Flexible and Accurate Summarization", 
    "arxiv-id": "1102.5146v1", 
    "author": "Nick Duffield", 
    "publish": "2011-02-25T03:39:45Z", 
    "summary": "In processing large quantities of data, a fundamental problem is to obtain a\nsummary which supports approximate query answering. Random sampling yields\nflexible summaries which naturally support subset-sum queries with unbiased\nestimators and well-understood confidence bounds.\n  Classic sample-based summaries, however, are designed for arbitrary subset\nqueries and are oblivious to the structure in the set of keys. The particular\nstructure, such as hierarchy, order, or product space (multi-dimensional),\nmakes range queries much more relevant for most analysis of the data.\n  Dedicated summarization algorithms for range-sum queries have also been\nextensively studied. They can outperform existing sampling schemes in terms of\naccuracy on range queries per summary size. Their accuracy, however, rapidly\ndegrades when, as is often the case, the query spans multiple ranges. They are\nalso less flexible - being targeted for range sum queries alone - and are often\nquite costly to build and use.\n  In this paper we propose and evaluate variance optimal sampling schemes that\nare structure-aware. These summaries improve over the accuracy of existing\nstructure-oblivious sampling schemes on range queries while retaining the\nbenefits of sample-based summaries: flexible summaries, with high accuracy on\nboth range queries and arbitrary subset queries."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5441v2", 
    "title": "Obtaining a Bipartite Graph by Contracting Few Edges", 
    "arxiv-id": "1102.5441v2", 
    "author": "Christophe Paul", 
    "publish": "2011-02-26T20:07:27Z", 
    "summary": "We initiate the study of the Bipartite Contraction problem from the\nperspective of parameterized complexity. In this problem we are given a graph\n$G$ and an integer $k$, and the task is to determine whether we can obtain a\nbipartite graph from $G$ by a sequence of at most $k$ edge contractions. Our\nmain result is an $f(k) n^{O(1)}$ time algorithm for Bipartite Contraction.\nDespite a strong resemblance between Bipartite Contraction and the classical\nOdd Cycle Transversal (OCT) problem, the methods developed to tackle OCT do not\nseem to be directly applicable to Bipartite Contraction. Our algorithm is based\non a novel combination of the irrelevant vertex technique, introduced by\nRobertson and Seymour, and the concept of important separators. Both techniques\nhave previously been used as key components of algorithms for fundamental\nproblems in parameterized complexity. However, to the best of our knowledge,\nthis is the first time the two techniques are applied in unison."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5450v1", 
    "title": "Minimum Makespan Multi-vehicle Dial-a-Ride", 
    "arxiv-id": "1102.5450v1", 
    "author": "R. Ravi", 
    "publish": "2011-02-26T21:37:08Z", 
    "summary": "Dial a ride problems consist of a metric space (denoting travel time between\nvertices) and a set of m objects represented as source-destination pairs, where\neach object requires to be moved from its source to destination vertex. We\nconsider the multi-vehicle Dial a ride problem, with each vehicle having\ncapacity k and its own depot-vertex, where the objective is to minimize the\nmaximum completion time (makespan) of the vehicles. We study the \"preemptive\"\nversion of the problem, where an object may be left at intermediate vertices\nand transported by more than one vehicle, while being moved from source to\ndestination. Our main results are an O(log^3 n)-approximation algorithm for\npreemptive multi-vehicle Dial a ride, and an improved O(log t)-approximation\nfor its special case when there is no capacity constraint. We also show that\nthe approximation ratios improve by a log-factor when the underlying metric is\ninduced by a fixed-minor-free graph."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5540v2", 
    "title": "Hierarchical Heavy Hitters with the Space Saving Algorithm", 
    "arxiv-id": "1102.5540v2", 
    "author": "Justin Thaler", 
    "publish": "2011-02-27T19:31:05Z", 
    "summary": "The Hierarchical Heavy Hitters problem extends the notion of frequent items\nto data arranged in a hierarchy. This problem has applications to network\ntraffic monitoring, anomaly detection, and DDoS detection. We present a new\nstreaming approximation algorithm for computing Hierarchical Heavy Hitters that\nhas several advantages over previous algorithms. It improves on the worst-case\ntime and space bounds of earlier algorithms, is conceptually simple and\nsubstantially easier to implement, offers improved accuracy guarantees, is\neasily adopted to a distributed or parallel setting, and can be efficiently\nimplemented in commodity hardware such as ternary content addressable memory\n(TCAMs). We present experimental results showing that for parameters of primary\npractical interest, our two-dimensional algorithm is superior to existing\nalgorithms in terms of speed and accuracy, and competitive in terms of space,\nwhile our one-dimensional algorithm is also superior in terms of speed and\naccuracy for a more limited range of parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0106v1", 
    "title": "Hypergraph Partitioning through Vertex Separators on Graphs", 
    "arxiv-id": "1103.0106v1", 
    "author": "Cevdet Aykanat", 
    "publish": "2011-03-01T08:42:24Z", 
    "summary": "The modeling flexibility provided by hypergraphs has drawn a lot of interest\nfrom the combinatorial scientific community, leading to novel models and\nalgorithms, their applications, and development of associated tools.\nHypergraphs are now a standard tool in combinatorial scientific computing. The\nmodeling flexibility of hypergraphs however, comes at a cost: algorithms on\nhypergraphs are inherently more complicated than those on graphs, which\nsometimes translate to nontrivial increases in processing times. Neither the\nmodeling flexibility of hypergraphs, nor the runtime efficiency of graph\nalgorithms can be overlooked. Therefore, the new research thrust should be how\nto cleverly trade-off between the two. This work addresses one method for this\ntrade-off by solving the hypergraph partitioning problem by finding vertex\nseparators on graphs. Specifically, we investigate how to solve the hypergraph\npartitioning problem by seeking a vertex separator on its net intersection\ngraph (NIG), where each net of the hypergraph is represented by a vertex, and\ntwo vertices share an edge if their nets have a common vertex. We propose a\nvertex-weighting scheme to attain good node-balanced hypergraphs, since NIG\nmodel cannot preserve node balancing information. Vertex-removal and\nvertex-splitting techniques are described to optimize cutnet and connectivity\nmetrics, respectively, under the recursive bipartitioning paradigm. We also\ndeveloped an implementation for our GPVS-based HP formulations by adopting and\nmodifying a state-of-the-art GPVS tool onmetis. Experiments conducted on a\nlarge collection of sparse matrices confirmed the validity of our proposed\ntechniques."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0318v1", 
    "title": "Listing All Maximal Cliques in Large Sparse Real-World Graphs", 
    "arxiv-id": "1103.0318v1", 
    "author": "Darren Strash", 
    "publish": "2011-03-02T00:00:26Z", 
    "summary": "We implement a new algorithm for listing all maximal cliques in sparse graphs\ndue to Eppstein, L\\\"offler, and Strash (ISAAC 2010) and analyze its performance\non a large corpus of real-world graphs. Our analysis shows that this algorithm\nis the first to offer a practical solution to listing all maximal cliques in\nlarge sparse graphs. All other theoretically-fast algorithms for sparse graphs\nhave been shown to be significantly slower than the algorithm of Tomita et al.\n(Theoretical Computer Science, 2006) in practice. However, the algorithm of\nTomita et al. uses an adjacency matrix, which requires too much space for large\nsparse graphs. Our new algorithm opens the door for fast analysis of large\nsparse graphs whose adjacency matrix will not fit into working memory."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0337v1", 
    "title": "Refinements of Miller's Algorithm over Weierstrass Curves Revisited", 
    "arxiv-id": "1103.0337v1", 
    "author": "Chao-Liang Liu", 
    "publish": "2011-03-02T02:50:33Z", 
    "summary": "In 1986 Victor Miller described an algorithm for computing the Weil pairing\nin his unpublished manuscript. This algorithm has then become the core of all\npairing-based cryptosystems. Many improvements of the algorithm have been\npresented. Most of them involve a choice of elliptic curves of a \\emph{special}\nforms to exploit a possible twist during Tate pairing computation. Other\nimprovements involve a reduction of the number of iterations in the Miller's\nalgorithm. For the generic case, Blake, Murty and Xu proposed three refinements\nto Miller's algorithm over Weierstrass curves. Though their refinements which\nonly reduce the total number of vertical lines in Miller's algorithm, did not\ngive an efficient computation as other optimizations, but they can be applied\nfor computing \\emph{both} of Weil and Tate pairings on \\emph{all}\npairing-friendly elliptic curves. In this paper we extend the Blake-Murty-Xu's\nmethod and show how to perform an elimination of all vertical lines in Miller's\nalgorithm during Weil/Tate pairings computation on \\emph{general} elliptic\ncurves. Experimental results show that our algorithm is faster about 25% in\ncomparison with the original Miller's algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0534v1", 
    "title": "Solving connectivity problems parameterized by treewidth in single   exponential time", 
    "arxiv-id": "1103.0534v1", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2011-03-02T20:36:20Z", 
    "summary": "For the vast majority of local graph problems standard dynamic programming\ntechniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input\ngraph. On the other hand, for problems with a global requirement (usually\nconnectivity) the best-known algorithms were naive dynamic programming schemes\nrunning in tw^O(tw) V^O(1) time.\n  We breach this gap by introducing a technique we dubbed Cut&Count that allows\nto produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type\nproblems, including Hamiltonian Path, Feedback Vertex Set and Connected\nDominating Set, consequently answering the question raised by Lokshtanov, Marx\nand Saurabh [SODA'11] in a surprising way. We also show that (under reasonable\ncomplexity assumptions) the gap cannot be breached for some problems for which\nCut&Count does not work, like CYCLE PACKING.\n  The constant c we obtain is in all cases small (at most 4 for undirected\nproblems and at most 6 for directed ones), and in several cases we are able to\nshow that improving those constants would cause the Strong Exponential Time\nHypothesis to fail.\n  Our results have numerous consequences in various fields, like FPT\nalgorithms, exact and approximate algorithms on planar and H-minor-free graphs\nand algorithms on graphs of bounded degree. In all these fields we are able to\nimprove the best-known results for some problems."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0985v1", 
    "title": "Locating Depots for Capacitated Vehicle Routing", 
    "arxiv-id": "1103.0985v1", 
    "author": "Viswanath Nagarajan", 
    "publish": "2011-03-04T21:15:44Z", 
    "summary": "We study a location-routing problem in the context of capacitated vehicle\nrouting. The input is a set of demand locations in a metric space and a fleet\nof k vehicles each of capacity Q. The objective is to locate k depots, one for\neach vehicle, and compute routes for the vehicles so that all demands are\nsatisfied and the total cost is minimized. Our main result is a constant-factor\napproximation algorithm for this problem. To achieve this result, we reduce to\nthe k-median-forest problem, which generalizes both k-median and minimum\nspanning tree, and which might be of independent interest. We give a\n(3+c)-approximation algorithm for k-median-forest, which leads to a\n(12+c)-approximation algorithm for the above location-routing problem, for any\nconstant c>0. The algorithm for k-median-forest is just t-swap local search,\nand we prove that it has locality gap 3+2/t; this generalizes the corresponding\nresult known for k-median. Finally we consider the \"non-uniform\"\nk-median-forest problem which has different cost functions for the MST and\nk-median parts. We show that the locality gap for this problem is unbounded\neven under multi-swaps, which contrasts with the uniform case. Nevertheless, we\nobtain a constant-factor approximation algorithm, using an LP based approach."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0995v3", 
    "title": "Near-Optimal Column-Based Matrix Reconstruction", 
    "arxiv-id": "1103.0995v3", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2011-03-04T23:50:35Z", 
    "summary": "We consider low-rank reconstruction of a matrix using its columns and we\npresent asymptotically optimal algorithms for both spectral norm and Frobenius\nnorm reconstruction. The main tools we introduce to obtain our r esults are:\n(i) the use of fast approximate SVD-like decompositions for column\nreconstruction, and (ii) two deter ministic algorithms for selecting rows from\nmatrices with orthonormal columns, building upon the sparse represen tation\ntheorem for decompositions of the identity that appeared in \\cite{BSS09}."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.1091v2", 
    "title": "A generalization of Hopcroft-Karp algorithm for semi-matchings and   covers in bipartite graphs", 
    "arxiv-id": "1103.1091v2", 
    "author": "Gabriel Semanisin", 
    "publish": "2011-03-06T00:54:03Z", 
    "summary": "An $(f,g)$-semi-matching in a bipartite graph $G=(U \\cup V,E)$ is a set of\nedges $M \\subseteq E$ such that each vertex $u\\in U$ is incident with at most\n$f(u)$ edges of $M$, and each vertex $v\\in V$ is incident with at most $g(v)$\nedges of $M$. In this paper we give an algorithm that for a graph with $n$\nvertices and $m$ edges, $n\\le m$, constructs a maximum $(f,g)$-semi-matching in\nrunning time $O(m\\cdot \\min (\\sqrt{\\sum_{u\\in U}f(u)}, \\sqrt{\\sum_{v\\in\nV}g(v)}))$. Using the reduction of [5], our result on maximum\n$(f,g)$-semi-matching problem directly implies an algorithm for the optimal\nsemi-matching problem with running time $O(\\sqrt{n}m \\log n)$."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.1109v4", 
    "title": "Fully dynamic maximal matching in O(log n) update time", 
    "arxiv-id": "1103.1109v4", 
    "author": "Sandeep Sen", 
    "publish": "2011-03-06T08:38:10Z", 
    "summary": "We present an algorithm for maintaining maximal matching in a graph under\naddition and deletion of edges. Our data structure is randomized that takes\nO(log n) expected amortized time for each edge update where n is the number of\nvertices in the graph. While there is a trivial O(n) algorithm for edge update,\nthe previous best known result for this problem for a graph with n vertices and\nm edges is O({(n+ m)}^{0.7072})which is sub-linear only for a sparse graph.\n  For the related problem of maximum matching, Onak and Rubinfield designed a\nrandomized data structure that achieves O(log^2 n) amortized time for each\nupdate for maintaining a c-approximate maximum matching for some large constant\nc. In contrast, we can maintain a factor two approximate maximum matching in\nO(log n) expected time per update as a direct corollary of the maximal matching\nscheme. This in turn also implies a two approximate vertex cover maintenance\nscheme that takes O(log n) expected time per update."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.1503v1", 
    "title": "An Exact Algorithm for Side-Chain Placement in Protein Design", 
    "arxiv-id": "1103.1503v1", 
    "author": "Gunnar W. Klau", 
    "publish": "2011-03-08T12:32:12Z", 
    "summary": "Computational protein design aims at constructing novel or improved functions\non the structure of a given protein backbone and has important applications in\nthe pharmaceutical and biotechnical industry. The underlying combinatorial\nside-chain placement problem consists of choosing a side-chain placement for\neach residue position such that the resulting overall energy is minimum. The\nchoice of the side-chain then also determines the amino acid for this position.\nMany algorithms for this NP-hard problem have been proposed in the context of\nhomology modeling, which, however, reach their limits when faced with large\nprotein design instances.\n  In this paper, we propose a new exact method for the side-chain placement\nproblem that works well even for large instance sizes as they appear in protein\ndesign. Our main contribution is a dedicated branch-and-bound algorithm that\ncombines tight upper and lower bounds resulting from a novel Lagrangian\nrelaxation approach for side-chain placement. Our experimental results show\nthat our method outperforms alternative state-of-the art exact approaches and\nmakes it possible to optimally solve large protein design instances routinely."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.1771v1", 
    "title": "Efficient Algorithms for Distributed Detection of Holes and Boundaries   in Wireless Networks", 
    "arxiv-id": "1103.1771v1", 
    "author": "Dorothea Wagner", 
    "publish": "2011-03-09T13:08:13Z", 
    "summary": "We propose two novel algorithms for distributed and location-free boundary\nrecognition in wireless sensor networks. Both approaches enable a node to\ndecide autonomously whether it is a boundary node, based solely on connectivity\ninformation of a small neighborhood. This makes our algorithms highly\napplicable for dynamic networks where nodes can move or become inoperative.\n  We compare our algorithms qualitatively and quantitatively with several\nprevious approaches. In extensive simulations, we consider various models and\nscenarios. Although our algorithms use less information than most other\napproaches, they produce significantly better results. They are very robust\nagainst variations in node degree and do not rely on simplified assumptions of\nthe communication model. Moreover, they are much easier to implement on real\nsensor nodes than most existing approaches."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2102v2", 
    "title": "A Randomized Algorithm Based on Threshold Accepting to Approximate the   Star Discrepancy", 
    "arxiv-id": "1103.2102v2", 
    "author": "Carola Winzen", 
    "publish": "2011-03-10T18:50:01Z", 
    "summary": "We present a new algorithm for estimating the star discrepancy of arbitrary\npoint sets. Similar to the algorithm for discrepancy approximation of Winker\nand Fang [SIAM J. Numer. Anal. 34 (1997), 2028--2042] it is based on the\noptimization algorithm threshold accepting. Our improvements include, amongst\nothers, a non-uniform sampling strategy which is more suited for\nhigher-dimensional inputs, and rounding steps which transform axis-parallel\nboxes, on which the discrepancy is to be tested, into \\emph{critical test\nboxes}. These critical test boxes provably yield higher discrepancy values, and\ncontain the box that exhibits the maximum value of the local discrepancy. We\nprovide comprehensive experiments to test the new algorithm. Our randomized\nalgorithm computes the exact discrepancy frequently in all cases where this can\nbe checked (i.e., where the exact discrepancy of the point set can be computed\nin feasible time). Most importantly, in higher dimension the new method behaves\nclearly better than all previously known methods."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2167v3", 
    "title": "Improved space-time tradeoffs for approximate full-text indexing with   one edit error", 
    "arxiv-id": "1103.2167v3", 
    "author": "Djamal Belazzougui", 
    "publish": "2011-03-10T23:25:45Z", 
    "summary": "In this paper we are interested in indexing texts for substring matching\nqueries with one edit error. That is, given a text $T$ of $n$ characters over\nan alphabet of size $\\sigma$, we are asked to build a data structure that\nanswers the following query: find all the $occ$ substrings of the text that are\nat edit distance at most $1$ from a given string $q$ of length $m$. In this\npaper we show two new results for this problem. The first result, suitable for\nan unbounded alphabet, uses $O(n\\log^\\epsilon n)$ (where $\\epsilon$ is any\nconstant such that $0<\\epsilon<1$) words of space and answers to queries in\ntime $O(m+occ)$. This improves simultaneously in space and time over the result\nof Cole et al. The second result, suitable only for a constant alphabet, relies\non compressed text indices and comes in two variants: the first variant uses\n$O(n\\log^{\\epsilon} n)$ bits of space and answers to queries in time\n$O(m+occ)$, while the second variant uses $O(n\\log\\log n)$ bits of space and\nanswers to queries in time $O((m+occ)\\log\\log n)$. This second result improves\non the previously best results for constant alphabets achieved in Lam et al.\n(Algorithmica 2008) and Chan et al. (Algorithmica 2010)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2275v1", 
    "title": "Channel Assignment via Fast Zeta Transform", 
    "arxiv-id": "1103.2275v1", 
    "author": "\u0141ukasz Kowalik", 
    "publish": "2011-03-11T14:12:12Z", 
    "summary": "We show an O*((l+1)^n)-time algorithm for the channel assignment problem,\nwhere l is the maximum edge weight. This improves on the previous\nO*((l+2)^n)-time algorithm by Kral, as well as algorithms for important special\ncases, like L(2,1)-labelling. For the latter problem, our algorithm works in\nO*(3^n) time. The progress is achieved by applying the fast zeta transform in\ncombination with the inclusion-exclusion principle."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2429v1", 
    "title": "Direction-Reversing Quasi-Random Rumor Spreading with Restarts", 
    "arxiv-id": "1103.2429v1", 
    "author": "Carola Winzen", 
    "publish": "2011-03-12T09:25:23Z", 
    "summary": "In a recent work, Doerr and Fouz [\\emph{Asymptotically Optimal Randomized\nRumor Spreading}, in ArXiv] present a new quasi-random PUSH algorithm for the\nrumor spreading problem (also known as gossip spreading or message propagation\nproblem). Their \\emph{hybrid protocol} outperforms all known PUSH protocols.\n  In this work, we add to the hybrid protocol a direction-reversing element. We\nshow that this \\emph{direction-reversing quasi-random rumor spreading protocol\nwith random restarts} yields a constant factor improvement over the hybrid\nmodel, if we allow the same dose of randomness.\n  Put differently, our protocol achieves the same broadcasting time as the\nhybrid model by employing only (roughly) half the number of random choices."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2581v1", 
    "title": "Constant-Time Algorithms for Sparsity Matroids", 
    "arxiv-id": "1103.2581v1", 
    "author": "Yuichi Yoshida", 
    "publish": "2011-03-14T03:51:27Z", 
    "summary": "A graph $G=(V,E)$ is called $(k,\\ell)$-full if $G$ contains a subgraph\n$H=(V,F)$ of $k|V|-\\ell$ edges such that, for any non-empty $F' \\subseteq F$,\n$|F'| \\leq k|V(F')| - \\ell$ holds. Here, $V(F')$ denotes the set of vertices\nincident to $F'$. It is known that the family of edge sets of $(k,\\ell)$-full\ngraphs forms a family of matroid, known as the sparsity matroid of $G$. In this\npaper, we give a constant-time approximation algorithm for the rank of the\nsparsity matroid of a degree-bounded undirected graph. This leads to a\nconstant-time tester for $(k,\\ell)$-fullness in the bounded-degree model,\n(i.e., we can decide with high probability whether an input graph satisfies a\nproperty $P$ or far from $P$). Depending on the values of $k$ and $\\ell$, it\ncan test various properties of a graph such as connectivity, rigidity, and how\nmany spanning trees can be packed. Based on this result, we also propose a\nconstant-time tester for $(k,\\ell)$-edge-connected-orientability in the\nbounded-degree model, where an undirected graph $G$ is called\n$(k,\\ell)$-edge-connected-orientable if there exists an orientation $\\vec{G}$\nof $G$ with a vertex $r \\in V$ such that $\\vec{G}$ contains $k$ arc-disjoint\ndipaths from $r$ to each vertex $v \\in V$ and $\\ell$ arc-disjoint dipaths from\neach vertex $v \\in V$ to $r$. A tester is called a one-sided error tester for\n$P$ if it always accepts a graph satisfying $P$. We show, for $k \\geq 2$ and\n(proper) $\\ell \\geq 0$, any one-sided error tester for $(k,\\ell)$-fullness and\n$(k,\\ell)$-edge-connected-orientability requires $\\Omega(n)$ queries."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.2613v1", 
    "title": "Linear pattern matching on sparse suffix trees", 
    "arxiv-id": "1103.2613v1", 
    "author": "Tatiana Starikovskaya", 
    "publish": "2011-03-14T09:49:10Z", 
    "summary": "Packing several characters into one computer word is a simple and natural way\nto compress the representation of a string and to speed up its processing.\nExploiting this idea, we propose an index for a packed string, based on a {\\em\nsparse suffix tree} \\cite{KU-96} with appropriately defined suffix links.\nAssuming, under the standard unit-cost RAM model, that a word can store up to\n$\\log_{\\sigma}n$ characters ($\\sigma$ the alphabet size), our index takes\n$O(n/\\log_{\\sigma}n)$ space, i.e. the same space as the packed string itself.\nThe resulting pattern matching algorithm runs in time $O(m+r^2+r\\cdot occ)$,\nwhere $m$ is the length of the pattern, $r$ is the actual number of characters\nstored in a word and $occ$ is the number of pattern occurrences."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.3114v2", 
    "title": "Fast $q$-gram Mining on SLP Compressed Strings", 
    "arxiv-id": "1103.3114v2", 
    "author": "Masayuki Takeda", 
    "publish": "2011-03-16T07:16:10Z", 
    "summary": "We present simple and efficient algorithms for calculating $q$-gram\nfrequencies on strings represented in compressed form, namely, as a straight\nline program (SLP). Given an SLP of size $n$ that represents string $T$, we\npresent an $O(qn)$ time and space algorithm that computes the occurrence\nfrequencies of $q$-grams in $T$. Computational experiments show that our\nalgorithm and its variation are practical for small $q$, actually running\nfaster on various real string data, compared to algorithms that work on the\nuncompressed text. We also discuss applications in data mining and\nclassification of string data, for which our algorithms can be useful."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.4566v2", 
    "title": "The Topology of Wireless Communication", 
    "arxiv-id": "1103.4566v2", 
    "author": "David Peleg", 
    "publish": "2011-03-23T16:03:19Z", 
    "summary": "In this paper we study the topological properties of wireless communication\nmaps and their usability in algorithmic design. We consider the SINR model,\nwhich compares the received power of a signal at a receiver against the sum of\nstrengths of other interfering signals plus background noise. To describe the\nbehavior of a multi-station network, we use the convenient representation of a\n\\emph{reception map}. In the SINR model, the resulting \\emph{SINR diagram}\npartitions the plane into reception zones, one per station, and the\ncomplementary region of the plane where no station can be heard. We consider\nthe general case where transmission energies are arbitrary (or non-uniform).\nUnder that setting, the reception zones are not necessarily convex or even\nconnected. This poses the algorithmic challenge of designing efficient point\nlocation techniques as well as the theoretical challenge of understanding the\ngeometry of SINR diagrams. We achieve several results in both directions. We\nestablish a form of weaker convexity in the case where stations are aligned on\na line. In addition, one of our key results concerns the behavior of a\n$(d+1)$-dimensional map. Specifically, although the $d$-dimensional map might\nbe highly fractured, drawing the map in one dimension higher \"heals\" the zones,\nwhich become connected. In addition, as a step toward establishing a weaker\nform of convexity for the $d$-dimensional map, we study the interference\nfunction and show that it satisfies the maximum principle. Finally, we turn to\nconsider algorithmic applications, and propose a new variant of approximate\npoint location."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.4875v2", 
    "title": "Constructing and Sampling Graphs with a Prescribed Joint Degree   Distribution", 
    "arxiv-id": "1103.4875v2", 
    "author": "Ali Pinar", 
    "publish": "2011-03-24T21:05:17Z", 
    "summary": "One of the most influential recent results in network analysis is that many\nnatural networks exhibit a power-law or log-normal degree distribution. This\nhas inspired numerous generative models that match this property. However, more\nrecent work has shown that while these generative models do have the right\ndegree distribution, they are not good models for real life networks due to\ntheir differences on other important metrics like conductance. We believe this\nis, in part, because many of these real-world networks have very different\njoint degree distributions, i.e. the probability that a randomly selected edge\nwill be between nodes of degree k and l. Assortativity is a sufficient\nstatistic of the joint degree distribution, and it has been previously noted\nthat social networks tend to be assortative, while biological and technological\nnetworks tend to be disassortative.\n  We suggest understanding the relationship between network structure and the\njoint degree distribution of graphs is an interesting avenue of further\nresearch. An important tool for such studies are algorithms that can generate\nrandom instances of graphs with the same joint degree distribution. This is the\nmain topic of this paper and we study the problem from both a theoretical and\npractical perspective. We provide an algorithm for constructing simple graphs\nfrom a given joint degree distribution, and a Monte Carlo Markov Chain method\nfor sampling them. We also show that the state space of simple graphs with a\nfixed degree distribution is connected via end point switches. We empirically\nevaluate the mixing time of this Markov Chain by using experiments based on the\nautocorrelation of each edge. These experiments show that our Markov Chain\nmixes quickly on real graphs, allowing for utilization of our techniques in\npractice."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5453v1", 
    "title": "Using a Non-Commutative Bernstein Bound to Approximate Some Matrix   Algorithms in the Spectral Norm", 
    "arxiv-id": "1103.5453v1", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2011-03-28T19:37:20Z", 
    "summary": "We focus on \\emph{row sampling} based approximations for matrix algorithms,\nin particular matrix multipication, sparse matrix reconstruction, and\n\\math{\\ell_2} regression. For \\math{\\matA\\in\\R^{m\\times d}} (\\math{m} points in\n\\math{d\\ll m} dimensions), and appropriate row-sampling probabilities, which\ntypically depend on the norms of the rows of the \\math{m\\times d} left singular\nmatrix of \\math{\\matA} (the \\emph{leverage scores}), we give row-sampling\nalgorithms with linear (up to polylog factors) dependence on the stable rank of\n\\math{\\matA}. This result is achieved through the application of\nnon-commutative Bernstein bounds. Keywords: row-sampling; matrix\nmultiplication; matrix reconstruction; estimating spectral norm; linear\nregression; randomized"
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5599v2", 
    "title": "Polynomial kernels for Proper Interval Completion and related problems", 
    "arxiv-id": "1103.5599v2", 
    "author": "Anthony Perez", 
    "publish": "2011-03-29T10:45:31Z", 
    "summary": "Given a graph G = (V,E) and a positive integer k, the Proper Interval\nCompletion problem asks whether there exists a set F of at most k pairs of (V\n\\times V)\\E such that the graph H = (V,E \\cup F) is a proper interval graph.\nThe Proper Interval Completion problem finds applications in molecular biology\nand genomic research. First announced by Kaplan, Tarjan and Shamir in FOCS '94,\nthis problem is known to be FPT, but no polynomial kernel was known to exist.\nWe settle this question by proving that Proper Interval Completion admits a\nkernel with at most O(k^5) vertices. Moreover, we prove that a related problem,\nthe so-called Bipartite Chain Deletion problem, admits a kernel with at most\nO(k^2) vertices, completing a previous result of Guo."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5609v1", 
    "title": "Recoverable Values for Independent Sets", 
    "arxiv-id": "1103.5609v1", 
    "author": "Daniel Reichman", 
    "publish": "2011-03-29T11:47:04Z", 
    "summary": "The notion of {\\em recoverable value} was advocated in work of Feige,\nImmorlica, Mirrokni and Nazerzadeh [Approx 2009] as a measure of quality for\napproximation algorithms. There this concept was applied to facility location\nproblems. In the current work we apply a similar framework to the maximum\nindependent set problem (MIS). We say that an approximation algorithm has {\\em\nrecoverable value} $\\rho$, if for every graph it recovers an independent set of\nsize at least $\\max_I \\sum_{v\\in I} \\min[1,\\rho/(d(v) + 1)]$, where $d(v)$ is\nthe degree of vertex $v$, and $I$ ranges over all independent sets in $G$.\nHence, in a sense, from every vertex $v$ in the maximum independent set the\nalgorithm recovers a value of at least $\\rho/(d_v + 1)$ towards the solution.\nThis quality measure is most effective in graphs in which the maximum\nindependent set is composed of low degree vertices. It easily follows from\nknown results that some simple algorithms for MIS ensure $\\rho \\ge 1$. We\ndesign a new randomized algorithm for MIS that ensures an expected recoverable\nvalue of at least $\\rho \\ge 7/3$. In addition, we show that approximating MIS\nin graphs with a given $k$-coloring within a ratio larger than $2/k$ is unique\ngames hard. This rules out a natural approach for obtaining $\\rho \\ge 2$."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.6246v2", 
    "title": "Sparse Vector Distributions and Recovery from Compressed Sensing", 
    "arxiv-id": "1103.6246v2", 
    "author": "Bob L. Sturm", 
    "publish": "2011-03-31T17:26:34Z", 
    "summary": "It is well known that the performance of sparse vector recovery algorithms\nfrom compressive measurements can depend on the distribution underlying the\nnon-zero elements of a sparse vector. However, the extent of these effects has\nyet to be explored, and formally presented. In this paper, I empirically\ninvestigate this dependence for seven distributions and fifteen recovery\nalgorithms. The two morals of this work are: 1) any judgement of the recovery\nperformance of one algorithm over that of another must be prefaced by the\nconditions for which this is observed to be true, including sparse vector\ndistributions, and the criterion for exact recovery; and 2) a recovery\nalgorithm must be selected carefully based on what distribution one expects to\nunderlie the sensed sparse signal."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0187v1", 
    "title": "An Improved Move-To-Front(IMTF) Off-line Algorithm for the List   Accessing Problem", 
    "arxiv-id": "1105.0187v1", 
    "author": "Sasmita Tripathy", 
    "publish": "2011-05-01T17:17:17Z", 
    "summary": "For the List Accessing Problem, Move-To-Front(MTF) algorithm has been proved\nto be the best performing online list accessing algorithm till date in the\nliterature[10]. In this paper, we have made a comprehensive analysis of MTF\nalgorithm and developed an Improved-MTF (IMTF) offline algorithm. We have\ngenerated two new types of data set and devise a new method of experimental\nanalysis for our proposed algorithm. Our experimental analysis shows that IMTF\nis performing better than MTF algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0464v3", 
    "title": "Improved Low-rank Matrix Decompositions via the Subsampled Randomized   Hadamard Transform", 
    "arxiv-id": "1105.0464v3", 
    "author": "Christos Boutsidis", 
    "publish": "2011-05-03T01:39:02Z", 
    "summary": "We comment on two randomized algorithms for constructing low-rank matrix\ndecompositions. Both algorithms employ the Subsampled Randomized Hadamard\nTransform [14]. The first algorithm appeared recently in [9]; here, we provide\na novel analysis that significantly improves the approximation bound obtained\nin [9]. A preliminary version of the second algorithm appeared in [7]; here, we\npresent a mild modification of this algorithm that achieves the same\napproximation bound but significantly improves the corresponding running time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0477v3", 
    "title": "The parameterized complexity of k-edge induced subgraphs", 
    "arxiv-id": "1105.0477v3", 
    "author": "Yijia Chen", 
    "publish": "2011-05-03T05:25:14Z", 
    "summary": "We prove that finding a $k$-edge induced subgraph is fixed-parameter\ntractable, thereby answering an open problem of Leizhen Cai. Our algorithm is\nbased on several combinatorial observations, Gauss' famous \\emph{Eureka}\ntheorem [Andrews, 86], and a generalization of the well-known fpt-algorithm for\nthe model-checking problem for first-order logic on graphs with locally bounded\ntree-width due to Frick and Grohe [Frick and Grohe, 01]. On the other hand, we\nshow that two natural counting versions of the problem are hard. Hence, the\n$k$-edge induced subgraph problem is one of the rare known examples in\nparameterized complexity that are easy for decision while hard for counting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.0608v1", 
    "title": "A simpler and more efficient algorithm for the next-to-shortest path   problem", 
    "arxiv-id": "1105.0608v1", 
    "author": "Bang Ye Wu", 
    "publish": "2011-05-03T15:27:00Z", 
    "summary": "Given an undirected graph $G=(V,E)$ with positive edge lengths and two\nvertices $s$ and $t$, the next-to-shortest path problem is to find an $st$-path\nwhich length is minimum amongst all $st$-paths strictly longer than the\nshortest path length. In this paper we show that the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nParticularly our new algorithm runs in $O(|V|\\log |V|+|E|)$ time for general\ngraphs, which improves the previous result of $O(|V|^2)$ time for sparse\ngraphs, and takes only linear time for unweighted graphs, planar graphs, and\ngraphs with positive integer edge lengths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.0709v1", 
    "title": "Topics in Matrix Sampling Algorithms", 
    "arxiv-id": "1105.0709v1", 
    "author": "Christos Boutsidis", 
    "publish": "2011-05-04T00:19:49Z", 
    "summary": "We study three fundamental problems of Linear Algebra, lying in the heart of\nvarious Machine Learning applications, namely: 1)\"Low-rank Column-based Matrix\nApproximation\". We are given a matrix A and a target rank k. The goal is to\nselect a subset of columns of A and, by using only these columns, compute a\nrank k approximation to A that is as good as the rank k approximation that\nwould have been obtained by using all the columns; 2) \"Coreset Construction in\nLeast-Squares Regression\". We are given a matrix A and a vector b. Consider the\n(over-constrained) least-squares problem of minimizing ||Ax-b||, over all\nvectors x in D. The domain D represents the constraints on the solution and can\nbe arbitrary. The goal is to select a subset of the rows of A and b and, by\nusing only these rows, find a solution vector that is as good as the solution\nvector that would have been obtained by using all the rows; 3) \"Feature\nSelection in K-means Clustering\". We are given a set of points described with\nrespect to a large number of features. The goal is to select a subset of the\nfeatures and, by using only this subset, obtain a k-partition of the points\nthat is as good as the partition that would have been obtained by using all the\nfeatures. We present novel algorithms for all three problems mentioned above.\nOur results can be viewed as follow-up research to a line of work known as\n\"Matrix Sampling Algorithms\". [Frieze, Kanna, Vempala, 1998] presented the\nfirst such algorithm for the Low-rank Matrix Approximation problem. Since then,\nsuch algorithms have been developed for several other problems, e.g. Graph\nSparsification and Linear Equation Solving. Our contributions to this line of\nresearch are: (i) improved algorithms for Low-rank Matrix Approximation and\nRegression (ii) algorithms for a new problem domain (K-means Clustering)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.1569v1", 
    "title": "Practical and theoretical improvements for bipartite matching using the   pseudoflow algorithm", 
    "arxiv-id": "1105.1569v1", 
    "author": "Dorit S. Hochbaum", 
    "publish": "2011-05-09T02:03:40Z", 
    "summary": "We show that the pseudoflow algorithm for maximum flow is particularly\nefficient for the bipartite matching problem both in theory and in practice. We\ndevelop several implementations of the pseudoflow algorithm for bipartite\nmatching, and compare them over a wide set of benchmark instances to\nstate-of-the-art implementations of push-relabel and augmenting path algorithms\nthat are specifically designed to solve these problems. The experiments show\nthat the pseudoflow variants are in most cases faster than the other\nalgorithms.\n  We also show that one particular implementation---the matching pseudoflow\nalgorithm---is theoretically efficient. For a graph with $n$ nodes, $m$ arcs,\n$n_1$ the size of the smaller set in the bipartition, and the maximum matching\nvalue $\\kappa \\leq n_1$, the algorithm's complexity given input in the form of\nadjacency lists is $O(\\min{n_1\\kappa,m} + \\sqrt{\\kappa}\\min{\\kappa^2,m})$.\nSimilar algorithmic ideas are shown to work for an adaptation of Hopcroft and\nKarp's bipartite matching algorithm with the same complexity. Using boolean\noperations on words of size $\\lambda$, the complexity of the pseudoflow\nalgorithm is further improved to $O(\\min{n_1\\kappa, \\frac{n_1n_2}{\\lambda}, m}\n+ \\kappa^2 + \\frac{\\kappa^{2.5}}{\\lambda})$. This run time is faster than for\nprevious algorithms such as Cheriyan and Mehlhorn's algorithm of complexity\n$O(\\frac{n^{2.5}}{\\lambda})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.1622v1", 
    "title": "Computing Majority with Triple Queries", 
    "arxiv-id": "1105.1622v1", 
    "author": "Gabor Wiener", 
    "publish": "2011-05-09T10:26:01Z", 
    "summary": "Consider a bin containing $n$ balls colored with two colors. In a $k$-query,\n$k$ balls are selected by a questioner and the oracle's reply is related\n(depending on the computation model being considered) to the distribution of\ncolors of the balls in this $k$-tuple; however, the oracle never reveals the\ncolors of the individual balls. Following a number of queries the questioner is\nsaid to determine the majority color if it can output a ball of the majority\ncolor if it exists, and can prove that there is no majority if it does not\nexist. We investigate two computation models (depending on the type of replies\nbeing allowed). We give algorithms to compute the minimum number of 3-queries\nwhich are needed so that the questioner can determine the majority color and\nprovide tight and almost tight upper and lower bounds on the number of queries\nneeded in each case."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2391v4", 
    "title": "LP-Based Approximation Algorithms for Traveling Salesman Path Problems", 
    "arxiv-id": "1105.2391v4", 
    "author": "David B. Shmoys", 
    "publish": "2011-05-12T07:30:50Z", 
    "summary": "This paper has been merged into 1110.4604."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2397v1", 
    "title": "Incremental Cycle Detection, Topological Ordering, and Strong Component   Maintenance", 
    "arxiv-id": "1105.2397v1", 
    "author": "Robert Endre Tarjan", 
    "publish": "2011-05-12T07:57:28Z", 
    "summary": "We present two on-line algorithms for maintaining a topological order of a\ndirected $n$-vertex acyclic graph as arcs are added, and detecting a cycle when\none is created. Our first algorithm handles $m$ arc additions in $O(m^{3/2})$\ntime. For sparse graphs ($m/n = O(1)$), this bound improves the best previous\nbound by a logarithmic factor, and is tight to within a constant factor among\nalgorithms satisfying a natural {\\em locality} property. Our second algorithm\nhandles an arbitrary sequence of arc additions in $O(n^{5/2})$ time. For\nsufficiently dense graphs, this bound improves the best previous bound by a\npolynomial factor. Our bound may be far from tight: we show that the algorithm\ncan take $\\Omega(n^2 2^{\\sqrt{2\\lg n}})$ time by relating its performance to a\ngeneralization of the $k$-levels problem of combinatorial geometry. A\ncompletely different algorithm running in $\\Theta(n^2 \\log n)$ time was given\nrecently by Bender, Fineman, and Gilbert. We extend both of our algorithms to\nthe maintenance of strong components, without affecting the asymptotic time\nbounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2686v2", 
    "title": "Smoothed Performance Guarantees for Local Search", 
    "arxiv-id": "1105.2686v2", 
    "author": "Tjark Vredeveld", 
    "publish": "2011-05-13T11:04:11Z", 
    "summary": "We study popular local search and greedy algorithms for scheduling. The\nperformance guarantee of these algorithms is well understood, but the\nworst-case lower bounds seem somewhat contrived and it is questionable if they\narise in practical applications. To find out how robust these bounds are, we\nstudy the algorithms in the framework of smoothed analysis, in which instances\nare subject to some degree of random noise.\n  While the lower bounds for all scheduling variants with restricted machines\nare rather robust, we find out that the bounds are fragile for unrestricted\nmachines. In particular, we show that the smoothed performance guarantee of the\njump and the lex-jump algorithm are (in contrast to the worst case) independent\nof the number of machines. They are Theta(phi) and Theta(log(phi)),\nrespectively, where 1/phi is a parameter measuring the magnitude of the\nperturbation. The latter immediately implies that also the smoothed price of\nanarchy is Theta(log(phi)) for routing games on parallel links. Additionally we\nshow that for unrestricted machines also the greedy list scheduling algorithm\nhas an approximation guarantee of Theta(log(phi))."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.2704v2", 
    "title": "Hitting and Harvesting Pumpkins", 
    "arxiv-id": "1105.2704v2", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2011-05-13T12:21:17Z", 
    "summary": "The \"c-pumpkin\" is the graph with two vertices linked by c>0 parallel edges.\nA c-pumpkin-model in a graph G is a pair A,B of disjoint subsets of vertices of\nG, each inducing a connected subgraph of G, such that there are at least c\nedges in G between A and B. We focus on covering and packing c-pumpkin-models\nin a given graph: On the one hand, we provide an FPT algorithm running in time\n2^O(k) n^O(1) deciding, for any fixed c>0, whether all c-pumpkin-models can be\ncovered by at most k vertices. This generalizes known single-exponential FPT\nalgorithms for Vertex Cover and Feedback Vertex Set, which correspond to the\ncases c=1,2 respectively. On the other hand, we present a O(log\nn)-approximation algorithm for both the problems of covering all\nc-pumpkin-models with a smallest number of vertices, and packing a maximum\nnumber of vertex-disjoint c-pumpkin-models."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.3748v1", 
    "title": "Scalably Scheduling Power-Heterogeneous Processors", 
    "arxiv-id": "1105.3748v1", 
    "author": "Kirk Pruhs", 
    "publish": "2011-05-18T20:51:32Z", 
    "summary": "We show that a natural online algorithm for scheduling jobs on a\nheterogeneous multiprocessor, with arbitrary power functions, is scalable for\nthe objective function of weighted flow plus energy."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.4250v1", 
    "title": "Approximating subset $k$-connectivity problems", 
    "arxiv-id": "1105.4250v1", 
    "author": "Zeev Nutov", 
    "publish": "2011-05-21T11:55:36Z", 
    "summary": "A subset $T \\subseteq V$ of terminals is $k$-connected to a root $s$ in a\ndirected/undirected graph $J$ if $J$ has $k$ internally-disjoint $vs$-paths for\nevery $v \\in T$; $T$ is $k$-connected in $J$ if $T$ is $k$-connected to every\n$s \\in T$. We consider the {\\sf Subset $k$-Connectivity Augmentation} problem:\ngiven a graph $G=(V,E)$ with edge/node-costs, node subset $T \\subseteq V$, and\na subgraph $J=(V,E_J)$ of $G$ such that $T$ is $k$-connected in $J$, find a\nminimum-cost augmenting edge-set $F \\subseteq E \\setminus E_J$ such that $T$ is\n$(k+1)$-connected in $J \\cup F$. The problem admits trivial ratio $O(|T|^2)$.\nWe consider the case $|T|>k$ and prove that for directed/undirected graphs and\nedge/node-costs, a $\\rho$-approximation for {\\sf Rooted Subset $k$-Connectivity\nAugmentation} implies the following ratios for {\\sf Subset $k$-Connectivity\nAugmentation}: (i) $b(\\rho+k) + {(\\frac{3|T|}{|T|-k})}^2\nH(\\frac{3|T|}{|T|-k})$; (ii) $\\rho \\cdot O(\\frac{|T|}{|T|-k} \\log k)$, where\nb=1 for undirected graphs and b=2 for directed graphs, and $H(k)$ is the $k$th\nharmonic number. The best known values of $\\rho$ on undirected graphs are\n$\\min\\{|T|,O(k)\\}$ for edge-costs and $\\min\\{|T|,O(k \\log |T|)\\}$ for\nnode-costs; for directed graphs $\\rho=|T|$ for both versions. Our results imply\nthat unless $k=|T|-o(|T|)$, {\\sf Subset $k$-Connectivity Augmentation} admits\nthe same ratios as the best known ones for the rooted version. This improves\nthe ratios in \\cite{N-focs,L}."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.5718v1", 
    "title": "Relational Schema Protocol (RSP)", 
    "arxiv-id": "1105.5718v1", 
    "author": "Vojtech Prehnal", 
    "publish": "2011-05-28T14:28:30Z", 
    "summary": "This document specifies the Relational Schema Protocol (RSP). RSP enables\nloosely coupled applications to share and exchange relational data. It defines\nfixed message format for an arbitrary relational schema so that the changes in\nthe data schema do not affect the message format. This prevents the interacting\napplications from having to be reimplemented during the data schema evolvement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1105.5915v2", 
    "title": "Algorithms for the minimum non-separating path and the balanced   connected bipartition problems on grid graphs (With erratum)", 
    "arxiv-id": "1105.5915v2", 
    "author": "Bang Ye Wu", 
    "publish": "2011-05-30T09:33:36Z", 
    "summary": "For given a pair of nodes in a graph, the minimum non-separating path problem\nlooks for a minimum weight path between the two nodes such that the remaining\ngraph after removing the path is still connected. The balanced connected\nbipartition (BCP$_2$) problem looks for a way to bipartition a graph into two\nconnected subgraphs with their weights as equal as possible. In this paper we\npresent an algorithm in time $O(N\\log N)$ for finding a minimum weight\nnon-separating path between two given nodes in a grid graph of $N$ nodes with\npositive weight. This result leads to a 5/4-approximation algorithm for the\nBCP$_2$ problem on grid graphs, which is the currently best ratio achieved in\npolynomial time. We also developed an exact algorithm for the BCP$_2$ problem\non grid graphs. Based on the exact algorithm and a rounding technique, we show\nan approximation scheme, which is a fully polynomial time approximation scheme\nfor fixed number of rows."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.2294v1", 
    "title": "Unconstrained and Constrained Fault-Tolerant Resource Allocation", 
    "arxiv-id": "1106.2294v1", 
    "author": "Hong Shen", 
    "publish": "2011-06-12T08:59:06Z", 
    "summary": "First, we study the Unconstrained Fault-Tolerant Resource Allocation (UFTRA)\nproblem (a.k.a. FTFA problem in \\cite{shihongftfa}). In the problem, we are\ngiven a set of sites equipped with an unconstrained number of facilities as\nresources, and a set of clients with set $\\mathcal{R}$ as corresponding\nconnection requirements, where every facility belonging to the same site has an\nidentical opening (operating) cost and every client-facility pair has a\nconnection cost. The objective is to allocate facilities from sites to satisfy\n$\\mathcal{R}$ at a minimum total cost. Next, we introduce the Constrained\nFault-Tolerant Resource Allocation (CFTRA) problem. It differs from UFTRA in\nthat the number of resources available at each site $i$ is limited by $R_{i}$.\nBoth problems are practical extensions of the classical Fault-Tolerant Facility\nLocation (FTFL) problem \\cite{Jain00FTFL}. For instance, their solutions\nprovide optimal resource allocation (w.r.t. enterprises) and leasing (w.r.t.\nclients) strategies for the contemporary cloud platforms.\n  In this paper, we consider the metric version of the problems. For UFTRA with\nuniform $\\mathcal{R}$, we present a star-greedy algorithm. The algorithm\nachieves the approximation ratio of 1.5186 after combining with the cost\nscaling and greedy augmentation techniques similar to\n\\cite{Charikar051.7281.853,Mahdian021.52}, which significantly improves the\nresult of \\cite{shihongftfa} using a phase-greedy algorithm. We also study the\ncapacitated extension of UFTRA and give a factor of 2.89. For CFTRA with\nuniform $\\mathcal{R}$, we slightly modify the algorithm to achieve\n1.5186-approximation. For a more general version of CFTRA, we show that it is\nreducible to FTFL using linear programming."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.2301v2", 
    "title": "A simple algorithm for the evaluation of the hypergeometric series using   quasi-linear time and linear space", 
    "arxiv-id": "1106.2301v2", 
    "author": "Sergey V. Yakhontov", 
    "publish": "2011-06-12T10:33:05Z", 
    "summary": "A simple algorithm with quasi-linear time complexity and linear space\ncomplexity for the evaluation of the hypergeometric series with rational\ncoefficients is constructed. It is shown that this algorithm is suitable in\npractical informatics for constructive analogues of often used constants of\nanalysis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.2351v1", 
    "title": "On vertex covers and matching number of trapezoid graphs", 
    "arxiv-id": "1106.2351v1", 
    "author": "Andreja Ilic", 
    "publish": "2011-06-12T22:11:56Z", 
    "summary": "The intersection graph of a collection of trapezoids with corner points lying\non two parallel lines is called a trapezoid graph. Using binary indexed tree\ndata structure, we improve algorithms for calculating the size and the number\nof minimum vertex covers (or independent sets), as well as the total number of\nvertex covers, and reduce the time complexity from $O (n^2)$ to $O (n \\log n)$,\nwhere $n$ is the number of trapezoids. Furthermore, we present the family of\ncounterexamples for recently proposed algorithm with time complexity $O (n^2)$\nfor calculating the maximum cardinality matching in trapezoid graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.2694v1", 
    "title": "Geometric Simultaneous RAC Drawings of Graphs", 
    "arxiv-id": "1106.2694v1", 
    "author": "Antonios Symvonis", 
    "publish": "2011-06-14T12:42:31Z", 
    "summary": "In this paper, we introduce and study \"geometric simultaneous RAC drawing\nproblems\", i.e., a combination of problems on geometric RAC drawings and\ngeometric simultaneous graph drawings. To the best of our knowledge, this is\nthe first time where such a combination is attempted."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.3126v1", 
    "title": "Testing List H-Homomorphisms", 
    "arxiv-id": "1106.3126v1", 
    "author": "Yuichi Yoshida", 
    "publish": "2011-06-16T01:21:34Z", 
    "summary": "Let $H$ be an undirected graph. In the List $H$-Homomorphism Problem, given\nan undirected graph $G$ with a list constraint $L(v) \\subseteq V(H)$ for each\nvariable $v \\in V(G)$, the objective is to find a list $H$-homomorphism $f:V(G)\n\\to V(H)$, that is, $f(v) \\in L(v)$ for every $v \\in V(G)$ and $(f(u),f(v)) \\in\nE(H)$ whenever $(u,v) \\in E(G)$.\n  We consider the following problem: given a map $f:V(G) \\to V(H)$ as an oracle\naccess, the objective is to decide with high probability whether $f$ is a list\n$H$-homomorphism or \\textit{far} from any list $H$-homomorphisms. The\nefficiency of an algorithm is measured by the number of accesses to $f$.\n  In this paper, we classify graphs $H$ with respect to the query complexity\nfor testing list $H$-homomorphisms and show the following trichotomy holds: (i)\nList $H$-homomorphisms are testable with a constant number of queries if and\nonly if $H$ is a reflexive complete graph or an irreflexive complete bipartite\ngraph. (ii) List $H$-homomorphisms are testable with a sublinear number of\nqueries if and only if $H$ is a bi-arc graph. (iii) Testing list\n$H$-homomorphisms requires a linear number of queries if $H$ is not a bi-arc\ngraph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.4412v1", 
    "title": "Space Lower Bounds for Online Pattern Matching", 
    "arxiv-id": "1106.4412v1", 
    "author": "Benjamin Sach", 
    "publish": "2011-06-22T10:39:48Z", 
    "summary": "We present space lower bounds for online pattern matching under a number of\ndifferent distance measures. Given a pattern of length m and a text that\narrives one character at a time, the online pattern matching problem is to\nreport the distance between the pattern and a sliding window of the text as\nsoon as the new character arrives. We require that the correct answer is given\nat each position with constant probability. We give Omega(m) bit space lower\nbounds for L_1, L_2, L_\\infty, Hamming, edit and swap distances as well as for\nany algorithm that computes the cross-correlation/convolution. We then show a\ndichotomy between distance functions that have wildcard-like properties and\nthose that do not. In the former case which includes, as an example, pattern\nmatching with character classes, we give Omega(m) bit space lower bounds. For\nother distance functions, we show that there exist space bounds of Omega(log m)\nand O(log^2 m) bits. Finally we discuss space lower bounds for non-binary\ninputs and show how in some cases they can be improved."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.4587v1", 
    "title": "An Efficient Partitioning Oracle for Bounded-Treewidth Graphs", 
    "arxiv-id": "1106.4587v1", 
    "author": "Krzysztof Onak", 
    "publish": "2011-06-22T21:57:02Z", 
    "summary": "Partitioning oracles were introduced by Hassidim et al. (FOCS 2009) as a\ngeneric tool for constant-time algorithms. For any epsilon > 0, a partitioning\noracle provides query access to a fixed partition of the input bounded-degree\nminor-free graph, in which every component has size poly(1/epsilon), and the\nnumber of edges removed is at most epsilon*n, where n is the number of vertices\nin the graph.\n  However, the oracle of Hassidimet al. makes an exponential number of queries\nto the input graph to answer every query about the partition. In this paper, we\nconstruct an efficient partitioning oracle for graphs with constant treewidth.\nThe oracle makes only O(poly(1/epsilon)) queries to the input graph to answer\neach query about the partition.\n  Examples of bounded-treewidth graph classes include k-outerplanar graphs for\nfixed k, series-parallel graphs, cactus graphs, and pseudoforests. Our oracle\nyields poly(1/epsilon)-time property testing algorithms for membership in these\nclasses of graphs. Another application of the oracle is a poly(1/epsilon)-time\nalgorithm that approximates the maximum matching size, the minimum vertex cover\nsize, and the minimum dominating set size up to an additive epsilon*n in graphs\nwith bounded treewidth. Finally, the oracle can be used to test in\npoly(1/epsilon) time whether the input bounded-treewidth graph is k-colorable\nor perfect."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.4677v1", 
    "title": "Optimal Bi-Valued Auctions", 
    "arxiv-id": "1106.4677v1", 
    "author": "Ilan Newman", 
    "publish": "2011-06-23T10:34:33Z", 
    "summary": "We investigate \\emph{bi-valued} auctions in the digital good setting and\nconstruct an explicit polynomial time deterministic auction. We prove an\nunconditional tight lower bound which holds even for random superpolynomial\nauctions. The analysis of the construction uses the adoption of the finer lens\nof \\emph{general competitiveness} which considers additive losses on top of\nmultiplicative ones. The result implies that general competitiveness is the\nright notion to use in this setting, as this optimal auction is uncompetitive\nwith respect to competitive measures which do not consider additive losses."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.5845v1", 
    "title": "Minimum Certificate Dispersal with Tree Structures", 
    "arxiv-id": "1106.5845v1", 
    "author": "Koichi Wada", 
    "publish": "2011-06-29T05:58:37Z", 
    "summary": "Given an n-vertex graph G=(V,E) and a set R \\subseteq {{x,y} | x,y \\in V} of\nrequests, we consider to assign a set of edges to each vertex in G so that for\nevery request {u, v} in R the union of the edge sets assigned to u and v\ncontains a path from u to v. The Minimum Certificate Dispersal Problem (MCD) is\ndefined as one to find an assignment that minimizes the sum of the cardinality\nof the edge set assigned to each vertex. This problem has been shown to be\nLOGAPX-complete for the most general setting, and APX-hard and 2-approximable\nin polynomial time for dense request sets, where R forms a clique. In this\npaper, we investigate the complexity of MCD with sparse (tree) structures. We\nfirst show that MCD is APX-hard when R is a tree, even a star. We then explore\nthe problem from the viewpoint of the maximum degree \\Delta of the tree: MCD\nfor tree request set with constant \\Delta is solvable in polynomial time, while\nthat with \\Delta=\\Omega(n) is 2.56-approximable in polynomial time but hard to\napproximate within 1.01 unless P=NP. As for the structure of G itself, we show\nthat the problem can be solved in polynomial time if G is a tree."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.6037v2", 
    "title": "Black Hole Search with Finite Automata Scattered in a Synchronous Torus", 
    "arxiv-id": "1106.6037v2", 
    "author": "Euripides Markou", 
    "publish": "2011-06-29T19:43:23Z", 
    "summary": "We consider the problem of locating a black hole in synchronous anonymous\nnetworks using finite state agents. A black hole is a harmful node in the\nnetwork that destroys any agent visiting that node without leaving any trace.\nThe objective is to locate the black hole without destroying too many agents.\nThis is difficult to achieve when the agents are initially scattered in the\nnetwork and are unaware of the location of each other. Previous studies for\nblack hole search used more powerful models where the agents had non-constant\nmemory, were labelled with distinct identifiers and could either write messages\non the nodes of the network or mark the edges of the network. In contrast, we\nsolve the problem using a small team of finite-state agents each carrying a\nconstant number of identical tokens that could be placed on the nodes of the\nnetwork. Thus, all resources used in our algorithms are independent of the\nnetwork size. We restrict our attention to oriented torus networks and first\nshow that no finite team of finite state agents can solve the problem in such\nnetworks, when the tokens are not movable. In case the agents are equipped with\nmovable tokens, we determine lower bounds on the number of agents and tokens\nrequired for solving the problem in torus networks of arbitrary size. Further,\nwe present a deterministic solution to the black hole search problem for\noriented torus networks, using the minimum number of agents and tokens."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.6136v1", 
    "title": "A Comparison of Performance Measures via Online Search", 
    "arxiv-id": "1106.6136v1", 
    "author": "Abyayananda Maiti", 
    "publish": "2011-06-30T07:35:00Z", 
    "summary": "Though competitive analysis has been a very useful performance measure for\nthe quality of online algorithms, it is recognized that it sometimes fails to\ndistinguish between algorithms of different quality in practice. A number of\nalternative measures have been proposed, but, with a few exceptions, these have\ngenerally been applied only to the online problem they were developed in\nconnection with. Recently, a systematic study of performance measures for\nonline algorithms was initiated [Boyar, Irani, Larsen: Eleventh International\nAlgorithms and Data Structures Symposium 2009], first focusing on a simple\nserver problem. We continue this work by studying a fundamentally different\nonline problem, online search, and the Reservation Price Policies in\nparticular. The purpose of this line of work is to learn more about the\napplicability of various performance measures in different situations and the\nproperties that the different measures emphasize. We investigate the following\nanalysis techniques: Competitive, Relative Worst Order, Bijective, Average,\nRelative Interval, Random Order, and Max/Max. In addition to drawing\nconclusions on this work, we also investigate the measures' sensitivity to\nintegral vs. real-valued domains, and as a part of this work, generalize some\nof the known performance measures. Finally, we have established the first\noptimality proof for Relative Interval Analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.6261v1", 
    "title": "External Memory Orthogonal Range Reporting with Fast Updates", 
    "arxiv-id": "1106.6261v1", 
    "author": "Yakov Nekrich", 
    "publish": "2011-06-30T15:11:57Z", 
    "summary": "In this paper we describe data structures for orthogonal range reporting in\nexternal memory that support fast update operations. The query costs either\nmatch the query costs of the best previously known data structures or differ by\na small multiplicative factor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.6336v1", 
    "title": "External-Memory Network Analysis Algorithms for Naturally Sparse Graphs", 
    "arxiv-id": "1106.6336v1", 
    "author": "Pawel Pszona", 
    "publish": "2011-06-30T18:43:43Z", 
    "summary": "In this paper, we present a number of network-analysis algorithms in the\nexternal-memory model. We focus on methods for large naturally sparse graphs,\nthat is, n-vertex graphs that have O(n) edges and are structured so that this\nsparsity property holds for any subgraph of such a graph. We give efficient\nexternal-memory algorithms for the following problems for such graphs: -\nFinding an approximate d-degeneracy ordering; - Finding a cycle of length\nexactly c; - Enumerating all maximal cliques. Such problems are of interest,\nfor example, in the analysis of social networks, where they are used to study\nnetwork cohesion."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1106.6342v1", 
    "title": "Quadratic-time Algorithm for the String Constrained LCS Problem", 
    "arxiv-id": "1106.6342v1", 
    "author": "Sebastian Deorowicz", 
    "publish": "2011-06-30T19:04:18Z", 
    "summary": "The problem of finding a longest common subsequence of two main sequences\nwith some constraint that must be a substring of the result (STR-IC-LCS) was\nformulated recently. It is a variant of the constrained longest common\nsubsequence problem. As the known algorithms for the STR-IC-LCS problem are\ncubic-time, the presented quadratic-time algorithm is significantly faster."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.0798v3", 
    "title": "Generalized Maneuvers in Route Planning", 
    "arxiv-id": "1107.0798v3", 
    "author": "Ondrej Moris", 
    "publish": "2011-07-05T07:02:14Z", 
    "summary": "We study an important practical aspect of the route planning problem in\nreal-world road networks -- maneuvers. Informally, maneuvers represent various\nirregularities of the road network graph such as turn-prohibitions, traffic\nlight delays, round-abouts, forbidden passages and so on. We propose a\ngeneralized model which can handle arbitrarily complex (and even negative)\nmaneuvers, and outline how to enhance Dijkstra's algorithm in order to solve\nroute planning queries in this model without prior adjustments of the\nunderlying road network graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1076v1", 
    "title": "Genome Halving by Block Interchange", 
    "arxiv-id": "1107.1076v1", 
    "author": "Jean-St\u00e9phane Varr\u00e9", 
    "publish": "2011-07-06T09:47:25Z", 
    "summary": "We address the problem of finding the minimal number of block interchanges\n(exchange of two intervals) required to transform a duplicated linear genome\ninto a tandem duplicated linear genome. We provide a formula for the distance\nas well as a polynomial time algorithm for the sorting problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1265v1", 
    "title": "Lift-and-Project Integrality Gaps for the Traveling Salesperson Problem", 
    "arxiv-id": "1107.1265v1", 
    "author": "Thomas Watson", 
    "publish": "2011-07-06T21:48:07Z", 
    "summary": "We study the lift-and-project procedures of Lov{\\'a}sz-Schrijver and\nSherali-Adams applied to the standard linear programming relaxation of the\ntraveling salesperson problem with triangle inequality. For the asymmetric TSP\ntour problem, Charikar, Goemans, and Karloff (FOCS 2004) proved that the\nintegrality gap of the standard relaxation is at least 2. We prove that after\none round of the Lov{\\'a}sz-Schrijver or Sherali-Adams procedures, the\nintegrality gap of the asymmetric TSP tour problem is at least 3/2, with a\nsmall caveat on which version of the standard relaxation is used. For the\nsymmetric TSP tour problem, the integrality gap of the standard relaxation is\nknown to be at least 4/3, and Cheung (SIOPT 2005) proved that it remains at\nleast 4/3 after $o(n)$ rounds of the Lov{\\'a}sz-Schrijver procedure, where $n$\nis the number of nodes. For the symmetric TSP path problem, the integrality gap\nof the standard relaxation is known to be at least 3/2, and we prove that it\nremains at least 3/2 after $o(n)$ rounds of the Lov{\\'a}sz-Schrijver procedure,\nby a simple reduction to Cheung's result."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1585v1", 
    "title": "On Multiway Cut parameterized above lower bounds", 
    "arxiv-id": "1107.1585v1", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2011-07-08T09:29:43Z", 
    "summary": "In this paper we consider two above lower bound parameterizations of the Node\nMultiway Cut problem - above the maximum separating cut and above a natural\nLP-relaxation - and prove them to be fixed-parameter tractable. Our results\nimply O*(4^k) algorithms for Vertex Cover above Maximum Matching and Almost\n2-SAT as well as an O*(2^k) algorithm for Node Multiway Cut with a standard\nparameterization by the solution size, improving previous bounds for these\nproblems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1628v1", 
    "title": "A Proof of the Boyd-Carr Conjecture", 
    "arxiv-id": "1107.1628v1", 
    "author": "Anke van Zuylen", 
    "publish": "2011-07-08T13:25:49Z", 
    "summary": "Determining the precise integrality gap for the subtour LP relaxation of the\ntraveling salesman problem is a significant open question, with little progress\nmade in thirty years in the general case of symmetric costs that obey triangle\ninequality. Boyd and Carr [3] observe that we do not even know the worst-case\nupper bound on the ratio of the optimal 2-matching to the subtour LP; they\nconjecture the ratio is at most 10/9. In this paper, we prove the Boyd-Carr\nconjecture. In the case that a fractional 2-matching has no cut edge, we can\nfurther prove that an optimal 2-matching is at most 10/9 times the cost of the\nfractional 2-matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1630v3", 
    "title": "On the Integrality Gap of the Subtour LP for the 1,2-TSP", 
    "arxiv-id": "1107.1630v3", 
    "author": "Anke van Zuylen", 
    "publish": "2011-07-08T13:39:09Z", 
    "summary": "In this paper, we study the integrality gap of the subtour LP relaxation for\nthe traveling salesman problem in the special case when all edge costs are\neither 1 or 2. For the general case of symmetric costs that obey triangle\ninequality, a famous conjecture is that the integrality gap is 4/3. Little\nprogress towards resolving this conjecture has been made in thirty years. We\nconjecture that when all edge costs $c_{ij}\\in \\{1,2\\}$, the integrality gap is\n$10/9$. We show that this conjecture is true when the optimal subtour LP\nsolution has a certain structure. Under a weaker assumption, which is an analog\nof a recent conjecture by Schalekamp, Williamson and van Zuylen, we show that\nthe integrality gap is at most $7/6$. When we do not make any assumptions on\nthe structure of the optimal subtour LP solution, we can show that integrality\ngap is at most $5/4$; this is the first bound on the integrality gap of the\nsubtour LP strictly less than $4/3$ known for an interesting special case of\nthe TSP. We show computationally that the integrality gap is at most $10/9$ for\nall instances with at most 12 cities."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.1780v1", 
    "title": "Hamiltonian Paths in Two Classes of Grid Graphs", 
    "arxiv-id": "1107.1780v1", 
    "author": "Alireza Bagheri", 
    "publish": "2011-07-09T12:44:53Z", 
    "summary": "In this paper, we give the necessary and sufficient conditions for the\nexistence of Hamiltonian paths in $L-$alphabet and $C-$alphabet grid graphs. We\nalso present a linear-time algorithm for finding Hamiltonian paths in these\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2000v1", 
    "title": "Tight Approximation Bounds for Vertex Cover on Dense k-Partite   Hypergraphs", 
    "arxiv-id": "1107.2000v1", 
    "author": "Claus Viehmann", 
    "publish": "2011-07-11T11:18:27Z", 
    "summary": "We establish almost tight upper and lower approximation bounds for the Vertex\nCover problem on dense k-partite hypergraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2033v1", 
    "title": "A note on the generalized min-sum set cover problem", 
    "arxiv-id": "1107.2033v1", 
    "author": "David P. Williamson", 
    "publish": "2011-07-11T14:03:32Z", 
    "summary": "In this paper, we consider the generalized min-sum set cover problem,\nintroduced by Azar, Gamzu, and Yin. Bansal, Gupta, and Krishnaswamy give a\n485-approximation algorithm for the problem. We are able to alter their\nalgorithm and analysis to obtain a 28-approximation algorithm, improving the\nperformance guarantee by an order of magnitude. We use concepts from\n$\\alpha$-point scheduling to obtain our improvements."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2105v2", 
    "title": "Speed Scaling on Parallel Processors with Migration", 
    "arxiv-id": "1107.2105v2", 
    "author": "Dimitrios Letsios", 
    "publish": "2011-07-11T19:56:53Z", 
    "summary": "We study the problem of scheduling a set of jobs with release dates,\ndeadlines and processing requirements (or works), on parallel speed-scaled\nprocessors so as to minimize the total energy consumption. We consider that\nboth preemption and migration of jobs are allowed. An exact polynomial-time\nalgorithm has been proposed for this problem, which is based on the Ellipsoid\nalgorithm. Here, we formulate the problem as a convex program and we propose a\nsimpler polynomial-time combinatorial algorithm which is based on a reduction\nto the maximum flow problem. Our algorithm runs in $O(nf(n)logP)$ time, where\n$n$ is the number of jobs, $P$ is the range of all possible values of\nprocessors' speeds divided by the desired accuracy and $f(n)$ is the complexity\nof computing a maximum flow in a layered graph with O(n) vertices.\nIndependently, Albers et al. \\cite{AAG11} proposed an $O(n^2f(n))$-time\nalgorithm exploiting the same relation with the maximum flow problem. We extend\nour algorithm to the multiprocessor speed scaling problem with migration where\nthe objective is the minimization of the makespan under a budget of energy."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2188v3", 
    "title": "The Simulated Greedy Algorithm for Several Submodular Matroid Secretary   Problems", 
    "arxiv-id": "1107.2188v3", 
    "author": "Yajun Wang", 
    "publish": "2011-07-12T04:35:02Z", 
    "summary": "We study the matroid secretary problems with submodular valuation functions.\nIn these problems, the elements arrive in random order. When one element\narrives, we have to make an immediate and irrevocable decision on whether to\naccept it or not. The set of accepted elements must form an {\\em independent\nset} in a predefined matroid. Our objective is to maximize the value of the\naccepted elements. In this paper, we focus on the case that the valuation\nfunction is a non-negative and monotonically non-decreasing submodular\nfunction.\n  We introduce a general algorithm for such {\\em submodular matroid secretary\nproblems}. In particular, we obtain constant competitive algorithms for the\ncases of laminar matroids and transversal matroids. Our algorithms can be\nfurther applied to any independent set system defined by the intersection of a\n{\\em constant} number of laminar matroids, while still achieving constant\ncompetitive ratios. Notice that laminar matroids generalize uniform matroids\nand partition matroids.\n  On the other hand, when the underlying valuation function is linear, our\nalgorithm achieves a competitive ratio of 9.6 for laminar matroids, which\nsignificantly improves the previous result."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2422v1", 
    "title": "A Linear Time Algorithm for Seeds Computation", 
    "arxiv-id": "1107.2422v1", 
    "author": "Tomasz Walen", 
    "publish": "2011-07-12T21:55:22Z", 
    "summary": "Periodicity in words is one of the most fundamental areas of text algorithms\nand combinatorics. Two classical and natural variations of periodicity are\nseeds and covers (also called quasiperiods). Linear-time algorithms are known\nfor finding all the covers of a word, however in case of seeds, for the past 15\nyears only an $O(n\\log{n})$ time algorithm was known (Iliopoulos, Moore and\nPark, 1996). Finding an $o(n\\log{n})$ time algorithm for the all-seeds problem\nwas mentioned as one of the most important open problems related to repetitions\nin words in a survey by Smyth (2000). We show a linear-time algorithm computing\nall the seeds of a word, in particular, the shortest seed. Our approach is\nbased on the use of a version of LZ-factorization and non-trivial combinatorial\nrelations between the LZ-factorization and seeds. It is used here for the first\ntime in context of seeds. It saves the work done for factors processed earlier,\nsimilarly as in Crochemore's square-free testing."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1107.2482v2", 
    "title": "Maximum Matchings via Glauber Dynamics", 
    "arxiv-id": "1107.2482v2", 
    "author": "Manjish Pal", 
    "publish": "2011-07-13T08:03:38Z", 
    "summary": "In this paper we study the classic problem of computing a maximum cardinality\nmatching in general graphs $G = (V, E)$. The best known algorithm for this\nproblem till date runs in $O(m \\sqrt{n})$ time due to Micali and Vazirani\n\\cite{MV80}. Even for general bipartite graphs this is the best known running\ntime (the algorithm of Karp and Hopcroft \\cite{HK73} also achieves this bound).\nFor regular bipartite graphs one can achieve an $O(m)$ time algorithm which,\nfollowing a series of papers, has been recently improved to $O(n \\log n)$ by\nGoel, Kapralov and Khanna (STOC 2010) \\cite{GKK10}. In this paper we present a\nrandomized algorithm based on the Markov Chain Monte Carlo paradigm which runs\nin $O(m \\log^2 n)$ time, thereby obtaining a significant improvement over\n\\cite{MV80}.\n  We use a Markov chain similar to the \\emph{hard-core model} for Glauber\nDynamics with \\emph{fugacity} parameter $\\lambda$, which is used to sample\nindependent sets in a graph from the Gibbs Distribution \\cite{V99}, to design a\nfaster algorithm for finding maximum matchings in general graphs. Our result\ncrucially relies on the fact that the mixing time of our Markov Chain is\nindependent of $\\lambda$, a significant deviation from the recent series of\nworks \\cite{GGSVY11,MWW09, RSVVY10, S10, W06} which achieve computational\ntransition (for estimating the partition function) on a threshold value of\n$\\lambda$. As a result we are able to design a randomized algorithm which runs\nin $O(m\\log^2 n)$ time that provides a major improvement over the running time\nof the algorithm due to Micali and Vazirani. Using the conductance bound, we\nalso prove that mixing takes $\\Omega(\\frac{m}{k})$ time where $k$ is the size\nof the maximum matching."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.2509v3", 
    "title": "Matching Pursuits with Random Sequential Subdictionaries", 
    "arxiv-id": "1107.2509v3", 
    "author": "Ga\u00ebl Richard", 
    "publish": "2011-07-13T10:06:35Z", 
    "summary": "Matching pursuits are a class of greedy algorithms commonly used in signal\nprocessing, for solving the sparse approximation problem. They rely on an atom\nselection step that requires the calculation of numerous projections, which can\nbe computationally costly for large dictionaries and burdens their\ncompetitiveness in coding applications. We propose using a non adaptive random\nsequence of subdictionaries in the decomposition process, thus parsing a large\ndictionary in a probabilistic fashion with no additional projection cost nor\nparameter estimation. A theoretical modeling based on order statistics is\nprovided, along with experimental evidence showing that the novel algorithm can\nbe efficiently used on sparse approximation problems. An application to audio\nsignal compression with multiscale time-frequency dictionaries is presented,\nalong with a discussion of the complexity and practical implementations."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.2554v1", 
    "title": "Routing in Undirected Graphs with Constant Congestion", 
    "arxiv-id": "1107.2554v1", 
    "author": "Julia Chuzhoy", 
    "publish": "2011-07-13T14:04:11Z", 
    "summary": "Given an undirected graph G=(V,E), a collection (s_1,t_1),...,(s_k,t_k) of k\nsource-sink pairs, and an integer c, the goal in the Edge Disjoint Paths with\nCongestion problem is to connect maximum possible number of the source-sink\npairs by paths, so that the maximum load on any edge (called edge congestion)\ndoes not exceed c.\n  We show an efficient randomized algorithm to route $\\Omega(OPT/\\poly\\log k)$\nsource-sink pairs with congestion at most 14, where OPT is the maximum number\nof pairs that can be simultaneously routed on edge-disjoint paths. The best\nprevious algorithm that routed $\\Omega(OPT/\\poly\\log n)$ pairs required\ncongestion $\\poly(\\log \\log n)$, and for the setting where the maximum allowed\ncongestion is bounded by a constant c, the best previous algorithms could only\nguarantee the routing of $OPT/n^{O(1/c)}$ pairs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.2686v2", 
    "title": "A Higher-Order Cheeger's Inequality", 
    "arxiv-id": "1107.2686v2", 
    "author": "Luca Trevisan", 
    "publish": "2011-07-13T22:13:52Z", 
    "summary": "A basic fact in algebraic graph theory is that the number of connected\ncomponents in an undirected graph is equal to the multiplicity of the\neigenvalue 1 in the normalized adjacency matrix of the graph. In particular,\nthe graph is disconnected if and only if there are at least two eigenvalues\nequal to 1.\n  Cheeger's inequality provides an \"approximate\" version of the latter fact,\nand it states that a graph has a sparse cut (it is \"almost disconnected\") if\nand only if there are at least two eigenvalues that are close to one.\n  It has been conjectured that an analogous characterization holds for higher\nmultiplicities, that is there are $k$ eigenvalues close to 1 if and only if the\nvertex set can be partitioned into $k$ subsets, each defining a sparse cut. In\nthis paper we resolve this conjecture. Our result provides a theoretical\njustification for clustering algorithms that use the top $k$ eigenvector to\nembed the vertices into $\\R^k$, and then apply geometric considerations to the\nembedding."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.2729v1", 
    "title": "Restructuring Compressed Texts without Explicit Decompression", 
    "arxiv-id": "1107.2729v1", 
    "author": "Masayuki Takeda", 
    "publish": "2011-07-14T05:35:09Z", 
    "summary": "We consider the problem of {\\em restructuring} compressed texts without\nexplicit decompression. We present algorithms which allow conversions from\ncompressed representations of a string $T$ produced by any grammar-based\ncompression algorithm, to representations produced by several specific\ncompression algorithms including LZ77, LZ78, run length encoding, and some\ngrammar based compression algorithms. These are the first algorithms that\nachieve running times polynomial in the size of the compressed input and output\nrepresentations of $T$. Since most of the representations we consider can\nachieve exponential compression, our algorithms are theoretically faster in the\nworst case, than any algorithm which first decompresses the string for the\nconversion."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3019v1", 
    "title": "Computing q-gram Frequencies on Collage Systems", 
    "arxiv-id": "1107.3019v1", 
    "author": "Masayuki Takeda", 
    "publish": "2011-07-15T09:22:46Z", 
    "summary": "Collage systems are a general framework for representing outputs of various\ntext compression algorithms. We consider the all $q$-gram frequency problem on\ncompressed string represented as a collage system, and present an $O((q+h\\log\nn)n)$-time $O(qn)$-space algorithm for calculating the frequencies for all\n$q$-grams that occur in the string. Here, $n$ and $h$ are respectively the size\nand height of the collage system."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3022v1", 
    "title": "Computing q-gram Non-overlapping Frequencies on SLP Compressed Texts", 
    "arxiv-id": "1107.3022v1", 
    "author": "Masayuki Takeda", 
    "publish": "2011-07-15T09:39:57Z", 
    "summary": "Length-$q$ substrings, or $q$-grams, can represent important characteristics\nof text data, and determining the frequencies of all $q$-grams contained in the\ndata is an important problem with many applications in the field of data mining\nand machine learning. In this paper, we consider the problem of calculating the\n{\\em non-overlapping frequencies} of all $q$-grams in a text given in\ncompressed form, namely, as a straight line program (SLP). We show that the\nproblem can be solved in $O(q^2n)$ time and $O(qn)$ space where $n$ is the size\nof the SLP. This generalizes and greatly improves previous work (Inenaga &\nBannai, 2009) which solved the problem only for $q=2$ in $O(n^4\\log n)$ time\nand $O(n^3)$ space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3622v1", 
    "title": "K-sort: A new sorting algorithm that beats Heap sort for n <= 70 lakhs!", 
    "arxiv-id": "1107.3622v1", 
    "author": "N. C. Mahanti", 
    "publish": "2011-07-19T04:21:58Z", 
    "summary": "Sundararajan and Chakraborty (2007) introduced a new version of Quick sort\nremoving the interchanges. Khreisat (2007) found this algorithm to be competing\nwell with some other versions of Quick sort. However, it uses an auxiliary\narray thereby increasing the space complexity. Here, we provide a second\nversion of our new sort where we have removed the auxiliary array. This second\nimproved version of the algorithm, which we call K-sort, is found to sort\nelements faster than Heap sort for an appreciably large array size (n <=\n70,00,000) for uniform U[0, 1] inputs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3658v1", 
    "title": "On Polynomial Kernels for Structural Parameterizations of Odd Cycle   Transversal", 
    "arxiv-id": "1107.3658v1", 
    "author": "Stefan Kratsch", 
    "publish": "2011-07-19T09:24:48Z", 
    "summary": "The Odd Cycle Transversal problem (OCT) asks whether a given graph can be\nmade bipartite (i.e., 2-colorable) by deleting at most l vertices. We study\nstructural parameterizations of OCT with respect to their polynomial\nkernelizability, i.e., whether instances can be efficiently reduced to a size\npolynomial in the chosen parameter. It is a major open problem in parameterized\ncomplexity whether Odd Cycle Transversal admits a polynomial kernel when\nparameterized by l. On the positive side, we show a polynomial kernel for OCT\nwhen parameterized by the vertex deletion distance to the class of bipartite\ngraphs of treewidth at most w (for any constant w); this generalizes the\nparameter feedback vertex set number (i.e., the distance to a forest).\nComplementing this, we exclude polynomial kernels for OCT parameterized by the\ndistance to outerplanar graphs, conditioned on the assumption that NP \\not\n\\subseteq coNP/poly. Thus the bipartiteness requirement for the treewidth w\ngraphs is necessary. Further lower bounds are given for parameterization by\ndistance from cluster and co-cluster graphs respectively, as well as for\nWeighted OCT parameterized by the vertex cover number (i.e., the distance from\nan independent set)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3704v2", 
    "title": "Co-nondeterminism in compositions: A kernelization lower bound for a   Ramsey-type problem", 
    "arxiv-id": "1107.3704v2", 
    "author": "Stefan Kratsch", 
    "publish": "2011-07-19T12:57:55Z", 
    "summary": "Until recently, techniques for obtaining lower bounds for kernelization were\none of the most sought after tools in the field of parameterized complexity.\nNow, after a strong influx of techniques, we are in the fortunate situation of\nhaving tools available that are even stronger than what has been required in\ntheir applications so far. Based on a result of Fortnow and Santhanam (JCSS\n2011), Bodlaender et al. (JCSS 2009) showed that, unless NP \\subseteq\ncoNP/poly, the existence of a deterministic polynomial-time composition\nalgorithm, i.e., an algorithm which outputs an instance of bounded parameter\nvalue which is yes if and only if one of t input instances is yes, rules out\nthe existence of polynomial kernels for a problem. Dell and van Melkebeek (STOC\n2010) continued this line of research and, amongst others, were able to rule\nout kernels of size O(k^d-eps) for certain problems, assuming NP !\\subseteq\ncoNP/poly. Their work implies that even the existence of a co-nondeterministic\ncomposition rules out polynomial kernels.\n  In this work we present the first example of how co-nondeterminism can help\nto make a composition algorithm. We study a Ramsey-type problem: Given a graph\nG and an integer k, the question is whether G contains an independent set or a\nclique of size at least k. It was asked by Rod Downey whether this problem\nadmits a polynomial kernelization. We provide a co-nondeterministic composition\nbased on embedding t instances into a single host graph H. The crux is that the\nhost graph H needs to observe a bound of L \\in O(log t) on both its maximum\nindependent set and maximum clique size, while also having a cover of its\nvertex set by independent sets and cliques all of size L; the\nco-nondeterministic composition is build around the search for such graphs.\nThus we show that, unless NP \\subseteq coNP/poly, the problem does not admit a\nkernelization with polynomial size guarantee."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.sigpro.2012.03.019", 
    "link": "http://arxiv.org/pdf/1107.3876v1", 
    "title": "Lower Bounds for the Average and Smoothed Number of Pareto Optima", 
    "arxiv-id": "1107.3876v1", 
    "author": "Luis Rademacher", 
    "publish": "2011-07-20T02:13:01Z", 
    "summary": "Smoothed analysis of multiobjective 0-1 linear optimization has drawn\nconsiderable attention recently. The number of Pareto-optimal solutions (i.e.,\nsolutions with the property that no other solution is at least as good in all\nthe coordinates and better in at least one) for multiobjective optimization\nproblems is the central object of study. In this paper, we prove several lower\nbounds for the expected number of Pareto optima. Our basic result is a lower\nbound of \\Omega_d(n^(d-1)) for optimization problems with d objectives and n\nvariables under fairly general conditions on the distributions of the linear\nobjectives. Our proof relates the problem of lower bounding the number of\nPareto optima to results in geometry connected to arrangements of hyperplanes.\nWe use our basic result to derive (1) To our knowledge, the first lower bound\nfor natural multiobjective optimization problems. We illustrate this for the\nmaximum spanning tree problem with randomly chosen edge weights. Our technique\nis sufficiently flexible to yield such lower bounds for other standard\nobjective functions studied in this setting (such as, multiobjective shortest\npath, TSP tour, matching). (2) Smoothed lower bound of min {\\Omega_d(n^(d-1.5)\n\\phi^{(d-log d) (1-\\Theta(1/\\phi))}), 2^{\\Theta(n)}}$ for the 0-1 knapsack\nproblem with d profits for phi-semirandom distributions for a version of the\nknapsack problem. This improves the recent lower bound of Brunsch and Roeglin."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.3977v2", 
    "title": "Detecting 2-joins faster", 
    "arxiv-id": "1107.3977v2", 
    "author": "Kristina Vu\u0161kovi\u0107", 
    "publish": "2011-07-20T13:40:23Z", 
    "summary": "2-joins are edge cutsets that naturally appear in the decomposition of\nseveral classes of graphs closed under taking induced subgraphs, such as\nbalanced bipartite graphs, even-hole-free graphs, perfect graphs and claw-free\ngraphs. Their detection is needed in several algorithms, and is the slowest\nstep for some of them. The classical method to detect a 2-join takes $O(n^3m)$\ntime where $n$ is the number of vertices of the input graph and $m$ the number\nof its edges. To detect \\emph{non-path} 2-joins (special kinds of 2-joins that\nare needed in all of the known algorithms that use 2-joins), the fastest known\nmethod takes time $O(n^4m)$. Here, we give an $O(n^2m)$-time algorithm for both\nof these problems. A consequence is a speed up of several known algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4223v1", 
    "title": "Sorting Algorithms with Restrictions", 
    "arxiv-id": "1107.4223v1", 
    "author": "Hakob Aslanyan", 
    "publish": "2011-07-21T10:11:15Z", 
    "summary": "Sorting is one of the most used and well investigated algorithmic problem\n[1]. Traditional postulation supposes the sorting data archived, and the\nelementary operation as comparisons of two numbers. In a view of appearance of\nnew processors and applied problems with data streams, sorting changed its\nface. This changes and generalizations are the subject of investigation in the\nresearch below."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4378v1", 
    "title": "Fully De-Amortized Cuckoo Hashing for Cache-Oblivious Dictionaries and   Multimaps", 
    "arxiv-id": "1107.4378v1", 
    "author": "Justin Thaler", 
    "publish": "2011-07-21T20:59:47Z", 
    "summary": "A dictionary (or map) is a key-value store that requires all keys be unique,\nand a multimap is a key-value store that allows for multiple values to be\nassociated with the same key. We design hashing-based indexing schemes for\ndictionaries and multimaps that achieve worst-case optimal performance for\nlookups and updates, with a small or negligible probability the data structure\nwill require a rehash operation, depending on whether we are working in the the\nexternal-memory (I/O) model or one of the well-known versions of the Random\nAccess Machine (RAM) model. One of the main features of our constructions is\nthat they are \\emph{fully de-amortized}, meaning that their performance bounds\nhold without one having to tune their constructions with certain performance\nparameters, such as the constant factors in the exponents of failure\nprobabilities or, in the case of the external-memory model, the size of blocks\nor cache lines and the size of internal memory (i.e., our external-memory\nalgorithms are cache oblivious). Our solutions are based on a fully\nde-amortized implementation of cuckoo hashing, which may be of independent\ninterest. This hashing scheme uses two cuckoo hash tables, one \"nested\" inside\nthe other, with one serving as a primary structure and the other serving as an\nauxiliary supporting queue/stash structure that is super-sized with respect to\ntraditional auxiliary structures but nevertheless adds negligible storage to\nour scheme. This auxiliary structure allows the success probability for cuckoo\nhashing to be very high, which is useful in cryptographic or data-intensive\napplications."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4466v2", 
    "title": "Counting Perfect Matchings as Fast as Ryser", 
    "arxiv-id": "1107.4466v2", 
    "author": "Andreas Bj\u00f6rklund", 
    "publish": "2011-07-22T09:49:47Z", 
    "summary": "We show that there is a polynomial space algorithm that counts the number of\nperfect matchings in an $n$-vertex graph in $O^*(2^{n/2})\\subset O(1.415^n)$\ntime. ($O^*(f(n))$ suppresses functions polylogarithmic in $f(n)$).The\npreviously fastest algorithms for the problem was the exponential space\n$O^*(((1+\\sqrt{5})/2)^n) \\subset O(1.619^n)$ time algorithm by Koivisto, and\nfor polynomial space, the $O(1.942^n)$ time algorithm by Nederlof. Our new\nalgorithm's runtime matches up to polynomial factors that of Ryser's 1963\nalgorithm for bipartite graphs. We present our algorithm in the more general\nsetting of computing the hafnian over an arbitrary ring, analogously to Ryser's\nalgorithm for permanent computation.\n  We also give a simple argument why the general exact set cover counting\nproblem over a slightly superpolynomial sized family of subsets of an $n$\nelement ground set cannot be solved in $O^*(2^{(1-\\epsilon_1)n})$ time for any\n$\\epsilon_1>0$ unless there are $O^*(2^{(1-\\epsilon_2)n})$ time algorithms for\ncomputing an $n\\times n$ 0/1 matrix permanent, for some $\\epsilon_2>0$\ndepending only on $\\epsilon_1$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4824v2", 
    "title": "Approximation Algorithms for Digraph Width Parameters", 
    "arxiv-id": "1107.4824v2", 
    "author": "Akash Kumar", 
    "publish": "2011-07-25T01:58:42Z", 
    "summary": "Several problems that are NP-hard on general graphs are efficiently solvable\non graphs with bounded treewidth. Efforts have been made to generalize\ntreewidth and the related notion of pathwidth to digraphs. Directed treewidth,\nDAG-width and Kelly-width are some such notions which generalize treewidth,\nwhereas directed pathwidth generalizes pathwidth. Each of these digraph width\nmeasures have an associated decomposition structure.\n  In this paper, we present approximation algorithms for all these digraph\nwidth parameters. In particular, we give an O(sqrt{logn})-approximation\nalgorithm for directed treewidth, and an O({\\log}^{3/2}{n})-approximation\nalgorithm for directed pathwidth, DAG-width and Kelly-width. Our algorithms\nconstruct the corresponding decompositions whose widths are within the above\nmentioned approximation factors."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4893v1", 
    "title": "Approximating minimum-power edge-multicovers", 
    "arxiv-id": "1107.4893v1", 
    "author": "Zeev Nutov", 
    "publish": "2011-07-25T11:07:16Z", 
    "summary": "Given a graph with edge costs, the {\\em power} of a node is themaximum cost\nof an edge incident to it, and the power of a graph is the sum of the powers of\nits nodes. Motivated by applications in wireless networks, we consider the\nfollowing fundamental problem in wireless network design. Given a graph\n$G=(V,E)$ with edge costs and degree bounds $\\{r(v):v \\in V\\}$, the {\\sf\nMinimum-Power Edge-Multi-Cover} ({\\sf MPEMC}) problem is to find a\nminimum-power subgraph $J$ of $G$ such that the degree of every node $v$ in $J$\nis at least $r(v)$. We give two approximation algorithms for {\\sf MPEMC}, with\nratios $O(\\log k)$ and $k+1/2$, where $k=\\max_{v \\in V} r(v)$ is the maximum\ndegree bound. This improves the previous ratios $O(\\log n)$ and $k+1$, and\nimplies ratios $O(\\log k)$ for the {\\sf Minimum-Power $k$-Outconnected\nSubgraph} and $O(\\log k \\log \\frac{n}{n-k})$ for the {\\sf Minimum-Power\n$k$-Connected Subgraph} problems; the latter is the currently best known ratio\nfor the min-cost version of the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jda.2012.11.003", 
    "link": "http://arxiv.org/pdf/1107.4970v1", 
    "title": "Drawing Graphs with Vertices at Specified Positions and Crossings at   Large Angles", 
    "arxiv-id": "1107.4970v1", 
    "author": "Alexander Wolff", 
    "publish": "2011-07-25T15:10:12Z", 
    "summary": "Point-set embeddings and large-angle crossings are two areas of graph drawing\nthat independently have received a lot of attention in the past few years. In\nthis paper, we consider problems in the intersection of these two areas. Given\nthe point-set-embedding scenario, we are interested in how much we gain in\nterms of computational complexity, curve complexity, and generality if we allow\nlarge-angle crossings as compared to the planar case. We investigate two\ndrawing styles where only bends or both bends and edges must be drawn on an\nunderlying grid. We present various results for drawings with one, two, and\nthree bends per edge."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0295v2", 
    "title": "Adaptive Drift Analysis", 
    "arxiv-id": "1108.0295v2", 
    "author": "Leslie Ann Goldberg", 
    "publish": "2011-08-01T12:22:27Z", 
    "summary": "We show that, for any c>0, the (1+1) evolutionary algorithm using an\narbitrary mutation rate p_n = c/n finds the optimum of a linear objective\nfunction over bit strings of length n in expected time Theta(n log n).\nPreviously, this was only known for c at most 1. Since previous work also shows\nthat universal drift functions cannot exist for c larger than a certain\nconstant, we instead define drift functions which depend crucially on the\nrelevant objective functions (and also on c itself). Using these\ncarefully-constructed drift functions, we prove that the expected optimisation\ntime is Theta(n log n). By giving an alternative proof of the multiplicative\ndrift theorem, we also show that our optimisation-time bound holds with high\nprobability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0388v1", 
    "title": "A Comprehensive Study of an Online Packet Scheduling Algorithm", 
    "arxiv-id": "1108.0388v1", 
    "author": "Fei Li", 
    "publish": "2011-08-01T18:49:37Z", 
    "summary": "We study the \\emph{bounded-delay model} for Qualify-of-Service buffer\nmanagement. Time is discrete. There is a buffer. Unit-length jobs (also called\n\\emph{packets}) arrive at the buffer over time. Each packet has an integer\nrelease time, an integer deadline, and a positive real value. A packet's\ncharacteristics are not known to an online algorithm until the packet actually\narrives. In each time step, at most one packet can be sent out of the buffer.\nThe objective is to maximize the total value of the packets sent by their\nrespective deadlines in an online manner. An online algorithm's performance is\nusually measured in terms of \\emph{competitive ratio}, when this online\nalgorithm is compared with a clairvoyant algorithm achieving the best total\nvalue. In this paper, we study a simple and intuitive online algorithm. We\nanalyze its performance in terms of competitive ratio for the general model and\na few important variants."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0554v2", 
    "title": "Towards an Optimal Space-and-Query-Time Index for Top-k Document   Retrieval", 
    "arxiv-id": "1108.0554v2", 
    "author": "Sharma V. Thankachan", 
    "publish": "2011-08-02T12:00:02Z", 
    "summary": "Let $\\D = $$ \\{d_1,d_2,...d_D\\}$ be a given set of $D$ string documents of\ntotal length $n$, our task is to index $\\D$, such that the $k$ most relevant\ndocuments for an online query pattern $P$ of length $p$ can be retrieved\nefficiently. We propose an index of size $|CSA|+n\\log D(2+o(1))$ bits and\n$O(t_{s}(p)+k\\log\\log n+poly\\log\\log n)$ query time for the basic relevance\nmetric \\emph{term-frequency}, where $|CSA|$ is the size (in bits) of a\ncompressed full text index of $\\D$, with $O(t_s(p))$ time for searching a\npattern of length $p$ . We further reduce the space to $|CSA|+n\\log D(1+o(1))$\nbits, however the query time will be $O(t_s(p)+k(\\log \\sigma \\log\\log\nn)^{1+\\epsilon}+poly\\log\\log n)$, where $\\sigma$ is the alphabet size and\n$\\epsilon >0$ is any constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0810v2", 
    "title": "Scheduling partially ordered jobs faster than 2^n", 
    "arxiv-id": "1108.0810v2", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2011-08-03T10:12:03Z", 
    "summary": "In the SCHED problem we are given a set of n jobs, together with their\nprocessing times and precedence constraints. The task is to order the jobs so\nthat their total completion time is minimized. SCHED is a special case of the\nTraveling Repairman Problem with precedences. A natural dynamic programming\nalgorithm solves both these problems in 2^n n^O(1) time, and whether there\nexists an algorithms solving SCHED in O(c^n) time for some constant c < 2 was\nan open problem posted in 2004 by Woeginger. In this paper we answer this\nquestion positively."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0866v1", 
    "title": "Towards Optimal Sorting of 16 Elements", 
    "arxiv-id": "1108.0866v1", 
    "author": "Marcin Peczarski", 
    "publish": "2011-08-03T15:24:49Z", 
    "summary": "One of the fundamental problem in the theory of sorting is to find the\npessimistic number of comparisons sufficient to sort a given number of\nelements. Currently 16 is the lowest number of elements for which we do not\nknow the exact value. We know that 46 comparisons suffices and that 44 do not.\nThere is an open question if 45 comparisons are sufficient. We present an\nattempt to resolve that problem by performing an exhaustive computer search. We\nalso present an algorithm for counting linear extensions which substantially\nspeeds up computations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1060v1", 
    "title": "Conauto-2.0: Fast Isomorphism Testing and Automorphism Group Computation", 
    "arxiv-id": "1108.1060v1", 
    "author": "Luis N\u00fa\u00f1ez Chiroque", 
    "publish": "2011-08-04T12:16:27Z", 
    "summary": "In this paper we present an algorithm, called conauto-2.0, that can\nefficiently compute a set of generators of the automorphism group of a graph,\nand test whether two graphs are isomorphic, finding an isomorphism if they are.\nThis algorithm uses the basic individualization/refinement technique, and is an\nimproved version of the algorithm conauto, which has been shown to be very fast\nfor random graphs and several families of hard graphs. In this paper, it is\nproved that, under some circumstances, it is not only possible to prune the\nsearch space (using already found generators of the automorphism group), but\nalso to infer new generators without the need of explicitly finding an\nautomorphism of the graph. This result is especially suited for graphs with\nregularly connected components, and can be applied in any isomorphism testing\nand canonical labeling algorithm (that use the individualization/refinement\ntechnique) to significantly improve its performance. Additionally, a dynamic\ntarget cell selection function is used to adapt to different graphs. The\nresulting algorithm preserves all the nice features of conauto, but reduces the\ntime for testing graphs with regularly connected components and other hard\ngraph families. We run extensive experiments, which show that the most popular\nalgorithms (namely, nauty, bliss, Traces, and saucy) are slower than\nconauto-2.0, among others, for the graph families based on components."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1130v2", 
    "title": "13/9-approximation for Graphic TSP", 
    "arxiv-id": "1108.1130v2", 
    "author": "Marcin Mucha", 
    "publish": "2011-08-04T16:23:48Z", 
    "summary": "The Travelling Salesman Problem is one the most fundamental and most studied\nproblems in approximation algorithms. For more than 30 years, the best\nalgorithm known for general metrics has been Christofides's algorithm with\napproximation factor of 3/2, even though the so-called Held-Karp LP relaxation\nof the problem is conjectured to have the integrality gap of only 4/3. Very\nrecently, significant progress has been made for the important special case of\ngraphic metrics, first by Oveis Gharan et al., and then by Momke and Svensson.\nIn this paper, we provide an improved analysis for the approach introduced by\nMomke and Svensson yielding a bound of 13/9 on the approximation factor, as\nwell as a bound of 19/12+epsilon for any epsilon>0 for a more general\nTravelling Salesman Path Problem in graphic metrics."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1176v1", 
    "title": "Combinatorial Algorithms for Capacitated Network Design", 
    "arxiv-id": "1108.1176v1", 
    "author": "Zeev Nutov", 
    "publish": "2011-08-04T19:34:38Z", 
    "summary": "We focus on designing combinatorial algorithms for the Capacitated Network\nDesign problem (Cap-SNDP). The Cap-SNDP is the problem of satisfying\nconnectivity requirements when edges have costs and hard capacities. We begin\nby showing that the Group Steiner tree problem (GST) is a special case of\nCap-SNDP even when there is connectivity requirement between only one\nsource-sink pair. This implies the first poly-logarithmic lower bound for the\nCap-SNDP. We next provide combinatorial algorithms for several special cases of\nthis problem. The Cap-SNDP is equivalent to its special case when every edge\nhas either zero cost or infinite capacity. We consider a special case, called\nConnected Cap-SNDP, where all infinite-capacity edges in the solution are\nrequired to form a connected component containing the sinks. This problem is\nmotivated by its similarity to the Connected Facility Location problem\n[G+01,SW04]. We solve this problem by reducing it to Submodular tree cover\nproblem, which is a common generalization of Connected Cap-SNDP and Group\nSteiner tree problem. We generalize the recursive greedy algorithm [CEK]\nachieving a poly-logarithmic approximation algorithm for Submodular tree cover\nproblem. This result is interesting in its own right and gives the first\npoly-logarithmic approximation algorithms for Connected hard capacities set\nmulti-cover and Connected source location.\n  We then study another special case of Cap-SNDP called Unbalanced\npoint-to-point connection problem. Besides its practical applications to shift\ndesign problems [EKS], it generalizes many problems such as k-MST, Steiner\nForest and Point-to-Point Connection. We give a combinatorial logarithmic\napproximation algorithm for this problem by reducing it to degree-bounded SNDP."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1351v1", 
    "title": "Fast k-means algorithm clustering", 
    "arxiv-id": "1108.1351v1", 
    "author": "Erik Test", 
    "publish": "2011-08-05T15:37:23Z", 
    "summary": "k-means has recently been recognized as one of the best algorithms for\nclustering unsupervised data. Since k-means depends mainly on distance\ncalculation between all data points and the centers, the time cost will be high\nwhen the size of the dataset is large (for example more than 500millions of\npoints). We propose a two stage algorithm to reduce the time cost of distance\ncalculation for huge datasets. The first stage is a fast distance calculation\nusing only a small portion of the data to produce the best possible location of\nthe centers. The second stage is a slow distance calculation in which the\ninitial centers used are taken from the first stage. The fast and slow stages\nrepresent the speed of the movement of the centers. In the slow stage, the\nwhole dataset can be used to get the exact location of the centers. The time\ncost of the distance calculation for the fast stage is very low due to the\nsmall size of the training data chosen. The time cost of the distance\ncalculation for the slow stage is also minimized due to small number of\niterations. Different initial locations of the clusters have been used during\nthe test of the proposed algorithms. For large datasets, experiments show that\nthe 2-stage clustering method achieves better speed-up (1-9 times)."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1751v1", 
    "title": "Efficient Sum-Based Hierarchical Smoothing Under \\ell_1-Norm", 
    "arxiv-id": "1108.1751v1", 
    "author": "Yuli Ye", 
    "publish": "2011-08-08T17:07:06Z", 
    "summary": "We introduce a new regression problem which we call the Sum-Based\nHierarchical Smoothing problem. Given a directed acyclic graph and a\nnon-negative value, called target value, for each vertex in the graph, we wish\nto find non-negative values for the vertices satisfying a certain constraint\nwhile minimizing the distance of these assigned values and the target values in\nthe lp-norm. The constraint is that the value assigned to each vertex should be\nno less than the sum of the values assigned to its children. We motivate this\nproblem with applications in information retrieval and web mining. While our\nproblem can be solved in polynomial time using linear programming, given the\ninput size in these applications such a solution might be too slow. We mainly\nstudy the \\ell_1-norm case restricting the underlying graphs to rooted trees.\nFor this case we provide an efficient algorithm, running in O(n^2) time. While\nthe algorithm is purely combinatorial, its proof of correctness is an elegant\nuse of linear programming duality. We believe that our approach may be\napplicable to similar problems, where comparable hierarchical constraints are\ninvolved, e.g. considering the average of the values assigned to the children\nof each vertex. While similar in flavor to other smoothing problems like\nIsotonic Regression (see for example [Angelov et al. SODA'06]), our problem is\narguably richer and theoretically more challenging."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1983v1", 
    "title": "Succinct Representations of Permutations and Functions", 
    "arxiv-id": "1108.1983v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2011-08-09T17:01:12Z", 
    "summary": "We investigate the problem of succinctly representing an arbitrary\npermutation, \\pi, on {0,...,n-1} so that \\pi^k(i) can be computed quickly for\nany i and any (positive or negative) integer power k. A representation taking\n(1+\\epsilon) n lg n + O(1) bits suffices to compute arbitrary powers in\nconstant time, for any positive constant \\epsilon <= 1. A representation taking\nthe optimal \\ceil{\\lg n!} + o(n) bits can be used to compute arbitrary powers\nin O(lg n / lg lg n) time.\n  We then consider the more general problem of succinctly representing an\narbitrary function, f: [n] \\rightarrow [n] so that f^k(i) can be computed\nquickly for any i and any integer power k. We give a representation that takes\n(1+\\epsilon) n lg n + O(1) bits, for any positive constant \\epsilon <= 1, and\ncomputes arbitrary positive powers in constant time. It can also be used to\ncompute f^k(i), for any negative integer k, in optimal O(1+|f^k(i)|) time.\n  We place emphasis on the redundancy, or the space beyond the\ninformation-theoretic lower bound that the data structure uses in order to\nsupport operations efficiently. A number of lower bounds have recently been\nshown on the redundancy of data structures. These lower bounds confirm the\nspace-time optimality of some of our solutions. Furthermore, the redundancy of\none of our structures \"surpasses\" a recent lower bound by Golynski [Golynski,\nSODA 2009], thus demonstrating the limitations of this lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.2157v1", 
    "title": "Optimal Indexes for Sparse Bit Vectors", 
    "arxiv-id": "1108.2157v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2011-08-10T11:36:22Z", 
    "summary": "We consider the problem of supporting Rank() and Select() operations on a bit\nvector of length m with n 1 bits. The problem is considered in the succinct\nindex model, where the bit vector is stored in \"read-only\" memory and an\nadditional data structure, called the index, is created during pre-processing\nto help answer the above queries. We give asymptotically optimal\ndensity-sensitive trade-offs, involving both m and n, that relate the size of\nthe index to the number of accesses to the bit vector (and processing time)\nneeded to answer the above queries. The results are particularly interesting\nfor the case where n = o(m)."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3048v2", 
    "title": "Fast Approximation Algorithms for Near-optimal Large-scale Network   Monitoring", 
    "arxiv-id": "1108.3048v2", 
    "author": "George Michailidis", 
    "publish": "2011-08-15T18:27:11Z", 
    "summary": "We study the problem of optimal traffic prediction and monitoring in\nlarge-scale networks. Our goal is to determine which subset of K links to\nmonitor in order to \"best\" predict the traffic on the remaining links in the\nnetwork. We consider several optimality criteria. This can be formulated as a\ncombinatorial optimization problem, belonging to the family of subset selection\nproblems. Similar NP-hard problems arise in statistics, machine learning and\nsignal processing. Some include subset selection for regression, variable\nselection, and sparse approximation. Exact solutions are computationally\nprohibitive. We present both new heuristics as well as new efficient algorithms\nimplementing the classical greedy heuristic - commonly used to tackle such\ncombinatorial problems. Our approach exploits connections to principal\ncomponent analysis (PCA), and yields new types of performance lower bounds\nwhich do not require submodularity of the objective functions. We show that an\nensemble method applied to our new randomized heuristic algorithm, often\noutperforms the classical greedy heuristic in practice. We evaluate our\nalgorithms under several large-scale networks, including real life networks."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3092v1", 
    "title": "Upward Point Set Embeddability for Convex Point Sets is in $P$", 
    "arxiv-id": "1108.3092v1", 
    "author": "Antonios Symvonis", 
    "publish": "2011-08-15T20:35:24Z", 
    "summary": "In this paper, we present a polynomial dynamic programming algorithm that\ntests whether a $n$-vertex directed tree $T$ has an upward planar embedding\ninto a convex point-set $S$ of size $n$. Further, we extend our approach to the\nclass of outerplanar digraphs. This nontrivial and surprising result implies\nthat any given digraph can be efficiently tested for an upward planar embedding\ninto a given convex point set."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3413v2", 
    "title": "Randomized Algorithms for Tracking Distributed Count, Frequencies, and   Ranks", 
    "arxiv-id": "1108.3413v2", 
    "author": "Qin Zhang", 
    "publish": "2011-08-17T07:31:27Z", 
    "summary": "We show that randomization can lead to significant improvements for a few\nfundamental problems in distributed tracking. Our basis is the {\\em\ncount-tracking} problem, where there are $k$ players, each holding a counter\n$n_i$ that gets incremented over time, and the goal is to track an\n$\\eps$-approximation of their sum $n=\\sum_i n_i$ continuously at all times,\nusing minimum communication. While the deterministic communication complexity\nof the problem is $\\Theta(k/\\eps \\cdot \\log N)$, where $N$ is the final value\nof $n$ when the tracking finishes, we show that with randomization, the\ncommunication cost can be reduced to $\\Theta(\\sqrt{k}/\\eps \\cdot \\log N)$. Our\nalgorithm is simple and uses only O(1) space at each player, while the lower\nbound holds even assuming each player has infinite computing power. Then, we\nextend our techniques to two related distributed tracking problems: {\\em\nfrequency-tracking} and {\\em rank-tracking}, and obtain similar improvements\nover previous deterministic algorithms. Both problems are of central importance\nin large data monitoring and analysis, and have been extensively studied in the\nliterature."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3516v3", 
    "title": "Model for networks of spatial objects and simulation of geographical   phenomena propagation", 
    "arxiv-id": "1108.3516v3", 
    "author": "Panteleimon Rodis", 
    "publish": "2011-08-17T16:13:53Z", 
    "summary": "The topic of this paper is the presentation of a new network model designed\nfor networks consisting of spatial objects. This model allows the development\nof more advance representations of systems of networked objects and the study\nof geographical phenomena propagated through networks. The capabilities of the\nmodel in simulation of geographical phenomena propagation are also studied and\nrelevant algorithms are presented. As examples of use, modeling of water supply\nnetwork and the simulation of traffic flow in road networks are presented."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3683v1", 
    "title": "Substring Range Reporting", 
    "arxiv-id": "1108.3683v1", 
    "author": "Inge Li Goertz", 
    "publish": "2011-08-18T08:31:11Z", 
    "summary": "We revisit various string indexing problems with range reporting features,\nnamely, position-restricted substring searching, indexing substrings with gaps,\nand indexing substrings with intervals. We obtain the following main results.\n{itemize} We give efficient reductions for each of the above problems to a new\nproblem, which we call \\emph{substring range reporting}. Hence, we unify the\nprevious work by showing that we may restrict our attention to a single problem\nrather than studying each of the above problems individually. We show how to\nsolve substring range reporting with optimal query time and little space.\nCombined with our reductions this leads to significantly improved time-space\ntrade-offs for the above problems. In particular, for each problem we obtain\nthe first solutions with optimal time query and $O(n\\log^{O(1)} n)$ space,\nwhere $n$ is the length of the indexed string. We show that our techniques for\nsubstring range reporting generalize to \\emph{substring range counting} and\n\\emph{substring range emptiness} variants. We also obtain non-trivial\ntime-space trade-offs for these problems. {itemize} Our bounds for substring\nrange reporting are based on a novel combination of suffix trees and range\nreporting data structures. The reductions are simple and general and may apply\nto other combinations of string indexing with range reporting."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4217v1", 
    "title": "Prismatic Algorithm for Discrete D.C. Programming Problems", 
    "arxiv-id": "1108.4217v1", 
    "author": "Takashi Washio", 
    "publish": "2011-08-21T22:09:21Z", 
    "summary": "In this paper, we propose the first exact algorithm for minimizing the\ndifference of two submodular functions (D.S.), i.e., the discrete version of\nthe D.C. programming problem. The developed algorithm is a\nbranch-and-bound-based algorithm which responds to the structure of this\nproblem through the relationship between submodularity and convexity. The D.S.\nprogramming problem covers a broad range of applications in machine learning\nbecause this generalizes the optimization of a wide class of set functions. We\nempirically investigate the performance of our algorithm, and illustrate the\ndifference between exact and approximate solutions respectively obtained by the\nproposed and existing algorithms in feature selection and discriminative\nstructure learning."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4408v1", 
    "title": "On Compressing Permutations and Adaptive Sorting", 
    "arxiv-id": "1108.4408v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-08-22T19:59:03Z", 
    "summary": "Previous compact representations of permutations have focused on adding a\nsmall index on top of the plain data $<\\pi(1), \\pi(2),...\\pi(n)>$, in order to\nefficiently support the application of the inverse or the iterated permutation.\n  In this paper we initiate the study of techniques that exploit the\ncompressibility of the data itself, while retaining efficient computation of\n$\\pi(i)$ and its inverse.\n  In particular, we focus on exploiting {\\em runs}, which are subsets\n(contiguous or not) of the domain where the permutation is monotonic.\n  Several variants of those types of runs arise in real applications such as\ninverted indexes and suffix arrays.\n  Furthermore, our improved results on compressed data structures for\npermutations also yield better adaptive sorting algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4606v1", 
    "title": "Capacitated Domination: Constant Factor Approximation for Planar Graphs", 
    "arxiv-id": "1108.4606v1", 
    "author": "D. T. Lee", 
    "publish": "2011-08-23T14:10:15Z", 
    "summary": "We consider the capacitated domination problem, which models a\nservice-requirement assigning scenario and which is also a generalization of\nthe dominating set problem. In this problem, we are given a graph with three\nparameters defined on the vertex set, which are cost, capacity, and demand. The\nobjective of this problem is to compute a demand assignment of least cost, such\nthat the demand of each vertex is fully-assigned to some of its closed\nneighbours without exceeding the amount of capacity they provide.\n  In this paper, we provide the first constant factor approximation for this\nproblem on planar graphs, based on a new perspective on the hierarchical\nstructure of outer-planar graphs. We believe that this new perspective and\ntechnique can be applied to other capacitated covering problems to help tackle\nvertices of large degrees."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.4983v2", 
    "title": "A $(k + 3)/2$-approximation algorithm for monotone submodular   maximization over a $k$-exchange system", 
    "arxiv-id": "1108.4983v2", 
    "author": "Justin Ward", 
    "publish": "2011-08-25T01:28:59Z", 
    "summary": "We consider the problem of maximizing a monotone submodular function in a\n$k$-exchange system. These systems, introduced by Feldman et al., generalize\nthe matroid k-parity problem in a wide class of matroids and capture many other\ncombinatorial optimization problems. Feldman et al. show that a simple\nnon-oblivious local search algorithm attains a $(k + 1)/2$ approximation ratio\nfor the problem of linear maximization in a $k$-exchange system. Here, we\nextend this approach to the case of monotone submodular objective functions. We\ngive a deterministic, non-oblivious local search algorithm that attains an\napproximation ratio of $(k + 3)/2$ for the problem of maximizing a monotone\nsubmodular function in a $k$-exchange system."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.5471v1", 
    "title": "New Results on the Fault-Tolerant Facility Placement Problem", 
    "arxiv-id": "1108.5471v1", 
    "author": "Marek Chrobak", 
    "publish": "2011-08-27T19:31:25Z", 
    "summary": "We studied the Fault-Tolerant Facility Placement problem (FTFP) which\ngeneralizes the uncapacitated facility location problem (UFL). In FTFP, we are\ngiven a set F of sites at which facilities can be built, and a set C of clients\nwith some demands that need to be satisfied by different facilities. A client\n$j$ has demand $r_j$. Building one facility at a site $i$ incurs a cost $f_i$,\nand connecting one unit of demand from client $j$ to a facility at site\n$i\\in\\fac$ costs $d_{ij}$. $d_{ij}$'s are assumed to form a metric. A feasible\nsolution specifies the number of facilities to be built at each site and the\nway to connect demands from clients to facilities, with the restriction that\ndemands from the same client must go to different facilities. Facilities at the\nsame site are considered different. The goal is to find a solution with minimum\ntotal cost. We gave a 1.7245-approximation algorithm to the FTFP problem. Our\ntechnique is via a reduction to the Fault-Tolerant Facility Location problem,\nin which each client has demand $r_j$ but each site can have at most one\nfacility built."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.5525v1", 
    "title": "The update complexity of selection and related problems", 
    "arxiv-id": "1108.5525v1", 
    "author": "Sandeep Sen", 
    "publish": "2011-08-29T12:40:03Z", 
    "summary": "We present a framework for computing with input data specified by intervals,\nrepresenting uncertainty in the values of the input parameters. To compute a\nsolution, the algorithm can query the input parameters that yield more refined\nestimates in form of sub-intervals and the objective is to minimize the number\nof queries. The previous approaches address the scenario where every query\nreturns an exact value. Our framework is more general as it can deal with a\nwider variety of inputs and query responses and we establish interesting\nrelationships between them that have not been investigated previously. Although\nsome of the approaches of the previous restricted models can be adapted to the\nmore general model, we require more sophisticated techniques for the analysis\nand we also obtain improved algorithms for the previous model.\n  We address selection problems in the generalized model and show that there\nexist 2-update competitive algorithms that do not depend on the lengths or\ndistribution of the sub-intervals and hold against the worst case adversary. We\nalso obtain similar bounds on the competitive ratio for the MST problem in\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1110.0180v1", 
    "title": "An efficient algorithm to find a set of nearest elements in a mesh", 
    "arxiv-id": "1110.0180v1", 
    "author": "Gleb Novichkov", 
    "publish": "2011-10-02T12:32:32Z", 
    "summary": "A linear time algorithm to find a set of nearest elements in a mesh."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1110.0583v1", 
    "title": "Algorithms for the strong chromatic index of Halin graphs,   distance-hereditary graphs and maximal outerplanar graphs", 
    "arxiv-id": "1110.0583v1", 
    "author": "Yue-Li Wang", 
    "publish": "2011-10-04T05:43:28Z", 
    "summary": "We show that there exist linear-time algorithms that compute the strong\nchromatic index of Halin graphs, of maximal outerplanar graphs and of\ndistance-hereditary graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.0620v1", 
    "title": "A 2.75-Approximation Algorithm for the Unconstrained Traveling   Tournament Problem", 
    "arxiv-id": "1110.0620v1", 
    "author": "Ryuhei Miyashiro", 
    "publish": "2011-10-04T09:35:21Z", 
    "summary": "A 2.75-approximation algorithm is proposed for the unconstrained traveling\ntournament problem, which is a variant of the traveling tournament problem. For\nthe unconstrained traveling tournament problem, this is the first proposal of\nan approximation algorithm with a constant approximation ratio. In addition,\nthe proposed algorithm yields a solution that meets both the no-repeater and\nmirrored constraints. Computational experiments show that the algorithm\ngenerates solutions of good quality."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.0990v1", 
    "title": "The Query-commit Problem", 
    "arxiv-id": "1110.0990v1", 
    "author": "R. Ravi", 
    "publish": "2011-10-05T14:08:33Z", 
    "summary": "In the query-commit problem we are given a graph where edges have distinct\nprobabilities of existing. It is possible to query the edges of the graph, and\nif the queried edge exists then its endpoints are irrevocably matched. The goal\nis to find a querying strategy which maximizes the expected size of the\nmatching obtained. This stochastic matching setup is motivated by applications\nin kidney exchanges and online dating.\n  In this paper we address the query-commit problem from both theoretical and\nexperimental perspectives. First, we show that a simple class of edges can be\nqueried without compromising the optimality of the strategy. This property is\nthen used to obtain in polynomial time an optimal querying strategy when the\ninput graph is sparse. Next we turn our attentions to the kidney exchange\napplication, focusing on instances modeled over real data from existing\nexchange programs. We prove that, as the number of nodes grows, almost every\ninstance admits a strategy which matches almost all nodes. This result supports\nthe intuition that more exchanges are possible on a larger pool of\npatient/donors and gives theoretical justification for unifying the existing\nexchange programs. Finally, we evaluate experimentally different querying\nstrategies over kidney exchange instances. We show that even very simple\nheuristics perform fairly well, being within 1.5% of an optimal clairvoyant\nstrategy, that knows in advance the edges in the graph. In such a\ntime-sensitive application, this result motivates the use of committing\nstrategies."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1064v1", 
    "title": "Approximating CSPs with Global Cardinality Constraints Using SDP   Hierarchies", 
    "arxiv-id": "1110.1064v1", 
    "author": "Ning Tan", 
    "publish": "2011-10-05T18:31:44Z", 
    "summary": "This work is concerned with approximating constraint satisfaction problems\n(CSPs) with an additional global cardinality constraints. For example, \\maxcut\nis a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find\na cut $S \\cup \\bar S = V$ that maximizes the numberof crossing edges,\n$|E(S,\\bar S)|$. The \\maxbisection problem is a variant of \\maxcut with an\nadditional global constraint that each side of the cut has exactly half the\nvertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like\n\\minbisection and approximating Graph Expansion can be formulated as CSPs with\nglobal constraints.\n  In this work, we formulate a general approach towards approximating CSPs with\nglobal constraints using SDP hierarchies. To demonstrate the approach we\npresent the following results:\n  Using the Lasserre hierarchy, we present an algorithm that runs in time\n$O(n^{poly(1/\\epsilon)})$ that given an instance of \\maxbisection with value\n$1-\\epsilon$, finds a bisection with value $1-O(\\sqrt{\\epsilon})$. This\napproximation is near-optimal (up to constant factors in $O()$) under the\nUnique Games Conjecture.\n  By a computer-assisted proof, we show that the same algorithm also achieves a\n0.85-approximation for \\maxbisection, improving on the previous bound of 0.70\n(note that it is \\uniquegames hard to approximate better than a 0.878 factor).\nThe same algorithm also yields a 0.92-approximation for \\maxtwosat with\ncardinality constraints.\n  For every CSP with a global cardinality constraints, we present a generic\nconversion from integrality gap instances for the Lasserre hierarchy to a {\\it\ndictatorship test} whose soundness is at most integrality gap. Dictatorship\ntesting gadgets are central to hardness results for CSPs, and a generic\nconversion of the above nature lies at the core of the tight Unique Games based\nhardness result for CSPs. \\cite{Raghavendra08}"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1079v1", 
    "title": "A Near-Optimal Sublinear-Time Algorithm for Approximating the Minimum   Vertex Cover Size", 
    "arxiv-id": "1110.1079v1", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2011-10-05T19:27:57Z", 
    "summary": "We give a nearly optimal sublinear-time algorithm for approximating the size\nof a minimum vertex cover in a graph G. The algorithm may query the degree\ndeg(v) of any vertex v of its choice, and for each 1 <= i <= deg(v), it may ask\nfor the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex\ncover in G, the algorithm outputs, with high constant success probability, an\nestimate VC_estimate(G) such that VC_opt(G) <= VC_estimate(G) <= 2 * VC_opt(G)\n+ epsilon*n, where epsilon is a given additive approximation parameter. We\nrefer to such an estimate as a (2,epsilon)-estimate. The query complexity and\nrunning time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg\ndenotes the average vertex degree in the graph. The best previously known\nsublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and\nrunning time O(d^4/epsilon^2), where d is the maximum degree in the graph.\nGiven the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining\nsuch an estimate (with any constant multiplicative factor) due to Parnas and\nRon (TCS 2007), our result is nearly optimal.\n  In the case that the graph is dense, that is, the number of edges is\nTheta(n^2), we consider another model, in which the algorithm may ask, for any\npair of vertices u and v, whether there is an edge between u and v. We show how\nto adapt the algorithm that uses neighbor queries to this model and obtain an\nalgorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex\ncover whose query complexity and running time are ~O(n) * poly(1/epsilon)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1124v1", 
    "title": "Optimal Deadline Scheduling with Commitment", 
    "arxiv-id": "1110.1124v1", 
    "author": "Ting He", 
    "publish": "2011-10-06T00:49:03Z", 
    "summary": "We consider an online preemptive scheduling problem where jobs with deadlines\narrive sporadically. A commitment requirement is imposed such that the\nscheduler has to either accept or decline a job immediately upon arrival. The\nscheduler's decision to accept an arriving job constitutes a contract with the\ncustomer; if the accepted job is not completed by its deadline as promised, the\nscheduler loses the value of the corresponding job and has to pay an additional\npenalty depending on the amount of unfinished workload. The objective of the\nonline scheduler is to maximize the overall profit, i.e., the total value of\nthe admitted jobs completed before their deadlines less the penalty paid for\nthe admitted jobs that miss their deadlines. We show that the maximum\ncompetitive ratio is $3-2\\sqrt{2}$ and propose a simple online algorithm to\nachieve this competitive ratio. The optimal scheduling includes a threshold\nadmission and a greedy scheduling policies. The proposed algorithm has direct\napplications to the charging of plug-in hybrid electrical vehicles (PHEV) at\ngarages or parking lots."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1194v1", 
    "title": "Efficient Encoding of Watermark Numbers as Reducible Permutation Graphs", 
    "arxiv-id": "1110.1194v1", 
    "author": "Stavros D. Nikolopoulos", 
    "publish": "2011-10-06T09:24:51Z", 
    "summary": "In a software watermarking environment, several graph theoretic watermark\nmethods use numbers as watermark values, where some of these methods encode the\nwatermark numbers as graph structures. In this paper we extended the class of\nerror correcting graphs by proposing an efficient and easily implemented codec\nsystem for encoding watermark numbers as reducible permutation flow-graphs.\nMore precisely, we first present an efficient algorithm which encodes a\nwatermark number $w$ as self-inverting permutation $\\pi^*$ and, then, an\nalgorithm which encodes the self-inverting permutation $\\pi^*$ as a reducible\npermutation flow-graph $F[\\pi^*]$ by exploiting domination relations on the\nelements of $\\pi^*$ and using an efficient DAG representation of $\\pi^*$. The\nwhole encoding process takes O(n) time and space, where $n$ is the binary size\nof the number $w$ or, equivalently, the number of elements of the permutation\n$\\pi^*$. We also propose efficient decoding algorithms which extract the number\n$w$ from the reducible permutation flow-graph $F[\\pi^*]$ within the same time\nand space complexity. The two main components of our proposed codec system,\ni.e., the self-inverting permutation $\\pi^*$ and the reducible permutation\ngraph $F[\\pi^*]$, incorporate important structural properties which make our\nsystem resilient to attacks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1320v2", 
    "title": "An efficient polynomial-time approximation scheme for Steiner forest in   planar graphs", 
    "arxiv-id": "1110.1320v2", 
    "author": "Claire Mathieu", 
    "publish": "2011-10-06T16:51:57Z", 
    "summary": "We give an $O(n \\log^3 n)$ approximation scheme for Steiner forest in planar\ngraphs, improving on the previous approximation scheme for this problem, which\nruns in $O(n^{f(\\epsilon)})$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1580v1", 
    "title": "A Polylogarithmic-Competitive Algorithm for the k-Server Problem", 
    "arxiv-id": "1110.1580v1", 
    "author": "Naor", 
    "publish": "2011-10-07T16:39:34Z", 
    "summary": "We give the first polylogarithmic-competitive randomized online algorithm for\nthe $k$-server problem on an arbitrary finite metric space. In particular, our\nalgorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any\nmetric space on n points. Our algorithm improves upon the deterministic\n(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]\nwhenever n is sub-exponential in k."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1693v1", 
    "title": "On the strong chromatic index and maximum induced matching of   tree-cographs and permutation graphs", 
    "arxiv-id": "1110.1693v1", 
    "author": "Yue-Li Wang", 
    "publish": "2011-10-08T04:04:41Z", 
    "summary": "We show that there exist linear-time algorithms that compute the strong\nchromatic index and a maximum induced matching of tree-cographs when the\ndecomposition tree is a part of the input. We also show that there exists an\nefficient algorithm for the strong chromatic index of permutation graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.2207v3", 
    "title": "Minimum Latency Submodular Cover", 
    "arxiv-id": "1110.2207v3", 
    "author": "Ruben van der Zwaan", 
    "publish": "2011-10-10T21:45:49Z", 
    "summary": "We study the Minimum Latency Submodular Cover problem (MLSC), which consists\nof a metric $(V,d)$ with source $r\\in V$ and $m$ monotone submodular functions\n$f_1, f_2, ..., f_m: 2^V \\rightarrow [0,1]$. The goal is to find a path\noriginating at $r$ that minimizes the total cover time of all functions. This\ngeneralizes well-studied problems, such as Submodular Ranking [AzarG11] and\nGroup Steiner Tree [GKR00]. We give a polynomial time $O(\\log \\frac{1}{\\eps}\n\\cdot \\log^{2+\\delta} |V|)$-approximation algorithm for MLSC, where\n$\\epsilon>0$ is the smallest non-zero marginal increase of any\n$\\{f_i\\}_{i=1}^m$ and $\\delta>0$ is any constant.\n  We also consider the Latency Covering Steiner Tree problem (LCST), which is\nthe special case of \\mlsc where the $f_i$s are multi-coverage functions. This\nis a common generalization of the Latency Group Steiner Tree\n[GuptaNR10a,ChakrabartyS11] and Generalized Min-sum Set Cover [AzarGY09,\nBansalGK10] problems. We obtain an $O(\\log^2|V|)$-approximation algorithm for\nLCST.\n  Finally we study a natural stochastic extension of the Submodular Ranking\nproblem, and obtain an adaptive algorithm with an $O(\\log 1/ \\eps)$\napproximation ratio, which is best possible. This result also generalizes some\npreviously studied stochastic optimization problems, such as Stochastic Set\nCover [GoemansV06] and Shared Filter Evaluation [MunagalaSW07, LiuPRY08]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.2893v1", 
    "title": "String Matching with Variable Length Gaps", 
    "arxiv-id": "1110.2893v1", 
    "author": "David Kofoed Wind", 
    "publish": "2011-10-13T11:13:48Z", 
    "summary": "We consider string matching with variable length gaps. Given a string $T$ and\na pattern $P$ consisting of strings separated by variable length gaps\n(arbitrary strings of length in a specified range), the problem is to find all\nending positions of substrings in $T$ that match $P$. This problem is a basic\nprimitive in computational biology applications. Let $m$ and $n$ be the lengths\nof $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We\npresent a new algorithm achieving time $O(n\\log k + m +\\alpha)$ and space $O(m\n+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in\n$P$ and $\\alpha$ is the total number of occurrences of the strings in $P$\nwithin $T$. Compared to the previous results this bound essentially achieves\nthe best known time and space complexities simultaneously. Consequently, our\nalgorithm obtains the best known bounds for almost all combinations of $m$,\n$n$, $k$, $A$, and $\\alpha$. Our algorithm is surprisingly simple and\nstraightforward to implement. We also present algorithms for finding and\nencoding the positions of all strings in $P$ for every match of the pattern."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3100v1", 
    "title": "Telling Two Distributions Apart: a Tight Characterization", 
    "arxiv-id": "1110.3100v1", 
    "author": "Mark Sandler", 
    "publish": "2011-10-14T00:46:23Z", 
    "summary": "We consider the problem of distinguishing between two arbitrary black-box\ndistributions defined over the domain [n], given access to $s$ samples from\nboth. It is known that in the worst case O(n^{2/3}) samples is both necessary\nand sufficient, provided that the distributions have L1 difference of at least\n{\\epsilon}. However, it is also known that in many cases fewer samples suffice.\nWe identify a new parameter, that provides an upper bound on how many samples\nneeded, and present an efficient algorithm that requires the number of samples\nindependent of the domain size. Also for a large subclass of distributions we\nprovide a lower bound, that matches our upper bound up to a poly-logarithmic\nfactor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3381v1", 
    "title": "Partial Data Compression and Text Indexing via Optimal Suffix   Multi-Selection", 
    "arxiv-id": "1110.3381v1", 
    "author": "S. Muthukrishnan", 
    "publish": "2011-10-15T05:16:18Z", 
    "summary": "Consider an input text string T[1,N] drawn from an unbounded alphabet. We\nstudy partial computation in suffix-based problems for Data Compression and\nText Indexing such as\n  (I) retrieve any segment of K<=N consecutive symbols from the Burrows-Wheeler\ntransform of T, and\n  (II) retrieve any chunk of K<=N consecutive entries of the Suffix Array or\nthe Suffix Tree.\n  Prior literature would take O(N log N) comparisons (and time) to solve these\nproblems by solving the total problem of building the entire Burrows-Wheeler\ntransform or Text Index for T, and performing a post-processing to single out\nthe wanted portion.\n  We introduce a novel adaptive approach to partial computational problems\nabove, and solve both the partial problems in O(K log K + N) comparisons and\ntime, improving the best known running times of O(N log N) for K=o(N).\n  These partial-computation problems are intimately related since they share a\ncommon bottleneck: the suffix multi-selection problem, which is to output the\nsuffixes of rank r_1,r_2,...,r_K under the lexicographic order, where\nr_1<r_2<...<r_K, r_i in [1,N]. Special cases of this problem are well known:\nK=N is the suffix sorting problem that is the workhorse in Stringology with\nhundreds of applications, and K=1 is the recently studied suffix selection.\n  We show that suffix multi-selection can be solved in Theta(N log N -\nsum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,\nr_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0<=j<=K. This is asymptotically\noptimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for\nmulti-selection on atomic elements (not suffixes). Matching the bound known for\natomic elements for strings is a long running theme and challenge from 70's,\nwhich we achieve for the suffix multi-selection problem. The partial suffix\nproblems as well as the suffix multi-selection problem have many applications."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3850v1", 
    "title": "On the Power of Adaptivity in Sparse Recovery", 
    "arxiv-id": "1110.3850v1", 
    "author": "David P. Woodruff", 
    "publish": "2011-10-17T23:35:11Z", 
    "summary": "The goal of (stable) sparse recovery is to recover a $k$-sparse approximation\n$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is\nto recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some\nconstant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or\n$p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ non-adaptive\nmeasurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].\n  In this paper we show that if one is allowed to perform measurements that are\nadaptive, then the number of measurements can be considerably reduced.\nSpecifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k\nlog log (n eps/k))$ measurements that uses $O(log* k \\log \\log (n eps/k))$\nrounds. This is a significant improvement over the best possible non-adaptive\nbound. - A scheme with $m=O((1/eps) k log (k/eps) + k \\log (n/k))$ measurements\nthat uses /two/ rounds. This improves over the best possible non-adaptive\nbound. To the best of our knowledge, these are the first results of this type.\nAs an independent application, we show how to solve the problem of finding a\nduplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using\n$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best\npossible space complexity achievable using a single pass."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4150v1", 
    "title": "Traffic-Redundancy Aware Network Design", 
    "arxiv-id": "1110.4150v1", 
    "author": "Shuchi Chawla", 
    "publish": "2011-10-19T00:43:25Z", 
    "summary": "We consider network design problems for information networks where routers\ncan replicate data but cannot alter it. This functionality allows the network\nto eliminate data-redundancy in traffic, thereby saving on routing costs. We\nconsider two problems within this framework and design approximation\nalgorithms.\n  The first problem we study is the traffic-redundancy aware network design\n(RAND) problem. We are given a weighted graph over a single server and many\nclients. The server owns a number of different data packets and each client\ndesires a subset of the packets; the client demand sets form a laminar set\nsystem. Our goal is to connect every client to the source via a single path,\nsuch that the collective cost of the resulting network is minimized. Here the\ntransportation cost over an edge is its weight times times the number of\ndistinct packets that it carries.\n  The second problem is a facility location problem that we call RAFL. Here the\ngoal is to find an assignment from clients to facilities such that the total\ncost of routing packets from the facilities to clients (along unshared paths),\nplus the total cost of \"producing\" one copy of each desired packet at each\nfacility is minimized.\n  We present a constant factor approximation for the RAFL and an O(log P)\napproximation for RAND, where P is the total number of distinct packets. We\nremark that P is always at most the number of different demand sets desired or\nthe number of clients, and is generally much smaller."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4319v2", 
    "title": "Min-Max Graph Partitioning and Small Set Expansion", 
    "arxiv-id": "1110.4319v2", 
    "author": "Roy Schwartz", 
    "publish": "2011-10-19T15:51:38Z", 
    "summary": "We study graph partitioning problems from a min-max perspective, in which an\ninput graph on n vertices should be partitioned into k parts, and the objective\nis to minimize the maximum number of edges leaving a single part. The two main\nversions we consider are where the k parts need to be of equal-size, and where\nthey must separate a set of k given terminals. We consider a common\ngeneralization of these two problems, and design for it an $O(\\sqrt{\\log n\\log\nk})$-approximation algorithm. This improves over an $O(\\log^2 n)$ approximation\nfor the second version, and roughly $O(k\\log n)$ approximation for the first\nversion that follows from other previous work. We also give an improved\nO(1)-approximation algorithm for graphs that exclude any fixed minor.\n  Our algorithm uses a new procedure for solving the Small-Set Expansion\nproblem. In this problem, we are given a graph G and the goal is to find a\nnon-empty set $S\\subseteq V$ of size $|S| \\leq \\rho n$ with minimum\nedge-expansion. We give an $O(\\sqrt{\\log{n}\\log{(1/\\rho)}})$ bicriteria\napproximation algorithm for the general case of Small-Set Expansion, and O(1)\napproximation algorithm for graphs that exclude any fixed minor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4350v1", 
    "title": "Taking Roots over High Extensions of Finite Fields", 
    "arxiv-id": "1110.4350v1", 
    "author": "Eric Schost", 
    "publish": "2011-10-19T18:33:28Z", 
    "summary": "We present a new algorithm for computing $m$-th roots over the finite field\n$\\F_q$, where $q = p^n$, with $p$ a prime, and $m$ any positive integer. In the\nparticular case $m=2$, the cost of the new algorithm is an expected\n$O(\\M(n)\\log (p) + \\CC(n)\\log(n))$ operations in $\\F_p$, where $\\M(n)$ and\n$\\CC(n)$ are bounds for the cost of polynomial multiplication and modular\npolynomial composition. Known results give $\\M(n) = O(n\\log (n) \\log\\log (n))$\nand $\\CC(n) = O(n^{1.67})$, so our algorithm is subquadratic in $n$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4428v3", 
    "title": "Improved Upper Bounds for Pairing Heaps", 
    "arxiv-id": "1110.4428v3", 
    "author": "John Iacono", 
    "publish": "2011-10-20T02:43:29Z", 
    "summary": "Pairing heaps are shown to have constant amortized time Insert and Meld, thus\nshowing that pairing heaps have the same amortized runtimes as Fibonacci heaps\nfor all operations but Decrease-key."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4493v1", 
    "title": "Improved Grammar-Based Compressed Indexes", 
    "arxiv-id": "1110.4493v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-10-20T11:03:07Z", 
    "summary": "We introduce the first grammar-compressed representation of a sequence that\nsupports searches in time that depends only logarithmically on the size of the\ngrammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar\nof $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of\nthe lengths of the right hands of the rules), a basic grammar-based\nrepresentation of $T$ takes $N\\lg n$ bits of space. Our representation requires\n$2N\\lg n + N\\lg u + \\epsilon\\, n\\lg n + o(N\\lg n)$ bits of space, for any\n$0<\\epsilon \\le 1$. It can find the positions of the $occ$ occurrences of a\npattern of length $m$ in $T$ in $O((m^2/\\epsilon)\\lg (\\frac{\\lg u}{\\lg n})\n+occ\\lg n)$ time, and extract any substring of length $\\ell$ of $T$ in time\n$O(\\ell+h\\lg(N/h))$, where $h$ is the height of the grammar tree."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4604v2", 
    "title": "Improving Christofides' Algorithm for the s-t Path TSP", 
    "arxiv-id": "1110.4604v2", 
    "author": "David B. Shmoys", 
    "publish": "2011-10-20T18:50:21Z", 
    "summary": "We present a deterministic (1+sqrt(5))/2-approximation algorithm for the s-t\npath TSP for an arbitrary metric. Given a symmetric metric cost on n vertices\nincluding two prespecified endpoints, the problem is to find a shortest\nHamiltonian path between the two endpoints; Hoogeveen showed that the natural\nvariant of Christofides' algorithm is a 5/3-approximation algorithm for this\nproblem, and this asymptotically tight bound in fact has been the best\napproximation ratio known until now. We modify this algorithm so that it\nchooses the initial spanning tree based on an optimal solution to the Held-Karp\nrelaxation rather than a minimum spanning tree; we prove this simple but\ncrucial modification leads to an improved approximation ratio, surpassing the\n20-year-old barrier set by the natural Christofides' algorithm variant. Our\nalgorithm also proves an upper bound of (1+sqrt(5))/2 on the integrality gap of\nthe path-variant Held-Karp relaxation. The techniques devised in this paper can\nbe applied to other optimization problems as well: these applications include\nimproved approximation algorithms and improved LP integrality gap upper bounds\nfor the prize-collecting s-t path problem and the unit-weight graphical metric\ns-t path TSP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4860v2", 
    "title": "Symmetry and approximability of submodular maximization problems", 
    "arxiv-id": "1110.4860v2", 
    "author": "Jan Vondrak", 
    "publish": "2011-10-21T18:31:38Z", 
    "summary": "A number of recent results on optimization problems involving submodular\nfunctions have made use of the multilinear relaxation of the problem. These\nresults hold typically in the value oracle model, where the objective function\nis accessible via a black box returning f(S) for a given S. We present a\ngeneral approach to deriving inapproximability results in the value oracle\nmodel, based on the notion of symmetry gap. Our main result is that for any\nfixed instance that exhibits a certain symmetry gap in its multilinear\nrelaxation, there is a naturally related class of instances for which a better\napproximation factor than the symmetry gap would require exponentially many\noracle queries. This unifies several known hardness results for submodular\nmaximization, and implies several new ones. In particular, we prove that there\nis no constant-factor approximation for the problem of maximizing a\nnon-negative submodular function over the bases of a matroid. We also provide a\nclosely matching approximation algorithm for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.5236v2", 
    "title": "String Indexing for Patterns with Wildcards", 
    "arxiv-id": "1110.5236v2", 
    "author": "S\u00f8ren Vind", 
    "publish": "2011-10-24T13:57:08Z", 
    "summary": "We consider the problem of indexing a string $t$ of length $n$ to report the\noccurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.\nLet $occ$ be the number of occurrences of $p$ in $t$, and $\\sigma$ the size of\nthe alphabet. We obtain the following results.\n  - A linear space index with query time $O(m+\\sigma^j \\log \\log n + occ)$.\nThis significantly improves the previously best known linear space index by Lam\net al. [ISAAC 2007], which requires query time $\\Theta(jn)$ in the worst case.\n  - An index with query time $O(m+j+occ)$ using space $O(\\sigma^{k^2} n \\log^k\n\\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.\nThis is the first non-trivial bound with this query time.\n  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].\n  We also show that these indexes can be generalized to allow variable length\ngaps in the pattern. Our results are obtained using a novel combination of\nwell-known and new techniques, which could be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.6600v2", 
    "title": "The generalized work function algorithm is competitive for the   generalized 2-server problem", 
    "arxiv-id": "1110.6600v2", 
    "author": "Rene Sitters", 
    "publish": "2011-10-30T11:04:36Z", 
    "summary": "The generalized 2-server problem is an online optimization problem where a\nsequence of requests has to be served at minimal cost. Requests arrive one by\none and need to be served instantly by at least one of two servers. We consider\nthe general model where the cost function of the two servers may be different.\nFormally, each server moves in its own metric space and a request consists of\none point in each metric space. It is served by moving one of the two servers\nto its request point. Requests have to be served without knowledge of the\nfuture requests. The objective is to minimize the total traveled distance. The\nspecial case where both servers move on the real line is known as the\nCNN-problem. We show that the generalized work function algorithm is constant\ncompetitive for the generalized 2-server problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1111.0376v2", 
    "title": "Outlier Detection for DNA Fragment Assembly", 
    "arxiv-id": "1111.0376v2", 
    "author": "Daniel Lokshtanov", 
    "publish": "2011-11-02T03:13:55Z", 
    "summary": "Given $n$ length-$\\ell$ strings $S =\\{s_1, ..., s_n\\}$ over a constant size\nalphabet $\\Sigma$ together with parameters $d$ and $k$, the objective in the\n{\\em Consensus String with Outliers} problem is to find a subset $S^*$ of $S$\nof size $n-k$ and a string $s$ such that $\\sum_{s_i \\in S^*} d(s_i, s) \\leq d$.\nHere $d(x, y)$ denotes the Hamming distance between the two strings $x$ and\n$y$. We prove\n  1. a variant of {\\em Consensus String with Outliers} where the number of\noutliers $k$ is fixed and the objective is to minimize the total distance\n$\\sum_{s_i \\in S^*} d(s_i, s)$ admits a simple PTAS. (ii) Under the natural\nassumption that the number of outliers $k$ is small, the PTAS for the distance\nminimization version of {\\em Consensus String with Outliers} performs well. In\nparticular, as long as $k\\leq cn$ for a fixed constant $c < 1$, the algorithm\nprovides a $(1+\\epsilon)$-approximate solution in time\n$f(1/\\epsilon)(n\\ell)^{O(1)}$ and thus, is an EPTAS.\n  2. In order to improve the PTAS for {\\em Consensus String with Outliers} to\nan EPTAS, the assumption that $k$ is small is necessary. Specifically, when $k$\nis allowed to be arbitrary the {\\em Consensus String with Outliers} problem\ndoes not admit an EPTAS unless FPT=W[1]. This hardness result holds even for\nbinary alphabets.\n  3. The decision version of {\\em Consensus String with Outliers} is fixed\nparameter tractable when parameterized by $\\frac{d}{n-k}$. and thus, also when\nparameterized by just $d$.\n  To the best of our knowledge, {\\em Consensus String with Outliers} is the\nfirst problem that admits a PTAS, and is fixed parameter tractable when\nparameterized by the value of the objective function but does not admit an\nEPTAS under plausible complexity assumptions."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214088", 
    "link": "http://arxiv.org/pdf/1111.0623v1", 
    "title": "Beating Randomized Response on Incoherent Matrices", 
    "arxiv-id": "1111.0623v1", 
    "author": "Aaron Roth", 
    "publish": "2011-11-02T19:49:50Z", 
    "summary": "Computing accurate low rank approximations of large matrices is a fundamental\ndata mining task. In many applications however the matrix contains sensitive\ninformation about individuals. In such case we would like to release a low rank\napproximation that satisfies a strong privacy guarantee such as differential\nprivacy. Unfortunately, to date the best known algorithm for this task that\nsatisfies differential privacy is based on naive input perturbation or\nrandomized response: Each entry of the matrix is perturbed independently by a\nsufficiently large random noise variable, a low rank approximation is then\ncomputed on the resulting matrix.\n  We give (the first) significant improvements in accuracy over randomized\nresponse under the natural and necessary assumption that the matrix has low\ncoherence. Our algorithm is also very efficient and finds a constant rank\napproximation of an m x n matrix in time O(mn). Note that even generating the\nnoise matrix required for randomized response already requires time O(mn)."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214088", 
    "link": "http://arxiv.org/pdf/1111.0762v2", 
    "title": "Multidimensional Balanced Allocation for Multiple Choice & (1 + Beta)   Processes", 
    "arxiv-id": "1111.0762v2", 
    "author": "Souvik Bhattacherjee", 
    "publish": "2011-11-03T09:14:41Z", 
    "summary": "Allocation of balls into bins is a well studied abstraction for load\nbalancing problems.The literature hosts numerous results for sequential(single\ndimensional) allocation case when m balls are thrown into n bins. In this paper\nwe study the symmetric multiple choice process for both unweighted and weighted\nballs as well as for both multidimensional and scalar models.Additionally,we\npresent the results on bounds on gap for (1+beta) choice process with\nmultidimensional balls and bins. We show that for the symmetric d choice\nprocess and with m=O(n), the upper bound on the gap is O(lnln(n)) w.h.p.This\nupper bound on the gap is within D=f factor of the lower bound. This is the\nfirst such tight result.For the general case of m>>n the expected gap is\nbounded by O(lnln(n)).For variable f and non-uniform distribution of the\npopulated dimensions,we obtain the upper bound on the expected gap as\nO(log(n)).\n  Further,for the multiple round parallel balls and bins,we show that the gap\nis also bounded by O(loglog(n)) for m=O(n).The same bound holds for the\nexpected gap when m>>n. Our analysis also has strong implications in the\nsequential scalar case.For the weighted balls and bins and general case m>>n,we\nshow that the upper bound on the expected gap is O(log(n)) which improves upon\nthe best prior bound of n^c.Moreover,we show that for the (1 + beta) choice\nprocess and m=O(n) the upper bound(assuming uniform distribution of f populated\ndimensions over D total dimensions) on the gap is O(log(n)/beta),which is\nwithin D=f factor of the lower bound.For fixed f with non-uniform distribution\nand for random f with Binomial distribution the expected gap remains\nO(log(n)/beta) independent of the total number of balls thrown. This is the\nfirst such tight result for (1 +beta) paradigm with multidimensional balls and\nbins."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214088", 
    "link": "http://arxiv.org/pdf/1111.0773v2", 
    "title": "On the Value of Job Migration in Online Makespan Minimization", 
    "arxiv-id": "1111.0773v2", 
    "author": "Matthias Hellwig", 
    "publish": "2011-11-03T10:27:20Z", 
    "summary": "Makespan minimization on identical parallel machines is a classical\nscheduling problem. We consider the online scenario where a sequence of $n$\njobs has to be scheduled non-preemptively on $m$ machines so as to minimize the\nmaximum completion time of any job. The best competitive ratio that can be\nachieved by deterministic online algorithms is in the range $[1.88,1.9201]$.\nCurrently no randomized online algorithm with a smaller competitiveness is\nknown, for general $m$.\n  In this paper we explore the power of job migration, i.e.\\ an online\nscheduler is allowed to perform a limited number of job reassignments.\nMigration is a common technique used in theory and practice to balance load in\nparallel processing environments. As our main result we settle the performance\nthat can be achieved by deterministic online algorithms. We develop an\nalgorithm that is $\\alpha_m$-competitive, for any $m\\geq 2$, where $\\alpha_m$\nis the solution of a certain equation. For $m=2$, $\\alpha_2 = 4/3$ and\n$\\lim_{m\\rightarrow \\infty} \\alpha_m = W_{-1}(-1/e^2)/(1+ W_{-1}(-1/e^2))\n\\approx 1.4659$. Here $W_{-1}$ is the lower branch of the Lambert $W$ function.\nFor $m\\geq 11$, the algorithm uses at most $7m$ migration operations. For\nsmaller $m$, $8m$ to $10m$ operations may be performed. We complement this\nresult by a matching lower bound: No online algorithm that uses $o(n)$ job\nmigrations can achieve a competitive ratio smaller than $\\alpha_m$. We finally\ntrade performance for migrations. We give a family of algorithms that is\n$c$-competitive, for any $5/3\\leq c \\leq 2$. For $c= 5/3$, the strategy uses at\nmost $4m$ job migrations. For $c=1.75$, at most $2.5m$ migrations are used."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214088", 
    "link": "http://arxiv.org/pdf/1111.0801v3", 
    "title": "Perfectly Balanced Allocation With Estimated Average Using Expected   Constant Retries", 
    "arxiv-id": "1111.0801v3", 
    "author": "Ankur Narang", 
    "publish": "2011-11-03T11:26:28Z", 
    "summary": "Balanced allocation of online balls-into-bins has long been an active area of\nresearch for efficient load balancing and hashing applications.There exists a\nlarge number of results in this domain for different settings, such as parallel\nallocations~\\cite{parallel}, multi-dimensional allocations~\\cite{multi},\nweighted balls~\\cite{weight} etc. For sequential multi-choice allocation, where\n$m$ balls are thrown into $n$ bins with each ball choosing $d$ (constant) bins\nindependently uniformly at random, the maximum load of a bin is $O(\\log \\log n)\n+ m/n$ with high probability~\\cite{heavily_load}. This offers the current best\nknown allocation scheme. However, for $d = \\Theta(\\log n)$, the gap reduces to\n$O(1)$~\\cite{soda08}.A similar constant gap bound has been established for\nparallel allocations with $O(\\log ^*n)$ communication rounds~\\cite{lenzen}.\n  In this paper we propose a novel multi-choice allocation algorithm,\n\\emph{Improved D-choice with Estimated Average} ($IDEA$) achieving a constant\ngap with a high probability for the sequential single-dimensional online\nallocation problem with constant $d$. We achieve a maximum load of $\\lceil m/n\n\\rceil$ with high probability for constant $d$ choice scheme with\n\\emph{expected} constant number of retries or rounds per ball. We also show\nthat the bound holds even for an arbitrary large number of balls, $m>>n$.\nFurther, we generalize this result to (i)~the weighted case, where balls have\nweights drawn from an arbitrary weight distribution with finite variance,\n(ii)~multi-dimensional setting, where balls have $D$ dimensions with $f$\nrandomly and uniformly chosen filled dimension for $m=n$, and (iii)~the\nparallel case, where $n$ balls arrive and are placed parallely in the bins. We\nshow that the gap in these case is also a constant w.h.p. (independent of $m$)\nfor constant value of $d$ with expected constant number of retries per ball."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214088", 
    "link": "http://arxiv.org/pdf/1111.0897v2", 
    "title": "Active Property Testing", 
    "arxiv-id": "1111.0897v2", 
    "author": "Liu Yang", 
    "publish": "2011-11-03T16:06:35Z", 
    "summary": "One of the motivations for property testing of boolean functions is the idea\nthat testing can serve as a preprocessing step before learning. However, in\nmost machine learning applications, it is not possible to request for labels of\nfictitious examples constructed by the algorithm. Instead, the dominant query\nparadigm in applied machine learning, called active learning, is one where the\nalgorithm may query for labels, but only on points in a given polynomial-sized\n(unlabeled) sample, drawn from some underlying distribution D. In this work, we\nbring this well-studied model in learning to the domain of testing.\n  We show that for a number of important properties, testing can still yield\nsubstantial benefits in this setting. This includes testing unions of\nintervals, testing linear separators, and testing various assumptions used in\nsemi-supervised learning. In addition to these specific results, we also\ndevelop a general notion of the testing dimension of a given property with\nrespect to a given distribution. We show this dimension characterizes (up to\nconstant factors) the intrinsic number of label requests needed to test that\nproperty. We develop such notions for both the active and passive testing\nmodels. We then use these dimensions to prove a number of lower bounds,\nincluding for linear separators and the class of dictator functions.\n  Our results show that testing can be a powerful tool in realistic models for\nlearning, and further that active testing exhibits an interesting and rich\nstructure. Our work in addition brings together tools from a range of areas\nincluding U-statistics, noise-sensitivity, self-correction, and spectral\nanalysis of random matrices, and develops new tools that may be of independent\ninterest."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.0965v1", 
    "title": "Many Sparse Cuts via Higher Eigenvalues", 
    "arxiv-id": "1111.0965v1", 
    "author": "Santosh Vempala", 
    "publish": "2011-11-03T19:57:26Z", 
    "summary": "Cheeger's fundamental inequality states that any edge-weighted graph has a\nvertex subset $S$ such that its expansion (a.k.a. conductance) is bounded as\nfollows: \\[ \\phi(S) \\defeq \\frac{w(S,\\bar{S})}{\\min \\set{w(S), w(\\bar{S})}}\n\\leq 2\\sqrt{\\lambda_2} \\] where $w$ is the total edge weight of a subset or a\ncut and $\\lambda_2$ is the second smallest eigenvalue of the normalized\nLaplacian of the graph. Here we prove the following natural generalization: for\nany integer $k \\in [n]$, there exist $ck$ disjoint subsets $S_1, ..., S_{ck}$,\nsuch that \\[ \\max_i \\phi(S_i) \\leq C \\sqrt{\\lambda_{k} \\log k} \\] where\n$\\lambda_i$ is the $i^{th}$ smallest eigenvalue of the normalized Laplacian and\n$c<1,C>0$ are suitable absolute constants. Our proof is via a polynomial-time\nalgorithm to find such subsets, consisting of a spectral projection and a\nrandomized rounding. As a consequence, we get the same upper bound for the\nsmall set expansion problem, namely for any $k$, there is a subset $S$ whose\nweight is at most a $\\bigO(1/k)$ fraction of the total weight and $\\phi(S) \\le\nC \\sqrt{\\lambda_k \\log k}$. Both results are the best possible up to constant\nfactors.\n  The underlying algorithmic problem, namely finding $k$ subsets such that the\nmaximum expansion is minimized, besides extending sparse cuts to more than one\nsubset, appears to be a natural clustering problem in its own right."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.1355v1", 
    "title": "A Compressed Self-Index for Genomic Databases", 
    "arxiv-id": "1111.1355v1", 
    "author": "Simon J. Puglisi", 
    "publish": "2011-11-05T21:53:33Z", 
    "summary": "Advances in DNA sequencing technology will soon result in databases of\nthousands of genomes. Within a species, individuals' genomes are almost exact\ncopies of each other; e.g., any two human genomes are 99.9% the same. Relative\nLempel-Ziv (RLZ) compression takes advantage of this property: it stores the\nfirst genome uncompressed or as an FM-index, then compresses the other genomes\nwith a variant of LZ77 that copies phrases only from the first genome. RLZ\nachieves good compression and supports fast random access; in this paper we\nshow how to support fast search as well, thus obtaining an efficient compressed\nself-index."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.1546v2", 
    "title": "Improved Smoothed Analysis of Multiobjective Optimization", 
    "arxiv-id": "1111.1546v2", 
    "author": "Heiko R\u00f6glin", 
    "publish": "2011-11-07T11:28:33Z", 
    "summary": "We present several new results about smoothed analysis of multiobjective\noptimization problems. Motivated by the discrepancy between worst-case analysis\nand practical experience, this line of research has gained a lot of attention\nin the last decade. We consider problems in which d linear and one arbitrary\nobjective function are to be optimized over a subset S of {0,1}^n of feasible\nsolutions. We improve the previously best known bound for the smoothed number\nof Pareto-optimal solutions to O(n^{2d} phi^d), where phi denotes the\nperturbation parameter. Additionally, we show that for any constant c the c-th\nmoment of the smoothed number of Pareto-optimal solutions is bounded by\nO((n^{2d} phi^d)^c). This improves the previously best known bounds\nsignificantly. Furthermore, we address the criticism that the perturbations in\nsmoothed analysis destroy the zero-structure of problems by showing that the\nsmoothed number of Pareto-optimal solutions remains polynomially bounded even\nfor zero-preserving perturbations. This broadens the class of problems captured\nby smoothed analysis and it has consequences for non-linear objective\nfunctions. One corollary of our result is that the smoothed number of\nPareto-optimal solutions is polynomially bounded for polynomial objective\nfunctions."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.1665v1", 
    "title": "De-amortizing Binary Search Trees", 
    "arxiv-id": "1111.1665v1", 
    "author": "Stefan Langerman", 
    "publish": "2011-11-07T18:16:59Z", 
    "summary": "We present a general method for de-amortizing essentially any Binary Search\nTree (BST) algorithm. In particular, by transforming Splay Trees, our method\nproduces a BST that has the same asymptotic cost as Splay Trees on any access\nsequence while performing each search in O(log n) worst case time. By\ntransforming Multi-Splay Trees, we obtain a BST that is O(log log n)\ncompetitive, satisfies the scanning theorem, the static optimality theorem, the\nstatic finger theorem, the working set theorem, and performs each search in\nO(log n) worst case time. Moreover, we prove that if there is a dynamically\noptimal BST algorithm, then there is a dynamically optimal BST algorithm that\nanswers every search in O(log n) worst case time."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.1672v3", 
    "title": "A Systematic Approach to Bound Factor-Revealing LPs and its Application   to the Metric and Squared Metric Facility Location Problems", 
    "arxiv-id": "1111.1672v3", 
    "author": "Lehilton L. C. Pedrosa", 
    "publish": "2011-11-07T18:37:12Z", 
    "summary": "A systematic technique to bound factor-revealing linear programs is\npresented. We show how to derive a family of upper bound factor-revealing\nprograms (UPFRP), and show that each such program can be solved by a computer\nto bound the approximation factor of an associated algorithm. Obtaining an\nUPFRP is straightforward, and can be used as an alternative to analytical\nproofs, that are usually very long and tedious. We apply this technique to the\nMetric Facility Location Problem (MFLP) and to a generalization where the\ndistance function is a squared metric. We call this generalization the Squared\nMetric Facility Location Problem (SMFLP) and prove that there is no\napproximation factor better than 2.04, assuming P $\\neq$ NP. Then, we analyze\nthe best known algorithms for the MFLP based on primal-dual and LP-rounding\ntechniques when they are applied to the SMFLP. We prove very tight bounds for\nthese algorithms, and show that the LP-rounding algorithm achieves a ratio of\n2.04, and therefore has the best factor for the SMFLP. We use UPFRPs in the\ndual-fitting analysis of the primal-dual algorithms for both the SMFLP and the\nMFLP, improving some of the previous analysis for the MFLP."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.1713v1", 
    "title": "Tight Approximation of Image Matching", 
    "arxiv-id": "1111.1713v1", 
    "author": "Gilad Tsur", 
    "publish": "2011-11-07T18:37:42Z", 
    "summary": "In this work we consider the {\\em image matching} problem for two grayscale\n$n \\times n$ images, $M_1$ and $M_2$ (where pixel values range from 0 to 1).\nOur goal is to find an affine transformation $T$ that maps pixels from $M_1$ to\npixels in $M_2$ so that the differences over pixels $p$ between $M_1(p)$ and\n$M_2(T(p))$ is minimized. Our focus here is on sublinear algorithms that give\nan approximate result for this problem, that is, we wish to perform this task\nwhile querying as few pixels from both images as possible, and give a\ntransformation that comes close to minimizing the difference.\n  We give an algorithm for the image matching problem that returns a\ntransformation $T$ which minimizes the sum of differences (normalized by $n^2$)\nup to an additive error of $\\epsilon$ and performs $\\tilde{O}(n/\\epsilon^2)$\nqueries. We give a corresponding lower bound of $\\Omega(n)$ queries showing\nthat this is the best possible result in the general case (with respect to $n$\nand up to low order terms).\n  In addition, we give a significantly better algorithm for a natural family of\nimages, namely, smooth images. We consider an image smooth when the total\ndifference between neighboring pixels is O(n). For such images we provide an\napproximation of the distance between the images to within an additive error of\n$\\epsilon$ using a number of queries depending polynomially on $1/\\epsilon$ and\nnot on $n$. To do this we first consider the image matching problem for 2 and\n3-dimensional {\\em binary} images, and then reduce the grayscale image matching\nproblem to the 3-dimensional binary case."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2213977.2214079", 
    "link": "http://arxiv.org/pdf/1111.2195v2", 
    "title": "Representative sets and irrelevant vertices: New tools for kernelization", 
    "arxiv-id": "1111.2195v2", 
    "author": "Magnus Wahlstr\u00f6m", 
    "publish": "2011-11-09T12:55:42Z", 
    "summary": "The existence of a polynomial kernel for Odd Cycle Transversal was a\nnotorious open problem in parameterized complexity. Recently, this was settled\nby the present authors (Kratsch and Wahlstr\\\"om, SODA 2012), with a randomized\npolynomial kernel for the problem, using matroid theory to encode flow\nquestions over a set of terminals in size polynomial in the number of\nterminals.\n  In the current work we further establish the usefulness of matroid theory to\nkernelization by showing applications of a result on representative sets due to\nLov\\'asz (Combinatorial Surveys 1977) and Marx (TCS 2009). We show how\nrepresentative sets can be used to give a polynomial kernel for the elusive\nAlmost 2-SAT problem. We further apply the representative sets tool to the\nproblem of finding irrelevant vertices in graph cut problems, i.e., vertices\nwhich can be made undeletable without affecting the status of the problem. This\ngives the first significant progress towards a polynomial kernel for the\nMultiway Cut problem; in particular, we get a kernel of O(k^{s+1}) vertices for\nMultiway Cut instances with at most s terminals. Both these kernelization\nresults have significant spin-off effects, producing the first polynomial\nkernels for a range of related problems.\n  More generally, the irrelevant vertex results have implications for covering\nmin-cuts in graphs. For a directed graph G=(V,E) and sets S, T \\subseteq V, let\nr be the size of a minimum (S,T)-vertex cut (which may intersect S and T). We\ncan find a set Z \\subseteq V of size O(|S|*|T|*r) which contains a minimum\n(A,B)-vertex cut for every A \\subseteq S, B \\subseteq T. Similarly, for an\nundirected graph G=(V,E), a set of terminals X \\subseteq V, and a constant s,\nwe can find a set Z\\subseteq V of size O(|X|^{s+1}) which contains a minimum\nmultiway cut for any partition of X into at most s pairwise disjoint subsets."
},{
    "category": "cs.DS", 
    "doi": "10.4304/jnw.9.2.239-243", 
    "link": "http://arxiv.org/pdf/1111.2527v1", 
    "title": "Testing the Connectivity of Networks", 
    "arxiv-id": "1111.2527v1", 
    "author": "Taha Sochi", 
    "publish": "2011-11-10T17:28:20Z", 
    "summary": "In this article we discuss general strategies and computer algorithms to test\nthe connectivity of unstructured networks which consist of a number of segments\nconnected through randomly distributed nodes."
},{
    "category": "cs.DS", 
    "doi": "10.4304/jnw.9.2.239-243", 
    "link": "http://arxiv.org/pdf/1111.2621v2", 
    "title": "Optimal Lower and Upper Bounds for Representing Sequences", 
    "arxiv-id": "1111.2621v2", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-11-10T21:51:27Z", 
    "summary": "Sequence representations supporting queries $access$, $select$ and $rank$ are\nat the core of many data structures. There is a considerable gap between the\nvarious upper bounds and the few lower bounds known for such representations,\nand how they relate to the space used. In this article we prove a strong lower\nbound for $rank$, which holds for rather permissive assumptions on the space\nused, and give matching upper bounds that require only a compressed\nrepresentation of the sequence. Within this compressed space, operations\n$access$ and $select$ can be solved in constant or almost-constant time, which\nis optimal for large alphabets. Our new upper bounds dominate all of the\nprevious work in the time/space map."
},{
    "category": "cs.DS", 
    "doi": "10.4304/jnw.9.2.239-243", 
    "link": "http://arxiv.org/pdf/1111.3244v4", 
    "title": "Faster fully compressed pattern matching by recompression", 
    "arxiv-id": "1111.3244v4", 
    "author": "Artur Je\u017c", 
    "publish": "2011-11-14T15:26:56Z", 
    "summary": "In this paper, a fully compressed pattern matching problem is studied. The\ncompression is represented by straight-line programs (SLPs), i.e. a\ncontext-free grammars generating exactly one string; the term fully means that\nboth the pattern and the text are given in the compressed form. The problem is\napproached using a recently developed technique of local recompression: the\nSLPs are refactored, so that substrings of the pattern and text are encoded in\nboth SLPs in the same way. To this end, the SLPs are locally decompressed and\nthen recompressed in a uniform way.\n  This technique yields an O((n+m)log M) algorithm for compressed pattern\nmatching, assuming that M fits in O(1) machine words, where n (m) is the size\nof the compressed representation of the text (pattern, respectively), while M\nis the size of the decompressed pattern. If only m+n fits in O(1) machine\nwords, the running time increases to O((n+m)log M log(n+m)). The previous best\nalgorithm due to Lifshits had O(n^2m) running time."
},{
    "category": "cs.DS", 
    "doi": "10.4304/jnw.9.2.239-243", 
    "link": "http://arxiv.org/pdf/1111.3297v1", 
    "title": "Cache optimized linear sieve", 
    "arxiv-id": "1111.3297v1", 
    "author": "E. Vatai", 
    "publish": "2011-11-14T17:31:29Z", 
    "summary": "Sieving is essential in different number theoretical algorithms. Sieving with\nlarge primes violates locality of memory access, thus degrading performance.\nOur suggestion on how to tackle this problem is to use cyclic data structures\nin combination with in-place bucket-sort. We present our results on the\nimplementation of the sieve of Eratosthenes, using these ideas, which show that\nthis approach is more robust and less affected by slow memory."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.suscom.2012.10.003", 
    "link": "http://arxiv.org/pdf/1111.3398v2", 
    "title": "Speed scaling with power down scheduling for agreeable deadlines", 
    "arxiv-id": "1111.3398v2", 
    "author": "Ioannis Milis", 
    "publish": "2011-11-15T00:28:24Z", 
    "summary": "We consider the problem of scheduling on a single processor a given set of n\njobs. Each job j has a workload w_j and a release time r_j. The processor can\nvary its speed and hibernate to reduce energy consumption. In a schedule\nminimizing overall consumed energy, it might be that some jobs complete\narbitrarily far from their release time. So in order to guarantee some quality\nof service, we would like to impose a deadline d_j=r_j+F for every job j, where\nF is a guarantee on the *flow time*. We provide an O(n^3) algorithm for the\nmore general case of *agreeable deadlines*, where jobs have release times and\ndeadlines and can be ordered such that for every i<j, both r_i<=r_j and\nd_i<=d_j."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.3548v1", 
    "title": "Fully dynamic recognition of proper circular-arc graphs", 
    "arxiv-id": "1111.3548v1", 
    "author": "Francisco J. Soulignac", 
    "publish": "2011-11-15T15:06:54Z", 
    "summary": "We present a fully dynamic algorithm for the recognition of proper\ncircular-arc (PCA) graphs. The allowed operations on the graph involve the\ninsertion and removal of vertices (together with its incident edges) or edges.\nEdge operations cost O(log n) time, where n is the number of vertices of the\ngraph, while vertex operations cost O(log n + d) time, where d is the degree of\nthe modified vertex. We also show incremental and decremental algorithms that\nwork in O(1) time per inserted or removed edge. As part of our algorithm, fully\ndynamic connectivity and co-connectivity algorithms that work in O(log n) time\nper operation are obtained. Also, an O(\\Delta) time algorithm for determining\nif a PCA representation corresponds to a co-bipartite graph is provided, where\n\\Delta\\ is the maximum among the degrees of the vertices. When the graph is\nco-bipartite, a co-bipartition of each of its co-components is obtained within\nthe same amount of time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.3663v1", 
    "title": "The debts' clearing problem: a new approach", 
    "arxiv-id": "1111.3663v1", 
    "author": "C. P\u0103tca\u015f", 
    "publish": "2011-11-15T21:35:48Z", 
    "summary": "The debts' clearing problem is about clearing all the debts in a group of $n$\nentities (e.g. persons, companies) using a minimal number of money transaction\noperations. In our previous works we studied the problem, gave a dynamic\nprogramming solution solving it and proved that it is NP-hard. In this paper we\nadapt the problem to dynamic graphs and give a data structure to solve it.\nBased on this data structure we develop a new algorithm, that improves our\nprevious one for the static version of the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.3668v1", 
    "title": "Modular exponentiation of matrices on FPGA-s", 
    "arxiv-id": "1111.3668v1", 
    "author": "R. Major", 
    "publish": "2011-11-15T21:47:03Z", 
    "summary": "We describe an efficient FPGA implementation for the exponentiation of large\nmatrices. The research is related to an algorithm for constructing uniformly\ndistributed linear recurring sequences. The design utilizes the special\nproperties of both the FPGA and the used matrices to achieve a very significant\nspeedup compared to traditional architectures."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.3670v1", 
    "title": "Large primes in generalized Pascal triangles", 
    "arxiv-id": "1111.3670v1", 
    "author": "G. Kiss", 
    "publish": "2011-11-15T21:54:54Z", 
    "summary": "In this paper, after presenting the results of the generalization of Pascal\ntriangle (using powers of base numbers), we examine some properties of the\n112-based triangle, most of all regarding to prime numbers. Additionally, an\neffective implementation of ECPP method is presented which enables Magma\ncomputer algebra system to prove the primality of numbers with more than 1000\ndecimal digits."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.4299v2", 
    "title": "The Feedback Arc Set Problem with Triangle Inequality is a Vertex Cover   Problem", 
    "arxiv-id": "1111.4299v2", 
    "author": "Monaldo Mastrolilli", 
    "publish": "2011-11-18T08:32:45Z", 
    "summary": "We consider the (precedence constrained) Minimum Feedback Arc Set problem\nwith triangle inequalities on the weights, which finds important applications\nin problems of ranking with inconsistent information. We present a surprising\nstructural insight showing that the problem is a special case of the minimum\nvertex cover in hypergraphs with edges of size at most 3. This result leads to\ncombinatorial approximation algorithms for the problem and opens the road to\nstudying the problem as a vertex cover problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.4395v1", 
    "title": "Practical Top-K Document Retrieval in Reduced Space", 
    "arxiv-id": "1111.4395v1", 
    "author": "Daniel Valenzuela", 
    "publish": "2011-11-18T15:30:49Z", 
    "summary": "Supporting top-k document retrieval queries on general text databases, that\nis, finding the k documents where a given pattern occurs most frequently, has\nbecome a topic of interest with practical applications. While the problem has\nbeen solved in optimal time and linear space, the actual space usage is a\nserious concern. In this paper we study various reduced-space structures that\nsupport top-k retrieval and propose new alternatives. Our experimental results\nshow that our novel algorithms and data structures dominate almost all the\nspace/time tradeoff."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.4766v5", 
    "title": "On Strong Graph Partitions and Universal Steiner Trees", 
    "arxiv-id": "1111.4766v5", 
    "author": "Srivathsan Srinivasagopalan", 
    "publish": "2011-11-21T06:03:45Z", 
    "summary": "We study the problem of constructing universal Steiner trees for undirected\ngraphs. Given a graph $G$ and a root node $r$, we seek a single spanning tree\n$T$ of minimum {\\em stretch}, where the stretch of $T$ is defined to be the\nmaximum ratio, over all terminal sets $X$, of the cost of the minimal sub-tree\n$T_X$ of $T$ that connects $X$ to $r$ to the cost of an optimal Steiner tree\nconnecting $X$ to $r$ in $G$. Universal Steiner trees (USTs) are important for\ndata aggregation problems where computing the Steiner tree from scratch for\nevery input instance of terminals is costly, as for example in low energy\nsensor network applications.\n  We provide a polynomial time \\ust\\ construction for general graphs with\n$2^{O(\\sqrt{\\log n})}$-stretch. We also give a polynomial time\n$\\polylog(n)$-stretch construction for minor-free graphs. One basic building\nblock of our algorithms is a hierarchy of graph partitions, each of which\nguarantees small strong diameter for each cluster and bounded neighbourhood\nintersections for each node. We show close connections between the problems of\nconstructing USTs and building such graph partitions. Our construction of\npartition hierarchies for general graphs is based on an iterative cluster\nmerging procedure, while the one for minor-free graphs is based on a separator\ntheorem for such graphs and the solution to a cluster aggregation problem that\nmay be of independent interest even for general graphs. To our knowledge, this\nis the first subpolynomial-stretch ($o(n^\\epsilon)$ for any $\\epsilon > 0$) UST\nconstruction for general graphs, and the first polylogarithmic-stretch UST\nconstruction for minor-free graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.5220v2", 
    "title": "Fast Compressed Tries through Path Decompositions", 
    "arxiv-id": "1111.5220v2", 
    "author": "Giuseppe Ottaviano", 
    "publish": "2011-11-22T15:28:03Z", 
    "summary": "Tries are popular data structures for storing a set of strings, where common\nprefixes are represented by common root-to-node paths. Over fifty years of\nusage have produced many variants and implementations to overcome some of their\nlimitations. We explore new succinct representations of path-decomposed tries\nand experimentally evaluate the corresponding reduction in space usage and\nmemory latency, comparing with the state of the art. We study two cases of\napplications: (1) a compressed dictionary for (compressed) strings, and (2) a\nmonotone minimal perfect hash for strings that preserves their lexicographic\norder.\n  For (1), we obtain data structures that outperform other state-of-the-art\ncompressed dictionaries in space efficiency, while obtaining predictable query\ntimes that are competitive with data structures preferred by the practitioners.\nIn (2), our tries perform several times faster than other trie-based monotone\nperfect hash functions, while occupying nearly the same space."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.5386v1", 
    "title": "Edit Distance to Monotonicity in Sliding Windows", 
    "arxiv-id": "1111.5386v1", 
    "author": "Qin Zhang", 
    "publish": "2011-11-23T02:08:09Z", 
    "summary": "Given a stream of items each associated with a numerical value, its edit\ndistance to monotonicity is the minimum number of items to remove so that the\nremaining items are non-decreasing with respect to the numerical value. The\nspace complexity of estimating the edit distance to monotonicity of a data\nstream is becoming well-understood over the past few years. Motivated by\napplications on network quality monitoring, we extend the study to estimating\nthe edit distance to monotonicity of a sliding window covering the $w$ most\nrecent items in the stream for any $w \\ge 1$. We give a deterministic algorithm\nwhich can return an estimate within a factor of $(4+\\eps)$ using\n$O(\\frac{1}{\\eps^2} \\log^2(\\eps w))$ space.\n  We also extend the study in two directions. First, we consider a stream where\neach item is associated with a value from a partial ordered set. We give a\nrandomized $(4+\\epsilon)$-approximate algorithm using $O(\\frac{1}{\\epsilon^2}\n\\log \\epsilon^2 w \\log w)$ space. Second, we consider an out-of-order stream\nwhere each item is associated with a creation time and a numerical value, and\nitems may be out of order with respect to their creation times. The goal is to\nestimate the edit distance to monotonicity with respect to the numerical value\nof items arranged in the order of creation times. We show that any randomized\nconstant-approximate algorithm requires linear space."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.5414v1", 
    "title": "Randomized Speedup of the Bellman-Ford Algorithm", 
    "arxiv-id": "1111.5414v1", 
    "author": "David Eppstein", 
    "publish": "2011-11-23T06:45:33Z", 
    "summary": "We describe a variant of the Bellman-Ford algorithm for single-source\nshortest paths in graphs with negative edges but no negative cycles that\nrandomly permutes the vertices and uses this randomized order to process the\nvertices within each pass of the algorithm. The modification reduces the\nworst-case expected number of relaxation steps of the algorithm, compared to\nthe previously-best variant by Yen (1970), by a factor of 2/3 with high\nprobability. We also use our high probability bound to add negative cycle\ndetection to the randomized algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.5473v2", 
    "title": "Directed Steiner Tree and the Lasserre Hierarchy", 
    "arxiv-id": "1111.5473v2", 
    "author": "Thomas Rothvo\u00df", 
    "publish": "2011-11-23T12:21:33Z", 
    "summary": "The goal for the Directed Steiner Tree problem is to find a minimum cost tree\nin a directed graph G=(V,E) that connects all terminals X to a given root r. It\nis well known that modulo a logarithmic factor it suffices to consider acyclic\ngraphs where the nodes are arranged in L <= log |X| levels. Unfortunately the\nnatural LP formulation has a |X|^(1/2) integrality gap already for 5 levels. We\nshow that for every L, the O(L)-round Lasserre Strengthening of this LP has\nintegrality gap O(L log |X|). This provides a polynomial time\n|X|^{epsilon}-approximation and a O(log^3 |X|) approximation in O(n^{log |X|)\ntime, matching the best known approximation guarantee obtained by a greedy\nalgorithm of Charikar et al."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9835-7", 
    "link": "http://arxiv.org/pdf/1111.6224v1", 
    "title": "Threshold phenomena in k-dominant skylines of random samples", 
    "arxiv-id": "1111.6224v1", 
    "author": "Wei-Mei Chen", 
    "publish": "2011-11-27T04:36:53Z", 
    "summary": "Skylines emerged as a useful notion in database queries for selecting\nrepresentative groups in multivariate data samples for further decision making,\nmulti-objective optimization or data processing, and the $k$-dominant skylines\nwere naturally introduced to resolve the abundance of skylines when the\ndimensionality grows or when the coordinates are negatively correlated. We\nprove in this paper that the expected number of $k$-dominant skylines is\nasymptotically zero for large samples when $1\\le k\\le d-1$ under two reasonable\n(continuous) probability assumptions of the input points, $d$ being the\n(finite) dimensionality, in contrast to the asymptotic unboundedness when\n$k=d$. In addition to such an asymptotic zero-infinity property, we also\nestablish a sharp threshold phenomenon for the expected ($d-1$)-dominant\nskylines when the dimensionality is allowed to grow with $n$. Several related\nissues such as the dominant cycle structures and numerical aspects, are also\nbriefly studied."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1111.6519v1", 
    "title": "A Combinatorial Algorithm for All-Pairs Shortest Paths in Directed   Vertex-Weighted Graphs with Applications to Disc Graphs", 
    "arxiv-id": "1111.6519v1", 
    "author": "Dzmitry Sledneu", 
    "publish": "2011-11-28T17:14:26Z", 
    "summary": "We consider the problem of computing all-pairs shortest paths in a directed\ngraph with real weights assigned to vertices.\n  For an $n\\times n$ 0-1 matrix $C,$ let $K_{C}$ be the complete weighted graph\non the rows of $C$ where the weight of an edge between two rows is equal to\ntheir Hamming distance. Let $MWT(C)$ be the weight of a minimum weight spanning\ntree of $K_{C}.$\n  We show that the all-pairs shortest path problem for a directed graph $G$ on\n$n$ vertices with nonnegative real weights and adjacency matrix $A_G$ can be\nsolved by a combinatorial randomized algorithm in time\n$$\\widetilde{O}(n^{2}\\sqrt {n + \\min\\{MWT(A_G), MWT(A_G^t)\\}})$$\n  As a corollary, we conclude that the transitive closure of a directed graph\n$G$ can be computed by a combinatorial randomized algorithm in the\naforementioned time. $\\widetilde{O}(n^{2}\\sqrt {n + \\min\\{MWT(A_G),\nMWT(A_G^t)\\}})$\n  We also conclude that the all-pairs shortest path problem for uniform disk\ngraphs, with nonnegative real vertex weights, induced by point sets of bounded\ndensity within a unit square can be solved in time $\\widetilde{O}(n^{2.75})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1111.6698v2", 
    "title": "On the Integrality Gap of the Directed-Component Relaxation for Steiner   Tree", 
    "arxiv-id": "1111.6698v2", 
    "author": "Shi Li", 
    "publish": "2011-11-29T05:29:28Z", 
    "summary": "In this note, we show that the integrality gap of the $k$-Directed-Component-\nRelaxation($k$-DCR) LP for the Steiner tree problem, introduced by Byrka,\nGrandoni, Rothvob and Sanita (STOC 2010), is at most $\\ln(4)<1.39$. The proof\nis constructive: we can efficiently find a Steiner tree whose cost is at most\n$\\ln(4)$ times the cost of the optimal fractional $k$-restricted Steiner tree\ngiven by the $k$-DCR LP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1111.7064v2", 
    "title": "Correlation Decay up to Uniqueness in Spin Systems", 
    "arxiv-id": "1111.7064v2", 
    "author": "Yitong Yin", 
    "publish": "2011-11-30T06:56:09Z", 
    "summary": "We give a complete characterization of the two-state anti-ferromagnetic spin\nsystems which are of strong spatial mixing on general graphs. We show that a\ntwo-state anti-ferromagnetic spin system is of strong spatial mixing on all\ngraphs of maximum degree at most \\Delta if and only if the system has a unique\nGibbs measure on infinite regular trees of degree up to \\Delta, where \\Delta\ncan be either bounded or unbounded. As a consequence, there exists an FPTAS for\nthe partition function of a two-state anti-ferromagnetic spin system on graphs\nof maximum degree at most \\Delta when the uniqueness condition is satisfied on\ninfinite regular trees of degree up to \\Delta. In particular, an FPTAS exists\nfor arbitrary graphs if the uniqueness is satisfied on all infinite regular\ntrees. This covers as special cases all previous algorithmic results for\ntwo-state anti-ferromagnetic systems on general-structure graphs.\n  Combining with the FPRAS for two-state ferromagnetic spin systems of\nJerrum-Sinclair and Goldberg-Jerrum-Paterson, and the very recent hardness\nresults of Sly-Sun and independently of Galanis-Stefankovic-Vigoda, this gives\na complete classification, except at the phase transition boundary, of the\napproximability of all two-state spin systems, on either degree-bounded\nfamilies of graphs or family of all graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0184v3", 
    "title": "Maximum Matching in Semi-Streaming with Few Passes", 
    "arxiv-id": "1112.0184v3", 
    "author": "Claire Mathieu", 
    "publish": "2011-12-01T14:13:45Z", 
    "summary": "In the semi-streaming model, an algorithm receives a stream of edges of a\ngraph in arbitrary order and uses a memory of size $O(n \\mbox{ polylog } n)$,\nwhere $n$ is the number of vertices of a graph. In this work, we present\nsemi-streaming algorithms that perform one or two passes over the input stream\nfor maximum matching with no restrictions on the input graph, and for the\nimportant special case of bipartite graphs that we refer to as maximum\nbipartite matching (MBM). The Greedy matching algorithm performs one pass over\nthe input and outputs a $1/2$ approximation. Whether there is a better one-pass\nalgorithm has been an open question since the appearance of the first paper on\nstreaming algorithms for matching problems in 2005 [Feigenbaum et al., SODA\n2005]. We make the following progress on this problem:\n  In the one-pass setting, we show that there is a deterministic semi-streaming\nalgorithm for MBM with expected approximation factor $1/2+0.005$, assuming that\nedges arrive one by one in (uniform) random order. We extend this algorithm to\ngeneral graphs, and we obtain a $1/2+0.003$ approximation.\n  In the two-pass setting, we do not require the random arrival order\nassumption (the edge stream is in arbitrary order). We present a simple\nrandomized two-pass semi-streaming algorithm for MBM with expected\napproximation factor $1/2 + 0.019$. Furthermore, we discuss a more involved\ndeterministic two-pass semi-streaming algorithm for MBM with approximation\nfactor $1/2 + 0.019$ and a generalization of this algorithm to general graphs\nwith approximation factor $1/2 + 0.0071$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0278v2", 
    "title": "Computing on Binary Strings", 
    "arxiv-id": "1112.0278v2", 
    "author": "Peng Zhang", 
    "publish": "2011-12-01T19:24:16Z", 
    "summary": "Many problems in Computer Science can be abstracted to the following\nquestion: given a set of objects and rules respectively, which new objects can\nbe produced? In the paper, we consider a succinct version of the question:\ngiven a set of binary strings and several operations like conjunction and\ndisjunction, which new binary strings can be generated? Although it is a\nfundamental problem, to the best of our knowledge, the problem hasn't been\nstudied yet. In this paper, an O(m^2n) algorithm is presented to determine\nwhether a string s is representable by a set W, where n is the number of\nstrings in W and each string has the same length m. However, looking for the\nminimum subset from a set to represent a given string is shown to be NP-hard.\nAlso, finding the smallest subset from a set to represent each string in the\noriginal set is NP-hard. We establishes inapproximability results and\napproximation algorithms for them. In addition, we prove that counting the\nnumber of strings representable is #P-complete. We then explore how the\nproblems change when the operator negation is available. For example, if the\noperator negation can be used, the number is some power of 2. This difference\nmaybe help us understand the problem more profoundly."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0534v1", 
    "title": "The interval ordering problem", 
    "arxiv-id": "1112.0534v1", 
    "author": "Gerhard J. Woeginger", 
    "publish": "2011-12-02T18:35:06Z", 
    "summary": "For a given set of intervals on the real line, we consider the problem of\nordering the intervals with the goal of minimizing an objective function that\ndepends on the exposed interval pieces (that is, the pieces that are not\ncovered by earlier intervals in the ordering). This problem is motivated by an\napplication in molecular biology that concerns the determination of the\nstructure of the backbone of a protein.\n  We present polynomial-time algorithms for several natural special cases of\nthe problem that cover the situation where the interval boundaries are\nagreeably ordered and the situation where the interval set is laminar. Also the\nbottleneck variant of the problem is shown to be solvable in polynomial time.\nFinally we prove that the general problem is NP-hard, and that the existence of\na constant-factor-approximation algorithm is unlikely."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0784v1", 
    "title": "A New Approach to Incremental Cycle Detection and Related Problems", 
    "arxiv-id": "1112.0784v1", 
    "author": "Robert E. Tarjan", 
    "publish": "2011-12-04T18:12:38Z", 
    "summary": "We consider the problem of detecting a cycle in a directed graph that grows\nby arc insertions, and the related problems of maintaining a topological order\nand the strong components of such a graph. For these problems, we give two\nalgorithms, one suited to sparse graphs, and the other to dense graphs. The\nformer takes the minimum of O(m^{3/2}) and O(mn^{2/3}) time to insert m arcs\ninto an n-vertex graph; the latter takes O(n^2 log(n)) time. Our sparse\nalgorithm is considerably simpler than a previous O(m^{3/2})-time algorithm; it\nis also faster on graphs of sufficient density. The time bound of our dense\nalgorithm beats the previously best time bound of O(n^{5/2}) for dense graphs.\nOur algorithms rely for their efficiency on topologically ordered vertex\nnumberings; bounds on the size of the numbers give bound on running times."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0790v1", 
    "title": "Scaling algorithms for approximate and exact maximum weight matching", 
    "arxiv-id": "1112.0790v1", 
    "author": "Hsin-Hao Su", 
    "publish": "2011-12-04T20:05:24Z", 
    "summary": "The {\\em maximum cardinality} and {\\em maximum weight matching} problems can\nbe solved in time $\\tilde{O}(m\\sqrt{n})$, a bound that has resisted improvement\ndespite decades of research. (Here $m$ and $n$ are the number of edges and\nvertices.) In this article we demonstrate that this \"$m\\sqrt{n}$ barrier\" is\nextremely fragile, in the following sense. For any $\\epsilon>0$, we give an\nalgorithm that computes a $(1-\\epsilon)$-approximate maximum weight matching in\n$O(m\\epsilon^{-1}\\log\\epsilon^{-1})$ time, that is, optimal {\\em linear time}\nfor any fixed $\\epsilon$. Our algorithm is dramatically simpler than the best\nexact maximum weight matching algorithms on general graphs and should be\nappealing in all applications that can tolerate a negligible relative error.\n  Our second contribution is a new {\\em exact} maximum weight matching\nalgorithm for integer-weighted bipartite graphs that runs in time\n$O(m\\sqrt{n}\\log N)$. This improves on the $O(Nm\\sqrt{n})$-time and\n$O(m\\sqrt{n}\\log(nN))$-time algorithms known since the mid 1980s, for $1\\ll\n\\log N \\ll \\log n$. Here $N$ is the maximum integer edge weight."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.0993v1", 
    "title": "Worst-Case Optimal Priority Queues via Extended Regular Counters", 
    "arxiv-id": "1112.0993v1", 
    "author": "Jyrki Katajainen", 
    "publish": "2011-12-05T16:55:27Z", 
    "summary": "We consider the classical problem of representing a collection of priority\nqueues under the operations \\Findmin{}, \\Insert{}, \\Decrease{}, \\Meld{},\n\\Delete{}, and \\Deletemin{}. In the comparison-based model, if the first four\noperations are to be supported in constant time, the last two operations must\ntake at least logarithmic time. Brodal showed that his worst-case efficient\npriority queues achieve these worst-case bounds. Unfortunately, this data\nstructure is involved and the time bounds hide large constants. We describe a\nnew variant of the worst-case efficient priority queues that relies on extended\nregular counters and provides the same asymptotic time and space bounds as the\noriginal. Due to the conceptual separation of the operations on regular\ncounters and all other operations, our data structure is simpler and easier to\ndescribe and understand. Also, the constants in the time and space bounds are\nsmaller. In addition, we give an implementation of our structure on a pointer\nmachine. For our pointer-machine implementation, \\Decrease{} and \\Meld{} are\nasymptotically slower and require $O(\\lg\\lg{n})$ worst-case time, where $n$\ndenotes the number of elements stored in the resulting priority queue."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.1116v6", 
    "title": "Approximating the Diameter of Planar Graphs in Near Linear Time", 
    "arxiv-id": "1112.1116v6", 
    "author": "Raphael Yuster", 
    "publish": "2011-12-05T22:46:04Z", 
    "summary": "We present a $(1+\\epsilon)$-approximation algorithm running in\n$O(f(\\epsilon)\\cdot n \\log^4 n)$ time for finding the diameter of an undirected\nplanar graph with non-negative edge lengths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-27660-6_31", 
    "link": "http://arxiv.org/pdf/1112.1136v1", 
    "title": "Secretary Problems with Convex Costs", 
    "arxiv-id": "1112.1136v1", 
    "author": "David Malec", 
    "publish": "2011-12-06T01:01:44Z", 
    "summary": "We consider online resource allocation problems where given a set of requests\nour goal is to select a subset that maximizes a value minus cost type of\nobjective function. Requests are presented online in random order, and each\nrequest possesses an adversarial value and an adversarial size. The online\nalgorithm must make an irrevocable accept/reject decision as soon as it sees\neach request. The \"profit\" of a set of accepted requests is its total value\nminus a convex cost function of its total size. This problem falls within the\nframework of secretary problems. Unlike previous work in that area, one of the\nmain challenges we face is that the objective function can be positive or\nnegative and we must guard against accepting requests that look good early on\nbut cause the solution to have an arbitrarily large cost as more requests are\naccepted. This requires designing new techniques.\n  We study this problem under various feasibility constraints and present\nonline algorithms with competitive ratios only a constant factor worse than\nthose known in the absence of costs for the same feasibility constraints. We\nalso consider a multi-dimensional version of the problem that generalizes\nmulti-dimensional knapsack within a secretary framework. In the absence of any\nfeasibility constraints, we present an O(l) competitive algorithm where l is\nthe number of dimensions; this matches within constant factors the best known\nratio for multi-dimensional knapsack secretary."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.1444v2", 
    "title": "On the complexity of strongly connected components in directed   hypergraphs", 
    "arxiv-id": "1112.1444v2", 
    "author": "Xavier Allamigeon", 
    "publish": "2011-12-06T23:26:05Z", 
    "summary": "We study the complexity of some algorithmic problems on directed hypergraphs\nand their strongly connected components (SCCs). The main contribution is an\nalmost linear time algorithm computing the terminal strongly connected\ncomponents (i.e. SCCs which do not reach any components but themselves).\n\"Almost linear\" here means that the complexity of the algorithm is linear in\nthe size of the hypergraph up to a factor alpha(n), where alpha is the inverse\nof Ackermann function, and n is the number of vertices. Our motivation to study\nthis problem arises from a recent application of directed hypergraphs to\ncomputational tropical geometry.\n  We also discuss the problem of computing all SCCs. We establish a superlinear\nlower bound on the size of the transitive reduction of the reachability\nrelation in directed hypergraphs, showing that it is combinatorially more\ncomplex than in directed graphs. Besides, we prove a linear time reduction from\nthe well-studied problem of finding all minimal sets among a given family to\nthe problem of computing the SCCs. Only subquadratic time algorithms are known\nfor the former problem. These results strongly suggest that the problem of\ncomputing the SCCs is harder in directed hypergraphs than in directed graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.1945v2", 
    "title": "Approximation Algorithms for Edge Partitioned Vertex Cover Problems", 
    "arxiv-id": "1112.1945v2", 
    "author": "Sambuddha Roy", 
    "publish": "2011-12-08T20:56:07Z", 
    "summary": "We consider a natural generalization of the Partial Vertex Cover problem.\nHere an instance consists of a graph G = (V,E), a positive cost function c: V->\nZ^{+}, a partition $P_1,..., P_r$ of the edge set $E$, and a parameter $k_i$\nfor each partition $P_i$. The goal is to find a minimum cost set of vertices\nwhich cover at least $k_i$ edges from the partition $P_i$. We call this the\nPartition Vertex Cover problem. In this paper, we give matching upper and lower\nbound on the approximability of this problem. Our algorithm is based on a novel\nLP relaxation for this problem. This LP relaxation is obtained by adding\nknapsack cover inequalities to a natural LP relaxation of the problem. We show\nthat this LP has integrality gap of $O(log r)$, where $r$ is the number of sets\nin the partition of the edge set. We also extend our result to more general\nsettings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2143v1", 
    "title": "Experiments on Density-Constrained Graph Clustering", 
    "arxiv-id": "1112.2143v1", 
    "author": "Dorothea Wagner", 
    "publish": "2011-12-09T16:43:59Z", 
    "summary": "Clustering a graph means identifying internally dense subgraphs which are\nonly sparsely interconnected. Formalizations of this notion lead to measures\nthat quantify the quality of a clustering and to algorithms that actually find\nclusterings. Since, most generally, corresponding optimization problems are\nhard, heuristic clustering algorithms are used in practice, or other approaches\nwhich are not based on an objective function. In this work we conduct a\ncomprehensive experimental evaluation of the qualitative behavior of greedy\nbottom-up heuristics driven by cut-based objectives and constrained by\nintracluster density, using both real-world data and artificial instances. Our\nstudy documents that a greedy strategy based on local movement is superior to\none based on merging. We further reveal that the former approach generally\noutperforms alternative setups and reference algorithms from the literature in\nterms of its own objective, while a modularity-based algorithm competes\nsurprisingly well. Finally, we exhibit which combinations of cut-based inter-\nand intracluster measures are suitable for identifying a hidden reference\nclustering in synthetic random graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2273v2", 
    "title": "Steiner Forest Orientation Problems", 
    "arxiv-id": "1112.2273v2", 
    "author": "Zeev Nutov", 
    "publish": "2011-12-10T12:24:13Z", 
    "summary": "We consider connectivity problems with orientation constraints. Given a\ndirected graph $D$ and a collection of ordered node pairs $P$ let $P[D]=\\{(u,v)\n\\in P: D {contains a} uv{-path}}$. In the {\\sf Steiner Forest Orientation}\nproblem we are given an undirected graph $G=(V,E)$ with edge-costs and a set $P\n\\subseteq V \\times V$ of ordered node pairs. The goal is to find a minimum-cost\nsubgraph $H$ of $G$ and an orientation $D$ of $H$ such that $P[D]=P$. We give a\n4-approximation algorithm for this problem.\n  In the {\\sf Maximum Pairs Orientation} problem we are given a graph $G$ and a\nmulti-collection of ordered node pairs $P$ on $V$. The goal is to find an\norientation $D$ of $G$ such that $|P[D]|$ is maximum. Generalizing the result\nof Arkin and Hassin [DAM'02] for $|P|=2$, we will show that for a mixed graph\n$G$ (that may have both directed and undirected edges), one can decide in\n$n^{O(|P|)}$ time whether $G$ has an orientation $D$ with $P[D]=P$ (for\nundirected graphs this problem admits a polynomial time algorithm for any $P$,\nbut it is NP-complete on mixed graphs). For undirected graphs, we will show\nthat one can decide whether $G$ admits an orientation $D$ with $|P[D]| \\geq k$\nin $O(n+m)+2^{O(k\\cdot \\log \\log k)}$ time; hence this decision problem is\nfixed-parameter tractable, which answers an open question from Dorn et al.\n[AMB'11]. We also show that {\\sf Maximum Pairs Orientation} admits ratio\n$O(\\log |P|/\\log\\log |P|)$, which is better than the ratio $O(\\log n/\\log\\log\nn)$ of Gamzu et al. [WABI'10] when $|P|<n$.\n  Finally, we show that the following node-connectivity problem can be solved\nin polynomial time: given a graph $G=(V,E)$ with edge-costs, $s,t \\in V$, and\nan integer $\\ell$, find a min-cost subgraph $H$ of $G$ with an orientation $D$\nsuch that $D$ contains $\\ell$ internally-disjoint $st$-paths, and $\\ell$\ninternally-disjoint $ts$-paths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2930v2", 
    "title": "Multiple Traveling Salesmen in Asymmetric Metrics", 
    "arxiv-id": "1112.2930v2", 
    "author": "Zachary Friggstad", 
    "publish": "2011-12-13T15:59:58Z", 
    "summary": "We consider some generalizations of the Asymmetric Traveling Salesman Path\nproblem. Suppose we have an asymmetric metric G = (V,A) with two distinguished\nnodes s,t. We are also given a positive integer k. The goal is to find k paths\nof minimum total cost from s to t whose union spans all nodes. We call this the\nk-Person Asymmetric Traveling Salesmen Path problem (k-ATSPP). Our main result\nfor k-ATSPP is a bicriteria approximation that, for some parameter b >= 1 we\nmay choose, finds between k and k + k/b paths of total length O(b log |V|)\ntimes the optimum value of an LP relaxation based on the Held-Karp relaxation\nfor the Traveling Salesman problem. On one extreme this is an O(log\n|V|)-approximation that uses up to 2k paths and on the other it is an O(k log\n|V|)-approximation that uses exactly k paths.\n  Next, we consider the case where we have k pairs of nodes (s_1,t_1), ...,\n(s_k,t_k). The goal is to find an s_i-t_i path for every pair such that each\nnode of G lies on at least one of these paths. Simple approximation algorithms\nare presented for the special cases where the metric is symmetric or where s_i\n= t_i for each i. We also show that the problem can be approximated within a\nfactor O(log n) when k=2. On the other hand, we demonstrate that the general\nproblem cannot be approximated within any bounded ratio unless P = NP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.3323v1", 
    "title": "Independence of Tabulation-Based Hash Classes", 
    "arxiv-id": "1112.3323v1", 
    "author": "Philipp Woelfel", 
    "publish": "2011-12-14T20:17:50Z", 
    "summary": "A tabulation-based hash function maps a key into d derived characters\nindexing random values in tables that are then combined with bitwise xor\noperations to give the hash. Thorup and Zhang (2004) presented d-wise\nindependent tabulation-based hash classes that use linear maps over finite\nfields to map a key, considered as a vector (a,b), to derived characters. We\nshow that a variant where the derived characters are a+b*i for i=0,..., q-1\n(using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is\nbased on an algebraic property that characterizes k-wise independence of\ntabulation-based hashing schemes, and combines this characterization with a\ngeometric argument. We also prove a non-trivial lower bound on the number of\nderived characters necessary for k-wise independence with our and related hash\nclasses."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.4109v4", 
    "title": "Approximating Non-Uniform Sparsest Cut via Generalized Spectra", 
    "arxiv-id": "1112.4109v4", 
    "author": "Ali Kemal Sinop", 
    "publish": "2011-12-18T02:41:12Z", 
    "summary": "We give an approximation algorithm for non-uniform sparsest cut with the\nfollowing guarantee: For any $\\epsilon,\\delta \\in (0,1)$, given cost and demand\ngraphs with edge weights $C, D$ respectively, we can find a set $T\\subseteq V$\nwith $\\frac{C(T,V\\setminus T)}{D(T,V\\setminus T)}$ at most\n$\\frac{1+\\epsilon}{\\delta}$ times the optimal non-uniform sparsest cut value,\nin time $2^{r/(\\delta\\epsilon)}\\poly(n)$ provided $\\lambda_r \\ge\n\\Phi^*/(1-\\delta)$. Here $\\lambda_r$ is the $r$'th smallest generalized\neigenvalue of the Laplacian matrices of cost and demand graphs; $C(T,V\\setminus\nT)$ (resp. $D(T,V\\setminus T)$) is the weight of edges crossing the\n$(T,V\\setminus T)$ cut in cost (resp. demand) graph and $\\Phi^*$ is the\nsparsity of the optimal cut. In words, we show that the non-uniform sparsest\ncut problem is easy when the generalized spectrum grows moderately fast. To the\nbest of our knowledge, there were no results based on higher order spectra for\nnon-uniform sparsest cut prior to this work.\n  Even for uniform sparsest cut, the quantitative aspects of our result are\nsomewhat stronger than previous methods. Similar results hold for other\nexpansion measures like edge expansion, normalized cut, and conductance, with\nthe $r$'th smallest eigenvalue of the normalized Laplacian playing the role of\n$\\lambda_r$ in the latter two cases.\n  Our proof is based on an l1-embedding of vectors from a semi-definite program\nfrom the Lasserre hierarchy. The embedded vectors are then rounded to a cut\nusing standard threshold rounding. We hope that the ideas connecting\n$\\ell_1$-embeddings to Lasserre SDPs will find other applications. Another\naspect of the analysis is the adaptation of the column selection paradigm from\nour earlier work on rounding Lasserre SDPs [GS11] to pick a set of edges rather\nthan vertices. This feature is important in order to extend the algorithms to\nnon-uniform sparsest cut."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.4578v1", 
    "title": "Self-Index based on LZ77 (thesis)", 
    "arxiv-id": "1112.4578v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-12-20T06:09:35Z", 
    "summary": "Domains like bioinformatics, version control systems, collaborative editing\nsystems (wiki), and others, are producing huge data collections that are very\nrepetitive. That is, there are few differences between the elements of the\ncollection. This fact makes the compressibility of the collection extremely\nhigh. For example, a collection with all different versions of a Wikipedia\narticle can be compressed up to the 0.1% of its original space, using the\nLempel-Ziv 1977 (LZ77) compression scheme.\n  Many of these repetitive collections handle huge amounts of text data. For\nthat reason, we require a method to store them efficiently, while providing the\nability to operate on them. The most common operations are the extraction of\nrandom portions of the collection and the search for all the occurrences of a\ngiven pattern inside the whole collection.\n  A self-index is a data structure that stores a text in compressed form and\nallows to find the occurrences of a pattern efficiently. On the other hand,\nself-indexes can extract any substring of the collection, hence they are able\nto replace the original text. One of the main goals when using these indexes is\nto store them within main memory.\n  In this thesis we present a scheme for random text extraction from text\ncompressed with a Lempel-Ziv parsing. Additionally, we present a variant of\nLZ77, called LZ-End, that efficiently extracts text using space close to that\nof LZ77.\n  The main contribution of this thesis is the first self-index based on\nLZ77/LZ-End and oriented to repetitive texts, which outperforms the state of\nthe art (the RLCSA self-index) in many aspects. Finally, we present a corpus of\nrepetitive texts, coming from several application domains. We aim at providing\na standard set of texts for research and experimentation, hence this corpus is\npublicly available."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.5153v3", 
    "title": "Tight Bounds for Distributed Functional Monitoring", 
    "arxiv-id": "1112.5153v3", 
    "author": "Qin Zhang", 
    "publish": "2011-12-21T20:56:38Z", 
    "summary": "We resolve several fundamental questions in the area of distributed\nfunctional monitoring, initiated by Cormode, Muthukrishnan, and Yi (SODA,\n2008). In this model there are $k$ sites each tracking their input and\ncommunicating with a central coordinator that continuously maintain an\napproximate output to a function $f$ computed over the union of the inputs. The\ngoal is to minimize the communication.\n  We show the randomized communication complexity of estimating the number of\ndistinct elements up to a $1+\\eps$ factor is $\\tilde{\\Omega}(k/\\eps^2)$,\nimproving the previous $\\Omega(k + 1/\\eps^2)$ bound and matching known upper\nbounds up to a logarithmic factor. For the $p$-th frequency moment $F_p$, $p >\n1$, we improve the previous $\\Omega(k + 1/\\eps^2)$ communication bound to\n$\\tilde{\\Omega}(k^{p-1}/\\eps^2)$. We obtain similar improvements for heavy\nhitters, empirical entropy, and other problems. We also show that we can\nestimate $F_p$, for any $p > 1$, using $\\tilde{O}(k^{p-1}\\poly(\\eps^{-1}))$\ncommunication. This greatly improves upon the previous\n$\\tilde{O}(k^{2p+1}N^{1-2/p} \\poly(\\eps^{-1}))$ bound of Cormode,\nMuthukrishnan, and Yi for general $p$, and their $\\tilde{O}(k^2/\\eps +\nk^{1.5}/\\eps^3)$ bound for $p = 2$. For $p = 2$, our bound resolves their main\nopen question.\n  Our lower bounds are based on new direct sum theorems for approximate\nmajority, and yield significant improvements to problems in the data stream\nmodel, improving the bound for estimating $F_p, p > 2,$ in $t$ passes from\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{2/p} t))$ to\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{4/p} t))$, giving the first bound for\nestimating $F_0$ in $t$ passes of $\\Omega(1/(\\eps^2 t))$ bits of space that\ndoes not use the gap-hamming problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.5396v2", 
    "title": "AdCell: Ad Allocation in Cellular Networks", 
    "arxiv-id": "1112.5396v2", 
    "author": "Barna Saha", 
    "publish": "2011-12-22T17:44:55Z", 
    "summary": "With more than four billion usage of cellular phones worldwide, mobile\nadvertising has become an attractive alternative to online advertisements. In\nthis paper, we propose a new targeted advertising policy for Wireless Service\nProviders (WSPs) via SMS or MMS- namely {\\em AdCell}. In our model, a WSP\ncharges the advertisers for showing their ads. Each advertiser has a valuation\nfor specific types of customers in various times and locations and has a limit\non the maximum available budget. Each query is in the form of time and location\nand is associated with one individual customer. In order to achieve a\nnon-intrusive delivery, only a limited number of ads can be sent to each\ncustomer. Recently, new services have been introduced that offer location-based\nadvertising over cellular network that fit in our model (e.g., ShopAlerts by\nAT&T) .\n  We consider both online and offline version of the AdCell problem and develop\napproximation algorithms with constant competitive ratio. For the online\nversion, we assume that the appearances of the queries follow a stochastic\ndistribution and thus consider a Bayesian setting. Furthermore, queries may\ncome from different distributions on different times. This model generalizes\nseveral previous advertising models such as online secretary problem\n\\cite{HKP04}, online bipartite matching \\cite{KVV90,FMMM09} and AdWords\n\\cite{saberi05}. ..."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.5472v3", 
    "title": "Cache-Oblivious Implicit Predecessor Dictionaries with the Working Set   Property", 
    "arxiv-id": "1112.5472v3", 
    "author": "Casper Kejlberg-Rasmussen", 
    "publish": "2011-12-22T21:45:16Z", 
    "summary": "In this paper we present an implicit dynamic dictionary with the working-set\nproperty, supporting insert(e) and delete(e) in O(log n) time, predecessor(e)\nin O(log l_{p(e)}) time, successor(e) in O(log l_{s(e)}) time and search(e) in\nO(log min(l_{p(e)},l_{e}, l_{s(e)})) time, where n is the number of elements\nstored in the dictionary, l_{e} is the number of distinct elements searched for\nsince element e was last searched for and p(e) and s(e) are the predecessor and\nsuccessor of e, respectively. The time-bounds are all worst-case. The\ndictionary stores the elements in an array of size n using no additional space.\nIn the cache-oblivious model the log is base B and the cache-obliviousness is\ndue to our black box use of an existing cache-oblivious implicit dictionary.\nThis is the first implicit dictionary supporting predecessor and successor\nsearches in the working-set bound. Previous implicit structures required O(log\nn) time."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.5636v1", 
    "title": "Tight lower bounds for online labeling problem", 
    "arxiv-id": "1112.5636v1", 
    "author": "Michael Saks", 
    "publish": "2011-12-23T19:02:01Z", 
    "summary": "We consider the file maintenance problem (also called the online labeling\nproblem) in which n integer items from the set {1,...,r} are to be stored in an\narray of size m >= n. The items are presented sequentially in an arbitrary\norder, and must be stored in the array in sorted order (but not necessarily in\nconsecutive locations in the array). Each new item must be stored in the array\nbefore the next item is received. If r<=m then we can simply store item j in\nlocation j but if r>m then we may have to shift the location of stored items to\nmake space for a newly arrived item. The algorithm is charged each time an item\nis stored in the array, or moved to a new location. The goal is to minimize the\ntotal number of such moves done by the algorithm. This problem is non-trivial\nwhen n=<m<r.\n  In the case that m=Cn for some C>1, algorithms for this problem with cost\nO(log(n)^2) per item have been given [IKR81, Wil92, BCD+02]. When m=n,\nalgorithms with cost O(log(n)^3) per item were given [Zha93, BS07]. In this\npaper we prove lower bounds that show that these algorithms are optimal, up to\nconstant factors. Previously, the only lower bound known for this range of\nparameters was a lower bound of \\Omega(log(n)^2) for the restricted class of\nsmooth algorithms [DSZ05a, Zha93].\n  We also provide an algorithm for the sparse case: If the number of items is\npolylogarithmic in the array size then the problem can be solved in amortized\nconstant time per item."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.6255v1", 
    "title": "On group feedback vertex set parameterized by the size of the cutset", 
    "arxiv-id": "1112.6255v1", 
    "author": "Micha\u0142 Pilipczuk", 
    "publish": "2011-12-29T09:02:35Z", 
    "summary": "We study the parameterized complexity of a robust generalization of the\nclassical Feedback Vertex Set problem, namely the Group Feedback Vertex Set\nproblem; we are given a graph G with edges labeled with group elements, and the\ngoal is to compute the smallest set of vertices that hits all cycles of G that\nevaluate to a non-null element of the group. This problem generalizes not only\nFeedback Vertex Set, but also Subset Feedback Vertex Set, Multiway Cut and Odd\nCycle Transversal. Completing the results of Guillemot [Discr. Opt. 2011], we\nprovide a fixed-parameter algorithm for the parameterization by the size of the\ncutset only. Our algorithm works even if the group is given as a\npolynomial-time oracle."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.6256v2", 
    "title": "(1+epsilon)-Distance Oracle for Planar Labeled Graph", 
    "arxiv-id": "1112.6256v2", 
    "author": "Li Ning", 
    "publish": "2011-12-29T09:23:49Z", 
    "summary": "Given a vertex-labeled graph, each vertex $v$ is attached with a label from a\nset of labels. The vertex-label query desires the length of the shortest path\nfrom the given vertex to the set of vertices with the given label. We show how\nto construct an oracle if the given graph is planar, such that\n$O(\\frac{1}{\\epsilon}n\\log n)$ storing space is needed, and any vertex-label\nquery could be answered in $O(\\frac{1}{\\epsilon}\\log n\\log \\rho)$ time with\nstretch $1+\\epsilon$. $\\rho$ is the radius of the given graph, which is half of\nthe diameter. For the case that $\\rho = O(\\log n)$, we construct an oracle that\nachieves $O(\\log n)$ query time, without changing the order of storing space."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.0749v2", 
    "title": "There is no 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues   Problem", 
    "arxiv-id": "1201.0749v2", 
    "author": "Gilles Civario", 
    "publish": "2012-01-01T19:04:10Z", 
    "summary": "The sudoku minimum number of clues problem is the following question: what is\nthe smallest number of clues that a sudoku puzzle can have? For several years\nit had been conjectured that the answer is 17. We have performed an exhaustive\ncomputer search for 16-clue sudoku puzzles, and did not find any, thus proving\nthat the answer is indeed 17. In this article we describe our method and the\nactual search. As a part of this project we developed a novel way for\nenumerating hitting sets. The hitting set problem is computationally hard; it\nis one of Karp's 21 classic NP-complete problems. A standard backtracking\nalgorithm for finding hitting sets would not be fast enough to search for a\n16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To\nmake an exhaustive search possible, we designed an algorithm that allowed us to\nefficiently enumerate hitting sets of a suitable size."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.1157v1", 
    "title": "Method of the Multidimensional Sieve in the Practical Realization of   some Combinatorial Algorithms", 
    "arxiv-id": "1201.1157v1", 
    "author": "Ana Markovska", 
    "publish": "2012-01-05T12:54:30Z", 
    "summary": "Some difficulties regarding the application of the well-known sieve method\nare considered in the case when a practical (program) realization of selecting\nelements, having a particular property among the elements of a set with a\nsufficiently great cardinal number(cardinality). In this paper the problem has\nbeen resolved by using a modified version of the method, utilizing\nmultidimensional arrays. As a theoretical illustration of the method of the\nmultidimensional sieve, the problem of obtaining a single representative of\neach equivalence class with respect to a given relation of equivalence and\nobtaining the cardinality of the respective factor set is considered with\nrelevant mathematical proofs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.1784v1", 
    "title": "Abstract unordered and ordered trees CRDT", 
    "arxiv-id": "1201.1784v1", 
    "author": "Pascal Urso", 
    "publish": "2012-01-09T14:42:45Z", 
    "summary": "Trees are fundamental data structure for many areas of computer science and\nsystem engineering. In this report, we show how to ensure eventual consistency\nof optimistically replicated trees. In optimistic replication, the different\nreplicas of a distributed system are allowed to diverge but should eventually\nreach the same value if no more mutations occur. A new method to ensure\neventual consistency is to design Conflict-free Replicated Data Types (CRDT).\nIn this report, we design a collection of tree CRDT using existing set CRDTs.\nThe remaining concurrency problems particular to tree data structure are\nresolved using one or two layers of correction algorithm. For each of these\nlayer, we propose different and independent policies. Any combination of set\nCRDT and policies can be constructed, giving to the distributed application\nprogrammer the entire control of the behavior of the shared data in face of\nconcurrent mutations. We also propose to order these trees by adding a\npositioning layer which is also independent to obtain a collection of ordered\ntree CRDTs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2000v1", 
    "title": "Dynamic Scope-Based Dijkstra's Algorithm", 
    "arxiv-id": "1201.2000v1", 
    "author": "Ondrej Moris", 
    "publish": "2012-01-10T10:11:11Z", 
    "summary": "We briefly report on the current state of a new dynamic algorithm for the\nroute planning problem based on a concept of scope (the static variant\npresented at ESA'11, HM2011A). We first motivate dynamization of the concept of\nscope admissibility, and then we briefly describe a modification of the\nscope-aware query algorithm of HM2011A to dynamic road networks. Finally, we\noutline our future work on this concept."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2501v2", 
    "title": "Nearly Optimal Sparse Fourier Transform", 
    "arxiv-id": "1201.2501v2", 
    "author": "Eric Price", 
    "publish": "2012-01-12T08:34:46Z", 
    "summary": "We consider the problem of computing the k-sparse approximation to the\ndiscrete Fourier transform of an n-dimensional signal. We show:\n  * An O(k log n)-time randomized algorithm for the case where the input signal\nhas at most k non-zero Fourier coefficients, and\n  * An O(k log n log(n/k))-time randomized algorithm for general input signals.\n  Both algorithms achieve o(n log n) time, and thus improve over the Fast\nFourier Transform, for any k = o(n). They are the first known algorithms that\nsatisfy this property. Also, if one assumes that the Fast Fourier Transform is\noptimal, the algorithm for the exactly k-sparse case is optimal for any k =\nn^{\\Omega(1)}.\n  We complement our algorithmic results by showing that any algorithm for\ncomputing the sparse Fourier transform of a general signal must use at least\n\\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform\nadaptive sampling."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2702v1", 
    "title": "Dynamic 3-sided Planar Range Queries with Expected Doubly Logarithmic   Time", 
    "arxiv-id": "1201.2702v1", 
    "author": "Kostas Tsichlas", 
    "publish": "2012-01-12T23:00:21Z", 
    "summary": "This work studies the problem of 2-dimensional searching for the 3-sided\nrange query of the form $[a, b]\\times (-\\infty, c]$ in both main and external\nmemory, by considering a variety of input distributions. We present three sets\nof solutions each of which examines the 3-sided problem in both RAM and I/O\nmodel respectively. The presented data structures are deterministic and the\nexpectation is with respect to the input distribution."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.3077v1", 
    "title": "A Bijective String Sorting Transform", 
    "arxiv-id": "1201.3077v1", 
    "author": "David Allen Scott", 
    "publish": "2012-01-15T10:17:36Z", 
    "summary": "Given a string of characters, the Burrows-Wheeler Transform rearranges the\ncharacters in it so as to produce another string of the same length which is\nmore amenable to compression techniques such as move to front, run-length\nencoding, and entropy encoders. We present a variant of the transform which\ngives rise to similar or better compression value, but, unlike the original,\nthe transform we present is bijective, in that the inverse transformation\nexists for all strings. Our experiments indicate that using our variant of the\ntransform gives rise to better compression ratio than the original\nBurrows-Wheeler transform. We also show that both the transform and its inverse\ncan be computed in linear time and consuming linear storage."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.3602v1", 
    "title": "Compact Binary Relation Representations with Rich Functionality", 
    "arxiv-id": "1201.3602v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2012-01-17T19:57:11Z", 
    "summary": "Binary relations are an important abstraction arising in many data\nrepresentation problems. The data structures proposed so far to represent them\nsupport just a few basic operations required to fit one particular application.\nWe identify many of those operations arising in applications and generalize\nthem into a wide set of desirable queries for a binary relation representation.\nWe also identify reductions among those operations. We then introduce several\nnovel binary relation representations, some simple and some quite\nsophisticated, that not only are space-efficient but also efficiently support a\nlarge subset of the desired queries."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4206v1", 
    "title": "A simple D^2-sampling based PTAS for k-means and other Clustering   Problems", 
    "arxiv-id": "1201.4206v1", 
    "author": "Sandeep Sen", 
    "publish": "2012-01-20T05:01:48Z", 
    "summary": "Given a set of points $P \\subset \\mathbb{R}^d$, the $k$-means clustering\nproblem is to find a set of $k$ {\\em centers} $C = \\{c_1,...,c_k\\}, c_i \\in\n\\mathbb{R}^d,$ such that the objective function $\\sum_{x \\in P} d(x,C)^2$,\nwhere $d(x,C)$ denotes the distance between $x$ and the closest center in $C$,\nis minimized. This is one of the most prominent objective functions that have\nbeen studied with respect to clustering.\n  $D^2$-sampling \\cite{ArthurV07} is a simple non-uniform sampling technique\nfor choosing points from a set of points. It works as follows: given a set of\npoints $P \\subseteq \\mathbb{R}^d$, the first point is chosen uniformly at\nrandom from $P$. Subsequently, a point from $P$ is chosen as the next sample\nwith probability proportional to the square of the distance of this point to\nthe nearest previously sampled points.\n  $D^2$-sampling has been shown to have nice properties with respect to the\n$k$-means clustering problem. Arthur and Vassilvitskii \\cite{ArthurV07} show\nthat $k$ points chosen as centers from $P$ using $D^2$-sampling gives an\n$O(\\log{k})$ approximation in expectation. Ailon et. al. \\cite{AJMonteleoni09}\nand Aggarwal et. al. \\cite{AggarwalDK09} extended results of \\cite{ArthurV07}\nto show that $O(k)$ points chosen as centers using $D^2$-sampling give $O(1)$\napproximation to the $k$-means objective function with high probability. In\nthis paper, we further demonstrate the power of $D^2$-sampling by giving a\nsimple randomized $(1 + \\epsilon)$-approximation algorithm that uses the\n$D^2$-sampling in its core."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4459v1", 
    "title": "An efficient parallel algorithm for the longest path problem in meshes", 
    "arxiv-id": "1201.4459v1", 
    "author": "Alireza Bagheri", 
    "publish": "2012-01-21T10:39:34Z", 
    "summary": "In this paper, first we give a sequential linear-time algorithm for the\nlongest path problem in meshes. This algorithm can be considered as an\nimprovement of [13]. Then based on this sequential algorithm, we present a\nconstant-time parallel algorithm for the problem which can be run on every\nparallel machine."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4899v2", 
    "title": "Finding Endogenously Formed Communities", 
    "arxiv-id": "1201.4899v2", 
    "author": "Shang-Hua Teng", 
    "publish": "2012-01-24T00:40:37Z", 
    "summary": "A central problem in e-commerce is determining overlapping communities among\nindividuals or objects in the absence of external identification or tagging. We\naddress this problem by introducing a framework that captures the notion of\ncommunities or clusters determined by the relative affinities among their\nmembers. To this end we define what we call an affinity system, which is a set\nof elements, each with a vector characterizing its preference for all other\nelements in the set. We define a natural notion of (potentially overlapping)\ncommunities in an affinity system, in which the members of a given community\ncollectively prefer each other to anyone else outside the community. Thus these\ncommunities are endogenously formed in the affinity system and are\n\"self-determined\" or \"self-certified\" by its members.\n  We provide a tight polynomial bound on the number of self-determined\ncommunities as a function of the robustness of the community. We present a\npolynomial-time algorithm for enumerating these communities. Moreover, we\nobtain a local algorithm with a strong stochastic performance guarantee that\ncan find a community in time nearly linear in the of size the community.\n  Social networks fit particularly naturally within the affinity system\nframework -- if we can appropriately extract the affinities from the relatively\nsparse yet rich information from social networks, our analysis then yields a\nset of efficient algorithms for enumerating self-determined communities in\nsocial networks. In the context of social networks we also connect our analysis\nwith results about $(\\alpha,\\beta)$-clusters introduced by Mishra, Schreiber,\nStanton, and Tarjan \\cite{msst}. In contrast with the polynomial bound we prove\non the number of communities in the affinity system model, we show that there\nexists a family of networks with superpolynomial number of\n$(\\alpha,\\beta)$-clusters."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5030v4", 
    "title": "Online Multi-Commodity Flow with High Demands", 
    "arxiv-id": "1201.5030v4", 
    "author": "Moti Medina", 
    "publish": "2012-01-24T16:11:45Z", 
    "summary": "This paper deals with the problem of computing, in an online fashion, a\nmaximum benefit multi-commodity flow (\\ONMCF), where the flow demands may be\nbigger than the edge capacities of the network.\n  We present an online, deterministic, centralized, all-or-nothing, bi-criteria\nalgorithm. The competitive ratio of the algorithm is constant, and the\nalgorithm augments the capacities by at most a logarithmic factor.\n  The algorithm can handle two types of flow requests: (i) low demand requests\nthat must be routed along a path, and (ii) high demand requests that may be\nrouted using a multi-path flow.\n  Two extensions are discussed: requests with known durations and machine\nscheduling."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5513v1", 
    "title": "Faster and Simpler Minimal Conflicting Set Identification", 
    "arxiv-id": "1201.5513v1", 
    "author": "Mathieu Raffinot", 
    "publish": "2012-01-26T13:35:33Z", 
    "summary": "Let C be a finite set of N elements and R = r_1,r_2,..., r_m a family of M\nsubsets of C. A subset X of R verifies the Consecutive Ones Property (C1P) if\nthere exists a permutation P of C such that each r_i in X is an interval of P.\nA Minimal Conflicting Set (MCS) S is a subset of R that does not verify the\nC1P, but such that any of its proper subsets does. In this paper, we present a\nnew simpler and faster algorithm to decide if a given element r in R belongs to\nat least one MCS. Our algorithm runs in O(N^2M^2 + NM^7), largely improving the\ncurrent O(M^6N^5 (M+N)^2 log(M+N)) fastest algorithm of [Blin {\\em et al}, CSR\n2011]. The new algorithm is based on an alternative approach considering\nminimal forbidden induced subgraphs of interval graphs instead of Tucker\nmatrices."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5985v1", 
    "title": "Implementation of exponential and parametrized algorithms in the AGAPE   project", 
    "arxiv-id": "1201.5985v1", 
    "author": "Vincent Levorato", 
    "publish": "2012-01-28T20:08:26Z", 
    "summary": "This technical report describes the implementation of exact and parametrized\nexponential algorithms, developed during the French ANR Agape during 2010-2012.\nThe developed algorithms are distributed under the CeCILL license and have been\nwritten in Java using the Jung graph library."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.6207v2", 
    "title": "k-Probe DH-graphs", 
    "arxiv-id": "1201.6207v2", 
    "author": "T. Kloks", 
    "publish": "2012-01-30T13:51:10Z", 
    "summary": "Let k be a natural number. Let G be a graph and let N_1,...,N_k be k\nindependent sets in G. The graph G is k-probe distance hereditary if G can be\nembedded into a DH-graph by adding edges between vertices that are contained in\nthe same independent set. We show that there exists a polynomial-time algorithm\nto check if a graph G is k-probe distance hereditary."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.6421v1", 
    "title": "The black-and-white coloring problem on permutation graphs", 
    "arxiv-id": "1201.6421v1", 
    "author": "Ton Kloks", 
    "publish": "2012-01-31T02:40:54Z", 
    "summary": "Given a graph G and integers b and w. The black-and-white coloring problem\nasks if there exist disjoint sets of vertices B and W with |B|=b and |W|=w such\nthat no vertex in B is adjacent to any vertex in W. In this paper we show that\nthe problem is polynomial when restricted to permutation graphs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.0082v1", 
    "title": "Dynamic Shortest Path Algorithms for Hypergraphs", 
    "arxiv-id": "1202.0082v1", 
    "author": "Amotz Bar-Noy", 
    "publish": "2012-02-01T02:19:46Z", 
    "summary": "A hypergraph is a set V of vertices and a set of non-empty subsets of V,\ncalled hyperedges. Unlike graphs, hypergraphs can capture higher-order\ninteractions in social and communication networks that go beyond a simple union\nof pairwise relationships. In this paper, we consider the shortest path problem\nin hypergraphs. We develop two algorithms for finding and maintaining the\nshortest hyperpaths in a dynamic network with both weight and topological\nchanges. These two algorithms are the first to address the fully dynamic\nshortest path problem in a general hypergraph. They complement each other by\npartitioning the application space based on the nature of the change dynamics\nand the type of the hypergraph."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.0319v1", 
    "title": "Real-Time Monitoring of Undirected Networks: Articulation Points,   Bridges, and Connected and Biconnected Components", 
    "arxiv-id": "1202.0319v1", 
    "author": "Luigi Laura", 
    "publish": "2012-02-01T23:10:58Z", 
    "summary": "In this paper we present the first algorithm in the streaming model to\ncharacterize completely the biconnectivity properties of undirected networks:\narticulation points, bridges, and connected and biconnected components. The\nmotivation of our work was the development of a real-time algorithm to monitor\nthe connectivity of the Autonomous Systems (AS) Network, but the solution\nprovided is general enough to be applied to any network.\n  The network structure is represented by a graph, and the algorithm is\nanalyzed in the datastream framework. Here, as in the \\emph{on-line} model, the\ninput graph is revealed one item (i.e., graph edge) after the other, in an\non-line fashion; but, if compared to traditional on-line computation, there are\nstricter requirements for both memory occupation and per item processing time.\nOur algorithm works by properly updating a forest over the graph nodes. All the\ngraph (bi)connectivity properties are stored in this forest. We prove the\ncorrectness of the algorithm, together with its space ($O(n\\,\\log n)$, with $n$\nbeing the number of nodes in the graph) and time bounds.\n  We also present the results of a brief experimental evaluation against\nreal-world graphs, including many samples of the AS network, ranging from\nmedium to massive size. These preliminary experimental results confirm the\neffectiveness of our approach."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.0364v1", 
    "title": "A note on probe cographs", 
    "arxiv-id": "1202.0364v1", 
    "author": "Ton Kloks", 
    "publish": "2012-02-02T05:24:25Z", 
    "summary": "Let G be a graph and let N_1, ..., N_k be k independent sets in G. The graph\nG is a k-probe cograph if G can be embedded into a cograph by adding edges\nbetween vertices that are contained in the same independent set. We show that\nthere exists an O(k n^5) algorithm to check if a graph G is a k-probe cograph."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.1041v1", 
    "title": "Packing interval graphs with vertex-disjoint triangles", 
    "arxiv-id": "1202.1041v1", 
    "author": "Ton Kloks", 
    "publish": "2012-02-06T03:36:23Z", 
    "summary": "We show that there exists a polynomial algorithm to pack interval graphs with\nvertex-disjoint triangles."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.1090v1", 
    "title": "The covert set-cover problem with application to Network Discovery", 
    "arxiv-id": "1202.1090v1", 
    "author": "V. N. Muralidhara", 
    "publish": "2012-02-06T10:14:24Z", 
    "summary": "We address a version of the set-cover problem where we do not know the sets\ninitially (and hence referred to as covert) but we can query an element to find\nout which sets contain this element as well as query a set to know the\nelements. We want to find a small set-cover using a minimal number of such\nqueries. We present a Monte Carlo randomized algorithm that approximates an\noptimal set-cover of size $OPT$ within $O(\\log N)$ factor with high probability\nusing $O(OPT \\cdot \\log^2 N)$ queries where $N$ is the input size.\n  We apply this technique to the network discovery problem that involves\ncertifying all the edges and non-edges of an unknown $n$-vertices graph based\non layered-graph queries from a minimal number of vertices. By reducing it to\nthe covert set-cover problem we present an $O(\\log^2 n)$-competitive Monte\nCarlo randomized algorithm for the covert version of network discovery problem.\nThe previously best known algorithm has a competitive ratio of $\\Omega\n(\\sqrt{n\\log n})$ and therefore our result achieves an exponential improvement."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.2820v1", 
    "title": "On Approximating String Selection Problems with Outliers", 
    "arxiv-id": "1202.2820v1", 
    "author": "Oren Weimann", 
    "publish": "2012-02-13T19:09:26Z", 
    "summary": "Many problems in bioinformatics are about finding strings that approximately\nrepresent a collection of given strings. We look at more general problems where\nsome input strings can be classified as outliers. The Close to Most Strings\nproblem is, given a set S of same-length strings, and a parameter d, find a\nstring x that maximizes the number of \"non-outliers\" within Hamming distance d\nof x. We prove this problem has no PTAS unless ZPP=NP, correcting a decade-old\nmistake. The Most Strings with Few Bad Columns problem is to find a\nmaximum-size subset of input strings so that the number of non-identical\npositions is at most k; we show it has no PTAS unless P=NP. We also observe\nClosest to k Strings has no EPTAS unless W[1]=FPT. In sum, outliers help model\nproblems associated with using biological data, but we show the problem of\nfinding an approximate solution is computationally difficult."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.3097v3", 
    "title": "Computing Resolution-Path Dependencies in Linear Time", 
    "arxiv-id": "1202.3097v3", 
    "author": "Stefan Szeider", 
    "publish": "2012-02-14T17:33:34Z", 
    "summary": "The alternation of existential and universal quantifiers in a quantified\nboolean formula (QBF) generates dependencies among variables that must be\nrespected when evaluating the formula. Dependency schemes provide a general\nframework for representing such dependencies. Since it is generally intractable\nto determine dependencies exactly, a set of potential dependencies is computed\ninstead, which may include false positives. Among the schemes proposed so far,\nresolution-path dependencies introduce the fewest spurious dependencies. In\nthis work, we describe an algorithm that detects resolution-path dependencies\nin linear time, resolving a problem posed by Van Gelder (CP 2011)."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1202.3208v1", 
    "title": "Linear-Space Substring Range Counting over Polylogarithmic Alphabets", 
    "arxiv-id": "1202.3208v1", 
    "author": "Pawe\u0142 Gawrychowski", 
    "publish": "2012-02-15T05:29:25Z", 
    "summary": "Bille and G{\\o}rtz (2011) recently introduced the problem of substring range\ncounting, for which we are asked to store compactly a string $S$ of $n$\ncharacters with integer labels in ([0, u]), such that later, given an interval\n([a, b]) and a pattern $P$ of length $m$, we can quickly count the occurrences\nof $P$ whose first characters' labels are in ([a, b]). They showed how to store\n$S$ in $\\Oh{n \\log n / \\log \\log n}$ space and answer queries in $\\Oh{m + \\log\n\\log u}$ time. We show that, if $S$ is over an alphabet of size (\\polylog (n)),\nthen we can achieve optimal linear space. Moreover, if (u = n \\polylog (n)),\nthen we can also reduce the time to $\\Oh{m}$. Our results give linear space and\ntime bounds for position-restricted substring counting and the counting\nversions of indexing substrings with intervals, indexing substrings with gaps\nand aligned pattern matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_18", 
    "link": "http://arxiv.org/pdf/1202.3311v1", 
    "title": "Speeding-up $q$-gram mining on grammar-based compressed texts", 
    "arxiv-id": "1202.3311v1", 
    "author": "Masayuki Takeda", 
    "publish": "2012-02-15T14:13:02Z", 
    "summary": "We present an efficient algorithm for calculating $q$-gram frequencies on\nstrings represented in compressed form, namely, as a straight line program\n(SLP). Given an SLP $\\mathcal{T}$ of size $n$ that represents string $T$, the\nalgorithm computes the occurrence frequencies of all $q$-grams in $T$, by\nreducing the problem to the weighted $q$-gram frequencies problem on a\ntrie-like structure of size $m = |T|-\\mathit{dup}(q,\\mathcal{T})$, where\n$\\mathit{dup}(q,\\mathcal{T})$ is a quantity that represents the amount of\nredundancy that the SLP captures with respect to $q$-grams. The reduced problem\ncan be solved in linear time. Since $m = O(qn)$, the running time of our\nalgorithm is $O(\\min\\{|T|-\\mathit{dup}(q,\\mathcal{T}),qn\\})$, improving our\nprevious $O(qn)$ algorithm when $q = \\Omega(|T|/n)$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_18", 
    "link": "http://arxiv.org/pdf/1202.3367v3", 
    "title": "Faster Approximate Multicommodity Flow Using Quadratically Coupled Flows", 
    "arxiv-id": "1202.3367v3", 
    "author": "Richard Peng", 
    "publish": "2012-02-15T17:01:35Z", 
    "summary": "The maximum multicommodity flow problem is a natural generalization of the\nmaximum flow problem to route multiple distinct flows. Obtaining a $1-\\epsilon$\napproximation to the multicommodity flow problem on graphs is a well-studied\nproblem. In this paper we present an adaptation of recent advances in\nsingle-commodity flow algorithms to this problem. As the underlying linear\nsystems in the electrical problems of multicommodity flow problems are no\nlonger Laplacians, our approach is tailored to generate specialized systems\nwhich can be preconditioned and solved efficiently using Laplacians. Given an\nundirected graph with m edges and k commodities, we give algorithms that find\n$1-\\epsilon$ approximate solutions to the maximum concurrent flow problem and\nthe maximum weighted multicommodity flow problem in time\n$\\tilde{O}(m^{4/3}\\poly(k,\\epsilon^{-1}))$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_18", 
    "link": "http://arxiv.org/pdf/1202.3470v2", 
    "title": "Pattern Matching in Multiple Streams", 
    "arxiv-id": "1202.3470v2", 
    "author": "Benjamin Sach", 
    "publish": "2012-02-15T23:11:48Z", 
    "summary": "We investigate the problem of deterministic pattern matching in multiple\nstreams. In this model, one symbol arrives at a time and is associated with one\nof s streaming texts. The task at each time step is to report if there is a new\nmatch between a fixed pattern of length m and a newly updated stream. As is\nusual in the streaming context, the goal is to use as little space as possible\nwhile still reporting matches quickly. We give almost matching upper and lower\nspace bounds for three distinct pattern matching problems. For exact matching\nwe show that the problem can be solved in constant time per arriving symbol and\nO(m+s) words of space. For the k-mismatch and k-difference problems we give\nO(k) time solutions that require O(m+ks) words of space. In all three cases we\nalso give space lower bounds which show our methods are optimal up to a single\nlogarithmic factor. Finally we set out a number of open problems related to\nthis new model for pattern matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_18", 
    "link": "http://arxiv.org/pdf/1202.4072v1", 
    "title": "An efficient polynomial time approximation scheme for load balancing on   uniformly related machines", 
    "arxiv-id": "1202.4072v1", 
    "author": "Asaf Levin", 
    "publish": "2012-02-18T11:48:17Z", 
    "summary": "We consider basic problems of non-preemptive scheduling on uniformly related\nmachines. For a given schedule, defined by a partition of the jobs into m\nsubsets corresponding to the m machines, C_i denotes the completion time of\nmachine i. Our goal is to find a schedule which minimizes or maximizes\n\\sum_{i=1}^m C_i^p for a fixed value of p such that 0<p<\\infty. For p>1 the\nminimization problem is equivalent to the well-known problem of minimizing the\n\\ell_p norm of the vector of the completion times of the machines, and for\n0<p<1 the maximization problem is of interest. Our main result is an efficient\npolynomial time approximation scheme (EPTAS) for each one of these problems.\nOur schemes use a non-standard application of the so-called shifting technique.\nWe focus on the work (total size of jobs) assigned to each machine and\nintroduce intervals of forbidden work. These intervals are defined so that the\nresulting effect on the goal function is sufficiently small. This allows the\npartition of the problem into sub-problems (with subsets of machines and jobs)\nwhose solutions are combined into the final solution using dynamic programming.\nOur results are the first EPTAS's for this natural class of load balancing\nproblems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_16", 
    "link": "http://arxiv.org/pdf/1202.4076v1", 
    "title": "Cross-Document Pattern Matching", 
    "arxiv-id": "1202.4076v1", 
    "author": "Tatiana Starikovskaya", 
    "publish": "2012-02-18T13:31:36Z", 
    "summary": "We study a new variant of the string matching problem called cross-document\nstring matching, which is the problem of indexing a collection of documents to\nsupport an efficient search for a pattern in a selected document, where the\npattern itself is a substring of another document. Several variants of this\nproblem are considered, and efficient linear-space solutions are proposed with\nquery time bounds that either do not depend at all on the pattern size or\ndepend on it in a very limited way (doubly logarithmic). As a side result, we\npropose an improved solution to the weighted level ancestor problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_16", 
    "link": "http://arxiv.org/pdf/1202.4326v2", 
    "title": "Space-Constrained Interval Selection", 
    "arxiv-id": "1202.4326v2", 
    "author": "Adi Rosen", 
    "publish": "2012-02-20T13:56:20Z", 
    "summary": "We study streaming algorithms for the interval selection problem: finding a\nmaximum cardinality subset of disjoint intervals on the line. A deterministic\n2-approximation streaming algorithm for this problem is developed, together\nwith an algorithm for the special case of proper intervals, achieving improved\napproximation ratio of 3/2. We complement these upper bounds by proving that\nthey are essentially best possible in the streaming setting: it is shown that\nan approximation ratio of $2 - \\epsilon$ (or $3 / 2 - \\epsilon$ for proper\nintervals) cannot be achieved unless the space is linear in the input size. In\npassing, we also answer an open question of Adler and Azar \\cite{AdlerAzar03}\nregarding the space complexity of constant-competitive randomized preemptive\nonline algorithms for the same problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_16", 
    "link": "http://arxiv.org/pdf/1202.4504v1", 
    "title": "A Constant Factor Approximation Algorithm for Reordering Buffer   Management", 
    "arxiv-id": "1202.4504v1", 
    "author": "Yuval Rabani", 
    "publish": "2012-02-21T01:28:05Z", 
    "summary": "In the reordering buffer management problem (RBM) a sequence of $n$ colored\nitems enters a buffer with limited capacity $k$. When the buffer is full, one\nitem is removed to the output sequence, making room for the next input item.\nThis step is repeated until the input sequence is exhausted and the buffer is\nempty. The objective is to find a sequence of removals that minimizes the total\nnumber of color changes in the output sequence. The problem formalizes numerous\napplications in computer and production systems, and is known to be NP-hard.\n  We give the first constant factor approximation guarantee for RBM. Our\nalgorithm is based on an intricate \"rounding\" of the solution to an LP\nrelaxation for RBM, so it also establishes a constant upper bound on the\nintegrality gap of this relaxation. Our results improve upon the best previous\nbound of $O(\\sqrt{\\log k})$ of Adamaszek et al. (STOC 2011) that used different\nmethods and gave an online algorithm. Our constant factor approximation beats\nthe super-constant lower bounds on the competitive ratio given by Adamaszek et\nal. This is the first demonstration of an offline algorithm for RBM that is\nprovably better than any online algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_16", 
    "link": "http://arxiv.org/pdf/1202.4945v1", 
    "title": "Algorithms for Sampling 3-Orientations of Planar Triangulations", 
    "arxiv-id": "1202.4945v1", 
    "author": "Prasad Tetali", 
    "publish": "2012-02-22T15:45:21Z", 
    "summary": "Given a planar triangulation, a 3-orientation is an orientation of the\ninternal edges so all internal vertices have out-degree three. Each\n3-orientation gives rise to a unique edge coloring known as a Schnyder wood\nthat has proven powerful for various computing and combinatorics applications.\nWe consider natural Markov chains for sampling uniformly from the set of\n3-orientations. First, we study a \"triangle-reversing\" chain on the space of\n3-orientations of a fixed triangulation that reverses the orientation of the\nedges around a triangle in each move. It was shown previously that this chain\nconnects the state space and we show that (i) when restricted to planar\ntriangulations of maximum degree six, the Markov chain is rapidly mixing, and\n(ii) there exists a triangulation with high degree on which this Markov chain\nmixes slowly. Next, we consider an \"edge-flipping\" chain on the larger state\nspace consisting of 3-orientations of all planar triangulations on a fixed\nnumber of vertices. It was also shown previously that this chain connects the\nstate space and we prove that the chain is always rapidly mixing. The\ntriangle-reversing and edge-flipping Markov chains both arise in the context of\nsampling other combinatorial structures, such as Eulerian orientations and\ntriangulations of planar point sets, so our results here may shed light on the\nmixing rate of these related chains as well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31265-6_16", 
    "link": "http://arxiv.org/pdf/1202.5074v2", 
    "title": "Solving Single-digit Sudoku Subproblems", 
    "arxiv-id": "1202.5074v2", 
    "author": "David Eppstein", 
    "publish": "2012-02-23T01:30:56Z", 
    "summary": "We show that single-digit \"Nishio\" subproblems in nxn Sudoku puzzles may be\nsolved in time o(2^n), faster than previous solutions such as the pattern\noverlay method. We also show that single-digit deduction in Sudoku is NP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2", 
    "link": "http://arxiv.org/pdf/1202.5233v4", 
    "title": "Computing Lempel-Ziv Factorization Online", 
    "arxiv-id": "1202.5233v4", 
    "author": "Tatiana Starikovskaya", 
    "publish": "2012-02-23T16:50:51Z", 
    "summary": "We present an algorithm which computes the Lempel-Ziv factorization of a word\n$W$ of length $n$ on an alphabet $\\Sigma$ of size $\\sigma$ online in the\nfollowing sense: it reads $W$ starting from the left, and, after reading each\n$r = O(\\log_{\\sigma} n)$ characters of $W$, updates the Lempel-Ziv\nfactorization. The algorithm requires $O(n \\log \\sigma)$ bits of space and O(n\n\\log^2 n) time. The basis of the algorithm is a sparse suffix tree combined\nwith wavelet trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2", 
    "link": "http://arxiv.org/pdf/1202.5619v2", 
    "title": "Persistent Monitoring in Discrete Environments: Minimizing the Maximum   Weighted Latency Between Observations", 
    "arxiv-id": "1202.5619v2", 
    "author": "Stephen L. Smith", 
    "publish": "2012-02-25T06:50:39Z", 
    "summary": "In this paper, we consider the problem of planning a path for a robot to\nmonitor a known set of features of interest in an environment. We represent the\nenvironment as a graph with vertex weights and edge lengths. The vertices\nrepresent regions of interest, edge lengths give travel times between regions,\nand the vertex weights give the importance of each region. As the robot\nrepeatedly performs a closed walk on the graph, we define the weighted latency\nof a vertex to be the maximum time between visits to that vertex, weighted by\nthe importance (vertex weight) of that vertex. Our goal is to find a closed\nwalk that minimizes the maximum weighted latency of any vertex. We show that\nthere does not exist a polynomial time algorithm for the problem. We then\nprovide two approximation algorithms; an $O(\\log n)$-approximation algorithm\nand an $O(\\log \\rho_G)$-approximation algorithm, where $\\rho_G$ is the ratio\nbetween the maximum and minimum vertex weights. We provide simulation results\nwhich demonstrate that our algorithms can be applied to problems consisting of\nthousands of vertices, and a case study for patrolling a city for crime."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2", 
    "link": "http://arxiv.org/pdf/1202.5670v1", 
    "title": "(Really) Tight bounds for dispatching binary methods", 
    "arxiv-id": "1202.5670v1", 
    "author": "Pawel Gawrychowski", 
    "publish": "2012-02-25T17:05:53Z", 
    "summary": "We consider binary dispatching problem originating from object oriented\nprogramming. We want to preprocess a hierarchy of classes and collection of\nmethods so that given a function call in the run-time we are able to retrieve\nthe most specialized implementation which can be invoked with the actual types\nof the arguments. For the binary dispatching, where the methods take exactly\ntwo arguments, logarithmic query time is possible, even if the structure is\nallowed to take linear space. Unfortunately, known solutions achieving such\ncomplexity require superlinear time for constructing the structure. Using a\ndifferent idea we are able to construct in (deterministic) linear time and\nspace a structure allowing dispatching binary methods in the same logarithmic\ntime. Then we show how to improve the query time to slightly sublogarithmic,\nwhich is easily seen to be optimal as a consequence of some already known lower\nbounds if we want to keep the size of the resulting structure close to linear."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1202.5985v1", 
    "title": "Fast computation of the performance evaluation of biometric systems:   application to multibiometric", 
    "arxiv-id": "1202.5985v1", 
    "author": "Christophe Rosenberger", 
    "publish": "2012-02-27T16:05:10Z", 
    "summary": "The performance evaluation of biometric systems is a crucial step when\ndesigning and evaluating such systems. The evaluation process uses the Equal\nError Rate (EER) metric proposed by the International Organization for\nStandardization (ISO/IEC). The EER metric is a powerful metric which allows\neasily comparing and evaluating biometric systems. However, the computation\ntime of the EER is, most of the time, very intensive. In this paper, we propose\na fast method which computes an approximated value of the EER. We illustrate\nthe benefit of the proposed method on two applications: the computing of non\nparametric confidence intervals and the use of genetic algorithms to compute\nthe parameters of fusion functions. Experimental results show the superiority\nof the proposed EER approximation method in term of computing time, and the\ninterest of its use to reduce the learning of parameters with genetic\nalgorithms. The proposed method opens new perspectives for the development of\nsecure multibiometrics systems by speeding up their computation time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1202.6256v1", 
    "title": "Diagonalization Matrix Method of Solving the First Problem of Hidden   Markov Model in Speech Recognition System", 
    "arxiv-id": "1202.6256v1", 
    "author": "G. Seenivasan", 
    "publish": "2012-02-28T15:28:50Z", 
    "summary": "This paper proposes a computationally efficient method of solving evaluation\nproblem of Hidden Markov Model (HMM) with a given set of discrete observation\nsymbols, number of states and probability distribution matrices. The\nobservation probability for a given HMM model is evaluated using an approach in\nwhich the probability evaluation is reduced to the problem of evaluating the\nproduct of matrices with different powers and formed out of state transition\nprobabilities and observation probabilities. Finding powers of a matrix is done\nby using the computationally efficient diagonalization method thereby reducing\nthe overall computational effort for evaluating the Evaluation problem of\nHMM.The proposed method is compared with the existing direct method. It is\nfound that evaluating matrix power by diagnolisation method is more suitable\nthan that of the direct, method."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1202.6562v2", 
    "title": "Dictionary learning under global sparsity constraint", 
    "arxiv-id": "1202.6562v2", 
    "author": "Zongben Xu", 
    "publish": "2012-02-29T14:56:57Z", 
    "summary": "A new method is proposed in this paper to learn overcomplete dictionary from\ntraining data samples. Differing from the current methods that enforce similar\nsparsity constraint on each of the input samples, the proposed method attempts\nto impose global sparsity constraint on the entire data set. This enables the\nproposed method to fittingly assign the atoms of the dictionary to represent\nvarious samples and optimally adapt to the complicated structures underlying\nthe entire data set. By virtue of the sparse coding and sparse PCA techniques,\na simple algorithm is designed for the implementation of the method. The\nefficiency and the convergence of the proposed algorithm are also theoretically\nanalyzed. Based on the experimental results implemented on a series of signal\nand image data sets, it is apparent that our method performs better than the\ncurrent dictionary learning methods in original dictionary recovering, input\ndata reconstructing, and salient data structure revealing."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1202.6598v1", 
    "title": "Sublinear Time Approximate Sum via Uniform Random Sampling", 
    "arxiv-id": "1202.6598v1", 
    "author": "Zhiyong Peng", 
    "publish": "2012-02-29T16:52:17Z", 
    "summary": "We investigate the approximation for computing the sum $a_1+...+a_n$ with an\ninput of a list of nonnegative elements $a_1,..., a_n$. If all elements are in\nthe range $[0,1]$, there is a randomized algorithm that can compute an\n$(1+\\epsilon)$-approximation for the sum problem in time ${O({n(\\log\\log\nn)\\over\\sum_{i=1}^n a_i})}$, where $\\epsilon$ is a constant in $(0,1)$. Our\nrandomized algorithm is based on the uniform random sampling, which selects one\nelement with equal probability from the input list each time. We also prove a\nlower bound $\\Omega({n\\over \\sum_{i=1}^n a_i})$, which almost matches the upper\nbound, for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1202.6642v1", 
    "title": "Deterministic parameterized connected vertex cover", 
    "arxiv-id": "1202.6642v1", 
    "author": "Marek Cygan", 
    "publish": "2012-02-29T18:48:21Z", 
    "summary": "In the Connected Vertex Cover problem we are given an undirected graph G\ntogether with an integer k and we are to find a subset of vertices X of size at\nmost k, such that X contains at least one end-point of each edge and moreover X\ninduces a connected subgraph. For this problem we present a deterministic\nalgorithm running in O(2^k n^O(1)) time and polynomial space, improving over\npreviously best O(2.4882^k n^O(1)) deterministic algorithm and O(2^k n^O(1))\nrandomized algorithm. Furthermore, when usage of exponential space is allowed,\nwe present an O(2^k k(n+m)) time algorithm that solves a more general variant\nwith arbitrary real weights.\n  Finally, we show that in O(2k k(n + m)) time and O(2^k k) space one can count\nthe number of connected vertex covers of size at most k, which can not be\nimproved to O((2 - eps)^k nO(1)) for any eps > 0 under the Strong Exponential\nTime Hypothesis, as shown by Cygan et al. [CCC'12]."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1203.0120v1", 
    "title": "How does the Shift-insertion sort behave when the sorting elements   follow a Normal distribution?", 
    "arxiv-id": "1203.0120v1", 
    "author": "N. C. Mahanti", 
    "publish": "2012-03-01T08:55:06Z", 
    "summary": "The present paper examines the behavior of Shift-insertion sort (insertion\nsort with shifting) for normal distribution inputs and is in continuation of\nour earlier work on this new algorithm for discrete distribution inputs,\nnamely, negative binomial. Shift insertion sort is found more sensitive for\nmain effects but not for all interaction effects compared to conventional\ninsertion sort."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1203.0259v1", 
    "title": "A rearrangement step with potential uses in priority queues", 
    "arxiv-id": "1203.0259v1", 
    "author": "M. Brian Jacokes", 
    "publish": "2012-03-01T18:21:14Z", 
    "summary": "Link-based data structures, such as linked lists and binary search trees,\nhave many well-known rearrangement steps allowing for efficient implementations\nof insertion, deletion, and other operations. We describe a rearrangement\nprimitive designed for link-based, heap-ordered priority queues in the\ncomparison model, such as those similar to Fibonacci heaps or binomial heaps.\n  In its most basic form, the primitive rearranges a collection of heap-ordered\nperfect binary trees. Doing so offers a data structure control on the number of\ntrees involved in such a collection, in particular keeping this number\nlogarithmic in the number of elements. The rearrangement step is free from an\namortized complexity standpoint (using an appropriate potential function)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1203.0289v3", 
    "title": "Secure Multi-Party Computation in Large Networks", 
    "arxiv-id": "1203.0289v3", 
    "author": "Mahdi Zamani", 
    "publish": "2012-03-01T20:44:41Z", 
    "summary": "We describe scalable protocols for solving the secure multi-party computation\n(MPC) problem among a large number of parties. We consider both the synchronous\nand the asynchronous communication models. In the synchronous setting, our\nprotocol is secure against a static malicious adversary corrupting less than a\n$1/3$ fraction of the parties. In the asynchronous setting, we allow the\nadversary to corrupt less than a $1/8$ fraction of parties. For any\ndeterministic function that can be computed by an arithmetic circuit with $m$\ngates, both of our protocols require each party to send a number of field\nelements and perform an amount of computation that is $\\tilde{O}(m/n + \\sqrt\nn)$. We also show that our protocols provide perfect and universally-composable\nsecurity.\n  To achieve our asynchronous MPC result, we define the \\emph{threshold\ncounting problem} and present a distributed protocol to solve it in the\nasynchronous setting. This protocol is load balanced, with computation,\ncommunication and latency complexity of $O(\\log{n})$, and can also be used for\ndesigning other load-balanced applications in the asynchronous communication\nmodel."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.future.2012.02.003", 
    "link": "http://arxiv.org/pdf/1203.1250v1", 
    "title": "An Exploratory Study of Critical Factors Affecting the Efficiency of   Sorting Techniques (Shell, Heap and Treap)", 
    "arxiv-id": "1203.1250v1", 
    "author": "Oluwatimilehin Salako", 
    "publish": "2012-03-03T21:09:42Z", 
    "summary": "The efficiency of sorting techniques has a significant impact on the overall\nefficiency of a program. The efficiency of Shell, Heap and Treap sorting\ntechniques in terms of both running time and memory usage was studied,\nexperiments conducted and results subjected to factor analysis by SPSS. The\nstudy revealed the main factor affecting these sorting techniques was time\ntaken to sort."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcsea.2012.2103", 
    "link": "http://arxiv.org/pdf/1203.1830v2", 
    "title": "Partition Sort Revisited: Reconfirming the Robustness in Average Case   and much more!", 
    "arxiv-id": "1203.1830v2", 
    "author": "Soubhik Chakraborty", 
    "publish": "2012-03-08T15:42:04Z", 
    "summary": "In our previous work there was some indication that Partition Sort could be\nhaving a more robust average case O(nlogn) complexity than the popular Quick\nSort. In our first study in this paper, we reconfirm this through computer\nexperiments for inputs from Cauchy distribution for which expectation\ntheoretically does not exist. Additionally, the algorithm is found to be\nsensitive to parameters of the input probability distribution demanding further\ninvestigation on parameterized complexity. The results on this algorithm for\nBinomial inputs in our second study are very encouraging in that direction."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcsea.2012.2103", 
    "link": "http://arxiv.org/pdf/1203.2414v1", 
    "title": "An Optimal Algorithm for Conflict-Free Coloring for Tree of Rings", 
    "arxiv-id": "1203.2414v1", 
    "author": "Einollah Pira", 
    "publish": "2012-03-12T07:20:56Z", 
    "summary": "An optimal algorithm is presented about Conflict-Free Coloring for connected\nsubgraphs of tree of rings. Suppose the number of the rings in the tree is |T|\nand the maximum length of rings is |R|. A presented algorithm in [1] for a Tree\nof rings used O(log|T|.log|R|) colors but this algorithm uses O(log|T|+log|R|)\ncolors. The coloring earned by this algorithm has the unique-min property, that\nis, the unique color is also minimum."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.2538v3", 
    "title": "Spanning trees and the complexity of flood-filling games", 
    "arxiv-id": "1203.2538v3", 
    "author": "Alexander Scott", 
    "publish": "2012-03-12T16:29:42Z", 
    "summary": "We consider problems related to the combinatorial game (Free-)Flood-It, in\nwhich players aim to make a coloured graph monochromatic with the minimum\npossible number of flooding operations. We show that the minimum number of\nmoves required to flood any given graph G is equal to the minimum, taken over\nall spanning trees T of G, of the number of moves required to flood T. This\nresult is then applied to give two polynomial-time algorithms for flood-filling\nproblems. Firstly, we can compute in polynomial time the minimum number of\nmoves required to flood a graph with only a polynomial number of connected\nsubgraphs. Secondly, given any coloured connected graph and a subset of the\nvertices of bounded size, the number of moves required to connect this subset\ncan be computed in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.2543v2", 
    "title": "Biclique-colouring verification complexity and biclique-colouring power   graphs", 
    "arxiv-id": "1203.2543v2", 
    "author": "Celina M. H. de Figueiredo", 
    "publish": "2012-03-12T16:45:21Z", 
    "summary": "Biclique-colouring is a colouring of the vertices of a graph in such a way\nthat no maximal complete bipartite subgraph with at least one edge is\nmonochromatic. We show that it is coNP-complete to check whether a given\nfunction that associates a colour to each vertex is a biclique-colouring, a\nresult that justifies the search for structured classes where the\nbiclique-colouring problem could be efficiently solved. We consider\nbiclique-colouring restricted to powers of paths and powers of cycles. We\ndetermine the biclique-chromatic number of powers of paths and powers of\ncycles. The biclique-chromatic number of a power of a path P_{n}^{k} is max(2k\n+ 2 - n, 2) if n >= k + 1 and exactly n otherwise. The biclique-chromatic\nnumber of a power of a cycle C_n^k is at most 3 if n >= 2k + 2 and exactly n\notherwise; we additionally determine the powers of cycles that are\n2-biclique-colourable. All proofs are algorithmic and provide polynomial-time\nbiclique-colouring algorithms for graphs in the investigated classes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3415v5", 
    "title": "acc-Motif Detection Tool", 
    "arxiv-id": "1203.3415v5", 
    "author": "Arlindo F. da Concei\u00e7\u00e3o", 
    "publish": "2012-03-15T16:54:42Z", 
    "summary": "Network motif algorithms have been a topic of research mainly after the\n2002-seminal paper from Milo \\emph{et al}, that provided motifs as a way to\nuncover the basic building blocks of most networks. In Bioinformatics, motifs\nhave been mainly applied in the field of gene regulation networks. This paper\nproposes new algorithms to exactly count isomorphic pattern motifs of sizes 3,\n4 and 5 in directed graphs. Let $G(V,E)$ be a directed graph with $m=|E|$. We\ndescribe an $O({m\\sqrt{m}})$ time complexity algorithm to count isomorphic\npatterns of size 3. In order to count isomorphic patterns of size 4, we propose\nan $O(m^2)$ algorithm. To count patterns with 5 vertices, the algorithm is\n$O(m^2n)$. The new algorithms were implemented and compared with FANMOD and\nKavosh motif detection tools. The experiments show that our algorithms are\nexpressively faster than FANMOD and Kavosh's. We also let our motif-detecting\ntool available in the Internet."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3578v4", 
    "title": "Iterative rounding approximation algorithms for degree-bounded   node-connectivity network design", 
    "arxiv-id": "1203.3578v4", 
    "author": "R. Ravi", 
    "publish": "2012-03-15T21:35:37Z", 
    "summary": "We consider the problem of finding a minimum edge cost subgraph of a graph\nsatisfying both given node-connectivity requirements and degree upper bounds on\nnodes. We present an iterative rounding algorithm of the biset LP relaxation\nfor this problem. For directed graphs and $k$-out-connectivity requirements\nfrom a root, our algorithm computes a solution that is a 2-approximation on the\ncost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$\nwhere $b(v)$ is the degree upper bound on $v$. For undirected graphs and\nelement-connectivity requirements with maximum connectivity requirement $k$,\nour algorithm computes a solution that is a $4$-approximation on the cost, and\nthe degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These\nratios improve the previous $O(\\log k)$-approximation on the cost and $O(2^k\nb(v))$ approximation on the degrees. Our algorithms can be used to improve\napproximation ratios for other node-connectivity problems such as undirected\n$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected\nrooted $k$-connectivity and subset $k$-connectivity."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3593v1", 
    "title": "Ad Serving Using a Compact Allocation Plan", 
    "arxiv-id": "1203.3593v1", 
    "author": "Jason Zien", 
    "publish": "2012-03-16T00:31:18Z", 
    "summary": "A large fraction of online display advertising is sold via guaranteed\ncontracts: a publisher guarantees to the advertiser a certain number of user\nvisits satisfying the targeting predicates of the contract. The publisher is\nthen tasked with solving the ad serving problem - given a user visit, which of\nthe thousands of matching contracts should be displayed, so that by the\nexpiration time every contract has obtained the requisite number of user\nvisits. The challenges of the problem come from (1) the sheer size of the\nproblem being solved, with tens of thousands of contracts and billions of user\nvisits, (2) the unpredictability of user behavior, since these contracts are\nsold months ahead of time, when only a forecast of user visits is available and\n(3) the minute amount of resources available online, as an ad server must\nrespond with a matching contract in a fraction of a second.\n  We present a solution to the guaranteed delivery ad serving problem using\n{\\em compact allocation plans}. These plans, computed offline, can be\nefficiently queried by the ad server during an ad call; they are small, using\nonly O(1) space for contract; and are stateless, allowing for distributed\nserving without any central coordination. We evaluate this approach on a real\nset of user visits and guaranteed contracts and show that the compact\nallocation plans are an effective way of solving the guaranteed delivery ad\nserving problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3619v1", 
    "title": "SHALE: An Efficient Algorithm for Allocation of Guaranteed Display   Advertising", 
    "arxiv-id": "1203.3619v1", 
    "author": "Jian Yang", 
    "publish": "2012-03-16T06:02:29Z", 
    "summary": "Motivated by the problem of optimizing allocation in guaranteed display\nadvertising, we develop an efficient, lightweight method of generating a\ncompact {\\em allocation plan} that can be used to guide ad server decisions.\nThe plan itself uses just O(1) state per guaranteed contract, is robust to\nnoise, and allows us to serve (provably) nearly optimally. The optimization\nmethod we develop is scalable, with a small in-memory footprint, and working in\nlinear time per iteration. It is also \"stop-anytime\", meaning that\ntime-critical applications can stop early and still get a good serving\nsolution. Thus, it is particularly useful for optimizing the large problems\narising in the context of display advertising. We demonstrate the effectiveness\nof our algorithm using actual Yahoo! data."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3883v1", 
    "title": "A note on the fast power series' exponential", 
    "arxiv-id": "1203.3883v1", 
    "author": "Igor S. Sergeev", 
    "publish": "2012-03-17T18:20:32Z", 
    "summary": "It is shown that the exponential of a complex power series up to order n can\nbe implemented via (23/12+o(1))M(n) binary arithmetic operations over complex\nfield, where M(n) stands for the (smoothed) complexity of multiplication of\npolynomials of degree <n in FFT-model. Yet, it is shown how to raise a power\nseries to a constant power with the complexity (27/8+o(1))M(n)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4117v1", 
    "title": "A More Reliable Greedy Heuristic for Maximum Matchings in Sparse Random   Graphs", 
    "arxiv-id": "1203.4117v1", 
    "author": "Michael Rink", 
    "publish": "2012-03-19T14:27:00Z", 
    "summary": "We propose a new greedy algorithm for the maximum cardinality matching\nproblem. We give experimental evidence that this algorithm is likely to find a\nmaximum matching in random graphs with constant expected degree c>0,\nindependent of the value of c. This is contrary to the behavior of commonly\nused greedy matching heuristics which are known to have some range of c where\nthey probably fail to compute a maximum matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4619v1", 
    "title": "Online Load Balancing on Unrelated Machines with Startup Costs", 
    "arxiv-id": "1203.4619v1", 
    "author": "Debmalya Panigrahi", 
    "publish": "2012-03-20T23:06:10Z", 
    "summary": "Motivated by applications in energy-efficient scheduling in data centers,\nKhuller, Li, and Saha introduced the {\\em machine activation} problem as a\ngeneralization of the classical optimization problems of set cover and load\nbalancing on unrelated machines. In this problem, a set of $n$ jobs have to be\ndistributed among a set of $m$ (unrelated) machines, given the processing time\nof each job on each machine, where each machine has a startup cost. The goal is\nto produce a schedule of minimum total startup cost subject to a constraint\n$\\bf L$ on its makespan. While Khuller {\\em et al} considered the offline\nversion of this problem, a typical scenario in scheduling is one where jobs\narrive online and have to be assigned to a machine immediately on arrival. We\ngive an $(O(\\log (mn)\\log m), O(\\log m))$-competitive randomized online\nalgorithm for this problem, i.e. the schedule produced by our algorithm has a\nmakespan of $O({\\bf L} \\log m)$ with high probability, and a total expected\nstartup cost of $O(\\log (mn)\\log m)$ times that of an optimal offline schedule\nwith makespan $\\bf L$. The competitive ratios of our algorithm are (almost)\noptimal.\n  Our algorithms use the online primal dual framework introduced by Alon {\\em\net al} for the online set cover problem, and subsequently developed further by\nBuchbinder, Naor, and co-authors. To the best of our knowledge, all previous\napplications of this framework have been to linear programs (LPs) with either\npacking or covering constraints. One novelty of our application is that we use\nthis framework for a mixed LP that has both covering and packing constraints.\nWe hope that the algorithmic techniques developed in this paper to\nsimultaneously handle packing and covering constraints will be useful for\nsolving other online optimization problems as well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4836v1", 
    "title": "On a New Method of Storing a Variable Size Array", 
    "arxiv-id": "1203.4836v1", 
    "author": "Anatolijs Gorbunovs", 
    "publish": "2012-03-21T21:15:44Z", 
    "summary": "This paper introduces a new data structure, log_vector, with the following\nproperties: constant time random access to individual elements; constant time\nelement addition to the end; constant time element removal from the end;\nconstant time empty data structure creation; amortized constant space per\nindividual elements; constant additional space used."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4900v1", 
    "title": "Single pass sparsification in the streaming model with edge deletions", 
    "arxiv-id": "1203.4900v1", 
    "author": "Ian Post", 
    "publish": "2012-03-22T07:45:13Z", 
    "summary": "In this paper we give a construction of cut sparsifiers of Benczur and Karger\nin the {\\em dynamic} streaming setting in a single pass over the data stream.\nPrevious constructions either required multiple passes or were unable to handle\nedge deletions. We use $\\tilde{O}(1/\\e^2)$ time for each stream update and\n$\\tilde{O}(n/\\e^2)$ time to construct a sparsifier. Our $\\e$-sparsifiers have\n$O(n\\log^3 n/\\e^2)$ edges. The main tools behind our result are an application\nof sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivity\ntogether with a novel application of sampling with limited independence and\nsparse recovery to produce the edges of the sparsifier."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4920v1", 
    "title": "Work function algorithm can forget history without losing   competitiveness", 
    "arxiv-id": "1203.4920v1", 
    "author": "Livio Colussi", 
    "publish": "2012-03-22T08:56:12Z", 
    "summary": "The Work Function Algorithm is the most effective deterministic on-line\nalgorithm for the k-server problem. Koutsoupias and Papadimitriou proved WFA is\n(2k-1) competitive. However the best known implementation of WFA requires time\nO(i^2) to process request r_i and this makes WFA impractical for long sequences\nof requests. The O(i^2) time is spent to compute the work function on the whole\nhistory of past requests. In order to make constant the time to process a\nrequest, Rudec and Menger proposed to restrict the history to a moving window\nof fixed size. However WFA restricted to a moving window loses its\ncompetitiveness. Here we give a condition that allows WFA to forget the whole\nprevious history and restart from scratch without losing competitiveness.\nMoreover for most of the metric spaces of practical interest (finite or bounded\nspaces) there is a constant bound on the length of the history before the\ncondition is verified and this makes O(1) the time to process each request."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.5235v1", 
    "title": "A linear time algorithm for the next-to-shortest path problem on   undirected graphs with nonnegative edge lengths", 
    "arxiv-id": "1203.5235v1", 
    "author": "Yue-Li Wang", 
    "publish": "2012-03-23T13:33:47Z", 
    "summary": "For two vertices $s$ and $t$ in a graph $G=(V,E)$, the next-to-shortest path\nis an $st$-path which length is minimum amongst all $st$-paths strictly longer\nthan the shortest path length. In this paper we show that, when the graph is\nundirected and all edge lengths are nonnegative, the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nThis result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to\nallowing zero-length edges."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.6274v1", 
    "title": "Small $\\ell$-edge-covers in $k$-connected graphs", 
    "arxiv-id": "1203.6274v1", 
    "author": "Zeev Nutov", 
    "publish": "2012-03-28T14:09:49Z", 
    "summary": "Let $G=(V,E)$ be a $k$-edge-connected graph with edge costs $\\{c(e):e \\in\nE\\}$ and let $1 \\leq \\ell \\leq k-1$. We show by a simple and short proof, that\n$G$ contains an $\\ell$-edge cover $I$ such that: $c(I) \\leq \\frac{\\ell}{k}c(E)$\nif $G$ is bipartite, or if $\\ell |V|$ is even, or if $|E| \\geq \\frac{k|V|}{2}\n+\\frac{k}{2\\ell}$; otherwise, $c(I) \\leq (\\frac{\\ell}{k}+\\frac{1}{k|V|})c(E)$.\nThe particular case $\\ell=k-1$ and unit costs already includes a result of\nCheriyan and Thurimella, that $G$ contains a $(k-1)$-edge-cover of size\n$|E|-\\lfloor |V|/2 \\rfloor$. Using our result, we slightly improve the\napproximation ratios for the {\\sf $k$-Connected Subgraph} problem (the\nnode-connectivity version) with uniform and $\\beta$-metric costs. We then\nconsider the dual problem of finding a spanning subgraph of maximum\nconnectivity $k^*$ with a prescribed number of edges. We give an algorithm that\ncomputes a $(k^*-1)$-connected subgraph, which is tight, since the problem is\nNP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.6695v2", 
    "title": "Online Mixed Packing and Covering", 
    "arxiv-id": "1203.6695v2", 
    "author": "Lisa Fleischer", 
    "publish": "2012-03-30T01:37:11Z", 
    "summary": "In many problems, the inputs arrive over time, and must be dealt with\nirrevocably when they arrive. Such problems are online problems. A common\nmethod of solving online problems is to first solve the corresponding linear\nprogram, and then round the fractional solution online to obtain an integral\nsolution.\n  We give algorithms for solving linear programs with mixed packing and\ncovering constraints online. We first consider mixed packing and covering\nlinear programs, where packing constraints are given offline and covering\nconstraints are received online. The objective is to minimize the maximum\nmultiplicative factor by which any packing constraint is violated, while\nsatisfying the covering constraints. No prior sublinear competitive algorithms\nare known for this problem. We give the first such --- a\npolylogarithmic-competitive algorithm for solving mixed packing and covering\nlinear programs online. We also show a nearly tight lower bound.\n  Our techniques for the upper bound use an exponential penalty function in\nconjunction with multiplicative updates. While exponential penalty functions\nare used previously to solve linear programs offline approximately, offline\nalgorithms know the constraints beforehand and can optimize greedily. In\ncontrast, when constraints arrive online, updates need to be more complex.\n  We apply our techniques to solve two online fixed-charge problems with\ncongestion. These problems are motivated by applications in machine scheduling\nand facility location. The linear program for these problems is more\ncomplicated than mixed packing and covering, and presents unique challenges. We\nshow that our techniques combined with a randomized rounding procedure give\npolylogarithmic-competitive integral solutions. These problems generalize\nonline set-cover, for which there is a polylogarithmic lower bound. Hence, our\nresults are close to tight."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0144v1", 
    "title": "Everywhere-Sparse Spanners via Dense Subgraphs", 
    "arxiv-id": "1205.0144v1", 
    "author": "Robert Krauthgamer", 
    "publish": "2012-05-01T11:52:11Z", 
    "summary": "The significant progress in constructing graph spanners that are sparse\n(small number of edges) or light (low total weight) has skipped spanners that\nare everywhere-sparse (small maximum degree). This disparity is in line with\nother network design problems, where the maximum-degree objective has been a\nnotorious technical challenge. Our main result is for the Lowest Degree\n2-Spanner (LD2S) problem, where the goal is to compute a 2-spanner of an input\ngraph so as to minimize the maximum degree. We design a polynomial-time\nalgorithm achieving approximation factor $\\tilde O(\\Delta^{3-2\\sqrt{2}})\n\\approx \\tilde O(\\Delta^{0.172})$, where $\\Delta$ is the maximum degree of the\ninput graph. The previous $\\tilde O(\\Delta^{1/4})$ -approximation was proved\nnearly two decades ago by Kortsarz and Peleg [SODA 1994, SICOMP 1998].\n  Our main conceptual contribution is to establish a formal connection between\nLD2S and a variant of the Densest k-Subgraph (DkS) problem. Specifically, we\ndesign for both problems strong relaxations based on the Sherali-Adams linear\nprogramming (LP) hierarchy, and show that \"faithful\" randomized rounding of the\nDkS-variant can be used to round LD2S solutions. Our notion of faithfulness\nintuitively means that all vertices and edges are chosen with probability\nproportional to their LP value, but the precise formulation is more subtle.\n  Unfortunately, the best algorithms known for DkS use the Lov\\'asz-Schrijver\nLP hierarchy in a non-faithful way [Bhaskara, Charikar, Chlamtac, Feige, and\nVijayaraghavan, STOC 2010]. Our main technical contribution is to overcome this\nshortcoming, while still matching the gap that arises in random graphs by\nplanting a subgraph with same log-density."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0175v1", 
    "title": "Approximating Sparse Covering Integer Programs Online", 
    "arxiv-id": "1205.0175v1", 
    "author": "Viswanath Nagarajan", 
    "publish": "2012-05-01T14:39:11Z", 
    "summary": "A covering integer program (CIP) is a mathematical program of the form: min\n{c^T x : Ax >= 1, 0 <= x <= u, x integer}, where A is an m x n matrix, and c\nand u are n-dimensional vectors, all having non-negative entries. In the online\nsetting, the constraints (i.e., the rows of the constraint matrix A) arrive\nover time, and the algorithm can only increase the coordinates of vector x to\nmaintain feasibility. As an intermediate step, we consider solving the covering\nlinear program (CLP) online, where the integrality requirement on x is dropped.\n  Our main results are (a) an O(log k)-competitive online algorithm for solving\nthe CLP, and (b) an O(log k log L)-competitive randomized online algorithm for\nsolving the CIP. Here k<=n and L<=m respectively denote the maximum number of\nnon-zero entries in any row and column of the constraint matrix A. By a result\nof Feige and Korman, this is the best possible for polynomial-time online\nalgorithms, even in the special case of set cover."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0458v2", 
    "title": "Better Balance by Being Biased: A 0.8776-Approximation for Max Bisection", 
    "arxiv-id": "1205.0458v2", 
    "author": "Konstantinos Georgiou", 
    "publish": "2012-05-02T15:19:29Z", 
    "summary": "Recently Raghavendra and Tan (SODA 2012) gave a 0.85-approximation algorithm\nfor the Max Bisection problem. We improve their algorithm to a\n0.8776-approximation. As Max Bisection is hard to approximate within\n$\\alpha_{GW} + \\epsilon \\approx 0.8786$ under the Unique Games Conjecture\n(UGC), our algorithm is nearly optimal. We conjecture that Max Bisection is\napproximable within $\\alpha_{GW}-\\epsilon$, i.e., the bisection constraint\n(essentially) does not make Max Cut harder.\n  We also obtain an optimal algorithm (assuming the UGC) for the analogous\nvariant of Max 2-Sat. Our approximation ratio for this problem exactly matches\nthe optimal approximation ratio for Max 2-Sat, i.e., $\\alpha_{LLZ} + \\epsilon\n\\approx 0.9401$, showing that the bisection constraint does not make Max 2-Sat\nharder. This improves on a 0.93-approximation for this problem due to\nRaghavendra and Tan."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0477v3", 
    "title": "Order-preserving Renaming in Synchronous Message Passing Systems with   Byzantine Faults", 
    "arxiv-id": "1205.0477v3", 
    "author": "Luis Rodrigues", 
    "publish": "2012-05-02T16:04:31Z", 
    "summary": "Renaming is a fundamental problem in distributed computing, which consists of\na set of processes picking distinct names from a given namespace. The paper\npresents algorithms that solve order-preserving renaming in synchronous message\npassing systems with Byzantine processes. To the best of our knowledge, this\nwork is the first to address order-preserving renaming in the given model.\nAlthough this problem can be solved by using consensus, it is known that\nrenaming is \"weaker\" than consensus, therefore we are mainly concerned with the\nefficiency of performing renaming and make three contributions in this\ndirection. We present an order-preserving renaming algorithm for $N > 3t$ with\ntarget namespace of size $N+t-1$ and logarithmic step complexity (where $N$ is\nthe number of processes and $t$ is an upper bound on the number of faults).\nSimilarly to the existing crash-tolerant solution, our algorithm employs the\nideas from the approximate agreement problem. We show that our algorithm has\nconstant step complexity if $N>t^2+2t$ and achieves tight namespace of size\n$N$. Finally, we present an algorithm that solves order-preserving renaming in\njust 2 communication steps, if $N > 2t^2 + t$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0581v1", 
    "title": "Scalable Mechanisms for Rational Secret Sharing", 
    "arxiv-id": "1205.0581v1", 
    "author": "Jared Saia", 
    "publish": "2012-05-02T23:28:01Z", 
    "summary": "We consider the classical secret sharing problem in the case where all agents\nare selfish but rational. In recent work, Kol and Naor show that, when there\nare two players, in the non-simultaneous communication model, i.e. when rushing\nis possible, there is no Nash equilibrium that ensures both players learn the\nsecret. However, they describe a mechanism for this problem, for any number of\nplayers, that is an epsilon-Nash equilibrium, in that no player can gain more\nthan epsilon utility by deviating from it. Unfortunately, the Kol and Naor\nmechanism, and, to the best of our knowledge, all previous mechanisms for this\nproblem require each agent to send O(n) messages in expectation, where n is the\nnumber of agents. This may be problematic for some applications of rational\nsecret sharing such as secure multi-party computation and simulation of a\nmediator.\n  We address this issue by describing mechanisms for rational secret sharing\nthat are designed for large n. Both of our results hold for n > 2, and are Nash\nequilbria, rather than just epsilon-Nash equilbria.\n  Our first result is a mechanism for n-out-of-n rational secret sharing that\nis scalable in the sense that it requires each agent to send only an expected\nO(log n) bits. Moreover, the latency of this mechanism is O(log n) in\nexpectation, compared to O(n) expected latency for the Kol and Naor result. Our\nsecond result is a mechanism for a relaxed variant of rational m-out-of-n\nsecret sharing where m = Theta(n). It requires each processor to send O(log n)\nbits and has O(log n) latency. Both of our mechanisms are non-cryptographic,\nand are not susceptible to backwards induction."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.0974v1", 
    "title": "Scheduling Unrelated Machines of Few Different Types", 
    "arxiv-id": "1205.0974v1", 
    "author": "Andreas Wiese", 
    "publish": "2012-05-04T15:12:46Z", 
    "summary": "A very well-known machine model in scheduling allows the machines to be\nunrelated, modelling jobs that might have different characteristics on each\nmachine. Due to its generality, many optimization problems of this form are\nvery difficult to tackle and typically APX-hard. However, in many applications\nthe number of different types of machines, such as processor cores, GPUs, etc.\nis very limited. In this paper, we address this point and study the assignment\nof jobs to unrelated machines in the case that each machine belongs to one of a\nfixed number of types and the machines of each type are identical. We present\npolynomial time approximation schemes (PTASs) for minimizing the makespan for\nmultidimensional jobs with a fixed number of dimensions and for minimizing the\nL_p-norm. In particular, our results subsume and generalize the existing PTASs\nfor a constant number of unrelated machines and for an arbitrary number of\nidentical machines for these problems. We employ a number of techniques which\ngo beyond the previously known results, including a new counting argument and a\nmethod for making the concept of sparse extreme point solutions usable for a\nconvex program."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1114v1", 
    "title": "A Durable Flash Memory Search Tree", 
    "arxiv-id": "1205.1114v1", 
    "author": "Kevin Wortman", 
    "publish": "2012-05-05T08:41:24Z", 
    "summary": "We consider the task of optimizing the B-tree data structure, used\nextensively in operating systems and databases, for sustainable usage on\nmulti-level flash memory. Empirical evidence shows that this new flash memory\ntree, or FM Tree, extends the operational lifespan of each block of flash\nmemory by a factor of roughly 27 to 70 times, while still supporting\nlogarithmic-time search tree operations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1195v1", 
    "title": "Sequential-Access FM-Indexes", 
    "arxiv-id": "1205.1195v1", 
    "author": "Travis Gagie", 
    "publish": "2012-05-06T09:26:45Z", 
    "summary": "Previous authors have shown how to build FM-indexes efficiently in external\nmemory, but querying them efficiently remains an open problem. Searching\nna\\\"{i}vely for a pattern $P$ requires (\\Theta (|P|)) random access. In this\npaper we show how, by storing a few small auxiliary tables, we can access data\nonly in the order in which they appear on disk, which should be faster."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1281v5", 
    "title": "LP-rounding Algorithms for the Fault-Tolerant Facility Placement Problem", 
    "arxiv-id": "1205.1281v5", 
    "author": "Marek Chrobak", 
    "publish": "2012-05-07T04:11:15Z", 
    "summary": "The Fault-Tolerant Facility Placement problem (FTFP) is a generalization of\nthe classic Uncapacitated Facility Location Problem (UFL). In FTFP we are given\na set of facility sites and a set of clients. Opening a facility at site $i$\ncosts $f_i$ and connecting client $j$ to a facility at site $i$ costs $d_{ij}$.\nWe assume that the connection costs (distances) $d_{ij}$ satisfy the triangle\ninequality. Multiple facilities can be opened at any site. Each client $j$ has\na demand $r_j$, which means that it needs to be connected to $r_j$ different\nfacilities (some of which could be located on the same site). The goal is to\nminimize the sum of facility opening cost and connection cost.\n  The main result of this paper is a 1.575-approximation algorithm for FTFP,\nbased on LP-rounding. The algorithm first reduces the demands to values\npolynomial in the number of sites. Then it uses a technique that we call\nadaptive partitioning, which partitions the instance by splitting clients into\nunit demands and creating a number of (not yet opened) facilities at each site.\nIt also partitions the optimal fractional solution to produce a fractional\nsolution for this new instance. The partitioned instance satisfies a number of\nproperties that allow us to exploit existing LP-rounding methods for UFL to\nround our partitioned solution to an integral solution, preserving the\napproximation ratio. In particular, our 1.575-approximation algorithm is based\non the ideas from the 1.575-approximation algorithm for UFL by Byrka et al.,\nwith changes necessary to satisfy the fault-tolerance requirement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1312v1", 
    "title": "Converting online algorithms to local computation algorithms", 
    "arxiv-id": "1205.1312v1", 
    "author": "Ning Xie", 
    "publish": "2012-05-07T08:02:48Z", 
    "summary": "We propose a general method for converting online algorithms to local\ncomputation algorithms by selecting a random permutation of the input, and\nsimulating running the online algorithm. We bound the number of steps of the\nalgorithm using a query tree, which models the dependencies between queries. We\nimprove previous analyses of query trees on graphs of bounded degree, and\nextend the analysis to the cases where the degrees are distributed binomially,\nand to a special case of bipartite graphs.\n  Using this method, we give a local computation algorithm for maximal matching\nin graphs of bounded degree, which runs in time and space O(log^3 n).\n  We also show how to convert a large family of load balancing algorithms\n(related to balls and bins problems) to local computation algorithms. This\ngives several local load balancing algorithms which achieve the same\napproximation ratios as the online algorithms, but run in O(log n) time and\nspace.\n  Finally, we modify existing local computation algorithms for hypergraph\n2-coloring and k-CNF and use our improved analysis to obtain better time and\nspace bounds, of O(log^4 n), removing the dependency on the maximal degree of\nthe graph from the exponent."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1373v2", 
    "title": "Quasi-Polynomial Local Search for Restricted Max-Min Fair Allocation", 
    "arxiv-id": "1205.1373v2", 
    "author": "Ola Svensson", 
    "publish": "2012-05-07T13:18:20Z", 
    "summary": "The restricted max-min fair allocation problem (also known as the restricted\nSanta Claus problem) is one of few problems that enjoys the intriguing status\nof having a better estimation algorithm than approximation algorithm. Indeed,\nAsadpour et al. proved that a certain configuration LP can be used to estimate\nthe optimal value within a factor ${1}/{(4+\\epsilon)}$, for any $\\epsilon>0$,\nbut at the same time it is not known how to efficiently find a solution with a\ncomparable performance guarantee.\n  A natural question that arises from their work is if the difference between\nthese guarantees is inherent or because of a lack of suitable techniques. We\naddress this problem by giving a quasi-polynomial approximation algorithm with\nthe mentioned performance guarantee. More specifically, we modify the local\nsearch of Asadpour et al. and provide a novel analysis that lets us\nsignificantly improve the bound on its running time: from $2^{O(n)}$ to\n$n^{O(\\log n)}$. Our techniques also have the interesting property that\nalthough we use the rather complex configuration LP in the analysis, we never\nactually solve it and therefore the resulting algorithm is purely\ncombinatorial."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1477v1", 
    "title": "Approximation Algorithms for Online Weighted Rank Function Maximization   under Matroid Constraints", 
    "arxiv-id": "1205.1477v1", 
    "author": "Mohit Singh", 
    "publish": "2012-05-07T18:42:05Z", 
    "summary": "Consider the following online version of the submodular maximization problem\nunder a matroid constraint: We are given a set of elements over which a matroid\nis defined. The goal is to incrementally choose a subset that remains\nindependent in the matroid over time. At each time, a new weighted rank\nfunction of a different matroid (one per time) over the same elements is\npresented; the algorithm can add a few elements to the incrementally\nconstructed set, and reaps a reward equal to the value of the new weighted rank\nfunction on the current set. The goal of the algorithm as it builds this\nindependent set online is to maximize the sum of these (weighted rank) rewards.\nAs in regular online analysis, we compare the rewards of our online algorithm\nto that of an offline optimum, namely a single independent set of the matroid\nthat maximizes the sum of the weighted rank rewards that arrive over time. This\nproblem is a natural extension of two well-studied streams of earlier work: the\nfirst is on online set cover algorithms (in particular for the max coverage\nversion) while the second is on approximately maximizing submodular functions\nunder a matroid constraint.\n  In this paper, we present the first randomized online algorithms for this\nproblem with poly-logarithmic competitive ratio. To do this, we employ the LP\nformulation of a scaled reward version of the problem. Then we extend a\nweighted-majority type update rule along with uncrossing properties of tight\nsets in the matroid polytope to find an approximately optimal fractional LP\nsolution. We use the fractional solution values as probabilities for a online\nrandomized rounding algorithm. To show that our rounding produces a\nsufficiently large reward independent set, we prove and use new covering\nproperties for randomly rounded fractional solutions in the matroid polytope\nthat may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.1758v3", 
    "title": "Faster Algorithms for Privately Releasing Marginals", 
    "arxiv-id": "1205.1758v3", 
    "author": "Salil Vadhan", 
    "publish": "2012-05-08T17:43:11Z", 
    "summary": "We study the problem of releasing $k$-way marginals of a database $D \\in\n(\\{0,1\\}^d)^n$, while preserving differential privacy. The answer to a $k$-way\nmarginal query is the fraction of $D$'s records $x \\in \\{0,1\\}^d$ with a given\nvalue in each of a given set of up to $k$ columns. Marginal queries enable a\nrich class of statistical analyses of a dataset, and designing efficient\nalgorithms for privately releasing marginal queries has been identified as an\nimportant open problem in private data analysis (cf. Barak et. al., PODS '07).\n  We give an algorithm that runs in time $d^{O(\\sqrt{k})}$ and releases a\nprivate summary capable of answering any $k$-way marginal query with at most\n$\\pm .01$ error on every query as long as $n \\geq d^{O(\\sqrt{k})}$. To our\nknowledge, ours is the first algorithm capable of privately releasing marginal\nqueries with non-trivial worst-case accuracy guarantees in time substantially\nsmaller than the number of $k$-way marginal queries, which is $d^{\\Theta(k)}$\n(for $k \\ll d$)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.2285v2", 
    "title": "Complex-Demand Knapsack Problems and Incentives in AC Power Systems", 
    "arxiv-id": "1205.2285v2", 
    "author": "Chi-Kin Chau", 
    "publish": "2012-05-10T14:59:58Z", 
    "summary": "We consider AC electrical systems where each electrical device has a power\ndemand expressed as a complex number, and there is a limit on the magnitude of\ntotal power supply. Motivated by this scenario, we introduce the complex-demand\nknapsack problem (C-KP), a new variation of the traditional knapsack problem,\nwhere each item is associated with a demand as a complex number, rather than a\nreal number often interpreted as weight or size of the item. While keeping the\nsame goal as to maximize the sum of values of the selected items, we put the\ncapacity limit on the magnitude of the sum of satisfied demands. For C-KP, we\nprove its inapproximability by FPTAS (unless P = NP), as well as presenting a\n(1/2-epsilon)-approximation algorithm. Furthermore, we investigate the selfish\nmulti-agent setting where each agent is in charge of one item, and an agent may\nmisreport the demand and value of his item for his own interest. We show a\nsimple way to adapt our approximation algorithm to be monotone, which is\nsufficient for the existence of incentive compatible payments such that no\nagent has an incentive to misreport. Our results shed insight on the design of\nmulti-agent systems for smart grid."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.2766v2", 
    "title": "Optimal Listing of Cycles and st-Paths in Undirected Graphs", 
    "arxiv-id": "1205.2766v2", 
    "author": "Gustavo Sacomoto", 
    "publish": "2012-05-12T11:12:10Z", 
    "summary": "We present the first optimal algorithm for the classical problem of listing\nall the cycles in an undirected graph. We exploit their properties so that the\ntotal cost is the time taken to read the input graph plus the time to list the\noutput, namely, the edges in each of the cycles. The algorithm uses a reduction\nto the problem of listing all the paths from a vertex s to a vertex t which we\nalso solve optimally."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.2888v1", 
    "title": "A Comparative Study on the Performance of Permutation Algorithms", 
    "arxiv-id": "1205.2888v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-05-13T17:47:51Z", 
    "summary": "Permutation is the different arrangements that can be made with a given\nnumber of things taking some or all of them at a time. The notation P(n,r) is\nused to denote the number of permutations of n things taken r at a time.\nPermutation is used in various fields such as mathematics, group theory,\nstatistics, and computing, to solve several combinatorial problems such as the\njob assignment problem and the traveling salesman problem. In effect,\npermutation algorithms have been studied and experimented for many years now.\nBottom-Up, Lexicography, and Johnson-Trotter are three of the most popular\npermutation algorithms that emerged during the past decades. In this paper, we\nare implementing three of the most eminent permutation algorithms, they are\nrespectively: Bottom-Up, Lexicography, and Johnson-Trotter algorithms. The\nimplementation of each algorithm will be carried out using two different\napproaches: brute-force and divide and conquer. The algorithms codes will be\ntested using a computer simulation tool to measure and evaluate the execution\ntime between the different implementations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.3397v1", 
    "title": "1.85 Approximation for Min-Power Strong Connectivity", 
    "arxiv-id": "1205.3397v1", 
    "author": "Gruia Calinescu", 
    "publish": "2012-05-15T14:46:01Z", 
    "summary": "Given a directed simple graph G=(V,E) and a nonnegative-valued cost function\nthe power of a vertex u in a directed spanning subgraph H is given by the\nmaximum cost of an arcs of H exiting u. The power of H is the sum of the power\nof its vertices.\n  Power Assignment seeks to minimize the power of H while H satisfies some\nconnectivity constraint. In this paper, we assume E is bidirected (for every\ndirected edge e in E, the opposite edge exists and has the same cost), while H\nis required to be strongly connected. This is the original power assignment\nproblem introduced by Chen and Huang in 1989, who proved that bidirected\nminimum spanning tree has approximation ratio at most 2 (this is tight). In\nApprox 2010, we introduced a Greedy approximation algorithm and claimed a ratio\nof 1.992. Here we improve the analysis to 1.85.\n  The proof also shows that a natural linear programming relaxation, introduced\nby us in 2012, has the same 1.85 integrality gap."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.3518v2", 
    "title": "Lower Bounds for Adaptive Sparse Recovery", 
    "arxiv-id": "1205.3518v2", 
    "author": "David P. Woodruff", 
    "publish": "2012-05-15T21:38:53Z", 
    "summary": "We give lower bounds for the problem of stable sparse recovery from\n/adaptive/ linear measurements. In this problem, one would like to estimate a\nvector $x \\in \\R^n$ from $m$ linear measurements $A_1x,..., A_mx$. One may\nchoose each vector $A_i$ based on $A_1x,..., A_{i-1}x$, and must output $x*$\nsatisfying |x* - x|_p \\leq (1 + \\epsilon) \\min_{k\\text{-sparse} x'} |x - x'|_p\nwith probability at least $1-\\delta>2/3$, for some $p \\in \\{1,2\\}$. For $p=2$,\nit was recently shown that this is possible with $m = O(\\frac{1}{\\epsilon}k\n\\log \\log (n/k))$, while nonadaptively it requires $\\Theta(\\frac{1}{\\epsilon}k\n\\log (n/k))$. It is also known that even adaptively, it takes $m =\n\\Omega(k/\\epsilon)$ for $p = 2$. For $p = 1$, there is a non-adaptive upper\nbound of $\\tilde{O}(\\frac{1}{\\sqrt{\\epsilon}} k\\log n)$. We show:\n  * For $p=2$, $m = \\Omega(\\log \\log n)$. This is tight for $k = O(1)$ and\nconstant $\\epsilon$, and shows that the $\\log \\log n$ dependence is correct.\n  * If the measurement vectors are chosen in $R$ \"rounds\", then $m = \\Omega(R\n\\log^{1/R} n)$. For constant $\\epsilon$, this matches the previously known\nupper bound up to an O(1) factor in $R$.\n  * For $p=1$, $m = \\Omega(k/(\\sqrt{\\epsilon} \\cdot \\log k/\\epsilon))$. This\nshows that adaptivity cannot improve more than logarithmic factors, providing\nthe analog of the $m = \\Omega(k/\\epsilon)$ bound for $p = 2$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.3605v1", 
    "title": "On Min-Power Steiner Tree", 
    "arxiv-id": "1205.3605v1", 
    "author": "Fabrizio Grandoni", 
    "publish": "2012-05-16T09:16:39Z", 
    "summary": "In the classical (min-cost) Steiner tree problem, we are given an\nedge-weighted undirected graph and a set of terminal nodes. The goal is to\ncompute a min-cost tree S which spans all terminals. In this paper we consider\nthe min-power version of the problem, which is better suited for wireless\napplications. Here, the goal is to minimize the total power consumption of\nnodes, where the power of a node v is the maximum cost of any edge of S\nincident to v. Intuitively, nodes are antennas (part of which are terminals\nthat we need to connect) and edge costs define the power to connect their\nendpoints via bidirectional links (so as to support protocols with ack\nmessages). Differently from its min-cost counterpart, min-power Steiner tree is\nNP-hard even in the spanning tree case, i.e. when all nodes are terminals.\nSince the power of any tree is within once and twice its cost, computing a rho\n\\leq ln(4)+eps [Byrka et al.'10] approximate min-cost Steiner tree provides a\n2rho<2.78 approximation for the problem. For min-power spanning tree the same\napproach provides a 2 approximation, which was improved to 5/3+eps with a\nnon-trivial approach in [Althaus et al.'06]. Here we present an improved\napproximation algorithm for min-power Steiner tree. Our result is based on two\nmain ingredients. We prove the first decomposition theorem for min-power\nSteiner tree, in the spirit of analogous structural results for min-cost\nSteiner tree and min-power spanning tree. Based on this theorem, we define a\nproper LP relaxation, that we exploit within the iterative randomized rounding\nframework in [Byrka et al.'10]. A careful analysis provides a 3ln\n4-9/4+eps<1.91 approximation factor. The same approach gives an improved\n1.5+eps approximation for min-power spanning tree as well, matching the\napproximation factor in [Nutov and Yaroshevitch'09] for the special case of\nmin-power spanning tree with edge weights in {0,1}."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.3754v1", 
    "title": "Scheduling and allocation algorithm for an elliptic filter", 
    "arxiv-id": "1205.3754v1", 
    "author": "Sangeetha Marikkannan", 
    "publish": "2012-02-23T15:23:17Z", 
    "summary": "A new evolutionary algorithm for scheduling and allocation algorithm is\ndeveloped for an elliptic filter. The elliptic filter is scheduled and\nallocated in the proposed work which is then compared with the different\nscheduling algorithms like As Soon As Possible algorithm, As Late As Possible\nalgorithm, Mobility Based Shift algorithm, FDLS, FDS and MOGS. In this paper\nexecution time and resource utilization is calculated using different\nscheduling algorithm for an Elliptic Filter and reported that proposed\nScheduling and Allocation increases the speed of operation by reducing the\ncontrol step. The proposed work to analyse the magnitude, phase and noise\nresponses for different scheduling algorithm in an elliptic filter."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.4363v1", 
    "title": "Detours in Scope-Based Route Planning", 
    "arxiv-id": "1205.4363v1", 
    "author": "Ondrej Mori\u0161", 
    "publish": "2012-05-19T22:00:51Z", 
    "summary": "We study a dynamic scenario of the static route planning problem in road\nnetworks. Particularly, we put accent on the most practical dynamic case -\nincreased edge weights (up to infinity). We show how to enhance the scope-based\nroute planning approach presented at ESA'11 to intuitively by-pass closures by\ndetours. Three variants of a detour \"admissibility\" are presented - from a\nsimple one with straightforward implementation through its enhanced version to\na full and very complex variant variant which always returns an optimal detour."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.4968v1", 
    "title": "SubGraD- An Approach for Subgraph Detection", 
    "arxiv-id": "1205.4968v1", 
    "author": "S. Nigam", 
    "publish": "2012-05-22T16:28:33Z", 
    "summary": "A new approach of graph matching is introduced in this paper, which\nefficiently solves the problem of graph isomorphism and subgraph isomorphism.\nIn this paper we are introducing a new approach called SubGraD, for query graph\ndetection in source graph. Firstly consider the model graph (query graph) and\nmake the possible sets called model sets starting from the chosen initial node\nor starter. Similarly, for the source graph (reference graph), all the possible\nsets called reference sets could be made. Our aim is to make the reference set\non the basis of the model set. If it is possible to make the reference set,\nthen it is said that query graph has been detected in the source graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.6787v1", 
    "title": "Lyndon Words and Short Superstrings", 
    "arxiv-id": "1205.6787v1", 
    "author": "Marcin Mucha", 
    "publish": "2012-05-30T19:44:17Z", 
    "summary": "In the Shortest-Superstring problem, we are given a set of strings S and want\nto find a string that contains all strings in S as substrings and has minimum\nlength. This is a classical problem in approximation and the best known\napproximation factor is 2 1/2, given by Sweedyk in 1999. Since then no\nimprovement has been made, howerever two other approaches yielding a 2\n1/2-approximation algorithms have been proposed by Kaplan et al. and recently\nby Paluch et al., both based on a reduction to maximum asymmetric TSP path\n(Max-ATSP-Path) and structural results of Breslauer et al.\n  In this paper we give an algorithm that achieves an approximation ratio of 2\n11/23, breaking through the long-standing bound of 2 1/2.\n  We use the standard reduction of Shortest-Superstring to Max-ATSP-Path. The\nnew, somewhat surprising, algorithmic idea is to take the better of the two\nsolutions obtained by using: (a) the currently best 2/3-approximation algorithm\nfor Max-ATSP-Path and (b) a naive cycle-cover based 1/2-approximation\nalgorithm. To prove that this indeed results in an improvement, we further\ndevelop a theory of string overlaps, extending the results of Breslauer et al.\nThis theory is based on the novel use of Lyndon words, as a substitute for\ngeneric unbordered rotations and critical factorizations, as used by Breslauer\net al."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1205.6960v2", 
    "title": "Minimizing Movement: Fixed-Parameter Tractability", 
    "arxiv-id": "1205.6960v2", 
    "author": "D\u00e1niel Marx", 
    "publish": "2012-05-31T11:23:32Z", 
    "summary": "We study an extensive class of movement minimization problems which arise\nfrom many practical scenarios but so far have little theoretical study. In\ngeneral, these problems involve planning the coordinated motion of a collection\nof agents (representing robots, people, map labels, network messages, etc.) to\nachieve a global property in the network while minimizing the maximum or\naverage movement (expended energy). The only previous theoretical results about\nthis class of problems are about approximation, and mainly negative: many\nmovement problems of interest have polynomial inapproximability. Given that the\nnumber of mobile agents is typically much smaller than the complexity of the\nenvironment, we turn to fixed-parameter tractability. We characterize the\nboundary between tractable and intractable movement problems in a very general\nset up: it turns out the complexity of the problem fundamentally depends on the\ntreewidth of the minimal configurations. Thus the complexity of a particular\nproblem can be determined by answering a purely combinatorial question. Using\nour general tools, we determine the complexity of several concrete problems and\nfortunately show that many movement problems of interest can be solved\nefficiently."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1206.0580v1", 
    "title": "O(1) Delta Component Computation Technique for the Quadratic Assignment   Problem", 
    "arxiv-id": "1206.0580v1", 
    "author": "Yuri Zorin", 
    "publish": "2012-06-04T11:06:27Z", 
    "summary": "The paper describes a novel technique that allows to reduce by half the\nnumber of delta values that were required to be computed with complexity O(N)\nin most of the heuristics for the quadratic assignment problem. Using the\ncorrelation between the old and new delta values, obtained in this work, a new\nformula of complexity O(1) is proposed. Found result leads up to 25%\nperformance increase in such well-known algorithms as Robust Tabu Search and\nothers based on it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1206.0594v6", 
    "title": "Simple and Deterministic Matrix Sketching", 
    "arxiv-id": "1206.0594v6", 
    "author": "Edo Liberty", 
    "publish": "2012-06-04T12:16:07Z", 
    "summary": "We adapt a well known streaming algorithm for approximating item frequencies\nto the matrix sketching setting. The algorithm receives the rows of a large\nmatrix $A \\in \\R^{n \\times m}$ one after the other in a streaming fashion. It\nmaintains a sketch matrix $B \\in \\R^ {1/\\eps \\times m}$ such that for any unit\nvector $x$ [\\|Ax\\|^2 \\ge \\|Bx\\|^2 \\ge \\|Ax\\|^2 - \\eps \\|A\\|_{f}^2 \\.] Sketch\nupdates per row in $A$ require $O(m/\\eps^2)$ operations in the worst case. A\nslight modification of the algorithm allows for an amortized update time of\n$O(m/\\eps)$ operations per row. The presented algorithm stands out in that it\nis: deterministic, simple to implement, and elementary to prove. It also\nexperimentally produces more accurate sketches than widely used approaches\nwhile still being computationally competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1206.2269v2", 
    "title": "Better bounds for matchings in the streaming model", 
    "arxiv-id": "1206.2269v2", 
    "author": "Michael Kapralov", 
    "publish": "2012-06-11T16:01:45Z", 
    "summary": "In this paper we present improved bounds for approximating maximum matchings\nin bipartite graphs in the streaming model. First, we consider the question of\nhow well maximum matching can be approximated in a single pass over the input\nusing \\tilde O(n)$ space, where $n$ is the number of vertices in the input\ngraph. Two natural variants of this problem have been considered in the\nliterature: (1) the edge arrival setting, where edges arrive in the stream and\n(2) the vertex arrival setting, where vertices on one side of the graph arrive\nin the stream together with all their incident edges. The latter setting has\nalso been studied extensively in the context of {\\em online algorithms}, where\neach arriving vertex has to either be matched irrevocably or discarded upon\narrival. In the online setting, the celebrated algorithm of\nKarp-Vazirani-Vazirani achieves a $1-1/e$ approximation. Despite the fact that\nthe streaming model is less restrictive in that the algorithm is not\nconstrained to match vertices irrevocably upon arrival, the best known\napproximation in the streaming model with vertex arrivals and $\\tilde O(n)$\nspace is the same factor of $1-1/e$.\n  We show that no single pass streaming algorithm that uses $\\tilde O(n)$ space\ncan achieve a better than $1-1/e$ approximation to maximum matching, even in\nthe vertex arrival setting. This leads to the striking conclusion that no\nsingle pass streaming algorithm can do better than online algorithms unless it\nuses significantly more than $\\tilde O(n)$ space. Additionally, our bound\nyields the best known impossibility result for approximating matchings in the\n{\\em edge arrival} model.\n  We also give a simple algorithm that achieves approximation ratio\n$1-e^{-k}k^{k-1}/(k-1)!=1-\\frac1{\\sqrt{2\\pi k}}+o(1/k)$ in $k$ passes in the\nvertex arrival model using linear space, improving upon previously best known\nconvergence."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.3483v2", 
    "title": "Constrained multilinear detection for faster functional motif discovery", 
    "arxiv-id": "1206.3483v2", 
    "author": "Ioannis Koutis", 
    "publish": "2012-06-15T14:40:31Z", 
    "summary": "The GRAPH MOTIF problem asks whether a given multiset of colors appears on a\nconnected subgraph of a vertex-colored graph. The fastest known parameterized\nalgorithm for this problem is based on a reduction to the $k$-Multilinear\nDetection (k-MlD) problem: the detection of multilinear terms of total degree k\nin polynomials presented as circuits. We revisit k-MLD and define k-CMLD, a\nconstrained version of it which reflects GRAPH MOTIF more faithfully. We then\ngive a fast algorithm for k-CMLD. As a result we obtain faster parameterized\nalgorithms for GRAPH MOTIF and variants of it."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.3511v1", 
    "title": "Comparison of Bucket Sort and RADIX Sort", 
    "arxiv-id": "1206.3511v1", 
    "author": "Panu Horsmalahti", 
    "publish": "2012-06-15T16:39:51Z", 
    "summary": "Bucket sort and RADIX sort are two well-known integer sorting algorithms.\nThis paper measures empirically what is the time usage and memory consumption\nfor different kinds of input sequences. The algorithms are compared both from a\ntheoretical standpoint but also on how well they do in six different use cases\nusing randomized sequences of numbers. The measurements provide data on how\ngood they are in different real-life situations.\n  It was found that bucket sort was faster than RADIX sort, but that bucket\nsort uses more memory in most cases. The sorting algorithms performed faster\nwith smaller integers. The RADIX sort was not quicker with already sorted\ninputs, but the bucket sort was."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.3603v1", 
    "title": "Approximation Algorithm for Non-Boolean MAX k-CSP", 
    "arxiv-id": "1206.3603v1", 
    "author": "Yury Makarychev", 
    "publish": "2012-06-15T22:40:40Z", 
    "summary": "In this paper, we present a randomized polynomial-time approximation\nalgorithm for k-CSPd. In k-CSPd, we are given a set of predicates of arity k\nover an alphabet of size d. Our goal is to find an assignment that maximizes\nthe number of satisfied constraints.\n  Our algorithm has approximation factor Omega(kd/d^k) (when k > \\Omega(log\nd)). This bound is asymptotically optimal assuming the Unique Games Conjecture.\nThe best previously known algorithm has approximation factor Omega(k log\nd/d^k).\n  We also give an approximation algorithm for the boolean MAX k-CSP2 problem\nwith a slightly improved approximation guarantee."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.5336v3", 
    "title": "Near-Optimal Online Multiselection in Internal and External Memory", 
    "arxiv-id": "1206.5336v3", 
    "author": "Jonathan Sorenson", 
    "publish": "2012-06-22T22:53:21Z", 
    "summary": "We introduce an online version of the multiselection problem, in which q\nselection queries are requested on an unsorted array of n elements. We provide\nthe first online algorithm that is 1-competitive with Kaligosi et al. [ICALP\n2005] in terms of comparison complexity. Our algorithm also supports online\nsearch queries efficiently.\n  We then extend our algorithm to the dynamic setting, while retaining online\nfunctionality, by supporting arbitrary insertions and deletions on the array.\nAssuming that the insertion of an element is immediately preceded by a search\nfor that element, we show that our dynamic online algorithm performs an optimal\nnumber of comparisons, up to lower order terms and an additive O(n) term.\n  For the external memory model, we describe the first online multiselection\nalgorithm that is O(1)-competitive. This result improves upon the work of\nSibeyn [Journal of Algorithms 2006] when q > m, where m is the number of blocks\nthat can be stored in main memory. We also extend it to support searches,\ninsertions, and deletions of elements efficiently."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.5392v2", 
    "title": "Metrical Service Systems with Multiple Servers", 
    "arxiv-id": "1206.5392v2", 
    "author": "Sundar Vishwanathan", 
    "publish": "2012-06-23T13:21:29Z", 
    "summary": "We study the problem of metrical service systems with multiple servers\n(MSSMS), which generalizes two well-known problems -- the $k$-server problem,\nand metrical service systems. The MSSMS problem is to service requests, each of\nwhich is an $l$-point subset of a metric space, using $k$ servers, with the\nobjective of minimizing the total distance traveled by the servers.\n  Feuerstein initiated a study of this problem by proving upper and lower\nbounds on the deterministic competitive ratio for uniform metric spaces. We\nimprove Feuerstein's analysis of the upper bound and prove that his algorithm\nachieves a competitive ratio of $k({{k+l}\\choose{l}}-1)$. In the randomized\nonline setting, for uniform metric spaces, we give an algorithm which achieves\na competitive ratio $\\mathcal{O}(k^3\\log l)$, beating the deterministic lower\nbound of ${{k+l}\\choose{l}}-1$. We prove that any randomized algorithm for\nMSSMS on uniform metric spaces must be $\\Omega(\\log kl)$-competitive. We then\nprove an improved lower bound of ${{k+2l-1}\\choose{k}}-{{k+l-1}\\choose{k}}$ on\nthe competitive ratio of any deterministic algorithm for $(k,l)$-MSSMS, on\ngeneral metric spaces. In the offline setting, we give a pseudo-approximation\nalgorithm for $(k,l)$-MSSMS on general metric spaces, which achieves an\napproximation ratio of $l$ using $kl$ servers. We also prove a matching\nhardness result, that a pseudo-approximation with less than $kl$ servers is\nunlikely, even for uniform metric spaces. For general metric spaces, we\nhighlight the limitations of a few popular techniques, that have been used in\nalgorithm design for the $k$-server problem and metrical service systems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.5959v1", 
    "title": "The Online Replacement Path Problem", 
    "arxiv-id": "1206.5959v1", 
    "author": "Marco Senatore", 
    "publish": "2012-06-26T11:38:44Z", 
    "summary": "We study a natural online variant of the replacement path problem. The\n\\textit{replacement path problem} asks to find for a given graph $G = (V,E)$,\ntwo designated vertices $s,t\\in V$ and a shortest $s$-$t$ path $P$ in $G$, a\n\\textit{replacement path} $P_e$ for every edge $e$ on the path $P$. The\nreplacement path $P_e$ is simply a shortest $s$-$t$ path in the graph, which\navoids the \\textit{failed} edge $e$. We adapt this problem to deal with the\nnatural scenario, that the edge which failed is not known at the time of\nsolution implementation. Instead, our problem assumes that the identity of the\nfailed edge only becomes available when the routing mechanism tries to cross\nthe edge. This situation is motivated by applications in distributed networks,\nwhere information about recent changes in the network is only stored locally,\nand fault-tolerant optimization, where an adversary tries to delay the\ndiscovery of the materialized scenario as much as possible. Consequently, we\ndefine the \\textit{online replacement path problem}, which asks to find a\nnominal $s$-$t$ path $Q$ and detours $Q_e$ for every edge on the path $Q$, such\nthat the worst-case arrival time at the destination is minimized. Our main\ncontribution is a label setting algorithm, which solves the problem in\nundirected graphs in time $O(m \\log n)$ and linear space for all sources and a\nsingle destination. We also present algorithms for extensions of the model to\nany bounded number of failed edges."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.6185v1", 
    "title": "Performance Evaluation of A Proposed Variant of Frequency Count (VFC)   List Accessing Algorithm", 
    "arxiv-id": "1206.6185v1", 
    "author": "Sangita Patel", 
    "publish": "2012-06-27T07:00:10Z", 
    "summary": "Frequency Count (FC) algorithm is considered as the static optimal algorithm\nfor the list accessing problem. In this paper, we have made a study of FC\nalgorithm and explore its limitation. Using the concept of weak look ahead, we\nhave proposed a novel Variant of Frequency Count (VFC) list accessing\nalgorithm. We have evaluated the performance of FC and our proposed VFC\nalgorithm experimentally using input data set from Calgary Corpus. Our\nexperiments show that for all request sequences and list generated from the\nabove data set VFC performs better than FC."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2012.08.008", 
    "link": "http://arxiv.org/pdf/1206.6187v1", 
    "title": "Some Novel Results From Analysis of Move To Front (MTF) List Accessing   Algorithm", 
    "arxiv-id": "1206.6187v1", 
    "author": "Burle Sharma", 
    "publish": "2012-06-27T07:13:16Z", 
    "summary": "List accessing problem has been studied as a problem of significant\ntheoretical and practical interest in the context of linear search. Various\nlist accessing algorithms have been proposed in the literature and their\nperformances have been analyzed theoretically and experimentally.\nMove-To-Front(MTF),Transpose (TRANS) and Frequency Count (FC) are the three\nprimitive and widely used list accessing algorithms. Most of the other list\naccessing algorithms are the variants of these three algorithms. As mentioned\nin the literature as an open problem, direct bounds on the behavior and\nperformance of these list accessing algorithms are needed to allow realistic\ncomparisons. MTF has been proved to be the best performing online algorithm\ntill date in the literature for real life inputs with locality of reference.\nMotivated by the above challenging research issue, in this paper, we have\ngenerated four types of input request sequences corresponding to real life\ninputs without locality of reference. Using these types of request sequences,\nwe have made an analytical study for evaluating the performance of MTF list\naccessing algorithm to obtain some novel and interesting theoretical results."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1206.6193v2", 
    "title": "Data Structures on Event Graphs", 
    "arxiv-id": "1206.6193v2", 
    "author": "Wolfgang Mulzer", 
    "publish": "2012-06-27T07:26:07Z", 
    "summary": "We investigate the behavior of data structures when the input and operations\nare generated by an event graph. This model is inspired by Markov chains. We\nare given a fixed graph G, whose nodes are annotated with operations of the\ntype insert, delete and query. The algorithm responds to the requests as it\nencounters them during a (random or adversarial) walk in G. We study the limit\nbehavior of such a walk and give an efficient algorithm for recognizing which\nstructures can be generated. We also give a near-optimal algorithm for\nsuccessor searching if the event graph is a cycle and the walk is adversarial.\nFor a random walk, the algorithm becomes optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1206.6982v2", 
    "title": "Optimal Dynamic Sequence Representations", 
    "arxiv-id": "1206.6982v2", 
    "author": "Yakov Nekrich", 
    "publish": "2012-06-29T10:43:34Z", 
    "summary": "We describe a data structure that supports access, rank and select queries,\nas well as symbol insertions and deletions, on a string $S[1,n]$ over alphabet\n$[1..\\sigma]$ in time $O(\\lg n/\\lg\\lg n)$, which is optimal even on binary\nsequences and in the amortized sense. Our time is worst-case for the queries\nand amortized for the updates. This complexity is better than the best previous\nones by a $\\Theta(1+\\lg\\sigma/\\lg\\lg n)$ factor. We also design a variant where\ntimes are worst-case, yet rank and updates take $O(\\lg n)$ time. Our structure\nuses $nH_0(S)+o(n\\lg\\sigma) + O(\\sigma\\lg n)$ bits, where $H_0(S)$ is the\nzero-order entropy of $S$. Finally, we pursue various extensions and\napplications of the result."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.0312v1", 
    "title": "A Model for Minimizing Active Processor Time", 
    "arxiv-id": "1208.0312v1", 
    "author": "Samir Khuller", 
    "publish": "2012-08-01T18:34:36Z", 
    "summary": "We introduce the following elementary scheduling problem. We are given a\ncollection of n jobs, where each job has an integer length as well as a set Ti\nof time intervals in which it can be feasibly scheduled. Given a parameter B,\nthe processor can schedule up to B jobs at a timeslot t so long as it is\n\"active\" at t. The goal is to schedule all the jobs in the fewest number of\nactive timeslots. The machine consumes a fixed amount of energy per active\ntimeslot, regardless of the number of jobs scheduled in that slot (as long as\nthe number of jobs is non-zero). In other words, subject to all units of each\njob being scheduled in its feasible region and at each slot at most B jobs\nbeing scheduled, we are interested in minimizing the total time during which\nthe machine is active. We present a linear time algorithm for the case where\njobs are unit length and each Ti is a single interval. For general Ti, we show\nthat the problem is NP-complete even for B = 3. However when B = 2, we show\nthat it can be efficiently solved. In addition, we consider a version of the\nproblem where jobs have arbitrary lengths and can be preempted at any point in\ntime. For general B, the problem can be solved by linear programming. For B =\n2, the problem amounts to finding a triangle-free 2-matching on a special\ngraph. We extend the algorithm of Babenko et. al. to handle our variant, and\nalso to handle non-unit length jobs. This yields an O(sqrt(L)m) time algorithm\nto solve the preemptive scheduling problem for B = 2, where L is the sum of the\njob lengths. We also show that for B = 2 and unit length jobs, the optimal\nnon-preemptive schedule has at most 4/3 times the active time of the optimal\npreemptive schedule; this bound extends to several versions of the problem when\njobs have arbitrary length."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.0396v3", 
    "title": "Solving Cyclic Longest Common Subsequence in Quadratic Time", 
    "arxiv-id": "1208.0396v3", 
    "author": "Andy Nguyen", 
    "publish": "2012-08-02T04:23:12Z", 
    "summary": "We present a practical algorithm for the cyclic longest common subsequence\n(CLCS) problem that runs in O(mn) time, where m and n are the lengths of the\ntwo input strings. While this is not necessarily an asymptotic improvement over\nthe existing record, it is far simpler to understand and to implement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.0542v1", 
    "title": "A Constructive Algorithm to Prove P=NP", 
    "arxiv-id": "1208.0542v1", 
    "author": "Wen-Qi Duan", 
    "publish": "2012-07-31T07:10:37Z", 
    "summary": "After reducing the undirected Hamiltonian cycle problem into the TSP problem\nwith cost 0 or 1, we developed an effective algorithm to compute the optimal\ntour of the transformed TSP. Our algorithm is described as a growth process:\ninitially, constructing 4-vertexes optimal tour; next, one new vertex being\nadded into the optimal tour in such a way to obtain the new optimal tour; then,\nrepeating the previous step until all vertexes are included into the optimal\ntour. This paper has shown that our constructive algorithm can solve the\nundirected Hamiltonian cycle problem in polynomial time. According to\nCook-Levin theorem, we argue that we have provided a constructive proof of\nP=NP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.0554v1", 
    "title": "Fast Monotone Summation over Disjoint Sets", 
    "arxiv-id": "1208.0554v1", 
    "author": "Janne H. Korhonen", 
    "publish": "2012-08-02T17:38:41Z", 
    "summary": "We study the problem of computing an ensemble of multiple sums where the\nsummands in each sum are indexed by subsets of size $p$ of an $n$-element\nground set. More precisely, the task is to compute, for each subset of size $q$\nof the ground set, the sum over the values of all subsets of size $p$ that are\ndisjoint from the subset of size $q$. We present an arithmetic circuit that,\nwithout subtraction, solves the problem using $O((n^p+n^q)\\log n)$ arithmetic\ngates, all monotone; for constant $p$, $q$ this is within the factor $\\log n$\nof the optimal. The circuit design is based on viewing the summation as a \"set\nnucleation\" task and using a tree-projection approach to implement the\nnucleation. Applications include improved algorithms for counting heaviest\n$k$-paths in a weighted graph, computing permanents of rectangular matrices,\nand dynamic feature selection in machine learning."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.0798v1", 
    "title": "Biff (Bloom Filter) Codes : Fast Error Correction for Large Data Sets", 
    "arxiv-id": "1208.0798v1", 
    "author": "George Varghese", 
    "publish": "2012-08-03T17:15:53Z", 
    "summary": "Large data sets are increasingly common in cloud and virtualized\nenvironments. For example, transfers of multiple gigabytes are commonplace, as\nare replicated blocks of such sizes. There is a need for fast error-correction\nor data reconciliation in such settings even when the expected number of errors\nis small.\n  Motivated by such cloud reconciliation problems, we consider error-correction\nschemes designed for large data, after explaining why previous approaches\nappear unsuitable. We introduce Biff codes, which are based on Bloom filters\nand are designed for large data. For Biff codes with a message of length $L$\nand $E$ errors, the encoding time is $O(L)$, decoding time is $O(L + E)$ and\nthe space overhead is $O(E)$. Biff codes are low-density parity-check codes;\nthey are similar to Tornado codes, but are designed for errors instead of\nerasures. Further, Biff codes are designed to be very simple, removing any\nexplicit graph structures and based entirely on hash tables. We derive Biff\ncodes by a simple reduction from a set reconciliation algorithm for a recently\ndeveloped data structure, invertible Bloom lookup tables. While the underlying\ntheory is extremely simple, what makes this code especially attractive is the\nease with which it can be implemented and the speed of decoding. We present\nresults from a prototype implementation that decodes messages of 1 million\nwords with thousands of errors in well under a second."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.1248v1", 
    "title": "On fixed-parameter algorithms for Split Vertex Deletion", 
    "arxiv-id": "1208.1248v1", 
    "author": "Marcin Pilipczuk", 
    "publish": "2012-08-06T19:23:26Z", 
    "summary": "In the Split Vertex Deletion problem, given a graph G and an integer k, we\nask whether one can delete k vertices from the graph G to obtain a split graph\n(i.e., a graph, whose vertex set can be partitioned into two sets: one inducing\na clique and the second one inducing an independent set). In this paper we\nstudy fixed-parameter algorithms for Split Vertex Deletion parameterized by k:\nwe show that, up to a factor quasipolynomial in k and polynomial in n, the\nSplit Vertex Deletion problem can be solved in the same time as the\nwell-studied Vertex Cover problem. Plugging the currently best fixed-parameter\nalgorithm for Vertex Cover due to Chen et al. [TCS 2010], we obtain an\nalgorithm that solves Split Vertex Deletion in time O(1.2738^k * k^O(log k) +\nn^O(1)).\n  To achieve our goal, we prove the following structural result that may be of\nindependent interest: for any graph G we may compute a family P of size n^O(log\nn) containing partitions of V(G) into two parts, such for any two disjoint\nsubsets X_C, X_I of V(G) where G[X_C] is a clique and G[X_I] is an independent\nset, there is a partition in P which contains all vertices of X_C on one side\nand all vertices of X_I on the other."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.1272v1", 
    "title": "A Polylogarithimic Approximation Algorithm for Edge-Disjoint Paths with   Congestion 2", 
    "arxiv-id": "1208.1272v1", 
    "author": "Shi Li", 
    "publish": "2012-08-06T20:28:20Z", 
    "summary": "In the Edge-Disjoint Paths with Congestion problem (EDPwC), we are given an\nundirected n-vertex graph G, a collection M={(s_1,t_1),...,(s_k,t_k)} of demand\npairs and an integer c. The goal is to connect the maximum possible number of\nthe demand pairs by paths, so that the maximum edge congestion - the number of\npaths sharing any edge - is bounded by c. When the maximum allowed congestion\nis c=1, this is the classical Edge-Disjoint Paths problem (EDP).\n  The best current approximation algorithm for EDP achieves an $O(\\sqrt\nn)$-approximation, by rounding the standard multi-commodity flow relaxation of\nthe problem. This matches the $\\Omega(\\sqrt n)$ lower bound on the integrality\ngap of this relaxation. We show an $O(poly log k)$-approximation algorithm for\nEDPwC with congestion c=2, by rounding the same multi-commodity flow\nrelaxation. This gives the best possible congestion for a sub-polynomial\napproximation of EDPwC via this relaxation. Our results are also close to\noptimal in terms of the number of pairs routed, since EDPwC is known to be hard\nto approximate to within a factor of $\\tilde{\\Omega}((\\log n)^{1/(c+1)})$ for\nany constant congestion c. Prior to our work, the best approximation factor for\nEDPwC with congestion 2 was $\\tilde O(n^{3/7})$, and the best algorithm\nachieving a polylogarithmic approximation required congestion 14."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.1688v3", 
    "title": "Don't Be Strict in Local Search!", 
    "arxiv-id": "1208.1688v3", 
    "author": "Stefan Szeider", 
    "publish": "2012-08-08T15:21:50Z", 
    "summary": "Local Search is one of the fundamental approaches to combinatorial\noptimization and it is used throughout AI. Several local search algorithms are\nbased on searching the k-exchange neighborhood. This is the set of solutions\nthat can be obtained from the current solution by exchanging at most k\nelements. As a rule of thumb, the larger k is, the better are the chances of\nfinding an improved solution. However, for inputs of size n, a na\\\"ive\nbrute-force search of the k-exchange neighborhood requires n to the power of\nO(k) time, which is not practical even for very small values of k.\n  Fellows et al. (IJCAI 2009) studied whether this brute-force search is\navoidable and gave positive and negative answers for several combinatorial\nproblems. They used the notion of local search in a strict sense. That is, an\nimproved solution needs to be found in the k-exchange neighborhood even if a\nglobal optimum can be found efficiently.\n  In this paper we consider a natural relaxation of local search, called\npermissive local search (Marx and Schlotter, IWPEC 2009) and investigate\nwhether it enhances the domain of tractable inputs. We exemplify this approach\non a fundamental combinatorial problem, Vertex Cover. More precisely, we show\nthat for a class of inputs, finding an optimum is hard, strict local search is\nhard, but permissive local search is tractable.\n  We carry out this investigation in the framework of parameterized complexity."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.2318v1", 
    "title": "A Novel Feature-Based Approach to Characterize Algorithm Performance for   the Traveling Salesman Problem", 
    "arxiv-id": "1208.2318v1", 
    "author": "Frank Neumann", 
    "publish": "2012-08-11T07:31:50Z", 
    "summary": "Meta-heuristics are frequently used to tackle NP-hard combinatorial\noptimization problems. With this paper we contribute to the understanding of\nthe success of 2-opt based local search algorithms for solving the traveling\nsalesman problem (TSP). Although 2-opt is widely used in practice, it is hard\nto understand its success from a theoretical perspective. We take a statistical\napproach and examine the features of TSP instances that make the problem either\nhard or easy to solve. As a measure of problem difficulty for 2-opt we use the\napproximation ratio that it achieves on a given instance. Our investigations\npoint out important features that make TSP instances hard or easy to be\napproximated by 2-opt."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.2724v5", 
    "title": "Caching with rental cost and zapping", 
    "arxiv-id": "1208.2724v5", 
    "author": "Neal E. Young", 
    "publish": "2012-08-13T22:49:00Z", 
    "summary": "The \\emph{file caching} problem is defined as follows. Given a cache of size\n$k$ (a positive integer), the goal is to minimize the total retrieval cost for\nthe given sequence of requests to files. A file $f$ has size $size(f)$ (a\npositive integer) and retrieval cost $cost(f)$ (a non-negative number) for\nbringing the file into the cache. A \\emph{miss} or \\emph{fault} occurs when the\nrequested file is not in the cache and the file has to be retrieved into the\ncache by paying the retrieval cost, and some other file may have to be removed\n(\\emph{evicted}) from the cache so that the total size of the files in the\ncache does not exceed $k$.\n  We study the following variants of the online file caching problem.\n\\textbf{\\emph{Caching with Rental Cost} (or \\emph{Rental Caching})}: There is a\nrental cost $\\lambda$ (a positive number) for each file in the cache at each\ntime unit. The goal is to minimize the sum of the retrieval costs and the\nrental costs. \\textbf{\\emph{Caching with Zapping}}: A file can be \\emph{zapped}\nby paying a zapping cost $N \\ge 1$. Once a file is zapped, all future requests\nof the file don't incur any cost. The goal is to minimize the sum of the\nretrieval costs and the zapping costs.\n  We study these two variants and also the variant which combines these two\n(rental caching with zapping). We present deterministic lower and upper bounds\nin the competitive-analysis framework. We study and extend the online covering\nalgorithm from \\citep{young02online} to give deterministic online algorithms.\nWe also present randomized lower and upper bounds for some of these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.2832v1", 
    "title": "Time- and space-efficient evaluation of the complex exponential function   using series expansion", 
    "arxiv-id": "1208.2832v1", 
    "author": "Sergey V. Yakhontov", 
    "publish": "2012-08-14T11:31:16Z", 
    "summary": "An algorithm for the evaluation of the complex exponential function is\nproposed which is quasi-linear in time and linear in space. This algorithm is\nbased on a modified binary splitting method for the hypergeometric series and a\nmodified Karatsuba method for the fast evaluation of the exponential function.\nThe time complexity of this algorithm is equal to that of the ordinary\nalgorithm for the evaluation of the exponential function based on the series\nexpansion: O(M(n)log(n)^2)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.2846v2", 
    "title": "Adapt or Die: Polynomial Lower Bounds for Non-Adaptive Dynamic Data   Structures", 
    "arxiv-id": "1208.2846v2", 
    "author": "Kasper Green Larsen", 
    "publish": "2012-08-14T12:34:25Z", 
    "summary": "In this paper, we study the role non-adaptivity plays in maintaining dynamic\ndata structures. Roughly speaking, a data structure is non-adaptive if the\nmemory locations it reads and/or writes when processing a query or update\ndepend only on the query or update and not on the contents of previously read\ncells. We study such non-adaptive data structures in the cell probe model. This\nmodel is one of the least restrictive lower bound models and in particular,\ncell probe lower bounds apply to data structures developed in the popular\nword-RAM model. Unfortunately, this generality comes at a high cost: the\nhighest lower bound proved for any data structure problem is only\npolylogarithmic. Our main result is to demonstrate that one can in fact obtain\npolynomial cell probe lower bounds for non-adaptive data structures.\n  To shed more light on the seemingly inherent polylogarithmic lower bound\nbarrier, we study several different notions of non-adaptivity and identify key\nproperties that must be dealt with if we are to prove polynomial lower bounds\nwithout restrictions on the data structures.\n  Finally, our results also unveil an interesting connection between data\nstructures and depth-2 circuits. This allows us to translate conjectured hard\ndata structure problems into good candidates for high circuit lower bounds; in\nparticular, in the area of linear circuits for linear operators. Building on\nlower bound proofs for data structures in slightly more restrictive models, we\nalso present a number of properties of linear operators which we believe are\nworth investigating in the realm of circuit lower bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.2956v2", 
    "title": "Local reconstructors and tolerant testers for connectivity and diameter", 
    "arxiv-id": "1208.2956v2", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2012-08-14T19:46:41Z", 
    "summary": "A local property reconstructor for a graph property is an algorithm which,\ngiven oracle access to the adjacency list of a graph that is \"close\" to having\nthe property, provides oracle access to the adjacency matrix of a \"correction\"\nof the graph, i.e. a graph which has the property and is close to the given\ngraph. For this model, we achieve local property reconstructors for the\nproperties of connectivity and $k$-connectivity in undirected graphs, and the\nproperty of strong connectivity in directed graphs. Along the way, we present a\nmethod of transforming a local reconstructor (which acts as a \"adjacency matrix\noracle\" for the corrected graph) into an \"adjacency list oracle\". This allows\nus to recursively use our local reconstructor for $(k-1)$-connectivity to\nobtain a local reconstructor for $k$-connectivity.\n  We also extend this notion of local property reconstruction to parametrized\ngraph properties (for instance, having diameter at most $D$ for some parameter\n$D$) and require that the corrected graph has the property with parameter close\nto the original. We obtain a local reconstructor for the low diameter property,\nwhere if the original graph is close to having diameter $D$, then the corrected\ngraph has diameter roughly 2D.\n  We also exploit a connection between local property reconstruction and\nproperty testing, observed by Brakerski, to obtain new tolerant property\ntesters for all of the aforementioned properties. Except for the one for\nconnectivity, these are the first tolerant property testers for these\nproperties."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.3054v1", 
    "title": "LP Rounding for k-Centers with Non-uniform Hard Capacities", 
    "arxiv-id": "1208.3054v1", 
    "author": "Samir Khuller", 
    "publish": "2012-08-15T08:20:10Z", 
    "summary": "In this paper we consider a generalization of the classical k-center problem\nwith capacities. Our goal is to select k centers in a graph, and assign each\nnode to a nearby center, so that we respect the capacity constraints on\ncenters. The objective is to minimize the maximum distance a node has to travel\nto get to its assigned center. This problem is NP-hard, even when centers have\nno capacity restrictions and optimal factor 2 approximation algorithms are\nknown. With capacities, when all centers have identical capacities, a 6\napproximation is known with no better lower bounds than for the infinite\ncapacity version.\n  While many generalizations and variations of this problem have been studied\nextensively, no progress was made on the capacitated version for a general\ncapacity function. We develop the first constant factor approximation algorithm\nfor this problem. Our algorithm uses an LP rounding approach to solve this\nproblem, and works for the case of non-uniform hard capacities, when multiple\ncopies of a node may not be chosen and can be extended to the case when there\nis a hard bound on the number of copies of a node that may be selected. In\naddition we establish a lower bound on the integrality gap of 7(5) for\nnon-uniform (uniform) hard capacities. In addition we prove that if there is a\n(3-eps)-factor approximation for this problem then P=NP.\n  Finally, for non-uniform soft capacities we present a much simpler\n11-approximation algorithm, which we find as one more evidence that hard\ncapacities are much harder to deal with."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.3747v1", 
    "title": "On Money as a Means of Coordination between Network Packets", 
    "arxiv-id": "1208.3747v1", 
    "author": "Remous-Aris Koutsiamanis", 
    "publish": "2012-08-18T14:11:17Z", 
    "summary": "In this work, we apply a common economic tool, namely money, to coordinate\nnetwork packets. In particular, we present a network economy, called\nPacketEconomy, where each flow is modeled as a population of rational network\npackets, and these packets can self-regulate their access to network resources\nby mutually trading their positions in router queues. Every packet of the\neconomy has its price, and this price determines if and when the packet will\nagree to buy or sell a better position. We consider a corresponding Markov\nmodel of trade and show that there are Nash equilibria (NE) where queue\npositions and money are exchanged directly between the network packets. This\nsimple approach, interestingly, delivers improvements even when fiat money is\nused. We present theoretical arguments and experimental results to support our\nclaims."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.3798v1", 
    "title": "On-line Indexing for General Alphabets via Predecessor Queries on   Subsets of an Ordered List", 
    "arxiv-id": "1208.3798v1", 
    "author": "Tsvi Kopelowitz", 
    "publish": "2012-08-19T01:26:33Z", 
    "summary": "The problem of Text Indexing is a fundamental algorithmic problem in which\none wishes to preprocess a text in order to quickly locate pattern queries\nwithin the text. In the ever evolving world of dynamic and on-line data, there\nis also a need for developing solutions to index texts which arrive on-line,\ni.e. a character at a time, and still be able to quickly locate said patterns.\nIn this paper, a new solution for on-line indexing is presented by providing an\non-line suffix tree construction in $O(\\log \\log n + \\log\\log |\\Sigma|)$\nworst-case expected time per character, where $n$ is the size of the string,\nand $\\Sigma$ is the alphabet. This improves upon all previously known on-line\nsuffix tree constructions for general alphabets, at the cost of having the run\ntime in expectation.\n  The main idea is to reduce the problem of constructing a suffix tree on-line\nto an interesting variant of the order maintenance problem, which may be of\nindependent interest. In the famous order maintenance problem, one wishes to\nmaintain a dynamic list $L$ of size $n$ under insertions, deletions, and order\nqueries. In an order query, one is given two nodes from $L$ and must determine\nwhich node precedes the other in $L$. In the Predecessor search on Dynamic\nSubsets of an Ordered Dynamic List problem (POLP) it is also necessary to\nmaintain dynamic subsets of $L$ such that given some $u\\in L$ it will be\npossible to quickly locate the predecessor of $u$ in any subset. This paper\nprovides an efficient data structure capable of solving the POLP with\nworst-case expected bounds that match the currently best known bounds for\npredecessor search in the RAM model, improving over a solution which may be\nimplicitly obtained from Dietz [Die89].\n  Furthermore, this paper improves or simplifies bounds for several additional\napplications, including fully-persistent arrays and the Order-Maintenance\nProblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.3835v4", 
    "title": "Constrained Fault-Tolerant Resource Allocation", 
    "arxiv-id": "1208.3835v4", 
    "author": "Longkun Guo", 
    "publish": "2012-08-19T13:46:18Z", 
    "summary": "In the Constrained Fault-Tolerant Resource Allocation (FTRA) problem, we are\ngiven a set of sites containing facilities as resources, and a set of clients\naccessing these resources. Specifically, each site i is allowed to open at most\nR_i facilities with cost f_i for each opened facility. Each client j requires\nan allocation of r_j open facilities and connecting j to any facility at site i\nincurs a connection cost c_ij. The goal is to minimize the total cost of this\nresource allocation scenario.\n  FTRA generalizes the Unconstrained Fault-Tolerant Resource Allocation\n(FTRA_{\\infty}) [18] and the classical Fault-Tolerant Facility Location (FTFL)\n[13] problems: for every site i, FTRA_{\\infty} does not have the constraint\nR_i, whereas FTFL sets R_i=1. These problems are said to be uniform if all\nr_j's are the same, and general otherwise.\n  For the general metric FTRA, we first give an LP-rounding algorithm achieving\nthe approximation ratio of 4. Then we show the problem reduces to FTFL,\nimplying the ratio of 1.7245 from [3]. For the uniform FTRA, we provide a\n1.52-approximation primal-dual algorithm in O(n^4) time, where n is the total\nnumber of sites and clients. We also consider the Constrained Fault-Tolerant\nk-Resource Allocation (k-FTRA) problem where additionally the total number of\nfacilities can be opened across all sites is bounded by k. For the uniform\nk-FTRA, we give the first constant-factor approximation algorithm with a factor\nof 4. Note that the above results carry over to FTRA_{\\infty} and\nk-FTRA_{\\infty}."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9838-4", 
    "link": "http://arxiv.org/pdf/1208.3874v1", 
    "title": "Upper bounds for the formula size of the majority function", 
    "arxiv-id": "1208.3874v1", 
    "author": "Igor S. Sergeev", 
    "publish": "2012-08-19T18:14:55Z", 
    "summary": "It is shown that the counting function of n Boolean variables can be\nimplemented with the formulae of size O(n^3.06) over the basis of all 2-input\nBoolean functions and of size O(n^4.54) over the standard basis. The same\nbounds follow for the complexity of any threshold symmetric function of n\nvariables and particularly for the majority function. Any bit of the product of\nbinary numbers of length n can be computed by formulae of size O(n^4.06) or\nO(n^5.54) depending on basis. Incidentally the bounds O(n^3.23) and O(n^4.82)\non the formula size of any symmetric function of n variables with respect to\nthe basis are obtained."
}]