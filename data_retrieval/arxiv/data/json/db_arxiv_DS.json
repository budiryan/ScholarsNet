[{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9301115v1", 
    "other_authors": "Donald E. Knuth", 
    "title": "Context-free multilanguages", 
    "arxiv-id": "cs/9301115v1", 
    "author": "Donald E. Knuth", 
    "publish": "1991-12-01T00:00:00Z", 
    "summary": "This article is a sketch of ideas that were once intended to appear in the\nauthor's famous series, \"The Art of Computer Programming\". He generalizes the\nnotion of a context-free language from a set to a multiset of words over an\nalphabet. The idea is to keep track of the number of ways to parse a string.\nFor example, \"fruit flies like a banana\" can famously be parsed in two ways;\nanalogous examples in the setting of programming languages may yet be important\nin the future.\n  The treatment is informal but essentially rigorous."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9608105v1", 
    "other_authors": "Svante Janson, Donald E. Knuth", 
    "title": "Shellsort with three increments", 
    "arxiv-id": "cs/9608105v1", 
    "author": "Donald E. Knuth", 
    "publish": "1996-08-22T00:00:00Z", 
    "summary": "A perturbation technique can be used to simplify and sharpen A. C. Yao's\ntheorems about the behavior of shellsort with increments $(h,g,1)$. In\nparticular, when $h=\\Theta(n^{7/15})$ and $g=\\Theta(h^{1/5})$, the average\nrunning time is $O(n^{23/15})$. The proof involves interesting properties of\nthe inversions in random permutations that have been $h$-sorted and $g$-sorted."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9801103v1", 
    "other_authors": "Donald E. Knuth", 
    "title": "Linear probing and graphs", 
    "arxiv-id": "cs/9801103v1", 
    "author": "Donald E. Knuth", 
    "publish": "1998-01-15T00:00:00Z", 
    "summary": "Mallows and Riordan showed in 1968 that labeled trees with a small number of\ninversions are related to labeled graphs that are connected and sparse. Wright\nenumerated sparse connected graphs in 1977, and Kreweras related the inversions\nof trees to the so-called ``parking problem'' in 1980. A~combination of these\nthree results leads to a surprisingly simple analysis of the behavior of\nhashing by linear probing, including higher moments of the cost of successful\nsearch."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9809012v1", 
    "other_authors": "David R. Karger", 
    "title": "A Fully Polynomial Randomized Approximation Scheme for the All Terminal   Network Reliability Problem", 
    "arxiv-id": "cs/9809012v1", 
    "author": "David R. Karger", 
    "publish": "1998-09-09T02:38:56Z", 
    "summary": "The classic all-terminal network reliability problem posits a graph, each of\nwhose edges fails independently with some given probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9812007v1", 
    "other_authors": "David R. Karger", 
    "title": "Minimum Cuts in Near-Linear Time", 
    "arxiv-id": "cs/9812007v1", 
    "author": "David R. Karger", 
    "publish": "1998-12-08T21:29:20Z", 
    "summary": "We significantly improve known time bounds for solving the minimum cut\nproblem on undirected graphs. We use a ``semi-duality'' between minimum cuts\nand maximum spanning tree packings combined with our previously developed\nrandom sampling techniques. We give a randomized algorithm that finds a minimum\ncut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We\nalso give a simpler randomized algorithm that finds all minimum cuts with high\nprobability in O(n^2 log n) time. This variant has an optimal RNC\nparallelization. Both variants improve on the previous best time bound of O(n^2\nlog^3 n). Other applications of the tree-packing approach are new, nearly tight\nbounds on the number of near minimum cuts a graph may have and a new data\nstructure for representing them in a space-efficient manner."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9812008v1", 
    "other_authors": "David Karger, Rajeev Motwani, Madhu Sudan", 
    "title": "Approximate Graph Coloring by Semidefinite Programming", 
    "arxiv-id": "cs/9812008v1", 
    "author": "Madhu Sudan", 
    "publish": "1998-12-08T22:03:36Z", 
    "summary": "We consider the problem of coloring k-colorable graphs with the fewest\npossible colors. We present a randomized polynomial time algorithm that colors\na 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log\nn), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any\nvertex. Besides giving the best known approximation ratio in terms of n, this\nmarks the first non-trivial approximation result as a function of the maximum\ndegree Delta. This result can be generalized to k-colorable graphs to obtain a\ncoloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}\nlog^{1/2} n) colors. Our results are inspired by the recent work of Goemans and\nWilliamson who used an algorithm for semidefinite optimization problems, which\ngeneralize linear programs, to obtain improved approximations for the MAX CUT\nand MAX 2-SAT problems. An intriguing outcome of our work is a duality\nrelationship established between the value of the optimum solution to our\nsemidefinite program and the Lovasz theta-function. We show lower bounds on the\ngap between the optimum solution of our semidefinite program and the actual\nchromatic number; by duality this also demonstrates interesting new facts about\nthe theta-function."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9903010v1", 
    "other_authors": "Anatoly D. Plotnikov", 
    "title": "A class of problems of NP to be worth to search an efficient solving   algorithm", 
    "arxiv-id": "cs/9903010v1", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "1999-03-11T19:36:05Z", 
    "summary": "We examine possibility to design an efficient solving algorithm for problems\nof the class \\np. It is introduced a classification of \\np problems by the\nproperty that a partial solution of size $k$ can be extended into a partial\nsolution of size $k+1$ in polynomial time. It is defined an unique class\nproblems to be worth to search an efficient solving algorithm. The problems,\nwhich are outside of this class, are inherently exponential. We show that the\nHamiltonian cycle problem is inherently exponential."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9906021v1", 
    "other_authors": "Christoph Durr, Marek Chrobak", 
    "title": "Reconstructing hv-Convex Polyominoes from Orthogonal Projections", 
    "arxiv-id": "cs/9906021v1", 
    "author": "Marek Chrobak", 
    "publish": "1999-06-22T09:56:53Z", 
    "summary": "Tomography is the area of reconstructing objects from projections. Here we\nwish to reconstruct a set of cells in a two dimensional grid, given the number\nof cells in every row and column. The set is required to be an hv-convex\npolyomino, that is all its cells must be connected and the cells in every row\nand column must be consecutive. A simple, polynomial algorithm for\nreconstructing hv-convex polyominoes is provided, which is several orders of\nmagnitudes faster than the best previously known algorithm from Barcucci et al.\nIn addition, the problem of reconstructing a special class of centered\nhv-convex polyominoes is addressed. (An object is centered if it contains a row\nwhose length equals the total width of the object). It is shown that in this\ncase the reconstruction problem can be solved in linear time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10207-014-0249-6", 
    "link": "http://arxiv.org/pdf/cs/9911003v1", 
    "other_authors": "David Eppstein", 
    "title": "Subgraph Isomorphism in Planar Graphs and Related Problems", 
    "arxiv-id": "cs/9911003v1", 
    "author": "David Eppstein", 
    "publish": "1999-11-09T18:58:58Z", 
    "summary": "We solve the subgraph isomorphism problem in planar graphs in linear time,\nfor any pattern of constant size. Our results are based on a technique of\npartitioning the planar graph into pieces of small tree-width, and applying\ndynamic programming within each piece. The same methods can be used to solve\nother planar graph problems including connectivity, diameter, girth, induced\nsubgraph isomorphism, and shortest paths."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/9912014v1", 
    "other_authors": "David Eppstein", 
    "title": "Fast Hierarchical Clustering and Other Applications of Dynamic Closest   Pairs", 
    "arxiv-id": "cs/9912014v1", 
    "author": "David Eppstein", 
    "publish": "1999-12-22T01:42:51Z", 
    "summary": "We develop data structures for dynamic closest pair problems with arbitrary\ndistance functions, that do not necessarily come from any geometric structure\non the objects. Based on a technique previously used by the author for\nEuclidean closest pairs, we show how to insert and delete objects from an\nn-object set, maintaining the closest pair, in O(n log^2 n) time per update and\nO(n) space. With quadratic space, we can instead use a quadtree-like structure\nto achieve an optimal time bound, O(n) per update. We apply these data\nstructures to hierarchical clustering, greedy matching, and TSP heuristics, and\ndiscuss other potential applications in machine learning, Groebner bases, and\nlocal improvement algorithms for partition and placement problems. Experiments\nshow our new methods to be faster in practice than previously used heuristics."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/9912020v2", 
    "other_authors": "Markus Hegland, Vladimir Pestov", 
    "title": "Additive models in high dimensions", 
    "arxiv-id": "cs/9912020v2", 
    "author": "Vladimir Pestov", 
    "publish": "1999-12-30T07:50:11Z", 
    "summary": "We discuss some aspects of approximating functions on high-dimensional data\nsets with additive functions or ANOVA decompositions, that is, sums of\nfunctions depending on fewer variables each. It is seen that under appropriate\nsmoothness conditions, the errors of the ANOVA decompositions are of order\n$O(n^{m/2})$ for approximations using sums of functions of up to $m$ variables\nunder some mild restrictions on the (possibly dependent) predictor variables.\nSeveral simulated examples illustrate this behaviour."
},{
    "category": "cs.DS", 
    "doi": "10.1145/351827.351829", 
    "link": "http://arxiv.org/pdf/cs/0003078v1", 
    "other_authors": "Anatoly D. Plotnikov", 
    "title": "About the finding of independent vertices of a graph", 
    "arxiv-id": "cs/0003078v1", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "2000-03-24T23:31:24Z", 
    "summary": "We examine the Maximum Independent Set Problem in an undirected graph. The\nmain result is that this problem can be considered as the solving the same\nproblem in a subclass of the weighted normal twin-orthogonal graphs. The\nproblem is formulated which is dual to the problem above. It is shown that, for\ntrivial twin-orthogonal graphs, any of its maximal independent set is also\nmaximum one."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0006046v1", 
    "other_authors": "Richard Beigel, David Eppstein", 
    "title": "3-Coloring in Time O(1.3289^n)", 
    "arxiv-id": "cs/0006046v1", 
    "author": "David Eppstein", 
    "publish": "2000-06-30T22:04:04Z", 
    "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT,\n3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a\nconstraint satisfaction (CSP) formulation of these problems. 3-SAT is\nequivalent to (2,3)-CSP while the other problems above are special cases of\n(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to\n(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the\ntime bounds for solving the other problems listed above. Our techniques involve\na mixture of Davis-Putnam-style backtracking with more sophisticated matching\nand network flow based ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0007029v1", 
    "other_authors": "Gabriel Istrate", 
    "title": "Dimension-Dependent behavior in the satisfability of random k-Horn   formulae", 
    "arxiv-id": "cs/0007029v1", 
    "author": "Gabriel Istrate", 
    "publish": "2000-07-18T21:50:04Z", 
    "summary": "We determine the asymptotical satisfiability probability of a random\nat-most-k-Horn formula, via a probabilistic analysis of a simple version,\ncalled PUR, of positive unit resolution. We show that for k=k(n)->oo the\nproblem can be ``reduced'' to the case k(n)=n, that was solved in\ncs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR\nis modeled by a simple queuing chain, leading to a closed-form solution when\nk=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.\nUnder a rescaled parameter, the graphs of satisfaction probability\ncorresponding to finite values of k converge to the one for the uniform case, a\n``dimension-dependent behavior'' similar to the one found experimentally by\nKirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively\nexplained by a threshold property for the number of iterations of PUR makes on\nrandom satisfiable Horn formulas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0007043v1", 
    "other_authors": "Suman Kumar Nath, Rezaul Alam Chowdhury, M. Kaykobad", 
    "title": "Min-Max Fine Heaps", 
    "arxiv-id": "cs/0007043v1", 
    "author": "M. Kaykobad", 
    "publish": "2000-07-31T00:13:17Z", 
    "summary": "In this paper we present a new data structure for double ended priority\nqueue, called min-max fine heap, which combines the techniques used in fine\nheap and traditional min-max heap. The standard operations on this proposed\nstructure are also presented, and their analysis indicates that the new\nstructure outperforms the traditional one."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0008011v1", 
    "other_authors": "Uri Zwick", 
    "title": "All Pairs Shortest Paths using Bridging Sets and Rectangular Matrix   Multiplication", 
    "arxiv-id": "cs/0008011v1", 
    "author": "Uri Zwick", 
    "publish": "2000-08-16T20:39:45Z", 
    "summary": "We present two new algorithms for solving the {\\em All Pairs Shortest Paths}\n(APSP) problem for weighted directed graphs. Both algorithms use fast matrix\nmultiplication algorithms.\n  The first algorithm solves the APSP problem for weighted directed graphs in\nwhich the edge weights are integers of small absolute value in $\\Ot(n^{2+\\mu})$\ntime, where $\\mu$ satisfies the equation $\\omega(1,\\mu,1)=1+2\\mu$ and\n$\\omega(1,\\mu,1)$ is the exponent of the multiplication of an $n\\times n^\\mu$\nmatrix by an $n^\\mu \\times n$ matrix. Currently, the best available bounds on\n$\\omega(1,\\mu,1)$, obtained by Coppersmith, imply that $\\mu<0.575$. The running\ntime of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on\nthe $\\Ot(n^{(3+\\omega)/2})$ time algorithm, where $\\omega=\\omega(1,1,1)<2.376$\nis the usual exponent of matrix multiplication, obtained by Alon, Galil and\nMargalit, whose running time is only known to be $O(n^{2.688})$.\n  The second algorithm solves the APSP problem {\\em almost} exactly for\ndirected graphs with {\\em arbitrary} non-negative real weights. The algorithm\nruns in $\\Ot((n^\\omega/\\eps)\\log (W/\\eps))$ time, where $\\eps>0$ is an error\nparameter and W is the largest edge weight in the graph, after the edge weights\nare scaled so that the smallest non-zero edge weight in the graph is 1. It\nreturns estimates of all the distances in the graph with a stretch of at most\n$1+\\eps$. Corresponding paths can also be found efficiently."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0009006v1", 
    "other_authors": "David Eppstein", 
    "title": "Improved Algorithms for 3-Coloring, 3-Edge-Coloring, and Constraint   Satisfaction", 
    "arxiv-id": "cs/0009006v1", 
    "author": "David Eppstein", 
    "publish": "2000-09-13T20:42:55Z", 
    "summary": "We consider worst case time bounds for NP-complete problems including 3-SAT,\n3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a\nconstraint satisfaction (CSP) formulation of these problems; 3-SAT is\nequivalent to (2,3)-CSP while the other problems above are special cases of\n(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the\ntime bounds for solving the other problems listed above. Our techniques involve\na mixture of Davis-Putnam-style backtracking with more sophisticated matching\nand network flow based ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0011047v1", 
    "other_authors": "Donald E. Knuth", 
    "title": "Dancing links", 
    "arxiv-id": "cs/0011047v1", 
    "author": "Donald E. Knuth", 
    "publish": "2000-11-15T00:00:00Z", 
    "summary": "The author presents two tricks to accelerate depth-first search algorithms\nfor a class of combinatorial puzzle problems, such as tiling a tray by a fixed\nset of polyominoes. The first trick is to implement each assumption of the\nsearch with reversible local operations on doubly linked lists. By this trick,\nevery step of the search affects the data incrementally.\n  The second trick is to add a ghost square that represents the identity of\neach polyomino. Thus puts the rule that each polyomino be used once on the same\nfooting as the rule that each square be covered once. The coding simplifies to\na more abstract form which is equivalent to 0-1 integer programming. More\nsignificantly for the total computation time, the search can naturally switch\nbetween placing a fixed polyomino or covering a fixed square at different\nstages, according to a combined heuristic.\n  Finally the author reports excellent performance for his algorithm for some\nfamiliar puzzles. These include tiling a hexagon by 19 hexiamonds and the N\nqueens problem for N up to 18."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0012002v1", 
    "other_authors": "Md. Enamul Karim, Abdun Naser Mahmood", 
    "title": "Random Shuffling to Reduce Disorder in Adaptive Sorting Scheme", 
    "arxiv-id": "cs/0012002v1", 
    "author": "Abdun Naser Mahmood", 
    "publish": "2000-12-02T17:47:26Z", 
    "summary": "In this paper we present a random shuffling scheme to apply with adaptive\nsorting algorithms. Adaptive sorting algorithms utilize the presortedness\npresent in a given sequence. We have probabilistically increased the amount of\npresortedness present in a sequence by using a random shuffling technique that\nrequires little computation. Theoretical analysis suggests that the proposed\nscheme can improve the performance of adaptive sorting. Experimental results\nshow that it significantly reduces the amount of disorder present in a given\nsequence and improves the execution time of adaptive sorting algorithm as well."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0111050v7", 
    "other_authors": "Daniel A. Spielman, Shang-Hua Teng", 
    "title": "Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes   Polynomial Time", 
    "arxiv-id": "cs/0111050v7", 
    "author": "Shang-Hua Teng", 
    "publish": "2001-11-19T23:37:14Z", 
    "summary": "We introduce the smoothed analysis of algorithms, which is a hybrid of the\nworst-case and average-case analysis of algorithms. In smoothed analysis, we\nmeasure the maximum over inputs of the expected performance of an algorithm\nunder small random perturbations of that input. We measure this performance in\nterms of both the input size and the magnitude of the perturbations. We show\nthat the simplex algorithm has polynomial smoothed complexity."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0112022v2", 
    "other_authors": "Qi Xiao Yang, Sung Sam Yuan, Lu Chun, Li Zhao, Sun Peng", 
    "title": "Faster Algorithm of String Comparison", 
    "arxiv-id": "cs/0112022v2", 
    "author": "Sun Peng", 
    "publish": "2001-12-21T05:58:12Z", 
    "summary": "In many applications, it is necessary to determine the string similarity.\nEdit distance[WF74] approach is a classic method to determine Field Similarity.\nA well known dynamic programming algorithm [GUS97] is used to calculate edit\ndistance with the time complexity O(nm). (for worst case, average case and even\nbest case) Instead of continuing with improving the edit distance approach,\n[LL+99] adopted a brand new approach-token-based approach. Its new concept of\ntoken-base-retain the original semantic information, good time complex-O(nm)\n(for worst, average and best case) and good experimental performance make it a\nmilestone paper in this area. Further study indicates that there is still room\nfor improvement of its Field Similarity algorithm. Our paper is to introduce a\npackage of substring-based new algorithms to determine Field Similarity.\nCombined together, our new algorithms not only achieve higher accuracy but also\ngain the time complexity O(knm) (k<0.75) for worst case, O(*n) where <6 for\naverage case and O(1) for best case. Throughout the paper, we use the approach\nof comparative examples to show higher accuracy of our algorithms compared to\nthe one proposed in [LL+99]. Theoretical analysis, concrete examples and\nexperimental result show that our algorithms can significantly improve the\naccuracy and time complexity of the calculation of Field Similarity. [US97] D.\nGuseld. Algorithms on Strings, Trees and Sequences, in Computer Science and\nComputational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and\nwarehousing, In Proceedings of the 10th International Conference on Database\nand Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R.\nWagner and M. Fisher, The String to String Correction Problem, JACM 21 pages\n168-173, 1974."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0203018v1", 
    "other_authors": "Adam L. Buchsbaum, Glenn S. Fowler, Raffaele Giancarlo", 
    "title": "Improving Table Compression with Combinatorial Optimization", 
    "arxiv-id": "cs/0203018v1", 
    "author": "Raffaele Giancarlo", 
    "publish": "2002-03-13T19:09:41Z", 
    "summary": "We study the problem of compressing massive tables within the\npartition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which\na table is partitioned by an off-line training procedure into disjoint\nintervals of columns, each of which is compressed separately by a standard,\non-line compressor like gzip. We provide a new theory that unifies previous\nexperimental observations on partitioning and heuristic observations on column\npermutation, all of which are used to improve compression rates. Based on the\ntheory, we devise the first on-line training algorithms for table compression,\nwhich can be applied to individual files, not just continuously operating\nsources; and also a new, off-line training algorithm, based on a link to the\nasymmetric traveling salesman problem, which improves on prior work by\nrearranging columns prior to partitioning. We demonstrate these results\nexperimentally. On various test files, the on-line algorithms provide 35-55%\nimprovement over gzip with negligible slowdown; the off-line reordering\nprovides up to 20% further improvement over partitioning alone. We also show\nthat a variation of the table compression problem is MAX-SNP hard."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.008", 
    "link": "http://arxiv.org/pdf/cs/0204033v1", 
    "other_authors": "Krzysztof C. Kiwiel", 
    "title": "Randomized selection revisited", 
    "arxiv-id": "cs/0204033v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2002-04-15T04:48:21Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select for\nfinding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This\nrectifies the analysis of Floyd and Rivest, and extends it to the case of\nnondistinct elements. Our computational results confirm that Select may be the\nbest algorithm in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1109/DCC.1997.582053", 
    "link": "http://arxiv.org/pdf/cs/0205029v1", 
    "other_authors": "Qin Zhang, John Danskin, Neal Young", 
    "title": "A Codebook Generation Algorithm for Document Image Compression", 
    "arxiv-id": "cs/0205029v1", 
    "author": "Neal Young", 
    "publish": "2002-05-17T23:52:11Z", 
    "summary": "Pattern-matching-based document-compression systems (e.g. for faxing) rely on\nfinding a small set of patterns that can be used to represent all of the ink in\nthe document. Finding an optimal set of patterns is NP-hard; previous\ncompression schemes have resorted to heuristics. This paper describes an\nextension of the cross-entropy approach, used previously for measuring pattern\nsimilarity, to this problem. This approach reduces the problem to a k-medians\nproblem, for which the paper gives a new algorithm with a provably good\nperformance guarantee. In comparison to previous heuristics (First Fit, with\nand without generalized Lloyd's/k-means postprocessing steps), the new\nalgorithm generates a better codebook, resulting in an overall improvement in\ncompression performance of almost 17%."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.2001.959930", 
    "link": "http://arxiv.org/pdf/cs/0205039v1", 
    "other_authors": "Neal E. Young", 
    "title": "Sequential and Parallel Algorithms for Mixed Packing and Covering", 
    "arxiv-id": "cs/0205039v1", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T15:12:41Z", 
    "summary": "Mixed packing and covering problems are problems that can be formulated as\nlinear programs using only non-negative coefficients. Examples include\nmulticommodity network flow, the Held-Karp lower bound on TSP, fractional\nrelaxations of set cover, bin-packing, knapsack, scheduling problems,\nminimum-weight triangulation, etc. This paper gives approximation algorithms\nfor the general class of problems. The sequential algorithm is a simple greedy\nalgorithm that can be implemented to find an epsilon-approximate solution in\nO(epsilon^-2 log m) linear-time iterations. The parallel algorithm does\ncomparable work but finishes in polylogarithmic time.\n  The results generalize previous work on pure packing and covering (the\nspecial case when the constraints are all \"less-than\" or all \"greater-than\") by\nMichael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/100794092", 
    "link": "http://arxiv.org/pdf/cs/0205048v2", 
    "other_authors": "Mordecai Golin, Claire Mathieu, Neal E. Young", 
    "title": "Huffman Coding with Letter Costs: A Linear-Time Approximation Scheme", 
    "arxiv-id": "cs/0205048v2", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T18:57:04Z", 
    "summary": "We give a polynomial-time approximation scheme for the generalization of\nHuffman Coding in which codeword letters have non-uniform costs (as in Morse\ncode, where the dash is twice as long as the dot). The algorithm computes a\n(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is\nthe input size."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0205049v1", 
    "other_authors": "Mordecai Golin, Neal E. Young", 
    "title": "Prefix Codes: Equiprobable Words, Unequal Letter Costs", 
    "arxiv-id": "cs/0205049v1", 
    "author": "Neal E. Young", 
    "publish": "2002-05-18T19:05:55Z", 
    "summary": "Describes a near-linear-time algorithm for a variant of Huffman coding, in\nwhich the letters may have non-uniform lengths (as in Morse code), but with the\nrestriction that each word to be encoded has equal probability. [See also\n``Huffman Coding with Unequal Letter Costs'' (2002).]"
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0206033v1", 
    "other_authors": "David Eppstein, Jean-Claude Falmagne", 
    "title": "Algorithms for Media", 
    "arxiv-id": "cs/0206033v1", 
    "author": "Jean-Claude Falmagne", 
    "publish": "2002-06-24T06:50:52Z", 
    "summary": "Falmagne recently introduced the concept of a medium, a combinatorial object\nencompassing hyperplane arrangements, topological orderings, acyclic\norientations, and many other familiar structures. We find efficient solutions\nfor several algorithmic problems on media: finding short reset sequences,\nshortest paths, testing whether a medium has a closed orientation, and listing\nthe states of a medium given a black-box description."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207061v2", 
    "other_authors": "Adam L. Buchsbaum, Loukas Georgiadis, Haim Kaplan, Anne Rogers, Robert E. Tarjan, Jeffery R. Westbrook", 
    "title": "Linear-Time Pointer-Machine Algorithms for Path-Evaluation Problems on   Trees and Graphs", 
    "arxiv-id": "cs/0207061v2", 
    "author": "Jeffery R. Westbrook", 
    "publish": "2002-07-15T18:47:57Z", 
    "summary": "We present algorithms that run in linear time on pointer machines for a\ncollection of problems, each of which either directly or indirectly requires\nthe evaluation of a function defined on paths in a tree. These problems\npreviously had linear-time algorithms but only for random-access machines\n(RAMs); the best pointer-machine algorithms were super-linear by an\ninverse-Ackermann-function factor. Our algorithms are also simpler, in some\ncases substantially, than the previous linear-time RAM algorithms. Our\nimprovements come primarily from three new ideas: a refined analysis of path\ncompression that gives a linear bound if the compressions favor certain nodes,\na pointer-based radix sort as a replacement for table-based methods, and a more\ncareful partitioning of a tree into easily managed parts. Our algorithms\ncompute nearest common ancestors off-line, verify and construct minimum\nspanning trees, do interval analysis on a flowgraph, find the dominators of a\nflowgraph, and build the component tree of a weighted tree."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207066v1", 
    "other_authors": "Jochen Alber, Michael R. Fellows, Rolf Niedermeier", 
    "title": "Polynomial Time Data Reduction for Dominating Set", 
    "arxiv-id": "cs/0207066v1", 
    "author": "Rolf Niedermeier", 
    "publish": "2002-07-16T17:58:48Z", 
    "summary": "Dealing with the NP-complete Dominating Set problem on undirected graphs, we\ndemonstrate the power of data reduction by preprocessing from a theoretical as\nwell as a practical side. In particular, we prove that Dominating Set\nrestricted to planar graphs has a so-called problem kernel of linear size,\nachieved by two simple and easy to implement reduction rules. Moreover, having\nimplemented our reduction rules, first experiments indicate the impressive\npractical potential of these rules. Thus, this work seems to open up a new and\nprospective way how to cope with one of the most important problems in graph\ntheory and combinatorial optimization."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0207082v1", 
    "other_authors": "David Eppstein", 
    "title": "Dynamic Generators of Topologically Embedded Graphs", 
    "arxiv-id": "cs/0207082v1", 
    "author": "David Eppstein", 
    "publish": "2002-07-24T19:56:17Z", 
    "summary": "We provide a data structure for maintaining an embedding of a graph on a\nsurface (represented combinatorially by a permutation of edges around each\nvertex) and computing generators of the fundamental group of the surface, in\namortized time O(log n + log g(log log g)^3) per update on a surface of genus\ng; we can also test orientability of the surface in the same time, and maintain\nthe minimum and maximum spanning tree of the graph in time O(log n + log^4 g)\nper update. Our data structure allows edge insertion and deletion as well as\nthe dual operations; these operations may implicitly change the genus of the\nembedding surface. We apply similar ideas to improve the constant factor in a\nseparator theorem for low-genus graphs, and to find in linear time a\ntree-decomposition of low-genus low-diameter graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0209033v2", 
    "other_authors": "Philippe Baptiste, Marek Chrobak, Christoph Durr, Wojciech Jawor, Nodari Vakhania", 
    "title": "Preemptive Scheduling of Equal-Length Jobs to Maximize Weighted   Throughput", 
    "arxiv-id": "cs/0209033v2", 
    "author": "Nodari Vakhania", 
    "publish": "2002-09-30T10:33:13Z", 
    "summary": "We study the problem of computing a preemptive schedule of equal-length jobs\nwith given release times, deadlines and weights. Our goal is to maximize the\nweighted throughput, which is the total weight of completed jobs. In Graham's\nnotation this problem is described as (1 | r_j;p_j=p;pmtn | sum w_j U_j). We\nprovide an O(n^4)-time algorithm for this problem, improving the previous bound\nof O(n^{10}) by Baptiste."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0210006v2", 
    "other_authors": "Arne Andersson, Mikkel Thorup", 
    "title": "Dynamic Ordered Sets with Exponential Search Trees", 
    "arxiv-id": "cs/0210006v2", 
    "author": "Mikkel Thorup", 
    "publish": "2002-10-09T04:49:56Z", 
    "summary": "We introduce exponential search trees as a novel technique for converting\nstatic polynomial space search structures for ordered sets into fully-dynamic\nlinear space data structures.\n  This leads to an optimal bound of O(sqrt(log n/loglog n)) for searching and\nupdating a dynamic set of n integer keys in linear space. Here searching an\ninteger y means finding the maximum key in the set which is smaller than or\nequal to y. This problem is equivalent to the standard text book problem of\nmaintaining an ordered set (see, e.g., Cormen, Leiserson, Rivest, and Stein:\nIntroduction to Algorithms, 2nd ed., MIT Press, 2001).\n  The best previous deterministic linear space bound was O(log n/loglog n) due\nFredman and Willard from STOC 1990. No better deterministic search bound was\nknown using polynomial space.\n  We also get the following worst-case linear space trade-offs between the\nnumber n, the word length w, and the maximal key U < 2^w: O(min{loglog n+log\nn/log w, (loglog n)(loglog U)/(logloglog U)}). These trade-offs are, however,\nnot likely to be optimal.\n  Our results are generalized to finger searching and string searching,\nproviding optimal results for both in terms of n."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0210013v2", 
    "other_authors": "Janos Csirik, David S. Johnson, Claire Kenyon, James B. Orlin, Peter W. Shor, Richard R. Weber", 
    "title": "On the Sum-of-Squares Algorithm for Bin Packing", 
    "arxiv-id": "cs/0210013v2", 
    "author": "Richard R. Weber", 
    "publish": "2002-10-14T17:17:28Z", 
    "summary": "In this paper we present a theoretical analysis of the deterministic on-line\n{\\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studied\nexperimentally in \\cite{CJK99}, along with several new variants. $SS$ is\napplicable to any instance of bin packing in which the bin capacity $B$ and\nitem sizes $s(a)$ are integral (or can be scaled to be so), and runs in time\n$O(nB)$. It performs remarkably well from an average case point of view: For\nany discrete distribution in which the optimal expected waste is sublinear,\n$SS$ also has sublinear expected waste. For any discrete distribution where the\noptimal expected waste is bounded, $SS$ has expected waste at most $O(\\log n)$.\nIn addition, we discuss several interesting variants on $SS$, including a\nrandomized $O(nB\\log B)$-time on-line algorithm $SS^*$, based on $SS$, whose\nexpected behavior is essentially optimal for all discrete distributions.\nAlgorithm $SS^*$ also depends on a new linear-programming-based\npseudopolynomial-time algorithm for solving the NP-hard problem of determining,\ngiven a discrete distribution $F$, just what is the growth rate for the optimal\nexpected waste. This article is a greatly expanded version of the conference\npaper \\cite{sumsq2000}."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211001v2", 
    "other_authors": "Ronald I. Greenberg", 
    "title": "Fast and Simple Computation of All Longest Common Subsequences", 
    "arxiv-id": "cs/0211001v2", 
    "author": "Ronald I. Greenberg", 
    "publish": "2002-11-01T06:23:00Z", 
    "summary": "This paper shows that a simple algorithm produces the {\\em\nall-prefixes-LCSs-graph} in $O(mn)$ time for two input sequences of size $m$\nand $n$. Given any prefix $p$ of the first input sequence and any prefix $q$ of\nthe second input sequence, all longest common subsequences (LCSs) of $p$ and\n$q$ can be generated in time proportional to the output size, once the\nall-prefixes-LCSs-graph has been constructed. The problem can be solved in the\ncontext of generating all the distinct character strings that represent an LCS\nor in the context of generating all ways of embedding an LCS in the two input\nstrings."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211009v1", 
    "other_authors": "Wing-Kai Hon, Ming-Yang Kao, Tak-Wah Lam, Wing-Kin Sung, Siu-Ming Yiu", 
    "title": "Improved Phylogeny Comparisons: Non-Shared Edges Nearest Neighbor   Interchanges, and Subtree Transfers", 
    "arxiv-id": "cs/0211009v1", 
    "author": "Siu-Ming Yiu", 
    "publish": "2002-11-11T12:02:30Z", 
    "summary": "The number of the non-shared edges of two phylogenies is a basic measure of\nthe dissimilarity between the phylogenies. The non-shared edges are also the\nbuilding block for approximating a more sophisticated metric called the nearest\nneighbor interchange (NNI) distance. In this paper, we give the first\nsubquadratic-time algorithm for finding the non-shared edges, which are then\nused to speed up the existing approximating algorithm for the NNI distance from\n$O(n^2)$ time to $O(n \\log n)$ time. Another popular distance metric for\nphylogenies is the subtree transfer (STT) distance. Previous work on computing\nthe STT distance considered degree-3 trees only. We give an approximation\nalgorithm for the STT distance for degree-$d$ trees with arbitrary $d$ and with\ngeneralized STT operations."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211010v2", 
    "other_authors": "Stephen Alstrup, Michael A. Bender, Erik D. Demaine, Martin Farach-Colton, Theis Rauhe, Mikkel Thorup", 
    "title": "Efficient Tree Layout in a Multilevel Memory Hierarchy", 
    "arxiv-id": "cs/0211010v2", 
    "author": "Mikkel Thorup", 
    "publish": "2002-11-12T03:32:02Z", 
    "summary": "We consider the problem of laying out a tree with fixed parent/child\nstructure in hierarchical memory. The goal is to minimize the expected number\nof block transfers performed during a search along a root-to-leaf path, subject\nto a given probability distribution on the leaves. This problem was previously\nconsidered by Gil and Itai, who developed optimal but slow algorithms when the\nblock-transfer size B is known. We present faster but approximate algorithms\nfor the same problem; the fastest such algorithm runs in linear time and\nproduces a solution that is within an additive constant of optimal.\n  In addition, we show how to extend any approximately optimal algorithm to the\ncache-oblivious setting in which the block-transfer size is unknown to the\nalgorithm. The query performance of the cache-oblivious layout is within a\nconstant factor of the query performance of the optimal known-block-size\nlayout. Computing the cache-oblivious layout requires only logarithmically many\ncalls to the layout algorithm for known block size; in particular, the\ncache-oblivious layout can be computed in O(N lg N) time, where N is the number\nof nodes.\n  Finally, we analyze two greedy strategies, and show that they have a\nperformance ratio between Omega(lg B / lg lg B) and O(lg B) when compared to\nthe optimal layout."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0211018v2", 
    "other_authors": "Vladimir Pestov, Aleksandar Stojmirovic", 
    "title": "Indexing schemes for similarity search: an illustrated paradigm", 
    "arxiv-id": "cs/0211018v2", 
    "author": "Aleksandar Stojmirovic", 
    "publish": "2002-11-14T19:10:16Z", 
    "summary": "We suggest a variation of the Hellerstein--Koutsoupias--Papadimitriou\nindexability model for datasets equipped with a similarity measure, with the\naim of better understanding the structure of indexing schemes for\nsimilarity-based search and the geometry of similarity workloads. This in\nparticular provides a unified approach to a great variety of schemes used to\nindex into metric spaces and facilitates their transfer to more general\nsimilarity measures such as quasi-metrics. We discuss links between performance\nof indexing schemes and high-dimensional geometry. The concepts and results are\nillustrated on a very large concrete dataset of peptide fragments equipped with\na biologically significant similarity measure."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0212044v1", 
    "other_authors": "Sandor P. Fekete, Henk Meijer, Andre Rohe, Walter Tietze", 
    "title": "Solving a \"Hard\" Problem to Approximate an \"Easy\" One: Heuristics for   Maximum Matchings and Maximum Traveling Salesman Problems", 
    "arxiv-id": "cs/0212044v1", 
    "author": "Walter Tietze", 
    "publish": "2002-12-16T09:39:16Z", 
    "summary": "We consider geometric instances of the Maximum Weighted Matching Problem\n(MWMP) and the Maximum Traveling Salesman Problem (MTSP) with up to 3,000,000\nvertices. Making use of a geometric duality relationship between MWMP, MTSP,\nand the Fermat-Weber-Problem (FWP), we develop a heuristic approach that yields\nin near-linear time solutions as well as upper bounds. Using various\ncomputational tools, we get solutions within considerably less than 1% of the\noptimum.\n  An interesting feature of our approach is that, even though an FWP is hard to\ncompute in theory and Edmonds' algorithm for maximum weighted matching yields a\npolynomial solution for the MWMP, the practical behavior is just the opposite,\nand we can solve the FWP with high accuracy in order to find a good heuristic\nsolution for the MWMP."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0301019v1", 
    "other_authors": "Daniel A. Spielman, Shang-Hua Teng", 
    "title": "Smoothed Analysis of Interior-Point Algorithms: Termination", 
    "arxiv-id": "cs/0301019v1", 
    "author": "Shang-Hua Teng", 
    "publish": "2003-01-21T17:47:05Z", 
    "summary": "We perform a smoothed analysis of the termination phase of an interior-point\nmethod. By combining this analysis with the smoothed analysis of Renegar's\ninterior-point algorithm by Dunagan, Spielman and Teng, we show that the\nsmoothed complexity of an interior-point algorithm for linear programming is $O\n(m^{3} \\log (m/\\sigma))$. In contrast, the best known bound on the worst-case\ncomplexity of linear programming is $O (m^{3} L)$, where $L$ could be as large\nas $m$. We include an introduction to smoothed analysis and a tutorial on proof\ntechniques that have been useful in smoothed analyses."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0301021v2", 
    "other_authors": "Lauro Lins, Sostenes Lins, Silvio Melo", 
    "title": "PHORMA: Perfectly Hashable Order Restricted Multidimensional Arrays", 
    "arxiv-id": "cs/0301021v2", 
    "author": "Silvio Melo", 
    "publish": "2003-01-21T23:55:17Z", 
    "summary": "In this paper we propose a simple and efficient data structure yielding a\nperfect hashing of quite general arrays. The data structure is named phorma,\nwhich is an acronym for perfectly hashable order restricted multidimensional\narray.\n  Keywords: Perfect hash function, Digraph, Implicit enumeration,\nNijenhuis-Wilf combinatorial family."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0302030v2", 
    "other_authors": "David Eppstein", 
    "title": "The traveling salesman problem for cubic graphs", 
    "arxiv-id": "cs/0302030v2", 
    "author": "David Eppstein", 
    "publish": "2003-02-20T06:36:35Z", 
    "summary": "We show how to find a Hamiltonian cycle in a graph of degree at most three\nwith n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithm\ncan find the minimum weight Hamiltonian cycle (traveling salesman problem), in\nthe same time bound. We can also count or list all Hamiltonian cycles in a\ndegree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the traveling\nsalesman problem in graphs of degree at most four, by randomized and\ndeterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n and\nO((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specify\na set of forced edges which must be part of any generated cycle. Our cycle\nlisting algorithm shows that every degree three graph has O(2^{3n/8})\nHamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltonian\ncycles per graph."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0304005v1", 
    "other_authors": "Oded Regev", 
    "title": "Quantum Computation and Lattice Problems", 
    "arxiv-id": "cs/0304005v1", 
    "author": "Oded Regev", 
    "publish": "2003-04-01T23:35:11Z", 
    "summary": "We present the first explicit connection between quantum computation and\nlattice problems. Namely, we show a solution to the Unique Shortest Vector\nProblem (SVP) under the assumption that there exists an algorithm that solves\nthe hidden subgroup problem on the dihedral group by coset sampling. Moreover,\nwe solve the hidden subgroup problem on the dihedral group by using an average\ncase subset sum routine. By combining the two results, we get a quantum\nreduction from $\\Theta(n^{2.5})$-unique-SVP to the average case subset sum\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306046v1", 
    "other_authors": "Paolo Boldi, Sebastiano Vigna", 
    "title": "Compact Approximation of Lattice Functions with Applications to   Large-Alphabet Text Search", 
    "arxiv-id": "cs/0306046v1", 
    "author": "Sebastiano Vigna", 
    "publish": "2003-06-11T09:13:39Z", 
    "summary": "We propose a very simple randomised data structure that stores an\napproximation from above of a lattice-valued function. Computing the function\nvalue requires a constant number of steps, and the error probability can be\nbalanced with space usage, much like in Bloom filters. The structure is\nparticularly well suited for functions that are bottom on most of their domain.\nWe then show how to use our methods to store in a compact way the bad-character\nshift function for variants of the Boyer-Moore text search algorithms. As a\nresult, we obtain practical implementations of these algorithms that can be\nused with large alphabets, such as Unicode collation elements, with a small\nsetup time. The ideas described in this paper have been implemented as free\nsoftware under the GNU General Public License within the MG4J project\n(http://mg4j.dsi.unimi.it/)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306104v1", 
    "other_authors": "Yossi Matias, Ely Porat", 
    "title": "Efficient pebbling for list traversal synopses", 
    "arxiv-id": "cs/0306104v1", 
    "author": "Ely Porat", 
    "publish": "2003-06-16T21:31:36Z", 
    "summary": "We show how to support efficient back traversal in a unidirectional list,\nusing small memory and with essentially no slowdown in forward steps. Using\n$O(\\log n)$ memory for a list of size $n$, the $i$'th back-step from the\nfarthest point reached so far takes $O(\\log i)$ time in the worst case, while\nthe overhead per forward step is at most $\\epsilon$ for arbitrary small\nconstant $\\epsilon>0$. An arbitrary sequence of forward and back steps is\nallowed. A full trade-off between memory usage and time per back-step is\npresented: $k$ vs. $kn^{1/k}$ and vice versa. Our algorithms are based on a\nnovel pebbling technique which moves pebbles on a virtual binary, or $t$-ary,\ntree that can only be traversed in a pre-order fashion. The compact data\nstructures used by the pebbling algorithms, called list traversal synopses,\nextend to general directed graphs, and have other interesting applications,\nincluding memory efficient hash-chain implementation. Perhaps the most\nsurprising application is in showing that for any program, arbitrary rollback\nsteps can be efficiently supported with small overhead in memory, and marginal\noverhead in its ordinary execution. More concretely: Let $P$ be a program that\nruns for at most $T$ steps, using memory of size $M$. Then, at the cost of\nrecording the input used by the program, and increasing the memory by a factor\nof $O(\\log T)$ to $O(M \\log T)$, the program $P$ can be extended to support an\narbitrary sequence of forward execution and rollback steps: the $i$'th rollback\nstep takes $O(\\log i)$ time in the worst case, while forward steps take O(1)\ntime in the worst case, and $1+\\epsilon$ amortized time per step."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0306123v1", 
    "other_authors": "Daniel Etzold", 
    "title": "Heuristic to reduce the complexity of complete bipartite graphs to   accelerate the search for maximum weighted matchings with small error", 
    "arxiv-id": "cs/0306123v1", 
    "author": "Daniel Etzold", 
    "publish": "2003-06-23T19:37:42Z", 
    "summary": "A maximum weighted matching for bipartite graphs $G=(A \\cup B,E)$ can be\nfound by using the algorithm of Edmonds and Karp with a Fibonacci Heap and a\nmodified Dijkstra in $O(nm + n^2 \\log{n})$ time where n is the number of nodes\nand m the number of edges. For the case that $|A|=|B|$ the number of edges is\n$n^2$ and therefore the complexity is $O(n^3)$. In this paper we want to\npresent a simple heuristic method to reduce the number of edges of complete\nbipartite graphs $G=(A \\cup B,E)$ with $|A|=|B|$ such that $m = n\\log{n}$ and\ntherefore the complexity of such that $m = n\\log{n}$ and therefore the\ncomplexity of $O(n^2 \\log{n})$. The weights of all edges in G must be uniformly\ndistributed in [0,1]."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0307034v1", 
    "other_authors": "Danny Krizanc, Pat Morin, Michiel Smid", 
    "title": "Range Mode and Range Median Queries on Lists and Trees", 
    "arxiv-id": "cs/0307034v1", 
    "author": "Michiel Smid", 
    "publish": "2003-07-12T21:41:56Z", 
    "summary": "We consider algorithms for preprocessing labelled lists and trees so that,\nfor any two nodes u and v we can answer queries of the form: What is the mode\nor median label in the sequence of labels on the path from u to v."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0307043v1", 
    "other_authors": "Aravind Srinivasan", 
    "title": "An Extension of the Lovasz Local Lemma, and its Applications to Integer   Programming", 
    "arxiv-id": "cs/0307043v1", 
    "author": "Aravind Srinivasan", 
    "publish": "2003-07-18T04:02:18Z", 
    "summary": "The Lovasz Local Lemma due to Erdos and Lovasz is a powerful tool in proving\nthe existence of rare events. We present an extension of this lemma, which\nworks well when the event to be shown to exist is a conjunction of individual\nevents, each of which asserts that a random variable does not deviate much from\nits mean. As applications, we consider two classes of NP-hard integer programs:\nminimax and covering integer programs. A key technique, randomized rounding of\nlinear relaxations, was developed by Raghavan and Thompson to derive good\napproximation algorithms for such problems. We use our extension of the Local\nLemma to prove that randomized rounding produces, with non-zero probability,\nmuch better feasible solutions than known before, if the constraint matrices of\nthese integer programs are column-sparse (e.g., routing using short paths,\nproblems on hypergraphs with small dimension/degree). This complements certain\nwell-known results from discrepancy theory. We also generalize the method of\npessimistic estimators due to Raghavan, to obtain constructive (algorithmic)\nversions of our results for covering integer programs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539794268388", 
    "link": "http://arxiv.org/pdf/cs/0308041v1", 
    "other_authors": "Andrej Brodnik, Andreas Nilsson", 
    "title": "Static Data Structure for Discrete Advance Bandwidth Reservations on the   Internet", 
    "arxiv-id": "cs/0308041v1", 
    "author": "Andreas Nilsson", 
    "publish": "2003-08-24T06:30:41Z", 
    "summary": "In this paper we present a discrete data structure for reservations of\nlimited resources. A reservation is defined as a tuple consisting of the time\ninterval of when the resource should be reserved, $I_R$, and the amount of the\nresource that is reserved, $B_R$, formally $R=\\{I_R,B_R\\}$.\n  The data structure is similar to a segment tree. The maximum spanning\ninterval of the data structure is fixed and defined in advance. The granularity\nand thereby the size of the intervals of the leaves is also defined in advance.\nThe data structure is built only once. Neither nodes nor leaves are ever\ninserted, deleted or moved. Hence, the running time of the operations does not\ndepend on the number of reservations previously made. The running time does not\ndepend on the size of the interval of the reservation either. Let $n$ be the\nnumber of leaves in the data structure. In the worst case, the number of\ntouched (i.e. traversed) nodes is in any operation $O(\\log n)$, hence the\nrunning time of any operation is also $O(\\log n)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0309043v1", 
    "other_authors": "A. H. L. Porto, V. C. Barbosa", 
    "title": "Finding approximate palindromes in strings", 
    "arxiv-id": "cs/0309043v1", 
    "author": "V. C. Barbosa", 
    "publish": "2003-09-23T13:45:48Z", 
    "summary": "We introduce a novel definition of approximate palindromes in strings, and\nprovide an algorithm to find all maximal approximate palindromes in a string\nwith up to $k$ errors. Our definition is based on the usual edit operations of\napproximate pattern matching, and the algorithm we give, for a string of size\n$n$ on a fixed alphabet, runs in $O(k^2 n)$ time. We also discuss two\nimplementation-related improvements to the algorithm, and demonstrate their\nefficacy in practice by means of both experiments and an average-case analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0310065v2", 
    "other_authors": "Stephen Alstrup, Jacob Holm, Kristian de Lichtenberg, Mikkel Thorup", 
    "title": "Maintaining Information in Fully-Dynamic Trees with Top Trees", 
    "arxiv-id": "cs/0310065v2", 
    "author": "Mikkel Thorup", 
    "publish": "2003-10-31T18:37:47Z", 
    "summary": "We introduce top trees as a design of a new simpler interface for data\nstructures maintaining information in a fully-dynamic forest. We demonstrate\nhow easy and versatile they are to use on a host of different applications. For\nexample, we show how to maintain the diameter, center, and median of each tree\nin the forest. The forest can be updated by insertion and deletion of edges and\nby changes to vertex and edge weights. Each update is supported in O(log n)\ntime, where n is the size of the tree(s) involved in the update. Also, we show\nhow to support nearest common ancestor queries and level ancestor queries with\nrespect to arbitrary roots in O(log n) time. Finally, with marked and unmarked\nvertices, we show how to compute distances to a nearest marked vertex. The\nlater has applications to approximate nearest marked vertex in general graphs,\nand thereby to static optimization problems over shortest path metrics.\n  Technically speaking, top trees are easily implemented either with\nFrederickson's topology trees [Ambivalent Data Structures for Dynamic\n2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp.\n484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure for\nDynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, we\nclaim that the interface is simpler for many applications, and indeed our new\nbounds are quadratic improvements over previous bounds where they exist."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0311030v1", 
    "other_authors": "Zoe Abrams, Ashish Goel, Serge Plotkin", 
    "title": "Set K-Cover Algorithms for Energy Efficient Monitoring in Wireless   Sensor Networks", 
    "arxiv-id": "cs/0311030v1", 
    "author": "Serge Plotkin", 
    "publish": "2003-11-20T22:47:11Z", 
    "summary": "Wireless sensor networks (WSNs) are emerging as an effective means for\nenvironment monitoring. This paper investigates a strategy for energy efficient\nmonitoring in WSNs that partitions the sensors into covers, and then activates\nthe covers iteratively in a round-robin fashion. This approach takes advantage\nof the overlap created when many sensors monitor a single area. Our work builds\nupon previous work in \"Power Efficient Organization of Wireless Sensor\nNetworks\" by Slijepcevic and Potkonjak, where the model is first formulated. We\nhave designed three approximation algorithms for a variation of the SET K-COVER\nproblem, where the objective is to partition the sensors into covers such that\nthe number of covers that include an area, summed over all areas, is maximized.\nThe first algorithm is randomized and partitions the sensors, in expectation,\nwithin a fraction 1 - 1/e (~.63) of the optimum. We present two other\ndeterministic approximation algorithms. One is a distributed greedy algorithm\nwith a 1/2 approximation ratio and the other is a centralized greedy algorithm\nwith a 1 - 1/e approximation ratio. We show that it is NP-Complete to guarantee\nbetter than 15/16 of the optimal coverage, indicating that all three algorithms\nperform well with respect to the best approximation algorithm possible.\nSimulations indicate that in practice, the deterministic algorithms perform far\nabove their worst case bounds, consistently covering more than 72% of what is\ncovered by an optimum solution. Simulations also indicate that the increase in\nlongevity is proportional to the amount of overlap amongst the sensors. The\nalgorithms are fast, easy to use, and according to simulations, significantly\nincrease the longevity of sensor networks. The randomized algorithm in\nparticular seems quite practical."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0312054v1", 
    "other_authors": "Krzysztof C. Kiwiel", 
    "title": "Partitioning schemes for quicksort and quickselect", 
    "arxiv-id": "cs/0312054v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2003-12-23T03:47:55Z", 
    "summary": "We introduce several modifications of the partitioning schemes used in\nHoare's quicksort and quickselect algorithms, including ternary schemes which\nidentify keys less or greater than the pivot. We give estimates for the numbers\nof swaps made by each scheme. Our computational experiments indicate that\nternary schemes allow quickselect to identify all keys equal to the selected\nkey at little additional cost."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0312055v1", 
    "other_authors": "Krzysztof C. Kiwiel", 
    "title": "Randomized selection with quintary partitions", 
    "arxiv-id": "cs/0312055v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2003-12-23T04:12:30Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select for\nfinding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This\nrectifies the analysis of Floyd and Rivest, and extends it to the case of\nnondistinct elements. Our computational results confirm that Select may be the\nbest algorithm in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0401003v1", 
    "other_authors": "Krzysztof C. Kiwiel", 
    "title": "Randomized selection with tripartitioning", 
    "arxiv-id": "cs/0401003v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2004-01-04T05:29:58Z", 
    "summary": "We show that several versions of Floyd and Rivest's algorithm Select [Comm.\\\nACM {\\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require\nat most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average, even when equal elements\noccur. This parallels our recent analysis of another variant due to Floyd and\nRivest [Comm. ACM {\\bf 18} (1975) 165--172]. Our computational results suggest\nthat both variants perform well in practice, and may compete with other\nselection methods, such as Hoare's Find or quickselect with median-of-3 pivots."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0402005v1", 
    "other_authors": "Krzysztof C. Kiwiel", 
    "title": "Improved randomized selection", 
    "arxiv-id": "cs/0402005v1", 
    "author": "Krzysztof C. Kiwiel", 
    "publish": "2004-02-02T14:16:52Z", 
    "summary": "We show that several versions of Floyd and Rivest's improved algorithm Select\nfor finding the $k$th smallest of $n$ elements require at most\n$n+\\min\\{k,n-k\\}+O(n^{1/2}\\ln^{1/2}n)$ comparisons on average and with high\nprobability. This rectifies the analysis of Floyd and Rivest, and extends it to\nthe case of nondistinct elements. Encouraging computational results on large\nmedian-finding problems are reported."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0402045v2", 
    "other_authors": "Esther M. Arkin, Michael A. Bender, Sandor P. Fekete, Joseph S. B. Mitchell, Martin Skutella", 
    "title": "The Freeze-Tag Problem: How to Wake Up a Swarm of Robots", 
    "arxiv-id": "cs/0402045v2", 
    "author": "Martin Skutella", 
    "publish": "2004-02-18T20:49:02Z", 
    "summary": "An optimization problem that naturally arises in the study of swarm robotics\nis the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, by\nhaving an awakened robot move to their locations. Once a robot is awake, it can\nassist in awakening other slumbering robots.The objective is to have all robots\nawake as early as possible. While the FTP bears some resemblance to problems\nfrom areas in combinatorial optimization such as routing, broadcasting,\nscheduling, and covering, its algorithmic characteristics are surprisingly\ndifferent. We consider both scenarios on graphs and in geometric\nenvironments.In graphs, robots sleep at vertices and there is a length function\non the edges. Awake robots travel along edges, with time depending on edge\nlength. For most scenarios, we consider the offline version of the problem, in\nwhich each awake robot knows the position of all other robots. We prove that\nthe problem is NP-hard, even for the special case of star graphs. We also\nestablish hardness of approximation, showing that it is NP-hard to obtain an\napproximation factor better than 5/3, even for graphs of bounded degree.These\nlower bounds are complemented with several positive algorithmic results,\nincluding: (1) We show that the natural greedy strategy on star graphs has a\ntight worst-case performance of 7/3 and give a polynomial-time approximation\nscheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive online\nalgorithm for graphs with maximum degree D and locally bounded edge weights.\n(3) We give a PTAS, running in nearly linear time, for geometrically embedded\ninstances."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0403022v2", 
    "other_authors": "Michael N\u00fcsken, Martin Ziegler", 
    "title": "Fast Multipoint-Evaluation of Bivariate Polynomials", 
    "arxiv-id": "cs/0403022v2", 
    "author": "Martin Ziegler", 
    "publish": "2004-03-12T14:31:43Z", 
    "summary": "We generalize univariate multipoint evaluation of polynomials of degree n at\nsublinear amortized cost per point. More precisely, it is shown how to evaluate\na bivariate polynomial p of maximum degree less than n, specified by its n^2\ncoefficients, simultaneously at n^2 given points using a total of O(n^{2.667})\narithmetic operations. In terms of the input size N being quadratic in n, this\namounts to an amortized cost of O(N^{0.334}) per point."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0404028v1", 
    "other_authors": "Saju Jude Dominic, G. Sajith", 
    "title": "The Random Buffer Tree : A Randomized Technique for I/O-efficient   Algorithms", 
    "arxiv-id": "cs/0404028v1", 
    "author": "G. Sajith", 
    "publish": "2004-04-13T13:49:11Z", 
    "summary": "In this paper, we present a probabilistic self-balancing dictionary data\nstructure for massive data sets, and prove expected amortized I/O-optimal\nbounds on the dictionary operations. We show how to use the structure as an\nI/O-optimal priority queue. The data structure, which we call as the random\nbuffer tree, abstracts the properties of the random treap and the buffer tree\nand has the same expected I/O-bounds as the buffer tree."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0404058v1", 
    "other_authors": "Donald E. Knuth, Frank Ruskey", 
    "title": "Efficient coroutine generation of constrained Gray sequences", 
    "arxiv-id": "cs/0404058v1", 
    "author": "Frank Ruskey", 
    "publish": "2004-04-30T00:00:00Z", 
    "summary": "We study an interesting family of cooperating coroutines, which is able to\ngenerate all patterns of bits that satisfy certain fairly general ordering\nconstraints, changing only one bit at a time. (More precisely, the directed\ngraph of constraints is required to be cycle-free when it is regarded as an\nundirected graph.) If the coroutines are implemented carefully, they yield an\nalgorithm that needs only a bounded amount of computation per bit change,\nthereby solving an open problem in the field of combinatorial pattern\ngeneration."
},{
    "category": "cs.DS", 
    "doi": "10.1016/S0031-3203(01)00179-0", 
    "link": "http://arxiv.org/pdf/cs/0405094v1", 
    "other_authors": "Taneli Mielik\u00e4inen, Esko Ukkonen", 
    "title": "The Complexity of Maximum Matroid-Greedoid Intersection and Weighted   Greedoid Maximization", 
    "arxiv-id": "cs/0405094v1", 
    "author": "Esko Ukkonen", 
    "publish": "2004-05-25T12:09:34Z", 
    "summary": "The maximum intersection problem for a matroid and a greedoid, given by\npolynomial-time oracles, is shown $NP$-hard by expressing the satisfiability of\nboolean formulas in 3-conjunctive normal form as such an intersection. The\ncorresponding approximation problems are shown $NP$-hard for certain\napproximation performance bounds. Moreover, some natural parameterized variants\nof the problem are shown $W[P]$-hard. The results are in contrast with the\nmaximum matroid-matroid intersection which is solvable in polynomial time by an\nold result of Edmonds. We also prove that it is $NP$-hard to approximate the\nweighted greedoid maximization within $2^{n^{O(1)}}$ where $n$ is the size of\nthe domain of the greedoid.\n  A preliminary version ``The Complexity of Maximum Matroid-Greedoid\nIntersection'' appeared in Proc. FCT 2001, LNCS 2138, pp. 535--539,\nSpringer-Verlag 2001."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2005.05.008", 
    "link": "http://arxiv.org/pdf/cs/0406028v1", 
    "other_authors": "Yair Bartal, Bela Bollobas, Manor Mendel", 
    "title": "Ramsey-type theorems for metric spaces with applications to online   problems", 
    "arxiv-id": "cs/0406028v1", 
    "author": "Manor Mendel", 
    "publish": "2004-06-16T21:56:48Z", 
    "summary": "A nearly logarithmic lower bound on the randomized competitive ratio for the\nmetrical task systems problem is presented. This implies a similar lower bound\nfor the extensively studied k-server problem. The proof is based on Ramsey-type\ntheorems for metric spaces, that state that every metric space contains a large\nsubspace which is approximately a hierarchically well-separated tree (and in\nparticular an ultrametric). These Ramsey-type theorems may be of independent\ninterest."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jalgor.2004.06.002", 
    "link": "http://arxiv.org/pdf/cs/0406033v2", 
    "other_authors": "Manor Mendel", 
    "title": "Randomized k-server algorithms for growth-rate bounded graphs", 
    "arxiv-id": "cs/0406033v2", 
    "author": "Manor Mendel", 
    "publish": "2004-06-17T15:11:54Z", 
    "summary": "The paper referred to in the title is withdrawn."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539700376159", 
    "link": "http://arxiv.org/pdf/cs/0406034v1", 
    "other_authors": "Amos Fiat, Manor Mendel", 
    "title": "Better algorithms for unfair metrical task systems and applications", 
    "arxiv-id": "cs/0406034v1", 
    "author": "Manor Mendel", 
    "publish": "2004-06-17T18:49:20Z", 
    "summary": "Unfair metrical task systems are a generalization of online metrical task\nsystems. In this paper we introduce new techniques to combine algorithms for\nunfair metrical task systems and apply these techniques to obtain improved\nrandomized online algorithms for metrical task systems on arbitrary metric\nspaces."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0406036v1", 
    "other_authors": "Manor Mendel, Steven S. Seiden", 
    "title": "Online Companion Caching", 
    "arxiv-id": "cs/0406036v1", 
    "author": "Steven S. Seiden", 
    "publish": "2004-06-18T16:20:24Z", 
    "summary": "This paper is concerned with online caching algorithms for the\n(n,k)-companion cache, defined by Brehob et. al. In this model the cache is\ncomposed of two components: a k-way set-associative cache and a companion\nfully-associative cache of size n. We show that the deterministic competitive\nratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio is\nO(\\log n \\log k) and \\Omega(\\log n +\\log k)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0406045v3", 
    "other_authors": "Erik D. Demaine, Sandor P. Fekete, Shmuel Gal", 
    "title": "Online Searching with Turn Cost", 
    "arxiv-id": "cs/0406045v3", 
    "author": "Shmuel Gal", 
    "publish": "2004-06-23T16:56:53Z", 
    "summary": "We consider the problem of searching for an object on a line at an unknown\ndistance OPT from the original position of the searcher, in the presence of a\ncost of d for each time the searcher changes direction. This is a\ngeneralization of the well-studied linear-search problem. We describe a\nstrategy that is guaranteed to find the object at a cost of at most 9*OPT + 2d,\nwhich has the optimal competitive ratio 9 with respect to OPT plus the minimum\ncorresponding additive term. Our argument for upper and lower bound uses an\ninfinite linear program, which we solve by experimental solution of an infinite\nseries of approximating finite linear programs, estimating the limits, and\nsolving the resulting recurrences. We feel that this technique is interesting\nin its own right and should help solve other searching problems. In particular,\nwe consider the star search or cow-path problem with turn cost, where the\nhidden object is placed on one of m rays emanating from the original position\nof the searcher. For this problem we give a tight bound of\n(1+(2(m^m)/((m-1)^(m-1))) OPT + m ((m/(m-1))^(m-1) - 1) d. We also discuss\ntradeoff between the corresponding coefficients, and briefly consider\nrandomized strategies on the line."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0407003v1", 
    "other_authors": "Michael A. Bender, Martin Farach-Colton, Miguel Mosteiro", 
    "title": "Insertion Sort is O(n log n)", 
    "arxiv-id": "cs/0407003v1", 
    "author": "Miguel Mosteiro", 
    "publish": "2004-07-01T15:50:26Z", 
    "summary": "Traditional Insertion Sort runs in O(n^2) time because each insertion takes\nO(n) time. When people run Insertion Sort in the physical world, they leave\ngaps between items to accelerate insertions. Gaps help in computers as well.\nThis paper shows that Gapped Insertion Sort has insertion times of O(log n)\nwith high probability, yielding a total running time of O(n log n) with high\nprobability."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2004.05.015", 
    "link": "http://arxiv.org/pdf/cs/0407023v1", 
    "other_authors": "Rina Panigrahy", 
    "title": "Efficient Hashing with Lookups in two Memory Accesses", 
    "arxiv-id": "cs/0407023v1", 
    "author": "Rina Panigrahy", 
    "publish": "2004-07-09T22:23:40Z", 
    "summary": "The study of hashing is closely related to the analysis of balls and bins. It\nis well-known that instead of using a single hash function if we randomly hash\na ball into two bins and place it in the smaller of the two, then this\ndramatically lowers the maximum load on bins. This leads to the concept of\ntwo-way hashing where the largest bucket contains $O(\\log\\log n)$ balls with\nhigh probability. The hash look up will now search in both the buckets an item\nhashes to. Since an item may be placed in one of two buckets, we could\npotentially move an item after it has been initially placed to reduce maximum\nload. with a maximum load of We show that by performing moves during inserts, a\nmaximum load of 2 can be maintained on-line, with high probability, while\nsupporting hash update operations. In fact, with $n$ buckets, even if the space\nfor two items are pre-allocated per bucket, as may be desirable in hardware\nimplementations, more than $n$ items can be stored giving a high memory\nutilization. We also analyze the trade-off between the number of moves\nperformed during inserts and the maximum load on a bucket. By performing at\nmost $h$ moves, we can maintain a maximum load of $O(\\frac{\\log \\log n}{h\n\\log(\\log\\log n/h)})$. So, even by performing one move, we achieve a better\nbound than by performing no moves at all."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1597036.1597042", 
    "link": "http://arxiv.org/pdf/cs/0407036v1", 
    "other_authors": "David Eppstein", 
    "title": "All Maximal Independent Sets and Dynamic Dominance for Sparse Graphs", 
    "arxiv-id": "cs/0407036v1", 
    "author": "David Eppstein", 
    "publish": "2004-07-15T21:04:45Z", 
    "summary": "We describe algorithms, based on Avis and Fukuda's reverse search paradigm,\nfor listing all maximal independent sets in a sparse graph in polynomial time\nand delay per output. For bounded degree graphs, our algorithms take constant\ntime per set generated; for minor-closed graph families, the time is O(n) per\nset, and for more general sparse graph families we achieve subquadratic time\nper set. We also describe new data structures for maintaining a dynamic vertex\nset S in a sparse or minor-closed graph family, and querying the number of\nvertices not dominated by S; for minor-closed graph families the time per\nupdate is constant, while it is sublinear for any sparse graph family. We can\nalso maintain a dynamic vertex set in an arbitrary m-edge graph and test the\nindependence of the maintained set in time O(sqrt m) per update. We use the\ndomination data structures as part of our enumeration algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0408003v1", 
    "other_authors": "Yair Bartal, Manor Mendel", 
    "title": "Multi-Embedding of Metric Spaces", 
    "arxiv-id": "cs/0408003v1", 
    "author": "Manor Mendel", 
    "publish": "2004-08-02T16:42:43Z", 
    "summary": "Metric embedding has become a common technique in the design of algorithms.\nIts applicability is often dependent on how high the embedding's distortion is.\nFor example, embedding finite metric space into trees may require linear\ndistortion as a function of its size. Using probabilistic metric embeddings,\nthe bound on the distortion reduces to logarithmic in the size.\n  We make a step in the direction of bypassing the lower bound on the\ndistortion in terms of the size of the metric. We define \"multi-embeddings\" of\nmetric spaces in which a point is mapped onto a set of points, while keeping\nthe target metric of polynomial size and preserving the distortion of paths.\nThe distortion obtained with such multi-embeddings into ultrametrics is at most\nO(log Delta loglog Delta) where Delta is the aspect ratio of the metric. In\nparticular, for expander graphs, we are able to obtain constant distortion\nembeddings into trees in contrast with the Omega(log n) lower bound for all\nprevious notions of embeddings.\n  We demonstrate the algorithmic application of the new embeddings for two\noptimization problems: group Steiner tree and metrical task systems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0408040v1", 
    "other_authors": "William F. Gilreath", 
    "title": "Hash sort: A linear time complexity multiple-dimensional sort algorithm", 
    "arxiv-id": "cs/0408040v1", 
    "author": "William F. Gilreath", 
    "publish": "2004-08-17T09:23:35Z", 
    "summary": "Sorting and hashing are two completely different concepts in computer\nscience, and appear mutually exclusive to one another. Hashing is a search\nmethod using the data as a key to map to the location within memory, and is\nused for rapid storage and retrieval. Sorting is a process of organizing data\nfrom a random permutation into an ordered arrangement, and is a common activity\nperformed frequently in a variety of applications.\n  Almost all conventional sorting algorithms work by comparison, and in doing\nso have a linearithmic greatest lower bound on the algorithmic time complexity.\nAny improvement in the theoretical time complexity of a sorting algorithm can\nresult in overall larger gains in implementation performance.. A gain in\nalgorithmic performance leads to much larger gains in speed for the application\nthat uses the sort algorithm. Such a sort algorithm needs to use an alternative\nmethod for ordering the data than comparison, to exceed the linearithmic time\ncomplexity boundary on algorithmic performance.\n  The hash sort is a general purpose non-comparison based sorting algorithm by\nhashing, which has some interesting features not found in conventional sorting\nalgorithms. The hash sort asymptotically outperforms the fastest traditional\nsorting algorithm, the quick sort. The hash sort algorithm has a linear time\ncomplexity factor -- even in the worst case. The hash sort opens an area for\nfurther work and investigation into alternative means of sorting."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0410046v1", 
    "other_authors": "Marek Chrobak, Christoph Durr, Wojciech Jawor, Lukasz Kowalik, Maciej Kurowski", 
    "title": "A Note on Scheduling Equal-Length Jobs to Maximize Throughput", 
    "arxiv-id": "cs/0410046v1", 
    "author": "Maciej Kurowski", 
    "publish": "2004-10-18T22:41:30Z", 
    "summary": "We study the problem of scheduling equal-length jobs with release times and\ndeadlines, where the objective is to maximize the number of completed jobs.\nPreemptions are not allowed. In Graham's notation, the problem is described as\n1|r_j;p_j=p|\\sum U_j. We give the following results: (1) We show that the often\ncited algorithm by Carlier from 1981 is not correct. (2) We give an algorithm\nfor this problem with running time O(n^5)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0410048v4", 
    "other_authors": "Erik D. Demaine, John Iacono, Stefan Langerman", 
    "title": "Worst-Case Optimal Tree Layout in External Memory", 
    "arxiv-id": "cs/0410048v4", 
    "author": "Stefan Langerman", 
    "publish": "2004-10-19T15:17:57Z", 
    "summary": "Consider laying out a fixed-topology tree of N nodes into external memory\nwith block size B so as to minimize the worst-case number of block memory\ntransfers required to traverse a path from the root to a node of depth D. We\nprove that the optimal number of memory transfers is $$ \\cases{\n  \\displaystyle\n  \\Theta\\left( {D \\over \\lg (1{+}B)} \\right)\n  & when $D = O(\\lg N)$, \\cr\n  \\displaystyle\n  \\Theta\\left( {\\lg N \\over \\lg \\left(1{+}{B \\lg N \\over D}\\right)} \\right)\n  & when $D = \\Omega(\\lg N)$ and $D = O(B \\lg N)$, \\cr\n  \\displaystyle\n  \\Theta\\left( {D \\over B} \\right)\n  & when $D = \\Omega(B \\lg N)$.\n  } $$"
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412004v1", 
    "other_authors": "L. Allison", 
    "title": "Finding Approximate Palindromes in Strings Quickly and Simply", 
    "arxiv-id": "cs/0412004v1", 
    "author": "L. Allison", 
    "publish": "2004-12-01T17:08:55Z", 
    "summary": "Described are two algorithms to find long approximate palindromes in a\nstring, for example a DNA sequence. A simple algorithm requires O(n)-space and\nalmost always runs in $O(k.n)$-time where n is the length of the string and k\nis the number of ``errors'' allowed in the palindrome. Its worst-case\ntime-complexity is $O(n^2)$ but this does not occur with real biological\nsequences. A more complex algorithm guarantees $O(k.n)$ worst-case time\ncomplexity."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412006v1", 
    "other_authors": "Sidi Mohamed Sedjelmaci", 
    "title": "The Accelerated Euclidean Algorithm", 
    "arxiv-id": "cs/0412006v1", 
    "author": "Sidi Mohamed Sedjelmaci", 
    "publish": "2004-12-02T15:01:39Z", 
    "summary": "We present a new GCD algorithm of two integers or polynomials. The algorithm\nis iterative and its time complexity is still $O(n \\\\log^2 n ~ log \\\\log n)$\nfor $n$-bit inputs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412089v1", 
    "other_authors": "Evgeny Yanenko", 
    "title": "Evolving Categories: Consistent Framework for Representation of Data and   Algorithms", 
    "arxiv-id": "cs/0412089v1", 
    "author": "Evgeny Yanenko", 
    "publish": "2004-12-17T22:58:13Z", 
    "summary": "A concept of \"evolving categories\" is suggested to build a simple, scalable,\nmathematically consistent framework for representing in uniform way both data\nand algorithms. A state machine for executing algorithms becomes clear, rich\nand powerful semantics, based on category theory, and still allows easy\nimplementation. Moreover, it gives an original insight into the nature and\nsemantics of algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412094v1", 
    "other_authors": "Philippe Baptiste, Marek Chrobak, Christoph Durr, Francis Sourd", 
    "title": "Preemptive Multi-Machine Scheduling of Equal-Length Jobs to Minimize the   Average Flow Time", 
    "arxiv-id": "cs/0412094v1", 
    "author": "Francis Sourd", 
    "publish": "2004-12-20T16:15:59Z", 
    "summary": "We study the problem of preemptive scheduling of n equal-length jobs with\ngiven release times on m identical parallel machines. The objective is to\nminimize the average flow time. Recently, Brucker and Kravchenko proved that\nthe optimal schedule can be computed in polynomial time by solving a linear\nprogram with O(n^3) variables and constraints, followed by some substantial\npost-processing (where n is the number of jobs.) In this note we describe a\nsimple linear program with only O(mn) variables and constraints. Our linear\nprogram produces directly the optimal schedule and does not require any\npost-processing."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0412100v1", 
    "other_authors": "Peter H. Deussen, Stephan Tobies", 
    "title": "Formal Test Purposes and The Validity of Test Cases", 
    "arxiv-id": "cs/0412100v1", 
    "author": "Stephan Tobies", 
    "publish": "2004-12-22T08:53:49Z", 
    "summary": "We give a formalization of the notion of test purpose based on (suitably\nrestricted) Message Sequence Charts. We define the validity of test cases with\nrespect to such a formal test purpose and provide a simple decision procedure\nfor validity."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0501020v1", 
    "other_authors": "Francesco Buccafurri, Gianluca Lax, Domenico Sacca', Luigi Pontieri, Domenico Rosaci", 
    "title": "Enhancing Histograms by Tree-Like Bucket Indices", 
    "arxiv-id": "cs/0501020v1", 
    "author": "Domenico Rosaci", 
    "publish": "2005-01-11T10:15:31Z", 
    "summary": "Histograms are used to summarize the contents of relations into a number of\nbuckets for the estimation of query result sizes. Several techniques (e.g.,\nMaxDiff and V-Optimal) have been proposed in the past for determining bucket\nboundaries which provide accurate estimations. However, while search strategies\nfor optimal bucket boundaries are rather sophisticated, no much attention has\nbeen paid for estimating queries inside buckets and all of the above techniques\nadopt naive methods for such an estimation. This paper focuses on the problem\nof improving the estimation inside a bucket once its boundaries have been\nfixed. The proposed technique is based on the addition, to each bucket, of\n32-bit additional information (organized into a 4-level tree index), storing\napproximate cumulative frequencies at 7 internal intervals of the bucket. Both\ntheoretical analysis and experimental results show that, among a number of\nalternative ways to organize the additional information, the 4-level tree index\nprovides the best frequency estimation inside a bucket. The index is later\nadded to two well-known histograms, MaxDiff and V-Optimal, obtaining the\nnon-obvious result that despite the spatial cost of 4LT which reduces the\nnumber of allowed buckets once the storage space has been fixed, the original\nmethods are strongly improved in terms of accuracy."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502032v1", 
    "other_authors": "Christian Worm Mortensen, Rasmus Pagh, Mihai Patrascu", 
    "title": "On Dynamic Range Reporting in One Dimension", 
    "arxiv-id": "cs/0502032v1", 
    "author": "Mihai Patrascu", 
    "publish": "2005-02-05T23:22:37Z", 
    "summary": "We consider the problem of maintaining a dynamic set of integers and\nanswering queries of the form: report a point (equivalently, all points) in a\ngiven interval. Range searching is a natural and fundamental variant of integer\nsearch, and can be solved using predecessor search. However, for a RAM with\nw-bit words, we show how to perform updates in O(lg w) time and answer queries\nin O(lglg w) time. The update time is identical to the van Emde Boas structure,\nbut the query time is exponentially faster. Existing lower bounds show that\nachieving our query time for predecessor search requires doubly-exponentially\nslower updates. We present some arguments supporting the conjecture that our\nsolution is optimal.\n  Our solution is based on a new and interesting recursion idea which is \"more\nextreme\" that the van Emde Boas recursion. Whereas van Emde Boas uses a simple\nrecursion (repeated halving) on each path in a trie, we use a nontrivial, van\nEmde Boas-like recursion on every such path. Despite this, our algorithm is\nquite clean when seen from the right angle. To achieve linear space for our\ndata structure, we solve a problem which is of independent interest. We develop\nthe first scheme for dynamic perfect hashing requiring sublinear space. This\ngives a dynamic Bloomier filter (an approximate storage scheme for sparse\nvectors) which uses low space. We strengthen previous lower bounds to show that\nthese results are optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502054v1", 
    "other_authors": "Ion I. Mandoiu, Claudia Prajescu, Dragos Trinca", 
    "title": "Improved Tag Set Design and Multiplexing Algorithms for Universal Arrays", 
    "arxiv-id": "cs/0502054v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-02-10T20:20:53Z", 
    "summary": "In this paper we address two optimization problems arising in the design of\ngenomic assays based on universal tag arrays. First, we address the universal\narray tag set design problem. For this problem, we extend previous formulations\nto incorporate antitag-to-antitag hybridization constraints in addition to\nconstraints on antitag-to-tag hybridization specificity, establish a\nconstructive upper bound on the maximum number of tags satisfying the extended\nconstraints, and propose a simple greedy tag selection algorithm. Second, we\ngive methods for improving the multiplexing rate in large-scale genomic assays\nby combining primer selection with tag assignment. Experimental results on\nsimulated data show that this integrated optimization leads to reductions of up\nto 50% in the number of required arrays."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502065v1", 
    "other_authors": "Bhaskar DasGupta, Kishori M. Konwar, Ion I. Mandoiu, Alex A. Shvartsman", 
    "title": "Highly Scalable Algorithms for Robust String Barcoding", 
    "arxiv-id": "cs/0502065v1", 
    "author": "Alex A. Shvartsman", 
    "publish": "2005-02-14T22:19:52Z", 
    "summary": "String barcoding is a recently introduced technique for genomic-based\nidentification of microorganisms. In this paper we describe the engineering of\nhighly scalable algorithms for robust string barcoding. Our methods enable\ndistinguisher selection based on whole genomic sequences of hundreds of\nmicroorganisms of up to bacterial size on a well-equipped workstation, and can\nbe easily parallelized to further extend the applicability range to thousands\nof bacterial size genomes. Experimental results on both randomly generated and\nNCBI genomic data show that whole-genome based selection results in a number of\ndistinguishers nearly matching the information theoretic lower bounds for the\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0502073v1", 
    "other_authors": "Maxime Crochemore, Jacques D\u00e9sarm\u00e9nien, Dominique Perrin", 
    "title": "A note on the Burrows-Wheeler transformation", 
    "arxiv-id": "cs/0502073v1", 
    "author": "Dominique Perrin", 
    "publish": "2005-02-17T07:06:28Z", 
    "summary": "We relate the Burrows-Wheeler transformation with a result in combinatorics\non words known as the Gessel-Reutenauer transformation."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0503057v1", 
    "other_authors": "Ion I. Mandoiu, Dragos Trinca", 
    "title": "Exact and Approximation Algorithms for DNA Tag Set Design", 
    "arxiv-id": "cs/0503057v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-03-23T02:36:14Z", 
    "summary": "In this paper we propose new solution methods for designing tag sets for use\nin universal DNA arrays. First, we give integer linear programming formulations\nfor two previous formalizations of the tag set design problem, and show that\nthese formulations can be solved to optimality for instance sizes of practical\ninterest by using general purpose optimization packages. Second, we note the\nbenefits of periodic tags, and establish an interesting connection between the\ntag design problem and the problem of packing the maximum number of\nvertex-disjoint directed cycles in a given graph. We show that combining a\nsimple greedy cycle packing algorithm with a previously proposed alphabetic\ntree search strategy yields an increase of over 40% in the number of tags\ncompared to previous methods."
},{
    "category": "cs.DS", 
    "doi": "10.1137/S0097539703433122", 
    "link": "http://arxiv.org/pdf/cs/0504023v1", 
    "other_authors": "Ioannis Giotis, Venkatesan Guruswami", 
    "title": "Correlation Clustering with a Fixed Number of Clusters", 
    "arxiv-id": "cs/0504023v1", 
    "author": "Venkatesan Guruswami", 
    "publish": "2005-04-06T22:36:03Z", 
    "summary": "We continue the investigation of problems concerning correlation clustering\nor clustering with qualitative information, which is a clustering formulation\nthat has been studied recently. The basic setup here is that we are given as\ninput a complete graph on n nodes (which correspond to nodes to be clustered)\nwhose edges are labeled + (for similar pairs of items) and - (for dissimilar\npairs of items). Thus we have only as input qualitative information on\nsimilarity and no quantitative distance measure between items. The quality of a\nclustering is measured in terms of its number of agreements, which is simply\nthe number of edges it correctly classifies, that is the sum of number of -\nedges whose endpoints it places in different clusters plus the number of +\nedges both of whose endpoints it places within the same cluster.\n  In this paper, we study the problem of finding clusterings that maximize the\nnumber of agreements, and the complementary minimization version where we seek\nclusterings that minimize the number of disagreements. We focus on the\nsituation when the number of clusters is stipulated to be a small constant k.\nOur main result is that for every k, there is a polynomial time approximation\nscheme for both maximizing agreements and minimizing disagreements. (The\nproblems are NP-hard for every k >= 2.) The main technical work is for the\nminimization version, as the PTAS for maximizing agreements follows along the\nlines of the property tester for Max k-CUT.\n  In contrast, when the number of clusters is not specified, the problem of\nminimizing disagreements was shown to be APX-hard, even though the maximization\nversion admits a PTAS."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-007-9005-x", 
    "link": "http://arxiv.org/pdf/cs/0504103v2", 
    "other_authors": "Marek Chrobak, Claire Kenyon, John Noga, Neal E. Young", 
    "title": "Oblivious Medians via Online Bidding", 
    "arxiv-id": "cs/0504103v2", 
    "author": "Neal E. Young", 
    "publish": "2005-04-27T00:07:32Z", 
    "summary": "Following Mettu and Plaxton, we study online algorithms for the k-medians\nproblem. Such an algorithm must produce a nested sequence F_1\\subseteq\nF_2\\subseteq...\\subseteq F_n of sets of facilities. Mettu and Plaxton show that\nonline metric medians has a (roughly) 40-competitive deterministic\npolynomial-time algorithm. We give improved algorithms, including a\n(24+\\epsilon)-competitive deterministic polynomial-time algorithm and a\n5.44-competitive, randomized, non-polynomial-time algorithm.\n  We also consider the competitive ratio with respect to size. An algorithm is\ns-size-competitive if, for each k, the cost of F_k is at most the minimum cost\nof any set of k facilities, while the size of F_k is at most s k. We present\noptimally competitive algorithms for this problem.\n  Our proofs reduce online medians to the following online bidding problem:\nfaced with some unknown threshold T>0, an algorithm must submit ``bids'' b>0\nuntil it submits a bid as large as T. The algorithm pays the sum of its bids.\nWe describe optimally competitive algorithms for online bidding.\n  Our results on cost-competitive online medians extend to approximately metric\ndistance functions, online fractional medians, and online bicriteria\napproximation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0504104v2", 
    "other_authors": "Marek Chrobak, Claire Kenyon, Neal E. Young", 
    "title": "The reverse greedy algorithm for the metric k-median problem", 
    "arxiv-id": "cs/0504104v2", 
    "author": "Neal E. Young", 
    "publish": "2005-04-27T19:36:08Z", 
    "summary": "The Reverse Greedy algorithm (RGreedy) for the k-median problem works as\nfollows. It starts by placing facilities on all nodes. At each step, it removes\na facility to minimize the resulting total distance from the customers to the\nremaining facilities. It stops when k facilities remain. We prove that, if the\ndistance function is metric, then the approximation ratio of RGreedy is between\n?(log n/ log log n) and O(log n)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505007v1", 
    "other_authors": "Dragos Trinca", 
    "title": "Adaptive Codes: A New Class of Non-standard Variable-length Codes", 
    "arxiv-id": "cs/0505007v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-05-02T09:40:02Z", 
    "summary": "We introduce a new class of non-standard variable-length codes, called\nadaptive codes. This class of codes associates a variable-length codeword to\nthe symbol being encoded depending on the previous symbols in the input data\nstring. An efficient algorithm for constructing adaptive codes of order one is\npresented. Then, we introduce a natural generalization of adaptive codes,\ncalled GA codes."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505009v17", 
    "other_authors": "Arindam Mitra", 
    "title": "Human being is a living random number generator", 
    "arxiv-id": "cs/0505009v17", 
    "author": "Arindam Mitra", 
    "publish": "2005-05-03T15:42:24Z", 
    "summary": "General wisdom is, mathematical operation is needed to generate number by\nnumbers. It is pointed out that without any mathematical operation true random\nnumbers can be generated by numbers through algorithmic process. It implies\nthat human brain itself is a living true random number generator. Human brain\ncan meet the enormous human demand of true random numbers."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2005.09.009", 
    "link": "http://arxiv.org/pdf/cs/0505027v1", 
    "other_authors": "Vincent Lef\u00e8vre", 
    "title": "The Generic Multiple-Precision Floating-Point Addition With Exact   Rounding (as in the MPFR Library)", 
    "arxiv-id": "cs/0505027v1", 
    "author": "Vincent Lef\u00e8vre", 
    "publish": "2005-05-11T14:22:54Z", 
    "summary": "We study the multiple-precision addition of two positive floating-point\nnumbers in base 2, with exact rounding, as specified in the MPFR library, i.e.\nwhere each number has its own precision. We show how the best possible\ncomplexity (up to a constant factor that depends on the implementation) can be\nobtain."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505048v1", 
    "other_authors": "David Eppstein, Michael T. Goodrich, Daniel S. Hirschberg", 
    "title": "Improved Combinatorial Group Testing Algorithms for Real-World Problem   Sizes", 
    "arxiv-id": "cs/0505048v1", 
    "author": "Daniel S. Hirschberg", 
    "publish": "2005-05-18T20:25:16Z", 
    "summary": "We study practically efficient methods for performing combinatorial group\ntesting. We present efficient non-adaptive and two-stage combinatorial group\ntesting algorithms, which identify the at most d items out of a given set of n\nitems that are defective, using fewer tests for all practical set sizes. For\nexample, our two-stage algorithm matches the information theoretic lower bound\nfor the number of tests in a combinatorial group testing regimen."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505061v1", 
    "other_authors": "Dragos Trinca", 
    "title": "EAH: A New Encoder based on Adaptive Variable-length Codes", 
    "arxiv-id": "cs/0505061v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-05-24T06:53:33Z", 
    "summary": "Adaptive variable-length codes associate a variable-length codeword to the\nsymbol being encoded depending on the previous symbols in the input string.\nThis class of codes has been recently presented in [Dragos Trinca,\narXiv:cs.DS/0505007] as a new class of non-standard variable-length codes. New\nalgorithms for data compression, based on adaptive variable-length codes of\norder one and Huffman's algorithm, have been recently presented in [Dragos\nTrinca, ITCC 2004]. In this paper, we extend the work done so far by the\nfollowing contributions: first, we propose an improved generalization of these\nalgorithms, called EAHn. Second, we compute the entropy bounds for EAHn, using\nthe well-known bounds for Huffman's algorithm. Third, we discuss implementation\ndetails and give reports of experimental results obtained on some well-known\ncorpora. Finally, we describe a parallel version of EAHn using the PRAM model\nof computation."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505066v1", 
    "other_authors": "Udayan Khuarana", 
    "title": "Decision Sort and its Parallel Implementation", 
    "arxiv-id": "cs/0505066v1", 
    "author": "Udayan Khuarana", 
    "publish": "2005-05-24T15:41:27Z", 
    "summary": "In this paper, a sorting technique is presented that takes as input a data\nset whose primary key domain is known to the sorting algorithm, and works with\nan time efficiency of O(n+k), where k is the primary key domain. It is shown\nthat the algorithm has applicability over a wide range of data sets. Later, a\nparallel formulation of the same is proposed and its effectiveness is argued.\nThough this algorithm is applicable over a wide range of general data sets, it\nfinds special application (much superior to others) in places where sorting\ninformation that arrives in parts and in cases where input data is huge in\nsize."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0505077v1", 
    "other_authors": "Shlomo Moran, Sagi Snir", 
    "title": "Efficient Approximation of Convex Recolorings", 
    "arxiv-id": "cs/0505077v1", 
    "author": "Sagi Snir", 
    "publish": "2005-05-27T23:16:48Z", 
    "summary": "A coloring of a tree is convex if the vertices that pertain to any color\ninduce a connected subtree; a partial coloring (which assigns colors to some of\nthe vertices) is convex if it can be completed to a convex (total) coloring.\nConvex coloring of trees arise in areas such as phylogenetics, linguistics,\netc. eg, a perfect phylogenetic tree is one in which the states of each\ncharacter induce a convex coloring of the tree. Research on perfect phylogeny\nis usually focused on finding a tree so that few predetermined partial\ncolorings of its vertices are convex.\n  When a coloring of a tree is not convex, it is desirable to know \"how far\" it\nis from a convex one. In [19], a natural measure for this distance, called the\nrecoloring distance was defined: the minimal number of color changes at the\nvertices needed to make the coloring convex. This can be viewed as minimizing\nthe number of \"exceptional vertices\" w.r.t. to a closest convex coloring. The\nproblem was proved to be NP-hard even for colored string.\n  In this paper we continue the work of [19], and present a 2-approximation\nalgorithm of convex recoloring of strings whose running time O(cn), where c is\nthe number of colors and n is the size of the input, and an O(cn^2)-time\n3-approximation algorithm for convex recoloring of trees."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0506027v1", 
    "other_authors": "Travis Gagie", 
    "title": "Sorting a Low-Entropy Sequence", 
    "arxiv-id": "cs/0506027v1", 
    "author": "Travis Gagie", 
    "publish": "2005-06-08T22:15:18Z", 
    "summary": "We give the first sorting algorithm with bounds in terms of higher-order\nentropies: let $S$ be a sequence of length $m$ containing $n$ distinct elements\nand let (H_\\ell (S)) be the $\\ell$th-order empirical entropy of $S$, with\n(n^{\\ell + 1} \\log n \\in O (m)); our algorithm sorts $S$ using ((H_\\ell (S) + O\n(1)) m) comparisons."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0507014v1", 
    "other_authors": "Moshe Schwartz", 
    "title": "Isomorphism of graphs-a polynomial test", 
    "arxiv-id": "cs/0507014v1", 
    "author": "Moshe Schwartz", 
    "publish": "2005-07-06T11:35:42Z", 
    "summary": "An explicit algorithm is presented for testing whether two non-directed\ngraphs are isomorphic or not. It is shown that for a graph of n vertices, the\nnumber of n independent operations needed for the test is polynomial in n. A\nproof that the algorithm actually performs the test is presented."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508045v1", 
    "other_authors": "Christoph Albrecht, Andrew B. Kahng, Ion I. Mandoiu, Alexander Zelikovsky", 
    "title": "Multicommodity Flow Algorithms for Buffered Global Routing", 
    "arxiv-id": "cs/0508045v1", 
    "author": "Alexander Zelikovsky", 
    "publish": "2005-08-06T12:44:09Z", 
    "summary": "In this paper we describe a new algorithm for buffered global routing\naccording to a prescribed buffer site map. Specifically, we describe a provably\ngood multi-commodity flow based algorithm that finds a global routing\nminimizing buffer and wire congestion subject to given constraints on routing\narea (wirelength and number of buffers) and sink delays. Our algorithm allows\ncomputing the tradeoff curve between routing area and wire/buffer congestion\nunder any combination of delay and capacity constraints, and simultaneously\nperforms buffer/wire sizing, as well as layer and pin assignment. Experimental\nresults show that near-optimal results are obtained with a practical runtime."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508086v1", 
    "other_authors": "Dragos Trinca", 
    "title": "High-performance BWT-based Encoders", 
    "arxiv-id": "cs/0508086v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T05:47:00Z", 
    "summary": "In 1994, Burrows and Wheeler developed a data compression algorithm which\nperforms significantly better than Lempel-Ziv based algorithms. Since then, a\nlot of work has been done in order to improve their algorithm, which is based\non a reversible transformation of the input string, called BWT (the\nBurrows-Wheeler transformation). In this paper, we propose a compression scheme\nbased on BWT, MTF (move-to-front coding), and a version of the algorithms\npresented in [Dragos Trinca, ITCC-2004]."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508087v1", 
    "other_authors": "Dragos Trinca", 
    "title": "Modelling the Eulerian Path Problem using a String Matching Framework", 
    "arxiv-id": "cs/0508087v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T06:08:40Z", 
    "summary": "The well-known Eulerian path problem can be solved in polynomial time (more\nexactly, there exists a linear time algorithm for this problem). In this paper,\nwe model the problem using a string matching framework, and then initiate an\nalgorithmic study on a variant of this problem, called the (2,1)-STRING-MATCH\nproblem (which is actually a generalization of the Eulerian path problem).\nThen, we present a polynomial-time algorithm for the (2,1)-STRING-MATCH\nproblem, which is the most important result of this paper. Specifically, we get\na lower bound of Omega(n), and an upper bound of O(n^{2})."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508089v1", 
    "other_authors": "Dragos Trinca", 
    "title": "Modelling the EAH Data Compression Algorithm using Graph Theory", 
    "arxiv-id": "cs/0508089v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T19:32:39Z", 
    "summary": "Adaptive codes associate variable-length codewords to symbols being encoded\ndepending on the previous symbols in the input data string. This class of codes\nhas been introduced in [Dragos Trinca, cs.DS/0505007] as a new class of\nnon-standard variable-length codes. New algorithms for data compression, based\non adaptive codes of order one, have been presented in [Dragos Trinca,\nITCC-2004], where we have behaviorally shown that for a large class of input\ndata strings, these algorithms substantially outperform the Lempel-Ziv\nuniversal data compression algorithm. EAH has been introduced in [Dragos\nTrinca, cs.DS/0505061], as an improved generalization of these algorithms. In\nthis paper, we present a translation of the EAH algorithm into the graph\ntheory."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508090v1", 
    "other_authors": "Dragos Trinca", 
    "title": "Translating the EAH Data Compression Algorithm into Automata Theory", 
    "arxiv-id": "cs/0508090v1", 
    "author": "Dragos Trinca", 
    "publish": "2005-08-21T19:56:31Z", 
    "summary": "Adaptive codes have been introduced in [Dragos Trinca, cs.DS/0505007] as a\nnew class of non-standard variable-length codes. These codes associate\nvariable-length codewords to symbols being encoded depending on the previous\nsymbols in the input data string. A new data compression algorithm, called EAH,\nhas been introduced in [Dragos Trinca, cs.DS/0505061], where we have\nbehaviorally shown that for a large class of input data strings, this algorithm\nsubstantially outperforms the well-known Lempel-Ziv universal data compression\nalgorithm. In this paper, we translate the EAH encoder into automata theory."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0508125v1", 
    "other_authors": "Sheng Bao, De-Shun Zheng", 
    "title": "A Sorting Algorithm Based on Calculation", 
    "arxiv-id": "cs/0508125v1", 
    "author": "De-Shun Zheng", 
    "publish": "2005-08-29T14:22:57Z", 
    "summary": "This article introduces an adaptive sorting algorithm that can relocate\nelements accurately by substituting their values into a function which we name\nit the guessing function. We focus on building this function which is the\nmapping relationship between record values and their corresponding sorted\nlocations essentially. The time complexity of this algorithm O(n),when records\ndistributed uniformly. Additionally, similar approach can be used in the\nsearching algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509026v1", 
    "other_authors": "Nick Duffield, Carsten Lund, Mikkel Thorup", 
    "title": "Sampling to estimate arbitrary subset sums", 
    "arxiv-id": "cs/0509026v1", 
    "author": "Mikkel Thorup", 
    "publish": "2005-09-09T21:47:52Z", 
    "summary": "Starting with a set of weighted items, we want to create a generic sample of\na certain size that we can later use to estimate the total weight of arbitrary\nsubsets. For this purpose, we propose priority sampling which tested on\nInternet data performed better than previous methods by orders of magnitude.\n  Priority sampling is simple to define and implement: we consider a steam of\nitems i=0,...,n-1 with weights w_i. For each item i, we generate a random\nnumber r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of\nthe k highest priority items. Let t be the (k+1)th highest priority. Each\nsampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled\nitems get weight estimate W_i=0.\n  Magically, it turns out that the weight estimates are unbiased, that is,\nE[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over\nany subset sum simply by adding the sampled weight estimates from the subset.\nAlso, we can estimate the variance of the estimates, and surpricingly, there is\nno co-variance between different weight estimates W_i and W_j.\n  We conjecture an extremely strong near-optimality; namely that for any weight\nsequence, there exists no specialized scheme for sampling k items with unbiased\nestimators that gets smaller total variance than priority sampling with k+1\nitems. Very recently Mario Szegedy has settled this conjecture."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509031v1", 
    "other_authors": "Janos Csirik, David S. Johnson, Claire Kenyon", 
    "title": "On the Worst-case Performance of the Sum-of-Squares Algorithm for Bin   Packing", 
    "arxiv-id": "cs/0509031v1", 
    "author": "Claire Kenyon", 
    "publish": "2005-09-12T14:49:48Z", 
    "summary": "The Sum of Squares algorithm for bin packing was defined in [2] and studied\nin great detail in [1], where it was proved that its worst case performance\nratio is at most 3. In this note, we improve the asymptotic worst case bound to\n2.7777..."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509038v1", 
    "other_authors": "Vilhelm Dahllof", 
    "title": "Algorithms for Max Hamming Exact Satisfiability", 
    "arxiv-id": "cs/0509038v1", 
    "author": "Vilhelm Dahllof", 
    "publish": "2005-09-14T09:04:20Z", 
    "summary": "We here study Max Hamming XSAT, ie, the problem of finding two XSAT models at\nmaximum Hamming distance. By using a recent XSAT solver as an auxiliary\nfunction, an O(1.911^n) time algorithm can be constructed, where n is the\nnumber of variables. This upper time bound can be further improved to\nO(1.8348^n) by introducing a new kind of branching, more directly suited for\nfinding models at maximum Hamming distance. The techniques presented here are\nlikely to be of practical use as well as of theoretical value, proving that\nthere are non-trivial algorithms for maximum Hamming distance problems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0509069v3", 
    "other_authors": "Philip Bille, Martin Farach-Colton", 
    "title": "Fast and Compact Regular Expression Matching", 
    "arxiv-id": "cs/0509069v3", 
    "author": "Martin Farach-Colton", 
    "publish": "2005-09-22T13:30:20Z", 
    "summary": "We study 4 problems in string matching, namely, regular expression matching,\napproximate regular expression matching, string edit distance, and subsequence\nindexing, on a standard word RAM model of computation that allows\nlogarithmic-sized words to be manipulated in constant time. We show how to\nimprove the space and/or remove a dependency on the alphabet size for each\nproblem using either an improved tabulation technique of an existing algorithm\nor by combining known algorithms in a new way."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0510019v2", 
    "other_authors": "Rina Panigrahy", 
    "title": "Entropy based Nearest Neighbor Search in High Dimensions", 
    "arxiv-id": "cs/0510019v2", 
    "author": "Rina Panigrahy", 
    "publish": "2005-10-07T00:55:06Z", 
    "summary": "In this paper we study the problem of finding the approximate nearest\nneighbor of a query point in the high dimensional space, focusing on the\nEuclidean space. The earlier approaches use locality-preserving hash functions\n(that tend to map nearby points to the same value) to construct several hash\ntables to ensure that the query point hashes to the same bucket as its nearest\nneighbor in at least one table. Our approach is different -- we use one (or a\nfew) hash table and hash several randomly chosen points in the neighborhood of\nthe query point showing that at least one of them will hash to the bucket\ncontaining its nearest neighbor. We show that the number of randomly chosen\npoints in the neighborhood of the query point $q$ required depends on the\nentropy of the hash value $h(p)$ of a random point $p$ at the same distance\nfrom $q$ at its nearest neighbor, given $q$ and the locality preserving hash\nfunction $h$ chosen randomly from the hash family. Precisely, we show that if\nthe entropy $I(h(p)|q,h) = M$ and $g$ is a bound on the probability that two\nfar-off points will hash to the same bucket, then we can find the approximate\nnearest neighbor in $O(n^\\rho)$ time and near linear $\\tilde O(n)$ space where\n$\\rho = M/\\log(1/g)$. Alternatively we can build a data structure of size\n$\\tilde O(n^{1/(1-\\rho)})$ to answer queries in $\\tilde O(d)$ time. By applying\nthis analysis to the locality preserving hash functions in and adjusting the\nparameters we show that the $c$ nearest neighbor can be computed in time\n$\\tilde O(n^\\rho)$ and near linear space where $\\rho \\approx 2.06/c$ as $c$\nbecomes large."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0510086v1", 
    "other_authors": "K. Kenthapadi, R. Panigrahy", 
    "title": "Balanced Allocation on Graphs", 
    "arxiv-id": "cs/0510086v1", 
    "author": "R. Panigrahy", 
    "publish": "2005-10-27T21:59:21Z", 
    "summary": "In this paper, we study the two choice balls and bins process when balls are\nnot allowed to choose any two random bins, but only bins that are connected by\nan edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the\ngraph is almost regular with degree $n^\\epsilon$, where $\\epsilon$ is not too\nsmall, the previous bounds on the maximum load continue to hold. Precisely, the\nmaximum load is $\\log \\log n + O(1/\\epsilon) + O(1)$. For general\n$\\Delta$-regular graphs, we show that the maximum load is $\\log\\log n +\nO(\\frac{\\log n}{\\log (\\Delta/\\log^4 n)}) + O(1)$ and also provide an almost\nmatching lower bound of $\\log \\log n + \\frac{\\log n}{\\log (\\Delta \\log n)}$.\n  V{\\\"o}cking [Voc99] showed that the maximum bin size with $d$ choice load\nbalancing can be further improved to $O(\\log\\log n /d)$ by breaking ties to the\nleft. This requires $d$ random bin choices. We show that such bounds can be\nachieved by making only two random accesses and querying $d/2$ contiguous bins\nin each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of\n$d/2$ consecutive bins, if each ball chooses two groups at random and inserts\nthe new ball into the least-loaded bin in the lesser loaded group, then the\nmaximum load is $O(\\log\\log n/d)$ with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1137/050631847", 
    "link": "http://arxiv.org/pdf/cs/0511020v2", 
    "other_authors": "David S. P\u0142aneta", 
    "title": "Pbit and other list sorting algorithms", 
    "arxiv-id": "cs/0511020v2", 
    "author": "David S. P\u0142aneta", 
    "publish": "2005-11-04T01:52:02Z", 
    "summary": "Pbit, besides its simplicity, is definitely the fastest list sorting\nalgorithm. It considerably surpasses all already known methods. Among many\nadvantages, it is stable, linear and be made to run in place. I will compare\nPbit with algorithm described by Donald E. Knuth in the third volume of ''The\nArt of Computer Programming'' and other (QuickerSort, MergeSort) list sorting\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0511082v1", 
    "other_authors": "Paola Bonizzoni, Gianluca Della Vedova, Riccardo Dondi", 
    "title": "Approximating Clustering of Fingerprint Vectors with Missing Values", 
    "arxiv-id": "cs/0511082v1", 
    "author": "Riccardo Dondi", 
    "publish": "2005-11-23T10:32:47Z", 
    "summary": "The problem of clustering fingerprint vectors is an interesting problem in\nComputational Biology that has been proposed in (Figureroa et al. 2004). In\nthis paper we show some improvements in closing the gaps between the known\nlower bounds and upper bounds on the approximability of some variants of the\nbiological problem. Namely we are able to prove that the problem is APX-hard\neven when each fingerprint contains only two unknown position. Moreover we have\nstudied some variants of the orginal problem, and we give two 2-approximation\nalgorithm for the IECMV and OECMV problems when the number of unknown entries\nfor each vector is at most a constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512046v3", 
    "other_authors": "George B. Mertzios", 
    "title": "A polynomial algorithm for the k-cluster problem on interval graphs", 
    "arxiv-id": "cs/0512046v3", 
    "author": "George B. Mertzios", 
    "publish": "2005-12-11T23:13:44Z", 
    "summary": "This paper deals with the problem of finding, for a given graph and a given\nnatural number k, a subgraph of k nodes with a maximum number of edges. This\nproblem is known as the k-cluster problem and it is NP-hard on general graphs\nas well as on chordal graphs. In this paper, it is shown that the k-cluster\nproblem is solvable in polynomial time on interval graphs. In particular, we\npresent two polynomial time algorithms for the class of proper interval graphs\nand the class of general interval graphs, respectively. Both algorithms are\nbased on a matrix representation for interval graphs. In contrast to\nrepresentations used in most of the previous work, this matrix representation\ndoes not make use of the maximal cliques in the investigated graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512061v3", 
    "other_authors": "Philip Bille, Inge Li Goertz", 
    "title": "Matching Subsequences in Trees", 
    "arxiv-id": "cs/0512061v3", 
    "author": "Inge Li Goertz", 
    "publish": "2005-12-15T10:28:04Z", 
    "summary": "Given two rooted, labeled trees $P$ and $T$ the tree path subsequence problem\nis to determine which paths in $P$ are subsequences of which paths in $T$. Here\na path begins at the root and ends at a leaf. In this paper we propose this\nproblem as a useful query primitive for XML data, and provide new algorithms\nimproving the previously best known time and space bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0512081v1", 
    "other_authors": "Erik D. Demaine, Friedhelm Meyer auf der Heide, Rasmus Pagh, Mihai Patrascu", 
    "title": "De Dictionariis Dynamicis Pauco Spatio Utentibus", 
    "arxiv-id": "cs/0512081v1", 
    "author": "Mihai Patrascu", 
    "publish": "2005-12-20T23:01:41Z", 
    "summary": "We develop dynamic dictionaries on the word RAM that use asymptotically\noptimal space, up to constant factors, subject to insertions and deletions, and\nsubject to supporting perfect-hashing queries and/or membership queries, each\noperation in constant time with high probability. When supporting only\nmembership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,\nwhere n and u are the sizes of the dictionary and the universe, respectively.\nPrevious dictionaries either did not achieve this space bound or had time\nbounds that were only expected and amortized. When supporting perfect-hashing\nqueries, the optimal space bound depends on the range {1,2,...,n+t} of\nhashcodes allowed as output. We prove that the optimal space bound is Theta(n\nlglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,\nand it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership\nqueries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0601084v1", 
    "other_authors": "Ming-Yang Kao, Manan Sanghi, Robert Schweller", 
    "title": "Randomized Fast Design of Short DNA Words", 
    "arxiv-id": "cs/0601084v1", 
    "author": "Robert Schweller", 
    "publish": "2006-01-19T00:22:56Z", 
    "summary": "We consider the problem of efficiently designing sets (codes) of equal-length\nDNA strings (words) that satisfy certain combinatorial constraints. This\nproblem has numerous motivations including DNA computing and DNA self-assembly.\nPrevious work has extended results from coding theory to obtain bounds on code\nsize for new biologically motivated constraints and has applied heuristic local\nsearch and genetic algorithm techniques for code design. This paper proposes a\nnatural optimization formulation of the DNA code design problem in which the\ngoal is to design n strings that satisfy a given set of constraints while\nminimizing the length of the strings. For multiple sets of constraints, we\nprovide high-probability algorithms that run in time polynomial in n and any\ngiven constraint parameters, and output strings of length within a constant\nfactor of the optimal. To the best of our knowledge, this work is the first to\nconsider this type of optimization problem in the context of DNA code design."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-008-9265-0", 
    "link": "http://arxiv.org/pdf/cs/0601117v2", 
    "other_authors": "Dhananjay D. Kulkarni, Shekhar Verma, Prashant", 
    "title": "Finding Cliques of a Graph using Prime Numbers", 
    "arxiv-id": "cs/0601117v2", 
    "author": "Prashant", 
    "publish": "2006-01-27T20:11:14Z", 
    "summary": "This paper proposes a new algorithm for solving maximal cliques for simple\nundirected graphs using the theory of prime numbers. A novel approach using\nprime numbers is used to find cliques and ends with a discussion of the\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0601127v1", 
    "other_authors": "Amos Fiat, Manor Mendel", 
    "title": "Truly Online Paging with Locality of Reference", 
    "arxiv-id": "cs/0601127v1", 
    "author": "Manor Mendel", 
    "publish": "2006-01-30T20:58:23Z", 
    "summary": "The competitive analysis fails to model locality of reference in the online\npaging problem. To deal with it, Borodin et. al. introduced the access graph\nmodel, which attempts to capture the locality of reference. However, the access\ngraph model has a number of troubling aspects. The access graph has to be known\nin advance to the paging algorithm and the memory required to represent the\naccess graph itself may be very large.\n  In this paper we present truly online strongly competitive paging algorithms\nin the access graph model that do not have any prior information on the access\nsequence. We present both deterministic and randomized algorithms. The\nalgorithms need only O(k log n) bits of memory, where k is the number of page\nslots available and n is the size of the virtual address space. I.e.,\nasymptotically no more memory than needed to store the virtual address\ntranslation table.\n  We also observe that our algorithms adapt themselves to temporal changes in\nthe locality of reference. We model temporal changes in the locality of\nreference by extending the access graph model to the so called extended access\ngraph model, in which many vertices of the graph can correspond to the same\nvirtual page. We define a measure for the rate of change in the locality of\nreference in G denoted by Delta(G). We then show our algorithms remain strongly\ncompetitive as long as Delta(G) >= (1+ epsilon)k, and no truly online algorithm\ncan be strongly competitive on a class of extended access graphs that includes\nall graphs G with Delta(G) >= k- o(k)."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602002v1", 
    "other_authors": "Marko A. Rodriguez, Johan Bollen", 
    "title": "Simulating Network Influence Algorithms Using Particle-Swarms: PageRank   and PageRank-Priors", 
    "arxiv-id": "cs/0602002v1", 
    "author": "Johan Bollen", 
    "publish": "2006-01-31T23:24:42Z", 
    "summary": "A particle-swarm is a set of indivisible processing elements that traverse a\nnetwork in order to perform a distributed function. This paper will describe a\nparticular implementation of a particle-swarm that can simulate the behavior of\nthe popular PageRank algorithm in both its {\\it global-rank} and {\\it\nrelative-rank} incarnations. PageRank is compared against the particle-swarm\nmethod on artificially generated scale-free networks of 1,000 nodes constructed\nusing a common gamma value, $\\gamma = 2.5$. The running time of the\nparticle-swarm algorithm is $O(|P|+|P|t)$ where $|P|$ is the size of the\nparticle population and $t$ is the number of particle propagation iterations.\nThe particle-swarm method is shown to be useful due to its ease of extension\nand running time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602057v1", 
    "other_authors": "Melanie J. Agnew, Christopher M. Homan", 
    "title": "Plane Decompositions as Tools for Approximation", 
    "arxiv-id": "cs/0602057v1", 
    "author": "Christopher M. Homan", 
    "publish": "2006-02-15T19:09:39Z", 
    "summary": "Tree decompositions were developed by Robertson and Seymour. Since then\nalgorithms have been developed to solve intractable problems efficiently for\ngraphs of bounded treewidth. In this paper we extend tree decompositions to\nallow cycles to exist in the decomposition graph; we call these new\ndecompositions plane decompositions because we require that the decomposition\ngraph be planar. First, we give some background material about tree\ndecompositions and an overview of algorithms both for decompositions and for\napproximations of planar graphs. Then, we give our plane decomposition\ndefinition and an algorithm that uses this decomposition to approximate the\nsize of the maximum independent set of the underlying graph in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SFCS.1997.646121", 
    "link": "http://arxiv.org/pdf/cs/0602073v2", 
    "other_authors": "Deepak Ajwani, Tobias Friedrich, Ulrich Meyer", 
    "title": "An O(n^{2.75}) algorithm for online topological ordering", 
    "arxiv-id": "cs/0602073v2", 
    "author": "Ulrich Meyer", 
    "publish": "2006-02-21T10:32:15Z", 
    "summary": "We present a simple algorithm which maintains the topological order of a\ndirected acyclic graph with n nodes under an online edge insertion sequence in\nO(n^{2.75}) time, independent of the number of edges m inserted. For dense\nDAGs, this is an improvement over the previous best result of O(min(m^{3/2}\nlog(n), m^{3/2} + n^2 log(n)) by Katriel and Bodlaender. We also provide an\nempirical comparison of our algorithm with other algorithms for online\ntopological sorting. Our implementation outperforms them on certain hard\ninstances while it is still competitive on random edge insertion sequences\nleading to complete DAGs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603048v1", 
    "other_authors": "Binh Minh Bui Xuan, Michel Habib, Vincent Limouzy, Fabien De Montgolfier", 
    "title": "Homogeneity vs. Adjacency: generalising some graph decomposition   algorithms", 
    "arxiv-id": "cs/0603048v1", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-03-13T09:48:49Z", 
    "summary": "In this paper, a new general decomposition theory inspired from modular graph\ndecomposition is presented. Our main result shows that, within this general\ntheory, most of the nice algorithmic tools developed for modular decomposition\nare still efficient. This theory not only unifies the usual modular\ndecomposition generalisations such as modular decomposition of directed graphs\nor decomposition of 2-structures, but also star cutsets and bimodular\ndecomposition. Our general framework provides a decomposition algorithm which\nimproves the best known algorithms for bimodular decomposition."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603050v1", 
    "other_authors": "Patrick Cegielski, Irene Guessarian, Yuri Matiyasevich", 
    "title": "Multiple serial episode matching", 
    "arxiv-id": "cs/0603050v1", 
    "author": "Yuri Matiyasevich", 
    "publish": "2006-03-13T11:03:34Z", 
    "summary": "In a previous paper we generalized the Knuth-Morris-Pratt (KMP) pattern\nmatching algorithm and defined a non-conventional kind of RAM, the MP--RAMs\n(RAMS equipped with extra operations), and designed an O(n) on-line algorithm\nfor solving the serial episode matching problem on MP--RAMs when there is only\none single episode. We here give two extensions of this algorithm to the case\nwhen we search for several patterns simultaneously and compare them. More\npreciseley, given $q+1$ strings (a text $t$ of length $n$ and $q$ patterns\n$m\\_1,...,m\\_q$) and a natural number $w$, the {\\em multiple serial episode\nmatching problem} consists in finding the number of size $w$ windows of text\n$t$ which contain patterns $m\\_1,...,m\\_q$ as subsequences, i.e. for each\n$m\\_i$, if $m\\_i=p\\_1,..., p\\_k$, the letters $p\\_1,..., p\\_k$ occur in the\nwindow, in the same order as in $m\\_i$, but not necessarily consecutively (they\nmay be interleaved with other letters).} The main contribution is an algorithm\nsolving this problem on-line in time $O(nq)$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/11917496\\_25", 
    "link": "http://arxiv.org/pdf/cs/0603122v1", 
    "other_authors": "Eug\u00e9nie Foustoucos, Irene Guessarian", 
    "title": "Complexity of Monadic inf-datalog. Application to temporal logic", 
    "arxiv-id": "cs/0603122v1", 
    "author": "Irene Guessarian", 
    "publish": "2006-03-30T15:25:11Z", 
    "summary": "In [11] we defined Inf-Datalog and characterized the fragments of Monadic\ninf-Datalog that have the same expressive power as Modal Logic (resp. $CTL$,\nalternation-free Modal $\\mu$-calculus and Modal $\\mu$-calculus). We study here\nthe time and space complexity of evaluation of Monadic inf-Datalog programs on\nfinite models. We deduce a new unified proof that model checking has 1. linear\ndata and program complexities (both in time and space) for $CTL$ and\nalternation-free Modal $\\mu$-calculus, and 2. linear-space (data and program)\ncomplexities, linear-time program complexity and polynomial-time data\ncomplexity for $L\\mu\\_k$ (Modal $\\mu$-calculus with fixed alternation-depth at\nmost $k$).}"
},{
    "category": "cs.DS", 
    "doi": "10.1145/1644015.1644017", 
    "link": "http://arxiv.org/pdf/cs/0604037v3", 
    "other_authors": "Erik D. Demaine, Shay Mozes, Benjamin Rossman, Oren Weimann", 
    "title": "An O(n^3)-Time Algorithm for Tree Edit Distance", 
    "arxiv-id": "cs/0604037v3", 
    "author": "Oren Weimann", 
    "publish": "2006-04-10T00:39:11Z", 
    "summary": "The {\\em edit distance} between two ordered trees with vertex labels is the\nminimum cost of transforming one tree into the other by a sequence of\nelementary operations consisting of deleting and relabeling existing nodes, as\nwell as inserting new nodes. In this paper, we present a worst-case\n$O(n^3)$-time algorithm for this problem, improving the previous best\n$O(n^3\\log n)$-time algorithm~\\cite{Klein}. Our result requires a novel\nadaptive strategy for deciding how a dynamic program divides into subproblems\n(which is interesting in its own right), together with a deeper understanding\nof the previous algorithms for the problem. We also prove the optimality of our\nalgorithm among the family of \\emph{decomposition strategy} algorithms--which\nalso includes the previous fastest algorithms--by tightening the known lower\nbound of $\\Omega(n^2\\log^2 n)$~\\cite{Touzet} to $\\Omega(n^3)$, matching our\nalgorithm's running time. Furthermore, we obtain matching upper and lower\nbounds of $\\Theta(n m^2 (1 + \\log \\frac{n}{m}))$ when the two trees have\ndifferent sizes $m$ and~$n$, where $m < n$."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1644015.1644017", 
    "link": "http://arxiv.org/pdf/cs/0604045v1", 
    "other_authors": "Sandor P. Fekete, Joerg Schepers, Jan C. van der Veen", 
    "title": "An exact algorithm for higher-dimensional orthogonal packing", 
    "arxiv-id": "cs/0604045v1", 
    "author": "Jan C. van der Veen", 
    "publish": "2006-04-11T13:55:03Z", 
    "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical\napplications, including packing, cutting, and scheduling. Combining the use of\nour data structure for characterizing feasible packings with our new classes of\nlower bounds, and other heuristics, we develop a two-level tree search\nalgorithm for solving higher-dimensional packing problems to optimality.\nComputational results are reported, including optimal solutions for all\ntwo--dimensional test problems from recent literature.\n  This is the third in a series of articles describing new approaches to\nhigher-dimensional packing; see cs.DS/0310032 and cs.DS/0402044."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0604065v3", 
    "other_authors": "Binh-Minh Bui-Xuan, Michel Habib, Vincent Limouzy, Fabien De Montgolfier", 
    "title": "Unifying two Graph Decompositions with Modular Decomposition", 
    "arxiv-id": "cs/0604065v3", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-04-16T19:41:38Z", 
    "summary": "We introduces the umodules, a generalisation of the notion of graph module.\nThe theory we develop captures among others undirected graphs, tournaments,\ndigraphs, and $2-$structures. We show that, under some axioms, a unique\ndecomposition tree exists for umodules. Polynomial-time algorithms are provided\nfor: non-trivial umodule test, maximal umodule computation, and decomposition\ntree computation when the tree exists. Our results unify many known\ndecomposition like modular and bi-join decomposition of graphs, and a new\ndecomposition of tournaments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0604097v4", 
    "other_authors": "Sudipto Guha, Boulos Harb", 
    "title": "Approximation algorithms for wavelet transform coding of data streams", 
    "arxiv-id": "cs/0604097v4", 
    "author": "Boulos Harb", 
    "publish": "2006-04-25T01:27:37Z", 
    "summary": "This paper addresses the problem of finding a B-term wavelet representation\nof a given discrete function $f \\in \\real^n$ whose distance from f is\nminimized. The problem is well understood when we seek to minimize the\nEuclidean distance between f and its representation. The first known algorithms\nfor finding provably approximate representations minimizing general $\\ell_p$\ndistances (including $\\ell_\\infty$) under a wide variety of compactly supported\nwavelet bases are presented in this paper. For the Haar basis, a polynomial\ntime approximation scheme is demonstrated. These algorithms are applicable in\nthe one-pass sublinear-space data stream model of computation. They generalize\nnaturally to multiple dimensions and weighted norms. A universal representation\nthat provides a provable approximation guarantee under all p-norms\nsimultaneously; and the first approximation algorithms for bit-budget versions\nof the problem, known as adaptive quantization, are also presented. Further, it\nis shown that the algorithms presented here can be used to select a basis from\na tree-structured dictionary of bases and find a B-term representation of the\ngiven function that provably approximates its best dictionary-basis\nrepresentation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0605078v1", 
    "other_authors": "Philippe Baptiste, Peter Brucker, Marek Chrobak, Christoph Durr, Svetlana A. Kravchenko, Francis Sourd", 
    "title": "The Complexity of Mean Flow Time Scheduling Problems with Release Times", 
    "arxiv-id": "cs/0605078v1", 
    "author": "Francis Sourd", 
    "publish": "2006-05-17T22:07:17Z", 
    "summary": "We study the problem of preemptive scheduling n jobs with given release times\non m identical parallel machines. The objective is to minimize the average flow\ntime. We show that when all jobs have equal processing times then the problem\ncan be solved in polynomial time using linear programming. Our algorithm can\nalso be applied to the open-shop problem with release times and unit processing\ntimes. For the general case (when processing times are arbitrary), we show that\nthe problem is unary NP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0605126v1", 
    "other_authors": "David P. Bunde", 
    "title": "Power-aware scheduling for makespan and flow", 
    "arxiv-id": "cs/0605126v1", 
    "author": "David P. Bunde", 
    "publish": "2006-05-26T21:57:35Z", 
    "summary": "We consider offline scheduling algorithms that incorporate speed scaling to\naddress the bicriteria problem of minimizing energy consumption and a\nscheduling metric. For makespan, we give linear-time algorithms to compute all\nnon-dominated solutions for the general uniprocessor problem and for the\nmultiprocessor problem when every job requires the same amount of work. We also\nshow that the multiprocessor problem becomes NP-hard when jobs can require\ndifferent amounts of work.\n  For total flow, we show that the optimal flow corresponding to a particular\nenergy budget cannot be exactly computed on a machine supporting arithmetic and\nthe extraction of roots. This hardness result holds even when scheduling\nequal-work jobs on a uniprocessor. We do, however, extend previous work by\nPruhs et al. to give an arbitrarily-good approximation for scheduling\nequal-work jobs on a multiprocessor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-77120-3", 
    "link": "http://arxiv.org/pdf/cs/0606042v1", 
    "other_authors": "Laurent Hascoet, Mauricio Araya-Polo", 
    "title": "Enabling user-driven Checkpointing strategies in Reverse-mode Automatic   Differentiation", 
    "arxiv-id": "cs/0606042v1", 
    "author": "Mauricio Araya-Polo", 
    "publish": "2006-06-09T16:01:46Z", 
    "summary": "This paper presents a new functionality of the Automatic Differentiation (AD)\ntool Tapenade. Tapenade generates adjoint codes which are widely used for\noptimization or inverse problems. Unfortunately, for large applications the\nadjoint code demands a great deal of memory, because it needs to store a large\nset of intermediates values. To cope with that problem, Tapenade implements a\nsub-optimal version of a technique called checkpointing, which is a trade-off\nbetween storage and recomputation. Our long-term goal is to provide an optimal\ncheckpointing strategy for every code, not yet achieved by any AD tool. Towards\nthat goal, we first introduce modifications in Tapenade in order to give the\nuser the choice to select the checkpointing strategy most suitable for their\ncode. Second, we conduct experiments in real-size scientific codes in order to\ngather hints that help us to deduce an optimal checkpointing strategy. Some of\nthe experimental results show memory savings up to 35% and execution time up to\n90%."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10951-007-0038-4", 
    "link": "http://arxiv.org/pdf/cs/0606067v2", 
    "other_authors": "Michael A. Bender, Raphael Clifford, Kostas Tsichlas", 
    "title": "Scheduling Algorithms for Procrastinators", 
    "arxiv-id": "cs/0606067v2", 
    "author": "Kostas Tsichlas", 
    "publish": "2006-06-14T16:55:44Z", 
    "summary": "This paper presents scheduling algorithms for procrastinators, where the\nspeed that a procrastinator executes a job increases as the due date\napproaches. We give optimal off-line scheduling policies for linearly\nincreasing speed functions. We then explain the computational/numerical issues\ninvolved in implementing this policy. We next explore the online setting,\nshowing that there exist adversaries that force any online scheduling policy to\nmiss due dates. This impossibility result motivates the problem of minimizing\nthe maximum interval stretch of any job; the interval stretch of a job is the\njob's flow time divided by the job's due date minus release time. We show that\nseveral common scheduling strategies, including the \"hit-the-highest-nail\"\nstrategy beloved by procrastinators, have arbitrarily large maximum interval\nstretch. Then we give the \"thrashing\" scheduling policy and show that it is a\n\\Theta(1) approximation algorithm for the maximum interval stretch."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606109v4", 
    "other_authors": "Manor Mendel, Assaf Naor", 
    "title": "Maximum gradient embeddings and monotone clustering", 
    "arxiv-id": "cs/0606109v4", 
    "author": "Assaf Naor", 
    "publish": "2006-06-26T19:32:29Z", 
    "summary": "Let (X,d_X) be an n-point metric space. We show that there exists a\ndistribution D over non-contractive embeddings into trees f:X-->T such that for\nevery x in X, the expectation with respect to D of the maximum over y in X of\nthe ratio d_T(f(x),f(y)) / d_X(x,y) is at most C (log n)^2, where C is a\nuniversal constant. Conversely we show that the above quadratic dependence on\nlog n cannot be improved in general. Such embeddings, which we call maximum\ngradient embeddings, yield a framework for the design of approximation\nalgorithms for a wide range of clustering problems with monotone costs,\nincluding fault-tolerant versions of k-median and facility location."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606116v1", 
    "other_authors": "Philip Bille", 
    "title": "New Algorithms for Regular Expression Matching", 
    "arxiv-id": "cs/0606116v1", 
    "author": "Philip Bille", 
    "publish": "2006-06-28T10:51:39Z", 
    "summary": "In this paper we revisit the classical regular expression matching problem,\nnamely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches\none of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and\n$Q$, respectively. On a standard unit-cost RAM with word length $w \\geq \\log\nn$, we show that the problem can be solved in $O(m)$ space with the following\nrunning times: \\begin{equation*} \\begin{cases}\n  O(n\\frac{m \\log w}{w} + m \\log w) & \\text{if $m > w$} \\\\\n  O(n\\log m + m\\log m) & \\text{if $\\sqrt{w} < m \\leq w$} \\\\\n  O(\\min(n+ m^2, n\\log m + m\\log m)) & \\text{if $m \\leq \\sqrt{w}$.} \\end{cases}\n\\end{equation*} This improves the best known time bound among algorithms using\n$O(m)$ space. Whenever $w \\geq \\log^2 n$ it improves all known time bounds\nregardless of how much space is used."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0606124v2", 
    "other_authors": "Sean M. Falconer, Dmitri Maslov", 
    "title": "Weighted hierarchical alignment of directed acyclic graph", 
    "arxiv-id": "cs/0606124v2", 
    "author": "Dmitri Maslov", 
    "publish": "2006-06-29T18:07:49Z", 
    "summary": "In some applications of matching, the structural or hierarchical properties\nof the two graphs being aligned must be maintained. The hierarchical properties\nare induced by the direction of the edges in the two directed graphs. These\nstructural relationships defined by the hierarchy in the graphs act as a\nconstraint on the alignment. In this paper, we formalize the above problem as\nthe weighted alignment between two directed acyclic graphs. We prove that this\nproblem is NP-complete, show several upper bounds for approximating the\nsolution, and finally introduce polynomial time algorithms for sub-classes of\ndirected acyclic graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607045v2", 
    "other_authors": "Xin Han, Deshi Ye, Yong Zhou", 
    "title": "Improved online hypercube packing", 
    "arxiv-id": "cs/0607045v2", 
    "author": "Yong Zhou", 
    "publish": "2006-07-11T09:43:45Z", 
    "summary": "In this paper, we study online multidimensional bin packing problem when all\nitems are hypercubes.\n  Based on the techniques in one dimensional bin packing algorithm Super\nHarmonic by Seiden, we give a framework for online hypercube packing problem\nand obtain new upper bounds of asymptotic competitive ratios.\n  For square packing, we get an upper bound of 2.1439, which is better than\n2.24437.\n  For cube packing, we also give a new upper bound 2.6852 which is better than\n2.9421 by Epstein and van Stee."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607046v2", 
    "other_authors": "Xin Han, Kazuo Iwama, Deshi Ye, Guochuan Zhang", 
    "title": "Strip Packing vs. Bin Packing", 
    "arxiv-id": "cs/0607046v2", 
    "author": "Guochuan Zhang", 
    "publish": "2006-07-11T09:58:34Z", 
    "summary": "In this paper we establish a general algorithmic framework between bin\npacking and strip packing, with which we achieve the same asymptotic bounds by\napplying bin packing algorithms to strip packing. More precisely we obtain the\nfollowing results: (1) Any offline bin packing algorithm can be applied to\nstrip packing maintaining the same asymptotic worst-case ratio. Thus using FFD\n(MFFD) as a subroutine, we get a practical (simple and fast) algorithm for\nstrip packing with an upper bound 11/9 (71/60). A simple AFPTAS for strip\npacking immediately follows. (2) A class of Harmonic-based algorithms for bin\npacking can be applied to online strip packing maintaining the same asymptotic\ncompetitive ratio. It implies online strip packing admits an upper bound of\n1.58889 on the asymptotic competitive ratio, which is very close to the lower\nbound 1.5401 and significantly improves the previously best bound of 1.6910 and\naffirmatively answers an open question posed by Csirik et. al."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0607100v1", 
    "other_authors": "Xin Han, Kazuo Iwama, Guochuan Zhang", 
    "title": "New Upper Bounds on The Approximability of 3D Strip Packing", 
    "arxiv-id": "cs/0607100v1", 
    "author": "Guochuan Zhang", 
    "publish": "2006-07-22T02:06:26Z", 
    "summary": "In this paper, we study the 3D strip packing problem in which we are given a\nlist of 3-dimensional boxes and required to pack all of them into a\n3-dimensional strip with length 1 and width 1 and unlimited height to minimize\nthe height used. Our results are below: i) we give an approximation algorithm\nwith asymptotic worst-case ratio 1.69103, which improves the previous best\nbound of $2+\\epsilon$ by Jansen and Solis-Oba of SODA 2006; ii) we also present\nan asymptotic PTAS for the case in which all items have {\\em square} bases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0608079v1", 
    "other_authors": "A. C. Gilbert, M. J. Strauss, J. A. Tropp, R. Vershynin", 
    "title": "Algorithmic linear dimension reduction in the l_1 norm for sparse   vectors", 
    "arxiv-id": "cs/0608079v1", 
    "author": "R. Vershynin", 
    "publish": "2006-08-19T01:55:14Z", 
    "summary": "This paper develops a new method for recovering m-sparse signals that is\nsimultaneously uniform and quick. We present a reconstruction algorithm whose\nrun time, O(m log^2(m) log^2(d)), is sublinear in the length d of the signal.\nThe reconstruction error is within a logarithmic factor (in m) of the optimal\nm-term approximation error in l_1. In particular, the algorithm recovers\nm-sparse signals perfectly and noisy signals are recovered with polylogarithmic\ndistortion. Our algorithm makes O(m log^2 (d)) measurements, which is within a\nlogarithmic factor of optimal. We also present a small-space implementation of\nthe algorithm. These sketching techniques and the corresponding reconstruction\nalgorithms provide an algorithmic dimension reduction in the l_1 norm. In\nparticular, vectors of support m in dimension d can be linearly embedded into\nO(m log^2 d) dimensions with polylogarithmic distortion. We can reconstruct a\nvector from its low-dimensional sketch in time O(m log^2(m) log^2(d)).\nFurthermore, this reconstruction is stable and robust under small\nperturbations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0608124v5", 
    "other_authors": "Philip Bille, Inge Li Goertz", 
    "title": "The Tree Inclusion Problem: In Linear Space and Faster", 
    "arxiv-id": "cs/0608124v5", 
    "author": "Inge Li Goertz", 
    "publish": "2006-08-31T12:23:37Z", 
    "summary": "Given two rooted, ordered, and labeled trees $P$ and $T$ the tree inclusion\nproblem is to determine if $P$ can be obtained from $T$ by deleting nodes in\n$T$. This problem has recently been recognized as an important query primitive\nin XML databases. Kilpel\\\"ainen and Mannila [\\emph{SIAM J. Comput. 1995}]\npresented the first polynomial time algorithm using quadratic time and space.\nSince then several improved results have been obtained for special cases when\n$P$ and $T$ have a small number of leaves or small depth. However, in the worst\ncase these algorithms still use quadratic time and space. Let $n_S$, $l_S$, and\n$d_S$ denote the number of nodes, the number of leaves, and the %maximum depth\nof a tree $S \\in \\{P, T\\}$. In this paper we show that the tree inclusion\nproblem can be solved in space $O(n_T)$ and time: O(\\min(l_Pn_T, l_Pl_T\\log\n\\log n_T + n_T, \\frac{n_Pn_T}{\\log n_T} + n_{T}\\log n_{T})). This improves or\nmatches the best known time complexities while using only linear space instead\nof quadratic. This is particularly important in practical applications, such as\nXML databases, where the space is likely to be a bottleneck."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0609032v3", 
    "other_authors": "Sumit Ganguly, Anirban Majumder", 
    "title": "CR-precis: A deterministic summary structure for update data streams", 
    "arxiv-id": "cs/0609032v3", 
    "author": "Anirban Majumder", 
    "publish": "2006-09-07T19:21:01Z", 
    "summary": "We present the \\crprecis structure, that is a general-purpose, deterministic\nand sub-linear data structure for summarizing \\emph{update} data streams. The\n\\crprecis structure yields the \\emph{first deterministic sub-linear space/time\nalgorithms for update streams} for answering a variety of fundamental stream\nqueries, such as, (a) point queries, (b) range queries, (c) finding approximate\nfrequent items, (d) finding approximate quantiles, (e) finding approximate\nhierarchical heavy hitters, (f) estimating inner-products, (g) near-optimal\n$B$-bucket histograms, etc.."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0609085v2", 
    "other_authors": "Philip Bille, Rolf Fagerberg, Inge Li Goertz", 
    "title": "Improved Approximate String Matching and Regular Expression Matching on   Ziv-Lempel Compressed Texts", 
    "arxiv-id": "cs/0609085v2", 
    "author": "Inge Li Goertz", 
    "publish": "2006-09-15T07:36:25Z", 
    "summary": "We study the approximate string matching and regular expression matching\nproblem for the case when the text to be searched is compressed with the\nZiv-Lempel adaptive dictionary compression schemes. We present a time-space\ntrade-off that leads to algorithms improving the previously known complexities\nfor both problems. In particular, we significantly improve the space bounds,\nwhich in practical applications are likely to be a bottleneck."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610001v1", 
    "other_authors": "Daisuke Okanohara, Kunihiko Sadakane", 
    "title": "Practical Entropy-Compressed Rank/Select Dictionary", 
    "arxiv-id": "cs/0610001v1", 
    "author": "Kunihiko Sadakane", 
    "publish": "2006-09-29T23:52:09Z", 
    "summary": "Rank/Select dictionaries are data structures for an ordered set $S \\subset\n\\{0,1,...,n-1\\}$ to compute $\\rank(x,S)$ (the number of elements in $S$ which\nare no greater than $x$), and $\\select(i,S)$ (the $i$-th smallest element in\n$S$), which are the fundamental components of \\emph{succinct data structures}\nof strings, trees, graphs, etc. In those data structures, however, only\nasymptotic behavior has been considered and their performance for real data is\nnot satisfactory. In this paper, we propose novel four Rank/Select\ndictionaries, esp, recrank, vcode and sdarray, each of which is small if the\nnumber of elements in $S$ is small, and indeed close to $nH_0(S)$ ($H_0(S) \\leq\n1$ is the zero-th order \\textit{empirical entropy} of $S$) in practice, and its\nquery time is superior to the previous ones. Experimental results reveal the\ncharacteristics of our data structures and also show that these data structures\nare superior to existing implementations in both size and query time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610046v5", 
    "other_authors": "Daniel Lemire", 
    "title": "Streaming Maximum-Minimum Filter Using No More than Three Comparisons   per Element", 
    "arxiv-id": "cs/0610046v5", 
    "author": "Daniel Lemire", 
    "publish": "2006-10-09T22:09:42Z", 
    "summary": "The running maximum-minimum (max-min) filter computes the maxima and minima\nover running windows of size w. This filter has numerous applications in signal\nprocessing and time series analysis. We present an easy-to-implement online\nalgorithm requiring no more than 3 comparisons per element, in the worst case.\nComparatively, no algorithm is known to compute the running maximum (or\nminimum) filter in 1.5 comparisons per element, in the worst case. Our\nalgorithm has reduced latency and memory usage."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0610119v1", 
    "other_authors": "Elad Hazan", 
    "title": "Approximate Convex Optimization by Online Game Playing", 
    "arxiv-id": "cs/0610119v1", 
    "author": "Elad Hazan", 
    "publish": "2006-10-19T22:10:32Z", 
    "summary": "Lagrangian relaxation and approximate optimization algorithms have received\nmuch attention in the last two decades. Typically, the running time of these\nmethods to obtain a $\\epsilon$ approximate solution is proportional to\n$\\frac{1}{\\epsilon^2}$. Recently, Bienstock and Iyengar, following Nesterov,\ngave an algorithm for fractional packing linear programs which runs in\n$\\frac{1}{\\epsilon}$ iterations. The latter algorithm requires to solve a\nconvex quadratic program every iteration - an optimization subroutine which\ndominates the theoretical running time.\n  We give an algorithm for convex programs with strictly convex constraints\nwhich runs in time proportional to $\\frac{1}{\\epsilon}$. The algorithm does NOT\nrequire to solve any quadratic program, but uses gradient steps and elementary\noperations only. Problems which have strictly convex constraints include\nmaximum entropy frequency estimation, portfolio optimization with loss risk\nconstraints, and various computational problems in signal processing.\n  As a side product, we also obtain a simpler version of Bienstock and\nIyengar's result for general linear programming, with similar running time.\n  We derive these algorithms using a new framework for deriving convex\noptimization algorithms from online game playing algorithms, which may be of\nindependent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611001v1", 
    "other_authors": "Michael Elkin", 
    "title": "A near-optimal fully dynamic distributed algorithm for maintaining   sparse spanners", 
    "arxiv-id": "cs/0611001v1", 
    "author": "Michael Elkin", 
    "publish": "2006-11-01T09:36:20Z", 
    "summary": "In this paper we devise an extremely efficient fully dynamic distributed\nalgorithm for maintaining sparse spanners. Our resuls also include the first\nfully dynamic centralized algorithm for the problem with non-trivial bounds for\nboth incremental and decremental update. Finally, we devise a very efficient\nstreaming algorithm for the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611019v2", 
    "other_authors": "Binh-Minh Bui-Xuan, Michel Habib, Vincent Limouzy, Fabien De Montgolfier", 
    "title": "Algorithmic Aspects of a General Modular Decomposition Theory", 
    "arxiv-id": "cs/0611019v2", 
    "author": "Fabien De Montgolfier", 
    "publish": "2006-11-04T18:32:23Z", 
    "summary": "A new general decomposition theory inspired from modular graph decomposition\nis presented. This helps unifying modular decomposition on different\nstructures, including (but not restricted to) graphs. Moreover, even in the\ncase of graphs, the terminology ``module'' not only captures the classical\ngraph modules but also allows to handle 2-connected components, star-cutsets,\nand other vertex subsets. The main result is that most of the nice algorithmic\ntools developed for modular decomposition of graphs still apply efficiently on\nour generalisation of modules. Besides, when an essential axiom is satisfied,\nalmost all the important properties can be retrieved. For this case, an\nalgorithm given by Ehrenfeucht, Gabow, McConnell and Sullivan 1994 is\ngeneralised and yields a very efficient solution to the associated\ndecomposition problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0611023v1", 
    "other_authors": "Surender Baswana", 
    "title": "Faster Streaming algorithms for graph spanners", 
    "arxiv-id": "cs/0611023v1", 
    "author": "Surender Baswana", 
    "publish": "2006-11-06T03:09:05Z", 
    "summary": "Given an undirected graph $G=(V,E)$ on $n$ vertices, $m$ edges, and an\ninteger $t\\ge 1$, a subgraph $(V,E_S)$, $E_S\\subseteq E$ is called a\n$t$-spanner if for any pair of vertices $u,v \\in V$, the distance between them\nin the subgraph is at most $t$ times the actual distance. We present streaming\nalgorithms for computing a $t$-spanner of essentially optimal size-stretch\ntrade offs for any undirected graph.\n  Our first algorithm is for the classical streaming model and works for\nunweighted graphs only. The algorithm performs a single pass on the stream of\nedges and requires $O(m)$ time to process the entire stream of edges. This\ndrastically improves the previous best single pass streaming algorithm for\ncomputing a $t$-spanner which requires $\\theta(mn^{\\frac{2}{t}})$ time to\nprocess the stream and computes spanner with size slightly larger than the\noptimal.\n  Our second algorithm is for {\\em StreamSort} model introduced by Aggarwal et\nal. [FOCS 2004], which is the streaming model augmented with a sorting\nprimitive. The {\\em StreamSort} model has been shown to be a more powerful and\nstill very realistic model than the streaming model for massive data sets\napplications. Our algorithm, which works of weighted graphs as well, performs\n$O(t)$ passes using $O(\\log n)$ bits of working memory only.\n  Our both the algorithms require elementary data structures."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0612037v1", 
    "other_authors": "J\u00e9r\u00f4me Leroux", 
    "title": "Least Significant Digit First Presburger Automata", 
    "arxiv-id": "cs/0612037v1", 
    "author": "J\u00e9r\u00f4me Leroux", 
    "publish": "2006-12-06T14:55:36Z", 
    "summary": "Since 1969 \\cite{C-MST69,S-SMJ77}, we know that any Presburger-definable set\n\\cite{P-PCM29} (a set of integer vectors satisfying a formula in the\nfirst-order additive theory of the integers) can be represented by a\nstate-based symmbolic representation, called in this paper Finite Digit Vector\nAutomata (FDVA). Efficient algorithms for manipulating these sets have been\nrecently developed. However, the problem of deciding if a FDVA represents such\na set, is a well-known hard problem first solved by Muchnik in 1991 with a\nquadruply-exponential time algorithm. In this paper, we show how to determine\nin polynomial time whether a FDVA represents a Presburger-definable set, and we\nprovide in this positive case a polynomial time algorithm that constructs a\nPresburger-formula that defines the same set."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0612100v1", 
    "other_authors": "Leah Epstein, Rob van Stee", 
    "title": "Improved results for a memory allocation problem", 
    "arxiv-id": "cs/0612100v1", 
    "author": "Rob van Stee", 
    "publish": "2006-12-20T13:39:18Z", 
    "summary": "We consider a memory allocation problem that can be modeled as a version of\nbin packing where items may be split, but each bin may contain at most two\n(parts of) items. A 3/2-approximation algorithm and an NP-hardness proof for\nthis problem was given by Chung et al. We give a simpler 3/2-approximation\nalgorithm for it which is in fact an online algorithm. This algorithm also has\ngood performance for the more general case where each bin may contain at most k\nparts of items. We show that this general case is also strongly NP-hard.\nAdditionally, we give an efficient 7/5-approximation algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0701020v2", 
    "other_authors": "Sumit Ganguly", 
    "title": "A nearly optimal and deterministic summary structure for update data   streams", 
    "arxiv-id": "cs/0701020v2", 
    "author": "Sumit Ganguly", 
    "publish": "2007-01-04T09:03:08Z", 
    "summary": "The paper has been withdrawn due to an error in Lemma 1."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0701142v1", 
    "other_authors": "Wolfgang Bein, Lawrence L. Larmore, R\u00fcdiger Reischuk", 
    "title": "Knowledge State Algorithms: Randomization with Limited Information", 
    "arxiv-id": "cs/0701142v1", 
    "author": "R\u00fcdiger Reischuk", 
    "publish": "2007-01-23T00:54:27Z", 
    "summary": "We introduce the concept of knowledge states; many well-known algorithms can\nbe viewed as knowledge state algorithms. The knowledge state approach can be\nused to to construct competitive randomized online algorithms and study the\ntradeoff between competitiveness and memory. A knowledge state simply states\nconditional obligations of an adversary, by fixing a work function, and gives a\ndistribution for the algorithm. When a knowledge state algorithm receives a\nrequest, it then calculates one or more \"subsequent\" knowledge states, together\nwith a probability of transition to each. The algorithm then uses randomization\nto select one of those subsequents to be the new knowledge state. We apply the\nmethod to the paging problem. We present optimally competitive algorithm for\npaging for the cases where the cache sizes are k=2 and k=3. These algorithms\nuse only a very limited number of bookmarks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702029v1", 
    "other_authors": "Mario Szegedy, Mikkel Thorup", 
    "title": "On the variance of subset sum estimation", 
    "arxiv-id": "cs/0702029v1", 
    "author": "Mikkel Thorup", 
    "publish": "2007-02-05T15:55:41Z", 
    "summary": "For high volume data streams and large data warehouses, sampling is used for\nefficient approximate answers to aggregate queries over selected subsets.\nMathematically, we are dealing with a set of weighted items and want to support\nqueries to arbitrary subset sums. With unit weights, we can compute subset\nsizes which together with the previous sums provide the subset averages. The\nquestion addressed here is which sampling scheme we should use to get the most\naccurate subset sum estimates.\n  We present a simple theorem on the variance of subset sum estimation and use\nit to prove variance optimality and near-optimality of subset sum estimation\nwith different known sampling schemes. This variance is measured as the average\nover all subsets of any given size. By optimal we mean there is no set of input\nweights for which any sampling scheme can have a better average variance. Such\npowerful results can never be established experimentally. The results of this\npaper are derived mathematically. For example, we show that appropriately\nweighted systematic sampling is simultaneously optimal for all subset sizes.\nMore standard schemes such as uniform sampling and\nprobability-proportional-to-size sampling with replacement can be arbitrarily\nbad.\n  Knowing the variance optimality of different sampling schemes can help\ndeciding which sampling scheme to apply in a given context."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702032v1", 
    "other_authors": "Reid Andersen", 
    "title": "Finding large and small dense subgraphs", 
    "arxiv-id": "cs/0702032v1", 
    "author": "Reid Andersen", 
    "publish": "2007-02-05T19:29:38Z", 
    "summary": "We consider two optimization problems related to finding dense subgraphs. The\ndensest at-least-k-subgraph problem (DalkS) is to find an induced subgraph of\nhighest average degree among all subgraphs with at least k vertices, and the\ndensest at-most-k-subgraph problem (DamkS) is defined similarly. These problems\nare related to the well-known densest k-subgraph problem (DkS), which is to\nfind the densest subgraph on exactly k vertices. We show that DalkS can be\napproximated efficiently, while DamkS is nearly as hard to approximate as the\ndensest k-subgraph problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702043v1", 
    "other_authors": "Ch\u00ednh T. Ho\u00e0ng, Marcin Kami\u0144ski, Vadim Lozin, J. Sawada, X. Shu", 
    "title": "Deciding k-colourability of $P_5$-free graphs in polynomial time", 
    "arxiv-id": "cs/0702043v1", 
    "author": "X. Shu", 
    "publish": "2007-02-07T15:29:32Z", 
    "summary": "The problem of computing the chromatic number of a $P_5$-free graph is known\nto be NP-hard. In contrast to this negative result, we show that determining\nwhether or not a $P_5$-free graph admits a $k$-colouring, for each fixed number\nof colours $k$, can be done in polynomial time. If such a colouring exists, our\nalgorithm produces it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702056v1", 
    "other_authors": "Hanene Mohamed", 
    "title": "A probabilistic analysis of a leader election algorithm", 
    "arxiv-id": "cs/0702056v1", 
    "author": "Hanene Mohamed", 
    "publish": "2007-02-09T15:16:48Z", 
    "summary": "A {\\em leader election} algorithm is an elimination process that divides\nrecursively into tow subgroups an initial group of n items, eliminates one\nsubgroup and continues the procedure until a subgroup is of size 1. In this\npaper the biased case is analyzed. We are interested in the {\\em cost} of the\nalgorithm, i.e. the number of operations needed until the algorithm stops.\nUsing a probabilistic approach, the asymptotic behavior of the algorithm is\nshown to be related to the behavior of a hitting time of two random sequences\non [0,1]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702057v2", 
    "other_authors": "Mohsen Bahramgiri, Salman Beigi", 
    "title": "An Efficient Algorithm to Recognize Locally Equivalent Graphs in   Non-Binary Case", 
    "arxiv-id": "cs/0702057v2", 
    "author": "Salman Beigi", 
    "publish": "2007-02-09T15:42:46Z", 
    "summary": "Let $v$ be a vertex of a graph $G$. By the local complementation of $G$ at\n$v$ we mean to complement the subgraph induced by the neighbors of $v$. This\noperator can be generalized as follows. Assume that, each edge of $G$ has a\nlabel in the finite field $\\mathbf{F}_q$. Let $(g_{ij})$ be set of labels\n($g_{ij}$ is the label of edge $ij$). We define two types of operators. For the\nfirst one, let $v$ be a vertex of $G$ and $a\\in \\mathbf{F}_q$, and obtain the\ngraph with labels $g'_{ij}=g_{ij}+ag_{vi}g_{vj}$. For the second, if $0\\neq\nb\\in \\mathbf{F}_q$ the resulted graph is a graph with labels $g''_{vi}=bg_{vi}$\nand $g''_{ij}=g_{ij}$, for $i,j$ unequal to $v$. It is clear that if the field\nis binary, the operators are just local complementations that we described.\n  The problem of whether two graphs are equivalent under local complementations\nhas been studied, \\cite{bouchalg}. Here we consider the general case and\nassuming that $q$ is odd, present the first known efficient algorithm to verify\nwhether two graphs are locally equivalent or not."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0702151v3", 
    "other_authors": "Vladimir Braverman, Rafail Ostrovsky, Carlo Zaniolo", 
    "title": "Succinct Sampling on Streams", 
    "arxiv-id": "cs/0702151v3", 
    "author": "Carlo Zaniolo", 
    "publish": "2007-02-25T17:20:48Z", 
    "summary": "A streaming model is one where data items arrive over long period of time,\neither one item at a time or in bursts. Typical tasks include computing various\nstatistics over a sliding window of some fixed time-horizon. What makes the\nstreaming model interesting is that as the time progresses, old items expire\nand new ones arrive. One of the simplest and central tasks in this model is\nsampling. That is, the task of maintaining up to $k$ uniformly distributed\nitems from a current time-window as old items expire and new ones arrive. We\ncall sampling algorithms {\\bf succinct} if they use provably optimal (up to\nconstant factors) {\\bf worst-case} memory to maintain $k$ items (either with or\nwithout replacement). We stress that in many applications structures that have\n{\\em expected} succinct representation as the time progresses are not\nsufficient, as small probability events eventually happen with probability 1.\nThus, in this paper we ask the following question: are Succinct Sampling on\nStreams (or $S^3$-algorithms)possible, and if so for what models? Perhaps\nsomewhat surprisingly, we show that $S^3$-algorithms are possible for {\\em all}\nvariants of the problem mentioned above, i.e. both with and without replacement\nand both for one-at-a-time and bursty arrival models. Finally, we use $S^3$\nalgorithms to solve various problems in sliding windows model, including\nfrequency moments, counting triangles, entropy and density estimations. For\nthese problems we present \\emph{first} solutions with provable worst-case\nmemory guarantees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0703006v1", 
    "other_authors": "Jing-Chao Chen", 
    "title": "XORSAT: An Efficient Algorithm for the DIMACS 32-bit Parity Problem", 
    "arxiv-id": "cs/0703006v1", 
    "author": "Jing-Chao Chen", 
    "publish": "2007-03-02T01:38:16Z", 
    "summary": "The DIMACS 32-bit parity problem is a satisfiability (SAT) problem hard to\nsolve. So far, EqSatz by Li is the only solver which can solve this problem.\nHowever, This solver is very slow. It is reported that it spent 11855 seconds\nto solve a par32-5 instance on a Maxintosh G3 300 MHz. The paper introduces a\nnew solver, XORSAT, which splits the original problem into two parts:\nstructured part and random part, and then solves separately them with WalkSAT\nand an XOR equation solver. Based our empirical observation, XORSAT is\nsurprisingly fast, which is approximately 1000 times faster than EqSatz. For a\npar32-5 instance, XORSAT took 2.9 seconds, while EqSatz took 2844 seconds on\nIntel Pentium IV 2.66GHz CPU. We believe that this method significantly\ndifferent from traditional methods is also useful beyond this domain."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00493-010-2302-z", 
    "link": "http://arxiv.org/pdf/cs/0703010v2", 
    "other_authors": "Jaroslaw Byrka, Karen Aardal", 
    "title": "An optimal bifactor approximation algorithm for the metric uncapacitated   facility location problem", 
    "arxiv-id": "cs/0703010v2", 
    "author": "Karen Aardal", 
    "publish": "2007-03-02T14:49:57Z", 
    "summary": "We obtain a 1.5-approximation algorithm for the metric uncapacitated facility\nlocation problem (UFL), which improves on the previously best known\n1.52-approximation algorithm by Mahdian, Ye and Zhang. Note, that the\napproximability lower bound by Guha and Khuller is 1.463.\n  An algorithm is a {\\em ($\\lambda_f$,$\\lambda_c$)-approximation algorithm} if\nthe solution it produces has total cost at most $\\lambda_f \\cdot F^* +\n\\lambda_c \\cdot C^*$, where $F^*$ and $C^*$ are the facility and the connection\ncost of an optimal solution. Our new algorithm, which is a modification of the\n$(1+2/e)$-approximation algorithm of Chudak and Shmoys, is a\n(1.6774,1.3738)-approximation algorithm for the UFL problem and is the first\none that touches the approximability limit curve $(\\gamma_f, 1+2e^{-\\gamma_f})$\nestablished by Jain, Mahdian and Saberi. As a consequence, we obtain the first\noptimal approximation algorithm for instances dominated by connection costs.\nWhen combined with a (1.11,1.7764)-approximation algorithm proposed by Jain et\nal., and later analyzed by Mahdian et al., we obtain the overall approximation\nguarantee of 1.5 for the metric UFL problem. We also describe how to use our\nalgorithm to improve the approximation ratio for the 3-level version of UFL."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74839-7_9", 
    "link": "http://arxiv.org/pdf/cs/0703013v1", 
    "other_authors": "Vincent Limouzy, Fabien De Montgolfier, Micha\u00ebl Rao", 
    "title": "NLC-2 graph recognition and isomorphism", 
    "arxiv-id": "cs/0703013v1", 
    "author": "Micha\u00ebl Rao", 
    "publish": "2007-03-03T06:44:57Z", 
    "summary": "NLC-width is a variant of clique-width with many application in graph\nalgorithmic. This paper is devoted to graphs of NLC-width two. After giving new\nstructural properties of the class, we propose a $O(n^2 m)$-time algorithm,\nimproving Johansson's algorithm \\cite{Johansson00}. Moreover, our alogrithm is\nsimple to understand. The above properties and algorithm allow us to propose a\nrobust $O(n^2 m)$-time isomorphism algorithm for NLC-2 graphs. As far as we\nknow, it is the first polynomial-time algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74839-7_9", 
    "link": "http://arxiv.org/pdf/cs/0703109v2", 
    "other_authors": "Owen Kaser, Daniel Lemire", 
    "title": "Tag-Cloud Drawing: Algorithms for Cloud Visualization", 
    "arxiv-id": "cs/0703109v2", 
    "author": "Daniel Lemire", 
    "publish": "2007-03-22T14:54:48Z", 
    "summary": "Tag clouds provide an aggregate of tag-usage statistics. They are typically\nsent as in-line HTML to browsers. However, display mechanisms suited for\nordinary text are not ideal for tags, because font sizes may vary widely on a\nline. As well, the typical layout does not account for relationships that may\nbe known between tags. This paper presents models and algorithms to improve the\ndisplay of tag clouds that consist of in-line HTML, as well as algorithms that\nuse nested tables to achieve a more general 2-dimensional layout in which tag\nrelationships are considered. The first algorithms leverage prior work in\ntypesetting and rectangle packing, whereas the second group of algorithms\nleverage prior work in Electronic Design Automation. Experiments show our\nalgorithms can be efficiently implemented and perform well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.0062v1", 
    "other_authors": "Rastislav \u0160r\u00e1mek, Bro\u0148a Brejov\u00e1, Tom\u00e1\u0161 Vina\u0159", 
    "title": "On-line Viterbi Algorithm and Its Relationship to Random Walks", 
    "arxiv-id": "0704.0062v1", 
    "author": "Tom\u00e1\u0161 Vina\u0159", 
    "publish": "2007-03-31T23:52:33Z", 
    "summary": "In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\ntwo-state HMMs suggests that the expected maximum memory used to decode\nsequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$,\nwithout a significant slow-down compared to the classical Viterbi algorithm.\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\nfor continuous data streams. We also experimentally demonstrate the performance\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\nsimulated and real DNA sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.0834v1", 
    "other_authors": "Anatoly Rodionov, Sergey Volkov", 
    "title": "P-adic arithmetic coding", 
    "arxiv-id": "0704.0834v1", 
    "author": "Sergey Volkov", 
    "publish": "2007-04-06T02:30:42Z", 
    "summary": "A new incremental algorithm for data compression is presented. For a sequence\nof input symbols algorithm incrementally constructs a p-adic integer number as\nan output. Decoding process starts with less significant part of a p-adic\ninteger and incrementally reconstructs a sequence of input symbols. Algorithm\nis based on certain features of p-adic numbers and p-adic norm. p-adic coding\nalgorithm may be considered as of generalization a popular compression\ntechnique - arithmetic coding algorithms. It is shown that for p = 2 the\nalgorithm works as integer variant of arithmetic coding; for a special class of\nmodels it gives exactly the same codes as Huffman's algorithm, for another\nspecial model and a specific alphabet it gives Golomb-Rice codes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.3313v3", 
    "other_authors": "David Eppstein, Michael T. Goodrich", 
    "title": "Straggler Identification in Round-Trip Data Streams via Newton's   Identities and Invertible Bloom Filters", 
    "arxiv-id": "0704.3313v3", 
    "author": "Michael T. Goodrich", 
    "publish": "2007-04-25T06:59:43Z", 
    "summary": "We introduce the straggler identification problem, in which an algorithm must\ndetermine the identities of the remaining members of a set after it has had a\nlarge number of insertion and deletion operations performed on it, and now has\nrelatively few remaining members. The goal is to do this in o(n) space, where n\nis the total number of identities. The straggler identification problem has\napplications, for example, in determining the set of unacknowledged packets in\na high-bandwidth multicast data stream. We provide a deterministic solution to\nthe straggler identification problem that uses only O(d log n) bits and is\nbased on a novel application of Newton's identities for symmetric polynomials.\nThis solution can identify any subset of d stragglers from a set of n O(log\nn)-bit identifiers, assuming that there are no false deletions of identities\nnot already in the set. Indeed, we give a lower bound argument that shows that\nany small-space deterministic solution to the straggler identification problem\ncannot be guaranteed to handle false deletions. Nevertheless, we show that\nthere is a simple randomized solution using O(d log n log(1/epsilon)) bits that\ncan maintain a multiset and solve the straggler identification problem,\ntolerating false deletions, where epsilon>0 is a user-defined parameter\nbounding the probability of an incorrect response. This randomized solution is\nbased on a new type of Bloom filter, which we call the invertible Bloom filter."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74126-8_23", 
    "link": "http://arxiv.org/pdf/0704.3773v2", 
    "other_authors": "Sam Tannous", 
    "title": "Avoiding Rotated Bitboards with Direct Lookup", 
    "arxiv-id": "0704.3773v2", 
    "author": "Sam Tannous", 
    "publish": "2007-04-28T03:11:59Z", 
    "summary": "This paper describes an approach for obtaining direct access to the attacked\nsquares of sliding pieces without resorting to rotated bitboards. The technique\ninvolves creating four hash tables using the built in hash arrays from an\ninterpreted, high level language. The rank, file, and diagonal occupancy are\nfirst isolated by masking the desired portion of the board. The attacked\nsquares are then directly retrieved from the hash tables. Maintaining\nincrementally updated rotated bitboards becomes unnecessary as does all the\nupdating, mapping and shifting required to access the attacked squares.\nFinally, rotated bitboard move generation speed is compared with that of the\ndirect hash table lookup method."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.0204v1", 
    "other_authors": "Lukasz A. Machowski, Tshilidzi Marwala", 
    "title": "Using Images to create a Hierarchical Grid Spatial Index", 
    "arxiv-id": "0705.0204v1", 
    "author": "Tshilidzi Marwala", 
    "publish": "2007-05-02T05:37:32Z", 
    "summary": "This paper presents a hybrid approach to spatial indexing of two dimensional\ndata. It sheds new light on the age old problem by thinking of the traditional\nalgorithms as working with images. Inspiration is drawn from an analogous\nsituation that is found in machine and human vision. Image processing\ntechniques are used to assist in the spatial indexing of the data. A fixed grid\napproach is used and bins with too many records are sub-divided hierarchically.\nSearch queries are pre-computed for bins that do not contain any data records.\nThis has the effect of dividing the search space up into non rectangular\nregions which are based on the spatial properties of the data. The bucketing\nquad tree can be considered as an image with a resolution of two by two for\neach layer. The results show that this method performs better than the quad\ntree if there are more divisions per layer. This confirms our suspicions that\nthe algorithm works better if it gets to look at the data with higher\nresolution images. An elegant class structure is developed where the\nimplementation of concrete spatial indexes for a particular data type merely\nrelies on rendering the data onto an image."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1025v2", 
    "other_authors": "David Eppstein", 
    "title": "Recognizing Partial Cubes in Quadratic Time", 
    "arxiv-id": "0705.1025v2", 
    "author": "David Eppstein", 
    "publish": "2007-05-08T17:59:08Z", 
    "summary": "We show how to test whether a graph with n vertices and m edges is a partial\ncube, and if so how to find a distance-preserving embedding of the graph into a\nhypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time\nsolutions."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1521v2", 
    "other_authors": "Frank Gurski", 
    "title": "A note on module-composed graphs", 
    "arxiv-id": "0705.1521v2", 
    "author": "Frank Gurski", 
    "publish": "2007-05-10T18:08:22Z", 
    "summary": "In this paper we consider module-composed graphs, i.e. graphs which can be\ndefined by a sequence of one-vertex insertions v_1,...,v_n, such that the\nneighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of\nthe graph defined by vertices v_1,..., v_{i-1}.\n  We show that module-composed graphs are HHDS-free and thus homogeneously\norderable, weakly chordal, and perfect. Every bipartite distance hereditary\ngraph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is\nmodule-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide\nwhether a given graph G is module-composed and construct a corresponding\nmodule-sequence.\n  For the case of bipartite graphs, module-composed graphs are exactly distance\nhereditary graphs, which implies simple linear time algorithms for their\nrecognition and construction of a corresponding module-sequence."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1750v6", 
    "other_authors": "Peng Cui", 
    "title": "A Tighter Analysis of Setcover Greedy Algorithm for Test Set", 
    "arxiv-id": "0705.1750v6", 
    "author": "Peng Cui", 
    "publish": "2007-05-12T04:18:36Z", 
    "summary": "Setcover greedy algorithm is a natural approximation algorithm for test set\nproblem. This paper gives a precise and tighter analysis of performance\nguarantee of this algorithm. The author improves the performance guarantee\n$2\\ln n$ which derives from set cover problem to $1.1354\\ln n$ by applying the\npotential function technique. In addition, the author gives a nontrivial lower\nbound $1.0004609\\ln n$ of performance guarantee of this algorithm. This lower\nbound, together with the matching bound of information content heuristic,\nconfirms the fact information content heuristic is slightly better than\nsetcover greedy algorithm in worst case."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1970v1", 
    "other_authors": "Nikolaos Laoutaris", 
    "title": "A Closed-Form Method for LRU Replacement under Generalized Power-Law   Demand", 
    "arxiv-id": "0705.1970v1", 
    "author": "Nikolaos Laoutaris", 
    "publish": "2007-05-14T16:04:48Z", 
    "summary": "We consider the well known \\emph{Least Recently Used} (LRU) replacement\nalgorithm and analyze it under the independent reference model and generalized\npower-law demand. For this extensive family of demand distributions we derive a\nclosed-form expression for the per object steady-state hit ratio. To the best\nof our knowledge, this is the first analytic derivation of the per object hit\nratio of LRU that can be obtained in constant time without requiring laborious\nnumeric computations or simulation. Since most applications of replacement\nalgorithms include (at least) some scenarios under i.i.d. requests, our method\nhas substantial practical value, especially when having to analyze multiple\ncaches, where existing numeric methods and simulation become too time\nconsuming."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.1986v1", 
    "other_authors": "Andrei Paun", 
    "title": "On the Hopcroft's minimization algorithm", 
    "arxiv-id": "0705.1986v1", 
    "author": "Andrei Paun", 
    "publish": "2007-05-14T17:15:53Z", 
    "summary": "We show that the absolute worst case time complexity for Hopcroft's\nminimization algorithm applied to unary languages is reached only for de Bruijn\nwords. A previous paper by Berstel and Carton gave the example of de Bruijn\nwords as a language that requires O(n log n) steps by carefully choosing the\nsplitting sets and processing these sets in a FIFO mode. We refine the previous\nresult by showing that the Berstel/Carton example is actually the absolute\nworst case time complexity in the case of unary languages. We also show that a\nLIFO implementation will not achieve the same worst time complexity for the\ncase of unary languages. Lastly, we show that the same result is valid also for\nthe cover automata and a modification of the Hopcroft's algorithm, modification\nused in minimization of cover automata."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0705.4171v1", 
    "other_authors": "Eva Borbely", 
    "title": "Grover search algorithm", 
    "arxiv-id": "0705.4171v1", 
    "author": "Eva Borbely", 
    "publish": "2007-05-29T09:42:46Z", 
    "summary": "A quantum algorithm is a set of instructions for a quantum computer, however,\nunlike algorithms in classical computer science their results cannot be\nguaranteed. A quantum system can undergo two types of operation, measurement\nand quantum state transformation, operations themselves must be unitary\n(reversible). Most quantum algorithms involve a series of quantum state\ntransformations followed by a measurement. Currently very few quantum\nalgorithms are known and no general design methodology exists for their\nconstruction."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.0046v1", 
    "other_authors": "Jing-Chao Chen", 
    "title": "Symmetry Partition Sort", 
    "arxiv-id": "0706.0046v1", 
    "author": "Jing-Chao Chen", 
    "publish": "2007-06-01T01:47:06Z", 
    "summary": "In this paper, we propose a useful replacement for quicksort-style utility\nfunctions. The replacement is called Symmetry Partition Sort, which has\nessentially the same principle as Proportion Extend Sort. The maximal\ndifference between them is that the new algorithm always places already\npartially sorted inputs (used as a basis for the proportional extension) on\nboth ends when entering the partition routine. This is advantageous to speeding\nup the partition routine. The library function based on the new algorithm is\nmore attractive than Psort which is a library function introduced in 2004. Its\nimplementation mechanism is simple. The source code is clearer. The speed is\nfaster, with O(n log n) performance guarantee. Both the robustness and\nadaptivity are better. As a library function, it is competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.1084v1", 
    "other_authors": "Sofya Raskhodnikova, Dana Ron, Ronitt Rubinfeld, Adam Smith", 
    "title": "Sublinear Algorithms for Approximating String Compressibility", 
    "arxiv-id": "0706.1084v1", 
    "author": "Adam Smith", 
    "publish": "2007-06-08T02:58:28Z", 
    "summary": "We raise the question of approximating the compressibility of a string with\nrespect to a fixed compression scheme, in sublinear time. We study this\nquestion in detail for two popular lossless compression schemes: run-length\nencoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for\napproximating compressibility with respect to both schemes. We also give\nseveral lower bounds that show that our algorithms for both schemes cannot be\nimproved significantly.\n  Our investigation of LZ yields results whose interest goes beyond the initial\nquestions we set out to study. In particular, we prove combinatorial structural\nlemmas that relate the compressibility of a string with respect to Lempel-Ziv\nto the number of distinct short substrings contained in it. In addition, we\nshow that approximating the compressibility with respect to LZ is related to\napproximating the support size of a distribution."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.3565v2", 
    "other_authors": "Anatoly D. Plotnikov", 
    "title": "Experimental Algorithm for the Maximum Independent Set Problem", 
    "arxiv-id": "0706.3565v2", 
    "author": "Anatoly D. Plotnikov", 
    "publish": "2007-06-25T06:45:49Z", 
    "summary": "We develop an experimental algorithm for the exact solving of the maximum\nindependent set problem. The algorithm consecutively finds the maximal\nindependent sets of vertices in an arbitrary undirected graph such that the\nnext such set contains more elements than the preceding one. For this purpose,\nwe use a technique, developed by Ford and Fulkerson for the finite partially\nordered sets, in particular, their method for partition of a poset into the\nminimum number of chains with finding the maximum antichain. In the process of\nsolving, a special digraph is constructed, and a conjecture is formulated\nconcerning properties of such digraph. This allows to offer of the solution\nalgorithm. Its theoretical estimation of running time equals to is $O(n^{8})$,\nwhere $n$ is the number of graph vertices. The offered algorithm was tested by\na program on random graphs. The testing the confirms correctness of the\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0706.4107v1", 
    "other_authors": "Gianni Franceschini, S. Muthukrishnan, Mihai Patrascu", 
    "title": "Radix Sorting With No Extra Space", 
    "arxiv-id": "0706.4107v1", 
    "author": "Mihai Patrascu", 
    "publish": "2007-06-27T22:04:40Z", 
    "summary": "It is well known that n integers in the range [1,n^c] can be sorted in O(n)\ntime in the RAM model using radix sorting. More generally, integers in any\nrange [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these\nalgorithms use O(n) words of extra memory. Is this necessary?\n  We present a simple, stable, integer sorting algorithm for words of size\nO(log n), which works in O(n) time and uses only O(1) words of extra memory on\na RAM model. This is the integer sorting case most useful in practice. We\nextend this result with same bounds to the case when the keys are read-only,\nwhich is of theoretical interest. Another interesting question is the case of\narbitrary c. Here we present a black-box transformation from any RAM sorting\nalgorithm to a sorting algorithm which uses only O(1) extra space and has the\nsame running time. This settles the complexity of in-place sorting in terms of\nthe complexity of sorting."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.0546v1", 
    "other_authors": "Juli\u00e1n Mestre", 
    "title": "Weighted Popular Matchings", 
    "arxiv-id": "0707.0546v1", 
    "author": "Juli\u00e1n Mestre", 
    "publish": "2007-07-04T06:55:43Z", 
    "summary": "We study the problem of assigning jobs to applicants. Each applicant has a\nweight and provides a preference list ranking a subset of the jobs. A matching\nM is popular if there is no other matching M' such that the weight of the\napplicants who prefer M' over M exceeds the weight of those who prefer M over\nM'. This paper gives efficient algorithms to find a popular matching if one\nexists."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.0648v1", 
    "other_authors": "Anupam Gupta, MohammadTaghi Hajiaghayi, Viswanath Nagarajan, R. Ravi", 
    "title": "Dial a Ride from k-forest", 
    "arxiv-id": "0707.0648v1", 
    "author": "R. Ravi", 
    "publish": "2007-07-04T16:08:40Z", 
    "summary": "The k-forest problem is a common generalization of both the k-MST and the\ndense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices\n$V$, with $m$ demand pairs $\\subseteq V \\times V$ and a ``target'' $k\\le m$,\nthe goal is to find a minimum cost subgraph that connects at least $k$ demand\npairs. In this paper, we give an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\})$-approximation\nalgorithm for $k$-forest, improving on the previous best ratio of\n$O(n^{2/3}\\log n)$ by Segev & Segev.\n  We then apply our algorithm for k-forest to obtain approximation algorithms\nfor several Dial-a-Ride problems. The basic Dial-a-Ride problem is the\nfollowing: given an $n$ point metric space with $m$ objects each with its own\nsource and destination, and a vehicle capable of carrying at most $k$ objects\nat any time, find the minimum length tour that uses this vehicle to move each\nobject from its source to destination. We prove that an $\\alpha$-approximation\nalgorithm for the $k$-forest problem implies an\n$O(\\alpha\\cdot\\log^2n)$-approximation algorithm for Dial-a-Ride. Using our\nresults for $k$-forest, we get an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\}\\cdot\\log^2 n)$-\napproximation algorithm for Dial-a-Ride. The only previous result known for\nDial-a-Ride was an $O(\\sqrt{k}\\log n)$-approximation by Charikar &\nRaghavachari; our results give a different proof of a similar approximation\nguarantee--in fact, when the vehicle capacity $k$ is large, we give a slight\nimprovement on their results."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.1051v1", 
    "other_authors": "Mark Braverman, Elchanan Mossel", 
    "title": "Noisy Sorting Without Resampling", 
    "arxiv-id": "0707.1051v1", 
    "author": "Elchanan Mossel", 
    "publish": "2007-07-06T21:30:24Z", 
    "summary": "In this paper we study noisy sorting without re-sampling. In this problem\nthere is an unknown order $a_{\\pi(1)} < ... < a_{\\pi(n)}$ where $\\pi$ is a\npermutation on $n$ elements. The input is the status of $n \\choose 2$ queries\nof the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least\n$1/2+\\ga$ if $\\pi(i) > \\pi(j)$ for all pairs $i \\neq j$, where $\\ga > 0$ is a\nconstant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that\nthe errors are independent. Given the status of the queries the goal is to find\nthe maximum likelihood order. In other words, the goal is find a permutation\n$\\sigma$ that minimizes the number of pairs $\\sigma(i) > \\sigma(j)$ where\n$q(\\sigma(i),\\sigma(j)) = -$. The problem so defined is the feedback arc set\nproblem on distributions of inputs, each of which is a tournament obtained as a\nnoisy perturbations of a linear order. Note that when $\\ga < 1/2$ and $n$ is\nlarge, it is impossible to recover the original order $\\pi$.\n  It is known that the weighted feedback are set problem on tournaments is\nNP-hard in general. Here we present an algorithm of running time\n$n^{O(\\gamma^{-4})}$ and sampling complexity $O_{\\gamma}(n \\log n)$ that with\nhigh probability solves the noisy sorting without re-sampling problem. We also\nshow that if $a_{\\sigma(1)},a_{\\sigma(2)},...,a_{\\sigma(n)}$ is an optimal\nsolution of the problem then it is ``close'' to the original order. More\nformally, with high probability it holds that $\\sum_i |\\sigma(i) - \\pi(i)| =\n\\Theta(n)$ and $\\max_i |\\sigma(i) - \\pi(i)| = \\Theta(\\log n)$.\n  Our results are of interest in applications to ranking, such as ranking in\nsports, or ranking of search items based on comparisons by experts."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.1714v1", 
    "other_authors": "Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, Michael W. Mahoney", 
    "title": "Sampling Algorithms and Coresets for Lp Regression", 
    "arxiv-id": "0707.1714v1", 
    "author": "Michael W. Mahoney", 
    "publish": "2007-07-11T22:04:18Z", 
    "summary": "The Lp regression problem takes as input a matrix $A \\in \\Real^{n \\times d}$,\na vector $b \\in \\Real^n$, and a number $p \\in [1,\\infty)$, and it returns as\noutput a number ${\\cal Z}$ and a vector $x_{opt} \\in \\Real^d$ such that ${\\cal\nZ} = \\min_{x \\in \\Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we\nconstruct coresets and obtain an efficient two-stage sampling-based\napproximation algorithm for the very overconstrained ($n \\gg d$) version of\nthis classical problem, for all $p \\in [1, \\infty)$. The first stage of our\nalgorithm non-uniformly samples $\\hat{r}_1 = O(36^p d^{\\max\\{p/2+1, p\\}+1})$\nrows of $A$ and the corresponding elements of $b$, and then it solves the Lp\nregression problem on the sample; we prove this is an 8-approximation. The\nsecond stage of our algorithm uses the output of the first stage to resample\n$\\hat{r}_1/\\epsilon^2$ constraints, and then it solves the Lp regression\nproblem on the new sample; we prove this is a $(1+\\epsilon)$-approximation. Our\nalgorithm unifies, improves upon, and extends the existing algorithms for\nspecial cases of Lp regression, namely $p = 1,2$. In course of proving our\nresult, we develop two concepts--well-conditioned bases and subspace-preserving\nsampling--that are of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1109/ICSMC.2006.385020", 
    "link": "http://arxiv.org/pdf/0707.2160v1", 
    "other_authors": "Seth Pettie", 
    "title": "Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture", 
    "arxiv-id": "0707.2160v1", 
    "author": "Seth Pettie", 
    "publish": "2007-07-14T16:38:08Z", 
    "summary": "We introduce a new technique to bound the asymptotic performance of splay\ntrees. The basic idea is to transcribe, in an indirect fashion, the rotations\nperformed by the splay tree as a Davenport-Schinzel sequence S, none of whose\nsubsequences are isomorphic to fixed forbidden subsequence. We direct this\ntechnique towards Tarjan's deque conjecture and prove that n deque operations\nrequire O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of\napplications of the inverse-Ackermann function mapping n to a constant. We are\noptimistic that this approach could be directed towards other open conjectures\non splay trees such as the traversal and split conjectures."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.0600v1", 
    "other_authors": "Michael J. Lee", 
    "title": "Complementary algorithms for graphs and percolation", 
    "arxiv-id": "0708.0600v1", 
    "author": "Michael J. Lee", 
    "publish": "2007-08-04T02:56:13Z", 
    "summary": "A pair of complementary algorithms are presented. One of the pair is a fast\nmethod for connecting graphs with an edge. The other is a fast method for\nremoving edges from a graph. Both algorithms employ the same tree based graph\nrepresentation and so, in concert, can arbitrarily modify any graph. Since the\nclusters of a percolation model may be described as simple connected graphs, an\nefficient Monte Carlo scheme can be constructed that uses the algorithms to\nsweep the occupation probability back and forth between two turning points.\nThis approach concentrates computational sampling time within a region of\ninterest. A high precision value of pc = 0.59274603(9) was thus obtained, by\nMersenne twister, for the two dimensional square site percolation threshold."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.2936v1", 
    "other_authors": "David S. Planeta", 
    "title": "Priority Queue Based on Multilevel Prefix Tree", 
    "arxiv-id": "0708.2936v1", 
    "author": "David S. Planeta", 
    "publish": "2007-08-21T22:59:49Z", 
    "summary": "Tree structures are very often used data structures. Among ordered types of\ntrees there are many variants whose basic operations such as insert, delete,\nsearch, delete-min are characterized by logarithmic time complexity. In the\narticle I am going to present the structure whose time complexity for each of\nthe above operations is $O(\\frac{M}{K} + K)$, where M is the size of data type\nand K is constant properly matching the size of data type. Properly matched K\nwill make the structure function as a very effective Priority Queue. The\nstructure size linearly depends on the number and size of elements. PTrie is a\nclever combination of the idea of prefix tree -- Trie, structure of logarithmic\ntime complexity for insert and remove operations, doubly linked list and\nqueues."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.3408v1", 
    "other_authors": "David S. Planeta", 
    "title": "Linear Time Algorithms Based on Multilevel Prefix Tree for Finding   Shortest Path with Positive Weights and Minimum Spanning Tree in a Networks", 
    "arxiv-id": "0708.3408v1", 
    "author": "David S. Planeta", 
    "publish": "2007-08-24T21:58:29Z", 
    "summary": "In this paper I present general outlook on questions relevant to the basic\ngraph algorithms; Finding the Shortest Path with Positive Weights and Minimum\nSpanning Tree. I will show so far known solution set of basic graph problems\nand present my own. My solutions to graph problems are characterized by their\nlinear worst-case time complexity. It should be noticed that the algorithms\nwhich compute the Shortest Path and Minimum Spanning Tree problems not only\nanalyze the weight of arcs (which is the main and often the only criterion of\nsolution hitherto known algorithms) but also in case of identical path weights\nthey select this path which walks through as few vertices as possible. I have\npresented algorithms which use priority queue based on multilevel prefix tree\n-- PTrie. PTrie is a clever combination of the idea of prefix tree -- Trie, the\nstructure of logarithmic time complexity for insert and remove operations,\ndoubly linked list and queues. In C++ I will implement linear worst-case time\nalgorithm computing the Single-Destination Shortest-Paths problem and I will\nexplain its usage."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.3696v1", 
    "other_authors": "Petros Drineas, Michael W. Mahoney, S. Muthukrishnan", 
    "title": "Relative-Error CUR Matrix Decompositions", 
    "arxiv-id": "0708.3696v1", 
    "author": "S. Muthukrishnan", 
    "publish": "2007-08-27T23:34:50Z", 
    "summary": "Many data analysis applications deal with large matrices and involve\napproximating the matrix using a small number of ``components.'' Typically,\nthese components are linear combinations of the rows and columns of the matrix,\nand are thus difficult to interpret in terms of the original features of the\ninput data. In this paper, we propose and study matrix approximations that are\nexplicitly expressed in terms of a small number of columns and/or rows of the\ndata matrix, and thereby more amenable to interpretation in terms of the\noriginal data. Our main algorithmic results are two randomized algorithms which\ntake as input an $m \\times n$ matrix $A$ and a rank parameter $k$. In our first\nalgorithm, $C$ is chosen, and we let $A'=CC^+A$, where $C^+$ is the\nMoore-Penrose generalized inverse of $C$. In our second algorithm $C$, $U$, $R$\nare chosen, and we let $A'=CUR$. ($C$ and $R$ are matrices that consist of\nactual columns and rows, respectively, of $A$, and $U$ is a generalized inverse\nof their intersection.) For each algorithm, we show that with probability at\nleast $1-\\delta$: $$ ||A-A'||_F \\leq (1+\\epsilon) ||A-A_k||_F, $$ where $A_k$\nis the ``best'' rank-$k$ approximation provided by truncating the singular\nvalue decomposition (SVD) of $A$. The number of columns of $C$ and rows of $R$\nis a low-degree polynomial in $k$, $1/\\epsilon$, and $\\log(1/\\delta)$. Our two\nalgorithms are the first polynomial time algorithms for such low-rank matrix\napproximations that come with relative-error guarantees; previously, in some\ncases, it was not even known whether such matrix decompositions exist. Both of\nour algorithms are simple, they take time of the order needed to approximately\ncompute the top $k$ singular vectors of $A$, and they use a novel, intuitive\nsampling method called ``subspace sampling.''"
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0708.4288v1", 
    "other_authors": "Philip Bille", 
    "title": "Pattern Matching in Trees and Strings", 
    "arxiv-id": "0708.4288v1", 
    "author": "Philip Bille", 
    "publish": "2007-08-31T08:07:32Z", 
    "summary": "We study the design of efficient algorithms for combinatorial pattern\nmatching. More concretely, we study algorithms for tree matching, string\nmatching, and string matching in compressed texts."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0709.0624v1", 
    "other_authors": "Katharina L\u00fcrwer-Br\u00fcggemeier, Martin Ziegler", 
    "title": "On Faster Integer Calculations using Non-Arithmetic Primitives", 
    "arxiv-id": "0709.0624v1", 
    "author": "Martin Ziegler", 
    "publish": "2007-09-05T11:34:54Z", 
    "summary": "The unit cost model is both convenient and largely realistic for describing\ninteger decision algorithms over (+,*). Additional operations like division\nwith remainder or bitwise conjunction, although equally supported by computing\nhardware, may lead to a considerable drop in complexity. We show a variety of\nconcrete problems to benefit from such NON-arithmetic primitives by presenting\nand analyzing corresponding fast algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.0083v1", 
    "other_authors": "Stanislav Angelov, Keshav Kunal, Andrew McGregor", 
    "title": "Sorting and Selection with Random Costs", 
    "arxiv-id": "0710.0083v1", 
    "author": "Andrew McGregor", 
    "publish": "2007-09-29T18:10:28Z", 
    "summary": "There is a growing body of work on sorting and selection in models other than\nthe unit-cost comparison model. This work is the first treatment of a natural\nstochastic variant of the problem where the cost of comparing two elements is a\nrandom variable. Each cost is chosen independently and is known to the\nalgorithm. In particular we consider the following three models: each cost is\nchosen uniformly in the range $[0,1]$, each cost is 0 with some probability $p$\nand 1 otherwise, or each cost is 1 with probability $p$ and infinite otherwise.\nWe present lower and upper bounds (optimal in most cases) for these problems.\nWe obtain our upper bounds by carefully designing algorithms to ensure that the\ncosts incurred at various stages are independent and using properties of random\npartial orders when appropriate."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.1435v4", 
    "other_authors": "Petros Drineas, Michael W. Mahoney, S. Muthukrishnan, Tamas Sarlos", 
    "title": "Faster Least Squares Approximation", 
    "arxiv-id": "0710.1435v4", 
    "author": "Tamas Sarlos", 
    "publish": "2007-10-07T17:37:37Z", 
    "summary": "Least squares approximation is a technique to find an approximate solution to\na system of linear equations that has no exact solution. In a typical setting,\none lets $n$ be the number of constraints and $d$ be the number of variables,\nwith $n \\gg d$. Then, existing exact methods find a solution vector in\n$O(nd^2)$ time. We present two randomized algorithms that provide very accurate\nrelative-error approximations to the optimal value and the solution vector of a\nleast squares approximation problem more rapidly than existing exact\nalgorithms. Both of our algorithms preprocess the data with the Randomized\nHadamard Transform. One then uniformly randomly samples constraints and solves\nthe smaller problem on those constraints, and the other performs a sparse\nrandom projection and solves the smaller problem on those projected\ncoordinates. In both cases, solving the smaller problem provides relative-error\napproximations, and, if $n$ is sufficiently larger than $d$, the approximate\nsolution can be computed in $O(nd \\log d)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.2532v1", 
    "other_authors": "Valerie King, Cynthia Phillips, Jared Saia, Maxwell Young", 
    "title": "Sleeping on the Job: Energy-Efficient Broadcast for Radio Networks", 
    "arxiv-id": "0710.2532v1", 
    "author": "Maxwell Young", 
    "publish": "2007-10-12T19:56:45Z", 
    "summary": "We address the problem of minimizing power consumption when performing\nreliable broadcast on a radio network under the following popular model. Each\nnode in the network is located on a point in a two dimensional grid, and\nwhenever a node sends a message, all awake nodes within distance r receive the\nmessage. In the broadcast problem, some node wants to successfully send a\nmessage to all other nodes in the network even when up to a 1/2 fraction of the\nnodes within every neighborhood can be deleted by an adversary. The set of\ndeleted nodes is carefully chosen by the adversary to foil our algorithm and\nmoreover, the set of deleted nodes may change periodically. This models\nworst-case behavior due to mobile nodes, static nodes losing power or simply\nsome points in the grid being unoccupied. A trivial solution requires each node\nin the network to be awake roughly 1/2 the time, and a trivial lower bound\nshows that each node must be awake for at least a 1/n fraction of the time. Our\nfirst result is an algorithm that requires each node to be awake for only a\n1/sqrt(n) fraction of the time in expectation. Our algorithm achieves this\nwhile ensuring correctness with probability 1, and keeping optimal values for\nother resource costs such as latency and number of messages sent. We give a\nlower-bound that shows that this reduction in power consumption is\nasymptotically optimal when latency and number of messages sent must be\noptimal. If we can increase the latency and messages sent by only a log*n\nfactor we give a Las Vegas algorithm that requires each node to be awake for\nonly a (log*n)/n expected fraction of the time; we give a lower-bound showing\nthat this second algorithm is near optimal. Finally, we show how to ensure\nenergy-efficient broadcast in the presence of Byzantine faults."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0710.4410v1", 
    "other_authors": "Richard Brent, Paul Zimmermann", 
    "title": "A Multi-level Blocking Distinct Degree Factorization Algorithm", 
    "arxiv-id": "0710.4410v1", 
    "author": "Paul Zimmermann", 
    "publish": "2007-10-24T09:18:33Z", 
    "summary": "We give a new algorithm for performing the distinct-degree factorization of a\npolynomial P(x) over GF(2), using a multi-level blocking strategy. The coarsest\nlevel of blocking replaces GCD computations by multiplications, as suggested by\nPollard (1975), von zur Gathen and Shoup (1992), and others. The novelty of our\napproach is that a finer level of blocking replaces multiplications by\nsquarings, which speeds up the computation in GF(2)[x]/P(x) of certain interval\npolynomials when P(x) is sparse. As an application we give a fast algorithm to\nsearch for all irreducible trinomials x^r + x^s + 1 of degree r over GF(2),\nwhile producing a certificate that can be checked in less time than the full\nsearch. Naive algorithms cost O(r^2) per trinomial, thus O(r^3) to search over\nall trinomials of given degree r. Under a plausible assumption about the\ndistribution of factors of trinomials, the new algorithm has complexity O(r^2\n(log r)^{3/2}(log log r)^{1/2}) for the search over all trinomials of degree r.\nOur implementation achieves a speedup of greater than a factor of 560 over the\nnaive algorithm in the case r = 24036583 (a Mersenne exponent). Using our\nprogram, we have found two new primitive trinomials of degree 24036583 over\nGF(2) (the previous record degree was 6972593)."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.0251v1", 
    "other_authors": "Telikepalli Kavitha, Rogers Mathew", 
    "title": "Faster Algorithms for Online Topological Ordering", 
    "arxiv-id": "0711.0251v1", 
    "author": "Rogers Mathew", 
    "publish": "2007-11-02T06:42:43Z", 
    "summary": "We present two algorithms for maintaining the topological order of a directed\nacyclic graph with n vertices, under an online edge insertion sequence of m\nedges. Efficient algorithms for online topological ordering have many\napplications, including online cycle detection, which is to discover the first\nedge that introduces a cycle under an arbitrary sequence of edge insertions in\na directed graph. In this paper we present efficient algorithms for the online\ntopological ordering problem.\n  We first present a simple algorithm with running time O(n^{5/2}) for the\nonline topological ordering problem. This is the current fastest algorithm for\nthis problem on dense graphs, i.e., when m > n^{5/3}. We then present an\nalgorithm with running time O((m + nlog n)\\sqrt{m}); this is more efficient for\nsparse graphs. Our results yield an improved upper bound of O(min(n^{5/2}, (m +\nnlog n)sqrt{m})) for the online topological ordering problem."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.1682v1", 
    "other_authors": "Loukas Georgiadis, Haim Kaplan, Nira Shafrir, Robert E. Tarjan, Renato F. Werneck", 
    "title": "Data Structures for Mergeable Trees", 
    "arxiv-id": "0711.1682v1", 
    "author": "Renato F. Werneck", 
    "publish": "2007-11-11T21:28:20Z", 
    "summary": "Motivated by an application in computational topology, we consider a novel\nvariant of the problem of efficiently maintaining dynamic rooted trees. This\nvariant requires merging two paths in a single operation. In contrast to the\nstandard problem, in which only one tree arc changes at a time, a single merge\noperation can change many arcs. In spite of this, we develop a data structure\nthat supports merges on an n-node forest in O(log^2 n) amortized time and all\nother standard tree operations in O(log n) time (amortized, worst-case, or\nrandomized depending on the underlying data structure). For the special case\nthat occurs in the motivating application, in which arbitrary arc deletions\n(cuts) are not allowed, we give a data structure with an O(log n) time bound\nper operation. This is asymptotically optimal under certain assumptions. For\nthe even-more special case in which both cuts and parent queries are\ndisallowed, we give an alternative O(log n)-time solution that uses standard\ndynamic trees as a black box. This solution also applies to the motivating\napplication. Our methods use previous work on dynamic trees in various ways,\nbut the analysis of each algorithm requires novel ideas. We also investigate\nlower bounds for the problem under various assumptions."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2157v3", 
    "other_authors": "Bodo Manthey", 
    "title": "On Approximating Multi-Criteria TSP", 
    "arxiv-id": "0711.2157v3", 
    "author": "Bodo Manthey", 
    "publish": "2007-11-14T10:53:49Z", 
    "summary": "We present approximation algorithms for almost all variants of the\nmulti-criteria traveling salesman problem (TSP).\n  First, we devise randomized approximation algorithms for multi-criteria\nmaximum traveling salesman problems (Max-TSP). For multi-criteria Max-STSP,\nwhere the edge weights have to be symmetric, we devise an algorithm with an\napproximation ratio of 2/3 - eps. For multi-criteria Max-ATSP, where the edge\nweights may be asymmetric, we present an algorithm with a ratio of 1/2 - eps.\nOur algorithms work for any fixed number k of objectives. Furthermore, we\npresent a deterministic algorithm for bi-criteria Max-STSP that achieves an\napproximation ratio of 7/27.\n  Finally, we present a randomized approximation algorithm for the asymmetric\nmulti-criteria minimum TSP with triangle inequality Min-ATSP. This algorithm\nachieves a ratio of log n + eps."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2399v3", 
    "other_authors": "Vladimir Deineko, Alexander Tiskin", 
    "title": "Minimum-weight double-tree shortcutting for Metric TSP: Bounding the   approximation ratio", 
    "arxiv-id": "0711.2399v3", 
    "author": "Alexander Tiskin", 
    "publish": "2007-11-15T13:19:01Z", 
    "summary": "The Metric Traveling Salesman Problem (TSP) is a classical NP-hard\noptimization problem. The double-tree shortcutting method for Metric TSP yields\nan exponentially-sized space of TSP tours, each of which approximates the\noptimal solution within at most a factor of 2. We consider the problem of\nfinding among these tours the one that gives the closest approximation, i.e.\\\nthe \\emph{minimum-weight double-tree shortcutting}. Previously, we gave an\nefficient algorithm for this problem, and carried out its experimental\nanalysis. In this paper, we address the related question of the worst-case\napproximation ratio for the minimum-weight double-tree shortcutting method. In\nparticular, we give lower bounds on the approximation ratio in some specific\nmetric spaces: the ratio of 2 in the discrete shortest path metric, 1.622 in\nthe planar Euclidean metric, and 1.666 in the planar Minkowski metric. The\nfirst of these lower bounds is tight; we conjecture that the other two bounds\nare also tight, and in particular that the minimum-weight double-tree method\nprovides a 1.622-approximation for planar Euclidean TSP."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.2710v2", 
    "other_authors": "Bernhard Haeupler, Robert E. Tarjan", 
    "title": "Finding a Feasible Flow in a Strongly Connected Network", 
    "arxiv-id": "0711.2710v2", 
    "author": "Robert E. Tarjan", 
    "publish": "2007-11-17T01:59:53Z", 
    "summary": "We consider the problem of finding a feasible single-commodity flow in a\nstrongly connected network with fixed supplies and demands, provided that the\nsum of supplies equals the sum of demands and the minimum arc capacity is at\nleast this sum. A fast algorithm for this problem improves the worst-case time\nbound of the Goldberg-Rao maximum flow method by a constant factor. Erlebach\nand Hagerup gave an linear-time feasible flow algorithm. We give an arguably\nsimpler one."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.3250v1", 
    "other_authors": "Venkata Seshu Kumar Kurapati", 
    "title": "Improved Fully Dynamic Reachability Algorithm for Directed Graph", 
    "arxiv-id": "0711.3250v1", 
    "author": "Venkata Seshu Kumar Kurapati", 
    "publish": "2007-11-21T03:22:12Z", 
    "summary": "We propose a fully dynamic algorithm for maintaining reachability information\nin directed graphs. The proposed deterministic dynamic algorithm has an update\ntime of $O((ins*n^{2}) + (del * (m+n*log(n))))$ where $m$ is the current number\nof edges, $n$ is the number of vertices in the graph, $ins$ is the number of\nedge insertions and $del$ is the number of edge deletions. Each query can be\nanswered in O(1) time after each update. The proposed algorithm combines\nexisting fully dynamic reachability algorithm with well known witness counting\ntechnique to improve efficiency of maintaining reachability information when\nedges are deleted. The proposed algorithm improves by a factor of\n$O(\\frac{n^2}{m+n*log(n)})$ for edge deletion over the best existing fully\ndynamic algorithm for maintaining reachability information."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.3861v5", 
    "other_authors": "Sudipto Guha, Kamesh Munagala, Peng Shi", 
    "title": "Approximation Algorithms for Restless Bandit Problems", 
    "arxiv-id": "0711.3861v5", 
    "author": "Peng Shi", 
    "publish": "2007-11-25T18:01:35Z", 
    "summary": "The restless bandit problem is one of the most well-studied generalizations\nof the celebrated stochastic multi-armed bandit problem in decision theory. In\nits ultimate generality, the restless bandit problem is known to be PSPACE-Hard\nto approximate to any non-trivial factor, and little progress has been made\ndespite its importance in modeling activity allocation under uncertainty.\n  We consider a special case that we call Feedback MAB, where the reward\nobtained by playing each of n independent arms varies according to an\nunderlying on/off Markov process whose exact state is only revealed when the\narm is played. The goal is to design a policy for playing the arms in order to\nmaximize the infinite horizon time average expected reward. This problem is\nalso an instance of a Partially Observable Markov Decision Process (POMDP), and\nis widely studied in wireless scheduling and unmanned aerial vehicle (UAV)\nrouting. Unlike the stochastic MAB problem, the Feedback MAB problem does not\nadmit to greedy index-based optimal policies.\n  We develop a novel and general duality-based algorithmic technique that\nyields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy\nto this problem. We then define a general sub-class of restless bandit problems\nthat we term Monotone bandits, for which our policy is a 2-approximation. Our\ntechnique is robust enough to handle generalizations of these problems to\nincorporate various side-constraints such as blocking plays and switching\ncosts. This technique is also of independent interest for other restless bandit\nproblems. By presenting the first (and efficient) O(1) approximations for\nnon-trivial instances of restless bandits as well as of POMDPs, our work\ninitiates the study of approximation algorithms in both these contexts."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.4573v1", 
    "other_authors": "Pierre Charbit, Michel Habib, Vincent Limouzy, Fabien De Montgolfier, Mathieu Raffinot, Micha\u00ebl Rao", 
    "title": "A Note On Computing Set Overlap Classes", 
    "arxiv-id": "0711.4573v1", 
    "author": "Micha\u00ebl Rao", 
    "publish": "2007-11-28T20:07:46Z", 
    "summary": "Let ${\\cal V}$ be a finite set of $n$ elements and ${\\cal F}=\\{X_1,X_2, >...,\nX_m\\}$ a family of $m$ subsets of ${\\cal V}.$ Two sets $X_i$ and $X_j$ of\n${\\cal F}$ overlap if $X_i \\cap X_j \\neq \\emptyset,$ $X_j \\setminus X_i \\neq\n\\emptyset,$ and $X_i \\setminus X_j \\neq \\emptyset.$ Two sets $X,Y\\in {\\cal F}$\nare in the same overlap class if there is a series $X=X_1,X_2, ..., X_k=Y$ of\nsets of ${\\cal F}$ in which each $X_iX_{i+1}$ overlaps. In this note, we focus\non efficiently identifying all overlap classes in $O(n+\\sum_{i=1}^m |X_i|)$\ntime. We thus revisit the clever algorithm of Dahlhaus of which we give a clear\npresentation and that we simplify to make it practical and implementable in its\nreal worst case complexity. An useful variant of Dahlhaus's approach is also\nexplained."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevE.76.027702", 
    "link": "http://arxiv.org/pdf/0711.4825v1", 
    "other_authors": "Chandra Chekuri, Nitish Korula", 
    "title": "Approximation Algorithms for Orienteering with Time Windows", 
    "arxiv-id": "0711.4825v1", 
    "author": "Nitish Korula", 
    "publish": "2007-11-29T21:10:48Z", 
    "summary": "Orienteering is the following optimization problem: given an edge-weighted\ngraph (directed or undirected), two nodes s,t and a time limit T, find an s-t\nwalk of total length at most T that maximizes the number of distinct nodes\nvisited by the walk. One obtains a generalization, namely orienteering with\ntime-windows (also referred to as TSP with time-windows), if each node v has a\nspecified time-window [R(v), D(v)] and a node v is counted as visited by the\nwalk only if v is visited during its time-window. For the time-window problem,\nan O(\\log \\opt) approximation can be achieved even for directed graphs if the\nalgorithm is allowed quasi-polynomial time. However, the best known polynomial\ntime approximation ratios are O(\\log^2 \\opt) for undirected graphs and O(\\log^4\n\\opt) in directed graphs. In this paper we make some progress towards closing\nthis discrepancy, and in the process obtain improved approximation ratios in\nseveral natural settings. Let L(v) = D(v) - R(v) denote the length of the\ntime-window for v and let \\lmax = \\max_v L(v) and \\lmin = \\min_v L(v). Our\nresults are given below with \\alpha denoting the known approximation ratio for\norienteering (without time-windows). Currently \\alpha = (2+\\eps) for undirected\ngraphs and \\alpha = O(\\log^2 \\opt) in directed graphs.\n  1. An O(\\alpha \\log \\lmax) approximation when R(v) and D(v) are integer\nvalued for each v.\n  2. An O(\\alpha \\max{\\log \\opt, \\log \\frac{\\lmax}{\\lmin}}) approximation.\n  3. An O(\\alpha \\log \\frac{\\lmax}{\\lmin}) approximation when no start and end\npoints are specified.\n  In particular, if \\frac{\\lmax}{\\lmin} is poly-bounded, we obtain an O(\\log n)\napproximation for the time-window problem in undirected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.2629v2", 
    "other_authors": "Ryoso Hamane, Toshiya Itoh, Kouhei Tomita", 
    "title": "Approximation Algorithms for the Highway Problem under the Coupon Model", 
    "arxiv-id": "0712.2629v2", 
    "author": "Kouhei Tomita", 
    "publish": "2007-12-17T04:47:38Z", 
    "summary": "When a store sells items to customers, the store wishes to determine the\nprices of the items to maximize its profit. Intuitively, if the store sells the\nitems with low (resp. high) prices, the customers buy more (resp. less) items,\nwhich provides less profit to the store. So it would be hard for the store to\ndecide the prices of items. Assume that the store has a set V of n items and\nthere is a set E of m customers who wish to buy those items, and also assume\nthat each item i \\in V has the production cost d_i and each customer e_j \\in E\nhas the valuation v_j on the bundle e_j \\subseteq V of items. When the store\nsells an item i \\in V at the price r_i, the profit for the item i is\np_i=r_i-d_i. The goal of the store is to decide the price of each item to\nmaximize its total profit. In most of the previous works, the item pricing\nproblem was considered under the assumption that p_i \\geq 0 for each i \\in V,\nhowever, Balcan, et al. [In Proc. of WINE, LNCS 4858, 2007] introduced the\nnotion of loss-leader, and showed that the seller can get more total profit in\nthe case that p_i < 0 is allowed than in the case that p_i < 0 is not allowed.\nIn this paper, we consider the line and the cycle highway problem, and show\napproximation algorithms for the line and/or cycle highway problem for which\nthe smallest valuation is s and the largest valuation is \\ell or all valuations\nare identical."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3360v1", 
    "other_authors": "Paolo Ferragina, Rodrigo Gonzalez, Gonzalo Navarro, Rossano Venturini", 
    "title": "Compressed Text Indexes:From Theory to Practice!", 
    "arxiv-id": "0712.3360v1", 
    "author": "Rossano Venturini", 
    "publish": "2007-12-20T10:42:54Z", 
    "summary": "A compressed full-text self-index represents a text in a compressed form and\nstill answers queries efficiently. This technology represents a breakthrough\nover the text indexing techniques of the previous decade, whose indexes\nrequired several times the size of the text. Although it is relatively new,\nthis technology has matured up to a point where theoretical research is giving\nway to practical developments. Nonetheless this requires significant\nprogramming skills, a deep engineering effort, and a strong algorithmic\nbackground to dig into the research results. To date only isolated\nimplementations and focused comparisons of compressed indexes have been\nreported, and they missed a common API, which prevented their re-use or\ndeployment within other applications.\n  The goal of this paper is to fill this gap. First, we present the existing\nimplementations of compressed indexes from a practitioner's point of view.\nSecond, we introduce the Pizza&Chili site, which offers tuned implementations\nand a standardized API for the most successful compressed full-text\nself-indexes, together with effective testbeds and scripts for their automatic\nvalidation and test. Third, we show the results of our extensive experiments on\nthese codes with the aim of demonstrating the practical relevance of this novel\nand exciting technology."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3568v1", 
    "other_authors": "Jochen Konemann, David Pritchard, Kunlun Tan", 
    "title": "A Partition-Based Relaxation For Steiner Trees", 
    "arxiv-id": "0712.3568v1", 
    "author": "Kunlun Tan", 
    "publish": "2007-12-20T21:06:35Z", 
    "summary": "The Steiner tree problem is a classical NP-hard optimization problem with a\nwide range of practical applications. In an instance of this problem, we are\ngiven an undirected graph G=(V,E), a set of terminals R, and non-negative costs\nc_e for all edges e in E. Any tree that contains all terminals is called a\nSteiner tree; the goal is to find a minimum-cost Steiner tree. The nodes V R\nare called Steiner nodes.\n  The best approximation algorithm known for the Steiner tree problem is due to\nRobins and Zelikovsky (SIAM J. Discrete Math, 2005); their greedy algorithm\nachieves a performance guarantee of 1+(ln 3)/2 ~ 1.55. The best known linear\n(LP)-based algorithm, on the other hand, is due to Goemans and Bertsimas (Math.\nProgramming, 1993) and achieves an approximation ratio of 2-2/|R|. In this\npaper we establish a link between greedy and LP-based approaches by showing\nthat Robins and Zelikovsky's algorithm has a natural primal-dual interpretation\nwith respect to a novel partition-based linear programming relaxation. We also\nexhibit surprising connections between the new formulation and existing LPs and\nwe show that the new LP is stronger than the bidirected cut formulation.\n  An instance is b-quasi-bipartite if each connected component of G R has at\nmost b vertices. We show that Robins' and Zelikovsky's algorithm has an\napproximation ratio better than 1+(ln 3)/2 for such instances, and we prove\nthat the integrality gap of our LP is between 8/7 and (2b+1)/(b+1)."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3858v1", 
    "other_authors": "Abraham P. Punnen, Ruonan Zhang", 
    "title": "Bottleneck flows in networks", 
    "arxiv-id": "0712.3858v1", 
    "author": "Ruonan Zhang", 
    "publish": "2007-12-22T13:49:45Z", 
    "summary": "The bottleneck network flow problem (BNFP) is a generalization of several\nwell-studied bottleneck problems such as the bottleneck transportation problem\n(BTP), bottleneck assignment problem (BAP), bottleneck path problem (BPP), and\nso on. In this paper we provide a review of important results on this topic and\nits various special cases. We observe that the BNFP can be solved as a sequence\nof $O(\\log n)$ maximum flow problems. However, special augmenting path based\nalgorithms for the maximum flow problem can be modified to obtain algorithms\nfor the BNFP with the property that these variations and the corresponding\nmaximum flow algorithms have identical worst case time complexity. On unit\ncapacity network we show that BNFP can be solved in $O(\\min \\{{m(n\\log\nn)}^{{2/3}}, m^{{3/2}}\\sqrt{\\log n}\\})$. This improves the best available\nalgorithm by a factor of $\\sqrt{\\log n}$. On unit capacity simple graphs, we\nshow that BNFP can be solved in $O(m \\sqrt {n \\log n})$ time. As a consequence\nwe have an $O(m \\sqrt {n \\log n})$ algorithm for the BTP with unit arc\ncapacities."
},{
    "category": "cs.DS", 
    "doi": "10.1587/transfun.E92.A.1779", 
    "link": "http://arxiv.org/pdf/0712.3876v5", 
    "other_authors": "Ely Porat, Amir Rothschild", 
    "title": "Explicit Non-Adaptive Combinatorial Group Testing Schemes", 
    "arxiv-id": "0712.3876v5", 
    "author": "Amir Rothschild", 
    "publish": "2007-12-22T21:04:34Z", 
    "summary": "Group testing is a long studied problem in combinatorics: A small set of $r$\nill people should be identified out of the whole ($n$ people) by using only\nqueries (tests) of the form \"Does set X contain an ill human?\". In this paper\nwe provide an explicit construction of a testing scheme which is better\n(smaller) than any known explicit construction. This scheme has $\\bigT{\\min[r^2\n\\ln n,n]}$ tests which is as many as the best non-explicit schemes have. In our\nconstruction we use a fact that may have a value by its own right: Linear\nerror-correction codes with parameters $[m,k,\\delta m]_q$ meeting the\nGilbert-Varshamov bound may be constructed quite efficiently, in $\\bigT{q^km}$\ntime."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0801.1987v2", 
    "other_authors": "Christos Koufogiannakis, Neal E. Young", 
    "title": "A Nearly Linear-Time PTAS for Explicit Fractional Packing and Covering   Linear Programs", 
    "arxiv-id": "0801.1987v2", 
    "author": "Neal E. Young", 
    "publish": "2008-01-13T22:04:49Z", 
    "summary": "We give an approximation algorithm for packing and covering linear programs\n(linear programs with non-negative coefficients). Given a constraint matrix\nwith n non-zeros, r rows, and c columns, the algorithm computes feasible primal\nand dual solutions whose costs are within a factor of 1+eps of the optimal cost\nin time O((r+c)log(n)/eps^2 + n)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0801.4238v1", 
    "other_authors": "Marek Chrobak, Christoph Durr, Mathilde Hurand, Julien Robert", 
    "title": "Algorithms for Temperature-Aware Task Scheduling in Microprocessor   Systems", 
    "arxiv-id": "0801.4238v1", 
    "author": "Julien Robert", 
    "publish": "2008-01-28T10:47:42Z", 
    "summary": "We study scheduling problems motivated by recently developed techniques for\nmicroprocessor thermal management at the operating systems level. The general\nscenario can be described as follows. The microprocessor's temperature is\ncontrolled by the hardware thermal management system that continuously monitors\nthe chip temperature and automatically reduces the processor's speed as soon as\nthe thermal threshold is exceeded. Some tasks are more CPU-intensive than other\nand thus generate more heat during execution. The cooling system operates\nnon-stop, reducing (at an exponential rate) the deviation of the processor's\ntemperature from the ambient temperature. As a result, the processor's\ntemperature, and thus the performance as well, depends on the order of the task\nexecution. Given a variety of possible underlying architectures, models for\ncooling and for hardware thermal management, as well as types of tasks, this\nscenario gives rise to a plethora of interesting and never studied scheduling\nproblems.\n  We focus on scheduling real-time jobs in a simplified model for cooling and\nthermal management. A collection of unit-length jobs is given, each job\nspecified by its release time, deadline and heat contribution. If, at some time\nstep, the temperature of the system is t and the processor executes a job with\nheat contribution h, then the temperature at the next step is (t+h)/2. The\ntemperature cannot exceed the given thermal threshold T. The objective is to\nmaximize the throughput, that is, the number of tasks that meet their\ndeadlines. We prove that, in the offline case, computing the optimum schedule\nis NP-hard, even if all jobs are released at the same time. In the online case,\nwe show a 2-competitive deterministic algorithm and a matching lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.0017v1", 
    "other_authors": "Amihood Amir, Klim Efremenko, Oren Kapah, Ely Porat, Amir Rothschild", 
    "title": "Improved Deterministic Length Reduction", 
    "arxiv-id": "0802.0017v1", 
    "author": "Amir Rothschild", 
    "publish": "2008-01-31T21:59:33Z", 
    "summary": "This paper presents a new technique for deterministic length reduction. This\ntechnique improves the running time of the algorithm presented in \\cite{LR07}\nfor performing fast convolution in sparse data. While the regular fast\nconvolution of vectors $V_1,V_2$ whose sizes are $N_1,N_2$ respectively, takes\n$O(N_1 \\log N_2)$ using FFT, using the new technique for length reduction, the\nalgorithm proposed in \\cite{LR07} performs the convolution in $O(n_1 \\log^3\nn_1)$, where $n_1$ is the number of non-zero values in $V_1$. The algorithm\nassumes that $V_1$ is given in advance, and $V_2$ is given in running time. The\nnovel technique presented in this paper improves the convolution time to $O(n_1\n\\log^2 n_1)$ {\\sl deterministically}, which equals the best running time given\nachieved by a {\\sl randomized} algorithm.\n  The preprocessing time of the new technique remains the same as the\npreprocessing time of \\cite{LR07}, which is $O(n_1^2)$. This assumes and deals\nthe case where $N_1$ is polynomial in $n_1$. In the case where $N_1$ is\nexponential in $n_1$, a reduction to a polynomial case can be used. In this\npaper we also improve the preprocessing time of this reduction from $O(n_1^4)$\nto $O(n_1^3{\\rm polylog}(n_1))$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1059v1", 
    "other_authors": "Deepak Ajwani, Tobias Friedrich", 
    "title": "Average-Case Analysis of Online Topological Ordering", 
    "arxiv-id": "0802.1059v1", 
    "author": "Tobias Friedrich", 
    "publish": "2008-02-07T20:27:17Z", 
    "summary": "Many applications like pointer analysis and incremental compilation require\nmaintaining a topological ordering of the nodes of a directed acyclic graph\n(DAG) under dynamic updates. All known algorithms for this problem are either\nonly analyzed for worst-case insertion sequences or only evaluated\nexperimentally on random DAGs. We present the first average-case analysis of\nonline topological ordering algorithms. We prove an expected runtime of O(n^2\npolylog(n)) under insertion of the edges of a complete DAG in a random order\nfor the algorithms of Alpern et al. (SODA, 1990), Katriel and Bodlaender (TALG,\n2006), and Pearce and Kelly (JEA, 2006). This is much less than the best known\nworst-case bound O(n^{2.75}) for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1427v1", 
    "other_authors": "Klim Efremenko, Ely Porat", 
    "title": "Approximating General Metric Distances Between a Pattern and a Text", 
    "arxiv-id": "0802.1427v1", 
    "author": "Ely Porat", 
    "publish": "2008-02-11T12:36:31Z", 
    "summary": "Let $T=t_0 ... t_{n-1}$ be a text and $P = p_0 ... p_{m-1}$ a pattern taken\nfrom some finite alphabet set $\\Sigma$, and let $\\dist$ be a metric on\n$\\Sigma$. We consider the problem of calculating the sum of distances between\nthe symbols of $P$ and the symbols of substrings of $T$ of length $m$ for all\npossible offsets. We present an $\\epsilon$-approximation algorithm for this\nproblem which runs in time $O(\\frac{1}{\\epsilon^2}n\\cdot\n\\mathrm{polylog}(n,\\abs{\\Sigma}))$"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1471v2", 
    "other_authors": "Ronald de Wolf", 
    "title": "Error-Correcting Data Structures", 
    "arxiv-id": "0802.1471v2", 
    "author": "Ronald de Wolf", 
    "publish": "2008-02-11T16:35:49Z", 
    "summary": "We study data structures in the presence of adversarial noise. We want to\nencode a given object in a succinct data structure that enables us to\nefficiently answer specific queries about the object, even if the data\nstructure has been corrupted by a constant fraction of errors. This new model\nis the common generalization of (static) data structures and locally decodable\nerror-correcting codes. The main issue is the tradeoff between the space used\nby the data structure and the time (number of probes) needed to answer a query\nabout the encoded object. We prove a number of upper and lower bounds on\nvarious natural error-correcting data structure problems. In particular, we\nshow that the optimal length of error-correcting data structures for the\nMembership problem (where we want to store subsets of size s from a universe of\nsize n) is closely related to the optimal length of locally decodable codes for\ns-bit strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1685v2", 
    "other_authors": "Marcin Bienkowski, Marek Chrobak, Christoph Durr, Mathilde Hurand, Artur Jez, Lukasz Jez, Jakub Lopuszanski, Grzegorz Stachowiak", 
    "title": "Generalized Whac-a-Mole", 
    "arxiv-id": "0802.1685v2", 
    "author": "Grzegorz Stachowiak", 
    "publish": "2008-02-12T18:41:46Z", 
    "summary": "We consider online competitive algorithms for the problem of collecting\nweighted items from a dynamic set S, when items are added to or deleted from S\nover time. The objective is to maximize the total weight of collected items. We\nstudy the general version, as well as variants with various restrictions,\nincluding the following: the uniform case, when all items have the same weight,\nthe decremental sets, when all items are present at the beginning and only\ndeletion operations are allowed, and dynamic queues, where the dynamic set is\nordered and only its prefixes can be deleted (with no restriction on\ninsertions). The dynamic queue case is a generalization of bounded-delay packet\nscheduling (also referred to as buffer management). We present several upper\nand lower bounds on the competitive ratio for these variants."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.1722v1", 
    "other_authors": "Omid Amini, Fedor V. Fomin, Saket Saurabh", 
    "title": "Parameterized Algorithms for Partial Cover Problems", 
    "arxiv-id": "0802.1722v1", 
    "author": "Saket Saurabh", 
    "publish": "2008-02-12T21:19:40Z", 
    "summary": "Covering problems are fundamental classical problems in optimization,\ncomputer science and complexity theory. Typically an input to these problems is\na family of sets over a finite universe and the goal is to cover the elements\nof the universe with as few sets of the family as possible.\n  The variations of covering problems include well known problems like Set\nCover, Vertex Cover, Dominating Set and Facility Location to name a few.\nRecently there has been a lot of study on partial covering problems, a natural\ngeneralization of covering problems. Here, the goal is not to cover all the\nelements but to cover the specified number of elements with the minimum number\nof sets.\n  In this paper we study partial covering problems in graphs in the realm of\nparameterized complexity. Classical (non-partial) version of all these problems\nhave been intensively studied in planar graphs and in graphs excluding a fixed\ngraph $H$ as a minor. However, the techniques developed for parameterized\nversion of non-partial covering problems cannot be applied directly to their\npartial counterparts. The approach we use, to show that various partial\ncovering problems are fixed parameter tractable on planar graphs, graphs of\nbounded local treewidth and graph excluding some graph as a minor, is quite\ndifferent from previously known techniques. The main idea behind our approach\nis the concept of implicit branching. We find implicit branching technique to\nbe interesting on its own and believe that it can be used for some other\nproblems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2184v1", 
    "other_authors": "Jean Cardinal, Christophe Dumeunier", 
    "title": "Set Covering Problems with General Objective Functions", 
    "arxiv-id": "0802.2184v1", 
    "author": "Christophe Dumeunier", 
    "publish": "2008-02-15T11:56:28Z", 
    "summary": "We introduce a parameterized version of set cover that generalizes several\npreviously studied problems. Given a ground set V and a collection of subsets\nS_i of V, a feasible solution is a partition of V such that each subset of the\npartition is included in one of the S_i. The problem involves maximizing the\nmean subset size of the partition, where the mean is the generalized mean of\nparameter p, taken over the elements. For p=-1, the problem is equivalent to\nthe classical minimum set cover problem. For p=0, it is equivalent to the\nminimum entropy set cover problem, introduced by Halperin and Karp. For p=1,\nthe problem includes the maximum-edge clique partition problem as a special\ncase. We prove that the greedy algorithm simultaneously approximates the\nproblem within a factor of (p+1)^1/p for any p in R^+, and that this is the\nbest possible unless P=NP. These results both generalize and simplify previous\nresults for special cases. We also consider the corresponding graph coloring\nproblem, and prove several tractability and inapproximability results. Finally,\nwe consider a further generalization of the set cover problem in which we aim\nat minimizing the sum of some concave function of the part sizes. As an\napplication, we derive an approximation ratio for a Rent-or-Buy set cover\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2528v1", 
    "other_authors": "Chandra Chekuri, Nitish Korula", 
    "title": "Min-Cost 2-Connected Subgraphs With k Terminals", 
    "arxiv-id": "0802.2528v1", 
    "author": "Nitish Korula", 
    "publish": "2008-02-18T18:34:28Z", 
    "summary": "In the k-2VC problem, we are given an undirected graph G with edge costs and\nan integer k; the goal is to find a minimum-cost 2-vertex-connected subgraph of\nG containing at least k vertices. A slightly more general version is obtained\nif the input also specifies a subset S \\subseteq V of terminals and the goal is\nto find a subgraph containing at least k terminals. Closely related to the\nk-2VC problem, and in fact a special case of it, is the k-2EC problem, in which\nthe goal is to find a minimum-cost 2-edge-connected subgraph containing k\nvertices. The k-2EC problem was introduced by Lau et al., who also gave a\npoly-logarithmic approximation for it. No previous approximation algorithm was\nknown for the more general k-2VC problem. We describe an O(\\log n \\log k)\napproximation for the k-2VC problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2827v1", 
    "other_authors": "Johan M. M. Van Rooij, Hans L. Bodlaender", 
    "title": "Design by Measure and Conquer, A Faster Exact Algorithm for Dominating   Set", 
    "arxiv-id": "0802.2827v1", 
    "author": "Hans L. Bodlaender", 
    "publish": "2008-02-20T14:05:58Z", 
    "summary": "The measure and conquer approach has proven to be a powerful tool to analyse\nexact algorithms for combinatorial problems, like Dominating Set and\nIndependent Set. In this paper, we propose to use measure and conquer also as a\ntool in the design of algorithms. In an iterative process, we can obtain a\nseries of branch and reduce algorithms. A mathematical analysis of an algorithm\nin the series with measure and conquer results in a quasiconvex programming\nproblem. The solution by computer to this problem not only gives a bound on the\nrunning time, but also can give a new reduction rule, thus giving a new,\npossibly faster algorithm. This makes design by measure and conquer a form of\ncomputer aided algorithm design. When we apply the methodology to a Set Cover\nmodelling of the Dominating Set problem, we obtain the currently fastest known\nexact algorithms for Dominating Set: an algorithm that uses $O(1.5134^n)$ time\nand polynomial space, and an algorithm that uses $O(1.5063^n)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2832v1", 
    "other_authors": "Zvi Lotker, Boaz Patt-Shamir, Dror Rawitz", 
    "title": "Rent, Lease or Buy: Randomized Algorithms for Multislope Ski Rental", 
    "arxiv-id": "0802.2832v1", 
    "author": "Dror Rawitz", 
    "publish": "2008-02-20T14:13:19Z", 
    "summary": "In the Multislope Ski Rental problem, the user needs a certain resource for\nsome unknown period of time. To use the resource, the user must subscribe to\none of several options, each of which consists of a one-time setup cost\n(``buying price''), and cost proportional to the duration of the usage\n(``rental rate''). The larger the price, the smaller the rent. The actual usage\ntime is determined by an adversary, and the goal of an algorithm is to minimize\nthe cost by choosing the best option at any point in time. Multislope Ski\nRental is a natural generalization of the classical Ski Rental problem (where\nthe only options are pure rent and pure buy), which is one of the fundamental\nproblems of online computation. The Multislope Ski Rental problem is an\nabstraction of many problems where online decisions cannot be modeled by just\ntwo options, e.g., power management in systems which can be shut down in parts.\nIn this paper we study randomized algorithms for Multislope Ski Rental. Our\nresults include the best possible online randomized strategy for any additive\ninstance, where the cost of switching from one option to another is the\ndifference in their buying prices; and an algorithm that produces an\n$e$-competitive randomized strategy for any (non-additive) instance."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2847v1", 
    "other_authors": "Ulrich Meyer", 
    "title": "On Dynamic Breadth-First Search in External-Memory", 
    "arxiv-id": "0802.2847v1", 
    "author": "Ulrich Meyer", 
    "publish": "2008-02-20T14:21:21Z", 
    "summary": "We provide the first non-trivial result on dynamic breadth-first search (BFS)\nin external-memory: For general sparse undirected graphs of initially $n$ nodes\nand O(n) edges and monotone update sequences of either $\\Theta(n)$ edge\ninsertions or $\\Theta(n)$ edge deletions, we prove an amortized\nhigh-probability bound of $O(n/B^{2/3}+\\sort(n)\\cdot \\log B)$ I/Os per update.\nIn contrast, the currently best approach for static BFS on sparse undirected\ngraphs requires $\\Omega(n/B^{1/2}+\\sort(n))$ I/Os."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2851v1", 
    "other_authors": "Pinyan Lu, Changyuan Yu", 
    "title": "An Improved Randomized Truthful Mechanism for Scheduling Unrelated   Machines", 
    "arxiv-id": "0802.2851v1", 
    "author": "Changyuan Yu", 
    "publish": "2008-02-20T14:22:30Z", 
    "summary": "We study the scheduling problem on unrelated machines in the mechanism design\nsetting. This problem was proposed and studied in the seminal paper (Nisan and\nRonen 1999), where they gave a 1.75-approximation randomized truthful mechanism\nfor the case of two machines. We improve this result by a 1.6737-approximation\nrandomized truthful mechanism. We also generalize our result to a\n$0.8368m$-approximation mechanism for task scheduling with $m$ machines, which\nimprove the previous best upper bound of $0.875m(Mu'alem and Schapira 2007)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2852v1", 
    "other_authors": "Martin Dietzfelbinger, Jonathan E. Rowe, Ingo Wegener, Philipp Woelfel", 
    "title": "Tight Bounds for Blind Search on the Integers", 
    "arxiv-id": "0802.2852v1", 
    "author": "Philipp Woelfel", 
    "publish": "2008-02-20T14:22:33Z", 
    "summary": "We analyze a simple random process in which a token is moved in the interval\n$A=\\{0,...,n\\$: Fix a probability distribution $\\mu$ over $\\{1,...,n\\$.\nInitially, the token is placed in a random position in $A$. In round $t$, a\nrandom value $d$ is chosen according to $\\mu$. If the token is in position\n$a\\geq d$, then it is moved to position $a-d$. Otherwise it stays put. Let $T$\nbe the number of rounds until the token reaches position 0. We show tight\nbounds for the expectation of $T$ for the optimal distribution $\\mu$. More\nprecisely, we show that $\\min_\\mu\\{E_\\mu(T)\\=\\Theta((\\log n)^2)$. For the\nproof, a novel potential function argument is introduced. The research is\nmotivated by the problem of approximating the minimum of a continuous function\nover $[0,1]$ with a ``blind'' optimization strategy."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2855v1", 
    "other_authors": "Thomas Erlebach, Michael Hoffmann, Danny Krizanc, Mat\u00fas Mihal'\u00e1k, Rajeev Raman", 
    "title": "Computing Minimum Spanning Trees with Uncertainty", 
    "arxiv-id": "0802.2855v1", 
    "author": "Rajeev Raman", 
    "publish": "2008-02-20T14:24:10Z", 
    "summary": "We consider the minimum spanning tree problem in a setting where information\nabout the edge weights of the given graph is uncertain. Initially, for each\nedge $e$ of the graph only a set $A_e$, called an uncertainty area, that\ncontains the actual edge weight $w_e$ is known. The algorithm can `update' $e$\nto obtain the edge weight $w_e \\in A_e$. The task is to output the edge set of\na minimum spanning tree after a minimum number of updates. An algorithm is\n$k$-update competitive if it makes at most $k$ times as many updates as the\noptimum. We present a 2-update competitive algorithm if all areas $A_e$ are\nopen or trivial, which is the best possible among deterministic algorithms. The\ncondition on the areas $A_e$ is to exclude degenerate inputs for which no\nconstant update competitive algorithm can exist. Next, we consider a setting\nwhere the vertices of the graph correspond to points in Euclidean space and the\nweight of an edge is equal to the distance of its endpoints. The location of\neach point is initially given as an uncertainty area, and an update reveals the\nexact location of the point. We give a general relation between the edge\nuncertainty and the vertex uncertainty versions of a problem and use it to\nderive a 4-update competitive algorithm for the minimum spanning tree problem\nin the vertex uncertainty model. Again, we show that this is best possible\namong deterministic algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2864v1", 
    "other_authors": "Iyad A. Kanj, Ljubomir Perkovic", 
    "title": "On Geometric Spanners of Euclidean and Unit Disk Graphs", 
    "arxiv-id": "0802.2864v1", 
    "author": "Ljubomir Perkovic", 
    "publish": "2008-02-20T14:36:52Z", 
    "summary": "We consider the problem of constructing bounded-degree planar geometric\nspanners of Euclidean and unit-disk graphs. It is well known that the Delaunay\nsubgraph is a planar geometric spanner with stretch factor $C_{del\\approx\n2.42$; however, its degree may not be bounded. Our first result is a very\nsimple linear time algorithm for constructing a subgraph of the Delaunay graph\nwith stretch factor $\\rho =1+2\\pi(k\\cos{\\frac{\\pi{k)^{-1$ and degree bounded by\n$k$, for any integer parameter $k\\geq 14$. This result immediately implies an\nalgorithm for constructing a planar geometric spanner of a Euclidean graph with\nstretch factor $\\rho \\cdot C_{del$ and degree bounded by $k$, for any integer\nparameter $k\\geq 14$. Moreover, the resulting spanner contains a Euclidean\nMinimum Spanning Tree (EMST) as a subgraph. Our second contribution lies in\ndeveloping the structural results necessary to transfer our analysis and\nalgorithm from Euclidean graphs to unit disk graphs, the usual model for\nwireless ad-hoc networks. We obtain a very simple distributed, {\\em\nstrictly-localized algorithm that, given a unit disk graph embedded in the\nplane, constructs a geometric spanner with the above stretch factor and degree\nbound, and also containing an EMST as a subgraph. The obtained results\ndramatically improve the previous results in all aspects, as shown in the\npaper."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0802.2867v1", 
    "other_authors": "Viet Tung Hoang, Wing-Kin Sung", 
    "title": "Fixed Parameter Polynomial Time Algorithms for Maximum Agreement and   Compatible Supertrees", 
    "arxiv-id": "0802.2867v1", 
    "author": "Wing-Kin Sung", 
    "publish": "2008-02-20T14:38:47Z", 
    "summary": "Consider a set of labels $L$ and a set of trees ${\\mathcal T} = \\{{\\mathcal\nT}^{(1), {\\mathcal T}^{(2), ..., {\\mathcal T}^{(k) \\$ where each tree\n${\\mathcal T}^{(i)$ is distinctly leaf-labeled by some subset of $L$. One\nfundamental problem is to find the biggest tree (denoted as supertree) to\nrepresent $\\mathcal T}$ which minimizes the disagreements with the trees in\n${\\mathcal T}$ under certain criteria. This problem finds applications in\nphylogenetics, database, and data mining. In this paper, we focus on two\nparticular supertree problems, namely, the maximum agreement supertree problem\n(MASP) and the maximum compatible supertree problem (MCSP). These two problems\nare known to be NP-hard for $k \\geq 3$. This paper gives the first polynomial\ntime algorithms for both MASP and MCSP when both $k$ and the maximum degree $D$\nof the trees are constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0804.0149v2", 
    "other_authors": "Bruno Gaume, Fabien Mathieu", 
    "title": "From Random Graph to Small World by Wandering", 
    "arxiv-id": "0804.0149v2", 
    "author": "Fabien Mathieu", 
    "publish": "2008-04-01T11:59:43Z", 
    "summary": "Numerous studies show that most known real-world complex networks share\nsimilar properties in their connectivity and degree distribution. They are\ncalled small worlds. This article gives a method to turn random graphs into\nSmall World graphs by the dint of random walks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-013-9771-6", 
    "link": "http://arxiv.org/pdf/0804.0277v1", 
    "other_authors": "Marko A. Rodriguez", 
    "title": "Mapping Semantic Networks to Undirected Networks", 
    "arxiv-id": "0804.0277v1", 
    "author": "Marko A. Rodriguez", 
    "publish": "2008-04-02T01:19:55Z", 
    "summary": "There exists an injective, information-preserving function that maps a\nsemantic network (i.e a directed labeled network) to a directed network (i.e. a\ndirected unlabeled network). The edge label in the semantic network is\nrepresented as a topological feature of the directed network. Also, there\nexists an injective function that maps a directed network to an undirected\nnetwork (i.e. an undirected unlabeled network). The edge directionality in the\ndirected network is represented as a topological feature of the undirected\nnetwork. Through function composition, there exists an injective function that\nmaps a semantic network to an undirected network. Thus, aside from space\nconstraints, the semantic network construct does not have any modeling\nfunctionality that is not possible with either a directed or undirected network\nrepresentation. Two proofs of this idea will be presented. The first is a proof\nof the aforementioned function composition concept. The second is a simpler\nproof involving an undirected binary encoding of a semantic network."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0722v3", 
    "other_authors": "Gregory Gutin, Daniel Karapetyan", 
    "title": "A Memetic Algorithm for the Generalized Traveling Salesman Problem", 
    "arxiv-id": "0804.0722v3", 
    "author": "Daniel Karapetyan", 
    "publish": "2008-04-04T13:21:40Z", 
    "summary": "The generalized traveling salesman problem (GTSP) is an extension of the\nwell-known traveling salesman problem. In GTSP, we are given a partition of\ncities into groups and we are required to find a minimum length tour that\nincludes exactly one city from each group. The recent studies on this subject\nconsider different variations of a memetic algorithm approach to the GTSP. The\naim of this paper is to present a new memetic algorithm for GTSP with a\npowerful local search procedure. The experiments show that the proposed\nalgorithm clearly outperforms all of the known heuristics with respect to both\nsolution quality and running time. While the other memetic algorithms were\ndesigned only for the symmetric GTSP, our algorithm can solve both symmetric\nand asymmetric instances."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0735v2", 
    "other_authors": "Gregory Gutin, Daniel Karapetyan", 
    "title": "Generalized Traveling Salesman Problem Reduction Algorithms", 
    "arxiv-id": "0804.0735v2", 
    "author": "Daniel Karapetyan", 
    "publish": "2008-04-04T13:36:19Z", 
    "summary": "The generalized traveling salesman problem (GTSP) is an extension of the\nwell-known traveling salesman problem. In GTSP, we are given a partition of\ncities into groups and we are required to find a minimum length tour that\nincludes exactly one city from each group. The aim of this paper is to present\na problem reduction algorithm that deletes redundant vertices and edges,\npreserving the optimal solution. The algorithm's running time is O(N^3) in the\nworst case, but it is significantly faster in practice. The algorithm has\nreduced the problem size by 15-20% on average in our experiments and this has\ndecreased the solution time by 10-60% for each of the considered solvers."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0936v1", 
    "other_authors": "Mark de Berg, Shripad Thite", 
    "title": "Cache-Oblivious Selection in Sorted X+Y Matrices", 
    "arxiv-id": "0804.0936v1", 
    "author": "Shripad Thite", 
    "publish": "2008-04-06T22:31:04Z", 
    "summary": "Let X[0..n-1] and Y[0..m-1] be two sorted arrays, and define the mxn matrix A\nby A[j][i]=X[i]+Y[j]. Frederickson and Johnson gave an efficient algorithm for\nselecting the k-th smallest element from A. We show how to make this algorithm\nIO-efficient. Our cache-oblivious algorithm performs O((m+n)/B) IOs, where B is\nthe block size of memory transfers."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.0940v1", 
    "other_authors": "Shripad Thite", 
    "title": "Optimum Binary Search Trees on the Hierarchical Memory Model", 
    "arxiv-id": "0804.0940v1", 
    "author": "Shripad Thite", 
    "publish": "2008-04-07T00:06:08Z", 
    "summary": "The Hierarchical Memory Model (HMM) of computation is similar to the standard\nRandom Access Machine (RAM) model except that the HMM has a non-uniform memory\norganized in a hierarchy of levels numbered 1 through h. The cost of accessing\na memory location increases with the level number, and accesses to memory\nlocations belonging to the same level cost the same. Formally, the cost of a\nsingle access to the memory location at address a is given by m(a), where m: N\n-> N is the memory cost function, and the h distinct values of m model the\ndifferent levels of the memory hierarchy.\n  We study the problem of constructing and storing a binary search tree (BST)\nof minimum cost, over a set of keys, with probabilities for successful and\nunsuccessful searches, on the HMM with an arbitrary number of memory levels,\nand for the special case h=2.\n  While the problem of constructing optimum binary search trees has been well\nstudied for the standard RAM model, the additional parameter m for the HMM\nincreases the combinatorial complexity of the problem. We present two dynamic\nprogramming algorithms to construct optimum BSTs bottom-up. These algorithms\nrun efficiently under some natural assumptions about the memory hierarchy. We\nalso give an efficient algorithm to construct a BST that is close to optimum,\nby modifying a well-known linear-time approximation algorithm for the RAM\nmodel. We conjecture that the problem of constructing an optimum BST for the\nHMM with an arbitrary memory cost function m is NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.1170v1", 
    "other_authors": "Satyaki Mahalanabis, Daniel Stefankovic", 
    "title": "Approximating L1-distances between mixture distributions using random   projections", 
    "arxiv-id": "0804.1170v1", 
    "author": "Daniel Stefankovic", 
    "publish": "2008-04-08T02:11:13Z", 
    "summary": "We consider the problem of computing L1-distances between every pair\nofcprobability densities from a given family. We point out that the technique\nof Cauchy random projections (Indyk'06) in this context turns into stochastic\nintegrals with respect to Cauchy motion.\n  For piecewise-linear densities these integrals can be sampled from if one can\nsample from the stochastic integral of the function x->(1,x). We give an\nexplicit density function for this stochastic integral and present an efficient\nsampling algorithm. As a consequence we obtain an efficient algorithm to\napproximate the L1-distances with a small relative error.\n  For piecewise-polynomial densities we show how to approximately sample from\nthe distributions resulting from the stochastic integrals. This also results in\nan efficient algorithm to approximate the L1-distances, although our inability\nto get exact samples worsens the dependence on the parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.3902v1", 
    "other_authors": "Tiziana Calamoneri, Andrea E. F. Clementi, Angelo Monti, Gianluca Rossi, Riccardo Silvestri", 
    "title": "Minimum-energy broadcast in random-grid ad-hoc networks: approximation   and distributed algorithms", 
    "arxiv-id": "0804.3902v1", 
    "author": "Riccardo Silvestri", 
    "publish": "2008-04-24T11:17:57Z", 
    "summary": "The Min Energy broadcast problem consists in assigning transmission ranges to\nthe nodes of an ad-hoc network in order to guarantee a directed spanning tree\nfrom a given source node and, at the same time, to minimize the energy\nconsumption (i.e. the energy cost) yielded by the range assignment. Min energy\nbroadcast is known to be NP-hard.\n  We consider random-grid networks where nodes are chosen independently at\nrandom from the $n$ points of a $\\sqrt n \\times \\sqrt n$ square grid in the\nplane. The probability of the existence of a node at a given point of the grid\ndoes depend on that point, that is, the probability distribution can be\nnon-uniform.\n  By using information-theoretic arguments, we prove a lower bound\n$(1-\\epsilon) \\frac n{\\pi}$ on the energy cost of any feasible solution for\nthis problem. Then, we provide an efficient solution of energy cost not larger\nthan $1.1204 \\frac n{\\pi}$.\n  Finally, we present a fully-distributed protocol that constructs a broadcast\nrange assignment of energy cost not larger than $8n$,thus still yielding\nconstant approximation. The energy load is well balanced and, at the same time,\nthe work complexity (i.e. the energy due to all message transmissions of the\nprotocol) is asymptotically optimal. The completion time of the protocol is\nonly an $O(\\log n)$ factor slower than the optimum. The approximation quality\nof our distributed solution is also experimentally evaluated.\n  All bounds hold with probability at least $1-1/n^{\\Theta(1)}$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.3947v1", 
    "other_authors": "Peter Sanders", 
    "title": "Time Dependent Contraction Hierarchies -- Basic Algorithmic Ideas", 
    "arxiv-id": "0804.3947v1", 
    "author": "Peter Sanders", 
    "publish": "2008-04-24T15:24:08Z", 
    "summary": "Contraction hierarchies are a simple hierarchical routing technique that has\nproved extremely efficient for static road networks. We explain how to\ngeneralize them to networks with time-dependent edge weights. This is the first\nhierarchical speedup technique for time-dependent routing that allows\nbidirectional query algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11047-009-9111-6", 
    "link": "http://arxiv.org/pdf/0804.4138v1", 
    "other_authors": "Nicholas J. A. Harvey, Jelani Nelson, Krzysztof Onak", 
    "title": "Sketching and Streaming Entropy via Approximation Theory", 
    "arxiv-id": "0804.4138v1", 
    "author": "Krzysztof Onak", 
    "publish": "2008-04-25T16:04:20Z", 
    "summary": "We conclude a sequence of work by giving near-optimal sketching and streaming\nalgorithms for estimating Shannon entropy in the most general streaming model,\nwith arbitrary insertions and deletions. This improves on prior results that\nobtain suboptimal space bounds in the general model, and near-optimal bounds in\nthe insertion-only model without sketching. Our high-level approach is simple:\nwe give algorithms to estimate Renyi and Tsallis entropy, and use them to\nextrapolate an estimate of Shannon entropy. The accuracy of our estimates is\nproven using approximation theory arguments and extremal properties of\nChebyshev polynomials, a technique which may be useful for other problems. Our\nwork also yields the best-known and near-optimal additive approximations for\nentropy, and hence also for conditional entropy and mutual information."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0804.4819v2", 
    "other_authors": "Michael A. Bender, S\u00e1ndor P. Fekete, Alexander Kr\u00f6ller, Vincenzo Liberatore, Joseph S. B. Mitchell, Valentin Polishchuk, Jukka Suomela", 
    "title": "The Minimum Backlog Problem", 
    "arxiv-id": "0804.4819v2", 
    "author": "Jukka Suomela", 
    "publish": "2008-04-30T13:13:12Z", 
    "summary": "We study the minimum backlog problem (MBP). This online problem arises, e.g.,\nin the context of sensor networks. We focus on two main variants of MBP.\n  The discrete MBP is a 2-person game played on a graph $G=(V,E)$. The player\nis initially located at a vertex of the graph. In each time step, the adversary\npours a total of one unit of water into cups that are located on the vertices\nof the graph, arbitrarily distributing the water among the cups. The player\nthen moves from her current vertex to an adjacent vertex and empties the cup at\nthat vertex. The player's objective is to minimize the backlog, i.e., the\nmaximum amount of water in any cup at any time.\n  The geometric MBP is a continuous-time version of the MBP: the cups are\npoints in the two-dimensional plane, the adversary pours water continuously at\na constant rate, and the player moves in the plane with unit speed. Again, the\nplayer's objective is to minimize the backlog.\n  We show that the competitive ratio of any algorithm for the MBP has a lower\nbound of $\\Omega(D)$, where $D$ is the diameter of the graph (for the discrete\nMBP) or the diameter of the point set (for the geometric MBP). Therefore we\nfocus on determining a strategy for the player that guarantees a uniform upper\nbound on the absolute value of the backlog.\n  For the absolute value of the backlog there is a trivial lower bound of\n$\\Omega(D)$, and the deamortization analysis of Dietz and Sleator gives an\nupper bound of $O(D\\log N)$ for $N$ cups. Our main result is a tight upper\nbound for the geometric MBP: we show that there is a strategy for the player\nthat guarantees a backlog of $O(D)$, independently of the number of cups."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1071v3", 
    "other_authors": "Zoya Svitkina, Lisa Fleischer", 
    "title": "Submodular approximation: sampling-based algorithms and lower bounds", 
    "arxiv-id": "0805.1071v3", 
    "author": "Lisa Fleischer", 
    "publish": "2008-05-07T21:37:18Z", 
    "summary": "We introduce several generalizations of classical computer science problems\nobtained by replacing simpler objective functions with general submodular\nfunctions. The new problems include submodular load balancing, which\ngeneralizes load balancing or minimum-makespan scheduling, submodular sparsest\ncut and submodular balanced cut, which generalize their respective graph cut\nproblems, as well as submodular function minimization with a cardinality lower\nbound. We establish upper and lower bounds for the approximability of these\nproblems with a polynomial number of queries to a function-value oracle. The\napproximation guarantees for most of our algorithms are of the order of\nsqrt(n/ln n). We show that this is the inherent difficulty of the problems by\nproving matching lower bounds. We also give an improved lower bound for the\nproblem of approximately learning a monotone submodular function. In addition,\nwe present an algorithm for approximately learning submodular functions with\nspecial structure, whose guarantee is close to the lower bound. Although quite\nrestrictive, the class of functions with this structure includes the ones that\nare used for lower bounds both by us and in previous work. This demonstrates\nthat if there are significantly stronger lower bounds for this problem, they\nrely on more general submodular functions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1598v1", 
    "other_authors": "Peiyush Jain", 
    "title": "A Simple In-Place Algorithm for In-Shuffle", 
    "arxiv-id": "0805.1598v1", 
    "author": "Peiyush Jain", 
    "publish": "2008-05-12T09:28:18Z", 
    "summary": "The paper presents a simple, linear time, in-place algorithm for performing a\n2-way in-shuffle which can be used with little modification for certain other\nk-way shuffles."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.1661v2", 
    "other_authors": "G. Hickey, P. Carmi, A. Maheshwari, N. Zeh", 
    "title": "NAPX: A Polynomial Time Approximation Scheme for the Noah's Ark Problem", 
    "arxiv-id": "0805.1661v2", 
    "author": "N. Zeh", 
    "publish": "2008-05-12T15:04:26Z", 
    "summary": "The Noah's Ark Problem (NAP) is an NP-Hard optimization problem with\nrelevance to ecological conservation management. It asks to maximize the\nphylogenetic diversity (PD) of a set of taxa given a fixed budget, where each\ntaxon is associated with a cost of conservation and a probability of\nextinction. NAP has received renewed interest with the rise in availability of\ngenetic sequence data, allowing PD to be used as a practical measure of\nbiodiversity. However, only simplified instances of the problem, where one or\nmore parameters are fixed as constants, have as of yet been addressed in the\nliterature. We present NAPX, the first algorithm for the general version of NAP\nthat returns a $1 - \\epsilon$ approximation of the optimal solution. It runs in\n$O(\\frac{n B^2 h^2 \\log^2n}{\\log^2(1 - \\epsilon)})$ time where $n$ is the\nnumber of species, and $B$ is the total budget and $h$ is the height of the\ninput tree. We also provide improved bounds for its expected running time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.2630v2", 
    "other_authors": "Sudipto Guha, Kamesh Munagala", 
    "title": "Sequential Design of Experiments via Linear Programming", 
    "arxiv-id": "0805.2630v2", 
    "author": "Kamesh Munagala", 
    "publish": "2008-05-17T22:48:22Z", 
    "summary": "The celebrated multi-armed bandit problem in decision theory models the basic\ntrade-off between exploration, or learning about the state of a system, and\nexploitation, or utilizing the system. In this paper we study the variant of\nthe multi-armed bandit problem where the exploration phase involves costly\nexperiments and occurs before the exploitation phase; and where each play of an\narm during the exploration phase updates a prior belief about the arm. The\nproblem of finding an inexpensive exploration strategy to optimize a certain\nexploitation objective is NP-Hard even when a single play reveals all\ninformation about an arm, and all exploration steps cost the same.\n  We provide the first polynomial time constant-factor approximation algorithm\nfor this class of problems. We show that this framework also generalizes\nseveral problems of interest studied in the context of data acquisition in\nsensor networks. Our analyses also extends to switching and setup costs, and to\nconcave utility objectives.\n  Our solution approach is via a novel linear program rounding technique based\non stochastic packing. In addition to yielding exploration policies whose\nperformance is within a small constant factor of the adaptive optimal policy, a\nnice feature of this approach is that the resulting policies explore the arms\nsequentially without revisiting any arm. Sequentiality is a well-studied\nconcept in decision theory, and is very desirable in domains where multiple\nexplorations can be conducted in parallel, for instance, in the sensor network\ncontext."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0805.2646v1", 
    "other_authors": "Ilias Diakonikolas, Mihalis Yannakakis", 
    "title": "Small Approximate Pareto Sets for Bi-objective Shortest Paths and Other   Problems", 
    "arxiv-id": "0805.2646v1", 
    "author": "Mihalis Yannakakis", 
    "publish": "2008-05-17T06:10:19Z", 
    "summary": "We investigate the problem of computing a minimum set of solutions that\napproximates within a specified accuracy $\\epsilon$ the Pareto curve of a\nmultiobjective optimization problem. We show that for a broad class of\nbi-objective problems (containing many important widely studied problems such\nas shortest paths, spanning tree, and many others), we can compute in\npolynomial time an $\\epsilon$-Pareto set that contains at most twice as many\nsolutions as the minimum such set. Furthermore we show that the factor of 2 is\ntight for these problems, i.e., it is NP-hard to do better. We present upper\nand lower bounds for three or more objectives, as well as for the dual problem\nof computing a specified number $k$ of solutions which provide a good\napproximation to the Pareto curve."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.0760v1", 
    "other_authors": "Paul Tarau", 
    "title": "Declarative Combinatorics: Boolean Functions, Circuit Synthesis and BDDs   in Haskell", 
    "arxiv-id": "0808.0760v1", 
    "author": "Paul Tarau", 
    "publish": "2008-08-06T01:41:51Z", 
    "summary": "We describe Haskell implementations of interesting combinatorial generation\nalgorithms with focus on boolean functions and logic circuit representations.\n  First, a complete exact combinational logic circuit synthesizer is described\nas a combination of catamorphisms and anamorphisms.\n  Using pairing and unpairing functions on natural number representations of\ntruth tables, we derive an encoding for Binary Decision Diagrams (BDDs) with\nthe unique property that its boolean evaluation faithfully mimics its\nstructural conversion to a a natural number through recursive application of a\nmatching pairing function.\n  We then use this result to derive ranking and unranking functions for BDDs\nand reduced BDDs.\n  Finally, a generalization of the encoding techniques to Multi-Terminal BDDs\nis provided.\n  The paper is organized as a self-contained literate Haskell program,\navailable at http://logic.csci.unt.edu/tarau/research/2008/fBDD.zip .\n  Keywords: exact combinational logic synthesis, binary decision diagrams,\nencodings of boolean functions, pairing/unpairing functions, ranking/unranking\nfunctions for BDDs and MTBDDs, declarative combinatorics in Haskell"
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.1108v2", 
    "other_authors": "Nicolas Bock, Emanuel H. Rubensson, Pawe\u0142 Sa\u0142ek, Anders M. N. Niklasson, Matt Challacombe", 
    "title": "Cache oblivious storage and access heuristics for blocked matrix-matrix   multiplication", 
    "arxiv-id": "0808.1108v2", 
    "author": "Matt Challacombe", 
    "publish": "2008-08-07T22:49:29Z", 
    "summary": "We investigate effects of ordering in blocked matrix--matrix multiplication.\nWe find that submatrices do not have to be stored contiguously in memory to\nachieve near optimal performance. Instead it is the choice of execution order\nof the submatrix multiplications that leads to a speedup of up to four times\nfor small block sizes. This is in contrast to results for single matrix\nelements showing that contiguous memory allocation quickly becomes irrelevant\nas the blocksize increases."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.1246v2", 
    "other_authors": "Mugurel Ionut Andreica, Romulus Andreica, Angela Andreica", 
    "title": "Minimum Dissatisfaction Personnel Scheduling", 
    "arxiv-id": "0808.1246v2", 
    "author": "Angela Andreica", 
    "publish": "2008-08-08T17:15:34Z", 
    "summary": "In this paper we consider two problems regarding the scheduling of available\npersonnel in order to perform a given quantity of work, which can be\narbitrarily decomposed into a sequence of activities. We are interested in\nschedules which minimize the overall dissatisfaction, where each employee's\ndissatisfaction is modeled as a time-dependent linear function. For the two\nsituations considered we provide a detailed mathematical analysis, as well as\nefficient algorithms for determining optimal schedules."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.1766v1", 
    "other_authors": "Ping Li", 
    "title": "The Optimal Quantile Estimator for Compressed Counting", 
    "arxiv-id": "0808.1766v1", 
    "author": "Ping Li", 
    "publish": "2008-08-13T01:38:45Z", 
    "summary": "Compressed Counting (CC) was recently proposed for very efficiently computing\nthe (approximate) $\\alpha$th frequency moments of data streams, where $0<\\alpha\n<= 2$. Several estimators were reported including the geometric mean estimator,\nthe harmonic mean estimator, the optimal power estimator, etc. The geometric\nmean estimator is particularly interesting for theoretical purposes. For\nexample, when $\\alpha -> 1$, the complexity of CC (using the geometric mean\nestimator) is $O(1/\\epsilon)$, breaking the well-known large-deviation bound\n$O(1/\\epsilon^2)$. The case $\\alpha\\approx 1$ has important applications, for\nexample, computing entropy of data streams.\n  For practical purposes, this study proposes the optimal quantile estimator.\nCompared with previous estimators, this estimator is computationally more\nefficient and is also more accurate when $\\alpha> 1$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.1771v2", 
    "other_authors": "Ping Li", 
    "title": "A Very Efficient Scheme for Estimating Entropy of Data Streams Using   Compressed Counting", 
    "arxiv-id": "0808.1771v2", 
    "author": "Ping Li", 
    "publish": "2008-08-13T03:05:33Z", 
    "summary": "Compressed Counting (CC)} was recently proposed for approximating the\n$\\alpha$th frequency moments of data streams, for $0<\\alpha \\leq 2$. Under the\nrelaxed strict-Turnstile model, CC dramatically improves the standard algorithm\nbased on symmetric stable random projections}, especially as $\\alpha\\to 1$. A\ndirect application of CC is to estimate the entropy, which is an important\nsummary statistic in Web/network measurement and often serves a crucial\n\"feature\" for data mining. The R\\'enyi entropy and the Tsallis entropy are\nfunctions of the $\\alpha$th frequency moments; and both approach the Shannon\nentropy as $\\alpha\\to 1$. A recent theoretical work suggested using the\n$\\alpha$th frequency moment to approximate the Shannon entropy with\n$\\alpha=1+\\delta$ and very small $|\\delta|$ (e.g., $<10^{-4}$).\n  In this study, we experiment using CC to estimate frequency moments, R\\'enyi\nentropy, Tsallis entropy, and Shannon entropy, on real Web crawl data. We\ndemonstrate the variance-bias trade-off in estimating Shannon entropy and\nprovide practical recommendations. In particular, our experiments enable us to\ndraw some important conclusions:\n  (1) As $\\alpha\\to 1$, CC dramatically improves {\\em symmetric stable random\nprojections} in estimating frequency moments, R\\'enyi entropy, Tsallis entropy,\nand Shannon entropy. The improvements appear to approach \"infinity.\"\n  (2) Using {\\em symmetric stable random projections} and $\\alpha = 1+\\delta$\nwith very small $|\\delta|$ does not provide a practical algorithm because the\nrequired sample size is enormous."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.2222v1", 
    "other_authors": "Alexandr Andoni, Andrew McGregor, Krzysztof Onak, Rina Panigrahy", 
    "title": "Better Bounds for Frequency Moments in Random-Order Streams", 
    "arxiv-id": "0808.2222v1", 
    "author": "Rina Panigrahy", 
    "publish": "2008-08-16T00:43:14Z", 
    "summary": "Estimating frequency moments of data streams is a very well studied problem\nand tight bounds are known on the amount of space that is necessary and\nsufficient when the stream is adversarially ordered. Recently, motivated by\nvarious practical considerations and applications in learning and statistics,\nthere has been growing interest into studying streams that are randomly\nordered. In the paper we improve the previous lower bounds on the space\nrequired to estimate the frequency moments of a randomly ordered streams."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0808.3197v1", 
    "other_authors": "Ming-Zhe Chen", 
    "title": "On the Monotonicity of Work Function in k-Server Conjecture", 
    "arxiv-id": "0808.3197v1", 
    "author": "Ming-Zhe Chen", 
    "publish": "2008-08-24T15:37:35Z", 
    "summary": "This paper presents a mistake in work function algorithm of k-server\nconjecture. That is, the monotonicity of the work function is not always true."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.0460v1", 
    "other_authors": "Shipra Agrawal, Amin Saberi, Yinyu Ye", 
    "title": "Stochastic Combinatorial Optimization under Probabilistic Constraints", 
    "arxiv-id": "0809.0460v1", 
    "author": "Yinyu Ye", 
    "publish": "2008-09-02T16:07:42Z", 
    "summary": "In this paper, we present approximation algorithms for combinatorial\noptimization problems under probabilistic constraints. Specifically, we focus\non stochastic variants of two important combinatorial optimization problems:\nthe k-center problem and the set cover problem, with uncertainty characterized\nby a probability distribution over set of points or elements to be covered. We\nconsider these problems under adaptive and non-adaptive settings, and present\nefficient approximation algorithms for the case when underlying distribution is\na product distribution. In contrast to the expected cost model prevalent in\nstochastic optimization literature, our problem definitions support\nrestrictions on the probability distributions of the total costs, via\nincorporating constraints that bound the probability with which the incurred\ncosts may exceed a given threshold."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.1715v1", 
    "other_authors": "Bodo Manthey, Heiko R\u00f6glin", 
    "title": "Improved Smoothed Analysis of the k-Means Method", 
    "arxiv-id": "0809.1715v1", 
    "author": "Heiko R\u00f6glin", 
    "publish": "2008-09-10T07:00:38Z", 
    "summary": "The k-means method is a widely used clustering algorithm. One of its\ndistinguished features is its speed in practice. Its worst-case running-time,\nhowever, is exponential, leaving a gap between practical and theoretical\nperformance. Arthur and Vassilvitskii (FOCS 2006) aimed at closing this gap,\nand they proved a bound of $\\poly(n^k, \\sigma^{-1})$ on the smoothed\nrunning-time of the k-means method, where n is the number of data points and\n$\\sigma$ is the standard deviation of the Gaussian perturbation. This bound,\nthough better than the worst-case bound, is still much larger than the\nrunning-time observed in practice.\n  We improve the smoothed analysis of the k-means method by showing two upper\nbounds on the expected running-time of k-means. First, we prove that the\nexpected running-time is bounded by a polynomial in $n^{\\sqrt k}$ and\n$\\sigma^{-1}$. Second, we prove an upper bound of $k^{kd} \\cdot \\poly(n,\n\\sigma^{-1})$, where d is the dimension of the data space. The polynomial is\nindependent of k and d, and we obtain a polynomial bound for the expected\nrunning-time for $k, d \\in O(\\sqrt{\\log n/\\log \\log n})$.\n  Finally, we show that k-means runs in smoothed polynomial time for\none-dimensional instances."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.1895v1", 
    "other_authors": "Yossi Azar, Benjamin Birnbaum, Anna R. Karlin, C. Thach Nguyen", 
    "title": "Thinking Twice about Second-Price Ad Auctions", 
    "arxiv-id": "0809.1895v1", 
    "author": "C. Thach Nguyen", 
    "publish": "2008-09-10T23:33:47Z", 
    "summary": "Recent work has addressed the algorithmic problem of allocating advertisement\nspace for keywords in sponsored search auctions so as to maximize revenue, most\nof which assume that pricing is done via a first-price auction. This does not\nrealistically model the Generalized Second Price (GSP) auction used in\npractice, in which bidders pay the next-highest bid for keywords that they are\nallocated. Towards the goal of more realistically modeling these auctions, we\nintroduce the Second-Price Ad Auctions problem, in which bidders' payments are\ndetermined by the GSP mechanism. We show that the complexity of the\nSecond-Price Ad Auctions problem is quite different than that of the more\nstudied First-Price Ad Auctions problem. First, unlike the first-price variant,\nfor which small constant-factor approximations are known, it is NP-hard to\napproximate the Second-Price Ad Auctions problem to any non-trivial factor,\neven when the bids are small compared to the budgets. Second, this discrepancy\nextends even to the 0-1 special case that we call the Second-Price Matching\nproblem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no\ndeterministic algorithm achieving a non-trivial competitive ratio and no\nrandomized algorithm achieving a competitive ratio better than 2. This\ncontrasts with the results for the analogous special case in the first-price\nmodel, the standard bipartite matching problem, which is solvable in polynomial\ntime and which has deterministic and randomized online algorithms achieving\nbetter competitive ratios. On the positive side, we provide a 2-approximation\nfor offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.\nThe latter result makes use of a new generalization of a result on the\nperformance of the \"Ranking\" algorithm for online bipartite matching."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.1902v2", 
    "other_authors": "Manor Mendel, Chaya Schwob", 
    "title": "Fast C-K-R Partitions of Sparse Graphs", 
    "arxiv-id": "0809.1902v2", 
    "author": "Chaya Schwob", 
    "publish": "2008-09-11T02:10:29Z", 
    "summary": "We present fast algorithms for constructing probabilistic embeddings and\napproximate distance oracles in sparse graphs. The main ingredient is a fast\nalgorithm for sampling the probabilistic partitions of Calinescu, Karloff, and\nRabani in sparse graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.1906v2", 
    "other_authors": "Shiva Kintali", 
    "title": "Betweenness Centrality : Algorithms and Lower Bounds", 
    "arxiv-id": "0809.1906v2", 
    "author": "Shiva Kintali", 
    "publish": "2008-09-11T02:49:07Z", 
    "summary": "One of the most fundamental problems in large scale network analysis is to\ndetermine the importance of a particular node in a network. Betweenness\ncentrality is the most widely used metric to measure the importance of a node\nin a network. In this paper, we present a randomized parallel algorithm and an\nalgebraic method for computing betweenness centrality of all nodes in a\nnetwork. We prove that any path-comparison based algorithm cannot compute\nbetweenness in less than O(nm) time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.2097v1", 
    "other_authors": "Hsiao-Fei Liu, Peng-An Chen, Kun-Mao Chao", 
    "title": "Algorithms for Locating Constrained Optimal Intervals", 
    "arxiv-id": "0809.2097v1", 
    "author": "Kun-Mao Chao", 
    "publish": "2008-09-11T20:12:08Z", 
    "summary": "In this work, we obtain the following new results.\n  1. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number\npairs, where $s_i>0$ for all $i$, and a number $L_h$, we propose an O(n)-time\nalgorithm for finding an index interval $[i,j]$ that maximizes\n$\\frac{\\sum_{k=i}^{j} h_k}{\\sum_{k=i}^{j} s_k}$ subject to $\\sum_{k=i}^{j} h_k\n\\geq L_h$.\n  2. Given a sequence $D=((h_1,s_1), (h_2,s_2) ..., (h_n,s_n))$ of number\npairs, where $s_i=1$ for all $i$, and an integer $L_s$ with $1\\leq L_s\\leq n$,\nwe propose an $O(n\\frac{T(L_s^{1/2})}{L_s^{1/2}})$-time algorithm for finding\nan index interval $[i,j]$ that maximizes $\\frac{\\sum_{k=i}^{j}\nh_k}{\\sqrt{\\sum_{k=i}^{j} s_k}}$ subject to $\\sum_{k=i}^{j} s_k \\geq L_s$,\nwhere $T(n')$ is the time required to solve the all-pairs shortest paths\nproblem on a graph of $n'$ nodes. By the latest result of Chan \\cite{Chan},\n$T(n')=O(n'^3 \\frac{(\\log\\log n')^3}{(\\log n')^2})$, so our algorithm runs in\nsubquadratic time $O(nL_s\\frac{(\\log\\log L_s)^3}{(\\log L_s)^2})$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.2554v1", 
    "other_authors": "Anupam Gupta, Kanat Tangwongsan", 
    "title": "Simpler Analyses of Local Search Algorithms for Facility Location", 
    "arxiv-id": "0809.2554v1", 
    "author": "Kanat Tangwongsan", 
    "publish": "2008-09-15T15:42:47Z", 
    "summary": "We study local search algorithms for metric instances of facility location\nproblems: the uncapacitated facility location problem (UFL), as well as\nuncapacitated versions of the $k$-median, $k$-center and $k$-means problems.\nAll these problems admit natural local search heuristics: for example, in the\nUFL problem the natural moves are to open a new facility, close an existing\nfacility, and to swap a closed facility for an open one; in $k$-medians, we are\nallowed only swap moves. The local-search algorithm for $k$-median was analyzed\nby Arya et al. (SIAM J. Comput. 33(3):544-562, 2004), who used a clever\n``coupling'' argument to show that local optima had cost at most constant times\nthe global optimum. They also used this argument to show that the local search\nalgorithm for UFL was 3-approximation; their techniques have since been applied\nto other facility location problems.\n  In this paper, we give a proof of the $k$-median result which avoids this\ncoupling argument. These arguments can be used in other settings where the Arya\net al. arguments have been used. We also show that for the problem of opening\n$k$ facilities $F$ to minimize the objective function $\\Phi_p(F) = \\big(\\sum_{j\n\\in V} d(j, F)^p\\big)^{1/p}$, the natural swap-based local-search algorithm is\na $\\Theta(p)$-approximation. This implies constant-factor approximations for\n$k$-medians (when $p=1$), and $k$-means (when $p = 2$), and an $O(\\log\nn)$-approximation algorithm for the $k$-center problem (which is essentially $p\n= \\log n$)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.2970v1", 
    "other_authors": "Raphael Yuster", 
    "title": "Single source shortest paths in $H$-minor free graphs", 
    "arxiv-id": "0809.2970v1", 
    "author": "Raphael Yuster", 
    "publish": "2008-09-17T17:51:09Z", 
    "summary": "We present an algorithm for the Single Source Shortest Paths (SSSP) problem\nin \\emph{$H$-minor free} graphs. For every fixed $H$, if $G$ is a graph with\n$n$ vertices having integer edge lengths and $s$ is a designated source vertex\nof $G$, the algorithm runs in $\\tilde{O}(n^{\\sqrt{11.5}-2} \\log L) \\le\nO(n^{1.392} \\log L)$ time, where $L$ is the absolute value of the smallest edge\nlength. The algorithm computes shortest paths and the distances from $s$ to all\nvertices of the graph, or else provides a certificate that $G$ is not $H$-minor\nfree. Our result improves an earlier $O(n^{1.5} \\log L)$ time algorithm for\nthis problem, which follows from a general SSSP algorithm of Goldberg."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.3527v2", 
    "other_authors": "Mugurel Ionut Andreica, Angela Andreica, Romulus Andreica", 
    "title": "Inferring Company Structure from Limited Available Information", 
    "arxiv-id": "0809.3527v2", 
    "author": "Romulus Andreica", 
    "publish": "2008-09-20T20:05:21Z", 
    "summary": "In this paper we present several algorithmic techniques for inferring the\nstructure of a company when only a limited amount of information is available.\nWe consider problems with two types of inputs: the number of pairs of employees\nwith a given property and restricted information about the hierarchical\nstructure of the company. We provide dynamic programming and greedy algorithms\nfor these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0809.3528v2", 
    "other_authors": "Mugurel Ionut Andreica, Cristina Teodora Andreica, Madalina Ecaterina Andreica", 
    "title": "Locating Restricted Facilities on Binary Maps", 
    "arxiv-id": "0809.3528v2", 
    "author": "Madalina Ecaterina Andreica", 
    "publish": "2008-09-20T20:06:34Z", 
    "summary": "In this paper we consider several facility location problems with\napplications to cost and social welfare optimization, when the area map is\nencoded as a binary (0,1) mxn matrix. We present algorithmic solutions for all\nthe problems. Some cases are too particular to be used in practical situations,\nbut they are at least a starting point for more generic solutions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.0075v1", 
    "other_authors": "Alvaro Salas", 
    "title": "Acerca del Algoritmo de Dijkstra", 
    "arxiv-id": "0810.0075v1", 
    "author": "Alvaro Salas", 
    "publish": "2008-10-01T04:55:11Z", 
    "summary": "In this paper we prove the correctness of Dijkstra's algorithm. We also\ndiscuss it and at the end we show an application."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.0264v1", 
    "other_authors": "David R. Musser, Gor V. Nishanov", 
    "title": "A Fast Generic Sequence Matching Algorithm", 
    "arxiv-id": "0810.0264v1", 
    "author": "Gor V. Nishanov", 
    "publish": "2008-10-01T19:54:51Z", 
    "summary": "A string matching -- and more generally, sequence matching -- algorithm is\npresented that has a linear worst-case computing time bound, a low worst-case\nbound on the number of comparisons (2n), and sublinear average-case behavior\nthat is better than that of the fastest versions of the Boyer-Moore algorithm.\nThe algorithm retains its efficiency advantages in a wide variety of sequence\nmatching problems of practical interest, including traditional string matching;\nlarge-alphabet problems (as in Unicode strings); and small-alphabet,\nlong-pattern problems (as in DNA searches). Since it is expressed as a generic\nalgorithm for searching in sequences over an arbitrary type T, it is well\nsuited for use in generic software libraries such as the C++ Standard Template\nLibrary. The algorithm was obtained by adding to the Knuth-Morris-Pratt\nalgorithm one of the pattern-shifting techniques from the Boyer-Moore\nalgorithm, with provision for use of hashing in this technique. In situations\nin which a hash function or random access to the sequences is not available,\nthe algorithm falls back to an optimized version of the Knuth-Morris-Pratt\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.0558v2", 
    "other_authors": "Ashish Goel, Sanjeev Khanna, Brad Null", 
    "title": "The Ratio Index for Budgeted Learning, with Applications", 
    "arxiv-id": "0810.0558v2", 
    "author": "Brad Null", 
    "publish": "2008-10-03T01:37:45Z", 
    "summary": "In the budgeted learning problem, we are allowed to experiment on a set of\nalternatives (given a fixed experimentation budget) with the goal of picking a\nsingle alternative with the largest possible expected payoff. Approximation\nalgorithms for this problem were developed by Guha and Munagala by rounding a\nlinear program that couples the various alternatives together. In this paper we\npresent an index for this problem, which we call the ratio index, which also\nguarantees a constant factor approximation. Index-based policies have the\nadvantage that a single number (i.e. the index) can be computed for each\nalternative irrespective of all other alternatives, and the alternative with\nthe highest index is experimented upon. This is analogous to the famous Gittins\nindex for the discounted multi-armed bandit problem.\n  The ratio index has several interesting structural properties. First, we show\nthat it can be computed in strongly polynomial time. Second, we show that with\nthe appropriate discount factor, the Gittins index and our ratio index are\nconstant factor approximations of each other, and hence the Gittins index also\ngives a constant factor approximation to the budgeted learning problem.\nFinally, we show that the ratio index can be used to create an index-based\npolicy that achieves an O(1)-approximation for the finite horizon version of\nthe multi-armed bandit problem. Moreover, the policy does not require any\nknowledge of the horizon (whereas we compare its performance against an optimal\nstrategy that is aware of the horizon). This yields the following surprising\nresult: there is an index-based policy that achieves an O(1)-approximation for\nthe multi-armed bandit problem, oblivious to the underlying discount factor."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.0674v1", 
    "other_authors": "Siddharth Barman, Shuchi Chawla", 
    "title": "Packing multiway cuts in capacitated graphs", 
    "arxiv-id": "0810.0674v1", 
    "author": "Shuchi Chawla", 
    "publish": "2008-10-03T16:13:00Z", 
    "summary": "We consider the following \"multiway cut packing\" problem in undirected\ngraphs: we are given a graph G=(V,E) and k commodities, each corresponding to a\nset of terminals located at different vertices in the graph; our goal is to\nproduce a collection of cuts {E_1,...,E_k} such that E_i is a multiway cut for\ncommodity i and the maximum load on any edge is minimized. The load on an edge\nis defined to be the number of cuts in the solution crossing the edge. In the\ncapacitated version of the problem the goal is to minimize the maximum relative\nload on any edge--the ratio of the edge's load to its capacity. Multiway cut\npacking arises in the context of graph labeling problems where we are given a\npartial labeling of a set of items and a neighborhood structure over them, and,\ninformally, the goal is to complete the labeling in the most consistent way.\nThis problem was introduced by Rabani, Schulman, and Swamy (SODA'08), who\ndeveloped an O(log n/log log n) approximation for it in general graphs, as well\nas an improved O(log^2 k) approximation in trees. Here n is the number of nodes\nin the graph. We present the first constant factor approximation for this\nproblem in arbitrary undirected graphs. Our approach is based on the\nobservation that every instance of the problem admits a near-optimal laminar\nsolution (that is, one in which no pair of cuts cross each other)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.0906v2", 
    "other_authors": "Toru Hasunuma, Toshimasa Ishii, Hirotaka Ono, Yushi Uno", 
    "title": "A linear time algorithm for L(2,1)-labeling of trees", 
    "arxiv-id": "0810.0906v2", 
    "author": "Yushi Uno", 
    "publish": "2008-10-06T08:21:17Z", 
    "summary": "An L(2,1)-labeling of a graph $G$ is an assignment $f$ from the vertex set\n$V(G)$ to the set of nonnegative integers such that $|f(x)-f(y)|\\ge 2$ if $x$\nand $y$ are adjacent and $|f(x)-f(y)|\\ge 1$ if $x$ and $y$ are at distance 2,\nfor all $x$ and $y$ in $V(G)$. A $k$-L(2,1)-labeling is an assignment\n$f:V(G)\\to\\{0,..., k\\}$, and the L(2,1)-labeling problem asks the minimum $k$,\nwhich we denote by $\\lambda(G)$, among all possible assignments. It is known\nthat this problem is NP-hard even for graphs of treewidth 2, and tree is one of\na very few classes for which the problem is polynomially solvable. The running\ntime of the best known algorithm for trees had been $\\mO(\\Delta^{4.5} n)$ for\nmore than a decade, however, an $\\mO(n^{1.75})$-time algorithm has been\nproposed recently, which substantially improved the previous one, where\n$\\Delta$ is the maximum degree of $T$ and $n=|V(T)|$. In this paper, we finally\nestablish a linear time algorithm for L(2,1)-labeling of trees."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.3438v1", 
    "other_authors": "Amit M Bhosle, Teofilo F Gonzalez", 
    "title": "Efficient Algorithms and Routing Protocols for Handling Transient Single   Node Failures", 
    "arxiv-id": "0810.3438v1", 
    "author": "Teofilo F Gonzalez", 
    "publish": "2008-10-19T22:57:53Z", 
    "summary": "Single node failures represent more than 85% of all node failures in the\ntoday's large communication networks such as the Internet. Also, these node\nfailures are usually transient. Consequently, having the routing paths globally\nrecomputed does not pay off since the failed nodes recover fairly quickly, and\nthe recomputed routing paths need to be discarded. Instead, we develop\nalgorithms and protocols for dealing with such transient single node failures\nby suppressing the failure (instead of advertising it across the network), and\nrouting messages to the destination via alternate paths that do not use the\nfailed node. We compare our solution to that of Ref. [11] wherein the authors\nhave presented a \"Failure Insensitive Routing\" protocol as a proactive recovery\nscheme for handling transient node failures. We show that our algorithms are\nfaster by an order of magnitude while our paths are equally good. We show via\nsimulation results that our paths are usually within 15% of the optimal for\nrandomly generated graph with 100-1000 nodes."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.4812v2", 
    "other_authors": "Robin A. Moser", 
    "title": "A constructive proof of the Lovasz Local Lemma", 
    "arxiv-id": "0810.4812v2", 
    "author": "Robin A. Moser", 
    "publish": "2008-10-27T14:02:48Z", 
    "summary": "The Lovasz Local Lemma [EL75] is a powerful tool to prove the existence of\ncombinatorial objects meeting a prescribed collection of criteria. The\ntechnique can directly be applied to the satisfiability problem, yielding that\na k-CNF formula in which each clause has common variables with at most 2^(k-2)\nother clauses is always satisfiable. All hitherto known proofs of the Local\nLemma are non-constructive and do thus not provide a recipe as to how a\nsatisfying assignment to such a formula can be efficiently found. In his\nbreakthrough paper [Bec91], Beck demonstrated that if the neighbourhood of each\nclause be restricted to O(2^(k/48)), a polynomial time algorithm for the search\nproblem exists. Alon simplified and randomized his procedure and improved the\nbound to O(2^(k/8)) [Alo91]. Srinivasan presented in [Sri08] a variant that\nachieves a bound of essentially O(2^(k/4)). In [Mos08], we improved this to\nO(2^(k/2)). In the present paper, we give a randomized algorithm that finds a\nsatisfying assignment to every k-CNF formula in which each clause has a\nneighbourhood of at most the asymptotic optimum of 2^(k-5)-1 other clauses and\nthat runs in expected time polynomial in the size of the formula, irrespective\nof k. If k is considered a constant, we can also give a deterministic variant.\nIn contrast to all previous approaches, our analysis does not anymore invoke\nthe standard non-constructive versions of the Local Lemma and can therefore be\nconsidered an alternative, constructive proof of it."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.4934v1", 
    "other_authors": "Marek Cygan, Lukasz Kowalik, Marcin Pilipczuk, Mateusz Wykurz", 
    "title": "Exponential-Time Approximation of Hard Problems", 
    "arxiv-id": "0810.4934v1", 
    "author": "Mateusz Wykurz", 
    "publish": "2008-10-27T20:18:00Z", 
    "summary": "We study optimization problems that are neither approximable in polynomial\ntime (at least with a constant factor) nor fixed parameter tractable, under\nwidely believed complexity assumptions. Specifically, we focus on Maximum\nIndependent Set, Vertex Coloring, Set Cover, and Bandwidth.\n  In recent years, many researchers design exact exponential-time algorithms\nfor these and other hard problems. The goal is getting the time complexity\nstill of order $O(c^n)$, but with the constant $c$ as small as possible. In\nthis work we extend this line of research and we investigate whether the\nconstant $c$ can be made even smaller when one allows constant factor\napproximation. In fact, we describe a kind of approximation schemes --\ntrade-offs between approximation factor and the time complexity.\n  We study two natural approaches. The first approach consists of designing a\nbacktracking algorithm with a small search tree. We present one result of that\nkind: a $(4r-1)$-approximation of Bandwidth in time $O^*(2^{n/r})$, for any\npositive integer $r$.\n  The second approach uses general transformations from exponential-time exact\nalgorithms to approximations that are faster but still exponential-time. For\nexample, we show that for any reduction rate $r$, one can transform any\n$O^*(c^n)$-time algorithm for Set Cover into a $(1+\\ln r)$-approximation\nalgorithm running in time $O^*(c^{n/r})$. We believe that results of that kind\nextend the applicability of exact algorithms for NP-hard problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.5263v1", 
    "other_authors": "Rahul Sami, Andy Twigg", 
    "title": "Lower bounds for distributed markov chain problems", 
    "arxiv-id": "0810.5263v1", 
    "author": "Andy Twigg", 
    "publish": "2008-10-29T12:52:59Z", 
    "summary": "We study the worst-case communication complexity of distributed algorithms\ncomputing a path problem based on stationary distributions of random walks in a\nnetwork $G$ with the caveat that $G$ is also the communication network. The\nproblem is a natural generalization of shortest path lengths to expected path\nlengths, and represents a model used in many practical applications such as\npagerank and eigentrust as well as other problems involving Markov chains\ndefined by networks.\n  For the problem of computing a single stationary probability, we prove an\n$\\Omega(n^2 \\log n)$ bits lower bound; the trivial centralized algorithm costs\n$O(n^3)$ bits and no known algorithm beats this. We also prove lower bounds for\nthe related problems of approximately computing the stationary probabilities,\ncomputing only the ranking of the nodes, and computing the node with maximal\nrank. As a corollary, we obtain lower bounds for labelling schemes for the\nhitting time between two nodes."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.08.027", 
    "link": "http://arxiv.org/pdf/0810.5477v1", 
    "other_authors": "Andrew Twigg", 
    "title": "Worst-case time decremental connectivity and k-edge witness", 
    "arxiv-id": "0810.5477v1", 
    "author": "Andrew Twigg", 
    "publish": "2008-10-30T12:15:33Z", 
    "summary": "We give a simple algorithm for decremental graph connectivity that handles\nedge deletions in worst-case time $O(k \\log n)$ and connectivity queries in\n$O(\\log k)$, where $k$ is the number of edges deleted so far, and uses\nworst-case space $O(m^2)$. We use this to give an algorithm for $k$-edge\nwitness (``does the removal of a given set of $k$ edges disconnect two vertices\n$u,v$?'') with worst-case time $O(k^2 \\log n)$ and space $O(k^2 n^2)$. For $k =\no(\\sqrt{n})$ these improve the worst-case $O(\\sqrt{n})$ bound for deletion due\nto Eppstein et al. We also give a decremental connectivity algorithm using\n$O(n^2 \\log n / \\log \\log n)$ space, whose time complexity depends on the\ntoughness and independence number of the input graph. Finally, we show how to\nconstruct a distributed data structure for \\kvw by giving a labeling scheme.\nThis is the first data structure for \\kvw that can efficiently distributed\nwithout just giving each vertex a copy of the whole structure. Its complexity\ndepends on being able to construct a linear layout with good properties."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.2572v2", 
    "other_authors": "Jean Cardinal, Samuel Fiorini, Gwena\u00ebl Joret, Rapha\u00ebl M. Jungers, J. Ian Munro", 
    "title": "An Efficient Algorithm for Partial Order Production", 
    "arxiv-id": "0811.2572v2", 
    "author": "J. Ian Munro", 
    "publish": "2008-11-17T16:23:45Z", 
    "summary": "We consider the problem of partial order production: arrange the elements of\nan unknown totally ordered set T into a target partially ordered set S, by\ncomparing a minimum number of pairs in T. Special cases include sorting by\ncomparisons, selection, multiple selection, and heap construction.\n  We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the\nworst case. Here, n denotes the size of the ground sets, and ITLB denotes a\nnatural information-theoretic lower bound on the number of comparisons needed\nto produce the target partial order.\n  Our approach is to replace the target partial order by a weak order (that is,\na partial order with a layered structure) extending it, without increasing the\ninformation theoretic lower bound too much. We then solve the problem by\napplying an efficient multiple selection algorithm. The overall complexity of\nour algorithm is polynomial. This answers a question of Yao (SIAM J. Comput.\n18, 1989).\n  We base our analysis on the entropy of the target partial order, a quantity\nthat can be efficiently computed and provides a good estimate of the\ninformation-theoretic lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3062v1", 
    "other_authors": "Zhewei Wei, Ke Yi, Qin Zhang", 
    "title": "Dynamic External Hashing: The Limit of Buffering", 
    "arxiv-id": "0811.3062v1", 
    "author": "Qin Zhang", 
    "publish": "2008-11-19T08:11:14Z", 
    "summary": "Hash tables are one of the most fundamental data structures in computer\nscience, in both theory and practice. They are especially useful in external\nmemory, where their query performance approaches the ideal cost of just one\ndisk access. Knuth gave an elegant analysis showing that with some simple\ncollision resolution strategies such as linear probing or chaining, the\nexpected average number of disk I/Os of a lookup is merely $1+1/2^{\\Omega(b)}$,\nwhere each I/O can read a disk block containing $b$ items. Inserting a new item\ninto the hash table also costs $1+1/2^{\\Omega(b)}$ I/Os, which is again almost\nthe best one can do if the hash table is entirely stored on disk. However, this\nassumption is unrealistic since any algorithm operating on an external hash\ntable must have some internal memory (at least $\\Omega(1)$ blocks) to work\nwith. The availability of a small internal memory buffer can dramatically\nreduce the amortized insertion cost to $o(1)$ I/Os for many external memory\ndata structures. In this paper we study the inherent query-insertion tradeoff\nof external hash tables in the presence of a memory buffer. In particular, we\nshow that for any constant $c>1$, if the query cost is targeted at\n$1+O(1/b^{c})$ I/Os, then it is not possible to support insertions in less than\n$1-O(1/b^{\\frac{c-1}{4}})$ I/Os amortized, which means that the memory buffer\nis essentially useless. While if the query cost is relaxed to $1+O(1/b^{c})$\nI/Os for any constant $c<1$, there is a simple dynamic hash table with $o(1)$\ninsertion cost. These results also answer the open question recently posed by\nJensen and Pagh."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3448v2", 
    "other_authors": "William F. Gilreath", 
    "title": "Binar Sort: A Linear Generalized Sorting Algorithm", 
    "arxiv-id": "0811.3448v2", 
    "author": "William F. Gilreath", 
    "publish": "2008-11-21T01:38:09Z", 
    "summary": "Sorting is a common and ubiquitous activity for computers. It is not\nsurprising that there exist a plethora of sorting algorithms. For all the\nsorting algorithms, it is an accepted performance limit that sorting algorithms\nare linearithmic or O(N lg N). The linearithmic lower bound in performance\nstems from the fact that the sorting algorithms use the ordering property of\nthe data. The sorting algorithm uses comparison by the ordering property to\narrange the data elements from an initial permutation into a sorted\npermutation.\n  Linear O(N) sorting algorithms exist, but use a priori knowledge of the data\nto use a specific property of the data and thus have greater performance. In\ncontrast, the linearithmic sorting algorithms are generalized by using a\nuniversal property of data-comparison, but have a linearithmic performance\nlower bound. The trade-off in sorting algorithms is generality for performance\nby the chosen property used to sort the data elements.\n  A general-purpose, linear sorting algorithm in the context of the trade-off\nof performance for generality at first consideration seems implausible. But,\nthere is an implicit assumption that only the ordering property is universal.\nBut, as will be discussed and examined, it is not the only universal property\nfor data elements. The binar sort is a general-purpose sorting algorithm that\nuses this other universal property to sort linearly."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3449v1", 
    "other_authors": "William F. Gilreath", 
    "title": "Binar Shuffle Algorithm: Shuffling Bit by Bit", 
    "arxiv-id": "0811.3449v1", 
    "author": "William F. Gilreath", 
    "publish": "2008-11-21T01:45:50Z", 
    "summary": "Frequently, randomly organized data is needed to avoid an anomalous operation\nof other algorithms and computational processes. An analogy is that a deck of\ncards is ordered within the pack, but before a game of poker or solitaire the\ndeck is shuffled to create a random permutation. Shuffling is used to assure\nthat an aggregate of data elements for a sequence S is randomly arranged, but\navoids an ordered or partially ordered permutation.\n  Shuffling is the process of arranging data elements into a random\npermutation. The sequence S as an aggregation of N data elements, there are N!\npossible permutations. For the large number of possible permutations, two of\nthe possible permutations are for a sorted or ordered placement of data\nelements--both an ascending and descending sorted permutation. Shuffling must\navoid inadvertently creating either an ascending or descending permutation.\n  Shuffling is frequently coupled to another algorithmic function --\npseudo-random number generation. The efficiency and quality of the shuffle is\ndirectly dependent upon the random number generation algorithm utilized. A more\neffective and efficient method of shuffling is to use parameterization to\nconfigure the shuffle, and to shuffle into sub-arrays by utilizing the encoding\nof the data elements. The binar shuffle algorithm uses the encoding of the data\nelements and parameterization to avoid any direct coupling to a random number\ngeneration algorithm, but still remain a linear O(N) shuffle algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3490v2", 
    "other_authors": "Philip Bille", 
    "title": "Faster Approximate String Matching for Short Patterns", 
    "arxiv-id": "0811.3490v2", 
    "author": "Philip Bille", 
    "publish": "2008-11-21T08:52:59Z", 
    "summary": "We study the classical approximate string matching problem, that is, given\nstrings $P$ and $Q$ and an error threshold $k$, find all ending positions of\nsubstrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$\nhave lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with\nword size $w \\geq \\log n$ we present an algorithm using time $$ O(nk \\cdot\n\\min(\\frac{\\log^2 m}{\\log n},\\frac{\\log^2 m\\log w}{w}) + n) $$ When $P$ is\nshort, namely, $m = 2^{o(\\sqrt{\\log n})}$ or $m = 2^{o(\\sqrt{w/\\log w})}$ this\nimproves the previously best known time bounds for the problem. The result is\nachieved using a novel implementation of the Landau-Vishkin algorithm based on\ntabulation and word-level parallelism."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3602v1", 
    "other_authors": "Travis Gagie, Marek Karpinski, Yakov Nekrich", 
    "title": "Low-Memory Adaptive Prefix Coding", 
    "arxiv-id": "0811.3602v1", 
    "author": "Yakov Nekrich", 
    "publish": "2008-11-21T18:23:00Z", 
    "summary": "In this paper we study the adaptive prefix coding problem in cases where the\nsize of the input alphabet is large. We present an online prefix coding\nalgorithm that uses $O(\\sigma^{1 / \\lambda + \\epsilon}) $ bits of space for any\nconstants $\\eps>0$, $\\lambda>1$, and encodes the string of symbols in $O(\\log\n\\log \\sigma)$ time per symbol \\emph{in the worst case}, where $\\sigma$ is the\nsize of the alphabet. The upper bound on the encoding length is $\\lambda n H\n(s) +(\\lambda \\ln 2 + 2 + \\epsilon) n + O (\\sigma^{1 / \\lambda} \\log^2 \\sigma)$\nbits."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0811.3779v1", 
    "other_authors": "Reid Andersen, Yuval Peres", 
    "title": "Finding Sparse Cuts Locally Using Evolving Sets", 
    "arxiv-id": "0811.3779v1", 
    "author": "Yuval Peres", 
    "publish": "2008-11-23T22:39:38Z", 
    "summary": "A {\\em local graph partitioning algorithm} finds a set of vertices with small\nconductance (i.e. a sparse cut) by adaptively exploring part of a large graph\n$G$, starting from a specified vertex. For the algorithm to be local, its\ncomplexity must be bounded in terms of the size of the set that it outputs,\nwith at most a weak dependence on the number $n$ of vertices in $G$. Previous\nlocal partitioning algorithms find sparse cuts using random walks and\npersonalized PageRank. In this paper, we introduce a randomized local\npartitioning algorithm that finds a sparse cut by simulating the {\\em\nvolume-biased evolving set process}, which is a Markov chain on sets of\nvertices. We prove that for any set of vertices $A$ that has conductance at\nmost $\\phi$, for at least half of the starting vertices in $A$ our algorithm\nwill output (with probability at least half), a set of conductance\n$O(\\phi^{1/2} \\log^{1/2} n)$. We prove that for a given run of the algorithm,\nthe expected ratio between its computational complexity and the volume of the\nset that it outputs is $O(\\phi^{-1/2} polylog(n))$. In comparison, the best\nprevious local partitioning algorithm, due to Andersen, Chung, and Lang, has\nthe same approximation guarantee, but a larger ratio of $O(\\phi^{-1}\npolylog(n))$ between the complexity and output volume. Using our local\npartitioning algorithm as a subroutine, we construct a fast algorithm for\nfinding balanced cuts. Given a fixed value of $\\phi$, the resulting algorithm\nhas complexity $O((m+n\\phi^{-1/2}) polylog(n))$ and returns a cut with\nconductance $O(\\phi^{1/2} \\log^{1/2} n)$ and volume at least $v_{\\phi}/2$,\nwhere $v_{\\phi}$ is the largest volume of any set with conductance at most\n$\\phi$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.0146v4", 
    "other_authors": "Vladimir Pestov", 
    "title": "Lower Bounds on Performance of Metric Tree Indexing Schemes for Exact   Similarity Search in High Dimensions", 
    "arxiv-id": "0812.0146v4", 
    "author": "Vladimir Pestov", 
    "publish": "2008-11-30T15:17:22Z", 
    "summary": "Within a mathematically rigorous model, we analyse the curse of\ndimensionality for deterministic exact similarity search in the context of\npopular indexing schemes: metric trees. The datasets $X$ are sampled randomly\nfrom a domain $\\Omega$, equipped with a distance, $\\rho$, and an underlying\nprobability distribution, $\\mu$. While performing an asymptotic analysis, we\nsend the intrinsic dimension $d$ of $\\Omega$ to infinity, and assume that the\nsize of a dataset, $n$, grows superpolynomially yet subexponentially in $d$.\nExact similarity search refers to finding the nearest neighbour in the dataset\n$X$ to a query point $\\omega\\in\\Omega$, where the query points are subject to\nthe same probability distribution $\\mu$ as datapoints. Let $\\mathscr F$ denote\na class of all 1-Lipschitz functions on $\\Omega$ that can be used as decision\nfunctions in constructing a hierarchical metric tree indexing scheme. Suppose\nthe VC dimension of the class of all sets $\\{\\omega\\colon f(\\omega)\\geq a\\}$,\n$a\\in\\R$ is $o(n^{1/4}/\\log^2n)$. (In view of a 1995 result of Goldberg and\nJerrum, even a stronger complexity assumption $d^{O(1)}$ is reasonable.) We\ndeduce the $\\Omega(n^{1/4})$ lower bound on the expected average case\nperformance of hierarchical metric-tree based indexing schemes for exact\nsimilarity search in $(\\Omega,X)$. In paricular, this bound is superpolynomial\nin $d$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.0209v1", 
    "other_authors": "Ke Yi, Qin Zhang", 
    "title": "Optimal Tracking of Distributed Heavy Hitters and Quantiles", 
    "arxiv-id": "0812.0209v1", 
    "author": "Qin Zhang", 
    "publish": "2008-12-01T03:51:12Z", 
    "summary": "We consider the the problem of tracking heavy hitters and quantiles in the\ndistributed streaming model. The heavy hitters and quantiles are two important\nstatistics for characterizing a data distribution. Let $A$ be a multiset of\nelements, drawn from the universe $U=\\{1,...,u\\}$. For a given $0 \\le \\phi \\le\n1$, the $\\phi$-heavy hitters are those elements of $A$ whose frequency in $A$\nis at least $\\phi |A|$; the $\\phi$-quantile of $A$ is an element $x$ of $U$\nsuch that at most $\\phi|A|$ elements of $A$ are smaller than $A$ and at most\n$(1-\\phi)|A|$ elements of $A$ are greater than $x$. Suppose the elements of $A$\nare received at $k$ remote {\\em sites} over time, and each of the sites has a\ntwo-way communication channel to a designated {\\em coordinator}, whose goal is\nto track the set of $\\phi$-heavy hitters and the $\\phi$-quantile of $A$\napproximately at all times with minimum communication. We give tracking\nalgorithms with worst-case communication cost $O(k/\\eps \\cdot \\log n)$ for both\nproblems, where $n$ is the total number of items in $A$, and $\\eps$ is the\napproximation error. This substantially improves upon the previous known\nalgorithms. We also give matching lower bounds on the communication costs for\nboth problems, showing that our algorithms are optimal. We also consider a more\ngeneral version of the problem where we simultaneously track the\n$\\phi$-quantiles for all $0 \\le \\phi \\le 1$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090759860", 
    "link": "http://arxiv.org/pdf/0812.1012v3", 
    "other_authors": "Sudipto Guha, Kamesh Munagala", 
    "title": "Adaptive Uncertainty Resolution in Bayesian Combinatorial Optimization   Problems", 
    "arxiv-id": "0812.1012v3", 
    "author": "Kamesh Munagala", 
    "publish": "2008-12-04T19:48:16Z", 
    "summary": "In several applications such as databases, planning, and sensor networks,\nparameters such as selectivity, load, or sensed values are known only with some\nassociated uncertainty. The performance of such a system (as captured by some\nobjective function over the parameters) is significantly improved if some of\nthese parameters can be probed or observed. In a resource constrained\nsituation, deciding which parameters to observe in order to optimize system\nperformance itself becomes an interesting and important optimization problem.\nThis general problem is the focus of this paper.\n  One of the most important considerations in this framework is whether\nadaptivity is required for the observations. Adaptive observations introduce\nblocking or sequential operations in the system whereas non-adaptive\nobservations can be performed in parallel. One of the important questions in\nthis regard is to characterize the benefit of adaptivity for probes and\nobservation.\n  We present general techniques for designing constant factor approximations to\nthe optimal observation schemes for several widely used scheduling and metric\nobjective functions. We show a unifying technique that relates this\noptimization problem to the outlier version of the corresponding deterministic\noptimization. By making this connection, our technique shows constant factor\nupper bounds for the benefit of adaptivity of the observation schemes. We show\nthat while probing yields significant improvement in the objective function,\nbeing adaptive about the probing is not beneficial beyond constant factors."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-74061-2_12", 
    "link": "http://arxiv.org/pdf/0812.2011v1", 
    "other_authors": "J\u00e9r\u00f4me Leroux, Gregoire Sutre", 
    "title": "Accelerated Data-Flow Analysis", 
    "arxiv-id": "0812.2011v1", 
    "author": "Gregoire Sutre", 
    "publish": "2008-12-10T20:08:08Z", 
    "summary": "Acceleration in symbolic verification consists in computing the exact effect\nof some control-flow loops in order to speed up the iterative fix-point\ncomputation of reachable states. Even if no termination guarantee is provided\nin theory, successful results were obtained in practice by different tools\nimplementing this framework. In this paper, the acceleration framework is\nextended to data-flow analysis. Compared to a classical\nwidening/narrowing-based abstract interpretation, the loss of precision is\ncontrolled here by the choice of the abstract domain and does not depend on the\nway the abstract value is computed. Our approach is geared towards precision,\nbut we don't loose efficiency on the way. Indeed, we provide a cubic-time\nacceleration-based algorithm for solving interval constraints with full\nmultiplication."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2014v1", 
    "other_authors": "J\u00e9r\u00f4me Leroux", 
    "title": "Convex Hull of Arithmetic Automata", 
    "arxiv-id": "0812.2014v1", 
    "author": "J\u00e9r\u00f4me Leroux", 
    "publish": "2008-12-10T20:33:27Z", 
    "summary": "Arithmetic automata recognize infinite words of digits denoting\ndecompositions of real and integer vectors. These automata are known expressive\nand efficient enough to represent the whole set of solutions of complex linear\nconstraints combining both integral and real variables. In this paper, the\nclosed convex hull of arithmetic automata is proved rational polyhedral.\nMoreover an algorithm computing the linear constraints defining these convex\nset is provided. Such an algorithm is useful for effectively extracting\ngeometrical properties of the whole set of solutions of complex constraints\nsymbolically represented by arithmetic automata."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2599v1", 
    "other_authors": "Raghunandan H. Keshavan, Andrea Montanari, Sewoong Oh", 
    "title": "Learning Low Rank Matrices from O(n) Entries", 
    "arxiv-id": "0812.2599v1", 
    "author": "Sewoong Oh", 
    "publish": "2008-12-14T18:30:44Z", 
    "summary": "How many random entries of an n by m, rank r matrix are necessary to\nreconstruct the matrix within an accuracy d? We address this question in the\ncase of a random matrix with bounded rank, whereby the observed entries are\nchosen uniformly at random. We prove that, for any d>0, C(r,d)n observations\nare sufficient. Finally we discuss the question of reconstructing the matrix\nefficiently, and demonstrate through extensive simulations that this task can\nbe accomplished in nPoly(log n) operations, for small rank."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2775v3", 
    "other_authors": "Johannes Fischer", 
    "title": "Optimal Succinctness for Range Minimum Queries", 
    "arxiv-id": "0812.2775v3", 
    "author": "Johannes Fischer", 
    "publish": "2008-12-15T12:03:31Z", 
    "summary": "For a static array A of n ordered objects, a range minimum query asks for the\nposition of the minimum between two specified array indices. We show how to\npreprocess A into a scheme of size 2n+o(n) bits that allows to answer range\nminimum queries on A in constant time. This space is asymptotically optimal in\nthe important setting where access to A is not permitted after the\npreprocessing step. Our scheme can be computed in linear time, using only n +\no(n) additional bits at construction time. In interesting by-product is that we\nalso improve on LCA-computation in BPS- or DFUDS-encoded trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2851v2", 
    "other_authors": "Amr Elmasry", 
    "title": "The Violation Heap: A Relaxed Fibonacci-Like Heap", 
    "arxiv-id": "0812.2851v2", 
    "author": "Amr Elmasry", 
    "publish": "2008-12-15T16:16:58Z", 
    "summary": "We give a priority queue that achieves the same amortized bounds as Fibonacci\nheaps. Namely, find-min requires O(1) worst-case time, insert, meld and\ndecrease-key require O(1) amortized time, and delete-min requires $O(\\log n)$\namortized time. Our structure is simple and promises an efficient practical\nbehavior when compared to other known Fibonacci-like heaps. The main idea\nbehind our construction is to propagate rank updates instead of performing\ncascaded cuts following a decrease-key operation, allowing for a relaxed\nstructure."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.2868v2", 
    "other_authors": "Pawel Gawrychowski, Travis Gagie", 
    "title": "Minimax Trees in Linear Time", 
    "arxiv-id": "0812.2868v2", 
    "author": "Travis Gagie", 
    "publish": "2008-12-15T17:15:51Z", 
    "summary": "A minimax tree is similar to a Huffman tree except that, instead of\nminimizing the weighted average of the leaves' depths, it minimizes the maximum\nof any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees\nand gave a Huffman-like, $\\Oh{n \\log n}$-time algorithm for building them.\nDrmota and Szpankowski (2002) gave another $\\Oh{n \\log n}$-time algorithm,\nwhich checks the Kraft Inequality in each step of a binary search. In this\npaper we show how Drmota and Szpankowski's algorithm can be made to run in\nlinear time on a word RAM with (\\Omega (\\log n))-bit words. We also discuss how\nour solution applies to problems in data compression, group testing and circuit\ndesign."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.3702v1", 
    "other_authors": "Michael W. Mahoney, Lek-Heng Lim, Gunnar E. Carlsson", 
    "title": "Algorithmic and Statistical Challenges in Modern Large-Scale Data   Analysis are the Focus of MMDS 2008", 
    "arxiv-id": "0812.3702v1", 
    "author": "Gunnar E. Carlsson", 
    "publish": "2008-12-19T03:53:03Z", 
    "summary": "The 2008 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2008),\nsponsored by the NSF, DARPA, LinkedIn, and Yahoo!, was held at Stanford\nUniversity, June 25--28. The goals of MMDS 2008 were (1) to explore novel\ntechniques for modeling and analyzing massive, high-dimensional, and\nnonlinearly-structured scientific and internet data sets; and (2) to bring\ntogether computer scientists, statisticians, mathematicians, and data analysis\npractitioners to promote cross-fertilization of ideas."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4293v2", 
    "other_authors": "Christos Boutsidis, Michael W. Mahoney, Petros Drineas", 
    "title": "An Improved Approximation Algorithm for the Column Subset Selection   Problem", 
    "arxiv-id": "0812.4293v2", 
    "author": "Petros Drineas", 
    "publish": "2008-12-22T21:16:55Z", 
    "summary": "We consider the problem of selecting the best subset of exactly $k$ columns\nfrom an $m \\times n$ matrix $A$. We present and analyze a novel two-stage\nalgorithm that runs in $O(\\min\\{mn^2,m^2n\\})$ time and returns as output an $m\n\\times k$ matrix $C$ consisting of exactly $k$ columns of $A$. In the first\n(randomized) stage, the algorithm randomly selects $\\Theta(k \\log k)$ columns\naccording to a judiciously-chosen probability distribution that depends on\ninformation in the top-$k$ right singular subspace of $A$. In the second\n(deterministic) stage, the algorithm applies a deterministic column-selection\nprocedure to select and return exactly $k$ columns from the set of columns\nselected in the first stage. Let $C$ be the $m \\times k$ matrix containing\nthose $k$ columns, let $P_C$ denote the projection matrix onto the span of\nthose columns, and let $A_k$ denote the best rank-$k$ approximation to the\nmatrix $A$. Then, we prove that, with probability at least 0.8, $$ \\FNorm{A -\nP_CA} \\leq \\Theta(k \\log^{1/2} k) \\FNorm{A-A_k}. $$ This Frobenius norm bound\nis only a factor of $\\sqrt{k \\log k}$ worse than the best previously existing\nexistential result and is roughly $O(\\sqrt{k!})$ better than the best previous\nalgorithmic result for the Frobenius norm version of this Column Subset\nSelection Problem (CSSP). We also prove that, with probability at least 0.8, $$\n\\TNorm{A - P_CA} \\leq \\Theta(k \\log^{1/2} k)\\TNorm{A-A_k} +\n\\Theta(k^{3/4}\\log^{1/4}k)\\FNorm{A-A_k}. $$ This spectral norm bound is not\ndirectly comparable to the best previously existing bounds for the spectral\nnorm version of this CSSP. Our bound depends on $\\FNorm{A-A_k}$, whereas\nprevious results depend on $\\sqrt{n-k}\\TNorm{A-A_k}$; if these two quantities\nare comparable, then our bound is asymptotically worse by a $(k \\log k)^{1/4}$\nfactor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4442v1", 
    "other_authors": "Julia Chuzhoy, Sanjeev Khanna", 
    "title": "An $O(k^{3} log n)$-Approximation Algorithm for Vertex-Connectivity   Survivable Network Design", 
    "arxiv-id": "0812.4442v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2008-12-23T19:04:25Z", 
    "summary": "In the Survivable Network Design problem (SNDP), we are given an undirected\ngraph $G(V,E)$ with costs on edges, along with a connectivity requirement\n$r(u,v)$ for each pair $u,v$ of vertices. The goal is to find a minimum-cost\nsubset $E^*$ of edges, that satisfies the given set of pairwise connectivity\nrequirements. In the edge-connectivity version we need to ensure that there are\n$r(u,v)$ edge-disjoint paths for every pair $u, v$ of vertices, while in the\nvertex-connectivity version the paths are required to be vertex-disjoint. The\nedge-connectivity version of SNDP is known to have a 2-approximation. However,\nno non-trivial approximation algorithm has been known so far for the vertex\nversion of SNDP, except for special cases of the problem. We present an\nextremely simple algorithm to achieve an $O(k^3 \\log n)$-approximation for this\nproblem, where $k$ denotes the maximum connectivity requirement, and $n$\ndenotes the number of vertices. We also give a simple proof of the recently\ndiscovered $O(k^2 \\log n)$-approximation result for the single-source version\nof vertex-connectivity SNDP. We note that in both cases, our analysis in fact\nyields slightly better guarantees in that the $\\log n$ term in the\napproximation guarantee can be replaced with a $\\log \\tau$ term where $\\tau$\ndenotes the number of distinct vertices that participate in one or more pairs\nwith a positive connectivity requirement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4547v2", 
    "other_authors": "Christos Boutsidis, Petros Drineas", 
    "title": "Random Projections for the Nonnegative Least-Squares Problem", 
    "arxiv-id": "0812.4547v2", 
    "author": "Petros Drineas", 
    "publish": "2008-12-24T16:43:22Z", 
    "summary": "Constrained least-squares regression problems, such as the Nonnegative Least\nSquares (NNLS) problem, where the variables are restricted to take only\nnonnegative values, often arise in applications. Motivated by the recent\ndevelopment of the fast Johnson-Lindestrauss transform, we present a fast\nrandom projection type approximation algorithm for the NNLS problem. Our\nalgorithm employs a randomized Hadamard transform to construct a much smaller\nNNLS problem and solves this smaller problem using a standard NNLS solver. We\nprove that our approach finds a nonnegative solution vector that, with high\nprobability, is close to the optimum nonnegative solution in a relative error\napproximation sense. We experimentally evaluate our approach on a large\ncollection of term-document data and verify that it does offer considerable\nspeedups without a significant loss in accuracy. Our analysis is based on a\nnovel random projection type result that might be of independent interest. In\nparticular, given a tall and thin matrix $\\Phi \\in \\mathbb{R}^{n \\times d}$ ($n\n\\gg d$) and a vector $y \\in \\mathbb{R}^d$, we prove that the Euclidean length\nof $\\Phi y$ can be estimated very accurately by the Euclidean length of\n$\\tilde{\\Phi}y$, where $\\tilde{\\Phi}$ consists of a small subset of\n(appropriately rescaled) rows of $\\Phi$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0812.4919v1", 
    "other_authors": "D\u00e1niel Marx, Ildik\u00f3 Schlotter", 
    "title": "Obtaining a Planar Graph by Vertex Deletion", 
    "arxiv-id": "0812.4919v1", 
    "author": "Ildik\u00f3 Schlotter", 
    "publish": "2008-12-29T14:57:14Z", 
    "summary": "In the k-Apex problem the task is to find at most k vertices whose deletion\nmakes the given graph planar. The graphs for which there exists a solution form\na minor closed class of graphs, hence by the deep results of Robertson and\nSeymour, there is an O(n^3) time algorithm for every fixed value of k. However,\nthe proof is extremely complicated and the constants hidden by the big-O\nnotation are huge. Here we give a much simpler algorithm for this problem with\nquadratic running time, by iteratively reducing the input graph and then\napplying techniques for graphs of bounded treewidth."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0205v1", 
    "other_authors": "Deeparnab Chakrabarty, Julia Chuzhoy, Sanjeev Khanna", 
    "title": "On Allocating Goods to Maximize Fairness", 
    "arxiv-id": "0901.0205v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2009-01-02T01:24:26Z", 
    "summary": "Given a set of $m$ agents and a set of $n$ items, where agent $A$ has utility\n$u_{A,i}$ for item $i$, our goal is to allocate items to agents to maximize\nfairness. Specifically, the utility of an agent is the sum of its utilities for\nitems it receives, and we seek to maximize the minimum utility of any agent.\nWhile this problem has received much attention recently, its approximability\nhas not been well-understood thus far: the best known approximation algorithm\nachieves an $\\tilde{O}(\\sqrt{m})$-approximation, and in contrast, the best\nknown hardness of approximation stands at 2.\n  Our main result is an approximation algorithm that achieves an\n$\\tilde{O}(n^{\\eps})$ approximation for any $\\eps=\\Omega(\\log\\log n/\\log n)$ in\ntime $n^{O(1/\\eps)}$. In particular, we obtain poly-logarithmic approximation\nin quasi-polynomial time, and for any constant $\\eps > 0$, we obtain\n$O(n^{\\eps})$ approximation in polynomial time. An interesting aspect of our\nalgorithm is that we use as a building block a linear program whose integrality\ngap is $\\Omega(\\sqrt m)$. We bypass this obstacle by iteratively using the\nsolutions produced by the LP to construct new instances with significantly\nsmaller integrality gaps, eventually obtaining the desired approximation.\n  We also investigate the special case of the problem, where every item has a\nnon-zero utility for at most two agents. We show that even in this restricted\nsetting the problem is hard to approximate upto any factor better tha 2, and\nshow a factor $(2+\\eps)$-approximation algorithm running in time\n$poly(n,1/\\eps)$ for any $\\eps>0$. This special case can be cast as a graph\nedge orientation problem, and our algorithm can be viewed as a generalization\nof Eulerian orientations to weighted graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0501v2", 
    "other_authors": "Morten K\u00fchnrich, Stefan Schwoon, Ji\u0159\u00ed Srba, Stefan Kiefer", 
    "title": "Interprocedural Dataflow Analysis over Weight Domains with Infinite   Descending Chains", 
    "arxiv-id": "0901.0501v2", 
    "author": "Stefan Kiefer", 
    "publish": "2009-01-05T16:47:21Z", 
    "summary": "We study generalized fixed-point equations over idempotent semirings and\nprovide an efficient algorithm for the detection whether a sequence of Kleene's\niterations stabilizes after a finite number of steps. Previously known\napproaches considered only bounded semirings where there are no infinite\ndescending chains. The main novelty of our work is that we deal with semirings\nwithout the boundedness restriction. Our study is motivated by several\napplications from interprocedural dataflow analysis. We demonstrate how the\nreachability problem for weighted pushdown automata can be reduced to solving\nequations in the framework mentioned above and we describe a few applications\nto demonstrate its usability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.0930v2", 
    "other_authors": "Marc M\u00f6rig, Dieter Rautenbach, Michiel Smid, Jan Tusch", 
    "title": "An \u03a9(n log n) lower bound for computing the sum of even-ranked   elements", 
    "arxiv-id": "0901.0930v2", 
    "author": "Jan Tusch", 
    "publish": "2009-01-07T21:55:59Z", 
    "summary": "Given a sequence A of 2n real numbers, the Even-Rank-Sum problem asks for the\nsum of the n values that are at the even positions in the sorted order of the\nelements in A. We prove that, in the algebraic computation-tree model, this\nproblem has time complexity \\Theta(n log n). This solves an open problem posed\nby Michael Shamos at the Canadian Conference on Computational Geometry in 2008."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.1761v1", 
    "other_authors": "Beat Gfeller, Peter Sanders", 
    "title": "Towards Optimal Range Medians", 
    "arxiv-id": "0901.1761v1", 
    "author": "Peter Sanders", 
    "publish": "2009-01-13T14:46:50Z", 
    "summary": "We consider the following problem: given an unsorted array of $n$ elements,\nand a sequence of intervals in the array, compute the median in each of the\nsubarrays defined by the intervals. We describe a simple algorithm which uses\nO(n) space and needs $O(n\\log k + k\\log n)$ time to answer the first $k$\nqueries. This improves previous algorithms by a logarithmic factor and matches\na lower bound for $k=O(n)$.\n  Since the algorithm decomposes the range of element values rather than the\narray, it has natural generalizations to higher dimensional problems -- it\nreduces a range median query to a logarithmic number of range counting queries."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2645v2", 
    "other_authors": "Michel Habib, Vincent Limouzy", 
    "title": "On some simplicial elimination schemes for chordal graphs", 
    "arxiv-id": "0901.2645v2", 
    "author": "Vincent Limouzy", 
    "publish": "2009-01-17T15:23:29Z", 
    "summary": "We present here some results on particular elimination schemes for chordal\ngraphs, namely we show that for any chordal graph we can construct in linear\ntime a simplicial elimination scheme starting with a pending maximal clique\nattached via a minimal separator maximal (resp. minimal) under inclusion among\nall minimal separators."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2897v2", 
    "other_authors": "Pawel Gawrychowski, Artur Jez, Lukasz Jez", 
    "title": "Online validation of the pi and pi' failure functions", 
    "arxiv-id": "0901.2897v2", 
    "author": "Lukasz Jez", 
    "publish": "2009-01-19T20:12:21Z", 
    "summary": "Let pi_w denote the failure function of the Morris-Pratt algorithm for a word\nw. In this paper we study the following problem: given an integer array\nA[1..n], is there a word w over arbitrary alphabet such that A[i]=pi_w[i] for\nall i? Moreover, what is the minimum required cardinality of the alphabet? We\ngive a real time linear algorithm for this problem in the unit-cost RAM model\nwith \\Theta(log n) bits word size. Our algorithm returns a word w over minimal\nalphabet such that pi_w = A as well and uses just o(n) words of memory. Then we\nconsider function pi' instead of pi and give an online O(n log n) algorithm for\nthis case. This is the first polynomial algorithm for online version of this\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.2900v1", 
    "other_authors": "Manoj Gupta, Ankit Sharma", 
    "title": "An O(log(n)) Fully Dynamic Algorithm for Maximum matching in a tree", 
    "arxiv-id": "0901.2900v1", 
    "author": "Ankit Sharma", 
    "publish": "2009-01-19T17:30:28Z", 
    "summary": "In this paper, we have developed a fully-dynamic algorithm for maintaining\ncardinality of maximum-matching in a tree using the construction of top-trees.\nThe time complexities are as follows:\n  1. Initialization Time: $O(n(log(n)))$ to build the Top-tree. 2. Update Time:\n$O(log(n))$ 3. Query Time: O(1) to query the cardinality of maximum-matching\nand $O(log(n))$ to find if a particular edge is matched."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.4002v1", 
    "other_authors": "Giorgio Lucarelli, Ioannis Milis, Vangelis Th. Paschos", 
    "title": "Max Edge Coloring of Trees", 
    "arxiv-id": "0901.4002v1", 
    "author": "Vangelis Th. Paschos", 
    "publish": "2009-01-26T13:27:34Z", 
    "summary": "We study the weighted generalization of the edge coloring problem where the\nweight of each color class (matching) equals to the weight of its heaviest edge\nand the goal is to minimize the sum of the colors' weights. We present a\n3/2-approximation algorithm for trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0901.4201v1", 
    "other_authors": "Denis Lugiez, St\u00e9phane Martin", 
    "title": "Peer to Peer Optimistic Collaborative Editing on XML-like trees", 
    "arxiv-id": "0901.4201v1", 
    "author": "St\u00e9phane Martin", 
    "publish": "2009-01-27T09:09:52Z", 
    "summary": "Collaborative editing consists in editing a common document shared by several\nindependent sites. This may give rise to conficts when two different users\nperform simultaneous uncompatible operations. Centralized systems solve this\nproblem by using locks that prevent some modifications to occur and leave the\nresolution of confict to users. On the contrary, peer to peer (P2P) editing\ndoesn't allow locks and the optimistic approach uses a Integration\nTransformation IT that reconciliates the conficting operations and ensures\nconvergence (all copies are identical on each site). Two properties TP1 and\nTP2, relating the set of allowed operations Op and the transformation IT, have\nbeen shown to ensure the correctness of the process. The choice of the set Op\nis crucial to define an integration operation that satisfies TP1 and TP2. Many\nexisting algorithms don't satisfy these properties and are indeed incorrect\ni.e. convergence is not guaranteed. No algorithm enjoying both properties is\nknown for strings and little work has been done for XML trees in a pure P2P\nframework (that doesn't use time-stamps for instance). We focus on editing\nunranked unordered labeled trees, so-called XML-like trees that are considered\nfor instance in the Harmony pro ject. We show that no transformation satisfying\nTP1 and TP2 can exist for a first set of operations but we show that TP1 and\nTP2 hold for a richer set of operations. We show how to combine our approach\nwith any convergent editing process on strings (not necessarily based on\nintegration transformation) to get a convergent process."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.0140v2", 
    "other_authors": "Kook Jin Ahn, Sudipto Guha", 
    "title": "Graph Sparsification in the Semi-streaming Model", 
    "arxiv-id": "0902.0140v2", 
    "author": "Sudipto Guha", 
    "publish": "2009-02-01T16:50:53Z", 
    "summary": "Analyzing massive data sets has been one of the key motivations for studying\nstreaming algorithms. In recent years, there has been significant progress in\nanalysing distributions in a streaming setting, but the progress on graph\nproblems has been limited. A main reason for this has been the existence of\nlinear space lower bounds for even simple problems such as determining the\nconnectedness of a graph. However, in many new scenarios that arise from social\nand other interaction networks, the number of vertices is significantly less\nthan the number of edges. This has led to the formulation of the semi-streaming\nmodel where we assume that the space is (near) linear in the number of vertices\n(but not necessarily the edges), and the edges appear in an arbitrary (and\npossibly adversarial) order.\n  In this paper we focus on graph sparsification, which is one of the major\nbuilding blocks in a variety of graph algorithms. There has been a long history\nof (non-streaming) sampling algorithms that provide sparse graph approximations\nand it a natural question to ask if the sparsification can be achieved using a\nsmall space, and in addition using a single pass over the data? The question is\ninteresting from the standpoint of both theory and practice and we answer the\nquestion in the affirmative, by providing a one pass\n$\\tilde{O}(n/\\epsilon^{2})$ space algorithm that produces a sparsification that\napproximates each cut to a $(1+\\epsilon)$ factor. We also show that $\\Omega(n\n\\log \\frac1\\epsilon)$ space is necessary for a one pass streaming algorithm to\napproximate the min-cut, improving upon the $\\Omega(n)$ lower bound that arises\nfrom lower bounds for testing connectivity."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1038v1", 
    "other_authors": "J\u00e9r\u00e9my Barbay, Gonzalo Navarro", 
    "title": "Compressed Representations of Permutations, and Applications", 
    "arxiv-id": "0902.1038v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2009-02-06T09:48:32Z", 
    "summary": "We explore various techniques to compress a permutation $\\pi$ over n\nintegers, taking advantage of ordered subsequences in $\\pi$, while supporting\nits application $\\pi$(i) and the application of its inverse $\\pi^{-1}(i)$ in\nsmall time. Our compression schemes yield several interesting byproducts, in\nmany cases matching, improving or extending the best existing results on\napplications such as the encoding of a permutation in order to support iterated\napplications $\\pi^k(i)$ of it, of integer functions, and of inverted lists and\nsuffix arrays."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1260v1", 
    "other_authors": "Ho-Leung Chan, Jeff Edmonds, Tak-Wah Lam, Lap-Kei Lee, Alberto Marchetti-Spaccamela, Kirk Pruhs", 
    "title": "Nonclairvoyant Speed Scaling for Flow and Energy", 
    "arxiv-id": "0902.1260v1", 
    "author": "Kirk Pruhs", 
    "publish": "2009-02-07T18:02:32Z", 
    "summary": "We study online nonclairvoyant speed scaling to minimize total flow time plus\nenergy. We first consider the traditional model where the power function is P\n(s) = s\\^\\propto. We give a nonclairvoyant algorithm that is shown to be\nO(\\propto\\^3)-competitive. We then show an \\Omega(\\propto\\^(1/3-\\epsilon))\nlower bound on the competitive ratio of any nonclairvoyant algorithm. We also\nshow that there are power functions for which no nonclairvoyant algorithm can\nbe O(1)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1378v1", 
    "other_authors": "Yuval Emek, Pierre Fraigniaud, Amos Korman, Adi Rosen", 
    "title": "On the Additive Constant of the k-server Work Function Algorithm", 
    "arxiv-id": "0902.1378v1", 
    "author": "Adi Rosen", 
    "publish": "2009-02-09T07:47:19Z", 
    "summary": "We consider the Work Function Algorithm for the k-server problem. We show\nthat if the Work Function Algorithm is c-competitive, then it is also strictly\n(2c)-competitive. As a consequence of [Koutsoupias and Papadimitriou, JACM\n1995] this also shows that the Work Function Algorithm is strictly\n(4k-2)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1604v1", 
    "other_authors": "Eda Baykan, Monika Henzinger, Stefan F. Keller, Sebastian De Castelberg, Markus Kinzler", 
    "title": "A Comparison of Techniques for Sampling Web Pages", 
    "arxiv-id": "0902.1604v1", 
    "author": "Markus Kinzler", 
    "publish": "2009-02-10T08:44:14Z", 
    "summary": "As the World Wide Web is growing rapidly, it is getting increasingly\nchallenging to gather representative information about it. Instead of crawling\nthe web exhaustively one has to resort to other techniques like sampling to\ndetermine the properties of the web. A uniform random sample of the web would\nbe useful to determine the percentage of web pages in a specific language, on a\ntopic or in a top level domain. Unfortunately, no approach has been shown to\nsample the web pages in an unbiased way. Three promising web sampling\nalgorithms are based on random walks. They each have been evaluated\nindividually, but making a comparison on different data sets is not possible.\nWe directly compare these algorithms in this paper. We performed three random\nwalks on the web under the same conditions and analyzed their outcomes in\ndetail. We discuss the strengths and the weaknesses of each algorithm and\npropose improvements based on experimental results."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-540-69166-2_4", 
    "link": "http://arxiv.org/pdf/0902.1605v1", 
    "other_authors": "Nicole Schweikardt", 
    "title": "Lower Bounds for Multi-Pass Processing of Multiple Data Streams", 
    "arxiv-id": "0902.1605v1", 
    "author": "Nicole Schweikardt", 
    "publish": "2009-02-10T08:46:27Z", 
    "summary": "This paper gives a brief overview of computation models for data stream\nprocessing, and it introduces a new model for multi-pass processing of multiple\nstreams, the so-called mp2s-automata. Two algorithms for solving the set\ndisjointness problem wi th these automata are presented. The main technical\ncontribution of this paper is the proof of a lower bound on the size of memory\nand the number of heads that are required for solvin g the set disjointness\nproblem with mp2s-automata."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.1693v4", 
    "other_authors": "Markus Bl\u00e4ser, Christian Hoffmann", 
    "title": "Fast Evaluation of Interlace Polynomials on Graphs of Bounded Treewidth", 
    "arxiv-id": "0902.1693v4", 
    "author": "Christian Hoffmann", 
    "publish": "2009-02-10T16:41:35Z", 
    "summary": "We consider the multivariate interlace polynomial introduced by Courcelle\n(2008), which generalizes several interlace polynomials defined by Arratia,\nBollobas, and Sorkin (2004) and by Aigner and van der Holst (2004). We present\nan algorithm to evaluate the multivariate interlace polynomial of a graph with\nn vertices given a tree decomposition of the graph of width k. The best\npreviously known result (Courcelle 2008) employs a general logical framework\nand leads to an algorithm with running time f(k)*n, where f(k) is doubly\nexponential in k. Analyzing the GF(2)-rank of adjacency matrices in the context\nof tree decompositions, we give a faster and more direct algorithm. Our\nalgorithm uses 2^{3k^2+O(k)}*n arithmetic operations and can be efficiently\nimplemented in parallel."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.1792v3", 
    "other_authors": "Shipra Agrawal, Yichuan Ding, Amin Saberi, Yinyu Ye", 
    "title": "Correlation Robust Stochastic Optimization", 
    "arxiv-id": "0902.1792v3", 
    "author": "Yinyu Ye", 
    "publish": "2009-02-11T01:51:42Z", 
    "summary": "We consider a robust model proposed by Scarf, 1958, for stochastic\noptimization when only the marginal probabilities of (binary) random variables\nare given, and the correlation between the random variables is unknown. In the\nrobust model, the objective is to minimize expected cost against worst possible\njoint distribution with those marginals. We introduce the concept of\ncorrelation gap to compare this model to the stochastic optimization model that\nignores correlations and minimizes expected cost under independent Bernoulli\ndistribution. We identify a class of functions, using concepts of summable cost\nsharing schemes from game theory, for which the correlation gap is well-bounded\nand the robust model can be approximated closely by the independent\ndistribution model. As a result, we derive efficient approximation factors for\nmany popular cost functions, like submodular functions, facility location, and\nSteiner tree. As a byproduct, our analysis also yields some new results in the\nareas of social welfare maximization and existence of Walrasian equilibria,\nwhich may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2209v3", 
    "other_authors": "Christoph Durr, Lukasz Jez, Nguyen Kim Thang", 
    "title": "Online Scheduling of Bounded Length Jobs to Maximize Throughput", 
    "arxiv-id": "0902.2209v3", 
    "author": "Nguyen Kim Thang", 
    "publish": "2009-02-12T21:24:57Z", 
    "summary": "We consider an online scheduling problem, motivated by the issues present at\nthe joints of networks using ATM and TCP/IP. Namely, IP packets have to broken\ndown to small ATM cells and sent out before their deadlines, but cells\ncorresponding to different packets can be interwoven. More formally, we\nconsider the online scheduling problem with preemptions, where each job j is\nrevealed at release time r_j, has processing time p_j, deadline d_j and weight\nw_j. A preempted job can be resumed at any time. The goal is to maximize the\ntotal weight of all jobs completed on time. Our main result are as follows: we\nprove that if all jobs have processing time exactly k, the deterministic\ncompetitive ratio is between 2.598 and 5, and when the processing times are at\nmost k, the deterministic competitive ratio is Theta(k/log k)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2648v1", 
    "other_authors": "Roberto Grossi, Alessio Orlandi, Rajeev Raman, S. Srinivasa Rao", 
    "title": "More Haste, Less Waste: Lowering the Redundancy in Fully Indexable   Dictionaries", 
    "arxiv-id": "0902.2648v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2009-02-16T10:14:08Z", 
    "summary": "We consider the problem of representing, in a compressed format, a bit-vector\n$S$ of $m$ bits with $n$ 1s, supporting the following operations, where $b \\in\n\\{0, 1 \\}$: $rank_b(S,i)$ returns the number of occurrences of bit $b$ in the\nprefix $S[1..i]$; $select_b(S,i)$ returns the position of the $i$th occurrence\nof bit $b$ in $S$. Such a data structure is called \\emph{fully indexable\ndictionary (FID)} [Raman et al.,2007], and is at least as powerful as\npredecessor data structures. Our focus is on space-efficient FIDs on the\n\\textsc{ram} model with word size $\\Theta(\\lg m)$ and constant time for all\noperations, so that the time cost is independent of the input size. Given the\nbitstring $S$ to be encoded, having length $m$ and containing $n$ ones, the\nminimal amount of information that needs to be stored is $B(n,m) = \\lceil \\log\n{{m}\\choose{n}} \\rceil$. The state of the art in building a FID for $S$ is\ngiven in [Patrascu,2008] using $B(m,n)+O(m / ((\\log m/ t) ^t)) + O(m^{3/4}) $\nbits, to support the operations in $O(t)$ time. Here, we propose a parametric\ndata structure exhibiting a time/space trade-off such that, for any real\nconstants $0 < \\delta \\leq 1/2$, $0 < \\eps \\leq 1$, and integer $s > 0$, it\nuses \\[ B(n,m) + O(n^{1+\\delta} + n (\\frac{m}{n^s})^\\eps) \\] bits and performs\nall the operations in time $O(s\\delta^{-1} + \\eps^{-1})$. The improvement is\ntwofold: our redundancy can be lowered parametrically and, fixing $s = O(1)$,\nwe get a constant-time FID whose space is $B(n,m) + O(m^\\eps/\\poly{n})$ bits,\nfor sufficiently large $m$. This is a significant improvement compared to the\nprevious bounds for the general case."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.2795v1", 
    "other_authors": "Chandra Chekuri, Nitish Korula", 
    "title": "A Graph Reduction Step Preserving Element-Connectivity and Applications", 
    "arxiv-id": "0902.2795v1", 
    "author": "Nitish Korula", 
    "publish": "2009-02-16T21:29:26Z", 
    "summary": "Given an undirected graph G=(V,E) and subset of terminals T \\subseteq V, the\nelement-connectivity of two terminals u,v \\in T is the maximum number of u-v\npaths that are pairwise disjoint in both edges and non-terminals V \\setminus T\n(the paths need not be disjoint in terminals). Element-connectivity is more\ngeneral than edge-connectivity and less general than vertex-connectivity. Hind\nand Oellermann gave a graph reduction step that preserves the global\nelement-connectivity of the graph. We show that this step also preserves local\nconnectivity, that is, all the pairwise element-connectivities of the\nterminals. We give two applications of this reduction step to connectivity and\nnetwork design problems:\n  1. Given a graph G and disjoint terminal sets T_1, T_2, ..., T_m, we seek a\nmaximum number of element-disjoint Steiner forests where each forest connects\neach T_i. We prove that if each T_i is k-element-connected then there exist\n\\Omega(\\frac{k}{\\log h \\log m}) element-disjoint Steiner forests, where h =\n|\\bigcup_i T_i|. If G is planar (or more generally, has fixed genus), we show\nthat there exist \\Omega(k) Steiner forests. Our proofs are constructive, giving\npoly-time algorithms to find these forests; these are the first non-trivial\nalgorithms for packing element-disjoint Steiner Forests.\n  2. We give a very short and intuitive proof of a spider-decomposition theorem\nof Chuzhoy and Khanna in the context of the single-sink k-vertex-connectivity\nproblem; this yields a simple and alternative analysis of an O(k \\log n)\napproximation.\n  Our results highlight the effectiveness of the element-connectivity reduction\nstep; we believe it will find more applications in the future."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0902.3121v1", 
    "other_authors": "Bernat Gacias, Christian Artigues, Pierre Lopez", 
    "title": "Parallel machine scheduling with precedence constraints and setup times", 
    "arxiv-id": "0902.3121v1", 
    "author": "Pierre Lopez", 
    "publish": "2009-02-18T14:00:16Z", 
    "summary": "This paper presents different methods for solving parallel machine scheduling\nproblems with precedence constraints and setup times between the jobs. Limited\ndiscrepancy search methods mixed with local search principles, dominance\nconditions and specific lower bounds are proposed. The proposed methods are\nevaluated on a set of randomly generated instances and compared with previous\nresults from the literature and those obtained with an efficient commercial\nsolver. We conclude that our propositions are quite competitive and our results\neven outperform other approaches in most cases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.0292v1", 
    "other_authors": "Khanh Do Ba, Huy L Nguyen, Huy N Nguyen, Ronitt Rubinfeld", 
    "title": "Sublinear Time Algorithms for Earth Mover's Distance", 
    "arxiv-id": "0904.0292v1", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2009-04-02T02:17:43Z", 
    "summary": "We study the problem of estimating the Earth Mover's Distance (EMD) between\nprobability distributions when given access only to samples. We give closeness\ntesters and additive-error estimators over domains in $[0, \\Delta]^d$, with\nsample complexities independent of domain size - permitting the testability\neven of continuous distributions over infinite domains. Instead, our algorithms\ndepend on other parameters, such as the diameter of the domain space, which may\nbe significantly smaller. We also prove lower bounds showing the dependencies\non these parameters to be essentially optimal. Additionally, we consider\nwhether natural classes of distributions exist for which there are algorithms\nwith better dependence on the dimension, and show that for highly clusterable\ndata, this is indeed the case. Lastly, we consider a variant of the EMD,\ndefined over tree metrics instead of the usual L1 metric, and give optimal\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.0352v2", 
    "other_authors": "Shlomi Dolev, Yuval Elovici, Rami Puzis, Polina Zilberman", 
    "title": "Incremental Deployment of Network Monitors Based on Group Betweenness   Centrality", 
    "arxiv-id": "0904.0352v2", 
    "author": "Polina Zilberman", 
    "publish": "2009-04-02T09:32:51Z", 
    "summary": "In many applications we are required to increase the deployment of a\ndistributed monitoring system on an evolving network. In this paper we present\na new method for finding candidate locations for additional deployment in the\nnetwork. This method is based on the Group Betweenness Centrality (GBC) measure\nthat is used to estimate the influence of a group of nodes over the information\nflow in the network. The new method assists in finding the location of k\nadditional monitors in the evolving network, such that the portion of\nadditional traffic covered is at least (1-1/e) of the optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.1002v1", 
    "other_authors": "Wolfgang Bein, Leah Epstein, Lawrence L. Larmore, John Noga", 
    "title": "A Program to Determine the Exact Competitive Ratio of List s-Batching   with Unit Jobs", 
    "arxiv-id": "0904.1002v1", 
    "author": "John Noga", 
    "publish": "2009-04-06T20:08:48Z", 
    "summary": "We consider the online list s-batch problem, where all the jobs have\nprocessing time 1 and we seek to minimize the sum of the completion times of\nthe jobs. We give a Java program which is used to verify that the\ncompetitiveness of this problem is 619/583."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.1705v1", 
    "other_authors": "Evripidis Bampis, Alexander Kononov, Giorgio Lucarelli, Ioannis Milis", 
    "title": "Bounded Max-Colorings of Graphs", 
    "arxiv-id": "0904.1705v1", 
    "author": "Ioannis Milis", 
    "publish": "2009-04-10T15:36:02Z", 
    "summary": "In a bounded max-coloring of a vertex/edge weighted graph, each color class\nis of cardinality at most $b$ and of weight equal to the weight of the heaviest\nvertex/edge in this class. The bounded max-vertex/edge-coloring problems ask\nfor such a coloring minimizing the sum of all color classes' weights.\n  In this paper we present complexity results and approximation algorithms for\nthose problems on general graphs, bipartite graphs and trees. We first show\nthat both problems are polynomial for trees, when the number of colors is\nfixed, and $H_b$ approximable for general graphs, when the bound $b$ is fixed.\nFor the bounded max-vertex-coloring problem, we show a 17/11-approximation\nalgorithm for bipartite graphs, a PTAS for trees as well as for bipartite\ngraphs when $b$ is fixed. For unit weights, we show that the known 4/3 lower\nbound for bipartite graphs is tight by providing a simple 4/3 approximation\nalgorithm. For the bounded max-edge-coloring problem, we prove approximation\nfactors of $3-2/\\sqrt{2b}$, for general graphs, $\\min\\{e, 3-2/\\sqrt{b}\\}$, for\nbipartite graphs, and 2, for trees. Furthermore, we show that this problem is\nNP-complete even for trees. This is the first complexity result for\nmax-coloring problems on trees."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.2027v1", 
    "other_authors": "Jelani Nelson, David P. Woodruff", 
    "title": "A Near-Optimal Algorithm for L1-Difference", 
    "arxiv-id": "0904.2027v1", 
    "author": "David P. Woodruff", 
    "publish": "2009-04-13T22:54:26Z", 
    "summary": "We give the first L_1-sketching algorithm for integer vectors which produces\nnearly optimal sized sketches in nearly linear time. This answers the first\nopen problem in the list of open problems from the 2006 IITK Workshop on\nAlgorithms for Data Streams. Specifically, suppose Alice receives a vector x in\n{-M,...,M}^n and Bob receives y in {-M,...,M}^n, and the two parties share\nrandomness. Each party must output a short sketch of their vector such that a\nthird party can later quickly recover a (1 +/- eps)-approximation to ||x-y||_1\nwith 2/3 probability given only the sketches. We give a sketching algorithm\nwhich produces O(eps^{-2}log(1/eps)log(nM))-bit sketches in O(n*log^2(nM))\ntime, independent of eps. The previous best known sketching algorithm for L_1\nis due to [Feigenbaum et al., SICOMP 2002], which achieved the optimal sketch\nlength of O(eps^{-2}log(nM)) bits but had a running time of O(n*log(nM)/eps^2).\nNotice that our running time is near-linear for every eps, whereas for\nsufficiently small values of eps, the running time of the previous algorithm\ncan be as large as quadratic. Like their algorithm, our sketching procedure\nalso yields a small-space, one-pass streaming algorithm which works even if the\nentries of x,y are given in arbitrary order."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.2129v1", 
    "other_authors": "Tamara Mchedlidze, Antonios Symvonis", 
    "title": "Crossing-Optimal Acyclic HP-Completion for Outerplanar st-Digraphs", 
    "arxiv-id": "0904.2129v1", 
    "author": "Antonios Symvonis", 
    "publish": "2009-04-14T14:29:56Z", 
    "summary": "Given an embedded planar acyclic digraph G, we define the problem of acyclic\nhamiltonian path completion with crossing minimization (Acyclic-HPCCM) to be\nthe problem of determining a hamiltonian path completion set of edges such\nthat, when these edges are embedded on G, they create the smallest possible\nnumber of edge crossings and turn G to a hamiltonian acyclic digraph. Our\nresults include: 1. We provide a characterization under which a planar\nst-digraph G is hamiltonian. 2. For an outerplanar st-digraph G, we define the\nst-polygon decomposition of G and, based on its properties, we develop a\nlinear-time algorithm that solves the Acyclic-HPCCM problem. 3. For the class\nof planar st-digraphs, we establish an equivalence between the Acyclic-HPCCM\nproblem and the problem of determining an upward 2-page topological book\nembedding with minimum number of spine crossings. We infer (based on this\nequivalence) for the class of outerplanar st-digraphs an upward topological\n2-page book embedding with minimum number of spine crossings. To the best of\nour knowledge, it is the first time that edge-crossing minimization is studied\nin conjunction with the acyclic hamiltonian completion problem and the first\ntime that an optimal algorithm with respect to spine crossing minimization is\npresented for upward topological book embeddings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.2576v1", 
    "other_authors": "Anna Adamaszek, Artur Czumaj, Andrzej Lingas", 
    "title": "PTAS for k-tour cover problem on the plane for moderately large values   of k", 
    "arxiv-id": "0904.2576v1", 
    "author": "Andrzej Lingas", 
    "publish": "2009-04-16T20:12:33Z", 
    "summary": "Let P be a set of n points in the Euclidean plane and let O be the origin\npoint in the plane. In the k-tour cover problem (called frequently the\ncapacitated vehicle routing problem), the goal is to minimize the total length\nof tours that cover all points in P, such that each tour starts and ends in O\nand covers at most k points from P.\n  The k-tour cover problem is known to be NP-hard. It is also known to admit\nconstant factor approximation algorithms for all values of k and even a\npolynomial-time approximation scheme (PTAS) for small values of k, i.e.,\nk=O(log n / log log n).\n  We significantly enlarge the set of values of k for which a PTAS is provable.\nWe present a new PTAS for all values of k <= 2^{log^{\\delta}n}, where \\delta =\n\\delta(\\epsilon). The main technical result proved in the paper is a novel\nreduction of the k-tour cover problem with a set of n points to a small set of\ninstances of the problem, each with O((k/\\epsilon)^O(1)) points."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.2728v1", 
    "other_authors": "Clemence Magnien, Matthieu Latapy, Michel Habib", 
    "title": "Fast Computation of Empirically Tight Bounds for the Diameter of Massive   Graphs", 
    "arxiv-id": "0904.2728v1", 
    "author": "Michel Habib", 
    "publish": "2009-04-17T15:44:49Z", 
    "summary": "The diameter of a graph is among its most basic parameters. Since a few\nyears, it moreover became a key issue to compute it for massive graphs in the\ncontext of complex network analysis. However, known algorithms, including the\nones producing approximate values, have too high a time and/or space complexity\nto be used in such cases. We propose here a new approach relying on very simple\nand fast algorithms that compute (upper and lower) bounds for the diameter. We\nshow empirically that, on various real-world cases representative of complex\nnetworks studied in the literature, the obtained bounds are very tight (and\neven equal in some cases). This leads to rigorous and very accurate estimations\nof the actual diameter in cases which were previously untractable in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9439-4", 
    "link": "http://arxiv.org/pdf/0904.3062v2", 
    "other_authors": "Miklos Csuros", 
    "title": "Approximate counting with a floating-point counter", 
    "arxiv-id": "0904.3062v2", 
    "author": "Miklos Csuros", 
    "publish": "2009-04-20T15:53:33Z", 
    "summary": "Memory becomes a limiting factor in contemporary applications, such as\nanalyses of the Webgraph and molecular sequences, when many objects need to be\ncounted simultaneously. Robert Morris [Communications of the ACM, 21:840--842,\n1978] proposed a probabilistic technique for approximate counting that is\nextremely space-efficient. The basic idea is to increment a counter containing\nthe value $X$ with probability $2^{-X}$. As a result, the counter contains an\napproximation of $\\lg n$ after $n$ probabilistic updates stored in $\\lg\\lg n$\nbits. Here we revisit the original idea of Morris, and introduce a binary\nfloating-point counter that uses a $d$-bit significand in conjunction with a\nbinary exponent. The counter yields a simple formula for an unbiased estimation\nof $n$ with a standard deviation of about $0.6\\cdot n2^{-d/2}$, and uses\n$d+\\lg\\lg n$ bits.\n  We analyze the floating-point counter's performance in a general framework\nthat applies to any probabilistic counter, and derive practical formulas to\nassess its accuracy."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3741v1", 
    "other_authors": "David Eppstein, Emma S. Spiro", 
    "title": "The h-Index of a Graph and its Application to Dynamic Subgraph   Statistics", 
    "arxiv-id": "0904.3741v1", 
    "author": "Emma S. Spiro", 
    "publish": "2009-04-23T18:37:36Z", 
    "summary": "We describe a data structure that maintains the number of triangles in a\ndynamic undirected graph, subject to insertions and deletions of edges and of\ndegree-zero vertices. More generally it can be used to maintain the number of\ncopies of each possible three-vertex subgraph in time O(h) per update, where h\nis the h-index of the graph, the maximum number such that the graph contains\n$h$ vertices of degree at least h. We also show how to maintain the h-index\nitself, and a collection of h high-degree vertices in the graph, in constant\ntime per update. Our data structure has applications in social network analysis\nusing the exponential random graph model (ERGM); its bound of O(h) time per\nedge is never worse than the Theta(sqrt m) time per edge necessary to list all\ntriangles in a static graph, and is strictly better for graphs obeying a power\nlaw degree distribution. In order to better understand the behavior of the\nh-index statistic and its implications for the performance of our algorithms,\nwe also study the behavior of the h-index on a set of 136 real-world networks."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3756v3", 
    "other_authors": "Wenliang Du, David Eppstein, Michael T. Goodrich, George S. Lueker", 
    "title": "On the Approximability of Geometric and Geographic Generalization and   the Min-Max Bin Covering Problem", 
    "arxiv-id": "0904.3756v3", 
    "author": "George S. Lueker", 
    "publish": "2009-04-23T21:06:58Z", 
    "summary": "We study the problem of abstracting a table of data about individuals so that\nno selection query can identify fewer than k individuals. We show that it is\nimpossible to achieve arbitrarily good polynomial-time approximations for a\nnumber of natural variations of the generalization technique, unless P = NP,\neven when the table has only a single quasi-identifying attribute that\nrepresents a geographic or unordered attribute:\n  Zip-codes: nodes of a planar graph generalized into connected subgraphs\n  GPS coordinates: points in R2 generalized into non-overlapping rectangles\n  Unordered data: text labels that can be grouped arbitrarily. In addition to\nimpossibility results, we provide approximation algorithms for these difficult\nsingle-attribute generalization problems, which, of course, apply to\nmultiple-attribute instances with one that is quasi-identifying. We show\ntheoretically and experimentally that our approximation algorithms can come\nreasonably close to optimal solutions. Incidentally, the generalization problem\nfor unordered data can be viewed as a novel type of bin packing\nproblem--min-max bin covering--which may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.3898v2", 
    "other_authors": "Mahmoud Fouz, Manfred Kufleitner, Bodo Manthey, Nima Zeini Jahromi", 
    "title": "On Smoothed Analysis of Quicksort and Hoare's Find", 
    "arxiv-id": "0904.3898v2", 
    "author": "Nima Zeini Jahromi", 
    "publish": "2009-04-24T16:12:40Z", 
    "summary": "We provide a smoothed analysis of Hoare's find algorithm and we revisit the\nsmoothed analysis of quicksort.\n  Hoare's find algorithm - often called quickselect - is an easy-to-implement\nalgorithm for finding the k-th smallest element of a sequence. While the\nworst-case number of comparisons that Hoare's find needs is quadratic, the\naverage-case number is linear. We analyze what happens between these two\nextremes by providing a smoothed analysis of the algorithm in terms of two\ndifferent perturbation models: additive noise and partial permutations.\n  Moreover, we provide lower bounds for the smoothed number of comparisons of\nquicksort and Hoare's find for the median-of-three pivot rule, which usually\nyields faster algorithms than always selecting the first element: The pivot is\nthe median of the first, middle, and last element of the sequence. We show that\nmedian-of-three does not yield a significant improvement over the classic rule:\nthe lower bounds for the classic rule carry over to median-of-three."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0904.4061v1", 
    "other_authors": "Agnes Chan, Rajmohan Rajaraman, Zhifeng Sun, Feng Zhu", 
    "title": "Approximation Algorithms for Key Management in Secure Multicast", 
    "arxiv-id": "0904.4061v1", 
    "author": "Feng Zhu", 
    "publish": "2009-04-27T15:28:58Z", 
    "summary": "Many data dissemination and publish-subscribe systems that guarantee the\nprivacy and authenticity of the participants rely on symmetric key\ncryptography. An important problem in such a system is to maintain the shared\ngroup key as the group membership changes. We consider the problem of\ndetermining a key hierarchy that minimizes the average communication cost of an\nupdate, given update frequencies of the group members and an edge-weighted\nundirected graph that captures routing costs. We first present a\npolynomial-time approximation scheme for minimizing the average number of\nmulticast messages needed for an update. We next show that when routing costs\nare considered, the problem is NP-hard even when the underlying routing network\nis a tree network or even when every group member has the same update\nfrequency. Our main result is a polynomial time constant-factor approximation\nalgorithm for the general case where the routing network is an arbitrary\nweighted graph and group members have nonuniform update frequencies."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.0283v1", 
    "other_authors": "David Eppstein, Kevin A. Wortman", 
    "title": "Optimal Embedding Into Star Metrics", 
    "arxiv-id": "0905.0283v1", 
    "author": "Kevin A. Wortman", 
    "publish": "2009-05-03T19:21:52Z", 
    "summary": "We present an O(n^3 log^2 n)-time algorithm for the following problem: given\na finite metric space X, create a star-topology network with the points of X as\nits leaves, such that the distances in the star are at least as large as in X,\nwith minimum dilation. As part of our algorithm, we solve in the same time\nbound the parametric negative cycle detection problem: given a directed graph\nwith edge weights that are increasing linear functions of a parameter lambda,\nfind the smallest value of lambda such that the graph contains no\nnegative-weight cycles."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.0768v5", 
    "other_authors": "Gonzalo Navarro, Kunihiko Sadakane", 
    "title": "Fully-Functional Static and Dynamic Succinct Trees", 
    "arxiv-id": "0905.0768v5", 
    "author": "Kunihiko Sadakane", 
    "publish": "2009-05-06T07:56:38Z", 
    "summary": "We propose new succinct representations of ordinal trees, which have been\nstudied extensively. It is known that any $n$-node static tree can be\nrepresented in $2n + o(n)$ bits and a number of operations on the tree can be\nsupported in constant time under the word-RAM model. However the data\nstructures are complicated and difficult to dynamize. We propose a simple and\nflexible data structure, called the range min-max tree, that reduces the large\nnumber of relevant tree operations considered in the literature to a few\nprimitives that are carried out in constant time on sufficiently small trees.\nThe result is extended to trees of arbitrary size, achieving $2n + O(n\n/\\polylog(n))$ bits of space. The redundancy is significantly lower than any\nprevious proposal. Our data structure builds on the range min-max tree to\nachieve $2n+O(n/\\log n)$ bits of space and $O(\\log n)$ time for all the\noperations. We also propose an improved data structure using $2n+O(n\\log\\log\nn/\\log n)$ bits and improving the time to the optimal $O(\\log n/\\log \\log n)$\nfor most operations. Furthermore, we support sophisticated operations that\nallow attaching and detaching whole subtrees, in time $\\Order(\\log^{1+\\epsilon}\nn / \\log\\log n)$. Our techniques are of independent interest. One allows\nrepresenting dynamic bitmaps and sequences supporting rank/select and indels,\nwithin zero-order entropy bounds and optimal time $O(\\log n / \\log\\log n)$ for\nall operations on bitmaps and polylog-sized alphabets, and $O(\\log n \\log\n\\sigma / (\\log\\log n)^2)$ on larger alphabet sizes $\\sigma$. This improves upon\nthe best existing bounds for entropy-bounded storage of dynamic sequences,\ncompressed full-text self-indexes, and compressed-space construction of the\nBurrows-Wheeler transform."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2028v1", 
    "other_authors": "Weng-Long Chang, Ting-Ting Ren, Mang Feng, Jun Luo, Kawuu Weicheng Lin, Minyi Guo, Lai Chin Lu", 
    "title": "Quantum Algorithms of Bio-molecular Solutions for the Clique Problem on   a Quantum Computer", 
    "arxiv-id": "0905.2028v1", 
    "author": "Lai Chin Lu", 
    "publish": "2009-05-13T07:38:48Z", 
    "summary": "In this paper, it is demonstrated that the DNA-based algorithm [Ho et al.\n2005] for solving an instance of the clique problem to any a graph G = (V, E)\nwith n vertices and p edges and its complementary graph G1 = (V, E1) with n\nvertices and m = (((n*(n-1))/2)-p) edges can be implemented by Hadamard gates,\nNOT gates, CNOT gates, CCNOT gates, Grover's operators, and quantum\nmeasurements on a quantum computer. It is also demonstrated that if Grovers\nalgorithm is employed to accomplish the readout step in the DNA-based\nalgorithm, the quantum implementation of the DNA-based algorithm is equivalent\nto the oracle work (in the language of Grover's algorithm), that is, the target\nstate labeling preceding Grover,s searching steps. It is shown that one oracle\nwork can be completed with O((2 * n) * (n + 1) * (n + 2) / 3) NOT gates, one\nCNOT gate and O((4 * m) + (((2 * n) * (n + 1) * (n + 14)) / 6)) CCNOT gates.\nThis is to say that for the quantum implementation of the DNA-based algorithm\n[Ho et al. 2005] a faster labeling of the target state is attained, which also\nimplies a speedy solution to an instance of the clique problem."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2141v1", 
    "other_authors": "Ilya Volnyansky", 
    "title": "Curse of Dimensionality in the Application of Pivot-based Indexes to the   Similarity Search Problem", 
    "arxiv-id": "0905.2141v1", 
    "author": "Ilya Volnyansky", 
    "publish": "2009-05-13T16:24:21Z", 
    "summary": "In this work we study the validity of the so-called curse of dimensionality\nfor indexing of databases for similarity search. We perform an asymptotic\nanalysis, with a test model based on a sequence of metric spaces $(\\Omega_d)$\nfrom which we pick datasets $X_d$ in an i.i.d. fashion. We call the subscript\n$d$ the dimension of the space $\\Omega_d$ (e.g. for $\\mathbb{R}^d$ the\ndimension is just the usual one) and we allow the size of the dataset $n=n_d$\nto be such that $d$ is superlogarithmic but subpolynomial in $n$.\n  We study the asymptotic performance of pivot-based indexing schemes where the\nnumber of pivots is $o(n/d)$. We pick the relatively simple cost model of\nsimilarity search where we count each distance calculation as a single\ncomputation and disregard the rest.\n  We demonstrate that if the spaces $\\Omega_d$ exhibit the (fairly common)\nconcentration of measure phenomenon the performance of similarity search using\nsuch indexes is asymptotically linear in $n$. That is for large enough $d$ the\ndifference between using such an index and performing a search without an index\nat all is negligeable. Thus we confirm the curse of dimensionality in this\nsetting."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.2213v3", 
    "other_authors": "Eduardo Hwang", 
    "title": "Outlining an elegant solver for 3-SAT", 
    "arxiv-id": "0905.2213v3", 
    "author": "Eduardo Hwang", 
    "publish": "2009-05-13T22:23:07Z", 
    "summary": "The purpose of this article is to incite clever ways to attack problems. It\nadvocates in favor of more elegant algorithms, in place of brute force (albeit\nits very well crafted) usages."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.3107v1", 
    "other_authors": "Travis Gagie, Gonzalo Navarro, Yakov Nekrich", 
    "title": "Fast and Compact Prefix Codes", 
    "arxiv-id": "0905.3107v1", 
    "author": "Yakov Nekrich", 
    "publish": "2009-05-19T14:19:08Z", 
    "summary": "It is well-known that, given a probability distribution over $n$ characters,\nin the worst case it takes (\\Theta (n \\log n)) bits to store a prefix code with\nminimum expected codeword length. However, in this paper we first show that,\nfor any $0<\\epsilon<1/2$ with (1 / \\epsilon = \\Oh{\\polylog{n}}), it takes\n$\\Oh{n \\log \\log (1 / \\epsilon)}$ bits to store a prefix code with expected\ncodeword length within $\\epsilon$ of the minimum. We then show that, for any\nconstant (c > 1), it takes $\\Oh{n^{1 / c} \\log n}$ bits to store a prefix code\nwith expected codeword length at most $c$ times the minimum. In both cases, our\ndata structures allow us to encode and decode any character in $\\Oh{1}$ time."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.4068v4", 
    "other_authors": "\u0141ukasz Je\u017c", 
    "title": "A 4/3-competitive randomized algorithm for online scheduling of packets   with agreeable deadlines", 
    "arxiv-id": "0905.4068v4", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2009-05-25T19:43:24Z", 
    "summary": "In 2005 Li et al. gave a phi-competitive deterministic online algorithm for\nscheduling of packets with agreeable deadlines with a very interesting\nanalysis. This is known to be optimal due to a lower bound by Hajek. We claim\nthat the algorithm by Li et al. can be slightly simplified, while retaining its\ncompetitive ratio. Then we introduce randomness to the modified algorithm and\nargue that the competitive ratio against oblivious adversary is at most 4/3.\nNote that this still leaves a gap between the best known lower bound of 5/4 by\nChin et al. for randomised algorithms against oblivious adversary."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00273", 
    "link": "http://arxiv.org/pdf/0905.4444v1", 
    "other_authors": "Greg N. Frederickson, Barry Wittman", 
    "title": "Approximation Algorithms for the Traveling Repairman and Speeding   Deliveryman Problems", 
    "arxiv-id": "0905.4444v1", 
    "author": "Barry Wittman", 
    "publish": "2009-05-27T15:05:28Z", 
    "summary": "Constant-factor, polynomial-time approximation algorithms are presented for\ntwo variations of the traveling salesman problem with time windows. In the\nfirst variation, the traveling repairman problem, the goal is to find a tour\nthat visits the maximum possible number of locations during their time windows.\nIn the second variation, the speeding deliveryman problem, the goal is to find\na tour that uses the minimum possible speedup to visit all locations during\ntheir time windows. For both variations, the time windows are of unit length,\nand the distance metric is based on a weighted, undirected graph. Algorithms\nwith improved approximation ratios are given for the case when the input is\ndefined on a tree rather than a general graph. The algorithms are also extended\nto handle time windows whose lengths fall in any bounded range."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2010.12.011", 
    "link": "http://arxiv.org/pdf/0905.4930v2", 
    "other_authors": "Therese Biedl, Stephane Durocher, Holger H. Hoos, Shuang Luan, Jared Saia, Maxwell Young", 
    "title": "Improved Approximation Algorithms for Segment Minimization in Intensity   Modulated Radiation Therapy", 
    "arxiv-id": "0905.4930v2", 
    "author": "Maxwell Young", 
    "publish": "2009-05-29T17:56:06Z", 
    "summary": "he segment minimization problem consists of finding the smallest set of\ninteger matrices that sum to a given intensity matrix, such that each summand\nhas only one non-zero value, and the non-zeroes in each row are consecutive.\nThis has direct applications in intensity-modulated radiation therapy, an\neffective form of cancer treatment. We develop three approximation algorithms\nfor matrices with arbitrarily many rows. Our first two algorithms improve the\napproximation factor from the previous best of $1+\\log_2 h $ to (roughly) $3/2\n\\cdot (1+\\log_3 h)$ and $11/6\\cdot(1+\\log_4{h})$, respectively, where $h$ is\nthe largest entry in the intensity matrix. We illustrate the limitations of the\nspecific approach used to obtain these two algorithms by proving a lower bound\nof $\\frac{(2b-2)}{b}\\cdot\\log_b{h} + \\frac{1}{b}$ on the approximation\nguarantee. Our third algorithm improves the approximation factor from $2 \\cdot\n(\\log D+1)$ to $24/13 \\cdot (\\log D+1)$, where $D$ is (roughly) the largest\ndifference between consecutive elements of a row of the intensity matrix.\nFinally, experimentation with these algorithms shows that they perform well\nwith respect to the optimum and outperform other approximation algorithms on\n77% of the 122 test cases we consider, which include both real world and\nsynthetic data."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2010.12.011", 
    "link": "http://arxiv.org/pdf/0906.0328v1", 
    "other_authors": "Andrea Pasquinucci", 
    "title": "Rivisiting Token/Bucket Algorithms in New Applications", 
    "arxiv-id": "0906.0328v1", 
    "author": "Andrea Pasquinucci", 
    "publish": "2009-06-01T18:06:45Z", 
    "summary": "We consider a somehow peculiar Token/Bucket problem which at first sight\nlooks confusing and difficult to solve. The winning approach to solve the\nproblem consists in going back to the simple and traditional methods to solve\ncomputer science problems like the one taught to us by Knuth. Somehow the main\ntrick is to be able to specify clearly what needs to be achieved, and then the\nsolution, even if complex, appears almost by itself."
},{
    "category": "cs.DS", 
    "doi": "10.1109/SISAP.2009.9", 
    "link": "http://arxiv.org/pdf/0906.0391v2", 
    "other_authors": "Ilya Volnyansky, Vladimir Pestov", 
    "title": "Curse of Dimensionality in Pivot-based Indexes", 
    "arxiv-id": "0906.0391v2", 
    "author": "Vladimir Pestov", 
    "publish": "2009-06-02T00:41:46Z", 
    "summary": "We offer a theoretical validation of the curse of dimensionality in the\npivot-based indexing of datasets for similarity search, by proving, in the\nframework of statistical learning, that in high dimensions no pivot-based\nindexing scheme can essentially outperform the linear scan.\n  A study of the asymptotic performance of pivot-based indexing schemes is\nperformed on a sequence of datasets modeled as samples $X_d$ picked in i.i.d.\nfashion from metric spaces $\\Omega_d$. We allow the size of the dataset $n=n_d$\nto be such that $d$, the ``dimension'', is superlogarithmic but subpolynomial\nin $n$. The number of pivots is allowed to grow as $o(n/d)$. We pick the least\nrestrictive cost model of similarity search where we count each distance\ncalculation as a single computation and disregard the rest.\n  We demonstrate that if the intrinsic dimension of the spaces $\\Omega_d$ in\nthe sense of concentration of measure phenomenon is $O(d)$, then the\nperformance of similarity search pivot-based indexes is asymptotically linear\nin $n$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03751-1_12", 
    "link": "http://arxiv.org/pdf/0906.0862v1", 
    "other_authors": "Gregory Gutin, Daniel Karapetyan", 
    "title": "A Memetic Algorithm for the Multidimensional Assignment Problem", 
    "arxiv-id": "0906.0862v1", 
    "author": "Daniel Karapetyan", 
    "publish": "2009-06-04T09:44:59Z", 
    "summary": "The Multidimensional Assignment Problem (MAP or s-AP in the case of s\ndimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we propose a memetic algorithm for\nMAP that is a combination of a genetic algorithm with a local search procedure.\nThe main contribution of the paper is an idea of dynamically adjusted\ngeneration size, that yields an outstanding flexibility of the algorithm to\nperform well for both small and large fixed running times. The results of\ncomputational experiments for several instance families show that the proposed\nalgorithm produces solutions of very high quality in a reasonable time and\noutperforms the state-of-the art 3-AP memetic algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03751-1_12", 
    "link": "http://arxiv.org/pdf/0906.1341v1", 
    "other_authors": "Madalina Ecaterina Andreica, Mugurel Ionut Andreica, Angela Andreica", 
    "title": "Efficient Algorithms for Several Constrained Activity Scheduling   Problems in the Time and Space Domains", 
    "arxiv-id": "0906.1341v1", 
    "author": "Angela Andreica", 
    "publish": "2009-06-07T09:35:01Z", 
    "summary": "In this paper we consider several constrained activity scheduling problems in\nthe time and space domains, like finding activity orderings which optimize the\nvalues of several objective functions (time scheduling) or finding optimal\nlocations where certain types of activities will take place (space scheduling).\nWe present novel, efficient algorithmic solutions for all the considered\nproblems, based on the dynamic programming and greedy techniques. In each case\nwe compute exact, optimal solutions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2020v1", 
    "other_authors": "Anupam Gupta, Ravishankar Krishnaswamy, Amit Kumar, Danny Segev", 
    "title": "Scheduling with Outliers", 
    "arxiv-id": "0906.2020v1", 
    "author": "Danny Segev", 
    "publish": "2009-06-10T21:22:22Z", 
    "summary": "In classical scheduling problems, we are given jobs and machines, and have to\nschedule all the jobs to minimize some objective function. What if each job has\na specified profit, and we are no longer required to process all jobs -- we can\nschedule any subset of jobs whose total profit is at least a (hard) target\nprofit requirement, while still approximately minimizing the objective\nfunction?\n  We refer to this class of problems as scheduling with outliers. This model\nwas initiated by Charikar and Khuller (SODA'06) on the minimum max-response\ntime in broadcast scheduling. We consider three other well-studied scheduling\nobjectives: the generalized assignment problem, average weighted completion\ntime, and average flow time, and provide LP-based approximation algorithms for\nthem. For the minimum average flow time problem on identical machines, we give\na logarithmic approximation algorithm for the case of unit profits based on\nrounding an LP relaxation; we also show a matching integrality gap. For the\naverage weighted completion time problem on unrelated machines, we give a\nconstant factor approximation. The algorithm is based on randomized rounding of\nthe time-indexed LP relaxation strengthened by the knapsack-cover inequalities.\nFor the generalized assignment problem with outliers, we give a simple\nreduction to GAP without outliers to obtain an algorithm whose makespan is\nwithin 3 times the optimum makespan, and whose cost is at most (1 + \\epsilon)\ntimes the optimal cost."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2048v1", 
    "other_authors": "Chandra Chekuri, Sungjin Im, Benjamin Moseley", 
    "title": "Minimizing Maximum Response Time and Delay Factor in Broadcast   Scheduling", 
    "arxiv-id": "0906.2048v1", 
    "author": "Benjamin Moseley", 
    "publish": "2009-06-11T07:24:52Z", 
    "summary": "We consider online algorithms for pull-based broadcast scheduling. In this\nsetting there are n pages of information at a server and requests for pages\narrive online. When the server serves (broadcasts) a page p, all outstanding\nrequests for that page are satisfied. We study two related metrics, namely\nmaximum response time (waiting time) and maximum delay-factor and their\nweighted versions. We obtain the following results in the worst-case online\ncompetitive model.\n  - We show that FIFO (first-in first-out) is 2-competitive even when the page\nsizes are different. Previously this was known only for unit-sized pages [10]\nvia a delicate argument. Our proof differs from [10] and is perhaps more\nintuitive.\n  - We give an online algorithm for maximum delay-factor that is\nO(1/eps^2)-competitive with (1+\\eps)-speed for unit-sized pages and with\n(2+\\eps)-speed for different sized pages. This improves on the algorithm in\n[12] which required (2+\\eps)-speed and (4+\\eps)-speed respectively. In addition\nwe show that the algorithm and analysis can be extended to obtain the same\nresults for maximum weighted response time and delay factor.\n  - We show that a natural greedy algorithm modeled after LWF\n(Longest-Wait-First) is not O(1)-competitive for maximum delay factor with any\nconstant speed even in the setting of standard scheduling with unit-sized jobs.\nThis complements our upper bound and demonstrates the importance of the\ntradeoff made in our algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2395v1", 
    "other_authors": "Chandra Chekuri, Sungjin Im, Benjamin Moseley", 
    "title": "Longest Wait First for Broadcast Scheduling", 
    "arxiv-id": "0906.2395v1", 
    "author": "Benjamin Moseley", 
    "publish": "2009-06-12T18:27:07Z", 
    "summary": "We consider online algorithms for broadcast scheduling. In the pull-based\nbroadcast model there are $n$ unit-sized pages of information at a server and\nrequests arrive online for pages. When the server transmits a page $p$, all\noutstanding requests for that page are satisfied. The longest-wait-first} (LWF)\nalgorithm is a natural algorithm that has been shown to have good empirical\nperformance. In this paper we make two main contributions to the analysis of\nLWF and broadcast scheduling. \\begin{itemize} \\item We give an intuitive and\neasy to understand analysis of LWF which shows that it is\n$O(1/\\eps^2)$-competitive for average flow-time with $(4+\\eps)$ speed. Using a\nmore involved analysis, we show that LWF is $O(1/\\eps^3)$-competitive for\naverage flow-time with $(3.4+\\epsilon)$ speed. \\item We show that a natural\nextension of LWF is O(1)-speed O(1)-competitive for more general objective\nfunctions such as average delay-factor and $L_k$ norms of delay-factor (for\nfixed $k$). \\end{itemize}"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2448v1", 
    "other_authors": "Karthekeyan Chandrasekaran, Amit Deshpande, Santosh Vempala", 
    "title": "The Limit of Convexity Based Isoperimetry: Sampling Harmonic-Concave   Functions", 
    "arxiv-id": "0906.2448v1", 
    "author": "Santosh Vempala", 
    "publish": "2009-06-13T05:15:53Z", 
    "summary": "Logconcave functions represent the current frontier of efficient algorithms\nfor sampling, optimization and integration in R^n. Efficient sampling\nalgorithms to sample according to a probability density (to which the other two\nproblems can be reduced) relies on good isoperimetry which is known to hold for\narbitrary logconcave densities. In this paper, we extend this frontier in two\nways: first, we characterize convexity-like conditions that imply good\nisoperimetry, i.e., what condition on function values along every line\nguarantees good isoperimetry? The answer turns out to be the set of\n(1/(n-1))-harmonic concave functions in R^n; we also prove that this is the\nbest possible characterization along every line, of functions having good\nisoperimetry. Next, we give the first efficient algorithm for sampling\naccording to such functions with complexity depending on a smoothness\nparameter. Further, noting that the multivariate Cauchy density is an important\ndistribution in this class, we exploit certain properties of the Cauchy density\nto give an efficient sampling algorithm based on random walks with a mixing\ntime that matches the current best bounds known for sampling logconcave\nfunctions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2671v1", 
    "other_authors": "Catherine Doss, Mich\u00e8le Thieullen", 
    "title": "Oscillations and Random Perturbations of a FitzHugh-Nagumo System", 
    "arxiv-id": "0906.2671v1", 
    "author": "Mich\u00e8le Thieullen", 
    "publish": "2009-06-15T12:29:32Z", 
    "summary": "We consider a stochastic perturbation of a FitzHugh-Nagumo system. We show\nthat it is possible to generate oscillations for values of parameters which do\nnot allow oscillations for the deterministic system. We also study the\nappearance of a new equilibrium point and new bifurcation parameters due to the\nnoisy component."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.2960v1", 
    "other_authors": "Daniel Karapetyan, Gregory Gutin, Boris Goldengorin", 
    "title": "Empirical evaluation of construction heuristics for the multidimensional   assignment problem", 
    "arxiv-id": "0906.2960v1", 
    "author": "Boris Goldengorin", 
    "publish": "2009-06-16T15:47:17Z", 
    "summary": "The multidimensional assignment problem (MAP) (abbreviated s-AP in the case\nof s dimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we consider four fast construction\nheuristics for MAP. One of the heuristics is new. A modification of the\nheuristics is proposed to optimize the access to slow computer memory. The\nresults of computational experiments for several instance families are provided\nand discussed."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-03685-9_12", 
    "link": "http://arxiv.org/pdf/0906.3056v1", 
    "other_authors": "Chi Zhang, Gang Wang, Xiaoguang Liu, Jing Liu", 
    "title": "Approximating Scheduling Machines with Capacity Constraints", 
    "arxiv-id": "0906.3056v1", 
    "author": "Jing Liu", 
    "publish": "2009-06-17T02:01:26Z", 
    "summary": "In the Scheduling Machines with Capacity Constraints problem, we are given k\nidentical machines, each of which can process at most m_i jobs. M jobs are also\ngiven, where job j has a non-negative processing time length t_j >= 0. The task\nis to find a schedule such that the makespan is minimized and the capacity\nconstraints are met. In this paper, we present a 3-approximation algorithm\nusing an extension of Iterative Rounding Method introduced by Jain. To the best\nof the authors' knowledge, this is the first attempt to apply Iterative\nRounding Method to scheduling problem with capacity constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5050v1", 
    "other_authors": "Leah Epstein, Asaf Levin", 
    "title": "AFPTAS results for common variants of bin packing: A new method to   handle the small items", 
    "arxiv-id": "0906.5050v1", 
    "author": "Asaf Levin", 
    "publish": "2009-06-27T08:39:18Z", 
    "summary": "We consider two well-known natural variants of bin packing, and show that\nthese packing problems admit asymptotic fully polynomial time approximation\nschemes (AFPTAS). In bin packing problems, a set of one-dimensional items of\nsize at most 1 is to be assigned (packed) to subsets of sum at most 1 (bins).\nIt has been known for a while that the most basic problem admits an AFPTAS. In\nthis paper, we develop methods that allow to extend this result to other\nvariants of bin packing. Specifically, the problems which we study in this\npaper, for which we design asymptotic fully polynomial time approximation\nschemes, are the following. The first problem is \"Bin packing with cardinality\nconstraints\", where a parameter k is given, such that a bin may contain up to k\nitems. The goal is to minimize the number of bins used. The second problem is\n\"Bin packing with rejection\", where every item has a rejection penalty\nassociated with it. An item needs to be either packed to a bin or rejected, and\nthe goal is to minimize the number of used bins plus the total rejection\npenalty of unpacked items. This resolves the complexity of two important\nvariants of the bin packing problem. Our approximation schemes use a novel\nmethod for packing the small items. This new method is the core of the improved\nrunning times of our schemes over the running times of the previous results,\nwhich are only asymptotic polynomial time approximation schemes (APTAS)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5051v1", 
    "other_authors": "Leah Epstein, Asaf Levin", 
    "title": "Bin packing with general cost structures", 
    "arxiv-id": "0906.5051v1", 
    "author": "Asaf Levin", 
    "publish": "2009-06-27T19:48:10Z", 
    "summary": "Following the work of Anily et al., we consider a variant of bin packing,\ncalled \"bin packing with general cost structures\" (GCBP) and design an\nasymptotic fully polynomial time approximation scheme (AFPTAS) for this\nproblem. In the classic bin packing problem, a set of one-dimensional items is\nto be assigned to subsets of total size at most 1, that is, to be packed into\nunit sized bins. However, in GCBP, the cost of a bin is not 1 as in classic bin\npacking, but it is a non-decreasing and concave function of the number of items\npacked in it, where the cost of an empty bin is zero. The construction of the\nAFPTAS requires novel techniques for dealing with small items, which are\ndeveloped in this work. In addition, we develop a fast approximation algorithm\nwhich acts identically for all non-decreasing and concave functions, and has an\nasymptotic approximation ratio of 1.5 for all functions simultaneously."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5062v2", 
    "other_authors": "Simant Dube", 
    "title": "Geometrical Interpretation of the Master Theorem for Divide-and-conquer   Recurrences", 
    "arxiv-id": "0906.5062v2", 
    "author": "Simant Dube", 
    "publish": "2009-06-29T04:43:17Z", 
    "summary": "We provide geometrical interpretation of the Master Theorem to solve\ndivide-and-conquer recurrences. We show how different cases of the recurrences\ncorrespond to different kinds of fractal images. Fractal dimension and\nHausdorff measure are shown to be closely related to the solution of such\nrecurrences."
},{
    "category": "cs.DS", 
    "doi": "10.1137/090767613", 
    "link": "http://arxiv.org/pdf/0906.5070v1", 
    "other_authors": "R. Thamilselvan, Dr. P. Balasubramanie", 
    "title": "Integrating Genetic Algorithm, Tabu Search Approach for Job Shop   Scheduling", 
    "arxiv-id": "0906.5070v1", 
    "author": "Dr. P. Balasubramanie", 
    "publish": "2009-06-27T11:16:30Z", 
    "summary": "This paper presents a new algorithm based on integrating Genetic Algorithms\nand Tabu Search methods to solve the Job Shop Scheduling problem. The idea of\nthe proposed algorithm is derived from Genetic Algorithms. Most of the\nscheduling problems require either exponential time or space to generate an\noptimal answer. Job Shop scheduling (JSS) is the general scheduling problem and\nit is a NP-complete problem, but it is difficult to find the optimal solution.\nThis paper applies Genetic Algorithms and Tabu Search for Job Shop Scheduling\nproblem and compares the results obtained by each. With the implementation of\nour approach the JSS problems reaches optimal solution and minimize the\nmakespan."
},{
    "category": "cs.DS", 
    "doi": "10.1137/100801901", 
    "link": "http://arxiv.org/pdf/0907.0305v1", 
    "other_authors": "Leah Epstein, Asaf Levin, Julian Mestre, Danny Segev", 
    "title": "Improved approximation guarantees for weighted matching in the   semi-streaming model", 
    "arxiv-id": "0907.0305v1", 
    "author": "Danny Segev", 
    "publish": "2009-07-02T08:11:22Z", 
    "summary": "We study the maximum weight matching problem in the semi-streaming model, and\nimprove on the currently best one-pass algorithm due to Zelke (Proc. of\nSTACS2008, pages 669-680) by devising a deterministic approach whose\nperformance guarantee is 4.91+epsilon. In addition, we study preemptive online\nalgorithms, a sub-class of one-pass algorithms where we are only allowed to\nmaintain a feasible matching in memory at any point in time. All known results\nprior to Zelke's belong to this sub-class. We provide a lower bound of 4.967 on\nthe competitive ratio of any such deterministic algorithm, and hence show that\nfuture improvements will have to store in memory a set of edges which is not\nnecessarily a feasible matching."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0718v2", 
    "other_authors": "Jonathan P. Sorenson", 
    "title": "A Randomized Sublinear Time Parallel GCD Algorithm for the EREW PRAM", 
    "arxiv-id": "0907.0718v2", 
    "author": "Jonathan P. Sorenson", 
    "publish": "2009-07-03T21:12:38Z", 
    "summary": "We present a randomized parallel algorithm that computes the greatest common\ndivisor of two integers of n bits in length with probability 1-o(1) that takes\nO(n loglog n / log n) expected time using n^{6+\\epsilon} processors on the EREW\nPRAM parallel model of computation. We believe this to be the first randomized\nsublinear time algorithm on the EREW PRAM for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0726v2", 
    "other_authors": "Zachary Friggstad, Mohammad R. Salavatipour, Zoya Svitkina", 
    "title": "Asymmetric Traveling Salesman Path and Directed Latency Problems", 
    "arxiv-id": "0907.0726v2", 
    "author": "Zoya Svitkina", 
    "publish": "2009-07-03T22:42:23Z", 
    "summary": "We study integrality gaps and approximability of two closely related problems\non directed graphs. Given a set V of n nodes in an underlying asymmetric metric\nand two specified nodes s and t, both problems ask to find an s-t path visiting\nall other nodes. In the asymmetric traveling salesman path problem (ATSPP), the\nobjective is to minimize the total cost of this path. In the directed latency\nproblem, the objective is to minimize the sum of distances on this path from s\nto each node. Both of these problems are NP-hard. The best known approximation\nalgorithms for ATSPP had ratio O(log n) until the very recent result that\nimproves it to O(log n/ log log n). However, only a bound of O(sqrt(n)) for the\nintegrality gap of its linear programming relaxation has been known. For\ndirected latency, the best previously known approximation algorithm has a\nguarantee of O(n^(1/2+eps)), for any constant eps > 0. We present a new\nalgorithm for the ATSPP problem that has an approximation ratio of O(log n),\nbut whose analysis also bounds the integrality gap of the standard LP\nrelaxation of ATSPP by the same factor. This solves an open problem posed by\nChekuri and Pal [2007]. We then pursue a deeper study of this linear program\nand its variations, which leads to an algorithm for the k-person ATSPP (where k\ns-t paths of minimum total length are sought) and an O(log n)-approximation for\nthe directed latency problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.0741v1", 
    "other_authors": "Travis Gagie, Yakov Nekrich", 
    "title": "Tight Bounds for Online Stable Sorting", 
    "arxiv-id": "0907.0741v1", 
    "author": "Yakov Nekrich", 
    "publish": "2009-07-04T06:18:38Z", 
    "summary": "Although many authors have considered how many ternary comparisons it takes\nto sort a multiset $S$ of size $n$, the best known upper and lower bounds still\ndiffer by a term linear in $n$. In this paper we restrict our attention to\nonline stable sorting and prove upper and lower bounds that are within (o (n))\nnot only of each other but also of the best known upper bound for offline\nsorting. Specifically, we first prove that if the number of distinct elements\n(\\sigma = o (n / \\log n)), then ((H + 1) n + o (n)) comparisons are sufficient,\nwhere $H$ is the entropy of the distribution of the elements in $S$. We then\ngive a simple proof that ((H + 1) n - o (n)) comparisons are necessary in the\nworst case."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1295v1", 
    "other_authors": "Ankur Gupta, Anna Kispert, Jonathan P. Sorenson", 
    "title": "Online Sorting via Searching and Selection", 
    "arxiv-id": "0907.1295v1", 
    "author": "Jonathan P. Sorenson", 
    "publish": "2009-07-07T20:37:19Z", 
    "summary": "In this paper, we present a framework based on a simple data structure and\nparameterized algorithms for the problems of finding items in an unsorted list\nof linearly ordered items based on their rank (selection) or value (search). As\na side-effect of answering these online selection and search queries, we\nprogressively sort the list. Our algorithms are based on Hoare's Quickselect,\nand are parameterized based on the pivot selection method.\n  For example, if we choose the pivot as the last item in a subinterval, our\nframework yields algorithms that will answer q<=n unique selection and/or\nsearch queries in a total of O(n log q) average time. After q=\\Omega(n) queries\nthe list is sorted. Each repeated selection query takes constant time, and each\nrepeated search query takes O(log n) time. The two query types can be\ninterleaved freely. By plugging different pivot selection methods into our\nframework, these results can, for example, become randomized expected time or\ndeterministic worst-case time. Our methods are easy to implement, and we show\nthey perform well in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1369v1", 
    "other_authors": "Manjish Pal", 
    "title": "Towards an $O(\\sqrt[3]{\\log n})$-Approximation Algorithm for {\\sc   Balanced Separator}", 
    "arxiv-id": "0907.1369v1", 
    "author": "Manjish Pal", 
    "publish": "2009-07-08T09:32:02Z", 
    "summary": "The {\\sc $c$-Balanced Separator} problem is a graph-partitioning problem in\nwhich given a graph $G$, one aims to find a cut of minimum size such that both\nthe sides of the cut have at least $cn$ vertices. In this paper, we present new\ndirections of progress in the {\\sc $c$-Balanced Separator} problem. More\nspecifically, we propose a new family of mathematical programs, which depends\nupon a parameter $\\epsilon > 0$, and extend the seminal work of\nArora-Rao-Vazirani ({\\sf ARV}) \\cite{ARV} to show that the polynomial time\nsolvability of the proposed family of programs implies an improvement in the\napproximation factor to $O(\\log^{{1/3} + \\epsilon} n)$ from the best-known\nfactor of $O(\\sqrt{\\log n})$ due to {\\sf ARV}. In fact, for $\\epsilon = 1/3$,\nthe program we get is the SDP proposed by {\\sf ARV}. For $\\epsilon < 1/3$, this\nfamily of programs is not convex but one can transform them into so called\n\\emph{\\textbf{concave programs}} in which one optimizes a concave function over\na convex feasible set. The properties of concave programs allows one to apply\ntechniques due to Hoffman \\cite{H81} or Tuy \\emph{et al} \\cite{TTT85} to solve\nsuch problems with arbitrary accuracy. But the problem of finding of a method\nto solve these programs that converges in polynomial time still remains open.\nOur result, although conditional, introduces a new family of programs which is\nmore powerful than semi-definite programming in the context of approximation\nalgorithms and hence it will of interest to investigate this family both in the\ndirection of designing efficient algorithms and proving hardness results."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.1840v1", 
    "other_authors": "Paola Bonizzoni, Gianluca Della Vedova, Riccardo Dondi", 
    "title": "A PTAS for the Minimum Consensus Clustering Problem with a Fixed Number   of Clusters", 
    "arxiv-id": "0907.1840v1", 
    "author": "Riccardo Dondi", 
    "publish": "2009-07-10T15:16:43Z", 
    "summary": "The Consensus Clustering problem has been introduced as an effective way to\nanalyze the results of different microarray experiments. The problem consists\nof looking for a partition that best summarizes a set of input partitions (each\ncorresponding to a different microarray experiment) under a simple and\nintuitive cost function. The problem admits polynomial time algorithms on two\ninput partitions, but is APX-hard on three input partitions. We investigate the\nrestriction of Consensus Clustering when the output partition is required to\ncontain at most k sets, giving a polynomial time approximation scheme (PTAS)\nwhile proving the NP-hardness of this restriction."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2050v2", 
    "other_authors": "\u0141ukasz Je\u017c", 
    "title": "Randomised Buffer Management with Bounded Delay against Adaptive   Adversary", 
    "arxiv-id": "0907.2050v2", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2009-07-12T16:11:45Z", 
    "summary": "We give a new analysis of the RMix algorithm by Chin et al. for the Buffer\nManagement with Bounded Delay problem (or online scheduling of unit jobs to\nmaximise weighted throughput). Unlike the original proof of\ne/(e-1)-competitiveness, the new one holds even in adaptive-online adversary\nmodel. In fact, the proof works also for a slightly more general problem\nstudied by Bie{\\'n}kowski et al."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2071v1", 
    "other_authors": "Prosenjit Bose, Karim Dou\u00efeb, Vida Dujmovi\u0107, John Howat", 
    "title": "Layered Working-Set Trees", 
    "arxiv-id": "0907.2071v1", 
    "author": "John Howat", 
    "publish": "2009-07-13T18:11:05Z", 
    "summary": "The working-set bound [Sleator and Tarjan, J. ACM, 1985] roughly states that\nsearching for an element is fast if the element was accessed recently. Binary\nsearch trees, such as splay trees, can achieve this property in the amortized\nsense, while data structures that are not binary search trees are known to have\nthis property in the worst case. We close this gap and present a binary search\ntree called a layered working-set tree that guarantees the working-set property\nin the worst case. The unified bound [Badoiu et al., TCS, 2007] roughly states\nthat searching for an element is fast if it is near (in terms of rank distance)\nto a recently accessed element. We show how layered working-set trees can be\nused to achieve the unified bound to within a small additive term in the\namortized sense while maintaining in the worst case an access time that is both\nlogarithmic and within a small multiplicative factor of the working-set bound."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.2741v1", 
    "other_authors": "Stanley P. Y. Fung", 
    "title": "Bounded Delay Packet Scheduling in a Bounded Buffer", 
    "arxiv-id": "0907.2741v1", 
    "author": "Stanley P. Y. Fung", 
    "publish": "2009-07-16T04:05:05Z", 
    "summary": "We study the problem of buffer management in QoS-enabled network switches in\nthe bounded delay model where each packet is associated with a weight and a\ndeadline. We consider the more realistic situation where the network switch has\na finite buffer size. A 9.82-competitive algorithm is known for the case of\nmultiple buffers (Azar and Levy, SWAT'06). Recently, for the case of a single\nbuffer, a 3-competitive deterministic algorithm and a 2.618-competitive\nrandomized algorithm was known (Li, INFOCOM'09). In this paper we give a simple\ndeterministic 2-competitive algorithm for the case of a single buffer."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.3135v2", 
    "other_authors": "Philip Bille", 
    "title": "Fast Searching in Packed Strings", 
    "arxiv-id": "0907.3135v2", 
    "author": "Philip Bille", 
    "publish": "2009-07-17T19:29:13Z", 
    "summary": "Given strings $P$ and $Q$ the (exact) string matching problem is to find all\npositions of substrings in $Q$ matching $P$. The classical Knuth-Morris-Pratt\nalgorithm [SIAM J. Comput., 1977] solves the string matching problem in linear\ntime which is optimal if we can only read one character at the time. However,\nmost strings are stored in a computer in a packed representation with several\ncharacters in a single word, giving us the opportunity to read multiple\ncharacters simultaneously. In this paper we study the worst-case complexity of\nstring matching on strings given in packed representation. Let $m \\leq n$ be\nthe lengths $P$ and $Q$, respectively, and let $\\sigma$ denote the size of the\nalphabet. On a standard unit-cost word-RAM with logarithmic word size we\npresent an algorithm using time $$ O\\left(\\frac{n}{\\log_\\sigma n} + m +\n\\occ\\right). $$ Here $\\occ$ is the number of occurrences of $P$ in $Q$. For $m\n= o(n)$ this improves the $O(n)$ bound of the Knuth-Morris-Pratt algorithm.\nFurthermore, if $m = O(n/\\log_\\sigma n)$ our algorithm is optimal since any\nalgorithm must spend at least $\\Omega(\\frac{(n+m)\\log\n  \\sigma}{\\log n} + \\occ) = \\Omega(\\frac{n}{\\log_\\sigma n} + \\occ)$ time to\nread the input and report all occurrences. The result is obtained by a novel\nautomaton construction based on the Knuth-Morris-Pratt algorithm combined with\na new compact representation of subautomata allowing an optimal\ntabulation-based simulation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5269v1", 
    "other_authors": "Robert Geisberger, Dennis Luxen, Sabine Neubauer, Peter Sanders, Lars Volker", 
    "title": "Fast Detour Computation for Ride Sharing", 
    "arxiv-id": "0907.5269v1", 
    "author": "Lars Volker", 
    "publish": "2009-07-30T11:31:05Z", 
    "summary": "Todays ride sharing services still mimic a better billboard. They list the\noffers and allow to search for the source and target city, sometimes enriched\nwith radial search. So finding a connection between big cities is quite easy.\nThese places are on a list of designated origin and distination points. But\nwhen you want to go from a small town to another small town, even when they are\nnext to a freeway, you run into problems. You can't find offers that would or\ncould pass by the town easily with little or no detour. We solve this\ninteresting problem by presenting a fast algorithm that computes the offers\nwith the smallest detours w.r.t. a request. Our experiments show that the\nproblem is efficiently solvable in times suitable for a web service\nimplementation. For realistic database size we achieve lookup times of about\n5ms and a matching rate of 90% instead of just 70% for the simple matching\nalgorithms used today."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5372v1", 
    "other_authors": "Greg N. Frederickson, Barry Wittman", 
    "title": "Speedup in the Traveling Repairman Problem with Unit Time Windows", 
    "arxiv-id": "0907.5372v1", 
    "author": "Barry Wittman", 
    "publish": "2009-07-30T16:30:15Z", 
    "summary": "The input to the unrooted traveling repairman problem is an undirected metric\ngraph and a subset of nodes, each of which has a time window of unit length.\nGiven that a repairman can start at any location, the goal is to plan a route\nthat visits as many nodes as possible during their respective time windows. A\npolynomial-time bicriteria approximation algorithm is presented for this\nproblem, gaining an increased fraction of repairman visits for increased\nspeedup of repairman motion. For speedup $s$, we find a $6\\gamma/(s +\n1)$-approximation for $s$ in the range $1 \\leq s \\leq 2$ and a\n$4\\gamma/s$-approximation for $s$ in the range $2 \\leq s \\leq 4$, where $\\gamma\n= 1$ on tree-shaped networks and $\\gamma = 2 + \\epsilon$ on general metric\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0907.5474v1", 
    "other_authors": "David Eppstein, Kevin A. Wortman", 
    "title": "Optimal Angular Resolution for Face-Symmetric Drawings", 
    "arxiv-id": "0907.5474v1", 
    "author": "Kevin A. Wortman", 
    "publish": "2009-07-31T06:27:19Z", 
    "summary": "Let G be a graph that may be drawn in the plane in such a way that all\ninternal faces are centrally symmetric convex polygons. We show how to find a\ndrawing of this type that maximizes the angular resolution of the drawing, the\nminimum angle between any two incident edges, in polynomial time, by reducing\nthe problem to one of finding parametric shortest paths in an auxiliary graph.\nThe running time is at most O(t^3), where t is a parameter of the input graph\nthat is at most O(n) but is more typically proportional to n^.5."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0239v1", 
    "other_authors": "Manfred Kufleitner", 
    "title": "On Bijective Variants of the Burrows-Wheeler Transform", 
    "arxiv-id": "0908.0239v1", 
    "author": "Manfred Kufleitner", 
    "publish": "2009-08-03T13:10:48Z", 
    "summary": "The sort transform (ST) is a modification of the Burrows-Wheeler transform\n(BWT). Both transformations map an arbitrary word of length n to a pair\nconsisting of a word of length n and an index between 1 and n. The BWT sorts\nall rotation conjugates of the input word, whereas the ST of order k only uses\nthe first k letters for sorting all such conjugates. If two conjugates start\nwith the same prefix of length k, then the indices of the rotations are used\nfor tie-breaking. Both transforms output the sequence of the last letters of\nthe sorted list and the index of the input within the sorted list. In this\npaper, we discuss a bijective variant of the BWT (due to Scott), proving its\ncorrectness and relations to other results due to Gessel and Reutenauer (1993)\nand Crochemore, Desarmenien, and Perrin (2005). Further, we present a novel\nbijective variant of the ST."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0350v1", 
    "other_authors": "Siddharth Barman, Shuchi Chawla", 
    "title": "Region growing for multi-route cuts", 
    "arxiv-id": "0908.0350v1", 
    "author": "Shuchi Chawla", 
    "publish": "2009-08-03T21:21:06Z", 
    "summary": "We study a number of multi-route cut problems: given a graph G=(V,E) and\nconnectivity thresholds k_(u,v) on pairs of nodes, the goal is to find a\nminimum cost set of edges or vertices the removal of which reduces the\nconnectivity between every pair (u,v) to strictly below its given threshold.\nThese problems arise in the context of reliability in communication networks;\nThey are natural generalizations of traditional minimum cut problems where the\nthresholds are either 1 (we want to completely separate the pair) or infinity\n(we don't care about the connectivity for the pair). We provide the first\nnon-trivial approximations to a number of variants of the problem including for\nboth node-disjoint and edge-disjoint connectivity thresholds. A main\ncontribution of our work is an extension of the region growing technique for\napproximating minimum multicuts to the multi-route setting. When the\nconnectivity thresholds are either 2 or infinity (the \"2-route cut\" case), we\nobtain polylogarithmic approximations while satisfying the thresholds exactly.\nFor arbitrary connectivity thresholds this approach leads to bicriteria\napproximations where we approximately satisfy the thresholds and approximately\nminimize the cost. We present a number of different algorithms achieving\ndifferent cost-connectivity tradeoffs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.0375v1", 
    "other_authors": "Karthekeyan Chandrasekaran, Navin Goyal, Bernhard Haeupler", 
    "title": "Deterministic Algorithms for the Lovasz Local Lemma", 
    "arxiv-id": "0908.0375v1", 
    "author": "Bernhard Haeupler", 
    "publish": "2009-08-04T02:13:54Z", 
    "summary": "The Lovasz Local Lemma (LLL) is a powerful result in probability theory that\nstates that the probability that none of a set of bad events happens is nonzero\nif the probability of each event is small compared to the number of events that\ndepend on it. It is often used in combination with the probabilistic method for\nnon-constructive existence proofs. A prominent application is to k-CNF\nformulas, where LLL implies that, if every clause in the formula shares\nvariables with at most d <= 2^k/e other clauses then such a formula has a\nsatisfying assignment. Recently, a randomized algorithm to efficiently\nconstruct a satisfying assignment was given by Moser. Subsequently Moser and\nTardos gave a randomized algorithm to construct the structures guaranteed by\nthe LLL in a very general algorithmic framework. We address the main problem\nleft open by Moser and Tardos of derandomizing these algorithms efficiently.\nSpecifically, for a k-CNF formula with m clauses and d <= 2^{k/(1+\\eps)}/e for\nsome \\eps\\in (0,1), we give an algorithm that finds a satisfying assignment in\ntime \\tilde{O}(m^{2(1+1/\\eps)}). This improves upon the deterministic\nalgorithms of Moser and of Moser-Tardos with running time m^{\\Omega(k^2)} which\nis superpolynomial for k=\\omega(1) and upon other previous algorithms which\nwork only for d\\leq 2^{k/16}/e. Our algorithm works efficiently for a general\nversion of LLL under the algorithmic framework of Moser and Tardos, and is also\nparallelizable, i.e., has polylogarithmic running time using polynomially many\nprocessors."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.1379v1", 
    "other_authors": "Jonah Sherman", 
    "title": "Breaking the Multicommodity Flow Barrier for sqrt(log(n))-Approximations   to Sparsest Cut", 
    "arxiv-id": "0908.1379v1", 
    "author": "Jonah Sherman", 
    "publish": "2009-08-10T19:57:32Z", 
    "summary": "This paper ties the line of work on algorithms that find an\nO(sqrt(log(n)))-approximation to the sparsest cut together with the line of\nwork on algorithms that run in sub-quadratic time by using only\nsingle-commodity flows. We present an algorithm that simultaneously achieves\nboth goals, finding an O(sqrt(log(n)/eps))-approximation using O(n^eps log^O(1)\nn) max-flows. The core of the algorithm is a stronger, algorithmic version of\nArora et al.'s structure theorem, where we show that matching-chaining argument\nat the heart of their proof can be viewed as an algorithm that finds good\naugmenting paths in certain geometric multicommodity flow networks. By using\nthat specialized algorithm in place of a black-box solver, we are able to solve\nthose instances much more efficiently. We also show the cut-matching game\nframework can not achieve an approximation any better than Omega(log(n)/log\nlog(n)) without re-routing flow."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2009.12.008", 
    "link": "http://arxiv.org/pdf/0908.1528v2", 
    "other_authors": "Robert Geisberger", 
    "title": "Contraction of Timetable Networks with Realistic Transfers", 
    "arxiv-id": "0908.1528v2", 
    "author": "Robert Geisberger", 
    "publish": "2009-08-11T16:03:46Z", 
    "summary": "We successfully contract timetable networks with realistic transfer times.\nContraction gradually removes nodes from the graph and adds shortcuts to\npreserve shortest paths. This reduces query times to 1 ms with preprocessing\ntimes around 6 minutes on all tested instances. We achieve this by an improved\ncontraction algorithm and by using a station graph model. Every node in our\ngraph has a one-to-one correspondence to a station and every edge has an\nassigned collection of connections. Our graph model does not need parallel\nedges. The query algorithm does not compute a single earliest arrival time at a\nstation but a set of arriving connections that allow best transfer\nopportunities."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.2256v3", 
    "other_authors": "Nikhil Bansal, Nitish Korula, Viswanath Nagarajan, Aravind Srinivasan", 
    "title": "On k-Column Sparse Packing Programs", 
    "arxiv-id": "0908.2256v3", 
    "author": "Aravind Srinivasan", 
    "publish": "2009-08-16T17:55:03Z", 
    "summary": "We consider the class of packing integer programs (PIPs) that are column\nsparse, i.e. there is a specified upper bound k on the number of constraints\nthat each variable appears in. We give an (ek+o(k))-approximation algorithm for\nk-column sparse PIPs, improving on recent results of $k^2\\cdot 2^k$ and\n$O(k^2)$. We also show that the integrality gap of our linear programming\nrelaxation is at least 2k-1; it is known that k-column sparse PIPs are\n$\\Omega(k/ \\log k)$-hard to approximate. We also extend our result (at the loss\nof a small constant factor) to the more general case of maximizing a submodular\nobjective over k-column sparse packing constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.2834v1", 
    "other_authors": "Yossi Azar, Benjamin Birnbaum, Anna R. Karlin, C. Thach Nguyen", 
    "title": "On Revenue Maximization in Second-Price Ad Auctions", 
    "arxiv-id": "0908.2834v1", 
    "author": "C. Thach Nguyen", 
    "publish": "2009-08-19T23:46:02Z", 
    "summary": "Most recent papers addressing the algorithmic problem of allocating\nadvertisement space for keywords in sponsored search auctions assume that\npricing is done via a first-price auction, which does not realistically model\nthe Generalized Second Price (GSP) auction used in practice. Towards the goal\nof more realistically modeling these auctions, we introduce the Second-Price Ad\nAuctions problem, in which bidders' payments are determined by the GSP\nmechanism. We show that the complexity of the Second-Price Ad Auctions problem\nis quite different than that of the more studied First-Price Ad Auctions\nproblem. First, unlike the first-price variant, for which small constant-factor\napproximations are known, it is NP-hard to approximate the Second-Price Ad\nAuctions problem to any non-trivial factor. Second, this discrepancy extends\neven to the 0-1 special case that we call the Second-Price Matching problem\n(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no\ndeterministic algorithm achieving a non-trivial competitive ratio and no\nrandomized algorithm achieving a competitive ratio better than 2. This stands\nin contrast to the results for the analogous special case in the first-price\nmodel, the standard bipartite matching problem, which is solvable in polynomial\ntime and which has deterministic and randomized online algorithms achieving\nbetter competitive ratios. On the positive side, we provide a 2-approximation\nfor offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.\nThe latter result makes use of a new generalization of a classic result on the\nperformance of the \"Ranking\" algorithm for online bipartite matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.3089v1", 
    "other_authors": "Maxim A. Kolosovskiy", 
    "title": "Data structure for representing a graph: combination of linked list and   hash table", 
    "arxiv-id": "0908.3089v1", 
    "author": "Maxim A. Kolosovskiy", 
    "publish": "2009-08-21T10:12:31Z", 
    "summary": "In this article we discuss a data structure, which combines advantages of two\ndifferent ways for representing graphs: adjacency matrix and collection of\nadjacency lists. This data structure can fast add and search edges (advantages\nof adjacency matrix), use linear amount of memory, let to obtain adjacency list\nfor certain vertex (advantages of collection of adjacency lists). Basic\nknowledge of linked lists and hash tables is required to understand this\narticle. The article contains examples of implementation on Java."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0908.3505v2", 
    "other_authors": "Philippe Baptiste, Marek Chrobak, Christoph Durr", 
    "title": "Polynomial Time Algorithms for Minimum Energy Scheduling", 
    "arxiv-id": "0908.3505v2", 
    "author": "Christoph Durr", 
    "publish": "2009-08-24T21:44:45Z", 
    "summary": "The aim of power management policies is to reduce the amount of energy\nconsumed by computer systems while maintaining satisfactory level of\nperformance. One common method for saving energy is to simply suspend the\nsystem during the idle times. No energy is consumed in the suspend mode.\nHowever, the process of waking up the system itself requires a certain fixed\namount of energy, and thus suspending the system is beneficial only if the idle\ntime is long enough to compensate for this additional energy expenditure. In\nthe specific problem studied in the paper, we have a set of jobs with release\ntimes and deadlines that need to be executed on a single processor. Preemptions\nare allowed. The processor requires energy L to be woken up and, when it is on,\nit uses one unit of energy per one unit of time. It has been an open problem\nwhether a schedule minimizing the overall energy consumption can be computed in\npolynomial time. We solve this problem in positive, by providing an O(n^5)-time\nalgorithm. In addition we provide an O(n^4)-time algorithm for computing the\nminimum energy schedule when all jobs have unit length."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0460v3", 
    "other_authors": "Andreas Bj\u00f6rklund", 
    "title": "Exact Covers via Determinants", 
    "arxiv-id": "0910.0460v3", 
    "author": "Andreas Bj\u00f6rklund", 
    "publish": "2009-10-02T19:36:39Z", 
    "summary": "Given a k-uniform hypergraph on n vertices, partitioned in k equal parts such\nthat every hyperedge includes one vertex from each part, the k-dimensional\nmatching problem asks whether there is a disjoint collection of the hyperedges\nwhich covers all vertices. We show it can be solved by a randomized polynomial\nspace algorithm in time O*(2^(n(k-2)/k)). The O*() notation hides factors\npolynomial in n and k.\n  When we drop the partition constraint and permit arbitrary hyperedges of\ncardinality k, we obtain the exact cover by k-sets problem. We show it can be\nsolved by a randomized polynomial space algorithm in time O*(c_k^n), where\nc_3=1.496, c_4=1.642, c_5=1.721, and provide a general bound for larger k.\n  Both results substantially improve on the previous best algorithms for these\nproblems, especially for small k, and follow from the new observation that\nLovasz' perfect matching detection via determinants (1979) admits an embedding\nin the recently proposed inclusion-exclusion counting scheme for set covers,\ndespite its inability to count the perfect matchings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0504v2", 
    "other_authors": "Jos\u00e9 Soto", 
    "title": "Improved Analysis of a Max Cut Algorithm Based on Spectral Partitioning", 
    "arxiv-id": "0910.0504v2", 
    "author": "Jos\u00e9 Soto", 
    "publish": "2009-10-05T19:58:59Z", 
    "summary": "Trevisan [SICOMP 2012] presented an algorithm for Max-Cut based on spectral\npartitioning techniques. This is the first algorithm for Max-Cut with an\napproximation guarantee strictly larger than 1/2 that is not based on\nsemidefinite programming. Trevisan showed that its approximation ratio is of at\nleast 0.531. In this paper we improve this bound up to 0.614247. We also define\nand extend this result for the more general Maximum Colored Cut problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_28", 
    "link": "http://arxiv.org/pdf/0910.0553v1", 
    "other_authors": "Michel X. Goemans", 
    "title": "Combining Approximation Algorithms for the Prize-Collecting TSP", 
    "arxiv-id": "0910.0553v1", 
    "author": "Michel X. Goemans", 
    "publish": "2009-10-03T15:48:44Z", 
    "summary": "We present a 1.91457-approximation algorithm for the prize-collecting\ntravelling salesman problem. This is obtained by combining a randomized variant\nof a rounding algorithm of Bienstock et al. and a primal-dual algorithm of\nGoemans and Williamson."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.0832v1", 
    "other_authors": "Anissa Lamani, Maria Potop-Butucaru, S\u00e9bastien Tixeuil", 
    "title": "Optimal deterministic ring exploration with oblivious asynchronous   robots", 
    "arxiv-id": "0910.0832v1", 
    "author": "S\u00e9bastien Tixeuil", 
    "publish": "2009-10-05T19:08:49Z", 
    "summary": "We consider the problem of exploring an anonymous unoriented ring of size $n$\nby $k$ identical, oblivious, asynchronous mobile robots, that are unable to\ncommunicate, yet have the ability to sense their environment and take decisions\nbased on their local view. Previous works in this weak scenario prove that $k$\nmust not divide $n$ for a deterministic solution to exist. Also, it is known\nthat the minimum number of robots (either deterministic or probabilistic) to\nexplore a ring of size $n$ is 4. An upper bound of 17 robots holds in the\ndeterministic case while 4 probabilistic robots are sufficient. In this paper,\nwe close the complexity gap in the deterministic setting, by proving that no\ndeterministic exploration is feasible with less than five robots whenever the\nsize of the ring is even, and that five robots are sufficient for any $n$ that\nis coprime with five. Our protocol completes exploration in O(n) robot moves,\nwhich is also optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.1969v1", 
    "other_authors": "Ajinkya Kale, Shaunak Vaidya, Ashish Joglekar", 
    "title": "A Generalized Recursive Algorithm for Binary Multiplication based on   Vedic Mathematics", 
    "arxiv-id": "0910.1969v1", 
    "author": "Ashish Joglekar", 
    "publish": "2009-10-11T04:57:22Z", 
    "summary": "A generalized algorithm for multiplication is proposed through recursive\napplication of the Nikhilam Sutra from Vedic Mathematics, operating in radix -\n2 number system environment suitable for digital platforms. Statistical\nanalysis has been carried out based on the number of recursions profile as a\nfunction of the smaller multiplicand. The proposed algorithm is efficient for\nsmaller multiplicands as well, unlike most of the asymptotically fast\nalgorithms. Further, a basic block schematic of Hardware Implementation of our\nalgorithm is suggested to exploit parallelism and speed up the implementation\nof the algorithm in a multiprocessor environment."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.3123v2", 
    "other_authors": "Johannes Fischer", 
    "title": "Wee LCP", 
    "arxiv-id": "0910.3123v2", 
    "author": "Johannes Fischer", 
    "publish": "2009-10-16T13:50:12Z", 
    "summary": "We prove that longest common prefix (LCP) information can be stored in much\nless space than previously known. More precisely, we show that in the presence\nof the text and the suffix array, o(n) additional bits are sufficient to answer\nLCP-queries asymptotically in the same time that is needed to retrieve an entry\nfrom the suffix array. This yields the smallest compressed suffix tree with\nsub-logarithmic navigation time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13284-1_15", 
    "link": "http://arxiv.org/pdf/0910.3243v1", 
    "other_authors": "Krzysztof Onak", 
    "title": "Testing Distribution Identity Efficiently", 
    "arxiv-id": "0910.3243v1", 
    "author": "Krzysztof Onak", 
    "publish": "2009-10-16T22:26:02Z", 
    "summary": "We consider the problem of testing distribution identity. Given a sequence of\nindependent samples from an unknown distribution on a domain of size n, the\ngoal is to check if the unknown distribution approximately equals a known\ndistribution on the same domain. While Batu, Fortnow, Fischer, Kumar,\nRubinfeld, and White (FOCS 2001) proved that the sample complexity of the\nproblem is O~(sqrt(n) * poly(1/epsilon)), the running time of their tester is\nmuch higher: O(n) + O~(sqrt(n) * poly(1/epsilon)). We modify their tester to\nachieve a running time of O~(sqrt(n) * poly(1/epsilon))."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-010-9424-y", 
    "link": "http://arxiv.org/pdf/0910.3503v2", 
    "other_authors": "Benjamin A. Burton", 
    "title": "Searching a bitstream in linear time for the longest substring of any   given density", 
    "arxiv-id": "0910.3503v2", 
    "author": "Benjamin A. Burton", 
    "publish": "2009-10-19T10:06:34Z", 
    "summary": "Given an arbitrary bitstream, we consider the problem of finding the longest\nsubstring whose ratio of ones to zeroes equals a given value. The central\nresult of this paper is an algorithm that solves this problem in linear time.\nThe method involves (i) reformulating the problem as a constrained walk through\na sparse matrix, and then (ii) developing a data structure for this sparse\nmatrix that allows us to perform each step of the walk in amortised constant\ntime. We also give a linear time algorithm to find the longest substring whose\nratio of ones to zeroes is bounded below by a given value. Both problems have\npractical relevance to cryptography and bioinformatics."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0910.5599v1", 
    "other_authors": "Boaz Patt-Shamir, Dror Rawitz", 
    "title": "Vector Bin Packing with Multiple-Choice", 
    "arxiv-id": "0910.5599v1", 
    "author": "Dror Rawitz", 
    "publish": "2009-10-29T10:00:37Z", 
    "summary": "We consider a variant of bin packing called multiple-choice vector bin\npacking. In this problem we are given a set of items, where each item can be\nselected in one of several $D$-dimensional incarnations. We are also given $T$\nbin types, each with its own cost and $D$-dimensional size. Our goal is to pack\nthe items in a set of bins of minimum overall cost. The problem is motivated by\nscheduling in networks with guaranteed quality of service (QoS), but due to its\ngeneral formulation it has many other applications as well. We present an\napproximation algorithm that is guaranteed to produce a solution whose cost is\nabout $\\ln D$ times the optimum. For the running time to be polynomial we\nrequire $D=O(1)$ and $T=O(\\log n)$. This extends previous results for vector\nbin packing, in which each item has a single incarnation and there is only one\nbin type. To obtain our result we also present a PTAS for the multiple-choice\nversion of multidimensional knapsack, where we are given only one bin and the\ngoal is to pack a maximum weight set of (incarnations of) items in that bin."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0910.5744v1", 
    "other_authors": "Lucie Galand, Olivier Spanjaard", 
    "title": "Exact algorithms for OWA-optimization in multiobjective spanning tree   problems", 
    "arxiv-id": "0910.5744v1", 
    "author": "Olivier Spanjaard", 
    "publish": "2009-10-29T23:17:24Z", 
    "summary": "This paper deals with the multiobjective version of the optimal spanning tree\nproblem. More precisely, we are interested in determining the optimal spanning\ntree according to an Ordered Weighted Average (OWA) of its objective values. We\nfirst show that the problem is weakly NP-hard. In the case where the weights of\nthe OWA are strictly decreasing, we then propose a mixed integer programming\nformulation, and provide dedicated optimality conditions yielding an important\nreduction of the size of the program. Next, we present two bounds that can be\nused to prune subspaces of solutions either in a shaving phase or in a branch\nand bound procedure. The validity of these bounds does not depend on specific\nproperties of the weights (apart from non-negativity). All these exact\nresolution algorithms are compared on the basis of numerical experiments,\naccording to their respective validity scopes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0174v1", 
    "other_authors": "Muhammad Aasim Qureshi, Dr. Fadzil B. Hassan, Sohail Safdar, Rehan Akbar", 
    "title": "A O(E) Time Shortest Path Algorithm For Non Negative Weighted Undirected   Graphs", 
    "arxiv-id": "0911.0174v1", 
    "author": "Rehan Akbar", 
    "publish": "2009-11-02T20:24:05Z", 
    "summary": "In most of the shortest path problems like vehicle routing problems and\nnetwork routing problems, we only need an efficient path between two points\nsource and destination, and it is not necessary to calculate the shortest path\nfrom source to all other nodes. This paper concentrates on this very idea and\npresents an algorithm for calculating shortest path for (i) nonnegative\nweighted undirected graphs (ii) unweighted undirected graphs. The algorithm\ncompletes its execution in O(E) for all graphs except few in which longer path\n(in terms of number of edges) from source to some node makes it best selection\nfor that node. The main advantage of the algorithms is its simplicity and it\ndoes not need complex data structures for implementations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0397v1", 
    "other_authors": "Keehang Kwon, Hong Pyo Ha", 
    "title": "Algorithm as Defining Dynamic Systems", 
    "arxiv-id": "0911.0397v1", 
    "author": "Hong Pyo Ha", 
    "publish": "2009-11-02T20:01:19Z", 
    "summary": "This paper proposes a new view to algorithms, Algorithms as defining dynamic\nsystems. This view extends the traditional, deterministic view that an\nalgorithm is a step by step procedure with nondeterminism. As a dynamic system\ncan be designed by a set of its defining laws, it is also desirable to design\nan algorithm by a (possibly nondeterministic) set of defining laws. This\nobservation requires some changes to algorithm development. We propose a two\nstep approach, the first step is to design an algorithm via a set of defining\nlaws of dynamic system. The second step is to translate these laws (written in\na natural language) into a formal language such as linear logic."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.0577v2", 
    "other_authors": "Philip Bille, Inge Li Goertz", 
    "title": "Fast Arc-Annotated Subsequence Matching in Linear Space", 
    "arxiv-id": "0911.0577v2", 
    "author": "Inge Li Goertz", 
    "publish": "2009-11-03T13:35:41Z", 
    "summary": "An arc-annotated string is a string of characters, called bases, augmented\nwith a set of pairs, called arcs, each connecting two bases. Given\narc-annotated strings $P$ and $Q$ the arc-preserving subsequence problem is to\ndetermine if $P$ can be obtained from $Q$ by deleting bases from $Q$. Whenever\na base is deleted any arc with an endpoint in that base is also deleted.\nArc-annotated strings where the arcs are ``nested'' are a natural model of RNA\nmolecules that captures both the primary and secondary structure of these. The\narc-preserving subsequence problem for nested arc-annotated strings is basic\nprimitive for investigating the function of RNA molecules. Gramm et al. [ACM\nTrans. Algorithms 2006] gave an algorithm for this problem using $O(nm)$ time\nand space, where $m$ and $n$ are the lengths of $P$ and $Q$, respectively. In\nthis paper we present a new algorithm using $O(nm)$ time and $O(n + m)$ space,\nthereby matching the previous time bound while significantly reducing the space\nfrom a quadratic term to linear. This is essential to process large RNA\nmolecules where the space is likely to be a bottleneck. To obtain our result we\nintroduce several novel ideas which may be of independent interest for related\nproblems on arc-annotated strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.1626v2", 
    "other_authors": "Marek Cygan, Lukasz Kowalik, Marcin Mucha, Marcin Pilipczuk, Piotr Sankowski", 
    "title": "Fast Approximation in Subspaces by Doubling Metric Decomposition", 
    "arxiv-id": "0911.1626v2", 
    "author": "Piotr Sankowski", 
    "publish": "2009-11-09T10:29:49Z", 
    "summary": "In this paper we propose and study a new complexity model for approximation\nalgorithms. The main motivation are practical problems over large data sets\nthat need to be solved many times for different scenarios, e.g., many multicast\ntrees that need to be constructed for different groups of users. In our model\nwe allow a preprocessing phase, when some information of the input graph\n$G=(V,E)$ is stored in a limited size data structure. Next, the data structure\nenables processing queries of the form ``solve problem A for an input\n$S\\subseteq V$''. We consider problems like {\\sc Steiner Forest}, {\\sc Facility\nLocation}, {\\sc $k$-Median}, {\\sc $k$-Center} and {\\sc TSP} in the case when\nthe graph induces a doubling metric. Our main results are data structures of\nnear-linear size that are able to answer queries in time close to linear in\n$|S|$. This improves over typical worst case reuniting time of approximation\nalgorithms in the classical setting which is $\\Omega(|E|)$ independently of the\nquery size. In most cases, our approximation guarantees are arbitrarily close\nto those in the classical setting. Additionally, we present the first fully\ndynamic algorithm for the Steiner tree problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.1765v1", 
    "other_authors": "Justin Kennedy, Ion I. Mandoiu, Bogdan Pasaniuc", 
    "title": "GEDI: Scalable Algorithms for Genotype Error Detection and Imputation", 
    "arxiv-id": "0911.1765v1", 
    "author": "Bogdan Pasaniuc", 
    "publish": "2009-11-09T23:35:41Z", 
    "summary": "Genome-wide association studies generate very large datasets that require\nscalable analysis algorithms. In this report we describe the GEDI software\npackage, which implements efficient algorithms for performing several common\ntasks in the analysis of population genotype data, including genotype error\ndetection and correction, imputation of both randomly missing and untyped\ngenotypes, and genotype phasing. Experimental results show that GEDI achieves\nhigh accuracy with a runtime scaling linearly with the number of markers and\nsamples. The open source C++ code of GEDI, released under the GNU General\nPublic License, is available for download at\nhttp://dna.engr.uconn.edu/software/GEDI/"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_24", 
    "link": "http://arxiv.org/pdf/0911.2924v2", 
    "other_authors": "Mika G\u00f6\u00f6s, Pekka Orponen", 
    "title": "Synthesizing Minimal Tile Sets for Patterned DNA Self-Assembly", 
    "arxiv-id": "0911.2924v2", 
    "author": "Pekka Orponen", 
    "publish": "2009-11-16T07:43:41Z", 
    "summary": "The Pattern self-Assembly Tile set Synthesis (PATS) problem is to determine a\nset of coloured tiles that self-assemble to implement a given rectangular\ncolour pattern. We give an exhaustive branch-and-bound algorithm to find tile\nsets of minimum cardinality for the PATS problem. Our algorithm makes use of a\nsearch tree in the lattice of partitions of the ambient rectangular grid, and\nan efficient bounding function to prune this search tree. Empirical data on the\nperformance of the algorithm shows that it compares favourably to previously\npresented heuristic solutions to the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_6", 
    "link": "http://arxiv.org/pdf/0911.3355v1", 
    "other_authors": "Zhi Xu", 
    "title": "A Minimal Periods Algorithm with Applications", 
    "arxiv-id": "0911.3355v1", 
    "author": "Zhi Xu", 
    "publish": "2009-11-17T17:34:23Z", 
    "summary": "Kosaraju in ``Computation of squares in a string'' briefly described a\nlinear-time algorithm for computing the minimal squares starting at each\nposition in a word. Using the same construction of suffix trees, we generalize\nhis result and describe in detail how to compute in O(k|w|)-time the minimal\nk-th power, with period of length larger than s, starting at each position in a\nword w for arbitrary exponent $k\\geq2$ and integer $s\\geq0$. We provide the\ncomplete proof of correctness of the algorithm, which is somehow not completely\nclear in Kosaraju's original paper. The algorithm can be used as a sub-routine\nto detect certain types of pseudo-patterns in words, which is our original\nintention to study the generalization."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_6", 
    "link": "http://arxiv.org/pdf/0911.4384v3", 
    "other_authors": "Magnus Lie Hetland", 
    "title": "Ptolemaic Indexing", 
    "arxiv-id": "0911.4384v3", 
    "author": "Magnus Lie Hetland", 
    "publish": "2009-11-23T12:16:15Z", 
    "summary": "This paper discusses a new family of bounds for use in similarity search,\nrelated to those used in metric indexing, but based on Ptolemy's inequality,\nrather than the metric axioms. Ptolemy's inequality holds for the well-known\nEuclidean distance, but is also shown here to hold for quadratic form metrics\nin general, with Mahalanobis distance as an important special case. The\ninequality is examined empirically on both synthetic and real-world data sets\nand is also found to hold approximately, with a very low degree of error, for\nimportant distances such as the angular pseudometric and several Lp norms.\nIndexing experiments demonstrate a highly increased filtering power compared to\nexisting, triangular methods. It is also shown that combining the Ptolemaic and\ntriangular filtering can lead to better results than using either approach on\nits own."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.4544v1", 
    "other_authors": "Anand Louis, Nisheeth Vishnoi", 
    "title": "Improved Algorithm for Degree Bounded Survivable Network Design Problem", 
    "arxiv-id": "0911.4544v1", 
    "author": "Nisheeth Vishnoi", 
    "publish": "2009-11-24T05:27:19Z", 
    "summary": "We consider the Degree-Bounded Survivable Network Design Problem: the\nobjective is to find a minimum cost subgraph satisfying the given connectivity\nrequirements as well as the degree bounds on the vertices. If we denote the\nupper bound on the degree of a vertex v by b(v), then we present an algorithm\nthat finds a solution whose cost is at most twice the cost of the optimal\nsolution while the degree of a degree constrained vertex v is at most 2b(v) +\n2. This improves upon the results of Lau and Singh and that of Lau, Naor,\nSalavatipour and Singh."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.4981v4", 
    "other_authors": "Jeremy Barbay, Francisco Claude, Travis Gagie, Gonzalo Navarro, Yakov Nekrich", 
    "title": "Efficient Fully-Compressed Sequence Representations", 
    "arxiv-id": "0911.4981v4", 
    "author": "Yakov Nekrich", 
    "publish": "2009-11-25T23:31:23Z", 
    "summary": "We present a data structure that stores a sequence $s[1..n]$ over alphabet\n$[1..\\sigma]$ in $n\\Ho(s) + o(n)(\\Ho(s){+}1)$ bits, where $\\Ho(s)$ is the\nzero-order entropy of $s$. This structure supports the queries \\access, \\rank\\\nand \\select, which are fundamental building blocks for many other compressed\ndata structures, in worst-case time $\\Oh{\\lg\\lg\\sigma}$ and average time\n$\\Oh{\\lg \\Ho(s)}$. The worst-case complexity matches the best previous results,\nyet these had been achieved with data structures using $n\\Ho(s)+o(n\\lg\\sigma)$\nbits. On highly compressible sequences the $o(n\\lg\\sigma)$ bits of the\nredundancy may be significant compared to the the $n\\Ho(s)$ bits that encode\nthe data. Our representation, instead, compresses the redundancy as well.\nMoreover, our average-case complexity is unprecedented. Our technique is based\non partitioning the alphabet into characters of similar frequency. The\nsubsequence corresponding to each group can then be encoded using fast\nuncompressed representations without harming the overall compression ratios,\neven in the redundancy. The result also improves upon the best current\ncompressed representations of several other data structures. For example, we\nachieve $(i)$ compressed redundancy, retaining the best time complexities, for\nthe smallest existing full-text self-indexes; $(ii)$ compressed permutations\n$\\pi$ with times for $\\pi()$ and $\\pii()$ improved to loglogarithmic; and\n$(iii)$ the first compressed representation of dynamic collections of disjoint\nsets. We also point out various applications to inverted indexes, suffix\narrays, binary relations, and data compressors. ..."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5031v2", 
    "other_authors": "Shihabur Rahman Chowdhury, Masud Hasan, Sumaiya Iqbal, M. Sohel Rahman", 
    "title": "An $O(n^2)$ Algorithm for Computing Longest Common Cyclic Subsequence", 
    "arxiv-id": "0911.5031v2", 
    "author": "M. Sohel Rahman", 
    "publish": "2009-11-26T08:49:48Z", 
    "summary": "The {\\em longest common subsequence (LCS)} problem is a classic and\nwell-studied problem in computer science. LCS is a central problem in\nstringology and finds broad applications in text compression, error-detecting\ncodes and biological sequence comparison. However, in numerous contexts, words\nrepresent cyclic sequences of symbols and LCS must be generalized to consider\nall circular shifts of the strings. This occurs especially in computational\nbiology when genetic material is sequenced form circular DNA or RNA molecules.\nThis initiates the problem of {\\em longest common cyclic subsequence (LCCS)}\nwhich finds the longest subsequence between all circular shifts of two strings.\nIn this paper, we give an $O(n^2)$ algorithm for solving LCCS problem where $n$\nis the number of symbols in the strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5094v1", 
    "other_authors": "Uriel Feige", 
    "title": "Faster FAST(Feedback Arc Set in Tournaments)", 
    "arxiv-id": "0911.5094v1", 
    "author": "Uriel Feige", 
    "publish": "2009-11-26T14:43:31Z", 
    "summary": "We present an algorithm that finds a feedback arc set of size $k$ in a\ntournament in time $n^{O(1)}2^{O(\\sqrt{k})}$. This is asymptotically faster\nthan the running time of previously known algorithms for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0911.5143v1", 
    "other_authors": "MohammadHossein Bateni, MohammadTaghi Hajiaghayi, D\u00e1niel Marx", 
    "title": "Approximation Schemes for Steiner Forest on Planar Graphs and Graphs of   Bounded Treewidth", 
    "arxiv-id": "0911.5143v1", 
    "author": "D\u00e1niel Marx", 
    "publish": "2009-11-26T19:19:53Z", 
    "summary": "We give the first polynomial-time approximation scheme (PTAS) for the Steiner\nforest problem on planar graphs and, more generally, on graphs of bounded\ngenus. As a first step, we show how to build a Steiner forest spanner for such\ngraphs. The crux of the process is a clustering procedure called\nprize-collecting clustering that breaks down the input instance into separate\nsubinstances which are easier to handle; moreover, the terminals in different\nsubinstances are far from each other. Each subinstance has a relatively\ninexpensive Steiner tree connecting all its terminals, and the subinstances can\nbe solved (almost) separately. Another building block is a PTAS for Steiner\nforest on graphs of bounded treewidth. Surprisingly, Steiner forest is NP-hard\neven on graphs of treewidth 3. Therefore, our PTAS for bounded treewidth graph\nneeds a nontrivial combination of approximation arguments and dynamic\nprogramming on the tree decomposition. We further show that Steiner forest can\nbe solved in polynomial time for series-parallel graphs (graphs of treewidth at\nmost two) by a novel combination of dynamic programming and minimum cut\ncomputations, completing our thorough complexity study of Steiner forest in the\nrange of bounded treewidth graphs, planar graphs, and bounded genus graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0287v3", 
    "other_authors": "Martin Dietzfelbinger, Andreas Goerdt, Michael Mitzenmacher, Andrea Montanari, Rasmus Pagh, Michael Rink", 
    "title": "Tight Thresholds for Cuckoo Hashing via XORSAT", 
    "arxiv-id": "0912.0287v3", 
    "author": "Michael Rink", 
    "publish": "2009-12-01T22:23:48Z", 
    "summary": "We settle the question of tight thresholds for offline cuckoo hashing. The\nproblem can be stated as follows: we have n keys to be hashed into m buckets\neach capable of holding a single key. Each key has k >= 3 (distinct) associated\nbuckets chosen uniformly at random and independently of the choices of other\nkeys. A hash table can be constructed successfully if each key can be placed\ninto one of its buckets. We seek thresholds alpha_k such that, as n goes to\ninfinity, if n/m <= alpha for some alpha < alpha_k then a hash table can be\nconstructed successfully with high probability, and if n/m >= alpha for some\nalpha > alpha_k a hash table cannot be constructed successfully with high\nprobability. Here we are considering the offline version of the problem, where\nall keys and hash values are given, so the problem is equivalent to previous\nmodels of multiple-choice hashing. We find the thresholds for all values of k >\n2 by showing that they are in fact the same as the previously known thresholds\nfor the random k-XORSAT problem. We then extend these results to the setting\nwhere keys can have differing number of choices, and provide evidence in the\nform of an algorithm for a conjecture extending this result to cuckoo hash\ntables that store multiple keys in a bucket."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0322v4", 
    "other_authors": "Shaddin Dughmi", 
    "title": "Submodular Functions: Extensions, Distributions, and Algorithms. A   Survey", 
    "arxiv-id": "0912.0322v4", 
    "author": "Shaddin Dughmi", 
    "publish": "2009-12-02T02:02:27Z", 
    "summary": "Submodularity is a fundamental phenomenon in combinatorial optimization.\nSubmodular functions occur in a variety of combinatorial settings such as\ncoverage problems, cut problems, welfare maximization, and many more.\nTherefore, a lot of work has been concerned with maximizing or minimizing a\nsubmodular function, often subject to combinatorial constraints. Many of these\nalgorithmic results exhibit a common structure. Namely, the function is\nextended to a continuous, usually non-linear, function on a convex domain.\nThen, this relaxation is solved, and the fractional solution rounded to yield\nan integral solution. Often, the continuous extension has a natural\ninterpretation in terms of distributions on subsets of the ground set. This\ninterpretation is often crucial to the results and their analysis. The purpose\nof this survey is to highlight this connection between extensions,\ndistributions, relaxations, and optimization in the context of submodular\nfunctions. We also present the first constant factor approximation algorithm\nfor minimizing symmetric submodular functions subject to a cardinality\nconstraint."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0681v3", 
    "other_authors": "Michael W. Mahoney, Lorenzo Orecchia, Nisheeth K. Vishnoi", 
    "title": "A Local Spectral Method for Graphs: with Applications to Improving Graph   Partitions and Exploring Data Graphs Locally", 
    "arxiv-id": "0912.0681v3", 
    "author": "Nisheeth K. Vishnoi", 
    "publish": "2009-12-03T15:20:59Z", 
    "summary": "The second eigenvalue of the Laplacian matrix and its associated eigenvector\nare fundamental features of an undirected graph, and as such they have found\nwidespread use in scientific computing, machine learning, and data analysis. In\nmany applications, however, graphs that arise have several \\emph{local} regions\nof interest, and the second eigenvector will typically fail to provide\ninformation fine-tuned to each local region. In this paper, we introduce a\nlocally-biased analogue of the second eigenvector, and we demonstrate its\nusefulness at highlighting local properties of data graphs in a semi-supervised\nmanner. To do so, we first view the second eigenvector as the solution to a\nconstrained optimization problem, and we incorporate the local information as\nan additional constraint; we then characterize the optimal solution to this new\nproblem and show that it can be interpreted as a generalization of a\nPersonalized PageRank vector; and finally, as a consequence, we show that the\nsolution can be computed in nearly-linear time. In addition, we show that this\nlocally-biased vector can be used to compute an approximation to the best\npartition \\emph{near} an input seed set in a manner analogous to the way in\nwhich the second eigenvector of the Laplacian can be used to obtain an\napproximation to the best partition in the entire input graph. Such a primitive\nis useful for identifying and refining clusters locally, as it allows us to\nfocus on a local region of interest in a semi-supervised manner. Finally, we\nprovide a detailed empirical evaluation of our method by showing how it can\napplied to finding locally-biased sparse cuts around an input vertex seed set\nin social and information networks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0850v4", 
    "other_authors": "Travis Gagie, Pawel Gawrychowski", 
    "title": "Grammar-Based Compression in a Streaming Model", 
    "arxiv-id": "0912.0850v4", 
    "author": "Pawel Gawrychowski", 
    "publish": "2009-12-04T13:17:22Z", 
    "summary": "We show that, given a string $s$ of length $n$, with constant memory and\nlogarithmic passes over a constant number of streams we can build a\ncontext-free grammar that generates $s$ and only $s$ and whose size is within\nan $\\Oh{\\min (g \\log g, \\sqrt{n \\log g})}$-factor of the minimum $g$. This\nstands in contrast to our previous result that, with polylogarithmic memory and\npolylogarithmic passes over a single stream, we cannot build such a grammar\nwhose size is within any polynomial of $g$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.0975v1", 
    "other_authors": "Julian J. McAuley, Tib\u00e9rio S. Caetano", 
    "title": "An expected-case sub-cubic solution to the all-pairs shortest path   problem in R", 
    "arxiv-id": "0912.0975v1", 
    "author": "Tib\u00e9rio S. Caetano", 
    "publish": "2009-12-05T03:31:07Z", 
    "summary": "It has been shown by Alon et al. that the so-called 'all-pairs shortest-path'\nproblem can be solved in O((MV)^2.688 * log^3(V)) for graphs with V vertices,\nwith integer distances bounded by M. We solve the more general problem for\ngraphs in R (assuming no negative cycles), with expected-case running time\nO(V^2.5 * log(V)). While our result appears to violate the Omega(V^3)\nrequirement of \"Funny Matrix Multiplication\" (due to Kerr), we find that it has\na sub-cubic expected time solution subject to reasonable conditions on the data\ndistribution. The expected time solution arises when certain sub-problems are\nuncorrelated, though we can do better/worse than the expected-case under\npositive/negative correlation (respectively). Whether we observe\npositive/negative correlation depends on the statistics of the graph in\nquestion. In practice, our algorithm is significantly faster than\nFloyd-Warshall, even for dense graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.1045v3", 
    "other_authors": "Anupam Gupta, Viswanath Nagarajan, R. Ravi", 
    "title": "Thresholded Covering Algorithms for Robust and Max-Min Optimization", 
    "arxiv-id": "0912.1045v3", 
    "author": "R. Ravi", 
    "publish": "2009-12-05T19:04:49Z", 
    "summary": "The general problem of robust optimization is this: one of several possible\nscenarios will appear tomorrow, but things are more expensive tomorrow than\nthey are today. What should you anticipatorily buy today, so that the\nworst-case cost (summed over both days) is minimized? Feige et al. and\nKhandekar et al. considered the k-robust model where the possible outcomes\ntomorrow are given by all demand-subsets of size k, and gave algorithms for the\nset cover problem, and the Steiner tree and facility location problems in this\nmodel, respectively.\n  In this paper, we give the following simple and intuitive template for\nk-robust problems: \"having built some anticipatory solution, if there exists a\nsingle demand whose augmentation cost is larger than some threshold, augment\nthe anticipatory solution to cover this demand as well, and repeat\". In this\npaper we show that this template gives us improved approximation algorithms for\nk-robust Steiner tree and set cover, and the first approximation algorithms for\nk-robust Steiner forest, minimum-cut and multicut. All our approximation ratios\n(except for multicut) are almost best possible.\n  As a by-product of our techniques, we also get algorithms for max-min\nproblems of the form: \"given a covering problem instance, which k of the\nelements are costliest to cover?\"."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.1137v1", 
    "other_authors": "MohammadHossein Bateni, MohammadTaghi Hajiaghayi", 
    "title": "Euclidean Prize-collecting Steiner Forest", 
    "arxiv-id": "0912.1137v1", 
    "author": "MohammadTaghi Hajiaghayi", 
    "publish": "2009-12-06T21:15:49Z", 
    "summary": "In this paper, we consider Steiner forest and its generalizations,\nprize-collecting Steiner forest and k-Steiner forest, when the vertices of the\ninput graph are points in the Euclidean plane and the lengths are Euclidean\ndistances. First, we present a simpler analysis of the polynomial-time\napproximation scheme (PTAS) of Borradaile et al. [12] for the Euclidean Steiner\nforest problem. This is done by proving a new structural property and modifying\nthe dynamic programming by adding a new piece of information to each dynamic\nprogramming state. Next we develop a PTAS for a well-motivated case, i.e., the\nmultiplicative case, of prize-collecting and budgeted Steiner forest. The ideas\nused in the algorithm may have applications in design of a broad class of\nbicriteria PTASs. At the end, we demonstrate why PTASs for these problems can\nbe hard in the general Euclidean case (and thus for PTASs we cannot go beyond\nthe multiplicative case)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.2269v4", 
    "other_authors": "Charles Sauerbier", 
    "title": "Computing a Discrete Logarithm in O(n^3)", 
    "arxiv-id": "0912.2269v4", 
    "author": "Charles Sauerbier", 
    "publish": "2009-12-11T16:07:20Z", 
    "summary": "This paper presents a means with time complexity of at worst O(n^3) to\ncompute the discrete logarithm on cyclic finite groups of integers modulo p.\nThe algorithm makes use of reduction of the problem to that of finding the\nconcurrent zeros of two periodic functions in the real numbers. The problem is\ntreated as an analog to a form of analog rotor-code computed cipher."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.2813v2", 
    "other_authors": "Dai Tri Man Le", 
    "title": "Combining Partial Order Alignment and Progressive Near-Optimal Alignment", 
    "arxiv-id": "0912.2813v2", 
    "author": "Dai Tri Man Le", 
    "publish": "2009-12-15T07:21:33Z", 
    "summary": "In this paper, I proposed to utilize partial-order alignment technique as a\nheuristic method to cope with the state-space explosion problem in progressive\nnear-optimal alignment. The key idea of my approach is a formal treatment of\nprogressive partial order alignment based on the graph product construction."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.3188v2", 
    "other_authors": "Shiri Chechik, David Peleg", 
    "title": "Robust Fault Tolerant uncapacitated facility location", 
    "arxiv-id": "0912.3188v2", 
    "author": "David Peleg", 
    "publish": "2009-12-16T16:46:55Z", 
    "summary": "In the uncapacitated facility location problem, given a graph, a set of\ndemands and opening costs, it is required to find a set of facilities R, so as\nto minimize the sum of the cost of opening the facilities in R and the cost of\nassigning all node demands to open facilities. This paper concerns the robust\nfault-tolerant version of the uncapacitated facility location problem (RFTFL).\nIn this problem, one or more facilities might fail, and each demand should be\nsupplied by the closest open facility that did not fail. It is required to find\na set of facilities R, so as to minimize the sum of the cost of opening the\nfacilities in R and the cost of assigning all node demands to open facilities\nthat did not fail, after the failure of up to \\alpha facilities. We present a\npolynomial time algorithm that yields a 6.5-approximation for this problem with\nat most one failure and a 1.5 + 7.5\\alpha-approximation for the problem with at\nmost \\alpha > 1 failures. We also show that the RFTFL problem is NP-hard even\non trees, and even in the case of a single failure."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.3963v1", 
    "other_authors": "Hani M. AL-Matari, Sattar J. Aboud, Nidal F. Shilbayeh", 
    "title": "Fast Fraction-Integer Method for Computing Multiplicative Inverse", 
    "arxiv-id": "0912.3963v1", 
    "author": "Nidal F. Shilbayeh", 
    "publish": "2009-12-20T02:32:42Z", 
    "summary": "Multiplicative inverse is a crucial operation in public key cryptography, and\nbeen widely used in cryptography. Public key cryptography has given rise to\nsuch a need, in which we need to generate a related public and private pair of\nnumbers, each of which is the inverse of the other. The basic method to find\nmultiplicative inverses is Extended-Euclidean method. In this paper we will\npropose a new algorithm for computing the inverse, based on continues subtract\nfraction from integer and divide by fraction to obtain integer that will be\nused to compute the inverse d. The authors claim that the proposed method more\nefficient and faster than the existed methods."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.4569v2", 
    "other_authors": "Ho-Leung Chan, Tak-Wah Lam, Lap-Kei Lee, Hing-Fung Ting", 
    "title": "Continuous Monitoring of Distributed Data Streams over a Time-based   Sliding Window", 
    "arxiv-id": "0912.4569v2", 
    "author": "Hing-Fung Ting", 
    "publish": "2009-12-23T09:56:50Z", 
    "summary": "The past decade has witnessed many interesting algorithms for maintaining\nstatistics over a data stream. This paper initiates a theoretical study of\nalgorithms for monitoring distributed data streams over a time-based sliding\nwindow (which contains a variable number of items and possibly out-of-order\nitems). The concern is how to minimize the communication between individual\nstreams and the root, while allowing the root, at any time, to be able to\nreport the global statistics of all streams within a given error bound. This\npaper presents communication-efficient algorithms for three classical\nstatistics, namely, basic counting, frequent items and quantiles. The\nworst-case communication cost over a window is $O(\\frac{k} {\\epsilon} \\log\n\\frac{\\epsilon N}{k})$ bits for basic counting and $O(\\frac{k}{\\epsilon} \\log\n\\frac{N}{k})$ words for the remainings, where $k$ is the number of distributed\ndata streams, $N$ is the total number of items in the streams that arrive or\nexpire in the window, and $\\epsilon < 1$ is the desired error bound. Matching\nand nearly matching lower bounds are also obtained."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.4798v1", 
    "other_authors": "Ratthachat Chatpatanasiri, Thavivongse Sriburi", 
    "title": "Demand-Supply Optimization with Risk Management for a Multi-Connection   Water Reservoir Network", 
    "arxiv-id": "0912.4798v1", 
    "author": "Thavivongse Sriburi", 
    "publish": "2009-12-24T06:04:15Z", 
    "summary": "In this paper, we propose a framework to solve a demand-supply optimization\nproblem of long-term water resource allocation on a multi-connection reservoir\nnetwork which, in two aspects, is different to the problem considered in\nprevious works. First, while all previous works consider a problem where each\nreservoir can transfer water to only one fixed reservoir, we consider a\nmulti-connection network being constructed in Thailand in which each reservoir\ncan transfer water to many reservoirs in one period of time. Second, a\ndemand-supply plan considered here is static, in contrast to a dynamic policy\nconsidered in previous works. Moreover, in order to efficiently develop a\nlong-term static plan, a severe loss (a risk) is taken into account, i.e. a\nrisk occurs if the real amount of water stored in each reservoir in each time\nperiod is less than what planned by the optimizer. The multi-connection\nfunction and the risk make the problem rather complex such that traditional\nstochastic dynamic programming and deterministic/heuristic approaches are\ninappropriate. Our framework is based on a novel convex programming formulation\nin which stochastic information can be naturally taken into account and an\noptimal solution is guaranteed to be found efficiently. Extensive experimental\nresults show promising results of the framework."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.5424v3", 
    "other_authors": "Yuriy Arbitman, Moni Naor, Gil Segev", 
    "title": "Backyard Cuckoo Hashing: Constant Worst-Case Operations with a Succinct   Representation", 
    "arxiv-id": "0912.5424v3", 
    "author": "Gil Segev", 
    "publish": "2009-12-30T07:55:20Z", 
    "summary": "The performance of a dynamic dictionary is measured mainly by its update\ntime, lookup time, and space consumption. In terms of update time and lookup\ntime there are known constructions that guarantee constant-time operations in\nthe worst case with high probability, and in terms of space consumption there\nare known constructions that use essentially optimal space. However, although\nthe first analysis of a dynamic dictionary dates back more than 45 years ago\n(when Knuth analyzed linear probing in 1963), the trade-off between these\naspects of performance is still not completely understood. In this paper we\nsettle two fundamental open problems: - We construct the first dynamic\ndictionary that enjoys the best of both worlds: it stores n elements using\n(1+epsilon)n memory words, and guarantees constant-time operations in the worst\ncase with high probability. Specifically, for any epsilon = \\Omega((\\log\\log n\n/ \\log n)^{1/2} ) and for any sequence of polynomially many operations, with\nhigh probability over the randomness of the initialization phase, all\noperations are performed in constant time which is independent of epsilon. The\nconstruction is a two-level variant of cuckoo hashing, augmented with a\n\"backyard\" that handles a large fraction of the elements, together with a\nde-amortized perfect hashing scheme for eliminating the dependency on epsilon.\n- We present a variant of the above construction that uses only (1+o(1))B bits,\nwhere B is the information-theoretic lower bound for representing a set of size\nn taken from a universe of size u, and guarantees constant-time operations in\nthe worst case with high probability, as before. This problem was open even in\nthe amortized setting. One of the main ingredients of our construction is a\npermutation-based variant of cuckoo hashing, which significantly improves the\nspace consumption of cuckoo hashing when dealing with a rather small universe."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_38", 
    "link": "http://arxiv.org/pdf/0912.5473v1", 
    "other_authors": "Gerald Paul", 
    "title": "A Variable Depth Sequential Search Heuristic for the Quadratic   Assignment Problem", 
    "arxiv-id": "0912.5473v1", 
    "author": "Gerald Paul", 
    "publish": "2009-12-30T14:42:30Z", 
    "summary": "We develop a variable depth search heuristic for the quadratic assignment\nproblem. The heuristic is based on sequential changes in assignments analogous\nto the Lin-Kernighan sequential edge moves for the traveling salesman problem.\nWe treat unstructured problem instances of sizes 60 to 400. When the heuristic\nis used in conjunction with robust tabu search, we measure performance\nimprovements of up to a factor of 15 compared to the use of robust tabu alone.\nThe performance improvement increases as the problem size increases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0639v1", 
    "other_authors": "Jurek Czyzowicz, David Ilcinkas, Arnaud Labourel, Andrzej Pelc", 
    "title": "Optimal Exploration of Terrains with Obstacles", 
    "arxiv-id": "1001.0639v1", 
    "author": "Andrzej Pelc", 
    "publish": "2010-01-05T07:29:11Z", 
    "summary": "A mobile robot represented by a point moving in the plane has to explore an\nunknown terrain with obstacles. Both the terrain and the obstacles are modeled\nas arbitrary polygons. We consider two scenarios: the unlimited vision, when\nthe robot situated at a point p of the terrain explores (sees) all points q of\nthe terrain for which the segment pq belongs to the terrain, and the limited\nvision, when we require additionally that the distance between p and q be at\nmost 1. All points of the terrain (except obstacles) have to be explored and\nthe performance of an exploration algorithm is measured by the length of the\ntrajectory of the robot. For unlimited vision we show an exploration algorithm\nwith complexity O(P + D?k), where P is the total perimeter of the terrain\n(including perimeters of obstacles), D is the diameter of the convex hull of\nthe terrain, and k is the number of obstacles. We do not assume knowledge of\nthese parameters. We also prove a matching lower bound showing that the above\ncomplexity is optimal, even if the terrain is known to the robot. For limited\nvision we show exploration algorithms with complexity O(P + A + ?Ak), where A\nis the area of the terrain (excluding obstacles). Our algorithms work either\nfor arbitrary terrains, if one of the parameters A or k is known, or for c-fat\nterrains, where c is any constant (unknown to the robot) and no additional\nknowledge is assumed. (A terrain T with obstacles is c-fat if R/r ? c, where R\nis the radius of the smallest disc containing T and r is the radius of the\nlargest disc contained in T .) We also prove a matching lower bound ?(P + A +\n?Ak) on the complexity of exploration for limited vision, even if the terrain\nis known to the robot."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0824v2", 
    "other_authors": "Neelesh Khanna Surender Baswana", 
    "title": "Approximate Shortest Paths Avoiding a Failed Vertex: Optimal Size Data   Structures for Unweighted Graph", 
    "arxiv-id": "1001.0824v2", 
    "author": "Neelesh Khanna Surender Baswana", 
    "publish": "2010-01-06T07:02:30Z", 
    "summary": "Let $G=(V,E)$ be any undirected graph on $V$ vertices and $E$ edges. A path\n$\\textbf{P}$ between any two vertices $u,v\\in V$ is said to be $t$-approximate\nshortest path if its length is at most $t$ times the length of the shortest\npath between $u$ and $v$. We consider the problem of building a compact data\nstructure for a given graph $G$ which is capable of answering the following\nquery for any $u,v,z\\in V$ and $t>1$:\n  Report $t$-approximate shortest path between $u$ and $v$ when vertex $z$\nfails\n  We present data structures for the single source as well all-pairs versions\nof this problem. Our data structures guarantee optimal query time. Most\nimpressive feature of our data structures is that their size {\\em nearly} match\nthe size of their best static counterparts."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0920v2", 
    "other_authors": "Claire Mathieu, Ocan Sankur, Warren Schudy", 
    "title": "Online Correlation Clustering", 
    "arxiv-id": "1001.0920v2", 
    "author": "Warren Schudy", 
    "publish": "2010-01-06T15:54:38Z", 
    "summary": "We study the online clustering problem where data items arrive in an online\nfashion. The algorithm maintains a clustering of data items into similarity\nclasses. Upon arrival of v, the relation between v and previously arrived items\nis revealed, so that for each u we are told whether v is similar to u. The\nalgorithm can create a new cluster for v and merge existing clusters.\n  When the objective is to minimize disagreements between the clustering and\nthe input, we prove that a natural greedy algorithm is O(n)-competitive, and\nthis is optimal.\n  When the objective is to maximize agreements between the clustering and the\ninput, we prove that the greedy algorithm is .5-competitive; that no online\nalgorithm can be better than .834-competitive; we prove that it is possible to\nget better than 1/2, by exhibiting a randomized algorithm with competitive\nratio .5+c for a small positive fixed constant c."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.0961v3", 
    "other_authors": "Charles Sauerbier", 
    "title": "Computing a Frobenius Coin Problem decision problem in O(n^2)", 
    "arxiv-id": "1001.0961v3", 
    "author": "Charles Sauerbier", 
    "publish": "2010-01-06T20:38:17Z", 
    "summary": "Expanding on recent results of another an algorithm is presented that\nprovides solution to the Frobenius Coin Problem in worst case O(n^2) in the\nmagnitude of the largest denomination."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.1565v3", 
    "other_authors": "Philip Bille, Gad M. Landau, Rajeev Raman, Kunihiko Sadakane, Srinivasa Rao Satti, Oren Weimann", 
    "title": "Random Access to Grammar Compressed Strings", 
    "arxiv-id": "1001.1565v3", 
    "author": "Oren Weimann", 
    "publish": "2010-01-11T20:29:41Z", 
    "summary": "Grammar based compression, where one replaces a long string by a small\ncontext-free grammar that generates the string, is a simple and powerful\nparadigm that captures many popular compression schemes. In this paper, we\npresent a novel grammar representation that allows efficient random access to\nany character or substring without decompressing the string.\n  Let $S$ be a string of length $N$ compressed into a context-free grammar\n$\\mathcal{S}$ of size $n$. We present two representations of $\\mathcal{S}$\nachieving $O(\\log N)$ random access time, and either $O(n\\cdot \\alpha_k(n))$\nconstruction time and space on the pointer machine model, or $O(n)$\nconstruction time and space on the RAM. Here, $\\alpha_k(n)$ is the inverse of\nthe $k^{th}$ row of Ackermann's function. Our representations also efficiently\nsupport decompression of any substring in $S$: we can decompress any substring\nof length $m$ in the same complexity as a single random access query and\nadditional $O(m)$ time. Combining these results with fast algorithms for\nuncompressed approximate string matching leads to several efficient algorithms\nfor approximate string matching on grammar-compressed strings without\ndecompression. For instance, we can find all approximate occurrences of a\npattern $P$ with at most $k$ errors in time $O(n(\\min\\{|P|k, k^4 + |P|\\} + \\log\nN) + occ)$, where $occ$ is the number of occurrences of $P$ in $S$. Finally, we\ngeneralize our results to navigation and other operations on grammar-compressed\nordered trees.\n  All of the above bounds significantly improve the currently best known\nresults. To achieve these bounds, we introduce several new techniques and data\nstructures of independent interest, including a predecessor data structure, two\n\"biased\" weighted ancestor data structures, and a compact representation of\nheavy paths in grammars."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_1", 
    "link": "http://arxiv.org/pdf/1001.1819v2", 
    "other_authors": "Sumitra Nuanmeesri, Chanasak Baitiang, Phayung Meesad", 
    "title": "Genealogical Information Search by Using Parent Bidirectional Breadth   Algorithm and Rule Based Relationship", 
    "arxiv-id": "1001.1819v2", 
    "author": "Phayung Meesad", 
    "publish": "2010-01-12T08:48:17Z", 
    "summary": "Genealogical information is the best histories resources for culture study\nand cultural heritage. The genealogical research generally presents family\ninformation and depict tree diagram. This paper presents Parent Bidirectional\nBreadth Algorithm (PBBA) to find consanguine relationship between two persons.\nIn addition, the paper utilizes rules based system in order to identify\nconsanguine relationship. The study reveals that PBBA is fast to solve the\ngenealogical information search problem and the Rule Based Relationship\nprovides more benefits in blood relationship identification."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_21", 
    "link": "http://arxiv.org/pdf/1001.2101v3", 
    "other_authors": "Jouni Sir\u00e9n", 
    "title": "Sampled Longest Common Prefix Array", 
    "arxiv-id": "1001.2101v3", 
    "author": "Jouni Sir\u00e9n", 
    "publish": "2010-01-13T09:18:24Z", 
    "summary": "When augmented with the longest common prefix (LCP) array and some other\nstructures, the suffix array can solve many string processing problems in\noptimal time and space. A compressed representation of the LCP array is also\none of the main building blocks in many compressed suffix tree proposals. In\nthis paper, we describe a new compressed LCP representation: the sampled LCP\narray. We show that when used with a compressed suffix array (CSA), the sampled\nLCP array often offers better time/space trade-offs than the existing\nalternatives. We also show how to construct the compressed representations of\nthe LCP array directly from a CSA."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.2860v2", 
    "other_authors": "Djamal Belazzougui", 
    "title": "Succinct Dictionary Matching With No Slowdown", 
    "arxiv-id": "1001.2860v2", 
    "author": "Djamal Belazzougui", 
    "publish": "2010-01-16T22:10:57Z", 
    "summary": "The problem of dictionary matching is a classical problem in string matching:\ngiven a set S of d strings of total length n characters over an (not\nnecessarily constant) alphabet of size sigma, build a data structure so that we\ncan match in a any text T all occurrences of strings belonging to S. The\nclassical solution for this problem is the Aho-Corasick automaton which finds\nall occ occurrences in a text T in time O(|T| + occ) using a data structure\nthat occupies O(m log m) bits of space where m <= n + 1 is the number of states\nin the automaton. In this paper we show that the Aho-Corasick automaton can be\nrepresented in just m(log sigma + O(1)) + O(d log(n/d)) bits of space while\nstill maintaining the ability to answer to queries in O(|T| + occ) time. To the\nbest of our knowledge, the currently fastest succinct data structure for the\ndictionary matching problem uses space O(n log sigma) while answering queries\nin O(|T|log log n + occ) time. In this paper we also show how the space\noccupancy can be reduced to m(H0 + O(1)) + O(d log(n/d)) where H0 is the\nempirical entropy of the characters appearing in the trie representation of the\nset S, provided that sigma < m^epsilon for any constant 0 < epsilon < 1. The\nquery time remains unchanged."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.2891v1", 
    "other_authors": "Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige, Aravindan Vijayaraghavan", 
    "title": "Detecting High Log-Densities -- an O(n^1/4) Approximation for Densest   k-Subgraph", 
    "arxiv-id": "1001.2891v1", 
    "author": "Aravindan Vijayaraghavan", 
    "publish": "2010-01-17T13:31:39Z", 
    "summary": "In the Densest k-Subgraph problem, given a graph G and a parameter k, one\nneeds to find a subgraph of G induced on k vertices that contains the largest\nnumber of edges. There is a significant gap between the best known upper and\nlower bounds for this problem. It is NP-hard, and does not have a PTAS unless\nNP has subexponential time algorithms. On the other hand, the current best\nknown algorithm of Feige, Kortsarz and Peleg, gives an approximation ratio of\nn^(1/3-epsilon) for some specific epsilon > 0 (estimated at around 1/60).\n  We present an algorithm that for every epsilon > 0 approximates the Densest\nk-Subgraph problem within a ratio of n^(1/4+epsilon) in time n^O(1/epsilon). In\nparticular, our algorithm achieves an approximation ratio of O(n^1/4) in time\nn^O(log n). Our algorithm is inspired by studying an average-case version of\nthe problem where the goal is to distinguish random graphs from graphs with\nplanted dense subgraphs. The approximation ratio we achieve for the general\ncase matches the distinguishing ratio we obtain for this planted problem.\n  At a high level, our algorithms involve cleverly counting appropriately\ndefined trees of constant size in G, and using these counts to identify the\nvertices of the dense subgraph. Our algorithm is based on the following\nprinciple. We say that a graph G(V,E) has log-density alpha if its average\ndegree is Theta(|V|^alpha). The algorithmic core of our result is a family of\nalgorithms that output k-subgraphs of nontrivial density whenever the\nlog-density of the densest k-subgraph is larger than the log-density of the\nhost graph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3044v2", 
    "other_authors": "Marcin Bienkowski, Marek Klonowski, Miroslaw Korzeniowski, Dariusz R. Kowalski", 
    "title": "Dynamic sharing of a multiple access channel", 
    "arxiv-id": "1001.3044v2", 
    "author": "Dariusz R. Kowalski", 
    "publish": "2010-01-18T12:51:04Z", 
    "summary": "In this paper we consider the mutual exclusion problem on a multiple access\nchannel. Mutual exclusion is one of the fundamental problems in distributed\ncomputing. In the classic version of this problem, n processes perform a\nconcurrent program which occasionally triggers some of them to use shared\nresources, such as memory, communication channel, device, etc. The goal is to\ndesign a distributed algorithm to control entries and exits to/from the shared\nresource in such a way that in any time there is at most one process accessing\nit. We consider both the classic and a slightly weaker version of mutual\nexclusion, called ep-mutual-exclusion, where for each period of a process\nstaying in the critical section the probability that there is some other\nprocess in the critical section is at most ep. We show that there are channel\nsettings, where the classic mutual exclusion is not feasible even for\nrandomized algorithms, while ep-mutual-exclusion is. In more relaxed channel\nsettings, we prove an exponential gap between the makespan complexity of the\nclassic mutual exclusion problem and its weaker ep-exclusion version. We also\nshow how to guarantee fairness of mutual exclusion algorithms, i.e., that each\nprocess that wants to enter the critical section will eventually succeed."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3493v1", 
    "other_authors": "A. K. Ojha, K. K. Biswal", 
    "title": "Posynomial Geometric Programming Problems with Multiple Parameters", 
    "arxiv-id": "1001.3493v1", 
    "author": "K. K. Biswal", 
    "publish": "2010-01-20T08:00:12Z", 
    "summary": "Geometric programming problem is a powerful tool for solving some special\ntype non-linear programming problems. It has a wide range of applications in\noptimization and engineering for solving some complex optimization problems.\nMany applications of geometric programming are on engineering design problems\nwhere parameters are estimated using geometric programming. When the parameters\nin the problems are imprecise, the calculated objective value should be\nimprecise as well. In this paper we have developed a method to solve geometric\nprogramming problems where the exponent of the variables in the objective\nfunction, cost coefficients and right hand side are multiple parameters. The\nequivalent mathematical programming problems are formulated to find their\ncorresponding value of the objective function based on the duality theorem. By\napplying a variable separable technique the multi-choice mathematical\nprogramming problem is transformed into multiple one level geometric\nprogramming problem which produces multiple objective values that helps\nengineers to handle more realistic engineering design problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.3713v1", 
    "other_authors": "Yuriy A. Reznik", 
    "title": "On Fast Algorithm for Computing Even-Length DCT", 
    "arxiv-id": "1001.3713v1", 
    "author": "Yuriy A. Reznik", 
    "publish": "2010-01-21T03:13:44Z", 
    "summary": "We study recursive algorithm for computing DCT of lengths $N=q 2^m$ ($m,q \\in\n\\mathbb{N}$, $q$ is odd) due to C.W.Kok. We show that this algorithm has the\nsame multiplicative complexity as theoretically achievable by the prime factor\ndecomposition, when $m \\leqslant 2$. We also show that C.W.Kok's factorization\nallows a simple conversion to a scaled form. We analyze complexity of such a\nscaled factorization, and show that for some lengths it achieves lower\nmultiplicative complexity than one of known prime factor-based scaled\ntransforms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.4420v3", 
    "other_authors": "Raphael Clifford, Markus Jalsenius, Ashley Montanaro, Benjamin Sach", 
    "title": "The Complexity of Flood Filling Games", 
    "arxiv-id": "1001.4420v3", 
    "author": "Benjamin Sach", 
    "publish": "2010-01-25T13:40:57Z", 
    "summary": "We study the complexity of the popular one player combinatorial game known as\nFlood-It. In this game the player is given an n by n board of tiles where each\ntile is allocated one of c colours. The goal is to make the colours of all\ntiles equal via the shortest possible sequence of flooding operations. In the\nstandard version, a flooding operation consists of the player choosing a colour\nk, which then changes the colour of all the tiles in the monochromatic region\nconnected to the top left tile to k. After this operation has been performed,\nneighbouring regions which are already of the chosen colour k will then also\nbecome connected, thereby extending the monochromatic region of the board. We\nshow that finding the minimum number of flooding operations is NP-hard for c>=3\nand that this even holds when the player can perform flooding operations from\nany position on the board. However, we show that this \"free\" variant is in P\nfor c=2. We also prove that for an unbounded number of colours, Flood-It\nremains NP-hard for boards of height at least 3, but is in P for boards of\nheight 2. Next we show how a c-1 approximation and a randomised 2c/3\napproximation algorithm can be derived, and that no polynomial time constant\nfactor, independent of c, approximation algorithm exists unless P=NP. We then\ninvestigate how many moves are required for the \"most demanding\" n by n boards\n(those requiring the most moves) and show that the number grows as fast as\nTheta(n*c^0.5). Finally, we consider boards where the colours of the tiles are\nchosen at random and show that for c>=2, the number of moves required to flood\nthe whole board is Omega(n) with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1001.5076v2", 
    "other_authors": "Jon Feldman, Monika Henzinger, Nitish Korula, Vahab S. Mirrokni, Cliff Stein", 
    "title": "Online Stochastic Packing Applied to Display Ad Allocation", 
    "arxiv-id": "1001.5076v2", 
    "author": "Cliff Stein", 
    "publish": "2010-01-28T00:51:03Z", 
    "summary": "Inspired by online ad allocation, we study online stochastic packing linear\nprograms from theoretical and practical standpoints. We first present a\nnear-optimal online algorithm for a general class of packing linear programs\nwhich model various online resource allocation problems including online\nvariants of routing, ad allocations, generalized assignment, and combinatorial\nauctions. As our main theoretical result, we prove that a simple primal-dual\ntraining-based algorithm achieves a (1 - o(1))-approximation guarantee in the\nrandom order stochastic model. This is a significant improvement over\nlogarithmic or constant-factor approximations for the adversarial variants of\nthe same problems (e.g. factor 1 - 1/e for online ad allocation, and \\log m for\nonline routing). We then focus on the online display ad allocation problem and\nstudy the efficiency and fairness of various training-based and online\nallocation algorithms on data sets collected from real-life display ad\nallocation system. Our experimental evaluation confirms the effectiveness of\ntraining-based primal-dual algorithms on real data sets, and also indicate an\nintrinsic trade-off between fairness and efficiency."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.0046v3", 
    "other_authors": "Olivier Bodini, Yann Ponty", 
    "title": "Multi-dimensional Boltzmann Sampling of Languages", 
    "arxiv-id": "1002.0046v3", 
    "author": "Yann Ponty", 
    "publish": "2010-01-30T06:24:13Z", 
    "summary": "This paper addresses the uniform random generation of words from a\ncontext-free language (over an alphabet of size $k$), while constraining every\nletter to a targeted frequency of occurrence. Our approach consists in a\nmultidimensional extension of Boltzmann samplers \\cite{Duchon2004}. We show\nthat, under mostly \\emph{strong-connectivity} hypotheses, our samplers return a\nword of size in $[(1-\\varepsilon)n, (1+\\varepsilon)n]$ and exact frequency in\n$\\mathcal{O}(n^{1+k/2})$ expected time. Moreover, if we accept tolerance\nintervals of width in $\\Omega(\\sqrt{n})$ for the number of occurrences of each\nletters, our samplers perform an approximate-size generation of words in\nexpected $\\mathcal{O}(n)$ time. We illustrate these techniques on the\ngeneration of Tetris tessellations with uniform statistics in the different\ntypes of tetraminoes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.0874v1", 
    "other_authors": "Roberto Grossi, Andrea Pietracaprina, Nadia Pisanti, Geppino Pucci, Eli Upfal, Fabio Vandin", 
    "title": "MADMX: A Novel Strategy for Maximal Dense Motif Extraction", 
    "arxiv-id": "1002.0874v1", 
    "author": "Fabio Vandin", 
    "publish": "2010-02-04T01:20:12Z", 
    "summary": "We develop, analyze and experiment with a new tool, called MADMX, which\nextracts frequent motifs, possibly including don't care characters, from\nbiological sequences. We introduce density, a simple and flexible measure for\nbounding the number of don't cares in a motif, defined as the ratio of solid\n(i.e., different from don't care) characters to the total length of the motif.\nBy extracting only maximal dense motifs, MADMX reduces the output size and\nimproves performance, while enhancing the quality of the discoveries. The\nefficiency of our approach relies on a newly defined combining operation,\ndubbed fusion, which allows for the construction of maximal dense motifs in a\nbottom-up fashion, while avoiding the generation of nonmaximal ones. We provide\nexperimental evidence of the efficiency and the quality of the motifs returned\nby MADMX"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_9", 
    "link": "http://arxiv.org/pdf/1002.1021v1", 
    "other_authors": "Robert Geisberger", 
    "title": "Heuristic Contraction Hierarchies with Approximation Guarantee", 
    "arxiv-id": "1002.1021v1", 
    "author": "Robert Geisberger", 
    "publish": "2010-02-04T15:27:41Z", 
    "summary": "We present a new heuristic point-to-point routing algorithm based on\ncontraction hierarchies (CH). Given an epsilon >= 0, we can prove that the\nlength of the path computed by our algorithm is at most (1+epsilon) times the\nlength of the optimal (shortest) path. CH is based on node contraction:\nremoving nodes from a network and adding shortcut edges to preserve shortest\npath distances. Our algorithm tries to avoid shortcuts even when a replacement\npath is epsilon times longer."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.1292v2", 
    "other_authors": "Igor Nor, Danny Hermelin, Sylvain Charlat, Jan Engelstadter, Max Reuter, Olivier Duron, Marie-France Sagot", 
    "title": "Mod/Resc Parsimony Inference", 
    "arxiv-id": "1002.1292v2", 
    "author": "Marie-France Sagot", 
    "publish": "2010-02-05T18:15:15Z", 
    "summary": "We address in this paper a new computational biology problem that aims at\nunderstanding a mechanism that could potentially be used to genetically\nmanipulate natural insect populations infected by inherited, intra-cellular\nparasitic bacteria. In this problem, that we denote by \\textsc{Mod/Resc\nParsimony Inference}, we are given a boolean matrix and the goal is to find two\nother boolean matrices with a minimum number of columns such that an\nappropriately defined operation on these matrices gives back the input. We show\nthat this is formally equivalent to the \\textsc{Bipartite Biclique Edge Cover}\nproblem and derive some complexity results for our problem using this\nequivalence. We provide a new, fixed-parameter tractability approach for\nsolving both that slightly improves upon a previously published algorithm for\nthe \\textsc{Bipartite Biclique Edge Cover}. Finally, we present experimental\nresults where we applied some of our techniques to a real-life data set."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.2147v1", 
    "other_authors": "Fabrizio Grandoni, Rico Zenklusen", 
    "title": "Optimization with More than One Budget", 
    "arxiv-id": "1002.2147v1", 
    "author": "Rico Zenklusen", 
    "publish": "2010-02-10T17:48:15Z", 
    "summary": "A natural way to deal with multiple, partially conflicting objectives is\nturning all the objectives but one into budget constraints. Some classical\npolynomial-time optimization problems, such as spanning tree and forest,\nshortest path, (perfect) matching, independent set (basis) in a matroid or in\nthe intersection of two matroids, become NP-hard even with one budget\nconstraint. Still, for most of these problems deterministic and randomized\npolynomial-time approximation schemes are known. In the case of two or more\nbudgets, typically only multi-criteria approximation schemes are available,\nwhich return slightly infeasible solutions. Not much is known however for the\ncase of strict budget constraints: filling this gap is the main goal of this\npaper.\n  We show that shortest path, perfect matching, and spanning tree (and hence\nmatroid basis and matroid intersection basis) are inapproximable already with\ntwo budget constraints. For the remaining problems, whose set of solutions\nforms an independence system, we present deterministic and randomized\npolynomial-time approximation schemes for a constant number k of budget\nconstraints. Our results are based on a variety of techniques:\n  1. We present a simple and powerful mechanism to transform multi-criteria\napproximation schemes into pure approximation schemes.\n  2. We show that points in low dimensional faces of any matroid polytope are\nalmost integral, an interesting result on its own. This gives a deterministic\napproximation scheme for k-budgeted matroid independent set.\n  3. We present a deterministic approximation scheme for 2-budgeted matching.\nThe backbone of this result is a purely topological property of curves in R^2."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4005v1", 
    "other_authors": "Rio G. L. D'Souza, K. Chandra Sekaran, A. Kandasamy", 
    "title": "Improved NSGA-II Based on a Novel Ranking Scheme", 
    "arxiv-id": "1002.4005v1", 
    "author": "A. Kandasamy", 
    "publish": "2010-02-21T19:43:45Z", 
    "summary": "Non-dominated Sorting Genetic Algorithm (NSGA) has established itself as a\nbenchmark algorithm for Multiobjective Optimization. The determination of\npareto-optimal solutions is the key to its success. However the basic algorithm\nsuffers from a high order of complexity, which renders it less useful for\npractical applications. Among the variants of NSGA, several attempts have been\nmade to reduce the complexity. Though successful in reducing the runtime\ncomplexity, there is scope for further improvements, especially considering\nthat the populations involved are frequently of large size. We propose a\nvariant which reduces the run-time complexity using the simple principle of\nspace-time trade-off. The improved algorithm is applied to the problem of\nclassifying types of leukemia based on microarray data. Results of comparative\ntests are presented showing that the improved algorithm performs well on large\npopulations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4034v1", 
    "other_authors": "Shi Li", 
    "title": "On constant factor approximation for earth mover distance over doubling   metrics", 
    "arxiv-id": "1002.4034v1", 
    "author": "Shi Li", 
    "publish": "2010-02-22T01:26:19Z", 
    "summary": "Given a metric space $(X,d_X)$, the earth mover distance between two\ndistributions over $X$ is defined as the minimum cost of a bipartite matching\nbetween the two distributions. The doubling dimension of a metric $(X, d_X)$ is\nthe smallest value $\\alpha$ such that every ball in $X$ can be covered by\n$2^\\alpha$ ball of half the radius. We study efficient algorithms for\napproximating earth mover distance over metrics with bounded doubling\ndimension.\n  Given a metric $(X, d_X)$, with $|X| = n$, we can use $\\tilde O(n^2)$\npreprocessing time to create a data structure of size $\\tilde O(n^{1 + \\e})$,\nsuch that subsequently queried EMDs can be $O(\\alpha_X/\\e)$-approximated in\n$\\tilde O(n)$ time.\n  We also show a weaker form of sketching scheme, which we call \"encoding\nscheme\". Given $(X, d_X)$, by using $\\tilde O(n^2)$ preprocessing time, every\nsubsequent distribution $\\mu$ over $X$ can be encoded into $F(\\mu)$ in $\\tilde\nO(n^{1 + \\e})$ time. Given $F(\\mu)$ and $F(\\nu)$, the EMD between $\\mu$ and\n$\\nu$ can be $O(\\alpha_X/\\e)$-approximated in $\\tilde O(n^\\e)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.4330v1", 
    "other_authors": "Jonathan Dees, Robert Geisberger, Peter Sanders, Roland Bader", 
    "title": "Defining and Computing Alternative Routes in Road Networks", 
    "arxiv-id": "1002.4330v1", 
    "author": "Roland Bader", 
    "publish": "2010-02-23T13:43:34Z", 
    "summary": "Every human likes choices. But today's fast route planning algorithms usually\ncompute just a single route between source and target. There are beginnings to\ncompute alternative routes, but this topic has not been studied thoroughly.\nOften, the aspect of meaningful alternative routes is neglected from a human\npoint of view. We fill in this gap by suggesting mathematical definitions for\nsuch routes. As a second contribution we propose heuristics to compute them, as\nthis is NP-hard in general."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13509-5_19", 
    "link": "http://arxiv.org/pdf/1002.5034v2", 
    "other_authors": "Eric Bach, Shuchi Chawla, Seeun Umboh", 
    "title": "Threshold rules for online sample selection", 
    "arxiv-id": "1002.5034v2", 
    "author": "Seeun Umboh", 
    "publish": "2010-02-26T17:52:28Z", 
    "summary": "We consider the following sample selection problem. We observe in an online\nfashion a sequence of samples, each endowed by a quality. Our goal is to either\nselect or reject each sample, so as to maximize the aggregate quality of the\nsubsample selected so far. There is a natural trade-off here between the rate\nof selection and the aggregate quality of the subsample. We show that for a\nnumber of such problems extremely simple and oblivious \"threshold rules\" for\nselection achieve optimal tradeoffs between rate of selection and aggregate\nquality in a probabilistic sense. In some cases we show that the same threshold\nrule is optimal for a large class of quality distributions and is thus\noblivious in a strong sense."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.0139v1", 
    "other_authors": "Prosenjit Bose, Karim Dou\u00efeb, Vida Dujmovic, Rolf Fagerberg", 
    "title": "An O(loglog n)-Competitive Binary Search Tree with Optimal Worst-Case   Access Times", 
    "arxiv-id": "1003.0139v1", 
    "author": "Rolf Fagerberg", 
    "publish": "2010-02-27T23:39:35Z", 
    "summary": "We present the zipper tree, an $O(\\log \\log n)$-competitive online binary\nsearch tree that performs each access in $O(\\log n)$ worst-case time. This\nshows that for binary search trees, optimal worst-case access time and\nnear-optimal amortized access time can be guaranteed simultaneously."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.0722v2", 
    "other_authors": "Anupam Gupta, Ravishankar Krishnaswamy, Viswanath Nagarajan, R. Ravi", 
    "title": "Approximation Algorithms for Optimal Decision Trees and Adaptive TSP   Problems", 
    "arxiv-id": "1003.0722v2", 
    "author": "R. Ravi", 
    "publish": "2010-03-03T03:58:29Z", 
    "summary": "We consider the problem of constructing optimal decision trees: given a\ncollection of tests which can disambiguate between a set of $m$ possible\ndiseases, each test having a cost, and the a-priori likelihood of the patient\nhaving any particular disease, what is a good adaptive strategy to perform\nthese tests to minimize the expected cost to identify the disease? We settle\nthe approximability of this problem by giving a tight $O(\\log m)$-approximation\nalgorithm. We also consider a more substantial generalization, the Adaptive TSP\nproblem. Given an underlying metric space, a random subset $S$ of cities is\ndrawn from a known distribution, but $S$ is initially unknown to us--we get\ninformation about whether any city is in $S$ only when we visit the city in\nquestion. What is a good adaptive way of visiting all the cities in the random\nsubset $S$ while minimizing the expected distance traveled? For this problem,\nwe give the first poly-logarithmic approximation, and show that this algorithm\nis best possible unless we can improve the approximation guarantees for the\nwell-known group Steiner tree problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13731-0_5", 
    "link": "http://arxiv.org/pdf/1003.1260v1", 
    "other_authors": "D\u00e1niel Marx, Ildik\u00f3 Schlotter", 
    "title": "Cleaning Interval Graphs", 
    "arxiv-id": "1003.1260v1", 
    "author": "Ildik\u00f3 Schlotter", 
    "publish": "2010-03-05T13:26:55Z", 
    "summary": "We investigate a special case of the Induced Subgraph Isomorphism problem,\nwhere both input graphs are interval graphs. We show the NP-hardness of this\nproblem, and we prove fixed-parameter tractability of the problem with\nnon-standard parameterization, where the parameter is the difference\n|V(G)|-|V(H)|, with G and H being the larger and the smaller input graph,\nrespectively. Intuitively, we can interpret this problem as \"cleaning\" the\ngraph G, regarded as a pattern containing extra vertices indicating errors, in\norder to obtain the graph H representing the original pattern. We also prove\nW[1]-hardness for the standard parameterization where the parameter is |V(H)|."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_27", 
    "link": "http://arxiv.org/pdf/1003.1507v1", 
    "other_authors": "Deeparnab Chakrabarty, Elyot Grant, Jochen Koenemann", 
    "title": "On Column-restricted and Priority Covering Integer Programs", 
    "arxiv-id": "1003.1507v1", 
    "author": "Jochen Koenemann", 
    "publish": "2010-03-07T18:08:12Z", 
    "summary": "In a column-restricted covering integer program (CCIP), all the non-zero\nentries of any column of the constraint matrix are equal. Such programs capture\ncapacitated versions of covering problems. In this paper, we study the\napproximability of CCIPs, in particular, their relation to the integrality gaps\nof the underlying 0,1-CIP.\n  If the underlying 0,1-CIP has an integrality gap O(gamma), and assuming that\nthe integrality gap of the priority version of the 0,1-CIP is O(omega), we give\na factor O(gamma + omega) approximation algorithm for the CCIP. Priority\nversions of 0,1-CIPs (PCIPs) naturally capture quality of service type\nconstraints in a covering problem.\n  We investigate priority versions of the line (PLC) and the (rooted) tree\ncover (PTC) problems. Apart from being natural objects to study, these problems\nfall in a class of fundamental geometric covering problems. We bound the\nintegrality of certain classes of this PCIP by a constant. Algorithmically, we\ngive a polytime exact algorithm for PLC, show that the PTC problem is APX-hard,\nand give a factor 2-approximation algorithm for it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_27", 
    "link": "http://arxiv.org/pdf/1003.2958v3", 
    "other_authors": "Ioannis Koutis, Gary L. Miller, Richard Peng", 
    "title": "Approaching optimality for solving SDD systems", 
    "arxiv-id": "1003.2958v3", 
    "author": "Richard Peng", 
    "publish": "2010-03-15T16:37:51Z", 
    "summary": "We present an algorithm that on input of an $n$-vertex $m$-edge weighted\ngraph $G$ and a value $k$, produces an {\\em incremental sparsifier} $\\hat{G}$\nwith $n-1 + m/k$ edges, such that the condition number of $G$ with $\\hat{G}$ is\nbounded above by $\\tilde{O}(k\\log^2 n)$, with probability $1-p$. The algorithm\nruns in time\n  $$\\tilde{O}((m \\log{n} + n\\log^2{n})\\log(1/p)).$$\n  As a result, we obtain an algorithm that on input of an $n\\times n$ symmetric\ndiagonally dominant matrix $A$ with $m$ non-zero entries and a vector $b$,\ncomputes a vector ${x}$ satisfying $||{x}-A^{+}b||_A<\\epsilon ||A^{+}b||_A $,\nin expected time\n  $$\\tilde{O}(m\\log^2{n}\\log(1/\\epsilon)).$$\n  The solver is based on repeated applications of the incremental sparsifier\nthat produces a chain of graphs which is then used as input to a recursive\npreconditioned Chebyshev iteration."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_9", 
    "link": "http://arxiv.org/pdf/1003.2977v2", 
    "other_authors": "Nikhil Bansal, Rohit Khandekar, Jochen Konemann, Viswanath Nagarajan, Britta Peis", 
    "title": "On Generalizations of Network Design Problems with Degree Bounds", 
    "arxiv-id": "1003.2977v2", 
    "author": "Britta Peis", 
    "publish": "2010-03-15T17:49:56Z", 
    "summary": "Iterative rounding and relaxation have arguably become the method of choice\nin dealing with unconstrained and constrained network design problems. In this\npaper we extend the scope of the iterative relaxation method in two directions:\n(1) by handling more complex degree constraints in the minimum spanning tree\nproblem (namely, laminar crossing spanning tree), and (2) by incorporating\n`degree bounds' in other combinatorial optimization problems such as matroid\nintersection and lattice polyhedra. We give new or improved approximation\nalgorithms, hardness results, and integrality gaps for these problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-13036-6_9", 
    "link": "http://arxiv.org/pdf/1003.3418v1", 
    "other_authors": "John Fearnley", 
    "title": "Exponential Lower Bounds For Policy Iteration", 
    "arxiv-id": "1003.3418v1", 
    "author": "John Fearnley", 
    "publish": "2010-03-17T17:48:58Z", 
    "summary": "We study policy iteration for infinite-horizon Markov decision processes. It\nhas recently been shown policy iteration style algorithms have exponential\nlower bounds in a two player game setting. We extend these lower bounds to\nMarkov decision processes with the total reward and average-reward optimality\ncriteria."
},{
    "category": "cs.DS", 
    "doi": "10.1162/EVCO_a_00026", 
    "link": "http://arxiv.org/pdf/1003.4314v1", 
    "other_authors": "Daniel Karapetyan, Gregory Gutin", 
    "title": "A New Approach to Population Sizing for Memetic Algorithms: A Case Study   for the Multidimensional Assignment Problem", 
    "arxiv-id": "1003.4314v1", 
    "author": "Gregory Gutin", 
    "publish": "2010-03-23T00:27:13Z", 
    "summary": "Memetic Algorithms are known to be a powerful technique in solving hard\noptimization problems. To design a memetic algorithm one needs to make a host\nof decisions; selecting a population size is one of the most important among\nthem. Most algorithms in the literature fix the population size to a certain\nconstant value. This reduces the algorithm's quality since the optimal\npopulation size varies for different instances, local search procedures and\nrunning times. In this paper we propose an adjustable population size. It is\ncalculated as a function of the running time of the whole algorithm and the\naverage running time of the local search for the given instance. Note that in\nmany applications the running time of a heuristic should be limited and\ntherefore we use this limit as a parameter of the algorithm. The average\nrunning time of the local search procedure is obtained during the algorithm's\nrun. Some coefficients which are independent with respect to the instance or\nthe local search are to be tuned before the algorithm run; we provide a\nprocedure to find these coefficients. The proposed approach was used to develop\na memetic algorithm for the Multidimensional Assignment Problem (MAP or s-AP in\nthe case of s dimensions) which is an extension of the well-known assignment\nproblem. MAP is NP-hard and has a host of applications. We show that using\nadjustable population size makes the algorithm flexible to perform well for\ninstances of very different sizes and types and for different running times and\nlocal searches. This allows us to select the most efficient local search for\nevery instance type. The results of computational experiments for several\ninstance families and sizes prove that the proposed algorithm performs\nefficiently for a wide range of the running times and clearly outperforms the\nstate-of-the art 3-AP memetic algorithm being given the same time."
},{
    "category": "cs.DS", 
    "doi": "10.1162/EVCO_a_00026", 
    "link": "http://arxiv.org/pdf/1003.4366v1", 
    "other_authors": "Marco Nissen", 
    "title": "Graph Iterators: Decoupling Graph Structures from Algorithms", 
    "arxiv-id": "1003.4366v1", 
    "author": "Marco Nissen", 
    "publish": "2010-03-23T10:23:07Z", 
    "summary": "I will present a way to implement graph algorithms which is different from\ntraditional methods. This work was motivated by the belief that some ideas from\nsoftware engineering should be applied to graph algorithms. Re-usability of\nsoftware is an important and difficult problem in general, and this is\nparticularly true for graph algorithms. The scientific literature demonstrates\nplenty of applications of graph algorithms as subroutines for other algorithms.\nMoreover, many practical problems from various domains may be modeled as graph\nproblems and hence solved by means of graph algorithms. Chapter 2 introduces\nsome data structures that will be used in 5 basic graph algorithms in chapter\n3. Chapter 4 discusses an implementation of a maximum cardinality matching\nalgorithm for general graphs. Chapter 5 explains some techniques in C++, which\nare useful to implement the data structures and algorithms in an efficient way.\nFinally chapter 6 contains some concluding remarks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5330v2", 
    "other_authors": "Daniel Karapetyan, Gregory Gutin", 
    "title": "Lin-Kernighan Heuristic Adaptations for the Generalized Traveling   Salesman Problem", 
    "arxiv-id": "1003.5330v2", 
    "author": "Gregory Gutin", 
    "publish": "2010-03-27T22:46:05Z", 
    "summary": "The Lin-Kernighan heuristic is known to be one of the most successful\nheuristics for the Traveling Salesman Problem (TSP). It has also proven its\nefficiency in application to some other problems. In this paper we discuss\npossible adaptations of TSP heuristics for the Generalized Traveling Salesman\nProblem (GTSP) and focus on the case of the Lin-Kernighan algorithm. At first,\nwe provide an easy-to-understand description of the original Lin-Kernighan\nheuristic. Then we propose several adaptations, both trivial and complicated.\nFinally, we conduct a fair competition between all the variations of the\nLin-Kernighan adaptation and some other GTSP heuristics. It appears that our\nadaptation of the Lin-Kernighan algorithm for the GTSP reproduces the success\nof the original heuristic. Different variations of our adaptation outperform\nall other heuristics in a wide range of trade-offs between solution quality and\nrunning time, making Lin-Kernighan the state-of-the-art GTSP local search."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5474v2", 
    "other_authors": "Ilia Zvedeniouk, Sanjay Chawla", 
    "title": "Angle Tree: Nearest Neighbor Search in High Dimensions with Low   Intrinsic Dimensionality", 
    "arxiv-id": "1003.5474v2", 
    "author": "Sanjay Chawla", 
    "publish": "2010-03-29T09:24:31Z", 
    "summary": "We propose an extension of tree-based space-partitioning indexing structures\nfor data with low intrinsic dimensionality embedded in a high dimensional\nspace. We call this extension an Angle Tree. Our extension can be applied to\nboth classical kd-trees as well as the more recent rp-trees. The key idea of\nour approach is to store the angle (the \"dihedral angle\") between the data\nregion (which is a low dimensional manifold) and the random hyperplane that\nsplits the region (the \"splitter\"). We show that the dihedral angle can be used\nto obtain a tight lower bound on the distance between the query point and any\npoint on the opposite side of the splitter. This in turn can be used to\nefficiently prune the search space. We introduce a novel randomized strategy to\nefficiently calculate the dihedral angle with a high degree of accuracy.\nExperiments and analysis on real and synthetic data sets shows that the Angle\nTree is the most efficient known indexing structure for nearest neighbor\nqueries in terms of preprocessing and space usage while achieving high accuracy\nand fast search time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1003.5907v2", 
    "other_authors": "Aleksander Madry", 
    "title": "Faster Approximation Schemes for Fractional Multicommodity Flow Problems   via Dynamic Graph Algorithms", 
    "arxiv-id": "1003.5907v2", 
    "author": "Aleksander Madry", 
    "publish": "2010-03-30T19:53:12Z", 
    "summary": "We combine the work of Garg and Konemann, and Fleischer with ideas from\ndynamic graph algorithms to obtain faster (1-eps)-approximation schemes for\nvarious versions of the multicommodity flow problem. In particular, if eps is\nmoderately small and the size of every number used in the input instance is\npolynomially bounded, the running times of our algorithms match - up to\npoly-logarithmic factors and some provably optimal terms - the Omega(mn)\nflow-decomposition barrier for single-commodity flow."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0351v1", 
    "other_authors": "Srivathsan Srinivasagopalan, Costas Busch, S. S. Iyengar", 
    "title": "An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems", 
    "arxiv-id": "1004.0351v1", 
    "author": "S. S. Iyengar", 
    "publish": "2010-04-02T15:51:33Z", 
    "summary": "We consider the problem of constructing a single spanning tree for the\nsingle-source buy-at-bulk network design problem for doubling-dimension graphs.\nWe compute a spanning tree to route a set of demands (or data) along a graph to\nor from a designated root node. The demands could be aggregated at (or\nsymmetrically distributed to) intermediate nodes where the fusion-cost is\nspecified by a non-negative concave function $f$. We describe a novel approach\nfor developing an oblivious spanning tree in the sense that it is independent\nof the number of data sources (or demands) and cost function at intermediate\nnodes. To our knowledge, this is the first paper to propose a single spanning\ntree solution to this problem (as opposed to multiple overlay trees). There has\nbeen no prior work where the tree is oblivious to both the fusion cost function\nand the set of sources (demands). We present a deterministic, polynomial-time\nalgorithm for constructing a spanning tree in low doubling graphs that\nguarantees $\\log^{3}D\\cdot\\log n$-approximation over the optimal cost, where\n$D$ is the diameter of the graph and $n$ the total number of nodes. With\nconstant fusion-cost function our spanning tree gives a $O(\\log^3\nD)$-approximation for every Steiner tree to the root."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0403v1", 
    "other_authors": "Alessandro Colantonio, Roberto Di Pietro", 
    "title": "CONCISE: Compressed 'n' Composable Integer Set", 
    "arxiv-id": "1004.0403v1", 
    "author": "Roberto Di Pietro", 
    "publish": "2010-04-03T00:07:00Z", 
    "summary": "Bit arrays, or bitmaps, are used to significantly speed up set operations in\nseveral areas, such as data warehousing, information retrieval, and data\nmining, to cite a few. However, bitmaps usually use a large storage space, thus\nrequiring compression. Nevertheless, there is a space-time tradeoff among\ncompression schemes. The Word Aligned Hybrid (WAH) bitmap compression trades\nsome space to allow for bitwise operations without first decompressing bitmaps.\nWAH has been recognized as the most efficient scheme in terms of computation\ntime. In this paper we present CONCISE (Compressed 'n' Composable Integer Set),\na new scheme that enjoys significatively better performances than those of WAH.\nIn particular, when compared to WAH, our algorithm is able to reduce the\nrequired memory up to 50%, by having similar or better performance in terms of\ncomputation time. Further, we show that CONCISE can be efficiently used to\nmanipulate bitmaps representing sets of integral numbers in lieu of well-known\ndata structures such as arrays, lists, hashtables, and self-balancing binary\nsearch trees. Extensive experiments over synthetic data show the effectiveness\nof our approach."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0424v2", 
    "other_authors": "Rapha\u00ebl Clifford, Zvi Gotthilf, Moshe Lewenstein, Alexandru Popa", 
    "title": "Restricted Common Superstring and Restricted Common Supersequence", 
    "arxiv-id": "1004.0424v2", 
    "author": "Alexandru Popa", 
    "publish": "2010-04-03T07:14:53Z", 
    "summary": "The {\\em shortest common superstring} and the {\\em shortest common\nsupersequence} are two well studied problems having a wide range of\napplications. In this paper we consider both problems with resource\nconstraints, denoted as the Restricted Common Superstring (shortly\n\\textit{RCSstr}) problem and the Restricted Common Supersequence (shortly\n\\textit{RCSseq}). In the \\textit{RCSstr} (\\textit{RCSseq}) problem we are given\na set $S$ of $n$ strings, $s_1$, $s_2$, $\\ldots$, $s_n$, and a multiset $t =\n\\{t_1, t_2, \\dots, t_m\\}$, and the goal is to find a permutation $\\pi : \\{1,\n\\dots, m\\} \\to \\{1, \\dots, m\\}$ to maximize the number of strings in $S$ that\nare substrings (subsequences) of $\\pi(t) = t_{\\pi(1)}t_{\\pi(2)}...t_{\\pi(m)}$\n(we call this ordering of the multiset, $\\pi(t)$, a permutation of $t$). We\nfirst show that in its most general setting the \\textit{RCSstr} problem is {\\em\nNP-complete} and hard to approximate within a factor of $n^{1-\\epsilon}$, for\nany $\\epsilon > 0$, unless P = NP. Afterwards, we present two separate\nreductions to show that the \\textit{RCSstr} problem remains NP-Hard even in the\ncase where the elements of $t$ are drawn from a binary alphabet or for the case\nwhere all input strings are of length two. We then present some approximation\nresults for several variants of the \\textit{RCSstr} problem. In the second part\nof this paper, we turn to the \\textit{RCSseq} problem, where we present some\nhardness results, tight lower bounds and approximation algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.0995v1", 
    "other_authors": "David Doty, Matthew J. Patitz, Dustin Reishus, Robert T. Schweller, Scott M. Summers", 
    "title": "Strong Fault-Tolerance for Self-Assembly with Fuzzy Temperature", 
    "arxiv-id": "1004.0995v1", 
    "author": "Scott M. Summers", 
    "publish": "2010-04-07T02:21:21Z", 
    "summary": "We consider the problem of fault-tolerance in nanoscale algorithmic\nself-assembly. We employ a variant of Winfree's abstract Tile Assembly Model\n(aTAM), the two-handed aTAM, in which square \"tiles\" -- a model of molecules\nconstructed from DNA for the purpose of engineering self-assembled\nnanostructures -- aggregate according to specific binding sites of varying\nstrengths, and in which large aggregations of tiles may attach to each other,\nin contrast to the seeded aTAM, in which tiles aggregate one at a time to a\nsingle specially-designated \"seed\" assembly. We focus on a major cause of\nerrors in tile-based self-assembly: that of unintended growth due to \"weak\"\nstrength-1 bonds, which if allowed to persist, may be stabilized by subsequent\nattachment of neighboring tiles in the sense that at least energy 2 is now\nrequired to break apart the resulting assembly; i.e., the errant assembly is\nstable at temperature 2. We study a common self-assembly benchmark problem,\nthat of assembling an n x n square using O(log n) unique tile types, under the\ntwo-handed model of self-assembly. Our main result achieves a much stronger\nnotion of fault-tolerance than those achieved previously. Arbitrary strength-1\ngrowth is allowed (i.e., the temperature is \"fuzzy\" and may drift from 2 to 1\nfor arbitrarily long); however, any assembly that grows sufficiently to become\nstable at temperature 2 is guaranteed to assemble at temperature 2 into the\ncorrect final assembly of an n x n square. In other words, errors due to\ninsufficient attachment, which is the cause of errors studied in earlier papers\non fault-tolerance, are prevented absolutely in our main construction, rather\nthan only with high probability and for sufficiently small structures, as in\nprevious fault-tolerance studies."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.1194v1", 
    "other_authors": "Danny Hermelin, Gad M. Landau, Shir Landau, Oren Weimann", 
    "title": "Unified Compression-Based Acceleration of Edit-Distance Computation", 
    "arxiv-id": "1004.1194v1", 
    "author": "Oren Weimann", 
    "publish": "2010-04-07T21:24:27Z", 
    "summary": "The edit distance problem is a classical fundamental problem in computer\nscience in general, and in combinatorial pattern matching in particular. The\nstandard dynamic programming solution for this problem computes the\nedit-distance between a pair of strings of total length O(N) in O(N^2) time. To\nthis date, this quadratic upper-bound has never been substantially improved for\ngeneral strings. However, there are known techniques for breaking this bound in\ncase the strings are known to compress well under a particular compression\nscheme. The basic idea is to first compress the strings, and then to compute\nthe edit distance between the compressed strings. As it turns out, practically\nall known o(N^2) edit-distance algorithms work, in some sense, under the same\nparadigm described above. It is therefore natural to ask whether there is a\nsingle edit-distance algorithm that works for strings which are compressed\nunder any compression scheme. A rephrasing of this question is to ask whether a\nsingle algorithm can exploit the compressibility properties of strings under\nany compression method, even if each string is compressed using a different\ncompression. In this paper we set out to answer this question by using straight\nline programs. These provide a generic platform for representing many popular\ncompression schemes including the LZ-family, Run-Length Encoding, Byte-Pair\nEncoding, and dictionary methods. For two strings of total length N having\nstraight-line program representations of total size n, we present an algorithm\nrunning in O(nN log(N/n)) time for computing the edit-distance of these two\nstrings under any rational scoring function, and an O(n^{2/3}N^{4/3}) time\nalgorithm for arbitrary scoring functions. Our new result, while providing a\nsigni cant speed up for highly compressible strings, does not surpass the\nquadratic time bound even in the worst case scenario."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ejor.2010.08.011", 
    "link": "http://arxiv.org/pdf/1004.1208v1", 
    "other_authors": "Pushkar Tripathi", 
    "title": "A Deterministic Algorithm for the Vertex Connectivity Survivable Network   Design Problem", 
    "arxiv-id": "1004.1208v1", 
    "author": "Pushkar Tripathi", 
    "publish": "2010-04-07T23:29:43Z", 
    "summary": "In the vertex connectivity survivable network design problem we are given an\nundirected graph G = (V,E) and connectivity requirement r(u,v) for each pair of\nvertices u,v. We are also given a cost function on the set of edges. Our goal\nis to find the minimum cost subset of edges such that for every pair (u,v) of\nvertices we have r(u,v) vertex disjoint paths in the graph induced by the\nchosen edges. Recently, Chuzhoy and Khanna presented a randomized algorithm\nthat achieves a factor of O(k^3 log n) for this problem where k is the maximum\nconnectivity requirement. In this paper we derandomize their algorithm to get a\ndeterministic O(k^3 log n) factor algorithm. Another problem of interest is the\nsingle source version of the problem, where there is a special vertex s and all\nnon-zero connectivity requirements must involve s. We also give a deterministic\nO(k^2 log n) algorithm for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1672v2", 
    "other_authors": "Yixin Cao, Jianer Chen, Yang Liu", 
    "title": "On Feedback Vertex Set: New Measure and New Structures", 
    "arxiv-id": "1004.1672v2", 
    "author": "Yang Liu", 
    "publish": "2010-04-10T03:05:53Z", 
    "summary": "We present a new parameterized algorithm for the {feedback vertex set}\nproblem ({\\sc fvs}) on undirected graphs. We approach the problem by\nconsidering a variation of it, the {disjoint feedback vertex set} problem ({\\sc\ndisjoint-fvs}), which finds a feedback vertex set of size $k$ that has no\noverlap with a given feedback vertex set $F$ of the graph $G$. We develop an\nimproved kernelization algorithm for {\\sc disjoint-fvs} and show that {\\sc\ndisjoint-fvs} can be solved in polynomial time when all vertices in $G\n\\setminus F$ have degrees upper bounded by three. We then propose a new\nbranch-and-search process on {\\sc disjoint-fvs}, and introduce a new\nbranch-and-search measure. The process effectively reduces a given graph to a\ngraph on which {\\sc disjoint-fvs} becomes polynomial-time solvable, and the new\nmeasure more accurately evaluates the efficiency of the process. These\nalgorithmic and combinatorial studies enable us to develop an\n$O^*(3.83^k)$-time parameterized algorithm for the general {\\sc fvs} problem,\nimproving all previous algorithms for the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1808v6", 
    "other_authors": "Michael I. Trofimov", 
    "title": "Polynomial Time Algorithm for Graph Isomorphism Testing", 
    "arxiv-id": "1004.1808v6", 
    "author": "Michael I. Trofimov", 
    "publish": "2010-04-11T13:41:39Z", 
    "summary": "This article deals with new polynomial time algorithm for graph isomorphism\ntesting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9904-6", 
    "link": "http://arxiv.org/pdf/1004.1823v1", 
    "other_authors": "Amit Kumar, Ravindran Kannan", 
    "title": "Clustering with Spectral Norm and the k-means Algorithm", 
    "arxiv-id": "1004.1823v1", 
    "author": "Ravindran Kannan", 
    "publish": "2010-04-11T17:36:33Z", 
    "summary": "There has been much progress on efficient algorithms for clustering data\npoints generated by a mixture of $k$ probability distributions under the\nassumption that the means of the distributions are well-separated, i.e., the\ndistance between the means of any two distributions is at least $\\Omega(k)$\nstandard deviations. These results generally make heavy use of the generative\nmodel and particular properties of the distributions. In this paper, we show\nthat a simple clustering algorithm works without assuming any generative\n(probabilistic) model. Our only assumption is what we call a \"proximity\ncondition\": the projection of any data point onto the line joining its cluster\ncenter to any other cluster center is $\\Omega(k)$ standard deviations closer to\nits own center than the other center. Here the notion of standard deviations is\nbased on the spectral norm of the matrix whose rows represent the difference\nbetween a point and the mean of the cluster to which it belongs. We show that\nin the generative models studied, our proximity condition is satisfied and so\nwe are able to derive most known results for generative models as corollaries\nof our main result. We also prove some new results for generative models -\ne.g., we can cluster all but a small fraction of points only assuming a bound\non the variance. Our algorithm relies on the well known $k$-means algorithm,\nand along the way, we prove a result of independent interest -- that the\n$k$-means algorithm converges to the \"true centers\" even in the presence of\nspurious points provided the initial (estimated) centers are close enough to\nthe corresponding actual centers and all but a small fraction of the points\nsatisfy the proximity condition. Finally, we present a new technique for\nboosting the ratio of inter-center separation to standard deviation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9505-6", 
    "link": "http://arxiv.org/pdf/1004.2033v1", 
    "other_authors": "Vincenzo Bonifaci, Alberto Marchetti-Spaccamela", 
    "title": "Feasibility Analysis of Sporadic Real-Time Multiprocessor Task Systems", 
    "arxiv-id": "1004.2033v1", 
    "author": "Alberto Marchetti-Spaccamela", 
    "publish": "2010-04-12T19:43:20Z", 
    "summary": "We give the first algorithm for testing the feasibility of a system of\nsporadic real-time tasks on a set of identical processors, solving one major\nopen problem in the area of multiprocessor real-time scheduling [S.K. Baruah\nand K. Pruhs, Journal of Scheduling, 2009]. We also investigate the related\nnotion of schedulability and a notion that we call online feasibility. Finally,\nwe show that discrete-time schedules are as powerful as continuous-time\nschedules, which answers another open question in the above mentioned survey."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.2393v3", 
    "other_authors": "John Augustine, Nick Gravin", 
    "title": "On the Continuous CNN Problem", 
    "arxiv-id": "1004.2393v3", 
    "author": "Nick Gravin", 
    "publish": "2010-04-14T13:34:03Z", 
    "summary": "In the (discrete) CNN problem, online requests appear as points in\n$\\mathbb{R}^2$. Each request must be served before the next one is revealed. We\nhave a server that can serve a request simply by aligning either its $x$ or $y$\ncoordinate with the request. The goal of the online algorithm is to minimize\nthe total $L_1$ distance traveled by the server to serve all the requests. The\nbest known competitive ratio for the discrete version is 879 (due to Sitters\nand Stougie).\n  We study the continuous version, in which, the request can move continuously\nin $\\mathbb{R}^2$ and the server must continuously serve the request. A simple\nadversarial argument shows that the lower bound on the competitive ratio of any\nonline algorithm for the continuous CNN problem is 3. Our main contribution is\nan online algorithm with competitive ratio $3+2 \\sqrt{3} \\approx 6.464$. Our\nanalysis is tight. The continuous version generalizes the discrete orthogonal\nCNN problem, in which every request must be $x$ or $y$ aligned with the\nprevious request. Therefore, Our result improves upon the previous best\ncompetitive ratio of 9 (due to Iwama and Yonezawa)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.2968v2", 
    "other_authors": "Jian Li, Ke Yi, Qin Zhang", 
    "title": "Clustering with diversity", 
    "arxiv-id": "1004.2968v2", 
    "author": "Qin Zhang", 
    "publish": "2010-04-17T16:30:08Z", 
    "summary": "We consider the {\\em clustering with diversity} problem: given a set of\ncolored points in a metric space, partition them into clusters such that each\ncluster has at least $\\ell$ points, all of which have distinct colors.\n  We give a 2-approximation to this problem for any $\\ell$ when the objective\nis to minimize the maximum radius of any cluster. We show that the\napproximation ratio is optimal unless $\\mathbf{P=NP}$, by providing a matching\nlower bound. Several extensions to our algorithm have also been developed for\nhandling outliers. This problem is mainly motivated by applications in\nprivacy-preserving data publication."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3205v2", 
    "other_authors": "Aaron Roth", 
    "title": "Differential Privacy and the Fat-Shattering Dimension of Linear Queries", 
    "arxiv-id": "1004.3205v2", 
    "author": "Aaron Roth", 
    "publish": "2010-04-19T14:19:56Z", 
    "summary": "In this paper, we consider the task of answering linear queries under the\nconstraint of differential privacy. This is a general and well-studied class of\nqueries that captures other commonly studied classes, including predicate\nqueries and histogram queries. We show that the accuracy to which a set of\nlinear queries can be answered is closely related to its fat-shattering\ndimension, a property that characterizes the learnability of real-valued\nfunctions in the agnostic-learning setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3363v5", 
    "other_authors": "Jittat Fakcharoenphol, Bundit Laekhanukit, Danupon Nanongkai", 
    "title": "Faster Algorithms for Semi-Matching Problems", 
    "arxiv-id": "1004.3363v5", 
    "author": "Danupon Nanongkai", 
    "publish": "2010-04-20T07:06:38Z", 
    "summary": "We consider the problem of finding \\textit{semi-matching} in bipartite graphs\nwhich is also extensively studied under various names in the scheduling\nliterature. We give faster algorithms for both weighted and unweighted case.\n  For the weighted case, we give an $O(nm\\log n)$-time algorithm, where $n$ is\nthe number of vertices and $m$ is the number of edges, by exploiting the\ngeometric structure of the problem. This improves the classical $O(n^3)$\nalgorithms by Horn [Operations Research 1973] and Bruno, Coffman and Sethi\n[Communications of the ACM 1974].\n  For the unweighted case, the bound could be improved even further. We give a\nsimple divide-and-conquer algorithm which runs in $O(\\sqrt{n}m\\log n)$ time,\nimproving two previous $O(nm)$-time algorithms by Abraham [MSc thesis,\nUniversity of Glasgow 2003] and Harvey, Ladner, Lov\\'asz and Tamir [WADS 2003\nand Journal of Algorithms 2006]. We also extend this algorithm to solve the\n\\textit{Balance Edge Cover} problem in $O(\\sqrt{n}m\\log n)$ time, improving the\nprevious $O(nm)$-time algorithm by Harada, Ono, Sadakane and Yamashita [ISAAC\n2008]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.3702v30", 
    "other_authors": "Lizhi Du", 
    "title": "A Polynomial time Algorithm for Hamilton Cycle and Its detailed proof", 
    "arxiv-id": "1004.3702v30", 
    "author": "Lizhi Du", 
    "publish": "2010-04-12T04:39:27Z", 
    "summary": "Popular algorithms to find a Hamilton circle in an undirected graph are\ngenerally based on the Rotation-Extension method developed by Posa. However,\ndue to the deficiencies of Posa's method, such algorithms are only efficient\nfor graphs that are either very dense or sparse but regular. This article\nintroduces a method called \"Enlarged Rotation-Extension\" that modifies and\nextends Posa's method, overcoming its deficiencies. Based on this technique,\nour algorithm is polynomial and we give a detailed proof for it."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4024v1", 
    "other_authors": "Vitaly Osipov, Peter Sanders", 
    "title": "n-Level Graph Partitioning", 
    "arxiv-id": "1004.4024v1", 
    "author": "Peter Sanders", 
    "publish": "2010-04-22T22:59:53Z", 
    "summary": "We present a multi-level graph partitioning algorithm based on the extreme\nidea to contract only a single edge on each level of the hierarchy. This\nobviates the need for a matching algorithm and promises very good partitioning\nquality since there are very few changes between two levels. Using an efficient\ndata structure and new flexible ways to break local search improvements early,\nwe obtain an algorithm that scales to large inputs and produces the best known\npartitioning results for many inputs. For example, in Walshaw's well known\nbenchmark tables we achieve 155 improvements dominating the entries for large\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4057v1", 
    "other_authors": "Amit Deshpande, Luis Rademacher", 
    "title": "Efficient volume sampling for row/column subset selection", 
    "arxiv-id": "1004.4057v1", 
    "author": "Luis Rademacher", 
    "publish": "2010-04-23T06:53:25Z", 
    "summary": "We give efficient algorithms for volume sampling, i.e., for picking\n$k$-subsets of the rows of any given matrix with probabilities proportional to\nthe squared volumes of the simplices defined by them and the origin (or the\nsquared volumes of the parallelepipeds defined by these subsets of rows). This\nsolves an open problem from the monograph on spectral algorithms by Kannan and\nVempala. Our first algorithm for volume sampling $k$-subsets of rows from an\n$m$-by-$n$ matrix runs in $O(kmn^{\\omega} \\log n)$ arithmetic operations and a\nsecond variant of it for $(1+\\epsilon)$-approximate volume sampling runs in\n$O(mn \\log m \\cdot k^{2}/\\epsilon^{2} + m \\log^{\\omega} m \\cdot\nk^{2\\omega+1}/\\epsilon^{2\\omega} \\cdot \\log(k \\epsilon^{-1} \\log m))$\narithmetic operations, which is almost linear in the size of the input (i.e.,\nthe number of entries) for small $k$. Our efficient volume sampling algorithms\nimply several interesting results for low-rank matrix approximation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4080v1", 
    "other_authors": "Ramesh Hariharan, Debmalya Panigrahi", 
    "title": "A General Framework for Graph Sparsification", 
    "arxiv-id": "1004.4080v1", 
    "author": "Debmalya Panigrahi", 
    "publish": "2010-04-23T09:36:13Z", 
    "summary": "Given a weighted graph $G$ and an error parameter $\\epsilon > 0$, the {\\em\ngraph sparsification} problem requires sampling edges in $G$ and giving the\nsampled edges appropriate weights to obtain a sparse graph $G_{\\epsilon}$\n(containing O(n\\log n) edges in expectation) with the following property: the\nweight of every cut in $G_{\\epsilon}$ is within a factor of $(1\\pm \\epsilon)$\nof the weight of the corresponding cut in $G$. We provide a generic framework\nthat sets out sufficient conditions for any particular sampling scheme to\nresult in good sparsifiers, and obtain a set of results by simple\ninstantiations of this framework. The results we obtain include the following:\n(1) We improve the time complexity of graph sparsification from O(m\\log^3 n) to\nO(m + n\\log^4 n) for graphs with polynomial edge weights. (2) We improve the\ntime complexity of graph sparsification from O(m\\log^3 n) to O(m\\log^2 n) for\ngraphs with arbitrary edge weights. (3) If the size of the sparsifier is\nallowed to be O(n\\log^2 n/\\epsilon^2) instead of O(n\\log n/\\epsilon^2), we\nimprove the time complexity of sparsification to O(m) for graphs with\npolynomial edge weights. (4) We show that sampling using standard\nconnectivities results in good sparsifiers, thus resolving an open question of\nBenczur and Karger. As a corollary, we give a simple proof of (a slightly\nweaker version of) a result due to Spielman and Srivastava showing that\nsampling using effective resistances produces good sparsifiers. (5) We give a\nsimple proof showing that sampling using strong connectivities results in good\nsparsifiers, a result obtained previously using a more involved proof by\nBenczur and Karger. A key ingredient of our proofs is a generalization of\nbounds on the number of small cuts in an undirected graph due to Karger; this\ngeneralization might be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17514-5_22", 
    "link": "http://arxiv.org/pdf/1004.4240v1", 
    "other_authors": "Anirban Dasgupta, Ravi Kumar, Tam\u00e1s Sarl\u00f3s", 
    "title": "A Sparse Johnson--Lindenstrauss Transform", 
    "arxiv-id": "1004.4240v1", 
    "author": "Tam\u00e1s Sarl\u00f3s", 
    "publish": "2010-04-23T23:57:17Z", 
    "summary": "Dimension reduction is a key algorithmic tool with many applications\nincluding nearest-neighbor search, compressed sensing and linear algebra in the\nstreaming model. In this work we obtain a {\\em sparse} version of the\nfundamental tool in dimension reduction --- the Johnson--Lindenstrauss\ntransform. Using hashing and local densification, we construct a sparse\nprojection matrix with just $\\tilde{O}(\\frac{1}{\\epsilon})$ non-zero entries\nper column. We also show a matching lower bound on the sparsity for a large\nclass of projection matrices. Our bounds are somewhat surprising, given the\nknown lower bounds of $\\Omega(\\frac{1}{\\epsilon^2})$ both on the number of rows\nof any projection matrix and on the sparsity of projection matrices generated\nby natural constructions.\n  Using this, we achieve an $\\tilde{O}(\\frac{1}{\\epsilon})$ update time per\nnon-zero element for a $(1\\pm\\epsilon)$-approximate projection, thereby\nsubstantially outperforming the $\\tilde{O}(\\frac{1}{\\epsilon^2})$ update time\nrequired by prior approaches. A variant of our method offers the same\nguarantees for sparse vectors, yet its $\\tilde{O}(d)$ worst case running time\nmatches the best approach of Ailon and Liberty."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.4420v1", 
    "other_authors": "Eric Angel, Evripidis Bampis, Gerasimos G. Pollatos, Vassilis Zissimopoulos", 
    "title": "Optimal Data Placement on Networks With Constant Number of Clients", 
    "arxiv-id": "1004.4420v1", 
    "author": "Vassilis Zissimopoulos", 
    "publish": "2010-04-26T07:41:35Z", 
    "summary": "We introduce optimal algorithms for the problems of data placement (DP) and\npage placement (PP) in networks with a constant number of clients each of which\nhas limited storage availability and issues requests for data objects. The\nobjective for both problems is to efficiently utilize each client's storage\n(deciding where to place replicas of objects) so that the total incurred access\nand installation cost over all clients is minimized. In the PP problem an extra\nconstraint on the maximum number of clients served by a single client must be\nsatisfied. Our algorithms solve both problems optimally when all objects have\nuniform lengths. When objects lengths are non-uniform we also find the optimal\nsolution, albeit a small, asymptotically tight violation of each client's\nstorage size by $\\epsilon$lmax where lmax is the maximum length of the objects\nand $\\epsilon$ some arbitrarily small positive constant. We make no assumption\non the underlying topology of the network (metric, ultrametric etc.), thus\nobtaining the first non-trivial results for non-metric data placement problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.4915v1", 
    "other_authors": "Ashish Goel, Michael Kapralov, Sanjeev Khanna", 
    "title": "Graph Sparsification via Refinement Sampling", 
    "arxiv-id": "1004.4915v1", 
    "author": "Sanjeev Khanna", 
    "publish": "2010-04-27T21:05:00Z", 
    "summary": "A graph G'(V,E') is an \\eps-sparsification of G for some \\eps>0, if every\n(weighted) cut in G' is within (1\\pm \\eps) of the corresponding cut in G. A\ncelebrated result of Benczur and Karger shows that for every undirected graph\nG, an \\eps-sparsification with O(n\\log n/\\e^2) edges can be constructed in\nO(m\\log^2n) time. Applications to modern massive data sets often constrain\nalgorithms to use computation models that restrict random access to the input.\nThe semi-streaming model, in which the algorithm is constrained to use \\tilde\nO(n) space, has been shown to be a good abstraction for analyzing graph\nalgorithms in applications to large data sets. Recently, a semi-streaming\nalgorithm for graph sparsification was presented by Anh and Guha; the total\nrunning time of their implementation is \\Omega(mn), too large for applications\nwhere both space and time are important. In this paper, we introduce a new\ntechnique for graph sparsification, namely refinement sampling, that gives an\n\\tilde{O}(m) time semi-streaming algorithm for graph sparsification.\n  Specifically, we show that refinement sampling can be used to design a\none-pass streaming algorithm for sparsification that takes O(\\log\\log n) time\nper edge, uses O(\\log^2 n) space per node, and outputs an \\eps-sparsifier with\nO(n\\log^3 n/\\eps^2) edges. At a slightly increased space and time complexity,\nwe can reduce the sparsifier size to O(n \\log n/\\e^2) edges matching the\nBenczur-Karger result, while improving upon the Benczur-Karger runtime for\nm=\\omega(n\\log^3 n). Finally, we show that an \\eps-sparsifier with O(n \\log\nn/\\eps^2) edges can be constructed in two passes over the data and O(m) time\nwhenever m =\\Omega(n^{1+\\delta}) for some constant \\delta>0. As a by-product of\nour approach, we also obtain an O(m\\log\\log n+n \\log n) time streaming\nalgorithm to compute a sparse k-connectivity certificate of a graph."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5010v2", 
    "other_authors": "Marek Cygan, Marcin Pilipczuk, Michal Pilipczuk, Jakub Onufry Wojtaszczyk", 
    "title": "The stubborn problem is stubborn no more (a polynomial algorithm for   3-compatible colouring and the stubborn list partition problem)", 
    "arxiv-id": "1004.5010v2", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2010-04-28T12:20:52Z", 
    "summary": "One of the driving problems in the CSP area is the Dichotomy Conjecture,\nformulated in 1993 by Feder and Vardi [STOC'93], stating that for any fixed\nrelational structure G the Constraint Satisfaction Problem CSP(G) is either\nNP--complete or polynomial time solvable. A large amount of research has gone\ninto checking various specific cases of this conjecture. One such variant which\nattracted a lot of attention in the recent years is the LIST MATRIX PARTITION\nproblem. In 2004 Cameron et al. [SODA'04] classified almost all LIST MATRIX\nPARTITION variants for matrices of size at most four. The only case which\nresisted the classification became known as the STUBBORN PROBLEM. In this paper\nwe show a result which enables us to finish the classification - thus solving a\nproblem which resisted attacks for the last six years.\n  Our approach is based on a combinatorial problem known to be at least as hard\nas the STUBBORN PROBLEM - the 3-COMPATIBLE COLOURING problem. In this problem\nwe are given a complete graph with each edge assigned one of 3 possible colours\nand we want to assign one of those 3 colours to each vertex in such a way that\nno edge has the same colour as both of its endpoints. The tractability of the\n3-COMPATIBLE COLOURING problem has been open for several years and the best\nknown algorithm prior to this paper is due to Feder et al. [SODA'05] - a\nquasipolynomial algorithm with a n^O(log n / log log n) time complexity. In\nthis paper we present a polynomial-time algorithm for the 3-COMPATIBLE\nCOLOURING problem and consequently we prove a dichotomy for the k-COMPATIBLE\nCOLOURING problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5012v1", 
    "other_authors": "Marek Cygan, Marcin Pilipczuk", 
    "title": "Bandwidth and Distortion Revisited", 
    "arxiv-id": "1004.5012v1", 
    "author": "Marcin Pilipczuk", 
    "publish": "2010-04-28T12:29:28Z", 
    "summary": "In this paper we merge recent developments on exact algorithms for finding an\nordering of vertices of a given graph that minimizes bandwidth (the BANDWIDTH\nproblem) and for finding an embedding of a given graph into a line that\nminimizes distortion (the DISTORTION problem). For both problems we develop\nalgorithms that work in O(9.363^n) time and polynomial space. For BANDWIDTH,\nthis improves O^*(10^n) algorithm by Feige and Kilian from 2000, for DISTORTION\nthis is the first polynomial space exact algorithm that works in O(c^n) time we\nare aware of. As a byproduct, we enhance the O(5^{n+o(n)})-time and\nO^*(2^n)-space algorithm for DISTORTION by Fomin et al. to an algorithm working\nin O(4.383^n) time and space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5186v1", 
    "other_authors": "Ilya Safro, Boris Temkin", 
    "title": "Multiscale approach for the network compression-friendly ordering", 
    "arxiv-id": "1004.5186v1", 
    "author": "Boris Temkin", 
    "publish": "2010-04-29T05:31:40Z", 
    "summary": "We present a fast multiscale approach for the network minimum logarithmic\narrangement problem. This type of arrangement plays an important role in a\nnetwork compression and fast node/link access operations. The algorithm is of\nlinear complexity and exhibits good scalability which makes it practical and\nattractive for using on large-scale instances. Its effectiveness is\ndemonstrated on a large set of real-life networks. These networks with\ncorresponding best-known minimization results are suggested as an open\nbenchmark for a research community to evaluate new methods for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.03.025", 
    "link": "http://arxiv.org/pdf/1004.5600v1", 
    "other_authors": "Ashwin Machanavajjhala, Aleksandra Korolova, Atish Das Sarma", 
    "title": "On the (Im)possibility of Preserving Utility and Privacy in Personalized   Social Recommendations", 
    "arxiv-id": "1004.5600v1", 
    "author": "Atish Das Sarma", 
    "publish": "2010-04-30T19:42:14Z", 
    "summary": "With the recent surge of social networks like Facebook, new forms of\nrecommendations have become possible -- personalized recommendations of ads,\ncontent, and even new social and product connections based on one's social\ninteractions. In this paper, we study whether \"social recommendations\", or\nrecommendations that utilize a user's social network, can be made without\ndisclosing sensitive links between users. More precisely, we quantify the loss\nin utility when existing recommendation algorithms are modified to satisfy a\nstrong notion of privacy called differential privacy. We propose lower bounds\non the minimum loss in utility for any recommendation algorithm that is\ndifferentially private. We also propose two recommendation algorithms that\nsatisfy differential privacy, analyze their performance in comparison to the\nlower bound, both analytically and experimentally, and show that good private\nsocial recommendations are feasible only for a few users in the social network\nor for a lenient setting of privacy parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.0407v2", 
    "other_authors": "Petros Drineas, Anastasios Zouzias", 
    "title": "A Note on Element-wise Matrix Sparsification via a Matrix-valued   Bernstein Inequality", 
    "arxiv-id": "1006.0407v2", 
    "author": "Anastasios Zouzias", 
    "publish": "2010-06-02T14:39:17Z", 
    "summary": "Given an n x n matrix A, we present a simple, element-wise sparsification\nalgorithm that zeroes out all sufficiently small elements of A and then retains\nsome of the remaining elements with probabilities proportional to the square of\ntheir magnitudes. We analyze the approximation accuracy of the proposed\nalgorithm using a recent, elegant non-commutative Bernstein inequality, and\ncompare our bounds with all existing (to the best of our knowledge)\nelement-wise matrix sparsification algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.0809v2", 
    "other_authors": "Szymon Grabowski, Wojciech Bieniecki", 
    "title": "Tight and simple Web graph compression", 
    "arxiv-id": "1006.0809v2", 
    "author": "Wojciech Bieniecki", 
    "publish": "2010-06-04T08:41:22Z", 
    "summary": "Analysing Web graphs has applications in determining page ranks, fighting Web\nspam, detecting communities and mirror sites, and more. This study is however\nhampered by the necessity of storing a major part of huge graphs in the\nexternal memory, which prevents efficient random access to edge (hyperlink)\nlists. A number of algorithms involving compression techniques have thus been\npresented, to represent Web graphs succinctly but also providing random access.\nThose techniques are usually based on differential encodings of the adjacency\nlists, finding repeating nodes or node regions in the successive lists, more\ngeneral grammar-based transformations or 2-dimensional representations of the\nbinary matrix of the graph. In this paper we present two Web graph compression\nalgorithms. The first can be seen as engineering of the Boldi and Vigna (2004)\nmethod. We extend the notion of similarity between link lists, and use a more\ncompact encoding of residuals. The algorithm works on blocks of varying size\n(in the number of input lines) and sacrifices access time for better\ncompression ratio, achieving more succinct graph representation than other\nalgorithms reported in the literature. The second algorithm works on blocks of\nthe same size, in the number of input lines, and its key mechanism is merging\nthe block into a single ordered list. This method achieves much more attractive\nspace-time tradeoffs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1104v1", 
    "other_authors": "Jacqueline E. Rice, Kenneth B. Kent", 
    "title": "Systolic Array Technique for Determining Common Approximate Substrings", 
    "arxiv-id": "1006.1104v1", 
    "author": "Kenneth B. Kent", 
    "publish": "2010-06-06T14:11:59Z", 
    "summary": "A technique using a systolic array structure is proposed for solving the\ncommon approximate substring (CAS) problem. This approach extends the technique\nintroduced in earlier work from the computation of the edit-distance between\ntwo strings to the more encompassing CAS problem. A comparison to existing work\nis given, and the technique presented is validated and analyzed based on\nsimulations."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1117v1", 
    "other_authors": "Hagai Cohen, Ely Porat", 
    "title": "On the hardness of distance oracle for sparse graph", 
    "arxiv-id": "1006.1117v1", 
    "author": "Ely Porat", 
    "publish": "2010-06-06T17:03:09Z", 
    "summary": "In this paper we show that set-intersection is harder than distance oracle on\nsparse graphs. Given a collection of total size n which consists of m sets\ndrawn from universe U, the set-intersection problem is to build a data\nstructure which can answer whether two sets have any intersection. A distance\noracle is a data structure which can answer distance queries on a given graph.\nWe show that if one can build distance oracle for sparse graph G=(V,E), which\nrequires s(|V|,|E|) space and answers a (2-\\epsilon,c)-approximate distance\nquery in time t(|V|,|E|) where (2-\\epsilon) is a multiplicative error and c is\na constant additive error, then, set-intersection can be solved in t(m+|U|,n)\ntime using s(m+|U|,n) space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2011.01.010", 
    "link": "http://arxiv.org/pdf/1006.1231v3", 
    "other_authors": "Nikolaos Fountoulakis, Konstantinos Panagiotou, Angelika Steger", 
    "title": "On the Insertion Time of Cuckoo Hashing", 
    "arxiv-id": "1006.1231v3", 
    "author": "Angelika Steger", 
    "publish": "2010-06-07T11:15:01Z", 
    "summary": "Cuckoo hashing is an efficient technique for creating large hash tables with\nhigh space utilization and guaranteed constant access times. There, each item\ncan be placed in a location given by any one out of k different hash functions.\nIn this paper we investigate further the random walk heuristic for inserting in\nan online fashion new items into the hash table. Provided that k > 2 and that\nthe number of items in the table is below (but arbitrarily close) to the\ntheoretically achievable load threshold, we show a polylogarithmic bound for\nthe maximum insertion time that holds with high probability."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.1431v1", 
    "other_authors": "Vedran Dunjko, Elham Kashefi", 
    "title": "Algebraic characterisation of one-way patterns", 
    "arxiv-id": "1006.1431v1", 
    "author": "Elham Kashefi", 
    "publish": "2010-06-08T01:16:41Z", 
    "summary": "We give a complete structural characterisation of the map the positive branch\nof a one-way pattern implements. We start with the representation of the\npositive branch in terms of the phase map decomposition, which is then further\nanalysed to obtain the primary structure of the matrix M, representing the\nphase map decomposition in the computational basis. Using this approach we\nobtain some preliminary results on the connection between the columns structure\nof a given unitary and the angles of measurements in a pattern that implements\nit. We believe this work is a step forward towards a full characterisation of\nthose unitaries with an efficient one-way model implementation."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.1990v2", 
    "other_authors": "Vladimir Kolmogorov", 
    "title": "Minimizing a sum of submodular functions", 
    "arxiv-id": "1006.1990v2", 
    "author": "Vladimir Kolmogorov", 
    "publish": "2010-06-10T10:18:36Z", 
    "summary": "We consider the problem of minimizing a function represented as a sum of\nsubmodular terms. We assume each term allows an efficient computation of {\\em\nexchange capacities}. This holds, for example, for terms depending on a small\nnumber of variables, or for certain cardinality-dependent terms.\n  A naive application of submodular minimization algorithms would not exploit\nthe existence of specialized exchange capacity subroutines for individual\nterms. To overcome this, we cast the problem as a {\\em submodular flow} (SF)\nproblem in an auxiliary graph, and show that applying most existing SF\nalgorithms would rely only on these subroutines.\n  We then explore in more detail Iwata's capacity scaling approach for\nsubmodular flows (Math. Programming, 76(2):299--308, 1997). In particular, we\nshow how to improve its complexity in the case when the function contains\ncardinality-dependent terms."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.2361v1", 
    "other_authors": "Marko A. Rodriguez, Peter Neubauer", 
    "title": "Constructions from Dots and Lines", 
    "arxiv-id": "1006.2361v1", 
    "author": "Peter Neubauer", 
    "publish": "2010-06-11T18:16:10Z", 
    "summary": "A graph is a data structure composed of dots (i.e. vertices) and lines (i.e.\nedges). The dots and lines of a graph can be organized into intricate\narrangements. The ability for a graph to denote objects and their relationships\nto one another allow for a surprisingly large number of things to be modeled as\na graph. From the dependencies that link software packages to the wood beams\nthat provide the framing to a house, most anything has a corresponding graph\nrepresentation. However, just because it is possible to represent something as\na graph does not necessarily mean that its graph representation will be useful.\nIf a modeler can leverage the plethora of tools and algorithms that store and\nprocess graphs, then such a mapping is worthwhile. This article explores the\nworld of graphs in computing and exposes situations in which graphical models\nare beneficial."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.26.8", 
    "link": "http://arxiv.org/pdf/1006.3368v2", 
    "other_authors": "Yuichi Yoshida", 
    "title": "Optimal Constant-Time Approximation Algorithms and (Unconditional)   Inapproximability Results for Every Bounded-Degree CSP", 
    "arxiv-id": "1006.3368v2", 
    "author": "Yuichi Yoshida", 
    "publish": "2010-06-17T04:38:53Z", 
    "summary": "Raghavendra (STOC 2008) gave an elegant and surprising result: if Khot's\nUnique Games Conjecture (STOC 2002) is true, then for every constraint\nsatisfaction problem (CSP), the best approximation ratio is attained by a\ncertain simple semidefinite programming and a rounding scheme for it. In this\npaper, we show that similar results hold for constant-time approximation\nalgorithms in the bounded-degree model. Specifically, we present the\nfollowings: (i) For every CSP, we construct an oracle that serves an access, in\nconstant time, to a nearly optimal solution to a basic LP relaxation of the\nCSP. (ii) Using the oracle, we give a constant-time rounding scheme that\nachieves an approximation ratio coincident with the integrality gap of the\nbasic LP. (iii) Finally, we give a generic conversion from integrality gaps of\nbasic LPs to hardness results. All of those results are \\textit{unconditional}.\nTherefore, for every bounded-degree CSP, we give the best constant-time\napproximation algorithm among all. A CSP instance is called $\\epsilon$-far from\nsatisfiability if we must remove at least an $\\epsilon$-fraction of constraints\nto make it satisfiable. A CSP is called testable if there is a constant-time\nalgorithm that distinguishes satisfiable instances from $\\epsilon$-far\ninstances with probability at least $2/3$. Using the results above, we also\nderive, under a technical assumption, an equivalent condition under which a CSP\nis testable in the bounded-degree model."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3541v1", 
    "other_authors": "Vin\u00edcius G. P. de S\u00e1, Guilherme D. da Fonseca, Raphael Machado, Celina M. H. de Figueiredo", 
    "title": "Complexity dichotomy on partial grid recognition", 
    "arxiv-id": "1006.3541v1", 
    "author": "Celina M. H. de Figueiredo", 
    "publish": "2010-06-17T18:30:09Z", 
    "summary": "Deciding whether a graph can be embedded in a grid using only unit-length\nedges is NP-complete, even when restricted to binary trees. However, it is not\ndifficult to devise a number of graph classes for which the problem is\npolynomial, even trivial. A natural step, outstanding thus far, was to provide\na broad classification of graphs that make for polynomial or NP-complete\ninstances. We provide such a classification based on the set of allowed vertex\ndegrees in the input graphs, yielding a full dichotomy on the complexity of the\nproblem. As byproducts, the previous NP-completeness result for binary trees\nwas strengthened to strictly binary trees, and the three-dimensional version of\nthe problem was for the first time proven to be NP-complete. Our results were\nmade possible by introducing the concepts of consistent orientations and robust\ngadgets, and by showing how the former allows NP-completeness proofs by local\nreplacement even in the absence of the latter."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3715v1", 
    "other_authors": "Prosenjit Bose, Karim Dou\u00efeb", 
    "title": "Should Static Search Trees Ever Be Unbalanced?", 
    "arxiv-id": "1006.3715v1", 
    "author": "Karim Dou\u00efeb", 
    "publish": "2010-06-18T15:08:28Z", 
    "summary": "In this paper we study the question of whether or not a static search tree\nshould ever be unbalanced. We present several methods to restructure an\nunbalanced k-ary search tree $T$ into a new tree $R$ that preserves many of the\nproperties of $T$ while having a height of $\\log_k n +1$ which is one unit off\nof the optimal height. More specifically, we show that it is possible to ensure\nthat the depth of the elements in $R$ is no more than their depth in $T$ plus\nat most $\\log_k \\log_k n +2$. At the same time it is possible to guarantee that\nthe average access time $P(R)$ in tree $R$ is no more than the average access\ntime $P(T)$ in tree $T$ plus $O(\\log_k P(T))$. This suggests that for most\napplications, a balanced tree is always a better option than an unbalanced one\nsince the balanced tree has similar average access time and much better worst\ncase access time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3968v1", 
    "other_authors": "Mugurel Ionut Andreica, Nicolae Tapus", 
    "title": "Practical Range Aggregation, Selection and Set Maintenance Techniques", 
    "arxiv-id": "1006.3968v1", 
    "author": "Nicolae Tapus", 
    "publish": "2010-06-20T23:23:07Z", 
    "summary": "In this paper we present several new and very practical methods and\ntechniques for range aggregation and selection problems in multidimensional\ndata structures and other types of sets of values. We also present some new\nextensions and applications for some fundamental set maintenance problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3970v2", 
    "other_authors": "Eden Chlamtac, Robert Krauthgamer, Prasad Raghavendra", 
    "title": "Approximating Sparsest Cut in Graphs of Bounded Treewidth", 
    "arxiv-id": "1006.3970v2", 
    "author": "Prasad Raghavendra", 
    "publish": "2010-06-21T00:15:00Z", 
    "summary": "We give the first constant-factor approximation algorithm for Sparsest Cut\nwith general demands in bounded treewidth graphs. In contrast to previous\nalgorithms, which rely on the flow-cut gap and/or metric embeddings, our\napproach exploits the Sherali-Adams hierarchy of linear programming\nrelaxations."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.3993v1", 
    "other_authors": "Cyril Prissette", 
    "title": "An Algorithm to List All the Fixed-Point Free Involutions on a Finite   Set", 
    "arxiv-id": "1006.3993v1", 
    "author": "Cyril Prissette", 
    "publish": "2010-06-21T06:38:46Z", 
    "summary": "An involution on a finite set is a bijection such as I(I(e))=e for all the\nelement of the set. A fixed-point free involution on a finite set is an\ninvolution such as I(e)=e for none element of the set. In this article, the\nfixed-point free involutions are represented as partitions of the set and some\nproperties linked to this representation are exhibited. Then an optimal\nalgorithm to list all the fixed-point free involutions is presented. Its\nsoundness relies on the representation of the fixed-point free involutions as\npartitions. Finally, an implementation of the algorithm is proposed, with an\neffective data representation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.01.018", 
    "link": "http://arxiv.org/pdf/1006.4093v1", 
    "other_authors": "Yakov Nekrich", 
    "title": "Dynamic Range Reporting in External Memory", 
    "arxiv-id": "1006.4093v1", 
    "author": "Yakov Nekrich", 
    "publish": "2010-06-21T15:26:40Z", 
    "summary": "In this paper we describe a dynamic external memory data structure that\nsupports range reporting queries in three dimensions in $O(\\log_B^2 N +\n\\frac{k}{B})$ I/O operations, where $k$ is the number of points in the answer\nand $B$ is the block size. This is the first dynamic data structure that\nanswers three-dimensional range reporting queries in $\\log_B^{O(1)} N +\nO(\\frac{k}{B})$ I/Os."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4136v1", 
    "other_authors": "Ferdinando Cicalese, Travis Gagie, Eduardo Laber, Martin Milanic", 
    "title": "Competitive Boolean Function Evaluation: Beyond Monotonicity, and the   Symmetric Case", 
    "arxiv-id": "1006.4136v1", 
    "author": "Martin Milanic", 
    "publish": "2010-06-21T19:07:52Z", 
    "summary": "We study the extremal competitive ratio of Boolean function evaluation. We\nprovide the first non-trivial lower and upper bounds for classes of Boolean\nfunctions which are not included in the class of monotone Boolean functions.\nFor the particular case of symmetric functions our bounds are matching and we\nexactly characterize the best possible competitiveness achievable by a\ndeterministic algorithm. Our upper bound is obtained by a simple polynomial\ntime algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4357v1", 
    "other_authors": "Chandra Chekuri, Alina Ene, Nitish Korula", 
    "title": "Prize-Collecting Steiner Tree and Forest in Planar Graphs", 
    "arxiv-id": "1006.4357v1", 
    "author": "Nitish Korula", 
    "publish": "2010-06-22T19:53:58Z", 
    "summary": "We obtain polynomial-time approximation-preserving reductions (up to a factor\nof 1 + \\epsilon) from the prize-collecting Steiner tree and prize-collecting\nSteiner forest problems in planar graphs to the corresponding problems in\ngraphs of bounded treewidth. We also give an exact algorithm for the\nprize-collecting Steiner tree problem that runs in polynomial time for graphs\nof bounded treewidth. This, combined with our reductions, yields a PTAS for the\nprize-collecting Steiner tree problem in planar graphs and generalizes the PTAS\nof Borradaile, Klein and Mathieu for the Steiner tree problem in planar graphs.\nOur results build upon the ideas of Borradaile, Klein and Mathieu and the work\nof Bateni, Hajiaghayi and Marx on a PTAS for the Steiner forest problem in\nplanar graphs. Our main technical result is on the properties of primal-dual\nalgorithms for Steiner tree and forest problems in general graphs when they are\nrun with scaled up penalties."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2010.05.016", 
    "link": "http://arxiv.org/pdf/1006.4536v1", 
    "other_authors": "Moses Charikar, Tom Leighton, Shi Li, Ankur Moitra", 
    "title": "Vertex Sparsifiers and Abstract Rounding Algorithms", 
    "arxiv-id": "1006.4536v1", 
    "author": "Ankur Moitra", 
    "publish": "2010-06-23T14:44:07Z", 
    "summary": "The notion of vertex sparsification is introduced in \\cite{M}, where it was\nshown that for any graph $G = (V, E)$ and a subset of $k$ terminals $K \\subset\nV$, there is a polynomial time algorithm to construct a graph $H = (K, E_H)$ on\njust the terminal set so that simultaneously for all cuts $(A, K-A)$, the value\nof the minimum cut in $G$ separating $A$ from $K -A$ is approximately the same\nas the value of the corresponding cut in $H$.\n  We give the first super-constant lower bounds for how well a cut-sparsifier\n$H$ can simultaneously approximate all minimum cuts in $G$. We prove a lower\nbound of $\\Omega(\\log^{1/4} k)$ -- this is polynomially-related to the known\nupper bound of $O(\\log k/\\log \\log k)$. This is an exponential improvement on\nthe $\\Omega(\\log \\log k)$ bound given in \\cite{LM} which in fact was for a\nstronger vertex sparsification guarantee, and did not apply to cut sparsifiers.\n  Despite this negative result, we show that for many natural problems, we do\nnot need to incur a multiplicative penalty for our reduction. We obtain optimal\n$O(\\log k)$-competitive Steiner oblivious routing schemes, which generalize the\nresults in \\cite{R}. We also demonstrate that for a wide range of graph packing\nproblems (which includes maximum concurrent flow, maximum multiflow and\nmulticast routing, among others, as a special case), the integrality gap of the\nlinear program is always at most $O(\\log k)$ times the integrality gap\nrestricted to trees. This result helps to explain the ubiquity of the $O(\\log\nk)$ guarantees for such problems.\n  Lastly, we use our ideas to give an efficient construction for\nvertex-sparsifiers that match the current best existential results -- this was\npreviously open. Our algorithm makes novel use of Earth-mover constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1137/130908440", 
    "link": "http://arxiv.org/pdf/1006.4586v3", 
    "other_authors": "Matthias Englert, Anupam Gupta, Robert Krauthgamer, Harald Raecke, Inbal Talgam, Kunal Talwar", 
    "title": "Vertex Sparsifiers: New Results from Old Techniques", 
    "arxiv-id": "1006.4586v3", 
    "author": "Kunal Talwar", 
    "publish": "2010-06-23T16:43:47Z", 
    "summary": "Given a capacitated graph $G = (V,E)$ and a set of terminals $K \\subseteq V$,\nhow should we produce a graph $H$ only on the terminals $K$ so that every\n(multicommodity) flow between the terminals in $G$ could be supported in $H$\nwith low congestion, and vice versa? (Such a graph $H$ is called a\nflow-sparsifier for $G$.) What if we want $H$ to be a \"simple\" graph? What if\nwe allow $H$ to be a convex combination of simple graphs?\n  Improving on results of Moitra [FOCS 2009] and Leighton and Moitra [STOC\n2010], we give efficient algorithms for constructing: (a) a flow-sparsifier $H$\nthat maintains congestion up to a factor of $O(\\log k/\\log \\log k)$, where $k =\n|K|$, (b) a convex combination of trees over the terminals $K$ that maintains\ncongestion up to a factor of $O(\\log k)$, and (c) for a planar graph $G$, a\nconvex combination of planar graphs that maintains congestion up to a constant\nfactor. This requires us to give a new algorithm for the 0-extension problem,\nthe first one in which the preimages of each terminal are connected in $G$.\nMoreover, this result extends to minor-closed families of graphs.\n  Our improved bounds immediately imply improved approximation guarantees for\nseveral terminal-based cut and ordering problems."
},{
    "category": "cs.DS", 
    "doi": "10.1137/130908440", 
    "link": "http://arxiv.org/pdf/1006.4607v2", 
    "other_authors": "Konstantin Makarychev, Yury Makarychev", 
    "title": "Metric Extension Operators, Vertex Sparsifiers and Lipschitz   Extendability", 
    "arxiv-id": "1006.4607v2", 
    "author": "Yury Makarychev", 
    "publish": "2010-06-23T18:43:33Z", 
    "summary": "We study vertex cut and flow sparsifiers that were recently introduced by\nMoitra, and Leighton and Moitra. We improve and generalize their results. We\ngive a new polynomial-time algorithm for constructing O(log k / log log k) cut\nand flow sparsifiers, matching the best existential upper bound on the quality\nof a sparsifier, and improving the previous algorithmic upper bound of O(log^2\nk / log log k). We show that flow sparsifiers can be obtained from linear\noperators approximating minimum metric extensions. We introduce the notion of\n(linear) metric extension operators, prove that they exist, and give an exact\npolynomial-time algorithm for finding optimal operators.\n  We then establish a direct connection between flow and cut sparsifiers and\nLipschitz extendability of maps in Banach spaces, a notion studied in\nfunctional analysis since 1930s. Using this connection, we prove a lower bound\nof Omega(sqrt{log k/log log k}) for flow sparsifiers and a lower bound of\nOmega(sqrt{log k}/log log k) for cut sparsifiers. We show that if a certain\nopen question posed by Ball in 1992 has a positive answer, then there exist\n\\tilde O(sqrt{log k}) cut sparsifiers. On the other hand, any lower bound on\ncut sparsifiers better than \\tilde Omega(sqrt{log k}) would imply a negative\nanswer to this question."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17458-2_16", 
    "link": "http://arxiv.org/pdf/1006.4828v1", 
    "other_authors": "Vamsi Kundeti, Sanguthevar Rajasekaran, Hieu Dinh", 
    "title": "An Efficient Algorithm For Chinese Postman Walk on Bi-directed de Bruijn   Graphs", 
    "arxiv-id": "1006.4828v1", 
    "author": "Hieu Dinh", 
    "publish": "2010-06-24T16:35:47Z", 
    "summary": "Sequence assembly from short reads is an important problem in biology. It is\nknown that solving the sequence assembly problem exactly on a bi-directed de\nBruijn graph or a string graph is intractable. However finding a Shortest\nDouble stranded DNA string (SDDNA) containing all the k-long words in the reads\nseems to be a good heuristic to get close to the original genome. This problem\nis equivalent to finding a cyclic Chinese Postman (CP) walk on the underlying\nun-weighted bi-directed de Bruijn graph built from the reads. The Chinese\nPostman walk Problem (CPP) is solved by reducing it to a general bi-directed\nflow on this graph which runs in O(|E|2 log2(|V |)) time. In this paper we show\nthat the cyclic CPP on bi-directed graphs can be solved without reducing it to\nbi-directed flow. We present a ?(p(|V | + |E|) log(|V |) + (dmaxp)3) time\nalgorithm to solve the cyclic CPP on a weighted bi-directed de Bruijn graph,\nwhere p = max{|{v|din(v) - dout(v) > 0}|, |{v|din(v) - dout(v) < 0}|} and dmax\n= max{|din(v) - dout(v)}. Our algorithm performs asymptotically better than the\nbidirected flow algorithm when the number of imbalanced nodes p is much less\nthan the nodes in the bi-directed graph. From our experimental results on\nvarious datasets, we have noticed that the value of p/|V | lies between 0.08%\nand 0.13% with 95% probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10618-010-0185-7", 
    "link": "http://arxiv.org/pdf/1006.5235v1", 
    "other_authors": "Andrea Pietracaprina, Matteo Riondato, Eli Upfal, Fabio Vandin", 
    "title": "Mining Top-K Frequent Itemsets Through Progressive Sampling", 
    "arxiv-id": "1006.5235v1", 
    "author": "Fabio Vandin", 
    "publish": "2010-06-27T20:38:39Z", 
    "summary": "We study the use of sampling for efficiently mining the top-K frequent\nitemsets of cardinality at most w. To this purpose, we define an approximation\nto the top-K frequent itemsets to be a family of itemsets which includes\n(resp., excludes) all very frequent (resp., very infrequent) itemsets, together\nwith an estimate of these itemsets' frequencies with a bounded error. Our first\nresult is an upper bound on the sample size which guarantees that the top-K\nfrequent itemsets mined from a random sample of that size approximate the\nactual top-K frequent itemsets, with probability larger than a specified value.\nWe show that the upper bound is asymptotically tight when w is constant. Our\nmain algorithmic contribution is a progressive sampling approach, combined with\nsuitable stopping conditions, which on appropriate inputs is able to extract\napproximate top-K frequent itemsets from samples whose sizes are smaller than\nthe general upper bound. In order to test the stopping conditions, this\napproach maintains the frequency of all itemsets encountered, which is\npractical only for small w. However, we show how this problem can be mitigated\nby using a variation of Bloom filters. A number of experiments conducted on\nboth synthetic and real bench- mark datasets show that using samples\nsubstantially smaller than the original dataset (i.e., of size defined by the\nupper bound or reached through the progressive sampling approach) enable to\napproximate the actual top-K frequent itemsets with accuracy much higher than\nwhat analytically proved."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.knosys.2010.05.009", 
    "link": "http://arxiv.org/pdf/1009.0670v1", 
    "other_authors": "Marko A. Rodriguez, Jennifer H. Watkins", 
    "title": "Grammar-Based Geodesics in Semantic Networks", 
    "arxiv-id": "1009.0670v1", 
    "author": "Jennifer H. Watkins", 
    "publish": "2010-09-03T13:49:47Z", 
    "summary": "A geodesic is the shortest path between two vertices in a connected network.\nThe geodesic is the kernel of various network metrics including radius,\ndiameter, eccentricity, closeness, and betweenness. These metrics are the\nfoundation of much network research and thus, have been studied extensively in\nthe domain of single-relational networks (both in their directed and undirected\nforms). However, geodesics for single-relational networks do not translate\ndirectly to multi-relational, or semantic networks, where vertices are\nconnected to one another by any number of edge labels. Here, a more\nsophisticated method for calculating a geodesic is necessary. This article\npresents a technique for calculating geodesics in semantic networks with a\nfocus on semantic networks represented according to the Resource Description\nFramework (RDF). In this framework, a discrete \"walker\" utilizes an abstract\npath description called a grammar to determine which paths to include in its\ngeodesic calculation. The grammar-based model forms a general framework for\nstudying geodesic metrics in semantic networks."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0783v1", 
    "other_authors": "David Eppstein, Michael T. Goodrich, Darren Strash, Lowell Trott", 
    "title": "Extended h-Index Parameterized Data Structures for Computing Dynamic   Subgraph Statistics", 
    "arxiv-id": "1009.0783v1", 
    "author": "Lowell Trott", 
    "publish": "2010-09-03T22:58:10Z", 
    "summary": "We present techniques for maintaining subgraph frequencies in a dynamic\ngraph, using data structures that are parameterized in terms of h, the h-index\nof the graph. Our methods extend previous results of Eppstein and Spiro for\nmaintaining statistics for undirected subgraphs of size three to directed\nsubgraphs and to subgraphs of size four. For the directed case, we provide a\ndata structure to maintain counts for all 3-vertex induced subgraphs in O(h)\namortized time per update. For the undirected case, we maintain the counts of\nsize-four subgraphs in O(h^2) amortized time per update. These extensions\nenable a number of new applications in Bioinformatics and Social Networking\nresearch."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0806v1", 
    "other_authors": "Geevarghese Philip, Venkatesh Raman, Yngve Villanger", 
    "title": "A Quartic Kernel for Pathwidth-One Vertex Deletion", 
    "arxiv-id": "1009.0806v1", 
    "author": "Yngve Villanger", 
    "publish": "2010-09-04T05:39:41Z", 
    "summary": "The pathwidth of a graph is a measure of how path-like the graph is. Given a\ngraph G and an integer k, the problem of finding whether there exist at most k\nvertices in G whose deletion results in a graph of pathwidth at most one is NP-\ncomplete. We initiate the study of the parameterized complexity of this\nproblem, parameterized by k. We show that the problem has a quartic\nvertex-kernel: We show that, given an input instance (G = (V, E), k); |V| = n,\nwe can construct, in polynomial time, an instance (G', k') such that (i) (G, k)\nis a YES instance if and only if (G', k') is a YES instance, (ii) G' has\nO(k^{4}) vertices, and (iii) k' \\leq k. We also give a fixed parameter\ntractable (FPT) algorithm for the problem that runs in O(7^{k} k \\cdot n^{2})\ntime."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.0909v2", 
    "other_authors": "Bonnie Kirkpatrick, Yakir Reshef, Hilary Finucane, Haitao Jiang, Binhai Zhu, Richard M. Karp", 
    "title": "Comparing Pedigree Graphs", 
    "arxiv-id": "1009.0909v2", 
    "author": "Richard M. Karp", 
    "publish": "2010-09-05T12:02:26Z", 
    "summary": "Pedigree graphs, or family trees, are typically constructed by an expensive\nprocess of examining genealogical records to determine which pairs of\nindividuals are parent and child. New methods to automate this process take as\ninput genetic data from a set of extant individuals and reconstruct ancestral\nindividuals. There is a great need to evaluate the quality of these methods by\ncomparing the estimated pedigree to the true pedigree.\n  In this paper, we consider two main pedigree comparison problems. The first\nis the pedigree isomorphism problem, for which we present a linear-time\nalgorithm for leaf-labeled pedigrees. The second is the pedigree edit distance\nproblem, for which we present 1) several algorithms that are fast and exact in\nvarious special cases, and 2) a general, randomized heuristic algorithm.\n  In the negative direction, we first prove that the pedigree isomorphism\nproblem is as hard as the general graph isomorphism problem, and that the\nsub-pedigree isomorphism problem is NP-hard. We then show that the pedigree\nedit distance problem is APX-hard in general and NP-hard on leaf-labeled\npedigrees.\n  We use simulated pedigrees to compare our edit-distance algorithms to each\nother as well as to a branch-and-bound algorithm that always finds an optimal\nsolution."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.1381v1", 
    "other_authors": "Serge Gaspers, Mathieu Liedloff", 
    "title": "A Branch-and-Reduce Algorithm for Finding a Minimum Independent   Dominating Set", 
    "arxiv-id": "1009.1381v1", 
    "author": "Mathieu Liedloff", 
    "publish": "2010-09-07T19:57:23Z", 
    "summary": "An independent dominating set D of a graph G = (V,E) is a subset of vertices\nsuch that every vertex in V \\ D has at least one neighbor in D and D is an\nindependent set, i.e. no two vertices of D are adjacent in G. Finding a minimum\nindependent dominating set in a graph is an NP-hard problem. Whereas it is hard\nto cope with this problem using parameterized and approximation algorithms,\nthere is a simple exact O(1.4423^n)-time algorithm solving the problem by\nenumerating all maximal independent sets. In this paper we improve the latter\nresult, providing the first non trivial algorithm computing a minimum\nindependent dominating set of a graph in time O(1.3569^n). Furthermore, we give\na lower bound of \\Omega(1.3247^n) on the worst-case running time of this\nalgorithm, showing that the running time analysis is almost tight."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.1697v1", 
    "other_authors": "Oleg Titov", 
    "title": "One method of storing information", 
    "arxiv-id": "1009.1697v1", 
    "author": "Oleg Titov", 
    "publish": "2010-09-09T07:41:22Z", 
    "summary": "Formulate the problem as follows. Split a file into n pieces so that it can\nbe restored without any m parts (1<=m<=n). Such problems are called problems\nsecret sharing. There exists a set of methods for solving such problems, but\nthey all require a fairly large number of calculations applied to the problem\nposed above. The proposed method does not require calculations, and requires\nonly the operations of the division of the file into equal (nearly equal) parts\nand gluing them in a certain order in one or more files."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2109v2", 
    "other_authors": "Evmorfia N. Argyriou, Michael A. Bekos, Antonios Symvonis", 
    "title": "Maximizing the Total Resolution of Graphs", 
    "arxiv-id": "1009.2109v2", 
    "author": "Antonios Symvonis", 
    "publish": "2010-09-10T21:46:39Z", 
    "summary": "A major factor affecting the readability of a graph drawing is its\nresolution. In the graph drawing literature, the resolution of a drawing is\neither measured based on the angles formed by consecutive edges incident to a\ncommon node (angular resolution) or by the angles formed at edge crossings\n(crossing resolution). In this paper, we evaluate both by introducing the\nnotion of \"total resolution\", that is, the minimum of the angular and crossing\nresolution. To the best of our knowledge, this is the first time where the\nproblem of maximizing the total resolution of a drawing is studied.\n  The main contribution of the paper consists of drawings of asymptotically\noptimal total resolution for complete graphs (circular drawings) and for\ncomplete bipartite graphs (2-layered drawings). In addition, we present and\nexperimentally evaluate a force-directed based algorithm that constructs\ndrawings of large total resolution."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2322v1", 
    "other_authors": "Joseph Wun-Tat Chan, Francis Y. L. Chin, Xin Han, Ka-Cheong Lam, Hing-Fung Ting, Yong Zhang", 
    "title": "Deterministic Online Call Control in Cellular Networks and Triangle-Free   Cellular Networks", 
    "arxiv-id": "1009.2322v1", 
    "author": "Yong Zhang", 
    "publish": "2010-09-13T08:36:19Z", 
    "summary": "Wireless Communication Networks based on Frequency Division Multiplexing (FDM\nin short) plays an important role in the field of communications, in which each\nrequest can be satisfied by assigning a frequency. To avoid interference, each\nassigned frequency must be different to the neighboring assigned frequencies.\nSince frequency is a scarce resource, the main problem in wireless networks is\nhow to fully utilize the given bandwidth of frequencies. In this paper, we\nconsider the online call control problem. Given a fixed bandwidth of\nfrequencies and a sequence of communication requests arrive over time, each\nrequest must be either satisfied immediately after its arrival by assigning an\navailable frequency, or rejected. The objective of call control problem is to\nmaximize the number of accepted requests. We study the asymptotic performance\nof this problem, i.e., the number of requests in the sequence and the bandwidth\nof frequencies are very large. In this paper, we give a 7/3-competitive\nalgorithm for call control problem in cellular network, improving the previous\n2.5-competitive result. Moreover, we investigate the triangle-free cellular\nnetwork, propose a 9/4-competitive algorithm and prove that the lower bound of\ncompetitive ratio is at least 5/3."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2452v1", 
    "other_authors": "Deeparnab Chakrabarty, Chaitanya Swamy", 
    "title": "Facility Location with Client Latencies: Linear-Programming based   Techniques for Minimum-Latency Problems", 
    "arxiv-id": "1009.2452v1", 
    "author": "Chaitanya Swamy", 
    "publish": "2010-09-13T17:04:34Z", 
    "summary": "We introduce a problem that is a common generalization of the uncapacitated\nfacility location and minimum latency (ML) problems, where facilities need to\nbe opened to serve clients and also need to be sequentially activated before\nthey can provide service. Formally, we are given a set \\F of n facilities with\nfacility-opening costs {f_i}, a set of m clients, and connection costs {c_{ij}}\nspecifying the cost of assigning a client j to a facility i, a root node r\ndenoting the depot, and a time metric d on \\F\\cup{r}. Our goal is to open a\nsubset F of facilities, find a path P starting at r and spanning F to activate\nthe open facilities, and connect each client j to a facility \\phi(j)\\in F, so\nas to minimize \\sum_{i\\in F}f_i +\\sum_{clients j}(c_{\\phi(j),j}+t_j), where t_j\nis the time taken to reach \\phi(j) along path P. We call this the minimum\nlatency uncapacitated facility location (MLUFL) problem.\n  Our main result is an O(\\log n\\max{\\log n,\\log m})-approximation for MLUFL.\nWe also show that any improvement in this approximation guarantee, implies an\nimprovement in the (current-best) approximation factor for group Steiner tree.\nWe obtain constant approximations for two natural special cases of the problem:\n(a) related MLUFL (metric connection costs that are a scalar multiple of the\ntime metric); (b) metric uniform MLUFL (metric connection costs, unform\ntime-metric). Our LP-based methods are versatile and easily adapted to yield\napproximation guarantees for MLUFL in various more general settings, such as\n(i) when the latency-cost of a client is a function of the delay faced by the\nfacility to which it is connected; and (ii) the k-route version, where k\nvehicles are routed in parallel to activate the open facilities. Our LP-based\nunderstanding of MLUFL also offers some LP-based insights into ML, which we\nbelieve is a promising direction for obtaining improvements for ML."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.2591v1", 
    "other_authors": "Telikepalli Kavitha, Meghana Nasre, Prajakta Nimbhorkar", 
    "title": "Popularity at Minimum Cost", 
    "arxiv-id": "1009.2591v1", 
    "author": "Prajakta Nimbhorkar", 
    "publish": "2010-09-14T08:31:31Z", 
    "summary": "We consider an extension of the {\\em popular matching} problem in this paper.\nThe input to the popular matching problem is a bipartite graph G = (A U B,E),\nwhere A is a set of people, B is a set of items, and each person a belonging to\nA ranks a subset of items in an order of preference, with ties allowed. The\npopular matching problem seeks to compute a matching M* between people and\nitems such that there is no matching M where more people are happier with M\nthan with M*. Such a matching M* is called a popular matching. However, there\nare simple instances where no popular matching exists.\n  Here we consider the following natural extension to the above problem:\nassociated with each item b belonging to B is a non-negative price cost(b),\nthat is, for any item b, new copies of b can be added to the input graph by\npaying an amount of cost(b) per copy. When G does not admit a popular matching,\nthe problem is to \"augment\" G at minimum cost such that the new graph admits a\npopular matching. We show that this problem is NP-hard; in fact, it is NP-hard\nto approximate it within a factor of sqrt{n1}/2, where n1 is the number of\npeople. This problem has a simple polynomial time algorithm when each person\nhas a preference list of length at most 2. However, if we consider the problem\nof \"constructing\" a graph at minimum cost that admits a popular matching that\nmatches all people, then even with preference lists of length 2, the problem\nbecomes NP-hard. On the other hand, when the number of copies of each item is\n\"fixed\", we show that the problem of computing a minimum cost popular matching\nor deciding that no popular matching exists can be solved in O(mn1) time, where\nm is the number of edges."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3502v1", 
    "other_authors": "Krishnam Raju Jampani, Anna Lubiw", 
    "title": "Simultaneous Interval Graphs", 
    "arxiv-id": "1009.3502v1", 
    "author": "Anna Lubiw", 
    "publish": "2010-09-17T20:25:56Z", 
    "summary": "In a recent paper, we introduced the simultaneous representation problem\n(defined for any graph class C) and studied the problem for chordal,\ncomparability and permutation graphs. For interval graphs, the problem is\ndefined as follows. Two interval graphs G_1 and G_2, sharing some vertices I\n(and the corresponding induced edges), are said to be `simultaneous interval\ngraphs' if there exist interval representations R_1 and R_2 of G_1 and G_2,\nsuch that any vertex of I is mapped to the same interval in both R_1 and R_2.\nEquivalently, G_1 and G_2 are simultaneous interval graphs if there exist edges\nE' between G_1-I and G_2-I such that G_1 \\cup G_2 \\cup E' is an interval graph.\n  Simultaneous representation problems are related to simultaneous planar\nembeddings, and have applications in any situation where it is desirable to\nconsistently represent two related graphs, for example: interval graphs\ncapturing overlaps of DNA fragments of two similar organisms; or graphs\nconnected in time, where one is an updated version of the other.\n  In this paper we give an O(n^2*logn) time algorithm for recognizing\nsimultaneous interval graphs,where n = |G_1 \\cup G_2|. This result complements\nthe polynomial time algorithms for recognizing probe interval graphs and\nprovides an efficient algorithm for the interval graph sandwich problem for the\nspecial case where the set of optional edges induce a complete bipartite graph."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3594v2", 
    "other_authors": "Pranjal Awasthi, Avrim Blum, Or Sheffet", 
    "title": "Center-based Clustering under Perturbation Stability", 
    "arxiv-id": "1009.3594v2", 
    "author": "Or Sheffet", 
    "publish": "2010-09-18T22:57:43Z", 
    "summary": "Clustering under most popular objective functions is NP-hard, even to\napproximate well, and so unlikely to be efficiently solvable in the worst case.\nRecently, Bilu and Linial \\cite{Bilu09} suggested an approach aimed at\nbypassing this computational barrier by using properties of instances one might\nhope to hold in practice. In particular, they argue that instances in practice\nshould be stable to small perturbations in the metric space and give an\nefficient algorithm for clustering instances of the Max-Cut problem that are\nstable to perturbations of size $O(n^{1/2})$. In addition, they conjecture that\ninstances stable to as little as O(1) perturbations should be solvable in\npolynomial time. In this paper we prove that this conjecture is true for any\ncenter-based clustering objective (such as $k$-median, $k$-means, and\n$k$-center). Specifically, we show we can efficiently find the optimal\nclustering assuming only stability to factor-3 perturbations of the underlying\nmetric in spaces without Steiner points, and stability to factor $2+\\sqrt{3}$\nperturbations for general metrics. In particular, we show for such instances\nthat the popular Single-Linkage algorithm combined with dynamic programming\nwill find the optimal clustering. We also present NP-hardness results under a\nweaker but related condition."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.3809v2", 
    "other_authors": "Ramesh C. Bagadi", 
    "title": "One, Two, Three and N Dimensional String Search Algorithms", 
    "arxiv-id": "1009.3809v2", 
    "author": "Ramesh C. Bagadi", 
    "publish": "2010-09-20T13:11:52Z", 
    "summary": "In this research endeavor, some Sequence Alignment Algorithms are detailed\nthat are useful for finding or comparing 1 dimensional (1-D), 2 dimensional\n(2-D), 3 dimensional (3-D) sequences in or against a parent or mother database\nwhich is 1 dimensional (1-D), 2 dimensional (2-D), 3 dimensional (3-D)\nsequence. Inner Product [1], [2] based schemes are used to lay down such\nalgorithms. Also,in this research, a Sequence Alignment Algorithms is detailed\nthat is useful for finding or comparing an N-Dimensional (N-D) sequence in or\nagainst a parent or mother database which N-Dimensional (N-D) sequence. Inner\nProduct [1], [2] based schemes are used to lay down such an algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4214v2", 
    "other_authors": "Pramod Ganapathi, Rama B", 
    "title": "A Versatile Algorithm to Generate Various Combinatorial Structures", 
    "arxiv-id": "1009.4214v2", 
    "author": "Rama B", 
    "publish": "2010-09-21T20:59:51Z", 
    "summary": "Algorithms to generate various combinatorial structures find tremendous\nimportance in computer science. In this paper, we begin by reviewing an\nalgorithm proposed by Rohl that generates all unique permutations of a list of\nelements which possibly contains repetitions, taking some or all of the\nelements at a time, in any imposed order. The algorithm uses an auxiliary array\nthat maintains the number of occurrences of each unique element in the input\nlist. We provide a proof of correctness of the algorithm. We then show how one\ncan efficiently generate other combinatorial structures like combinations,\nsubsets, n-Parenthesizations, derangements and integer partitions &\ncompositions with minor changes to the same algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4355v1", 
    "other_authors": "Ho-Leung Chan, Nicole Megow, Rob van Stee, Rene Sitters", 
    "title": "The Sorting Buffer Problem is NP-hard", 
    "arxiv-id": "1009.4355v1", 
    "author": "Rene Sitters", 
    "publish": "2010-09-22T13:14:51Z", 
    "summary": "We consider the offline sorting buffer problem. The input is a sequence of\nitems of different types. All items must be processed one by one by a server.\nThe server is equipped with a random-access buffer of limited capacity which\ncan be used to rearrange items. The problem is to design a scheduling strategy\nthat decides upon the order in which items from the buffer are sent to the\nserver. Each type change incurs unit cost, and thus, the cost minimizing\nobjective is to minimize the total number of type changes for serving the\nentire sequence. This problem is motivated by various applications in\nmanufacturing processes and computer science, and it has attracted significant\nattention in the last few years. The main focus has been on online competitive\nalgorithms. Surprisingly little is known on the basic offline problem. In this\npaper, we show that the sorting buffer problem with uniform cost is NP-hard\nand, thus, close one of the most fundamental questions for the offline problem.\nOn the positive side, we give an O(1)-approximation algorithm when the\nscheduler is given a buffer only slightly larger than double the original size.\nWe also give a dynamic programming algorithm for the special case of buffer\nsize two that solves the problem exactly in linear time, improving on the\nstandard DP which runs in cubic time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4517v2", 
    "other_authors": "Bernhard Haeupler, Krishnam Raju Jampani, Anna Lubiw", 
    "title": "Testing Simultaneous Planarity when the Common Graph is 2-Connected", 
    "arxiv-id": "1009.4517v2", 
    "author": "Anna Lubiw", 
    "publish": "2010-09-23T04:24:51Z", 
    "summary": "Two planar graphs G1 and G2 sharing some vertices and edges are\n`simultaneously planar' if they have planar drawings such that a shared vertex\n[edge] is represented by the same point [curve] in both drawings. It is an open\nproblem whether simultaneous planarity can be tested efficiently. We give a\nlinear-time algorithm to test simultaneous planarity when the two graphs share\na 2-connected subgraph. Our algorithm extends to the case of k planar graphs\nwhere each vertex [edge] is either common to all graphs or belongs to exactly\none of them."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4529v1", 
    "other_authors": "Ulrich M. Schwarz", 
    "title": "A PTAS for Scheduling with Tree Assignment Restrictions", 
    "arxiv-id": "1009.4529v1", 
    "author": "Ulrich M. Schwarz", 
    "publish": "2010-09-23T07:12:49Z", 
    "summary": "Scheduling with assignment restrictions is an important special case of\nscheduling unrelated machines which has attracted much attention in the recent\npast. While a lower bound on approximability of 3/2 is known for its most\ngeneral setting, subclasses of the problem admit polynomial-time approximation\nschemes. This note provides a PTAS for tree-like hierarchical structures,\nimproving on a recent 4/3-approximation by Huo and Leung."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4830v3", 
    "other_authors": "Timon Hertli, Robin A. Moser, Dominik Scheder", 
    "title": "Improving PPSZ for 3-SAT using Critical Variables", 
    "arxiv-id": "1009.4830v3", 
    "author": "Dominik Scheder", 
    "publish": "2010-09-24T13:06:34Z", 
    "summary": "A critical variable of a satisfiable CNF formula is a variable that has the\nsame value in all satisfying assignments. Using a simple case distinction on\nthe fraction of critical variables of a CNF formula, we improve the running\ntime for 3-SAT from O(1.32216^n) by Rolf [2006] to O(1.32153^n). Using a\ndifferent approach, Iwama et al. [2010] very recently achieved a running time\nof O(1.32113^n). Our method nicely combines with theirs, yielding the currently\nfastest known algorithm with running time O(1.32065^n). We also improve the\nbound for 4-SAT from O(1.47390^n) [Iwama, Tamaki 2004] to O(1.46928^n), where\nO(1.46981^n) can be obtained using the methods of [Iwama, Tamaki 2004] and\n[Rolf 2006]."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.4880v1", 
    "other_authors": "Gerald Paul", 
    "title": "An Efficient Implementation of the Robust Tabu Search Heuristic for   Sparse Quadratic Assignment Problems", 
    "arxiv-id": "1009.4880v1", 
    "author": "Gerald Paul", 
    "publish": "2010-09-24T16:05:29Z", 
    "summary": "We propose and develop an efficient implementation of the robust tabu search\nheuristic for sparse quadratic assignment problems. The traditional\nimplementation of the heuristic applicable to all quadratic assignment problems\nis of O(N^2) complexity per iteration for problems of size N. Using multiple\npriority queues to determine the next best move instead of scanning all\npossible moves, and using adjacency lists to minimize the operations needed to\ndetermine the cost of moves, we reduce the asymptotic complexity per iteration\nto O(N log N ). For practical sized problems, the complexity is O(N)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.5143v2", 
    "other_authors": "Yixin Cao, Jianer Chen", 
    "title": "FAST: Kernelization based on Graph Modular Decomposition", 
    "arxiv-id": "1009.5143v2", 
    "author": "Jianer Chen", 
    "publish": "2010-09-27T02:29:59Z", 
    "summary": "Kernelization algorithms, usually a preprocessing step before other more\ntraditional algorithms, are very special in the sense that they return\n(reduced) instances, instead of final results. This characteristic excludes the\nfreedom of applying a kernelization algorithm for the weighted version of a\nproblem to its unweighted instances. Thus with only very few special cases,\nkernelization algorithms have to be studied separately for weigthed and\nunweighted versions of a single problem. {\\sc feedback arc set on tournament}\nis currently a very popular problem in recent research of parameterized, as\nwell as approximation computation, and its wide applications in many areas make\nit appear in all top conferences. The theory of graph modular decompositions is\na general approach in the study of graph structures, which only had its\nsurfaces touched in previous work on kernelization algorithms of {\\sc feedback\narc set on tournament}. In this paper, we study further properties of graph\nmodular decompositions and apply them to obtain the first linear kernel for the\nunweighted {\\sc feedback arc set on tournament} problem, which only admits\nlinear kernel in its weighted version, while quadratic kernel for the\nunweighted."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2011.11.034", 
    "link": "http://arxiv.org/pdf/1009.5168v2", 
    "other_authors": "Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua Teng, Yu Xia", 
    "title": "Efficient Clustering with Limited Distance Information", 
    "arxiv-id": "1009.5168v2", 
    "author": "Yu Xia", 
    "publish": "2010-09-27T06:29:35Z", 
    "summary": "Given a point set S and an unknown metric d on S, we study the problem of\nefficiently partitioning S into k clusters while querying few distances between\nthe points. In our model we assume that we have access to one versus all\nqueries that given a point s in S return the distances between s and all other\npoints. We show that given a natural assumption about the structure of the\ninstance, we can efficiently find an accurate clustering using only O(k)\ndistance queries. Our algorithm uses an active selection strategy to choose a\nsmall set of points that we call landmarks, and considers only the distances\nbetween landmarks and other points to produce a clustering. We use our\nalgorithm to cluster proteins by sequence similarity. This setting nicely fits\nour model because we can use a fast sequence database search program to query a\nsequence against an entire dataset. We conduct an empirical study that shows\nthat even though we query a small fraction of the distances between the points,\nwe produce clusterings that are close to a desired clustering given by manual\nclassification."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5227v1", 
    "other_authors": "Evmorfia N. Argyriou, Michael A. Bekos, Antonios Symvonis", 
    "title": "The Straight-Line RAC Drawing Problem is NP-Hard", 
    "arxiv-id": "1009.5227v1", 
    "author": "Antonios Symvonis", 
    "publish": "2010-09-27T11:33:39Z", 
    "summary": "Recent cognitive experiments have shown that the negative impact of an edge\ncrossing on the human understanding of a graph drawing, tends to be eliminated\nin the case where the crossing angles are greater than 70 degrees. This\nmotivated the study of RAC drawings, in which every pair of crossing edges\nintersects at right angle. In this work, we demonstrate a class of graphs with\nunique RAC combinatorial embedding and we employ members of this class in order\nto show that it is NP-hard to decide whether a graph admits a straight-line RAC\ndrawing."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5538v1", 
    "other_authors": "Amr Elmasry, Arash Farzan, John Iacono", 
    "title": "Priority Queues with Multiple Time Fingers", 
    "arxiv-id": "1009.5538v1", 
    "author": "John Iacono", 
    "publish": "2010-09-28T11:48:57Z", 
    "summary": "A priority queue is presented that supports the operations insert and\nfind-min in worst-case constant time, and delete and delete-min on element x in\nworst-case O(lg(min{w_x, q_x}+2)) time, where w_x (respectively q_x) is the\nnumber of elements inserted after x (respectively before x) and are still\npresent at the time of the deletion of x. Our priority queue then has both the\nworking-set and the queueish properties, and more strongly it satisfies these\nproperties in the worst-case sense. We also define a new distribution-sensitive\nproperty---the time-finger property, which encapsulates and generalizes both\nthe working-set and queueish properties, and present a priority queue that\nsatisfies this property.\n  In addition, we prove a strong implication that the working-set property is\nequivalent to the unified bound (which is the minimum per operation among the\nstatic finger, static optimality, and the working-set bounds). This latter\nresult is of tremendous interest by itself as it had gone unnoticed since the\nintroduction of such bounds by Sleater and Tarjan [JACM 1985]. Accordingly, our\npriority queue satisfies other distribution-sensitive properties as the static\nfinger, static optimality, and the unified bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5787v1", 
    "other_authors": "Martin R. Ehmsen, Jens S. Kohrt, Kim S. Larsen", 
    "title": "List Factoring and Relative Worst Order Analysis", 
    "arxiv-id": "1009.5787v1", 
    "author": "Kim S. Larsen", 
    "publish": "2010-09-29T07:17:38Z", 
    "summary": "Relative worst order analysis is a supplement or alternative to competitive\nanalysis which has been shown to give results more in accordance with observed\nbehavior of online algorithms for a range of different online problems. The\ncontribution of this paper is twofold. First, it adds the static list accessing\nproblem to the collection of online problems where relative worst order\nanalysis gives better results. Second, and maybe more interesting, it adds the\nnon-trivial supplementary proof technique of list factoring to the theoretical\ntoolbox for relative worst order analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5791v1", 
    "other_authors": "Yoram Bachrach, Ely Porat", 
    "title": "Fast Pseudo-Random Fingerprints", 
    "arxiv-id": "1009.5791v1", 
    "author": "Ely Porat", 
    "publish": "2010-09-29T07:32:27Z", 
    "summary": "We propose a method to exponentially speed up computation of various\nfingerprints, such as the ones used to compute similarity and rarity in massive\ndata sets. Rather then maintaining the full stream of $b$ items of a universe\n$[u]$, such methods only maintain a concise fingerprint of the stream, and\nperform computations using the fingerprints. The computations are done\napproximately, and the required fingerprint size $k$ depends on the desired\naccuracy $\\epsilon$ and confidence $\\delta$. Our technique maintains a single\nbit per hash function, rather than a single integer, thus requiring a\nfingerprint of length $k = O(\\frac{\\ln \\frac{1}{\\delta}}{\\epsilon^2})$ bits,\nrather than $O(\\log u \\cdot \\frac{\\ln \\frac{1}{\\delta}}{\\epsilon^2})$ bits\nrequired by previous approaches. The main advantage of the fingerprints we\npropose is that rather than computing the fingerprint of a stream of $b$ items\nin time of $O(b \\cdot k)$, we can compute it in time $O(b \\log k)$. Thus this\nallows an exponential speedup for the fingerprint construction, or\nalternatively allows achieving a much higher accuracy while preserving\ncomputation time. Our methods rely on a specific family of pseudo-random hashes\nfor which we can quickly locate hashes resulting in small values."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1009.5863v1", 
    "other_authors": "J\u00e9r\u00e9my Barbay, Johannes Fischer", 
    "title": "LRM-Trees: Compressed Indices, Adaptive Sorting, and Compressed   Permutations", 
    "arxiv-id": "1009.5863v1", 
    "author": "Johannes Fischer", 
    "publish": "2010-09-29T12:28:52Z", 
    "summary": "LRM-Trees are an elegant way to partition a sequence of values into sorted\nconsecutive blocks, and to express the relative position of the first element\nof each block within a previous block. They were used to encode ordinal trees\nand to index integer arrays in order to support range minimum queries on them.\nWe describe how they yield many other convenient results in a variety of areas,\nfrom data structures to algorithms: some compressed succinct indices for range\nminimum queries; a new adaptive sorting algorithm; and a compressed succinct\ndata structure for permutations supporting direct and indirect application in\ntime all the shortest as the permutation is compressible."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0141v1", 
    "other_authors": "Mithun Das Gupta, Sanjeev Kumar, Jing Xiao", 
    "title": "L1 Projections with Box Constraints", 
    "arxiv-id": "1010.0141v1", 
    "author": "Jing Xiao", 
    "publish": "2010-09-30T18:38:11Z", 
    "summary": "We study the L1 minimization problem with additional box constraints. We\nmotivate the problem with two different views of optimality considerations. We\nlook into imposing such constraints in projected gradient techniques and\npropose a worst case linear time algorithm to perform such projections. We\ndemonstrate the merits and effectiveness of our algorithms on synthetic as well\nas real experiments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0157v1", 
    "other_authors": "Gerald Paul", 
    "title": "Comparative Performance of Tabu Search and Simulated Annealing   Heuristics for the Quadratic Assignment Problem", 
    "arxiv-id": "1010.0157v1", 
    "author": "Gerald Paul", 
    "publish": "2010-10-01T13:29:22Z", 
    "summary": "For almost two decades the question of whether tabu search (TS) or simulated\nannealing (SA) performs better for the quadratic assignment problem has been\nunresolved. To answer this question satisfactorily, we compare performance at\nvarious values of targeted solution quality, running each heuristic at its\noptimal number of iterations for each target. We find that for a number of\nvaried problem instances, SA performs better for higher quality targets while\nTS performs better for lower quality targets."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0401v1", 
    "other_authors": "Srivathsan Srinivasagopalan, Costas Busch, S. S. Iyengar", 
    "title": "Oblivious Buy-at-Bulk in Planar Graphs", 
    "arxiv-id": "1010.0401v1", 
    "author": "S. S. Iyengar", 
    "publish": "2010-10-03T13:43:00Z", 
    "summary": "In the oblivious buy-at-bulk network design problem in a graph, the task is\nto compute a fixed set of paths for every pair of source-destinations in the\ngraph, such that any set of demands can be routed along these paths. The\ndemands could be aggregated at intermediate edges where the fusion-cost is\nspecified by a canonical (non-negative concave) function $f$. We give a novel\nalgorithm for planar graphs which is oblivious with respect to the demands, and\nis also oblivious with respect to the fusion function $f$. The algorithm is\ndeterministic and computes the fixed set of paths in polynomial time, and\nguarantees a $O(\\log n)$ approximation ratio for any set of demands and any\ncanonical fusion function $f$, where $n$ is the number of nodes. The algorithm\nis asymptotically optimal, since it is known that this problem cannot be\napproximated with better than $\\Omega(\\log n)$ ratio. To our knowledge, this is\nthe first tight analysis for planar graphs, and improves the approximation\nratio by a factor of $\\log n$ with respect to previously known results."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0406v1", 
    "other_authors": "Uriel Feige, Shlomo Jozeph", 
    "title": "Oblivious Algorithms for the Maximum Directed Cut Problem", 
    "arxiv-id": "1010.0406v1", 
    "author": "Shlomo Jozeph", 
    "publish": "2010-10-03T14:05:40Z", 
    "summary": "This paper introduces a special family of randomized algorithms for Max DICUT\nthat we call oblivious algorithms. Let the bias of a vertex be the ratio\nbetween the total weight of its outgoing edges and the total weight of all its\nedges. An oblivious algorithm selects at random in which side of the cut to\nplace a vertex v, with probability that only depends on the bias of v,\nindependently of other vertices. The reader may observe that the algorithm that\nignores the bias and chooses each side with probability 1/2 has an\napproximation ratio of 1/4, whereas no oblivious algorithm can have an\napproximation ratio better than 1/2 (with an even directed cycle serving as a\nnegative example). We attempt to characterize the best approximation ratio\nachievable by oblivious algorithms, and present results that are nearly tight.\nThe paper also discusses natural extensions of the notion of oblivious\nalgorithms, and extensions to the more general problem of Max 2-AND."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.0809v1", 
    "other_authors": "Robert Geisberger", 
    "title": "Engineering Time-dependent One-To-All Computation", 
    "arxiv-id": "1010.0809v1", 
    "author": "Robert Geisberger", 
    "publish": "2010-10-05T09:20:50Z", 
    "summary": "Very recently a new algorithm to the nonnegative single-source shortest path\nproblem on road networks has been discovered. It is very cache-efficient, but\nonly on static road networks. We show how to augment it to the time-dependent\nscenario. The advantage if the new approach is that it settles nodes, even for\na profile query, by scanning all downward edges. We improve the scanning of the\ndownward edges with techniques developed for time-dependent many-to-many\ncomputations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.1047v1", 
    "other_authors": "Anand Louis", 
    "title": "Cut-Matching Games on Directed Graphs", 
    "arxiv-id": "1010.1047v1", 
    "author": "Anand Louis", 
    "publish": "2010-10-06T02:14:09Z", 
    "summary": "We give O(log^2 n)-approximation algorithm based on the cut-matching\nframework of [10, 13, 14] for computing the sparsest cut on directed graphs.\nOur algorithm uses only O(log^2 n) single commodity max-flow computations and\nthus breaks the multicommodity-flow barrier for computing the sparsest cut on\ndirected graphs"
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.2833v1", 
    "other_authors": "Weiya Yue, John Franco, Weiwei Cao", 
    "title": "Improved Complexity Bound of Vertex Cover for Low degree Graph", 
    "arxiv-id": "1010.2833v1", 
    "author": "Weiwei Cao", 
    "publish": "2010-10-14T06:16:46Z", 
    "summary": "In this paper, we use a new method to decrease the parameterized complexity\nbound for finding the minimum vertex cover of connected max-degree-3 undirected\ngraphs. The key operation of this method is reduction of the size of a\nparticular subset of edges which we introduce in this paper and is called as\n\"real-cycle\" subset. Using \"real-cycle\" reductions alone we compute a\ncomplexity bound $O(1.15855^k)$ where $k$ is size of the optimal vertex cover.\nCombined with other techniques, the complexity bound can be further improved to\nbe $O(1.1504^k)$. This is currently the best complexity bound."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.2885v1", 
    "other_authors": "Valentin Polishchuk, Mikko Sysikaski", 
    "title": "Improved approximations for robust mincut and shortest path", 
    "arxiv-id": "1010.2885v1", 
    "author": "Mikko Sysikaski", 
    "publish": "2010-10-14T11:52:33Z", 
    "summary": "In two-stage robust optimization the solution to a problem is built in two\nstages: In the first stage a partial, not necessarily feasible, solution is\nexhibited. Then the adversary chooses the \"worst\" scenario from a predefined\nset of scenarios. In the second stage, the first-stage solution is extended to\nbecome feasible for the chosen scenario. The costs at the second stage are\nlarger than at the first one, and the objective is to minimize the total cost\npaid in the two stages.\n  We give a 2-approximation algorithm for the robust mincut problem and a\n({\\gamma}+2)-approximation for the robust shortest path problem, where {\\gamma}\nis the approximation ratio for the Steiner tree. This improves the factors\n(1+\\sqrt2) and 2({\\gamma}+2) from [Golovin, Goyal and Ravi. Pay today for a\nrainy day: Improved approximation algorithms for demand-robust min-cut and\nshortest path problems. STACS 2006]. In addition, our solution for robust\nshortest path is simpler and more efficient than the earlier ones; this is\nachieved by a more direct algorithm and analysis, not using some of the\nstandard demand-robust optimization techniques."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_6", 
    "link": "http://arxiv.org/pdf/1010.3633v3", 
    "other_authors": "D\u00e1niel Marx, Igor Razgon", 
    "title": "Fixed-parameter tractability of multicut parameterized by the size of   the cutset", 
    "arxiv-id": "1010.3633v3", 
    "author": "Igor Razgon", 
    "publish": "2010-10-18T15:42:32Z", 
    "summary": "Given an undirected graph $G$, a collection $\\{(s_1,t_1),..., (s_k,t_k)\\}$ of\npairs of vertices, and an integer $p$, the Edge Multicut problem ask if there\nis a set $S$ of at most $p$ edges such that the removal of $S$ disconnects\nevery $s_i$ from the corresponding $t_i$. Vertex Multicut is the analogous\nproblem where $S$ is a set of at most $p$ vertices. Our main result is that\nboth problems can be solved in time $2^{O(p^3)}... n^{O(1)}$, i.e.,\nfixed-parameter tractable parameterized by the size $p$ of the cutset in the\nsolution. By contrast, it is unlikely that an algorithm with running time of\nthe form $f(p)... n^{O(1)}$ exists for the directed version of the problem, as\nwe show it to be W[1]-hard parameterized by the size of the cutset."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2010.08.010", 
    "link": "http://arxiv.org/pdf/1010.3685v1", 
    "other_authors": "Vincent Blondel, St\u00e9phane Gaubert, Natacha Portier", 
    "title": "The set of realizations of a max-plus linear sequence is semi-polyhedral", 
    "arxiv-id": "1010.3685v1", 
    "author": "Natacha Portier", 
    "publish": "2010-10-18T19:09:11Z", 
    "summary": "We show that the set of realizations of a given dimension of a max-plus\nlinear sequence is a finite union of polyhedral sets, which can be computed\nfrom any realization of the sequence. This yields an (expensive) algorithm to\nsolve the max-plus minimal realization problem. These results are derived from\ngeneral facts on rational expressions over idempotent commutative semirings: we\nshow more generally that the set of values of the coefficients of a commutative\nrational expression in one letter that yield a given max-plus linear sequence\nis a semi-algebraic set in the max-plus sense. In particular, it is a finite\nunion of polyhedral sets."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.jcss.2010.08.010", 
    "link": "http://arxiv.org/pdf/1010.4108v1", 
    "other_authors": "Lorenzo Orecchia, Nisheeth K. Vishnoi", 
    "title": "Towards an SDP-based Approach to Spectral Methods: A Nearly-Linear-Time   Algorithm for Graph Partitioning and Decomposition", 
    "arxiv-id": "1010.4108v1", 
    "author": "Nisheeth K. Vishnoi", 
    "publish": "2010-10-20T06:37:28Z", 
    "summary": "In this paper, we consider the following graph partitioning problem: The\ninput is an undirected graph $G=(V,E),$ a balance parameter $b \\in (0,1/2]$ and\na target conductance value $\\gamma \\in (0,1).$ The output is a cut which, if\nnon-empty, is of conductance at most $O(f),$ for some function $f(G, \\gamma),$\nand which is either balanced or well correlated with all cuts of conductance at\nmost $\\gamma.$ Spielman and Teng gave an $\\tilde{O}(|E|/\\gamma^{2})$-time\nalgorithm for $f= \\sqrt{\\gamma \\log^{3}|V|}$ and used it to decompose graphs\ninto a collection of near-expanders. We present a new spectral algorithm for\nthis problem which runs in time $\\tilde{O}(|E|/\\gamma)$ for $f=\\sqrt{\\gamma}.$\nOur result yields the first nearly-linear time algorithm for the classic\nBalanced Separator problem that achieves the asymptotically optimal\napproximation guarantee for spectral methods. Our method has the advantage of\nbeing conceptually simple and relies on a primal-dual semidefinite-programming\nSDP approach. We first consider a natural SDP relaxation for the Balanced\nSeparator problem. While it is easy to obtain from this SDP a certificate of\nthe fact that the graph has no balanced cut of conductance less than $\\gamma,$\nsomewhat surprisingly, we can obtain a certificate for the stronger correlation\ncondition. This is achieved via a novel separation oracle for our SDP and by\nappealing to Arora and Kale's framework to bound the running time. Our result\ncontains technical ingredients that may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.4110v2", 
    "other_authors": "Hongyang Sun, Yuxiong He, Wen-Jing Hsu, Rui Fan", 
    "title": "Energy-Efficient Multiprocessor Scheduling for Flow Time and Makespan", 
    "arxiv-id": "1010.4110v2", 
    "author": "Rui Fan", 
    "publish": "2010-10-20T06:48:46Z", 
    "summary": "We consider energy-efficient scheduling on multiprocessors, where the speed\nof each processor can be individually scaled, and a processor consumes power\n$s^{\\alpha}$ when running at speed $s$, for $\\alpha>1$. A scheduling algorithm\nneeds to decide at any time both processor allocations and processor speeds for\na set of parallel jobs with time-varying parallelism. The objective is to\nminimize the sum of the total energy consumption and certain performance\nmetric, which in this paper includes total flow time and makespan. For both\nobjectives, we present instantaneous parallelism clairvoyant (IP-clairvoyant)\nalgorithms that are aware of the instantaneous parallelism of the jobs at any\ntime but not their future characteristics, such as remaining parallelism and\nwork. For total flow time plus energy, we present an $O(1)$-competitive\nalgorithm, which significantly improves upon the best known non-clairvoyant\nalgorithm and is the first constant competitive result on multiprocessor speed\nscaling for parallel jobs. In the case of makespan plus energy, which is\nconsidered for the first time in the literature, we present an\n$O(\\ln^{1-1/\\alpha}P)$-competitive algorithm, where $P$ is the total number of\nprocessors. We show that this algorithm is asymptotically optimal by providing\na matching lower bound. In addition, we also study non-clairvoyant scheduling\nfor total flow time plus energy, and present an algorithm that achieves $O(\\ln\nP)$-competitive for jobs with arbitrary release time and\n$O(\\ln^{1/\\alpha}P)$-competitive for jobs with identical release time. Finally,\nwe prove an $\\Omega(\\ln^{1/\\alpha}P)$ lower bound on the competitive ratio of\nany non-clairvoyant algorithm, matching the upper bound of our algorithm for\njobs with identical release time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.4925v1", 
    "other_authors": "Victor Chen, Madhu Sudan, Ning Xie", 
    "title": "Property Testing via Set-Theoretic Operations", 
    "arxiv-id": "1010.4925v1", 
    "author": "Ning Xie", 
    "publish": "2010-10-24T04:02:45Z", 
    "summary": "Given two testable properties $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$, under\nwhat conditions are the union, intersection or set-difference of these two\nproperties also testable? We initiate a systematic study of these basic\nset-theoretic operations in the context of property testing. As an application,\nwe give a conceptually different proof that linearity is testable, albeit with\nmuch worse query complexity. Furthermore, for the problem of testing\ndisjunction of linear functions, which was previously known to be one-sided\ntestable with a super-polynomial query complexity, we give an improved analysis\nand show it has query complexity $O(1/\\eps^2)$, where $\\eps$ is the distance\nparameter."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5197v3", 
    "other_authors": "Nicolas Bousquet, Jean Daligault, St\u00e9phan Thomass\u00e9", 
    "title": "Multicut is FPT", 
    "arxiv-id": "1010.5197v3", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2010-10-25T17:18:00Z", 
    "summary": "Let $G=(V,E)$ be a graph on $n$ vertices and $R$ be a set of pairs of\nvertices in $V$ called \\emph{requests}. A \\emph{multicut} is a subset $F$ of\n$E$ such that every request $xy$ of $R$ is cut by $F$, \\i.e. every $xy$-path of\n$G$ intersects $F$. We show that there exists an $O(f(k)n^c)$ algorithm which\ndecides if there exists a multicut of size at most $k$. In other words, the\n\\M{} problem parameterized by the solution size $k$ is Fixed-Parameter\nTractable. The proof extends to vertex multicuts."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5717v1", 
    "other_authors": "Dominik Scheder", 
    "title": "PPZ For More Than Two Truth Values - An Algorithm for Constraint   Satisfaction Problems", 
    "arxiv-id": "1010.5717v1", 
    "author": "Dominik Scheder", 
    "publish": "2010-10-27T15:38:24Z", 
    "summary": "We analyze the so-called ppz algorithm for (d,k)-CSP problems for general\nvalues of d (number of values a variable can take) and k (number of literals\nper constraint). To analyze its success probability, we prove a correlation\ninequality for submodular functions."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.07.007", 
    "link": "http://arxiv.org/pdf/1010.5890v1", 
    "other_authors": "Andrzej Kapanowski", 
    "title": "Python for education: the exact cover problem", 
    "arxiv-id": "1010.5890v1", 
    "author": "Andrzej Kapanowski", 
    "publish": "2010-10-28T08:53:26Z", 
    "summary": "Python implementation of Algorithm X by Knuth is presented. Algorithm X finds\nall solutions to the exact cover problem. The exemplary results for\npentominoes, Latin squares and Sudoku are given."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1010.5937v1", 
    "other_authors": "Markus Geyer, Michael Kaufmann, Tamara Mchedlidze, Antonios Symvonis", 
    "title": "Upward Point-Set Embeddability", 
    "arxiv-id": "1010.5937v1", 
    "author": "Antonios Symvonis", 
    "publish": "2010-10-28T12:19:53Z", 
    "summary": "We study the problem of Upward Point-Set Embeddability, that is the problem\nof deciding whether a given upward planar digraph $D$ has an upward planar\nembedding into a point set $S$. We show that any switch tree admits an upward\nplanar straight-line embedding into any convex point set. For the class of\n$k$-switch trees, that is a generalization of switch trees (according to this\ndefinition a switch tree is a $1$-switch tree), we show that not every\n$k$-switch tree admits an upward planar straight-line embedding into any convex\npoint set, for any $k \\geq 2$. Finally we show that the problem of Upward\nPoint-Set Embeddability is NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1010.5974v1", 
    "other_authors": "Paul Bonsma, Daniel Lokshtanov", 
    "title": "Feedback Vertex Set in Mixed Graphs", 
    "arxiv-id": "1010.5974v1", 
    "author": "Daniel Lokshtanov", 
    "publish": "2010-10-28T14:04:50Z", 
    "summary": "A mixed graph is a graph with both directed and undirected edges. We present\nan algorithm for deciding whether a given mixed graph on $n$ vertices contains\na feedback vertex set (FVS) of size at most $k$, in time $2^{O(k)}k! O(n^4)$.\nThis is the first fixed parameter tractable algorithm for FVS that applies to\nboth directed and undirected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.0108v11", 
    "other_authors": "Nir Ailon", 
    "title": "An Active Learning Algorithm for Ranking from Pairwise Preferences with   an Almost Optimal Query Complexity", 
    "arxiv-id": "1011.0108v11", 
    "author": "Nir Ailon", 
    "publish": "2010-10-30T21:47:19Z", 
    "summary": "We study the problem of learning to rank from pairwise preferences, and solve\na long-standing open problem that has led to development of many heuristics but\nno provable results for our particular problem. Given a set $V$ of $n$\nelements, we wish to linearly order them given pairwise preference labels. A\npairwise preference label is obtained as a response, typically from a human, to\nthe question \"which if preferred, u or v?$ for two elements $u,v\\in V$. We\nassume possible non-transitivity paradoxes which may arise naturally due to\nhuman mistakes or irrationality. The goal is to linearly order the elements\nfrom the most preferred to the least preferred, while disagreeing with as few\npairwise preference labels as possible. Our performance is measured by two\nparameters: The loss and the query complexity (number of pairwise preference\nlabels we obtain). This is a typical learning problem, with the exception that\nthe space from which the pairwise preferences is drawn is finite, consisting of\n${n\\choose 2}$ possibilities only. We present an active learning algorithm for\nthis problem, with query bounds significantly beating general (non active)\nbounds for the same error guarantee, while almost achieving the information\ntheoretical lower bound. Our main construct is a decomposition of the input\ns.t. (i) each block incurs high loss at optimum, and (ii) the optimal solution\nrespecting the decomposition is not much worse than the true opt. The\ndecomposition is done by adapting a recent result by Kenyon and Schudy for a\nrelated combinatorial optimization problem to the query efficient setting. We\nthus settle an open problem posed by learning-to-rank theoreticians and\npractitioners: What is a provably correct way to sample preference labels? To\nfurther show the power and practicality of our solution, we show how to use it\nin concert with an SVM relaxation."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.0531v1", 
    "other_authors": "Andrej Bogdanov, Fan Li", 
    "title": "A better tester for bipartiteness?", 
    "arxiv-id": "1011.0531v1", 
    "author": "Fan Li", 
    "publish": "2010-11-02T07:48:15Z", 
    "summary": "Alon and Krivelevich (SIAM J. Discrete Math. 15(2): 211-227 (2002)) show that\nif a graph is {\\epsilon}-far from bipartite, then the subgraph induced by a\nrandom subset of O(1/{\\epsilon}) vertices is bipartite with high probability.\nWe conjecture that the induced subgraph is {\\Omega}~({\\epsilon})-far from\nbipartite with high probability. Gonen and Ron (RANDOM 2007) proved this\nconjecture in the case when the degrees of all vertices are at most\nO({\\epsilon}n). We give a more general proof that works for any d-regular (or\nalmost d-regular) graph for arbitrary degree d. Assuming this conjecture, we\nprove that bipartiteness is testable with one-sided error in time\nO(1/{\\epsilon}^c), where c is a constant strictly smaller than two, improving\nupon the tester of Alon and Krivelevich. As it is known that non-adaptive\ntesters for bipartiteness require {\\Omega}(1/{\\epsilon}^2) queries (Bogdanov\nand Trevisan, CCC 2004), our result shows, assuming the conjecture, that\nadaptivity helps in testing bipartiteness."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1168v2", 
    "other_authors": "Ola Svensson", 
    "title": "Santa Claus Schedules Jobs on Unrelated Machines", 
    "arxiv-id": "1011.1168v2", 
    "author": "Ola Svensson", 
    "publish": "2010-11-04T14:22:11Z", 
    "summary": "One of the classic results in scheduling theory is the 2-approximation\nalgorithm by Lenstra, Shmoys, and Tardos for the problem of scheduling jobs to\nminimize makespan on unrelated machines, i.e., job j requires time p_{ij} if\nprocessed on machine i. More than two decades after its introduction it is\nstill the algorithm of choice even in the restricted model where processing\ntimes are of the form p_{ij} in {p_j, \\infty}. This problem, also known as the\nrestricted assignment problem, is NP-hard to approximate within a factor less\nthan 1.5 which is also the best known lower bound for the general version.\n  Our main result is a polynomial time algorithm that estimates the optimal\nmakespan of the restricted assignment problem within a factor 33/17 + \\epsilon\n\\approx 1.9412 + \\epsilon, where \\epsilon > 0 is an arbitrarily small constant.\nThe result is obtained by upper bounding the integrality gap of a certain\nstrong linear program, known as configuration LP, that was previously\nsuccessfully used for the related Santa Claus problem. Similar to the strongest\nanalysis for that problem our proof is based on a local search algorithm that\nwill eventually find a schedule of the mentioned approximation guarantee, but\nis not known to converge in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1202v1", 
    "other_authors": "Alexandru Popa, Prudence W. H. Wong, Fencol C. C. Yung", 
    "title": "Hardness and Approximation of The Asynchronous Border Minimization   Problem", 
    "arxiv-id": "1011.1202v1", 
    "author": "Fencol C. C. Yung", 
    "publish": "2010-11-04T16:25:26Z", 
    "summary": "We study a combinatorial problem arising from microarrays synthesis. The\nsynthesis is done by a light-directed chemical process. The objective is to\nminimize unintended illumination that may contaminate the quality of\nexperiments. Unintended illumination is measured by a notion called border\nlength and the problem is called Border Minimization Problem (BMP). The\nobjective of the BMP is to place a set of probe sequences in the array and find\nan embedding (deposition of nucleotides/residues to the array cells) such that\nthe sum of border length is minimized. A variant of the problem, called P-BMP,\nis that the placement is given and the concern is simply to find the embedding.\nApproximation algorithms have been previously proposed for the problem but it\nis unknown whether the problem is NP-hard or not. In this paper, we give a\nthorough study of different variations of BMP by giving NP-hardness proofs and\nimproved approximation algorithms. We show that P-BMP, 1D-BMP, and BMP are all\nNP-hard. Contrast with the previous result that 1D-P-BMP is polynomial time\nsolvable, the interesting implications include (i) the array dimension (1D or\n2D) differentiates the complexity of P-BMP; (ii) for 1D array, whether\nplacement is given differentiates the complexity of BMP; (iii) BMP is NP-hard\nregardless of the dimension of the array. Another contribution of the paper is\nimproving the approximation for BMP from $O(n^{1/2} \\log^2 n)$ to $O(n^{1/4}\n\\log^2 n)$, where $n$ is the total number of sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1337v1", 
    "other_authors": "Peter Becker", 
    "title": "Optimal Binary Search Trees with Near Minimal Height", 
    "arxiv-id": "1011.1337v1", 
    "author": "Peter Becker", 
    "publish": "2010-11-05T08:16:42Z", 
    "summary": "Suppose we have n keys, n access probabilities for the keys, and n+1 access\nprobabilities for the gaps between the keys. Let h_min(n) be the minimal height\nof a binary search tree for n keys. We consider the problem to construct an\noptimal binary search tree with near minimal height, i.e.\\ with height h <=\nh_min(n) + Delta for some fixed Delta. It is shown, that for any fixed Delta\noptimal binary search trees with near minimal height can be constructed in time\nO(n^2). This is as fast as in the unrestricted case.\n  So far, the best known algorithms for the construction of height-restricted\noptimal binary search trees have running time O(L n^2), whereby L is the\nmaximal permitted height. Compared to these algorithms our algorithm is at\nleast faster by a factor of log n, because L is lower bounded by log n."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1708v2", 
    "other_authors": "Jesper Jansson, Kunihiko Sadakane, Wing-Kin Sung", 
    "title": "CRAM: Compressed Random Access Memory", 
    "arxiv-id": "1011.1708v2", 
    "author": "Wing-Kin Sung", 
    "publish": "2010-11-08T04:10:42Z", 
    "summary": "We present a new data structure called the \\emph{Compressed Random Access\nMemory} (CRAM) that can store a dynamic string $T$ of characters, e.g.,\nrepresenting the memory of a computer, in compressed form while achieving\nasymptotically almost-optimal bounds (in terms of empirical entropy) on the\ncompression ratio. It allows short substrings of $T$ to be decompressed and\nretrieved efficiently and, significantly, characters at arbitrary positions of\n$T$ to be modified quickly during execution \\emph{without decompressing the\nentire string}. This can be regarded as a new type of data compression that can\nupdate a compressed file directly. Moreover, at the cost of slightly increasing\nthe time spent per operation, the CRAM can be extended to also support\ninsertions and deletions. Our key observation that the empirical entropy of a\nstring does not change much after a small change to the string, as well as our\nsimple yet efficient method for maintaining an array of variable-length blocks\nunder length modifications, may be useful for many other applications as well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.1827v1", 
    "other_authors": "Martin Grohe, Ken-ichi Kawarabayashi, D\u00e1niel Marx, Paul Wollan", 
    "title": "Finding topological subgraphs is fixed-parameter tractable", 
    "arxiv-id": "1011.1827v1", 
    "author": "Paul Wollan", 
    "publish": "2010-11-08T15:23:20Z", 
    "summary": "We show that for every fixed undirected graph $H$, there is a $O(|V(G)|^3)$\ntime algorithm that tests, given a graph $G$, if $G$ contains $H$ as a\ntopological subgraph (that is, a subdivision of $H$ is subgraph of $G$). This\nshows that topological subgraph testing is fixed-parameter tractable, resolving\na longstanding open question of Downey and Fellows from 1992. As a corollary,\nfor every $H$ we obtain an $O(|V(G)|^3)$ time algorithm that tests if there is\nan immersion of $H$ into a given graph $G$. This answers another open question\nraised by Downey and Fellows in 1992."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2187v1", 
    "other_authors": "Kyle Fox, Benjamin Moseley", 
    "title": "Online Scheduling on Identical Machines using SRPT", 
    "arxiv-id": "1011.2187v1", 
    "author": "Benjamin Moseley", 
    "publish": "2010-11-09T20:18:56Z", 
    "summary": "Due to its optimality on a single machine for the problem of minimizing\naverage flow time, Shortest-Remaining-Processing-Time (\\srpt) appears to be the\nmost natural algorithm to consider for the problem of minimizing average flow\ntime on multiple identical machines. It is known that $\\srpt$ achieves the best\npossible competitive ratio on multiple machines up to a constant factor. Using\nresource augmentation, $\\srpt$ is known to achieve total flow time at most that\nof the optimal solution when given machines of speed $2- \\frac{1}{m}$. Further,\nit is known that $\\srpt$'s competitive ratio improves as the speed increases;\n$\\srpt$ is $s$-speed $\\frac{1}{s}$-competitive when $s \\geq 2- \\frac{1}{m}$.\n  However, a gap has persisted in our understanding of $\\srpt$. Before this\nwork, the performance of $\\srpt$ was not known when $\\srpt$ is given\n$(1+\\eps)$-speed when $0 < \\eps < 1-\\frac{1}{m}$, even though it has been\nthought that $\\srpt$ is $(1+\\eps)$-speed $O(1)$-competitive for over a decade.\nResolving this question was suggested in Open Problem 2.9 from the survey\n\"Online Scheduling\" by Pruhs, Sgall, and Torng \\cite{PruhsST}, and we answer\nthe question in this paper. We show that $\\srpt$ is \\emph{scalable} on $m$\nidentical machines. That is, we show $\\srpt$ is $(1+\\eps)$-speed\n$O(\\frac{1}{\\eps})$-competitive for $\\eps >0$. We complement this by showing\nthat $\\srpt$ is $(1+\\eps)$-speed $O(\\frac{1}{\\eps^2})$-competitive for the\nobjective of minimizing the $\\ell_k$-norms of flow time on $m$ identical\nmachines. Both of our results rely on new potential functions that capture the\nstructure of \\srpt. Our results, combined with previous work, show that $\\srpt$\nis the best possible online algorithm in essentially every aspect when\nmigration is permissible."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2249v1", 
    "other_authors": "Ankur Moitra, Ryan O'Donnell", 
    "title": "Pareto Optimal Solutions for Smoothed Analysts", 
    "arxiv-id": "1011.2249v1", 
    "author": "Ryan O'Donnell", 
    "publish": "2010-11-10T01:31:44Z", 
    "summary": "Consider an optimization problem with $n$ binary variables and $d+1$ linear\nobjective functions. Each valid solution $x \\in \\{0,1\\}^n$ gives rise to an\nobjective vector in $\\R^{d+1}$, and one often wants to enumerate the Pareto\noptima among them. In the worst case there may be exponentially many Pareto\noptima; however, it was recently shown that in (a generalization of) the\nsmoothed analysis framework, the expected number is polynomial in $n$.\nUnfortunately, the bound obtained had a rather bad dependence on $d$; roughly\n$n^{d^d}$. In this paper we show a significantly improved bound of $n^{2d}$.\n  Our proof is based on analyzing two algorithms. The first algorithm, on input\na Pareto optimal $x$, outputs a \"testimony\" containing clues about $x$'s\nobjective vector, $x$'s coordinates, and the region of space $B$ in which $x$'s\nobjective vector lies. The second algorithm can be regarded as a {\\em\nspeculative} execution of the first -- it can uniquely reconstruct $x$ from the\ntestimony's clues and just \\emph{some} of the probability space's outcomes. The\nremainder of the probability space's outcomes are just enough to bound the\nprobability that $x$'s objective vector falls into the region $B$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2480v1", 
    "other_authors": "Michael T. Goodrich", 
    "title": "Spin-the-bottle Sort and Annealing Sort: Oblivious Sorting via   Round-robin Random Comparisons", 
    "arxiv-id": "1011.2480v1", 
    "author": "Michael T. Goodrich", 
    "publish": "2010-11-10T20:22:42Z", 
    "summary": "We study sorting algorithms based on randomized round-robin comparisons.\nSpecifically, we study Spin-the-bottle sort, where comparisons are\nunrestricted, and Annealing sort, where comparisons are restricted to a\ndistance bounded by a \\emph{temperature} parameter. Both algorithms are simple,\nrandomized, data-oblivious sorting algorithms, which are useful in\nprivacy-preserving computations, but, as we show, Annealing sort is much more\nefficient. We show that there is an input permutation that causes\nSpin-the-bottle sort to require $\\Omega(n^2\\log n)$ expected time in order to\nsucceed, and that in $O(n^2\\log n)$ time this algorithm succeeds with high\nprobability for any input. We also show there is an implementation of Annealing\nsort that runs in $O(n\\log n)$ time and succeeds with very high probability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2571v1", 
    "other_authors": "Vladimir Braverman, Rafail Ostrovsky", 
    "title": "Recursive Sketching For Frequency Moments", 
    "arxiv-id": "1011.2571v1", 
    "author": "Rafail Ostrovsky", 
    "publish": "2010-11-11T06:07:18Z", 
    "summary": "In a ground-breaking paper, Indyk and Woodruff (STOC 05) showed how to\ncompute $F_k$ (for $k>2$) in space complexity $O(\\mbox{\\em poly-log}(n,m)\\cdot\nn^{1-\\frac2k})$, which is optimal up to (large) poly-logarithmic factors in $n$\nand $m$, where $m$ is the length of the stream and $n$ is the upper bound on\nthe number of distinct elements in a stream. The best known lower bound for\nlarge moments is $\\Omega(\\log(n)n^{1-\\frac2k})$. A follow-up work of\nBhuvanagiri, Ganguly, Kesh and Saha (SODA 2006) reduced the poly-logarithmic\nfactors of Indyk and Woodruff to $O(\\log^2(m)\\cdot (\\log n+ \\log m)\\cdot\nn^{1-{2\\over k}})$. Further reduction of poly-log factors has been an elusive\ngoal since 2006, when Indyk and Woodruff method seemed to hit a natural\n\"barrier.\" Using our simple recursive sketch, we provide a different yet simple\napproach to obtain a $O(\\log(m)\\log(nm)\\cdot (\\log\\log n)^4\\cdot n^{1-{2\\over\nk}})$ algorithm for constant $\\epsilon$ (our bound is, in fact, somewhat\nstronger, where the $(\\log\\log n)$ term can be replaced by any constant number\nof $\\log $ iterations instead of just two or three, thus approaching $log^*n$.\nOur bound also works for non-constant $\\epsilon$ (for details see the body of\nthe paper). Further, our algorithm requires only $4$-wise independence, in\ncontrast to existing methods that use pseudo-random generators for computing\nlarge frequency moments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2590v1", 
    "other_authors": "Vladimir Braverman, Rafail Ostrovsky, Yuval Rabani", 
    "title": "Rademacher Chaos, Random Eulerian Graphs and The Sparse   Johnson-Lindenstrauss Transform", 
    "arxiv-id": "1011.2590v1", 
    "author": "Yuval Rabani", 
    "publish": "2010-11-11T08:13:15Z", 
    "summary": "The celebrated dimension reduction lemma of Johnson and Lindenstrauss has\nnumerous computational and other applications. Due to its application in\npractice, speeding up the computation of a Johnson-Lindenstrauss style\ndimension reduction is an important question. Recently, Dasgupta, Kumar, and\nSarlos (STOC 2010) constructed such a transform that uses a sparse matrix. This\nis motivated by the desire to speed up the computation when applied to sparse\ninput vectors, a scenario that comes up in applications. The sparsity of their\nconstruction was further improved by Kane and Nelson (ArXiv 2010).\n  We improve the previous bound on the number of non-zero entries per column of\nKane and Nelson from $O(1/\\epsilon \\log(1/\\delta)\\log(k/\\delta))$ (where the\ntarget dimension is $k$, the distortion is $1\\pm \\epsilon$, and the failure\nprobability is $\\delta$) to $$ O\\left({1\\over\\epsilon}\n\\left({\\log(1/\\delta)\\log\\log\\log(1/\\delta) \\over\n\\log\\log(1/\\delta)}\\right)^2\\right). $$\n  We also improve the amount of randomness needed to generate the matrix. Our\nresults are obtained by connecting the moments of an order 2 Rademacher chaos\nto the combinatorial properties of random Eulerian multigraphs. Estimating the\nchance that a random multigraph is composed of a given number of node-disjoint\nEulerian components leads to a new tail bound on the chaos. Our estimates may\nbe of independent interest, and as this part of the argument is decoupled from\nthe analysis of the coefficients of the chaos, we believe that our methods can\nbe useful in the analysis of other chaoses."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-18381-2_23", 
    "link": "http://arxiv.org/pdf/1011.2843v2", 
    "other_authors": "Giuseppe F. Italiano, Piotr Sankowski", 
    "title": "Improved Minimum Cuts and Maximum Flows in Undirected Planar Graphs", 
    "arxiv-id": "1011.2843v2", 
    "author": "Piotr Sankowski", 
    "publish": "2010-11-12T08:45:01Z", 
    "summary": "In this paper we study minimum cut and maximum flow problems on planar\ngraphs, both in static and in dynamic settings. First, we present an algorithm\nthat given an undirected planar graph computes the minimum cut between any two\ngiven vertices in O(n log log n) time. Second, we show how to achieve the same\nO(n log log n) bound for the problem of computing maximum flows in undirected\nplanar graphs. To the best of our knowledge, these are the first algorithms for\nthose two problems that break the O(n log n) barrier, which has been standing\nfor more than 25 years. Third, we present a fully dynamic algorithm that is\nable to maintain information about minimum cuts and maximum flows in a plane\ngraph (i.e., a planar graph with a fixed embedding): our algorithm is able to\ninsert edges, delete edges and answer min-cut and max-flow queries between any\npair of vertices in O(n^(2/3) log^3 n) time per operation. This result is based\non a new dynamic shortest path algorithm for planar graphs which may be of\nindependent interest. We remark that this is the first known non-trivial\nalgorithm for min-cut and max-flow problems in a dynamic setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3441v2", 
    "other_authors": "Djamal Belazzougui", 
    "title": "Worst case efficient single and multiple string matching in the Word-RAM   model", 
    "arxiv-id": "1011.3441v2", 
    "author": "Djamal Belazzougui", 
    "publish": "2010-11-15T16:37:43Z", 
    "summary": "In this paper, we explore worst-case solutions for the problems of single and\nmultiple matching on strings in the word RAM model with word length w. In the\nfirst problem, we have to build a data structure based on a pattern p of length\nm over an alphabet of size sigma such that we can answer to the following\nquery: given a text T of length n, where each character is encoded using\nlog(sigma) bits return the positions of all the occurrences of p in T (in the\nfollowing we refer by occ to the number of reported occurrences). For the\nmulti-pattern matching problem we have a set S of d patterns of total length m\nand a query on a text T consists in finding all positions of all occurrences in\nT of the patterns in S. As each character of the text is encoded using log\nsigma bits and we can read w bits in constant time in the RAM model, we assume\nthat we can read up to (w/log sigma) consecutive characters of the text in one\ntime step. This implies that the fastest possible query time for both problems\nis O((n(log sigma/w)+occ). In this paper we present several different results\nfor both problems which come close to that best possible query time. We first\npresent two different linear space data structures for the first and second\nproblem: the first one answers to single pattern matching queries in time\nO(n(1/m+log sigma/w)+occ) while the second one answers to multiple pattern\nmatching queries to O(n((log d+log y+log log d)/y+log sigma/w)+occ) where y is\nthe length of the shortest pattern in the case of multiple pattern-matching. We\nthen show how a simple application of the four russian technique permits to get\ndata structures with query times independent of the length of the shortest\npattern (the length of the only pattern in case of single string matching) at\nthe expense of using more space."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3480v1", 
    "other_authors": "Travis Gagie, Juha K\u00e4rkk\u00e4inen", 
    "title": "Counting Colours in Compressed Strings", 
    "arxiv-id": "1011.3480v1", 
    "author": "Juha K\u00e4rkk\u00e4inen", 
    "publish": "2010-11-15T19:30:19Z", 
    "summary": "Suppose we are asked to preprocess a string \\(s [1..n]\\) such that later,\ngiven a substring's endpoints, we can quickly count how many distinct\ncharacters it contains. In this paper we give a data structure for this problem\nthat takes \\(n H_0 (s) + \\Oh{n} + \\oh{n H_0 (s)}\\) bits, where \\(H_0 (s)\\) is\nthe 0th-order empirical entropy of $s$, and answers queries in $\\Oh{\\log^{1 +\n\\epsilon} n}$ time for any constant \\(\\epsilon > 0\\). We also show how our data\nstructure can be made partially dynamic."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3491v2", 
    "other_authors": "Travis Gagie, Kalle Karhu, Juha K\u00e4rkk\u00e4inen, Veli M\u00e4kinen, Leena Salmela", 
    "title": "Pattern Kits", 
    "arxiv-id": "1011.3491v2", 
    "author": "Leena Salmela", 
    "publish": "2010-11-15T20:21:33Z", 
    "summary": "Suppose we have just performed searches in a self-index for two patterns $A$\nand $B$ and now we want to search for their concatenation \\A B); how can we\nbest make use of our previous computations? In this paper we consider this\nproblem and, more generally, how we can store a dynamic library of patterns\nthat we can easily manipulate in interesting ways. We give a space- and\ntime-efficient data structure for this problem that is compatible with many of\nthe best self-indexes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3701v2", 
    "other_authors": "Michael Dinitz, Robert Krauthgamer", 
    "title": "Directed Spanners via Flow-Based Linear Programs", 
    "arxiv-id": "1011.3701v2", 
    "author": "Robert Krauthgamer", 
    "publish": "2010-11-16T14:14:17Z", 
    "summary": "We examine directed spanners through flow-based linear programming\nrelaxations. We design an $\\~O(n^{2/3})$-approximation algorithm for the\ndirected $k$-spanner problem that works for all $k\\geq 1$, which is the first\nsublinear approximation for arbitrary edge-lengths. Even in the more restricted\nsetting of unit edge-lengths, our algorithm improves over the previous\n$\\~O(n^{1-1/k})$ approximation of Bhattacharyya et al. when $k\\ge 4$. For the\nspecial case of $k=3$ we design a different algorithm achieving an\n$\\~O(\\sqrt{n})$-approximation, improving the previous $\\~O(n^{2/3})$. Both of\nour algorithms easily extend to the fault-tolerant setting, which has recently\nattracted attention but not from an approximation viewpoint. We also prove a\nnearly matching integrality gap of $\\Omega(n^{\\frac13 - \\epsilon})$ for any\nconstant $\\epsilon > 0$.\n  A virtue of all our algorithms is that they are relatively simple.\nTechnically, we introduce a new yet natural flow-based relaxation, and show how\nto approximately solve it even when its size is not polynomial. The main\nchallenge is to design a rounding scheme that \"coordinates\" the choices of\nflow-paths between the many demand pairs while using few edges overall. We\nachieve this, roughly speaking, by randomization at the level of vertices."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3770v2", 
    "other_authors": "Anand Bhalgat, Deeparnab Chakrabarty, Sanjeev Khanna", 
    "title": "Optimal Lower Bounds for Universal and Differentially Private Steiner   Tree and TSP", 
    "arxiv-id": "1011.3770v2", 
    "author": "Sanjeev Khanna", 
    "publish": "2010-11-16T17:49:27Z", 
    "summary": "Given a metric space on n points, an {\\alpha}-approximate universal algorithm\nfor the Steiner tree problem outputs a distribution over rooted spanning trees\nsuch that for any subset X of vertices containing the root, the expected cost\nof the induced subtree is within an {\\alpha} factor of the optimal Steiner tree\ncost for X. An {\\alpha}-approximate differentially private algorithm for the\nSteiner tree problem takes as input a subset X of vertices, and outputs a tree\ndistribution that induces a solution within an {\\alpha} factor of the optimal\nas before, and satisfies the additional property that for any set X' that\ndiffers in a single vertex from X, the tree distributions for X and X' are\n\"close\" to each other. Universal and differentially private algorithms for TSP\nare defined similarly. An {\\alpha}-approximate universal algorithm for the\nSteiner tree problem or TSP is also an {\\alpha}-approximate differentially\nprivate algorithm. It is known that both problems admit O(logn)-approximate\nuniversal algorithms, and hence O(log n)-approximate differentially private\nalgorithms as well. We prove an {\\Omega}(logn) lower bound on the approximation\nratio achievable for the universal Steiner tree problem and the universal TSP,\nmatching the known upper bounds. Our lower bound for the Steiner tree problem\nholds even when the algorithm is allowed to output a more general solution of a\ndistribution on paths to the root."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.3944v2", 
    "other_authors": "V. F. Romanov", 
    "title": "Non-Orthodox Combinatorial Models Based on Discordant Structures", 
    "arxiv-id": "1011.3944v2", 
    "author": "V. F. Romanov", 
    "publish": "2010-11-17T11:19:31Z", 
    "summary": "This paper introduces a novel method for compact representation of sets of\nn-dimensional binary sequences in a form of compact triplets structures (CTS),\nsupposing both logic and arithmetic interpretations of data. Suitable\nillustration of CTS application is the unique graph-combinatorial model for the\nclassic intractable 3-Satisfiability problem and a polynomial algorithm for the\nmodel synthesis. The method used for Boolean formulas analysis and\nclassification by means of the model is defined as a bijective mapping\nprinciple for sets of components of discordant structures to a basic set. The\nstatistic computer-aided experiment showed efficiency of the algorithm in a\nlarge scale of problem dimension parameters, including those that make\nenumeration procedures of no use. The formulated principle expands resources of\nconstructive approach to investigation of intractable problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.4465v1", 
    "other_authors": "Gernot Veit Batz, Robert Geisberger, Dennis Luxen, Peter Sanders", 
    "title": "Compressed Transmission of Route Descriptions", 
    "arxiv-id": "1011.4465v1", 
    "author": "Peter Sanders", 
    "publish": "2010-11-19T16:36:30Z", 
    "summary": "We present two methods to compress the description of a route in a road\nnetwork, i.e., of a path in a directed graph. The first method represents a\npath by a sequence of via edges. The subpaths between the via edges have to be\nunique shortest paths. Instead of via edges also via nodes can be used, though\nthis requires some simple preprocessing. The second method uses contraction\nhierarchies to replace subpaths of the original path by shortcuts. The two\nmethods can be combined with each other. Also, we propose the application to\nmobile server based routing: We compute the route on a server which has access\nto the latest information about congestions for example. Then we transmit the\ncomputed route to the car using some mobile radio communication. There, we\napply the compression to save costs and transmission time. If the compression\nworks well, we can transmit routes even when the bandwidth is low. Although we\nhave not evaluated our ideas with realistic data yet, they are quite promising."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.5200v2", 
    "other_authors": "Mihai Patrascu, Mikkel Thorup", 
    "title": "The Power of Simple Tabulation Hashing", 
    "arxiv-id": "1011.5200v2", 
    "author": "Mikkel Thorup", 
    "publish": "2010-11-23T19:18:30Z", 
    "summary": "Randomized algorithms are often enjoyed for their simplicity, but the hash\nfunctions used to yield the desired theoretical guarantees are often neither\nsimple nor practical. Here we show that the simplest possible tabulation\nhashing provides unexpectedly strong guarantees.\n  The scheme itself dates back to Carter and Wegman (STOC'77). Keys are viewed\nas consisting of c characters. We initialize c tables T_1, ..., T_c mapping\ncharacters to random hash codes. A key x=(x_1, ..., x_q) is hashed to T_1[x_1]\nxor ... xor T_c[x_c].\n  While this scheme is not even 4-independent, we show that it provides many of\nthe guarantees that are normally obtained via higher independence, e.g.,\nChernoff-type concentration, min-wise hashing for estimating set intersection,\nand cuckoo hashing."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1011.6181v2", 
    "other_authors": "Raphael Yuster", 
    "title": "Computing the diameter polynomially faster than APSP", 
    "arxiv-id": "1011.6181v2", 
    "author": "Raphael Yuster", 
    "publish": "2010-11-29T10:26:27Z", 
    "summary": "We present a new randomized algorithm for computing the diameter of a\nweighted directed graph. The algorithm runs in\n$\\Ot(M^{\\w/(\\w+1)}n^{(\\w^2+3)/(\\w+1)})$ time, where $\\w < 2.376$ is the\nexponent of fast matrix multiplication, $n$ is the number of vertices of the\ngraph, and the edge weights are integers in $\\{-M,...,0,...,M\\}$. For bounded\ninteger weights the running time is $O(n^{2.561})$ and if $\\w=2+o(1)$ it is\n$\\Ot(n^{7/3})$. This is the first algorithm that computes the diameter of an\ninteger weighted directed graph polynomially faster than any known All-Pairs\nShortest Paths (APSP) algorithm. For bounded integer weights, the fastest\nalgorithm for APSP runs in $O(n^{2.575})$ time for the present value of $\\w$\nand runs in $\\Ot(n^{2.5})$ time if $\\w=2+o(1)$.\n  For directed graphs with {\\em positive} integer weights in $\\{1,...,M\\}$ we\nobtain a deterministic algorithm that computes the diameter in $\\Ot(Mn^\\w)$\ntime. This extends a simple $\\Ot(n^\\w)$ algorithm for computing the diameter of\nan {\\em unweighted} directed graph to the positive integer weighted setting and\nis the first algorithm in this setting whose time complexity matches that of\nthe fastest known Diameter algorithm for {\\em undirected} graphs.\n  The diameter algorithms are consequences of a more general result. We\nconstruct algorithms that for any given integer $d$, report all ordered pairs\nof vertices having distance {\\em at most} $d$. The diameter can therefore be\ncomputed using binary search for the smallest $d$ for which all pairs are\nreported."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.0021v1", 
    "other_authors": "Katarzyna Paluch", 
    "title": "Popular b-matchings", 
    "arxiv-id": "1101.0021v1", 
    "author": "Katarzyna Paluch", 
    "publish": "2010-12-29T23:32:39Z", 
    "summary": "Suppose that each member of a set of agents has a preference list of a subset\nof houses, possibly involving ties and each agent and house has their capacity\ndenoting the maximum number of correspondingly agents/houses that can be\nmatched to him/her/it. We want to find a matching $M$, for which there is no\nother matching $M'$ such that more agents prefer $M'$ to $M$ than $M$ to $M'$.\n(What it means that an agent prefers one matching to the other is explained in\nthe paper.) Popular matchings have been studied quite extensively, especially\nin the one-to-one setting. We provide a characterization of popular b-matchings\nfor two defintions of popularity, show some $NP$-hardness results and for\ncertain versions describe polynomial algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.0056v1", 
    "other_authors": "Jagbeer Singh", 
    "title": "An Algorithm to Reduce the Time Complexity of Earliest Deadline First   Scheduling Algorithm in Real-Time System", 
    "arxiv-id": "1101.0056v1", 
    "author": "Jagbeer Singh", 
    "publish": "2010-12-30T08:50:49Z", 
    "summary": "In this paper I have study to Reduce the time Complexity of Earliest Deadline\nFirst (EDF), a global scheduling scheme for Earliest Deadline First in Real\nTime System tasks on a Multiprocessors system. Several admission control\nalgorithms for Earliest Deadline First (EDF) are presented, both for hard and\nsoft real-time tasks. The average performance of these admission control\nalgorithms is compared with the performance of known partitioning schemes. I\nhave applied some modification to the global Earliest Deadline First (EDF)\nalgorithms to decrease the number of task migration and also to add\npredictability to its behavior. The Aim of this work is to provide a\nsensitivity analysis for task deadline context of multiprocessor system by\nusing a new approach of EFDF (Earliest Feasible Deadline First) algorithm. In\norder to decrease the number of migrations we prevent a job from moving one\nprocessor to another processor if it is among the m higher priority jobs.\nTherefore, a job will continue its execution on the same processor if possible\n(processor affinity). The result of these comparisons outlines some situations\nwhere one scheme is preferable over the other. Partitioning schemes are better\nsuited for hard real-time systems, while a global scheme is preferable for soft\nreal-time systems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.0080v3", 
    "other_authors": "Naoya Kishiue, Masaya Nakahara, Shirou Maruyama, Hiroshi Sakamoto", 
    "title": "A Searchable Compressed Edit-Sensitive Parsing", 
    "arxiv-id": "1101.0080v3", 
    "author": "Hiroshi Sakamoto", 
    "publish": "2010-12-30T13:04:56Z", 
    "summary": "Practical data structures for the edit-sensitive parsing (ESP) are proposed.\nGiven a string S, its ESP tree is equivalent to a context-free grammar G\ngenerating just S, which is represented by a DAG. Using the succinct data\nstructures for trees and permutations, G is decomposed to two LOUDS bit strings\nand single array in (1+\\epsilon)n\\log n+4n+o(n) bits for any 0<\\epsilon <1 and\nthe number n of variables in G. The time to count occurrences of P in S is in\nO(\\frac{1}{\\epsilon}(m\\log n+occ_c(\\log m\\log u)), whereas m = |P|, u = |S|,\nand occ_c is the number of occurrences of a maximal common subtree in ESPs of P\nand S. The efficiency of the proposed index is evaluated by the experiments\nconducted on several benchmarks complying with the other compressed indexes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.1256v1", 
    "other_authors": "Johanne Cohen, Christoph Durr, Nguyen Kim Thang", 
    "title": "Non-clairvoyant Scheduling Games", 
    "arxiv-id": "1101.1256v1", 
    "author": "Nguyen Kim Thang", 
    "publish": "2011-01-06T17:05:50Z", 
    "summary": "In a scheduling game, each player owns a job and chooses a machine to execute\nit. While the social cost is the maximal load over all machines (makespan), the\ncost (disutility) of each player is the completion time of its own job. In the\ngame, players may follow selfish strategies to optimize their cost and\ntherefore their behaviors do not necessarily lead the game to an equilibrium.\nEven in the case there is an equilibrium, its makespan might be much larger\nthan the social optimum, and this inefficiency is measured by the price of\nanarchy -- the worst ratio between the makespan of an equilibrium and the\noptimum. Coordination mechanisms aim to reduce the price of anarchy by\ndesigning scheduling policies that specify how jobs assigned to a same machine\nare to be scheduled. Typically these policies define the schedule according to\nthe processing times as announced by the jobs. One could wonder if there are\npolicies that do not require this knowledge, and still provide a good price of\nanarchy. This would make the processing times be private information and avoid\nthe problem of truthfulness. In this paper we study these so-called\nnon-clairvoyant policies. In particular, we study the RANDOM policy that\nschedules the jobs in a random order without preemption, and the EQUI policy\nthat schedules the jobs in parallel using time-multiplexing, assigning each job\nan equal fraction of CPU time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.1941v1", 
    "other_authors": "Howard Karloff, Flip Korn, Konstantin Makarychev, Yuval Rabani", 
    "title": "On Parsimonious Explanations for 2-D Tree- and Linearly-Ordered Data", 
    "arxiv-id": "1101.1941v1", 
    "author": "Yuval Rabani", 
    "publish": "2011-01-10T20:15:03Z", 
    "summary": "This paper studies the \"explanation problem\" for tree- and linearly-ordered\narray data, a problem motivated by database applications and recently solved\nfor the one-dimensional tree-ordered case. In this paper, one is given a matrix\nA whose rows and columns have semantics: special subsets of the rows and\nspecial subsets of the columns are meaningful, others are not. A submatrix in A\nis said to be meaningful if and only if it is the cross product of a meaningful\nrow subset and a meaningful column subset, in which case we call it an \"allowed\nrectangle.\" The goal is to \"explain\" A as a sparse sum of weighted allowed\nrectangles. Specifically, we wish to find as few weighted allowed rectangles as\npossible such that, for all i,j, a_{ij} equals the sum of the weights of all\nrectangles which include cell (i,j).\n  In this paper we consider the natural cases in which the matrix dimensions\nare tree-ordered or linearly-ordered. In the tree-ordered case, we are given a\nrooted tree T1 whose leaves are the rows of A and another, T2, whose leaves are\nthe columns. Nodes of the trees correspond in an obvious way to the sets of\ntheir leaf descendants. In the linearly-ordered case, a set of rows or columns\nis meaningful if and only if it is contiguous.\n  For tree-ordered data, we prove the explanation problem NP-Hard and give a\nrandomized 2-approximation algorithm for it. For linearly-ordered data, we\nprove the explanation problem NP-Hard and give a 2.56-approximation algorithm.\nTo our knowledge, these are the first results for the problem of sparsely and\nexactly representing matrices by weighted rectangles."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.2973v5", 
    "other_authors": "Salman Fadaei, MohammadAmin Fazli, MohammadAli Safari", 
    "title": "Maximizing Non-monotone Submodular Set Functions Subject to Different   Constraints: Combined Algorithms", 
    "arxiv-id": "1101.2973v5", 
    "author": "MohammadAli Safari", 
    "publish": "2011-01-15T11:30:05Z", 
    "summary": "We study the problem of maximizing constrained non-monotone submodular\nfunctions and provide approximation algorithms that improve existing algorithms\nin terms of either the approximation factor or simplicity. Our algorithms\ncombine existing local search and greedy based algorithms. Different\nconstraints that we study are exact cardinality and multiple knapsack\nconstraints. For the multiple-knapsack constraints we achieve a\n$(0.25-2\\epsilon)$-factor algorithm.\n  We also show, as our main contribution, how to use the continuous greedy\nprocess for non-monotone functions and, as a result, obtain a $0.13$-factor\napproximation algorithm for maximization over any solvable down-monotone\npolytope. The continuous greedy process has been previously used for maximizing\nsmooth monotone submodular function over a down-monotone polytope\n\\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,\nsuch as maximizing a non-negative submodular function subject to a matroid\nconstraint and/or multiple knapsack constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.3182v2", 
    "other_authors": "Petr Hlineny, Ondrej Moris", 
    "title": "Multi-Stage Improved Route Planning Approach: theoretical foundations", 
    "arxiv-id": "1101.3182v2", 
    "author": "Ondrej Moris", 
    "publish": "2011-01-17T11:04:58Z", 
    "summary": "A new approach to the static route planning problem, based on a multi-staging\nconcept and a \\emph{scope} notion, is presented. The main goal (besides implied\nefficiency of planning) of our approach is to address---with a solid\ntheoretical foundation---the following two practically motivated aspects: a\n\\emph{route comfort} and a very \\emph{limited storage} space of a small\nnavigation device, which both do not seem to be among the chief objectives of\nmany other studies. We show how our novel idea can tackle both these seemingly\nunrelated aspects at once, and may also contribute to other established route\nplanning approaches with which ours can be naturally combined. We provide a\ntheoretical proof that our approach efficiently computes exact optimal routes\nwithin this concept, as well as we demonstrate with experimental results on\npublicly available road networks of the US the good practical performance of\nthe solution."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.3396v1", 
    "other_authors": "Imen Harbaoui Dridi, Ryan Kammarti, Pierre Borne, Mekki Ksouri", 
    "title": "Multi-objective Optimization For The Dynamic Multi-Pickup and Delivery   Problem with Time Windows", 
    "arxiv-id": "1101.3396v1", 
    "author": "Mekki Ksouri", 
    "publish": "2011-01-18T08:14:47Z", 
    "summary": "The PDPTW is an optimization vehicles routing problem which must meet\nrequests for transport between suppliers and customers satisfying precedence,\ncapacity and time constraints. We present, in this paper, a genetic algorithm\nfor multi-objective optimization of a dynamic multi pickup and delivery problem\nwith time windows (Dynamic m-PDPTW). We propose a brief literature review of\nthe PDPTW, present our approach based on Pareto dominance method and lower\nbounds, to give a satisfying solution to the Dynamic m-PDPTW minimizing the\ncompromise between total travel cost and total tardiness time. Computational\nresults indicate that the proposed algorithm gives good results with a total\ntardiness equal to zero with a tolerable cost."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.3448v1", 
    "other_authors": "Johannes Fischer", 
    "title": "Inducing the LCP-Array", 
    "arxiv-id": "1101.3448v1", 
    "author": "Johannes Fischer", 
    "publish": "2011-01-18T12:58:02Z", 
    "summary": "We show how to modify the linear-time construction algorithm for suffix\narrays based on induced sorting (Nong et al., DCC'09) such that it computes the\narray of longest common prefixes (LCP-array) as well. Practical tests show that\nthis outperforms recent LCP-array construction algorithms (Gog and Ohlebusch,\nALENEX'11)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.3953v1", 
    "other_authors": "Greg N. Frederickson, Barry Wittman", 
    "title": "Two Multivehicle Routing Problems with Unit-Time Windows", 
    "arxiv-id": "1101.3953v1", 
    "author": "Barry Wittman", 
    "publish": "2011-01-20T16:39:00Z", 
    "summary": "Two multivehicle routing problems are considered in the framework that a\nvisit to a location must take place during a specific time window in order to\nbe counted and all time windows are the same length. In the first problem, the\ngoal is to visit as many locations as possible using a fixed number of\nvehicles. In the second, the goal is to visit all locations using the smallest\nnumber of vehicles possible. For the first problem, we present an approximation\nalgorithm whose output path collects a reward within a constant factor of\noptimal for any fixed number of vehicles. For the second problem, our algorithm\nfinds a 6-approximation to the problem on a tree metric, whenever a single\nvehicle could visit all locations during their time windows."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.3960v1", 
    "other_authors": "Greg N. Frederickson, Barry Wittman", 
    "title": "Speedup in the Traveling Repairman Problem with Constrained Time Windows", 
    "arxiv-id": "1101.3960v1", 
    "author": "Barry Wittman", 
    "publish": "2011-01-20T16:51:55Z", 
    "summary": "A bicriteria approximation algorithm is presented for the unrooted traveling\nrepairman problem, realizing increased profit in return for increased speedup\nof repairman motion. The algorithm generalizes previous results from the case\nin which all time windows are the same length to the case in which their\nlengths can range between l and 2. This analysis can extend to any range of\ntime window lengths, following our earlier techniques. This relationship\nbetween repairman profit and speedup is applicable over a range of values that\nis dependent on the cost of putting the input in an especially desirable form,\ninvolving what are called \"trimmed windows.\" For time windows with lengths\nbetween 1 and 2, the range of values for speedup $s$ for which our analysis\nholds is $1 \\leq s \\leq 6$. In this range, we establish an approximation ratio\nthat is constant for any specific value of $s$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.4065v1", 
    "other_authors": "Sebastian Kreft, Gonzalo Navarro", 
    "title": "Self-Index Based on LZ77", 
    "arxiv-id": "1101.4065v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-01-21T02:43:16Z", 
    "summary": "We introduce the first self-index based on the Lempel-Ziv 1977 compression\nformat (LZ77). It is particularly competitive for highly repetitive text\ncollections such as sequence databases of genomes of related species, software\nrepositories, versioned document collections, and temporal text databases. Such\ncollections are extremely compressible but classical self-indexes fail to\ncapture that source of compressibility. Our self-index takes in practice a few\ntimes the space of the text compressed with LZ77 (as little as 2.6 times),\nextracts 1--2 million characters of the text per second, and finds patterns at\na rate of 10--50 microseconds per occurrence. It is smaller (up to one half)\nthan the best current self-index for repetitive collections, and faster in many\ncases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.4068v1", 
    "other_authors": "Stephane Durocher, Jason Morrison", 
    "title": "Linear-Space Data Structures for Range Mode Query in Arrays", 
    "arxiv-id": "1101.4068v1", 
    "author": "Jason Morrison", 
    "publish": "2011-01-21T03:23:57Z", 
    "summary": "A mode of a multiset $S$ is an element $a \\in S$ of maximum multiplicity;\nthat is, $a$ occurs at least as frequently as any other element in $S$. Given a\nlist $A[1:n]$ of $n$ items, we consider the problem of constructing a data\nstructure that efficiently answers range mode queries on $A$. Each query\nconsists of an input pair of indices $(i, j)$ for which a mode of $A[i:j]$ must\nbe returned. We present an $O(n^{2-2\\epsilon})$-space static data structure\nthat supports range mode queries in $O(n^\\epsilon)$ time in the worst case, for\nany fixed $\\epsilon \\in [0,1/2]$. When $\\epsilon = 1/2$, this corresponds to\nthe first linear-space data structure to guarantee $O(\\sqrt{n})$ query time. We\nthen describe three additional linear-space data structures that provide\n$O(k)$, $O(m)$, and $O(|j-i|)$ query time, respectively, where $k$ denotes the\nnumber of distinct elements in $A$ and $m$ denotes the frequency of the mode of\n$A$. Finally, we examine generalizing our data structures to higher dimensions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.4446v1", 
    "other_authors": "Andrew Drucker", 
    "title": "High-Confidence Predictions under Adversarial Uncertainty", 
    "arxiv-id": "1101.4446v1", 
    "author": "Andrew Drucker", 
    "publish": "2011-01-24T05:35:05Z", 
    "summary": "We study the setting in which the bits of an unknown infinite binary sequence\nx are revealed sequentially to an observer. We show that very limited\nassumptions about x allow one to make successful predictions about unseen bits\nof x. First, we study the problem of successfully predicting a single 0 from\namong the bits of x. In our model we have only one chance to make a prediction,\nbut may do so at a time of our choosing. We describe and motivate this as the\nproblem of a frog who wants to cross a road safely.\n  Letting N_t denote the number of 1s among the first t bits of x, we say that\nx is \"eps-weakly sparse\" if lim inf (N_t/t) <= eps. Our main result is a\nrandomized algorithm that, given any eps-weakly sparse sequence x, predicts a 0\nof x with success probability as close as desired to 1 - \\eps. Thus we can\nperform this task with essentially the same success probability as under the\nmuch stronger assumption that each bit of x takes the value 1 independently\nwith probability eps. We apply this result to show how to successfully predict\na bit (0 or 1) under a broad class of possible assumptions on the sequence x.\nThe assumptions are stated in terms of the behavior of a finite automaton M\nreading the bits of x.\n  We also propose and solve a variant of the well-studied \"ignorant\nforecasting\" problem. For every eps > 0, we give a randomized forecasting\nalgorithm S_eps that, given sequential access to a binary sequence x, makes a\nprediction of the form: \"A p fraction of the next N bits will be 1s.\" (The\nalgorithm gets to choose p, N, and the time of the prediction.) For any fixed\nsequence x, the forecast fraction p is accurate to within +-eps with\nprobability 1 - eps."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.4491v3", 
    "other_authors": "Christophe Paul, Anthony Perez, St\u00e9phan Thomass\u00e9", 
    "title": "Conflict Packing: an unifying technique to obtain polynomial kernels for   editing problems on dense instances", 
    "arxiv-id": "1101.4491v3", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2011-01-24T10:42:46Z", 
    "summary": "We develop a technique that we call Conflict Packing in the context of\nkernelization, obtaining (and improving) several polynomial kernels for editing\nproblems on dense instances. We apply this technique on several well-studied\nproblems: Feedback Arc Set in (Bipartite) Tournaments, Dense Rooted Triplet\nInconsistency and Betweenness in Tournaments. For the former, one is given a\n(bipartite) tournament $T = (V,A)$ and seeks a set of at most $k$ arcs whose\nreversal in $T$ results in an acyclic (bipartite) tournament. While a linear\nvertex-kernel is already known for the first problem, using the Conflict\nPacking allows us to find a so-called safe partition, the central tool of the\nkernelization algorithm in, with simpler arguments. For the case of bipartite\ntournaments, the same technique allows us to obtain a quadratic vertex-kernel.\nAgain, such a kernel was already known to exist, using the concept of so-called\nbimodules. We believe however that providing an unifying technique to cope with\nsuch problems is interesting. Regarding Dense Rooted Triplet Inconsistency, one\nis given a set of vertices $V$ and a dense collection $\\mathcal{R}$ of rooted\nbinary trees over three vertices of $V$ and seeks a rooted tree over $V$\ncontaining all but at most $k$ triplets from $\\mathcal{R}$. As a main\nconsequence of our technique, we prove that the Dense Rooted Triplet\nInconsistency problem admits a linear vertex-kernel. This result improves the\nbest known bound of $O(k^2)$ vertices for this problem. Finally, we use this\ntechnique to obtain a linear vertex-kernel for Betweenness in Tournaments,\nwhere one is given a set of vertices $V$ and a dense collection $\\mathcal{R}$\nof so-called betweenness triplets and seeks a linear ordering of the vertices\ncontaining all but at most $k$ triplets from $\\mathcal{R}$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.5407v1", 
    "other_authors": "Michael A. Bender, S\u00e1ndor P. Fekete, Tom Kamphans, Nils Schweer", 
    "title": "Maintaining Arrays of Contiguous Objects", 
    "arxiv-id": "1101.5407v1", 
    "author": "Nils Schweer", 
    "publish": "2011-01-27T22:48:51Z", 
    "summary": "In this paper we consider methods for dynamically storing a set of different\nobjects (\"modules\") in a physical array. Each module requires one free\ncontiguous subinterval in order to be placed. Items are inserted or removed,\nresulting in a fragmented layout that makes it harder to insert further\nmodules. It is possible to relocate modules, one at a time, to another free\nsubinterval that is contiguous and does not overlap with the current location\nof the module. These constraints clearly distinguish our problem from classical\nmemory allocation. We present a number of algorithmic results, including a\nbound of Theta(n^2) on physical sorting if there is a sufficiently large free\nspace and sum up NP-hardness results for arbitrary initial layouts. For online\nscenarios in which modules arrive one at a time, we present a method that\nrequires O(1) moves per insertion or deletion and amortized cost O(m_i log M)\nper insertion or deletion, where m_i is the module's size, M is the size of the\nlargest module and costs for moves are linear in the size of a module."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-19222-7_10", 
    "link": "http://arxiv.org/pdf/1101.5506v1", 
    "other_authors": "Nieves R. Brisaboa, Rodrigo C\u00e1novas, Miguel A. Mart\u00ednez-Prieto, Gonzalo Navarro", 
    "title": "Compressed String Dictionaries", 
    "arxiv-id": "1101.5506v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-01-28T11:08:46Z", 
    "summary": "The problem of storing a set of strings --- a string dictionary --- in\ncompact form appears naturally in many cases. While classically it has\nrepresented a small part of the whole data to be processed (e.g., for Natural\nLanguage processing or for indexing text collections), more recent applications\nin Web engines, Web mining, RDF graphs, Internet routing, Bioinformatics, and\nmany others, make use of very large string dictionaries, whose size is a\nsignificant fraction of the whole data. Thus novel approaches to compress them\nefficiently are necessary. In this paper we experimentally compare time and\nspace performance of some existing alternatives, as well as new ones we\npropose. We show that space reductions of up to 20% of the original size of the\nstrings is possible while supporting fast dictionary searches."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.06.010", 
    "link": "http://arxiv.org/pdf/1101.5518v3", 
    "other_authors": "Kitty Meeks, Alexander Scott", 
    "title": "The complexity of Free-Flood-It on 2xn boards", 
    "arxiv-id": "1101.5518v3", 
    "author": "Alexander Scott", 
    "publish": "2011-01-28T11:51:32Z", 
    "summary": "We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Our main result is that\ncomputing the length of an optimal sequence is fixed parameter tractable (with\nthe number of colours present as a parameter) when restricted to rectangular\n2xn boards. We also show that, when the number of colours is unbounded, the\nproblem remains NP-hard on such boards. This resolves a question of Clifford,\nJalsenius, Montanaro and Sach (2010)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2013.06.010", 
    "link": "http://arxiv.org/pdf/1101.5586v1", 
    "other_authors": "Nishita Aggarwal, Naveen Garg, Swati Gupta", 
    "title": "A 4/3-approximation for TSP on cubic 3-edge-connected graphs", 
    "arxiv-id": "1101.5586v1", 
    "author": "Swati Gupta", 
    "publish": "2011-01-28T17:45:05Z", 
    "summary": "We provide a polynomial time 4/3 approximation algorithm for TSP on metrics\narising from the metric completion of cubic 3-edge connected graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1101.5876v3", 
    "other_authors": "Kitty Meeks, Alexander Scott", 
    "title": "The complexity of flood-filling games on graphs", 
    "arxiv-id": "1101.5876v3", 
    "author": "Alexander Scott", 
    "publish": "2011-01-31T08:42:14Z", 
    "summary": "We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Although computing the\nminimum number of moves required to flood an arbitrary graph is known to be\nNP-hard, we demonstrate a polynomial time algorithm to compute the minimum\nnumber of moves required to link each pair of vertices. We apply this result to\ncompute in polynomial time the minimum number of moves required to flood a\npath, and an additive approximation to this quantity for an arbitrary k x n\nboard, coloured with a bounded number of colours, for any fixed k. On the other\nhand, we show that, for k>=3, determining the minimum number of moves required\nto flood a k x n board coloured with at least four colours remains NP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1101.5944v2", 
    "other_authors": "Oksana Denysyuk, Luis Rodrigues", 
    "title": "Random Walk on Directed Dynamic Graphs", 
    "arxiv-id": "1101.5944v2", 
    "author": "Luis Rodrigues", 
    "publish": "2011-01-31T13:01:30Z", 
    "summary": "Dynamic graphs have emerged as an appropriate model to capture the changing\nnature of many modern networks, such as peer-to-peer overlays and mobile ad hoc\nnetworks. Most of the recent research on dynamic networks has only addressed\nthe undirected dynamic graph model. However, realistic networks such as the\nones identified above are directed. In this paper we present early work in\naddressing the properties of directed dynamic graphs. In particular, we explore\nthe problem of random walk in such graphs. We assume the existence of an\noblivious adversary that makes arbitrary changes in every communication round.\nWe explore the problem of covering the dynamic graph, that even in the static\ncase can be exponential, and we establish an upper bound O(d_max n^3 log^2 n)\nof the cover time for balanced dynamic graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0041v2", 
    "other_authors": "Giovanni Battaglia, Roberto Grossi, Noemi Scutell\u00e0", 
    "title": "Consecutive Ones Property and PQ-Trees for Multisets: Hardness of   Counting Their Orderings", 
    "arxiv-id": "1102.0041v2", 
    "author": "Noemi Scutell\u00e0", 
    "publish": "2011-01-31T23:59:40Z", 
    "summary": "A binary matrix satisfies the consecutive ones property (COP) if its columns\ncan be permuted such that the ones in each row of the resulting matrix are\nconsecutive. Equivalently, a family of sets F = {Q_1,..,Q_m}, where Q_i is\nsubset of R for some universe R, satisfies the COP if the symbols in R can be\npermuted such that the elements of each set Q_i occur consecutively, as a\ncontiguous segment of the permutation of R's symbols. We consider the COP\nversion on multisets and prove that counting its solutions is difficult\n(#P-complete). We prove completeness results also for counting the frontiers of\nPQ-trees, which are typically used for testing the COP on sets, thus showing\nthat a polynomial algorithm is unlikely to exist when dealing with multisets.\nWe use a combinatorial approach based on parsimonious reductions from the\nHamiltonian path problem, showing that the decisional version of our problems\nis therefore NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0395v1", 
    "other_authors": "Johannes Fischer", 
    "title": "Combined Data Structure for Previous- and Next-Smaller-Values", 
    "arxiv-id": "1102.0395v1", 
    "author": "Johannes Fischer", 
    "publish": "2011-02-02T10:20:58Z", 
    "summary": "Let $A$ be a static array storing $n$ elements from a totally ordered set. We\npresent a data structure of optimal size at most $n\\log_2(3+2\\sqrt{2})+o(n)$\nbits that allows us to answer the following queries on $A$ in constant time,\nwithout accessing $A$: (1) previous smaller value queries, where given an index\n$i$, we wish to find the first index to the left of $i$ where $A$ is strictly\nsmaller than at $i$, and (2) next smaller value queries, which search to the\nright of $i$. As an additional bonus, our data structure also allows to answer\na third kind of query: given indices $i<j$, find the position of the minimum in\n$A[i..j]$. Our data structure has direct consequences for the space-efficient\nstorage of suffix trees."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0471v2", 
    "other_authors": "Sergey Ishkov, Elena Ishkova", 
    "title": "Matrix method for the multi salesmen problem (TSP) with several vehicles", 
    "arxiv-id": "1102.0471v2", 
    "author": "Elena Ishkova", 
    "publish": "2011-02-01T11:56:13Z", 
    "summary": "In this paper discussed procedure of separation of the original problem with\nseveral vehicles to a number of simpler problems with one vehicle which based\non the matrix approach."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.0588v2", 
    "other_authors": "J\u00f6rg L\u00e4ssig, Dirk Sudholt", 
    "title": "Adaptive Population Models for Offspring Populations and Parallel   Evolutionary Algorithms", 
    "arxiv-id": "1102.0588v2", 
    "author": "Dirk Sudholt", 
    "publish": "2011-02-02T23:39:57Z", 
    "summary": "We present two adaptive schemes for dynamically choosing the number of\nparallel instances in parallel evolutionary algorithms. This includes the\nchoice of the offspring population size in a (1+$\\lambda$) EA as a special\ncase. Our schemes are parameterless and they work in a black-box setting where\nno knowledge on the problem is available. Both schemes double the number of\ninstances in case a generation ends without finding an improvement. In a\nsuccessful generation, the first scheme resets the system to one instance,\nwhile the second scheme halves the number of instances. Both schemes provide\nnear-optimal speed-ups in terms of the parallel time. We give upper bounds for\nthe asymptotic sequential time (i.e., the total number of function evaluations)\nthat are not larger than upper bounds for a corresponding non-parallel\nalgorithm derived by the fitness-level method."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1124v3", 
    "other_authors": "Golnaz Ghasemiesfeh, Hanieh Mirzaei, Yahya Tabesh", 
    "title": "A Polynomial Time Algorithm for a Special Case of Linear Integer   Programming", 
    "arxiv-id": "1102.1124v3", 
    "author": "Yahya Tabesh", 
    "publish": "2011-02-06T04:53:31Z", 
    "summary": "This article has been withdrawn."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1273v1", 
    "other_authors": "\u0141ukasz Je\u017c", 
    "title": "One to Rule Them All: a General Randomized Algorithm for Buffer   Management with Bounded Delay", 
    "arxiv-id": "1102.1273v1", 
    "author": "\u0141ukasz Je\u017c", 
    "publish": "2011-02-07T11:09:34Z", 
    "summary": "We give a memoryless scale-invariant randomized algorithm for the Buffer\nManagement with Bounded Delay problem that is e/(e-1)-competitive against an\nadaptive adversary, together with better performance guarantees for many\nrestricted variants, including the s-bounded instances. In particular, our\nalgorithm attains the optimum competitive ratio of 4/3 on 2-bounded instances.\n  Both the algorithm and its analysis are applicable to a more general problem,\ncalled Collecting Items, in which only the relative order between packets'\ndeadlines is known. Our algorithm is the optimal randomized memoryless\nalgorithm against adaptive adversary for that problem in a strong sense.\n  While some of provided upper bounds were already known, in general, they were\nattained by several different algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.dam.2011.09.001", 
    "link": "http://arxiv.org/pdf/1102.1472v1", 
    "other_authors": "Karthekeyan Chandrasekaran, Richard Karp, Erick Moreno-Centeno, Santosh Vempala", 
    "title": "Algorithms for Implicit Hitting Set Problems", 
    "arxiv-id": "1102.1472v1", 
    "author": "Santosh Vempala", 
    "publish": "2011-02-07T23:40:58Z", 
    "summary": "A hitting set for a collection of sets is a set that has a non-empty\nintersection with each set in the collection; the hitting set problem is to\nfind a hitting set of minimum cardinality. Motivated by instances of the\nhitting set problem where the number of sets to be hit is large, we introduce\nthe notion of implicit hitting set problems. In an implicit hitting set problem\nthe collection of sets to be hit is typically too large to list explicitly;\ninstead, an oracle is provided which, given a set H, either determines that H\nis a hitting set or returns a set that H does not hit. We show a number of\nexamples of classic implicit hitting set problems, and give a generic algorithm\nfor solving such problems optimally. The main contribution of this paper is to\nshow that this framework is valuable in developing approximation algorithms. We\nillustrate this methodology by presenting a simple on-line algorithm for the\nminimum feedback vertex set problem on random graphs. In particular our\nalgorithm gives a feedback vertex set of size n-(1/p)\\log{np}(1-o(1)) with\nprobability at least 3/4 for the random graph G_{n,p} (the smallest feedback\nvertex set is of size n-(2/p)\\log{np}(1+o(1))). We also consider a planted\nmodel for the feedback vertex set in directed random graphs. Here we show that\na hitting set for a polynomial-sized subset of cycles is a hitting set for the\nplanted random graph and this allows us to exactly recover the planted feedback\nvertex set."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.1746v1", 
    "other_authors": "P\u00e9ter Burcsi, Ferdinando Cicalese, Gabriele Fici, Zsuzsanna Lipt\u00e1k", 
    "title": "Algorithms for Jumbled Pattern Matching in Strings", 
    "arxiv-id": "1102.1746v1", 
    "author": "Zsuzsanna Lipt\u00e1k", 
    "publish": "2011-02-08T23:11:17Z", 
    "summary": "The Parikh vector p(s) of a string s is defined as the vector of\nmultiplicities of the characters. Parikh vector q occurs in s if s has a\nsubstring t with p(t)=q. We present two novel algorithms for searching for a\nquery q in a text s. One solves the decision problem over a binary text in\nconstant time, using a linear size index of the text. The second algorithm, for\na general finite alphabet, finds all occurrences of a given Parikh vector q and\nhas sub-linear expected time complexity; we present two variants, which both\nuse a linear size index of the text."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3393v1", 
    "other_authors": "Marek Chrobak, \u0141ukasz Je\u017c, Ji\u0159\u00ed Sgall", 
    "title": "Better Bounds for Incremental Frequency Allocation in Bipartite Graphs", 
    "arxiv-id": "1102.3393v1", 
    "author": "Ji\u0159\u00ed Sgall", 
    "publish": "2011-02-16T18:32:07Z", 
    "summary": "We study frequency allocation in wireless networks. A wireless network is\nmodeled by an undirected graph, with vertices corresponding to cells. In each\nvertex we have a certain number of requests, and each of those requests must be\nassigned a different frequency. Edges represent conflicts between cells,\nmeaning that frequencies in adjacent vertices must be different as well. The\nobjective is to minimize the total number of used frequencies.\n  The offline version of the problem is known to be NP-hard. In the incremental\nversion, requests for frequencies arrive over time and the algorithm is\nrequired to assign a frequency to a request as soon as it arrives. Competitive\nincremental algorithms have been studied for several classes of graphs. For\npaths, the optimal (asymptotic) ratio is known to be 4/3, while for\nhexagonal-cell graphs it is between 1.5 and 1.9126. For k-colorable graphs, the\nratio of (k+1)/2 can be achieved.\n  In this paper, we prove nearly tight bounds on the asymptotic competitive\nratio for bipartite graphs, showing that it is between 1.428 and 1.433. This\nimproves the previous lower bound of 4/3 and upper bound of 1.5. Our proofs are\nbased on reducing the incremental problem to a purely combinatorial\n(equivalent) problem of constructing set families with certain intersection\nproperties."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3491v1", 
    "other_authors": "Jos\u00e9 A. Soto", 
    "title": "A simple PTAS for Weighted Matroid Matching on Strongly Base Orderable   Matroids", 
    "arxiv-id": "1102.3491v1", 
    "author": "Jos\u00e9 A. Soto", 
    "publish": "2011-02-17T04:22:13Z", 
    "summary": "We give a simple polynomial time approximation scheme for the weighted\nmatroid matching problem on strongly base orderable matroids. We also show that\neven the unweighted version of this problem is NP-complete and not in\noracle-coNP."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3537v1", 
    "other_authors": "Guy Feigenblat, Ely Porat, Ariel Shiftan", 
    "title": "Even Better Framework for min-wise Based Algorithms", 
    "arxiv-id": "1102.3537v1", 
    "author": "Ariel Shiftan", 
    "publish": "2011-02-17T09:43:17Z", 
    "summary": "In a recent paper from SODA11 \\cite{kminwise} the authors introduced a\ngeneral framework for exponential time improvement of \\minwise based algorithms\nby defining and constructing almost \\kmin independent family of hash functions.\nHere we take it a step forward and reduce the space and the independent needed\nfor representing the functions, by defining and constructing a \\dkmin\nindependent family of hash functions. Surprisingly, for most cases only 8-wise\nindependent is needed for exponential time and space improvement. Moreover, we\nbypass the $O(\\log{\\frac{1}{\\epsilon}})$ independent lower bound for\napproximately \\minwise functions \\cite{patrascu10kwise-lb}, as we use\nalternative definition. In addition, as the independent's degree is a small\nconstant it can be implemented efficiently.\n  Informally, under this definition, all subsets of size $d$ of any fixed set\n$X$ have an equal probability to have hash values among the minimal $k$ values\nin $X$, where the probability is over the random choice of hash function from\nthe family. This property measures the randomness of the family, as choosing a\ntruly random function, obviously, satisfies the definition for $d=k=|X|$. We\ndefine and give an efficient time and space construction of approximately\n\\dkmin independent family of hash functions. The degree of independent required\nis optimal, i.e. only $O(d)$ for $2 \\le d < k=O(\\frac{d}{\\epsilon^2})$, where\n$\\epsilon \\in (0,1)$ is the desired error bound. This construction can be used\nto improve many \\minwise based algorithms, such as\n\\cite{sizeEstimationFramework,Datar02estimatingrarity,NearDuplicate,SimilaritySearch,DBLP:conf/podc/CohenK07},\nas will be discussed here. To our knowledge such definitions, for hash\nfunctions, were never studied and no construction was given before."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3643v2", 
    "other_authors": "Paul Bonsma, Jens Schulz, Andreas Wiese", 
    "title": "A Constant Factor Approximation Algorithm for Unsplittable Flow on Paths", 
    "arxiv-id": "1102.3643v2", 
    "author": "Andreas Wiese", 
    "publish": "2011-02-17T17:43:08Z", 
    "summary": "In the unsplittable flow problem on a path, we are given a capacitated path\n$P$ and $n$ tasks, each task having a demand, a profit, and start and end\nvertices. The goal is to compute a maximum profit set of tasks, such that for\neach edge $e$ of $P$, the total demand of selected tasks that use $e$ does not\nexceed the capacity of $e$. This is a well-studied problem that has been\nstudied under alternative names, such as resource allocation, bandwidth\nallocation, resource constrained scheduling, temporal knapsack and interval\npacking.\n  We present a polynomial time constant-factor approximation algorithm for this\nproblem. This improves on the previous best known approximation ratio of\n$O(\\log n)$. The approximation ratio of our algorithm is $7+\\epsilon$ for any\n$\\epsilon>0$.\n  We introduce several novel algorithmic techniques, which might be of\nindependent interest: a framework which reduces the problem to instances with a\nbounded range of capacities, and a new geometrically inspired dynamic program\nwhich solves a special case of the maximum weight independent set of rectangles\nproblem to optimality. In the setting of resource augmentation, wherein the\ncapacities can be slightly violated, we give a $(2+\\epsilon)$-approximation\nalgorithm. In addition, we show that the problem is strongly NP-hard even if\nall edge capacities are equal and all demands are either~1,~2, or~3."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3749v1", 
    "other_authors": "Anupam Gupta, Ravishankar Krishnaswamy, Marco Molinaro, R. Ravi", 
    "title": "Approximation Algorithms for Correlated Knapsacks and Non-Martingale   Bandits", 
    "arxiv-id": "1102.3749v1", 
    "author": "R. Ravi", 
    "publish": "2011-02-18T04:05:21Z", 
    "summary": "In the stochastic knapsack problem, we are given a knapsack of size B, and a\nset of jobs whose sizes and rewards are drawn from a known probability\ndistribution. However, we know the actual size and reward only when the job\ncompletes. How should we schedule jobs to maximize the expected total reward?\nWe know O(1)-approximations when we assume that (i) rewards and sizes are\nindependent random variables, and (ii) we cannot prematurely cancel jobs. What\ncan we say when either or both of these assumptions are changed?\n  The stochastic knapsack problem is of interest in its own right, but\ntechniques developed for it are applicable to other stochastic packing\nproblems. Indeed, ideas for this problem have been useful for budgeted learning\nproblems, where one is given several arms which evolve in a specified\nstochastic fashion with each pull, and the goal is to pull the arms a total of\nB times to maximize the reward obtained. Much recent work on this problem focus\non the case when the evolution of the arms follows a martingale, i.e., when the\nexpected reward from the future is the same as the reward at the current state.\nWhat can we say when the rewards do not form a martingale?\n  In this paper, we give constant-factor approximation algorithms for the\nstochastic knapsack problem with correlations and/or cancellations, and also\nfor budgeted learning problems where the martingale condition is not satisfied.\nIndeed, we can show that previously proposed LP relaxations have large\nintegrality gaps. We propose new time-indexed LP relaxations, and convert the\nfractional solutions into distributions over strategies, and then use the LP\nvalues and the time ordering information from these strategies to devise a\nrandomized adaptive scheduling algorithm. We hope our LP formulation and\ndecomposition methods may provide a new way to address other correlated bandit\nproblems with more general contexts."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.3813v2", 
    "other_authors": "Keisuke Murakami, Takeaki Uno", 
    "title": "Efficient Algorithms for Dualizing Large-Scale Hypergraphs", 
    "arxiv-id": "1102.3813v2", 
    "author": "Takeaki Uno", 
    "publish": "2011-02-18T11:51:28Z", 
    "summary": "A hypergraph ${\\cal F}$ is a set family defined on vertex set $V$. The dual\nof ${\\cal F}$ is the set of minimal subsets $H$ of $V$ such that $F\\cap H \\ne\n\\emptyset$ for any $F\\in {\\cal F}$. The computation of the dual is equivalent\nto many problems, such as minimal hitting set enumeration of a subset family,\nminimal set cover enumeration, and the enumeration of hypergraph transversals.\nAlthough many algorithms have been proposed for solving the problem, to the\nbest of our knowledge, none of them can work on large-scale input with a large\nnumber of output minimal hitting sets. This paper focuses on developing time-\nand space-efficient algorithms for solving the problem. We propose two new\nalgorithms with new search methods, new pruning methods, and fast techniques\nfor the minimality check. The computational experiments show that our\nalgorithms are quite fast even for large-scale input for which existing\nalgorithms do not terminate in a practical time."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4480v3", 
    "other_authors": "Yasuo Tabei, Daisuke Okanohara, Shuichi Hirose, Koji Tsuda", 
    "title": "LGM: Mining Frequent Subgraphs from Linear Graphs", 
    "arxiv-id": "1102.4480v3", 
    "author": "Koji Tsuda", 
    "publish": "2011-02-22T12:20:18Z", 
    "summary": "A linear graph is a graph whose vertices are totally ordered. Biological and\nlinguistic sequences with interactions among symbols are naturally represented\nas linear graphs. Examples include protein contact maps, RNA secondary\nstructures and predicate-argument structures. Our algorithm, linear graph miner\n(LGM), leverages the vertex order for efficient enumeration of frequent\nsubgraphs. Based on the reverse search principle, the pattern space is\nsystematically traversed without expensive duplication checking. Disconnected\nsubgraph patterns are particularly important in linear graphs due to their\nsequential nature. Unlike conventional graph mining algorithms detecting\nconnected patterns only, LGM can detect disconnected patterns as well. The\nutility and efficiency of LGM are demonstrated in experiments on protein\ncontact maps."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4523v1", 
    "other_authors": "Navin Goyal, Manoj Gupta", 
    "title": "On Dynamic Optimality for Binary Search Trees", 
    "arxiv-id": "1102.4523v1", 
    "author": "Manoj Gupta", 
    "publish": "2011-02-22T14:47:19Z", 
    "summary": "Does there exist O(1)-competitive (self-adjusting) binary search tree (BST)\nalgorithms? This is a well-studied problem. A simple offline BST algorithm\nGreedyFuture was proposed independently by Lucas and Munro, and they\nconjectured it to be O(1)-competitive. Recently, Demaine et al. gave a\ngeometric view of the BST problem. This view allowed them to give an online\nalgorithm GreedyArb with the same cost as GreedyFuture. However, no\no(n)-competitive ratio was known for GreedyArb. In this paper we make progress\ntowards proving O(1)-competitive ratio for GreedyArb by showing that it is\nO(\\log n)-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4842v4", 
    "other_authors": "Ioannis Koutis, Gary Miller, Richard Peng", 
    "title": "A nearly-mlogn time solver for SDD linear systems", 
    "arxiv-id": "1102.4842v4", 
    "author": "Richard Peng", 
    "publish": "2011-02-23T20:53:03Z", 
    "summary": "We present an improved algorithm for solving symmetrically diagonally\ndominant linear systems. On input of an $n\\times n$ symmetric diagonally\ndominant matrix $A$ with $m$ non-zero entries and a vector $b$ such that\n$A\\bar{x} = b$ for some (unknown) vector $\\bar{x}$, our algorithm computes a\nvector $x$ such that $||{x}-\\bar{x}||_A < \\epsilon ||\\bar{x}||_A $\n{$||\\cdot||_A$ denotes the A-norm} in time $${\\tilde O}(m\\log n \\log\n(1/\\epsilon)).$$\n  The solver utilizes in a standard way a `preconditioning' chain of\nprogressively sparser graphs. To claim the faster running time we make a\ntwo-fold improvement in the algorithm for constructing the chain. The new chain\nexploits previously unknown properties of the graph sparsification algorithm\ngiven in [Koutis,Miller,Peng, FOCS 2010], allowing for stronger preconditioning\nproperties. We also present an algorithm of independent interest that\nconstructs nearly-tight low-stretch spanning trees in time\n$\\tilde{O}(m\\log{n})$, a factor of $O(\\log{n})$ faster than the algorithm in\n[Abraham,Bartal,Neiman, FOCS 2008]. This speedup directly reflects on the\nconstruction time of the preconditioning chain."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.4884v3", 
    "other_authors": "Kyle Fox", 
    "title": "Upper Bounds for Maximally Greedy Binary Search Trees", 
    "arxiv-id": "1102.4884v3", 
    "author": "Kyle Fox", 
    "publish": "2011-02-24T00:19:46Z", 
    "summary": "At SODA 2009, Demaine et al. presented a novel connection between binary\nsearch trees (BSTs) and subsets of points on the plane. This connection was\nindependently discovered by Derryberry et al. As part of their results, Demaine\net al. considered GreedyFuture, an offline BST algorithm that greedily\nrearranges the search path to minimize the cost of future searches. They showed\nthat GreedyFuture is actually an online algorithm in their geometric view, and\nthat there is a way to turn GreedyFuture into an online BST algorithm with only\na constant factor increase in total search cost. Demaine et al. conjectured\nthis algorithm was dynamically optimal, but no upper bounds were given in their\npaper. We prove the first non-trivial upper bounds for the cost of search\noperations using GreedyFuture including giving an access lemma similar to that\nfound in Sleator and Tarjan's classic paper on splay trees."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5105v1", 
    "other_authors": "Marek Cygan, Fabrizio Grandoni, Stefano Leonardi, Marcin Mucha, Marcin Pilipczuk, Piotr Sankowski", 
    "title": "Approximation Algorithms for Union and Intersection Covering Problems", 
    "arxiv-id": "1102.5105v1", 
    "author": "Piotr Sankowski", 
    "publish": "2011-02-24T21:33:20Z", 
    "summary": "In a classical covering problem, we are given a set of requests that we need\nto satisfy (fully or partially), by buying a subset of items at minimum cost.\nFor example, in the k-MST problem we want to find the cheapest tree spanning at\nleast k nodes of an edge-weighted graph. Here nodes and edges represent\nrequests and items, respectively.\n  In this paper, we initiate the study of a new family of multi-layer covering\nproblems. Each such problem consists of a collection of h distinct instances of\na standard covering problem (layers), with the constraint that all layers share\nthe same set of requests. We identify two main subfamilies of these problems: -\nin a union multi-layer problem, a request is satisfied if it is satisfied in at\nleast one layer; - in an intersection multi-layer problem, a request is\nsatisfied if it is satisfied in all layers. To see some natural applications,\nconsider both generalizations of k-MST. Union k-MST can model a problem where\nwe are asked to connect a set of users to at least one of two communication\nnetworks, e.g., a wireless and a wired network. On the other hand, intersection\nk-MST can formalize the problem of connecting a subset of users to both\nelectricity and water.\n  We present a number of hardness and approximation results for union and\nintersection versions of several standard optimization problems: MST, Steiner\ntree, set cover, facility location, TSP, and their partial covering variants."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5146v1", 
    "other_authors": "Edith Cohen, Graham Cormode, Nick Duffield", 
    "title": "Structure-Aware Sampling: Flexible and Accurate Summarization", 
    "arxiv-id": "1102.5146v1", 
    "author": "Nick Duffield", 
    "publish": "2011-02-25T03:39:45Z", 
    "summary": "In processing large quantities of data, a fundamental problem is to obtain a\nsummary which supports approximate query answering. Random sampling yields\nflexible summaries which naturally support subset-sum queries with unbiased\nestimators and well-understood confidence bounds.\n  Classic sample-based summaries, however, are designed for arbitrary subset\nqueries and are oblivious to the structure in the set of keys. The particular\nstructure, such as hierarchy, order, or product space (multi-dimensional),\nmakes range queries much more relevant for most analysis of the data.\n  Dedicated summarization algorithms for range-sum queries have also been\nextensively studied. They can outperform existing sampling schemes in terms of\naccuracy on range queries per summary size. Their accuracy, however, rapidly\ndegrades when, as is often the case, the query spans multiple ranges. They are\nalso less flexible - being targeted for range sum queries alone - and are often\nquite costly to build and use.\n  In this paper we propose and evaluate variance optimal sampling schemes that\nare structure-aware. These summaries improve over the accuracy of existing\nstructure-oblivious sampling schemes on range queries while retaining the\nbenefits of sample-based summaries: flexible summaries, with high accuracy on\nboth range queries and arbitrary subset queries."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5441v2", 
    "other_authors": "Pinar Heggernes, Pim van 't Hof, Daniel Lokshtanov, Christophe Paul", 
    "title": "Obtaining a Bipartite Graph by Contracting Few Edges", 
    "arxiv-id": "1102.5441v2", 
    "author": "Christophe Paul", 
    "publish": "2011-02-26T20:07:27Z", 
    "summary": "We initiate the study of the Bipartite Contraction problem from the\nperspective of parameterized complexity. In this problem we are given a graph\n$G$ and an integer $k$, and the task is to determine whether we can obtain a\nbipartite graph from $G$ by a sequence of at most $k$ edge contractions. Our\nmain result is an $f(k) n^{O(1)}$ time algorithm for Bipartite Contraction.\nDespite a strong resemblance between Bipartite Contraction and the classical\nOdd Cycle Transversal (OCT) problem, the methods developed to tackle OCT do not\nseem to be directly applicable to Bipartite Contraction. Our algorithm is based\non a novel combination of the irrelevant vertex technique, introduced by\nRobertson and Seymour, and the concept of important separators. Both techniques\nhave previously been used as key components of algorithms for fundamental\nproblems in parameterized complexity. However, to the best of our knowledge,\nthis is the first time the two techniques are applied in unison."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5450v1", 
    "other_authors": "Inge Li Goertz, Viswanath Nagarajan, R. Ravi", 
    "title": "Minimum Makespan Multi-vehicle Dial-a-Ride", 
    "arxiv-id": "1102.5450v1", 
    "author": "R. Ravi", 
    "publish": "2011-02-26T21:37:08Z", 
    "summary": "Dial a ride problems consist of a metric space (denoting travel time between\nvertices) and a set of m objects represented as source-destination pairs, where\neach object requires to be moved from its source to destination vertex. We\nconsider the multi-vehicle Dial a ride problem, with each vehicle having\ncapacity k and its own depot-vertex, where the objective is to minimize the\nmaximum completion time (makespan) of the vehicles. We study the \"preemptive\"\nversion of the problem, where an object may be left at intermediate vertices\nand transported by more than one vehicle, while being moved from source to\ndestination. Our main results are an O(log^3 n)-approximation algorithm for\npreemptive multi-vehicle Dial a ride, and an improved O(log t)-approximation\nfor its special case when there is no capacity constraint. We also show that\nthe approximation ratios improve by a log-factor when the underlying metric is\ninduced by a fixed-minor-free graph."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1102.5540v2", 
    "other_authors": "Michael Mitzenmacher, Thomas Steinke, Justin Thaler", 
    "title": "Hierarchical Heavy Hitters with the Space Saving Algorithm", 
    "arxiv-id": "1102.5540v2", 
    "author": "Justin Thaler", 
    "publish": "2011-02-27T19:31:05Z", 
    "summary": "The Hierarchical Heavy Hitters problem extends the notion of frequent items\nto data arranged in a hierarchy. This problem has applications to network\ntraffic monitoring, anomaly detection, and DDoS detection. We present a new\nstreaming approximation algorithm for computing Hierarchical Heavy Hitters that\nhas several advantages over previous algorithms. It improves on the worst-case\ntime and space bounds of earlier algorithms, is conceptually simple and\nsubstantially easier to implement, offers improved accuracy guarantees, is\neasily adopted to a distributed or parallel setting, and can be efficiently\nimplemented in commodity hardware such as ternary content addressable memory\n(TCAMs). We present experimental results showing that for parameters of primary\npractical interest, our two-dimensional algorithm is superior to existing\nalgorithms in terms of speed and accuracy, and competitive in terms of space,\nwhile our one-dimensional algorithm is also superior in terms of speed and\naccuracy for a more limited range of parameters."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0106v1", 
    "other_authors": "Enver Kayaaslan, Ali Pinar, Umit V. Catalyurek, Cevdet Aykanat", 
    "title": "Hypergraph Partitioning through Vertex Separators on Graphs", 
    "arxiv-id": "1103.0106v1", 
    "author": "Cevdet Aykanat", 
    "publish": "2011-03-01T08:42:24Z", 
    "summary": "The modeling flexibility provided by hypergraphs has drawn a lot of interest\nfrom the combinatorial scientific community, leading to novel models and\nalgorithms, their applications, and development of associated tools.\nHypergraphs are now a standard tool in combinatorial scientific computing. The\nmodeling flexibility of hypergraphs however, comes at a cost: algorithms on\nhypergraphs are inherently more complicated than those on graphs, which\nsometimes translate to nontrivial increases in processing times. Neither the\nmodeling flexibility of hypergraphs, nor the runtime efficiency of graph\nalgorithms can be overlooked. Therefore, the new research thrust should be how\nto cleverly trade-off between the two. This work addresses one method for this\ntrade-off by solving the hypergraph partitioning problem by finding vertex\nseparators on graphs. Specifically, we investigate how to solve the hypergraph\npartitioning problem by seeking a vertex separator on its net intersection\ngraph (NIG), where each net of the hypergraph is represented by a vertex, and\ntwo vertices share an edge if their nets have a common vertex. We propose a\nvertex-weighting scheme to attain good node-balanced hypergraphs, since NIG\nmodel cannot preserve node balancing information. Vertex-removal and\nvertex-splitting techniques are described to optimize cutnet and connectivity\nmetrics, respectively, under the recursive bipartitioning paradigm. We also\ndeveloped an implementation for our GPVS-based HP formulations by adopting and\nmodifying a state-of-the-art GPVS tool onmetis. Experiments conducted on a\nlarge collection of sparse matrices confirmed the validity of our proposed\ntechniques."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0318v1", 
    "other_authors": "David Eppstein, Darren Strash", 
    "title": "Listing All Maximal Cliques in Large Sparse Real-World Graphs", 
    "arxiv-id": "1103.0318v1", 
    "author": "Darren Strash", 
    "publish": "2011-03-02T00:00:26Z", 
    "summary": "We implement a new algorithm for listing all maximal cliques in sparse graphs\ndue to Eppstein, L\\\"offler, and Strash (ISAAC 2010) and analyze its performance\non a large corpus of real-world graphs. Our analysis shows that this algorithm\nis the first to offer a practical solution to listing all maximal cliques in\nlarge sparse graphs. All other theoretically-fast algorithms for sparse graphs\nhave been shown to be significantly slower than the algorithm of Tomita et al.\n(Theoretical Computer Science, 2006) in practice. However, the algorithm of\nTomita et al. uses an adjacency matrix, which requires too much space for large\nsparse graphs. Our new algorithm opens the door for fast analysis of large\nsparse graphs whose adjacency matrix will not fit into working memory."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0337v1", 
    "other_authors": "Duc-Phong Le, Chao-Liang Liu", 
    "title": "Refinements of Miller's Algorithm over Weierstrass Curves Revisited", 
    "arxiv-id": "1103.0337v1", 
    "author": "Chao-Liang Liu", 
    "publish": "2011-03-02T02:50:33Z", 
    "summary": "In 1986 Victor Miller described an algorithm for computing the Weil pairing\nin his unpublished manuscript. This algorithm has then become the core of all\npairing-based cryptosystems. Many improvements of the algorithm have been\npresented. Most of them involve a choice of elliptic curves of a \\emph{special}\nforms to exploit a possible twist during Tate pairing computation. Other\nimprovements involve a reduction of the number of iterations in the Miller's\nalgorithm. For the generic case, Blake, Murty and Xu proposed three refinements\nto Miller's algorithm over Weierstrass curves. Though their refinements which\nonly reduce the total number of vertical lines in Miller's algorithm, did not\ngive an efficient computation as other optimizations, but they can be applied\nfor computing \\emph{both} of Weil and Tate pairings on \\emph{all}\npairing-friendly elliptic curves. In this paper we extend the Blake-Murty-Xu's\nmethod and show how to perform an elimination of all vertical lines in Miller's\nalgorithm during Weil/Tate pairings computation on \\emph{general} elliptic\ncurves. Experimental results show that our algorithm is faster about 25% in\ncomparison with the original Miller's algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0534v1", 
    "other_authors": "Marek Cygan, Jesper Nederlof, Marcin Pilipczuk, Micha\u0142 Pilipczuk, Johan van Rooij, Jakub Onufry Wojtaszczyk", 
    "title": "Solving connectivity problems parameterized by treewidth in single   exponential time", 
    "arxiv-id": "1103.0534v1", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2011-03-02T20:36:20Z", 
    "summary": "For the vast majority of local graph problems standard dynamic programming\ntechniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input\ngraph. On the other hand, for problems with a global requirement (usually\nconnectivity) the best-known algorithms were naive dynamic programming schemes\nrunning in tw^O(tw) V^O(1) time.\n  We breach this gap by introducing a technique we dubbed Cut&Count that allows\nto produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type\nproblems, including Hamiltonian Path, Feedback Vertex Set and Connected\nDominating Set, consequently answering the question raised by Lokshtanov, Marx\nand Saurabh [SODA'11] in a surprising way. We also show that (under reasonable\ncomplexity assumptions) the gap cannot be breached for some problems for which\nCut&Count does not work, like CYCLE PACKING.\n  The constant c we obtain is in all cases small (at most 4 for undirected\nproblems and at most 6 for directed ones), and in several cases we are able to\nshow that improving those constants would cause the Strong Exponential Time\nHypothesis to fail.\n  Our results have numerous consequences in various fields, like FPT\nalgorithms, exact and approximate algorithms on planar and H-minor-free graphs\nand algorithms on graphs of bounded degree. In all these fields we are able to\nimprove the best-known results for some problems."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0985v1", 
    "other_authors": "Inge Li Goertz, Viswanath Nagarajan", 
    "title": "Locating Depots for Capacitated Vehicle Routing", 
    "arxiv-id": "1103.0985v1", 
    "author": "Viswanath Nagarajan", 
    "publish": "2011-03-04T21:15:44Z", 
    "summary": "We study a location-routing problem in the context of capacitated vehicle\nrouting. The input is a set of demand locations in a metric space and a fleet\nof k vehicles each of capacity Q. The objective is to locate k depots, one for\neach vehicle, and compute routes for the vehicles so that all demands are\nsatisfied and the total cost is minimized. Our main result is a constant-factor\napproximation algorithm for this problem. To achieve this result, we reduce to\nthe k-median-forest problem, which generalizes both k-median and minimum\nspanning tree, and which might be of independent interest. We give a\n(3+c)-approximation algorithm for k-median-forest, which leads to a\n(12+c)-approximation algorithm for the above location-routing problem, for any\nconstant c>0. The algorithm for k-median-forest is just t-swap local search,\nand we prove that it has locality gap 3+2/t; this generalizes the corresponding\nresult known for k-median. Finally we consider the \"non-uniform\"\nk-median-forest problem which has different cost functions for the MST and\nk-median parts. We show that the locality gap for this problem is unbounded\neven under multi-swaps, which contrasts with the uniform case. Nevertheless, we\nobtain a constant-factor approximation algorithm, using an LP based approach."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.0995v3", 
    "other_authors": "Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail", 
    "title": "Near-Optimal Column-Based Matrix Reconstruction", 
    "arxiv-id": "1103.0995v3", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2011-03-04T23:50:35Z", 
    "summary": "We consider low-rank reconstruction of a matrix using its columns and we\npresent asymptotically optimal algorithms for both spectral norm and Frobenius\nnorm reconstruction. The main tools we introduce to obtain our r esults are:\n(i) the use of fast approximate SVD-like decompositions for column\nreconstruction, and (ii) two deter ministic algorithms for selecting rows from\nmatrices with orthonormal columns, building upon the sparse represen tation\ntheorem for decompositions of the identity that appeared in \\cite{BSS09}."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.1091v2", 
    "other_authors": "J\u00e1n Katrenic, Gabriel Semanisin", 
    "title": "A generalization of Hopcroft-Karp algorithm for semi-matchings and   covers in bipartite graphs", 
    "arxiv-id": "1103.1091v2", 
    "author": "Gabriel Semanisin", 
    "publish": "2011-03-06T00:54:03Z", 
    "summary": "An $(f,g)$-semi-matching in a bipartite graph $G=(U \\cup V,E)$ is a set of\nedges $M \\subseteq E$ such that each vertex $u\\in U$ is incident with at most\n$f(u)$ edges of $M$, and each vertex $v\\in V$ is incident with at most $g(v)$\nedges of $M$. In this paper we give an algorithm that for a graph with $n$\nvertices and $m$ edges, $n\\le m$, constructs a maximum $(f,g)$-semi-matching in\nrunning time $O(m\\cdot \\min (\\sqrt{\\sum_{u\\in U}f(u)}, \\sqrt{\\sum_{v\\in\nV}g(v)}))$. Using the reduction of [5], our result on maximum\n$(f,g)$-semi-matching problem directly implies an algorithm for the optimal\nsemi-matching problem with running time $O(\\sqrt{n}m \\log n)$."
},{
    "category": "cs.DS", 
    "doi": "10.1142/S0129054112400175", 
    "link": "http://arxiv.org/pdf/1103.1109v4", 
    "other_authors": "Surender Baswana, Manoj Gupta, Sandeep Sen", 
    "title": "Fully dynamic maximal matching in O(log n) update time", 
    "arxiv-id": "1103.1109v4", 
    "author": "Sandeep Sen", 
    "publish": "2011-03-06T08:38:10Z", 
    "summary": "We present an algorithm for maintaining maximal matching in a graph under\naddition and deletion of edges. Our data structure is randomized that takes\nO(log n) expected amortized time for each edge update where n is the number of\nvertices in the graph. While there is a trivial O(n) algorithm for edge update,\nthe previous best known result for this problem for a graph with n vertices and\nm edges is O({(n+ m)}^{0.7072})which is sub-linear only for a sparse graph.\n  For the related problem of maximum matching, Onak and Rubinfield designed a\nrandomized data structure that achieves O(log^2 n) amortized time for each\nupdate for maintaining a c-approximate maximum matching for some large constant\nc. In contrast, we can maintain a factor two approximate maximum matching in\nO(log n) expected time per update as a direct corollary of the maximal matching\nscheme. This in turn also implies a two approximate vertex cover maintenance\nscheme that takes O(log n) expected time per update."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.1503v1", 
    "other_authors": "Stefan Canzar, Nora C. Toussaint, Gunnar W. Klau", 
    "title": "An Exact Algorithm for Side-Chain Placement in Protein Design", 
    "arxiv-id": "1103.1503v1", 
    "author": "Gunnar W. Klau", 
    "publish": "2011-03-08T12:32:12Z", 
    "summary": "Computational protein design aims at constructing novel or improved functions\non the structure of a given protein backbone and has important applications in\nthe pharmaceutical and biotechnical industry. The underlying combinatorial\nside-chain placement problem consists of choosing a side-chain placement for\neach residue position such that the resulting overall energy is minimum. The\nchoice of the side-chain then also determines the amino acid for this position.\nMany algorithms for this NP-hard problem have been proposed in the context of\nhomology modeling, which, however, reach their limits when faced with large\nprotein design instances.\n  In this paper, we propose a new exact method for the side-chain placement\nproblem that works well even for large instance sizes as they appear in protein\ndesign. Our main contribution is a dedicated branch-and-bound algorithm that\ncombines tight upper and lower bounds resulting from a novel Lagrangian\nrelaxation approach for side-chain placement. Our experimental results show\nthat our method outperforms alternative state-of-the art exact approaches and\nmakes it possible to optimally solve large protein design instances routinely."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.1771v1", 
    "other_authors": "Dennis Schieferdecker, Markus V\u00f6lker, Dorothea Wagner", 
    "title": "Efficient Algorithms for Distributed Detection of Holes and Boundaries   in Wireless Networks", 
    "arxiv-id": "1103.1771v1", 
    "author": "Dorothea Wagner", 
    "publish": "2011-03-09T13:08:13Z", 
    "summary": "We propose two novel algorithms for distributed and location-free boundary\nrecognition in wireless sensor networks. Both approaches enable a node to\ndecide autonomously whether it is a boundary node, based solely on connectivity\ninformation of a small neighborhood. This makes our algorithms highly\napplicable for dynamic networks where nodes can move or become inoperative.\n  We compare our algorithms qualitatively and quantitatively with several\nprevious approaches. In extensive simulations, we consider various models and\nscenarios. Although our algorithms use less information than most other\napproaches, they produce significantly better results. They are very robust\nagainst variations in node degree and do not rely on simplified assumptions of\nthe communication model. Moreover, they are much easier to implement on real\nsensor nodes than most existing approaches."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2102v2", 
    "other_authors": "Michael Gnewuch, Magnus Wahlstr\u00f6m, Carola Winzen", 
    "title": "A Randomized Algorithm Based on Threshold Accepting to Approximate the   Star Discrepancy", 
    "arxiv-id": "1103.2102v2", 
    "author": "Carola Winzen", 
    "publish": "2011-03-10T18:50:01Z", 
    "summary": "We present a new algorithm for estimating the star discrepancy of arbitrary\npoint sets. Similar to the algorithm for discrepancy approximation of Winker\nand Fang [SIAM J. Numer. Anal. 34 (1997), 2028--2042] it is based on the\noptimization algorithm threshold accepting. Our improvements include, amongst\nothers, a non-uniform sampling strategy which is more suited for\nhigher-dimensional inputs, and rounding steps which transform axis-parallel\nboxes, on which the discrepancy is to be tested, into \\emph{critical test\nboxes}. These critical test boxes provably yield higher discrepancy values, and\ncontain the box that exhibits the maximum value of the local discrepancy. We\nprovide comprehensive experiments to test the new algorithm. Our randomized\nalgorithm computes the exact discrepancy frequently in all cases where this can\nbe checked (i.e., where the exact discrepancy of the point set can be computed\nin feasible time). Most importantly, in higher dimension the new method behaves\nclearly better than all previously known methods."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2167v3", 
    "other_authors": "Djamal Belazzougui", 
    "title": "Improved space-time tradeoffs for approximate full-text indexing with   one edit error", 
    "arxiv-id": "1103.2167v3", 
    "author": "Djamal Belazzougui", 
    "publish": "2011-03-10T23:25:45Z", 
    "summary": "In this paper we are interested in indexing texts for substring matching\nqueries with one edit error. That is, given a text $T$ of $n$ characters over\nan alphabet of size $\\sigma$, we are asked to build a data structure that\nanswers the following query: find all the $occ$ substrings of the text that are\nat edit distance at most $1$ from a given string $q$ of length $m$. In this\npaper we show two new results for this problem. The first result, suitable for\nan unbounded alphabet, uses $O(n\\log^\\epsilon n)$ (where $\\epsilon$ is any\nconstant such that $0<\\epsilon<1$) words of space and answers to queries in\ntime $O(m+occ)$. This improves simultaneously in space and time over the result\nof Cole et al. The second result, suitable only for a constant alphabet, relies\non compressed text indices and comes in two variants: the first variant uses\n$O(n\\log^{\\epsilon} n)$ bits of space and answers to queries in time\n$O(m+occ)$, while the second variant uses $O(n\\log\\log n)$ bits of space and\nanswers to queries in time $O((m+occ)\\log\\log n)$. This second result improves\non the previously best results for constant alphabets achieved in Lam et al.\n(Algorithmica 2008) and Chan et al. (Algorithmica 2010)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2275v1", 
    "other_authors": "Marek Cygan, \u0141ukasz Kowalik", 
    "title": "Channel Assignment via Fast Zeta Transform", 
    "arxiv-id": "1103.2275v1", 
    "author": "\u0141ukasz Kowalik", 
    "publish": "2011-03-11T14:12:12Z", 
    "summary": "We show an O*((l+1)^n)-time algorithm for the channel assignment problem,\nwhere l is the maximum edge weight. This improves on the previous\nO*((l+2)^n)-time algorithm by Kral, as well as algorithms for important special\ncases, like L(2,1)-labelling. For the latter problem, our algorithm works in\nO*(3^n) time. The progress is achieved by applying the fast zeta transform in\ncombination with the inclusion-exclusion principle."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2429v1", 
    "other_authors": "Carola Winzen", 
    "title": "Direction-Reversing Quasi-Random Rumor Spreading with Restarts", 
    "arxiv-id": "1103.2429v1", 
    "author": "Carola Winzen", 
    "publish": "2011-03-12T09:25:23Z", 
    "summary": "In a recent work, Doerr and Fouz [\\emph{Asymptotically Optimal Randomized\nRumor Spreading}, in ArXiv] present a new quasi-random PUSH algorithm for the\nrumor spreading problem (also known as gossip spreading or message propagation\nproblem). Their \\emph{hybrid protocol} outperforms all known PUSH protocols.\n  In this work, we add to the hybrid protocol a direction-reversing element. We\nshow that this \\emph{direction-reversing quasi-random rumor spreading protocol\nwith random restarts} yields a constant factor improvement over the hybrid\nmodel, if we allow the same dose of randomness.\n  Put differently, our protocol achieves the same broadcasting time as the\nhybrid model by employing only (roughly) half the number of random choices."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s11590-011-0308-0", 
    "link": "http://arxiv.org/pdf/1103.2581v1", 
    "other_authors": "Hiro Ito, Shin-ichi Tanigawa, Yuichi Yoshida", 
    "title": "Constant-Time Algorithms for Sparsity Matroids", 
    "arxiv-id": "1103.2581v1", 
    "author": "Yuichi Yoshida", 
    "publish": "2011-03-14T03:51:27Z", 
    "summary": "A graph $G=(V,E)$ is called $(k,\\ell)$-full if $G$ contains a subgraph\n$H=(V,F)$ of $k|V|-\\ell$ edges such that, for any non-empty $F' \\subseteq F$,\n$|F'| \\leq k|V(F')| - \\ell$ holds. Here, $V(F')$ denotes the set of vertices\nincident to $F'$. It is known that the family of edge sets of $(k,\\ell)$-full\ngraphs forms a family of matroid, known as the sparsity matroid of $G$. In this\npaper, we give a constant-time approximation algorithm for the rank of the\nsparsity matroid of a degree-bounded undirected graph. This leads to a\nconstant-time tester for $(k,\\ell)$-fullness in the bounded-degree model,\n(i.e., we can decide with high probability whether an input graph satisfies a\nproperty $P$ or far from $P$). Depending on the values of $k$ and $\\ell$, it\ncan test various properties of a graph such as connectivity, rigidity, and how\nmany spanning trees can be packed. Based on this result, we also propose a\nconstant-time tester for $(k,\\ell)$-edge-connected-orientability in the\nbounded-degree model, where an undirected graph $G$ is called\n$(k,\\ell)$-edge-connected-orientable if there exists an orientation $\\vec{G}$\nof $G$ with a vertex $r \\in V$ such that $\\vec{G}$ contains $k$ arc-disjoint\ndipaths from $r$ to each vertex $v \\in V$ and $\\ell$ arc-disjoint dipaths from\neach vertex $v \\in V$ to $r$. A tester is called a one-sided error tester for\n$P$ if it always accepts a graph satisfying $P$. We show, for $k \\geq 2$ and\n(proper) $\\ell \\geq 0$, any one-sided error tester for $(k,\\ell)$-fullness and\n$(k,\\ell)$-edge-connected-orientability requires $\\Omega(n)$ queries."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.2613v1", 
    "other_authors": "Roman Kolpakov, Gregory Kucherov, Tatiana Starikovskaya", 
    "title": "Linear pattern matching on sparse suffix trees", 
    "arxiv-id": "1103.2613v1", 
    "author": "Tatiana Starikovskaya", 
    "publish": "2011-03-14T09:49:10Z", 
    "summary": "Packing several characters into one computer word is a simple and natural way\nto compress the representation of a string and to speed up its processing.\nExploiting this idea, we propose an index for a packed string, based on a {\\em\nsparse suffix tree} \\cite{KU-96} with appropriately defined suffix links.\nAssuming, under the standard unit-cost RAM model, that a word can store up to\n$\\log_{\\sigma}n$ characters ($\\sigma$ the alphabet size), our index takes\n$O(n/\\log_{\\sigma}n)$ space, i.e. the same space as the packed string itself.\nThe resulting pattern matching algorithm runs in time $O(m+r^2+r\\cdot occ)$,\nwhere $m$ is the length of the pattern, $r$ is the actual number of characters\nstored in a word and $occ$ is the number of pattern occurrences."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.3114v2", 
    "other_authors": "Keisuke Goto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", 
    "title": "Fast $q$-gram Mining on SLP Compressed Strings", 
    "arxiv-id": "1103.3114v2", 
    "author": "Masayuki Takeda", 
    "publish": "2011-03-16T07:16:10Z", 
    "summary": "We present simple and efficient algorithms for calculating $q$-gram\nfrequencies on strings represented in compressed form, namely, as a straight\nline program (SLP). Given an SLP of size $n$ that represents string $T$, we\npresent an $O(qn)$ time and space algorithm that computes the occurrence\nfrequencies of $q$-grams in $T$. Computational experiments show that our\nalgorithm and its variation are practical for small $q$, actually running\nfaster on various real string data, compared to algorithms that work on the\nuncompressed text. We also discuss applications in data mining and\nclassification of string data, for which our algorithms can be useful."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.4566v2", 
    "other_authors": "Erez Kantor, Zvi Lotker, Merav Parter, David Peleg", 
    "title": "The Topology of Wireless Communication", 
    "arxiv-id": "1103.4566v2", 
    "author": "David Peleg", 
    "publish": "2011-03-23T16:03:19Z", 
    "summary": "In this paper we study the topological properties of wireless communication\nmaps and their usability in algorithmic design. We consider the SINR model,\nwhich compares the received power of a signal at a receiver against the sum of\nstrengths of other interfering signals plus background noise. To describe the\nbehavior of a multi-station network, we use the convenient representation of a\n\\emph{reception map}. In the SINR model, the resulting \\emph{SINR diagram}\npartitions the plane into reception zones, one per station, and the\ncomplementary region of the plane where no station can be heard. We consider\nthe general case where transmission energies are arbitrary (or non-uniform).\nUnder that setting, the reception zones are not necessarily convex or even\nconnected. This poses the algorithmic challenge of designing efficient point\nlocation techniques as well as the theoretical challenge of understanding the\ngeometry of SINR diagrams. We achieve several results in both directions. We\nestablish a form of weaker convexity in the case where stations are aligned on\na line. In addition, one of our key results concerns the behavior of a\n$(d+1)$-dimensional map. Specifically, although the $d$-dimensional map might\nbe highly fractured, drawing the map in one dimension higher \"heals\" the zones,\nwhich become connected. In addition, as a step toward establishing a weaker\nform of convexity for the $d$-dimensional map, we study the interference\nfunction and show that it satisfies the maximum principle. Finally, we turn to\nconsider algorithmic applications, and propose a new variant of approximate\npoint location."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.4875v2", 
    "other_authors": "Isabelle Stanton, Ali Pinar", 
    "title": "Constructing and Sampling Graphs with a Prescribed Joint Degree   Distribution", 
    "arxiv-id": "1103.4875v2", 
    "author": "Ali Pinar", 
    "publish": "2011-03-24T21:05:17Z", 
    "summary": "One of the most influential recent results in network analysis is that many\nnatural networks exhibit a power-law or log-normal degree distribution. This\nhas inspired numerous generative models that match this property. However, more\nrecent work has shown that while these generative models do have the right\ndegree distribution, they are not good models for real life networks due to\ntheir differences on other important metrics like conductance. We believe this\nis, in part, because many of these real-world networks have very different\njoint degree distributions, i.e. the probability that a randomly selected edge\nwill be between nodes of degree k and l. Assortativity is a sufficient\nstatistic of the joint degree distribution, and it has been previously noted\nthat social networks tend to be assortative, while biological and technological\nnetworks tend to be disassortative.\n  We suggest understanding the relationship between network structure and the\njoint degree distribution of graphs is an interesting avenue of further\nresearch. An important tool for such studies are algorithms that can generate\nrandom instances of graphs with the same joint degree distribution. This is the\nmain topic of this paper and we study the problem from both a theoretical and\npractical perspective. We provide an algorithm for constructing simple graphs\nfrom a given joint degree distribution, and a Monte Carlo Markov Chain method\nfor sampling them. We also show that the state space of simple graphs with a\nfixed degree distribution is connected via end point switches. We empirically\nevaluate the mixing time of this Markov Chain by using experiments based on the\nautocorrelation of each edge. These experiments show that our Markov Chain\nmixes quickly on real graphs, allowing for utilization of our techniques in\npractice."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5453v1", 
    "other_authors": "Malik Magdon-Ismail", 
    "title": "Using a Non-Commutative Bernstein Bound to Approximate Some Matrix   Algorithms in the Spectral Norm", 
    "arxiv-id": "1103.5453v1", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2011-03-28T19:37:20Z", 
    "summary": "We focus on \\emph{row sampling} based approximations for matrix algorithms,\nin particular matrix multipication, sparse matrix reconstruction, and\n\\math{\\ell_2} regression. For \\math{\\matA\\in\\R^{m\\times d}} (\\math{m} points in\n\\math{d\\ll m} dimensions), and appropriate row-sampling probabilities, which\ntypically depend on the norms of the rows of the \\math{m\\times d} left singular\nmatrix of \\math{\\matA} (the \\emph{leverage scores}), we give row-sampling\nalgorithms with linear (up to polylog factors) dependence on the stable rank of\n\\math{\\matA}. This result is achieved through the application of\nnon-commutative Bernstein bounds. Keywords: row-sampling; matrix\nmultiplication; matrix reconstruction; estimating spectral norm; linear\nregression; randomized"
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5599v2", 
    "other_authors": "St\u00e9phane Bessy, Anthony Perez", 
    "title": "Polynomial kernels for Proper Interval Completion and related problems", 
    "arxiv-id": "1103.5599v2", 
    "author": "Anthony Perez", 
    "publish": "2011-03-29T10:45:31Z", 
    "summary": "Given a graph G = (V,E) and a positive integer k, the Proper Interval\nCompletion problem asks whether there exists a set F of at most k pairs of (V\n\\times V)\\E such that the graph H = (V,E \\cup F) is a proper interval graph.\nThe Proper Interval Completion problem finds applications in molecular biology\nand genomic research. First announced by Kaplan, Tarjan and Shamir in FOCS '94,\nthis problem is known to be FPT, but no polynomial kernel was known to exist.\nWe settle this question by proving that Proper Interval Completion admits a\nkernel with at most O(k^5) vertices. Moreover, we prove that a related problem,\nthe so-called Bipartite Chain Deletion problem, admits a kernel with at most\nO(k^2) vertices, completing a previous result of Guo."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.5609v1", 
    "other_authors": "Uriel Feige, Daniel Reichman", 
    "title": "Recoverable Values for Independent Sets", 
    "arxiv-id": "1103.5609v1", 
    "author": "Daniel Reichman", 
    "publish": "2011-03-29T11:47:04Z", 
    "summary": "The notion of {\\em recoverable value} was advocated in work of Feige,\nImmorlica, Mirrokni and Nazerzadeh [Approx 2009] as a measure of quality for\napproximation algorithms. There this concept was applied to facility location\nproblems. In the current work we apply a similar framework to the maximum\nindependent set problem (MIS). We say that an approximation algorithm has {\\em\nrecoverable value} $\\rho$, if for every graph it recovers an independent set of\nsize at least $\\max_I \\sum_{v\\in I} \\min[1,\\rho/(d(v) + 1)]$, where $d(v)$ is\nthe degree of vertex $v$, and $I$ ranges over all independent sets in $G$.\nHence, in a sense, from every vertex $v$ in the maximum independent set the\nalgorithm recovers a value of at least $\\rho/(d_v + 1)$ towards the solution.\nThis quality measure is most effective in graphs in which the maximum\nindependent set is composed of low degree vertices. It easily follows from\nknown results that some simple algorithms for MIS ensure $\\rho \\ge 1$. We\ndesign a new randomized algorithm for MIS that ensures an expected recoverable\nvalue of at least $\\rho \\ge 7/3$. In addition, we show that approximating MIS\nin graphs with a given $k$-coloring within a ratio larger than $2/k$ is unique\ngames hard. This rules out a natural approach for obtaining $\\rho \\ge 2$."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1103.6246v2", 
    "other_authors": "Bob L. Sturm", 
    "title": "Sparse Vector Distributions and Recovery from Compressed Sensing", 
    "arxiv-id": "1103.6246v2", 
    "author": "Bob L. Sturm", 
    "publish": "2011-03-31T17:26:34Z", 
    "summary": "It is well known that the performance of sparse vector recovery algorithms\nfrom compressive measurements can depend on the distribution underlying the\nnon-zero elements of a sparse vector. However, the extent of these effects has\nyet to be explored, and formally presented. In this paper, I empirically\ninvestigate this dependence for seven distributions and fifteen recovery\nalgorithms. The two morals of this work are: 1) any judgement of the recovery\nperformance of one algorithm over that of another must be prefaced by the\nconditions for which this is observed to be true, including sparse vector\ndistributions, and the criterion for exact recovery; and 2) a recovery\nalgorithm must be selected carefully based on what distribution one expects to\nunderlie the sensed sparse signal."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0187v1", 
    "other_authors": "Rakesh Mohanty, Sasmita Tripathy", 
    "title": "An Improved Move-To-Front(IMTF) Off-line Algorithm for the List   Accessing Problem", 
    "arxiv-id": "1105.0187v1", 
    "author": "Sasmita Tripathy", 
    "publish": "2011-05-01T17:17:17Z", 
    "summary": "For the List Accessing Problem, Move-To-Front(MTF) algorithm has been proved\nto be the best performing online list accessing algorithm till date in the\nliterature[10]. In this paper, we have made a comprehensive analysis of MTF\nalgorithm and developed an Improved-MTF (IMTF) offline algorithm. We have\ngenerated two new types of data set and devise a new method of experimental\nanalysis for our proposed algorithm. Our experimental analysis shows that IMTF\nis performing better than MTF algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0464v3", 
    "other_authors": "Christos Boutsidis", 
    "title": "Improved Low-rank Matrix Decompositions via the Subsampled Randomized   Hadamard Transform", 
    "arxiv-id": "1105.0464v3", 
    "author": "Christos Boutsidis", 
    "publish": "2011-05-03T01:39:02Z", 
    "summary": "We comment on two randomized algorithms for constructing low-rank matrix\ndecompositions. Both algorithms employ the Subsampled Randomized Hadamard\nTransform [14]. The first algorithm appeared recently in [9]; here, we provide\na novel analysis that significantly improves the approximation bound obtained\nin [9]. A preliminary version of the second algorithm appeared in [7]; here, we\npresent a mild modification of this algorithm that achieves the same\napproximation bound but significantly improves the corresponding running time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/CCP.2011.45", 
    "link": "http://arxiv.org/pdf/1105.0477v3", 
    "other_authors": "Bingkai Lin, Yijia Chen", 
    "title": "The parameterized complexity of k-edge induced subgraphs", 
    "arxiv-id": "1105.0477v3", 
    "author": "Yijia Chen", 
    "publish": "2011-05-03T05:25:14Z", 
    "summary": "We prove that finding a $k$-edge induced subgraph is fixed-parameter\ntractable, thereby answering an open problem of Leizhen Cai. Our algorithm is\nbased on several combinatorial observations, Gauss' famous \\emph{Eureka}\ntheorem [Andrews, 86], and a generalization of the well-known fpt-algorithm for\nthe model-checking problem for first-order logic on graphs with locally bounded\ntree-width due to Frick and Grohe [Frick and Grohe, 01]. On the other hand, we\nshow that two natural counting versions of the problem are hard. Hence, the\n$k$-edge induced subgraph problem is one of the rare known examples in\nparameterized complexity that are easy for decision while hard for counting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.0608v1", 
    "other_authors": "Bang Ye Wu", 
    "title": "A simpler and more efficient algorithm for the next-to-shortest path   problem", 
    "arxiv-id": "1105.0608v1", 
    "author": "Bang Ye Wu", 
    "publish": "2011-05-03T15:27:00Z", 
    "summary": "Given an undirected graph $G=(V,E)$ with positive edge lengths and two\nvertices $s$ and $t$, the next-to-shortest path problem is to find an $st$-path\nwhich length is minimum amongst all $st$-paths strictly longer than the\nshortest path length. In this paper we show that the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nParticularly our new algorithm runs in $O(|V|\\log |V|+|E|)$ time for general\ngraphs, which improves the previous result of $O(|V|^2)$ time for sparse\ngraphs, and takes only linear time for unweighted graphs, planar graphs, and\ngraphs with positive integer edge lengths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.0709v1", 
    "other_authors": "Christos Boutsidis", 
    "title": "Topics in Matrix Sampling Algorithms", 
    "arxiv-id": "1105.0709v1", 
    "author": "Christos Boutsidis", 
    "publish": "2011-05-04T00:19:49Z", 
    "summary": "We study three fundamental problems of Linear Algebra, lying in the heart of\nvarious Machine Learning applications, namely: 1)\"Low-rank Column-based Matrix\nApproximation\". We are given a matrix A and a target rank k. The goal is to\nselect a subset of columns of A and, by using only these columns, compute a\nrank k approximation to A that is as good as the rank k approximation that\nwould have been obtained by using all the columns; 2) \"Coreset Construction in\nLeast-Squares Regression\". We are given a matrix A and a vector b. Consider the\n(over-constrained) least-squares problem of minimizing ||Ax-b||, over all\nvectors x in D. The domain D represents the constraints on the solution and can\nbe arbitrary. The goal is to select a subset of the rows of A and b and, by\nusing only these rows, find a solution vector that is as good as the solution\nvector that would have been obtained by using all the rows; 3) \"Feature\nSelection in K-means Clustering\". We are given a set of points described with\nrespect to a large number of features. The goal is to select a subset of the\nfeatures and, by using only this subset, obtain a k-partition of the points\nthat is as good as the partition that would have been obtained by using all the\nfeatures. We present novel algorithms for all three problems mentioned above.\nOur results can be viewed as follow-up research to a line of work known as\n\"Matrix Sampling Algorithms\". [Frieze, Kanna, Vempala, 1998] presented the\nfirst such algorithm for the Low-rank Matrix Approximation problem. Since then,\nsuch algorithms have been developed for several other problems, e.g. Graph\nSparsification and Linear Equation Solving. Our contributions to this line of\nresearch are: (i) improved algorithms for Low-rank Matrix Approximation and\nRegression (ii) algorithms for a new problem domain (K-means Clustering)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.1569v1", 
    "other_authors": "Bala G. Chandran, Dorit S. Hochbaum", 
    "title": "Practical and theoretical improvements for bipartite matching using the   pseudoflow algorithm", 
    "arxiv-id": "1105.1569v1", 
    "author": "Dorit S. Hochbaum", 
    "publish": "2011-05-09T02:03:40Z", 
    "summary": "We show that the pseudoflow algorithm for maximum flow is particularly\nefficient for the bipartite matching problem both in theory and in practice. We\ndevelop several implementations of the pseudoflow algorithm for bipartite\nmatching, and compare them over a wide set of benchmark instances to\nstate-of-the-art implementations of push-relabel and augmenting path algorithms\nthat are specifically designed to solve these problems. The experiments show\nthat the pseudoflow variants are in most cases faster than the other\nalgorithms.\n  We also show that one particular implementation---the matching pseudoflow\nalgorithm---is theoretically efficient. For a graph with $n$ nodes, $m$ arcs,\n$n_1$ the size of the smaller set in the bipartition, and the maximum matching\nvalue $\\kappa \\leq n_1$, the algorithm's complexity given input in the form of\nadjacency lists is $O(\\min{n_1\\kappa,m} + \\sqrt{\\kappa}\\min{\\kappa^2,m})$.\nSimilar algorithmic ideas are shown to work for an adaptation of Hopcroft and\nKarp's bipartite matching algorithm with the same complexity. Using boolean\noperations on words of size $\\lambda$, the complexity of the pseudoflow\nalgorithm is further improved to $O(\\min{n_1\\kappa, \\frac{n_1n_2}{\\lambda}, m}\n+ \\kappa^2 + \\frac{\\kappa^{2.5}}{\\lambda})$. This run time is faster than for\nprevious algorithms such as Cheriyan and Mehlhorn's algorithm of complexity\n$O(\\frac{n^{2.5}}{\\lambda})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.1622v1", 
    "other_authors": "Gianluca De Marco, Evangelos Kranakis, Gabor Wiener", 
    "title": "Computing Majority with Triple Queries", 
    "arxiv-id": "1105.1622v1", 
    "author": "Gabor Wiener", 
    "publish": "2011-05-09T10:26:01Z", 
    "summary": "Consider a bin containing $n$ balls colored with two colors. In a $k$-query,\n$k$ balls are selected by a questioner and the oracle's reply is related\n(depending on the computation model being considered) to the distribution of\ncolors of the balls in this $k$-tuple; however, the oracle never reveals the\ncolors of the individual balls. Following a number of queries the questioner is\nsaid to determine the majority color if it can output a ball of the majority\ncolor if it exists, and can prove that there is no majority if it does not\nexist. We investigate two computation models (depending on the type of replies\nbeing allowed). We give algorithms to compute the minimum number of 3-queries\nwhich are needed so that the questioner can determine the majority color and\nprovide tight and almost tight upper and lower bounds on the number of queries\nneeded in each case."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2391v4", 
    "other_authors": "Hyung-Chan An, David B. Shmoys", 
    "title": "LP-Based Approximation Algorithms for Traveling Salesman Path Problems", 
    "arxiv-id": "1105.2391v4", 
    "author": "David B. Shmoys", 
    "publish": "2011-05-12T07:30:50Z", 
    "summary": "This paper has been merged into 1110.4604."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2397v1", 
    "other_authors": "Bernhard Haeupler, Telikepalli Kavitha, Rogers Mathew, Siddhartha Sen, Robert Endre Tarjan", 
    "title": "Incremental Cycle Detection, Topological Ordering, and Strong Component   Maintenance", 
    "arxiv-id": "1105.2397v1", 
    "author": "Robert Endre Tarjan", 
    "publish": "2011-05-12T07:57:28Z", 
    "summary": "We present two on-line algorithms for maintaining a topological order of a\ndirected $n$-vertex acyclic graph as arcs are added, and detecting a cycle when\none is created. Our first algorithm handles $m$ arc additions in $O(m^{3/2})$\ntime. For sparse graphs ($m/n = O(1)$), this bound improves the best previous\nbound by a logarithmic factor, and is tight to within a constant factor among\nalgorithms satisfying a natural {\\em locality} property. Our second algorithm\nhandles an arbitrary sequence of arc additions in $O(n^{5/2})$ time. For\nsufficiently dense graphs, this bound improves the best previous bound by a\npolynomial factor. Our bound may be far from tight: we show that the algorithm\ncan take $\\Omega(n^2 2^{\\sqrt{2\\lg n}})$ time by relating its performance to a\ngeneralization of the $k$-levels problem of combinatorial geometry. A\ncompletely different algorithm running in $\\Theta(n^2 \\log n)$ time was given\nrecently by Bender, Fineman, and Gilbert. We extend both of our algorithms to\nthe maintenance of strong components, without affecting the asymptotic time\nbounds."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9601-7", 
    "link": "http://arxiv.org/pdf/1105.2686v2", 
    "other_authors": "Tobias Brunsch, Heiko R\u00f6glin, Cyriel Rutten, Tjark Vredeveld", 
    "title": "Smoothed Performance Guarantees for Local Search", 
    "arxiv-id": "1105.2686v2", 
    "author": "Tjark Vredeveld", 
    "publish": "2011-05-13T11:04:11Z", 
    "summary": "We study popular local search and greedy algorithms for scheduling. The\nperformance guarantee of these algorithms is well understood, but the\nworst-case lower bounds seem somewhat contrived and it is questionable if they\narise in practical applications. To find out how robust these bounds are, we\nstudy the algorithms in the framework of smoothed analysis, in which instances\nare subject to some degree of random noise.\n  While the lower bounds for all scheduling variants with restricted machines\nare rather robust, we find out that the bounds are fragile for unrestricted\nmachines. In particular, we show that the smoothed performance guarantee of the\njump and the lex-jump algorithm are (in contrast to the worst case) independent\nof the number of machines. They are Theta(phi) and Theta(log(phi)),\nrespectively, where 1/phi is a parameter measuring the magnitude of the\nperturbation. The latter immediately implies that also the smoothed price of\nanarchy is Theta(log(phi)) for routing games on parallel links. Additionally we\nshow that for unrestricted machines also the greedy list scheduling algorithm\nhas an approximation guarantee of Theta(log(phi))."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.2704v2", 
    "other_authors": "Gwena\u00ebl Joret, Christophe Paul, Ignasi Sau, Saket Saurabh, St\u00e9phan Thomass\u00e9", 
    "title": "Hitting and Harvesting Pumpkins", 
    "arxiv-id": "1105.2704v2", 
    "author": "St\u00e9phan Thomass\u00e9", 
    "publish": "2011-05-13T12:21:17Z", 
    "summary": "The \"c-pumpkin\" is the graph with two vertices linked by c>0 parallel edges.\nA c-pumpkin-model in a graph G is a pair A,B of disjoint subsets of vertices of\nG, each inducing a connected subgraph of G, such that there are at least c\nedges in G between A and B. We focus on covering and packing c-pumpkin-models\nin a given graph: On the one hand, we provide an FPT algorithm running in time\n2^O(k) n^O(1) deciding, for any fixed c>0, whether all c-pumpkin-models can be\ncovered by at most k vertices. This generalizes known single-exponential FPT\nalgorithms for Vertex Cover and Feedback Vertex Set, which correspond to the\ncases c=1,2 respectively. On the other hand, we present a O(log\nn)-approximation algorithm for both the problems of covering all\nc-pumpkin-models with a smallest number of vertices, and packing a maximum\nnumber of vertex-disjoint c-pumpkin-models."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.3748v1", 
    "other_authors": "Anupam Gupta, Ravishankar Krishnaswamy, Kirk Pruhs", 
    "title": "Scalably Scheduling Power-Heterogeneous Processors", 
    "arxiv-id": "1105.3748v1", 
    "author": "Kirk Pruhs", 
    "publish": "2011-05-18T20:51:32Z", 
    "summary": "We show that a natural online algorithm for scheduling jobs on a\nheterogeneous multiprocessor, with arbitrary power functions, is scalable for\nthe objective function of weighted flow plus energy."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.4250v1", 
    "other_authors": "Zeev Nutov", 
    "title": "Approximating subset $k$-connectivity problems", 
    "arxiv-id": "1105.4250v1", 
    "author": "Zeev Nutov", 
    "publish": "2011-05-21T11:55:36Z", 
    "summary": "A subset $T \\subseteq V$ of terminals is $k$-connected to a root $s$ in a\ndirected/undirected graph $J$ if $J$ has $k$ internally-disjoint $vs$-paths for\nevery $v \\in T$; $T$ is $k$-connected in $J$ if $T$ is $k$-connected to every\n$s \\in T$. We consider the {\\sf Subset $k$-Connectivity Augmentation} problem:\ngiven a graph $G=(V,E)$ with edge/node-costs, node subset $T \\subseteq V$, and\na subgraph $J=(V,E_J)$ of $G$ such that $T$ is $k$-connected in $J$, find a\nminimum-cost augmenting edge-set $F \\subseteq E \\setminus E_J$ such that $T$ is\n$(k+1)$-connected in $J \\cup F$. The problem admits trivial ratio $O(|T|^2)$.\nWe consider the case $|T|>k$ and prove that for directed/undirected graphs and\nedge/node-costs, a $\\rho$-approximation for {\\sf Rooted Subset $k$-Connectivity\nAugmentation} implies the following ratios for {\\sf Subset $k$-Connectivity\nAugmentation}: (i) $b(\\rho+k) + {(\\frac{3|T|}{|T|-k})}^2\nH(\\frac{3|T|}{|T|-k})$; (ii) $\\rho \\cdot O(\\frac{|T|}{|T|-k} \\log k)$, where\nb=1 for undirected graphs and b=2 for directed graphs, and $H(k)$ is the $k$th\nharmonic number. The best known values of $\\rho$ on undirected graphs are\n$\\min\\{|T|,O(k)\\}$ for edge-costs and $\\min\\{|T|,O(k \\log |T|)\\}$ for\nnode-costs; for directed graphs $\\rho=|T|$ for both versions. Our results imply\nthat unless $k=|T|-o(|T|)$, {\\sf Subset $k$-Connectivity Augmentation} admits\nthe same ratios as the best known ones for the rooted version. This improves\nthe ratios in \\cite{N-focs,L}."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120883736", 
    "link": "http://arxiv.org/pdf/1105.5718v1", 
    "other_authors": "Vojtech Prehnal", 
    "title": "Relational Schema Protocol (RSP)", 
    "arxiv-id": "1105.5718v1", 
    "author": "Vojtech Prehnal", 
    "publish": "2011-05-28T14:28:30Z", 
    "summary": "This document specifies the Relational Schema Protocol (RSP). RSP enables\nloosely coupled applications to share and exchange relational data. It defines\nfixed message format for an arbitrary relational schema so that the changes in\nthe data schema do not affect the message format. This prevents the interacting\napplications from having to be reimplemented during the data schema evolvement."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10878-012-9481-z", 
    "link": "http://arxiv.org/pdf/1105.5915v2", 
    "other_authors": "Bang Ye Wu", 
    "title": "Algorithms for the minimum non-separating path and the balanced   connected bipartition problems on grid graphs (With erratum)", 
    "arxiv-id": "1105.5915v2", 
    "author": "Bang Ye Wu", 
    "publish": "2011-05-30T09:33:36Z", 
    "summary": "For given a pair of nodes in a graph, the minimum non-separating path problem\nlooks for a minimum weight path between the two nodes such that the remaining\ngraph after removing the path is still connected. The balanced connected\nbipartition (BCP$_2$) problem looks for a way to bipartition a graph into two\nconnected subgraphs with their weights as equal as possible. In this paper we\npresent an algorithm in time $O(N\\log N)$ for finding a minimum weight\nnon-separating path between two given nodes in a grid graph of $N$ nodes with\npositive weight. This result leads to a 5/4-approximation algorithm for the\nBCP$_2$ problem on grid graphs, which is the currently best ratio achieved in\npolynomial time. We also developed an exact algorithm for the BCP$_2$ problem\non grid graphs. Based on the exact algorithm and a rounding technique, we show\nan approximation scheme, which is a fully polynomial time approximation scheme\nfor fixed number of rows."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0295v2", 
    "other_authors": "Benjamin Doerr, Leslie Ann Goldberg", 
    "title": "Adaptive Drift Analysis", 
    "arxiv-id": "1108.0295v2", 
    "author": "Leslie Ann Goldberg", 
    "publish": "2011-08-01T12:22:27Z", 
    "summary": "We show that, for any c>0, the (1+1) evolutionary algorithm using an\narbitrary mutation rate p_n = c/n finds the optimum of a linear objective\nfunction over bit strings of length n in expected time Theta(n log n).\nPreviously, this was only known for c at most 1. Since previous work also shows\nthat universal drift functions cannot exist for c larger than a certain\nconstant, we instead define drift functions which depend crucially on the\nrelevant objective functions (and also on c itself). Using these\ncarefully-constructed drift functions, we prove that the expected optimisation\ntime is Theta(n log n). By giving an alternative proof of the multiplicative\ndrift theorem, we also show that our optimisation-time bound holds with high\nprobability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0388v1", 
    "other_authors": "Fei Li", 
    "title": "A Comprehensive Study of an Online Packet Scheduling Algorithm", 
    "arxiv-id": "1108.0388v1", 
    "author": "Fei Li", 
    "publish": "2011-08-01T18:49:37Z", 
    "summary": "We study the \\emph{bounded-delay model} for Qualify-of-Service buffer\nmanagement. Time is discrete. There is a buffer. Unit-length jobs (also called\n\\emph{packets}) arrive at the buffer over time. Each packet has an integer\nrelease time, an integer deadline, and a positive real value. A packet's\ncharacteristics are not known to an online algorithm until the packet actually\narrives. In each time step, at most one packet can be sent out of the buffer.\nThe objective is to maximize the total value of the packets sent by their\nrespective deadlines in an online manner. An online algorithm's performance is\nusually measured in terms of \\emph{competitive ratio}, when this online\nalgorithm is compared with a clairvoyant algorithm achieving the best total\nvalue. In this paper, we study a simple and intuitive online algorithm. We\nanalyze its performance in terms of competitive ratio for the general model and\na few important variants."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0554v2", 
    "other_authors": "Wing-Kai Hon, Rahul Shah, Sharma V. Thankachan", 
    "title": "Towards an Optimal Space-and-Query-Time Index for Top-k Document   Retrieval", 
    "arxiv-id": "1108.0554v2", 
    "author": "Sharma V. Thankachan", 
    "publish": "2011-08-02T12:00:02Z", 
    "summary": "Let $\\D = $$ \\{d_1,d_2,...d_D\\}$ be a given set of $D$ string documents of\ntotal length $n$, our task is to index $\\D$, such that the $k$ most relevant\ndocuments for an online query pattern $P$ of length $p$ can be retrieved\nefficiently. We propose an index of size $|CSA|+n\\log D(2+o(1))$ bits and\n$O(t_{s}(p)+k\\log\\log n+poly\\log\\log n)$ query time for the basic relevance\nmetric \\emph{term-frequency}, where $|CSA|$ is the size (in bits) of a\ncompressed full text index of $\\D$, with $O(t_s(p))$ time for searching a\npattern of length $p$ . We further reduce the space to $|CSA|+n\\log D(1+o(1))$\nbits, however the query time will be $O(t_s(p)+k(\\log \\sigma \\log\\log\nn)^{1+\\epsilon}+poly\\log\\log n)$, where $\\sigma$ is the alphabet size and\n$\\epsilon >0$ is any constant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0810v2", 
    "other_authors": "Marek Cygan, Marcin Pilipczuk, Micha\u0142 Pilipczuk, Jakub Onufry Wojtaszczyk", 
    "title": "Scheduling partially ordered jobs faster than 2^n", 
    "arxiv-id": "1108.0810v2", 
    "author": "Jakub Onufry Wojtaszczyk", 
    "publish": "2011-08-03T10:12:03Z", 
    "summary": "In the SCHED problem we are given a set of n jobs, together with their\nprocessing times and precedence constraints. The task is to order the jobs so\nthat their total completion time is minimized. SCHED is a special case of the\nTraveling Repairman Problem with precedences. A natural dynamic programming\nalgorithm solves both these problems in 2^n n^O(1) time, and whether there\nexists an algorithms solving SCHED in O(c^n) time for some constant c < 2 was\nan open problem posted in 2004 by Woeginger. In this paper we answer this\nquestion positively."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.0866v1", 
    "other_authors": "Marcin Peczarski", 
    "title": "Towards Optimal Sorting of 16 Elements", 
    "arxiv-id": "1108.0866v1", 
    "author": "Marcin Peczarski", 
    "publish": "2011-08-03T15:24:49Z", 
    "summary": "One of the fundamental problem in the theory of sorting is to find the\npessimistic number of comparisons sufficient to sort a given number of\nelements. Currently 16 is the lowest number of elements for which we do not\nknow the exact value. We know that 46 comparisons suffices and that 44 do not.\nThere is an open question if 45 comparisons are sufficient. We present an\nattempt to resolve that problem by performing an exhaustive computer search. We\nalso present an algorithm for counting linear extensions which substantially\nspeeds up computations."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1060v1", 
    "other_authors": "Jos\u00e9 Luis L\u00f3pez-Presa, Antonio Fern\u00e1ndez Anta, Luis N\u00fa\u00f1ez Chiroque", 
    "title": "Conauto-2.0: Fast Isomorphism Testing and Automorphism Group Computation", 
    "arxiv-id": "1108.1060v1", 
    "author": "Luis N\u00fa\u00f1ez Chiroque", 
    "publish": "2011-08-04T12:16:27Z", 
    "summary": "In this paper we present an algorithm, called conauto-2.0, that can\nefficiently compute a set of generators of the automorphism group of a graph,\nand test whether two graphs are isomorphic, finding an isomorphism if they are.\nThis algorithm uses the basic individualization/refinement technique, and is an\nimproved version of the algorithm conauto, which has been shown to be very fast\nfor random graphs and several families of hard graphs. In this paper, it is\nproved that, under some circumstances, it is not only possible to prune the\nsearch space (using already found generators of the automorphism group), but\nalso to infer new generators without the need of explicitly finding an\nautomorphism of the graph. This result is especially suited for graphs with\nregularly connected components, and can be applied in any isomorphism testing\nand canonical labeling algorithm (that use the individualization/refinement\ntechnique) to significantly improve its performance. Additionally, a dynamic\ntarget cell selection function is used to adapt to different graphs. The\nresulting algorithm preserves all the nice features of conauto, but reduces the\ntime for testing graphs with regularly connected components and other hard\ngraph families. We run extensive experiments, which show that the most popular\nalgorithms (namely, nauty, bliss, Traces, and saucy) are slower than\nconauto-2.0, among others, for the graph families based on components."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1130v2", 
    "other_authors": "Marcin Mucha", 
    "title": "13/9-approximation for Graphic TSP", 
    "arxiv-id": "1108.1130v2", 
    "author": "Marcin Mucha", 
    "publish": "2011-08-04T16:23:48Z", 
    "summary": "The Travelling Salesman Problem is one the most fundamental and most studied\nproblems in approximation algorithms. For more than 30 years, the best\nalgorithm known for general metrics has been Christofides's algorithm with\napproximation factor of 3/2, even though the so-called Held-Karp LP relaxation\nof the problem is conjectured to have the integrality gap of only 4/3. Very\nrecently, significant progress has been made for the important special case of\ngraphic metrics, first by Oveis Gharan et al., and then by Momke and Svensson.\nIn this paper, we provide an improved analysis for the approach introduced by\nMomke and Svensson yielding a bound of 13/9 on the approximation factor, as\nwell as a bound of 19/12+epsilon for any epsilon>0 for a more general\nTravelling Salesman Path Problem in graphic metrics."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-011-9585-3", 
    "link": "http://arxiv.org/pdf/1108.1176v1", 
    "other_authors": "MohammadTaghi Hajiaghayi, Rohit Khandekar, Guy Kortsarz, Zeev Nutov", 
    "title": "Combinatorial Algorithms for Capacitated Network Design", 
    "arxiv-id": "1108.1176v1", 
    "author": "Zeev Nutov", 
    "publish": "2011-08-04T19:34:38Z", 
    "summary": "We focus on designing combinatorial algorithms for the Capacitated Network\nDesign problem (Cap-SNDP). The Cap-SNDP is the problem of satisfying\nconnectivity requirements when edges have costs and hard capacities. We begin\nby showing that the Group Steiner tree problem (GST) is a special case of\nCap-SNDP even when there is connectivity requirement between only one\nsource-sink pair. This implies the first poly-logarithmic lower bound for the\nCap-SNDP. We next provide combinatorial algorithms for several special cases of\nthis problem. The Cap-SNDP is equivalent to its special case when every edge\nhas either zero cost or infinite capacity. We consider a special case, called\nConnected Cap-SNDP, where all infinite-capacity edges in the solution are\nrequired to form a connected component containing the sinks. This problem is\nmotivated by its similarity to the Connected Facility Location problem\n[G+01,SW04]. We solve this problem by reducing it to Submodular tree cover\nproblem, which is a common generalization of Connected Cap-SNDP and Group\nSteiner tree problem. We generalize the recursive greedy algorithm [CEK]\nachieving a poly-logarithmic approximation algorithm for Submodular tree cover\nproblem. This result is interesting in its own right and gives the first\npoly-logarithmic approximation algorithms for Connected hard capacities set\nmulti-cover and Connected source location.\n  We then study another special case of Cap-SNDP called Unbalanced\npoint-to-point connection problem. Besides its practical applications to shift\ndesign problems [EKS], it generalizes many problems such as k-MST, Steiner\nForest and Point-to-Point Connection. We give a combinatorial logarithmic\napproximation algorithm for this problem by reducing it to degree-bounded SNDP."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1351v1", 
    "other_authors": "Raied Salman, Vojislav Kecman, Qi Li, Robert Strack, Erik Test", 
    "title": "Fast k-means algorithm clustering", 
    "arxiv-id": "1108.1351v1", 
    "author": "Erik Test", 
    "publish": "2011-08-05T15:37:23Z", 
    "summary": "k-means has recently been recognized as one of the best algorithms for\nclustering unsupervised data. Since k-means depends mainly on distance\ncalculation between all data points and the centers, the time cost will be high\nwhen the size of the dataset is large (for example more than 500millions of\npoints). We propose a two stage algorithm to reduce the time cost of distance\ncalculation for huge datasets. The first stage is a fast distance calculation\nusing only a small portion of the data to produce the best possible location of\nthe centers. The second stage is a slow distance calculation in which the\ninitial centers used are taken from the first stage. The fast and slow stages\nrepresent the speed of the movement of the centers. In the slow stage, the\nwhole dataset can be used to get the exact location of the centers. The time\ncost of the distance calculation for the fast stage is very low due to the\nsmall size of the training data chosen. The time cost of the distance\ncalculation for the slow stage is also minimized due to small number of\niterations. Different initial locations of the clusters have been used during\nthe test of the proposed algorithms. For large datasets, experiments show that\nthe 2-stage clustering method achieves better speed-up (1-9 times)."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1751v1", 
    "other_authors": "Siavosh Benabbas, Hyun Chul Lee, Joel Oren, Yuli Ye", 
    "title": "Efficient Sum-Based Hierarchical Smoothing Under \\ell_1-Norm", 
    "arxiv-id": "1108.1751v1", 
    "author": "Yuli Ye", 
    "publish": "2011-08-08T17:07:06Z", 
    "summary": "We introduce a new regression problem which we call the Sum-Based\nHierarchical Smoothing problem. Given a directed acyclic graph and a\nnon-negative value, called target value, for each vertex in the graph, we wish\nto find non-negative values for the vertices satisfying a certain constraint\nwhile minimizing the distance of these assigned values and the target values in\nthe lp-norm. The constraint is that the value assigned to each vertex should be\nno less than the sum of the values assigned to its children. We motivate this\nproblem with applications in information retrieval and web mining. While our\nproblem can be solved in polynomial time using linear programming, given the\ninput size in these applications such a solution might be too slow. We mainly\nstudy the \\ell_1-norm case restricting the underlying graphs to rooted trees.\nFor this case we provide an efficient algorithm, running in O(n^2) time. While\nthe algorithm is purely combinatorial, its proof of correctness is an elegant\nuse of linear programming duality. We believe that our approach may be\napplicable to similar problems, where comparable hierarchical constraints are\ninvolved, e.g. considering the average of the values assigned to the children\nof each vertex. While similar in flavor to other smoothing problems like\nIsotonic Regression (see for example [Angelov et al. SODA'06]), our problem is\narguably richer and theoretically more challenging."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.1983v1", 
    "other_authors": "J. Ian Munro, Rajeev Raman, Venkatesh Raman, S. Srinivasa Rao", 
    "title": "Succinct Representations of Permutations and Functions", 
    "arxiv-id": "1108.1983v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2011-08-09T17:01:12Z", 
    "summary": "We investigate the problem of succinctly representing an arbitrary\npermutation, \\pi, on {0,...,n-1} so that \\pi^k(i) can be computed quickly for\nany i and any (positive or negative) integer power k. A representation taking\n(1+\\epsilon) n lg n + O(1) bits suffices to compute arbitrary powers in\nconstant time, for any positive constant \\epsilon <= 1. A representation taking\nthe optimal \\ceil{\\lg n!} + o(n) bits can be used to compute arbitrary powers\nin O(lg n / lg lg n) time.\n  We then consider the more general problem of succinctly representing an\narbitrary function, f: [n] \\rightarrow [n] so that f^k(i) can be computed\nquickly for any i and any integer power k. We give a representation that takes\n(1+\\epsilon) n lg n + O(1) bits, for any positive constant \\epsilon <= 1, and\ncomputes arbitrary positive powers in constant time. It can also be used to\ncompute f^k(i), for any negative integer k, in optimal O(1+|f^k(i)|) time.\n  We place emphasis on the redundancy, or the space beyond the\ninformation-theoretic lower bound that the data structure uses in order to\nsupport operations efficiently. A number of lower bounds have recently been\nshown on the redundancy of data structures. These lower bounds confirm the\nspace-time optimality of some of our solutions. Furthermore, the redundancy of\none of our structures \"surpasses\" a recent lower bound by Golynski [Golynski,\nSODA 2009], thus demonstrating the limitations of this lower bound."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.2157v1", 
    "other_authors": "Alexander Golynski, Alessio Orlandi, Rajeev Raman, S. Srinivasa Rao", 
    "title": "Optimal Indexes for Sparse Bit Vectors", 
    "arxiv-id": "1108.2157v1", 
    "author": "S. Srinivasa Rao", 
    "publish": "2011-08-10T11:36:22Z", 
    "summary": "We consider the problem of supporting Rank() and Select() operations on a bit\nvector of length m with n 1 bits. The problem is considered in the succinct\nindex model, where the bit vector is stored in \"read-only\" memory and an\nadditional data structure, called the index, is created during pre-processing\nto help answer the above queries. We give asymptotically optimal\ndensity-sensitive trade-offs, involving both m and n, that relate the size of\nthe index to the number of accesses to the bit vector (and processing time)\nneeded to answer the above queries. The results are particularly interesting\nfor the case where n = o(m)."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3048v2", 
    "other_authors": "Michael Kallitsis, Stilian Stoev, George Michailidis", 
    "title": "Fast Approximation Algorithms for Near-optimal Large-scale Network   Monitoring", 
    "arxiv-id": "1108.3048v2", 
    "author": "George Michailidis", 
    "publish": "2011-08-15T18:27:11Z", 
    "summary": "We study the problem of optimal traffic prediction and monitoring in\nlarge-scale networks. Our goal is to determine which subset of K links to\nmonitor in order to \"best\" predict the traffic on the remaining links in the\nnetwork. We consider several optimality criteria. This can be formulated as a\ncombinatorial optimization problem, belonging to the family of subset selection\nproblems. Similar NP-hard problems arise in statistics, machine learning and\nsignal processing. Some include subset selection for regression, variable\nselection, and sparse approximation. Exact solutions are computationally\nprohibitive. We present both new heuristics as well as new efficient algorithms\nimplementing the classical greedy heuristic - commonly used to tackle such\ncombinatorial problems. Our approach exploits connections to principal\ncomponent analysis (PCA), and yields new types of performance lower bounds\nwhich do not require submodularity of the objective functions. We show that an\nensemble method applied to our new randomized heuristic algorithm, often\noutperforms the classical greedy heuristic in practice. We evaluate our\nalgorithms under several large-scale networks, including real life networks."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3092v1", 
    "other_authors": "Michael Kaufmann, Tamara Mchedlidze, Antonios Symvonis", 
    "title": "Upward Point Set Embeddability for Convex Point Sets is in $P$", 
    "arxiv-id": "1108.3092v1", 
    "author": "Antonios Symvonis", 
    "publish": "2011-08-15T20:35:24Z", 
    "summary": "In this paper, we present a polynomial dynamic programming algorithm that\ntests whether a $n$-vertex directed tree $T$ has an upward planar embedding\ninto a convex point-set $S$ of size $n$. Further, we extend our approach to the\nclass of outerplanar digraphs. This nontrivial and surprising result implies\nthat any given digraph can be efficiently tested for an upward planar embedding\ninto a given convex point set."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3413v2", 
    "other_authors": "Zengfeng Huang, Ke Yi, Qin Zhang", 
    "title": "Randomized Algorithms for Tracking Distributed Count, Frequencies, and   Ranks", 
    "arxiv-id": "1108.3413v2", 
    "author": "Qin Zhang", 
    "publish": "2011-08-17T07:31:27Z", 
    "summary": "We show that randomization can lead to significant improvements for a few\nfundamental problems in distributed tracking. Our basis is the {\\em\ncount-tracking} problem, where there are $k$ players, each holding a counter\n$n_i$ that gets incremented over time, and the goal is to track an\n$\\eps$-approximation of their sum $n=\\sum_i n_i$ continuously at all times,\nusing minimum communication. While the deterministic communication complexity\nof the problem is $\\Theta(k/\\eps \\cdot \\log N)$, where $N$ is the final value\nof $n$ when the tracking finishes, we show that with randomization, the\ncommunication cost can be reduced to $\\Theta(\\sqrt{k}/\\eps \\cdot \\log N)$. Our\nalgorithm is simple and uses only O(1) space at each player, while the lower\nbound holds even assuming each player has infinite computing power. Then, we\nextend our techniques to two related distributed tracking problems: {\\em\nfrequency-tracking} and {\\em rank-tracking}, and obtain similar improvements\nover previous deterministic algorithms. Both problems are of central importance\nin large data monitoring and analysis, and have been extensively studied in the\nliterature."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3516v3", 
    "other_authors": "Panteleimon Rodis", 
    "title": "Model for networks of spatial objects and simulation of geographical   phenomena propagation", 
    "arxiv-id": "1108.3516v3", 
    "author": "Panteleimon Rodis", 
    "publish": "2011-08-17T16:13:53Z", 
    "summary": "The topic of this paper is the presentation of a new network model designed\nfor networks consisting of spatial objects. This model allows the development\nof more advance representations of systems of networked objects and the study\nof geographical phenomena propagated through networks. The capabilities of the\nmodel in simulation of geographical phenomena propagation are also studied and\nrelevant algorithms are presented. As examples of use, modeling of water supply\nnetwork and the simulation of traffic flow in road networks are presented."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.3683v1", 
    "other_authors": "Philip Bille, Inge Li Goertz", 
    "title": "Substring Range Reporting", 
    "arxiv-id": "1108.3683v1", 
    "author": "Inge Li Goertz", 
    "publish": "2011-08-18T08:31:11Z", 
    "summary": "We revisit various string indexing problems with range reporting features,\nnamely, position-restricted substring searching, indexing substrings with gaps,\nand indexing substrings with intervals. We obtain the following main results.\n{itemize} We give efficient reductions for each of the above problems to a new\nproblem, which we call \\emph{substring range reporting}. Hence, we unify the\nprevious work by showing that we may restrict our attention to a single problem\nrather than studying each of the above problems individually. We show how to\nsolve substring range reporting with optimal query time and little space.\nCombined with our reductions this leads to significantly improved time-space\ntrade-offs for the above problems. In particular, for each problem we obtain\nthe first solutions with optimal time query and $O(n\\log^{O(1)} n)$ space,\nwhere $n$ is the length of the indexed string. We show that our techniques for\nsubstring range reporting generalize to \\emph{substring range counting} and\n\\emph{substring range emptiness} variants. We also obtain non-trivial\ntime-space trade-offs for these problems. {itemize} Our bounds for substring\nrange reporting are based on a novel combination of suffix trees and range\nreporting data structures. The reductions are simple and general and may apply\nto other combinations of string indexing with range reporting."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4217v1", 
    "other_authors": "Yoshinobu Kawahara, Takashi Washio", 
    "title": "Prismatic Algorithm for Discrete D.C. Programming Problems", 
    "arxiv-id": "1108.4217v1", 
    "author": "Takashi Washio", 
    "publish": "2011-08-21T22:09:21Z", 
    "summary": "In this paper, we propose the first exact algorithm for minimizing the\ndifference of two submodular functions (D.S.), i.e., the discrete version of\nthe D.C. programming problem. The developed algorithm is a\nbranch-and-bound-based algorithm which responds to the structure of this\nproblem through the relationship between submodularity and convexity. The D.S.\nprogramming problem covers a broad range of applications in machine learning\nbecause this generalizes the optimization of a wide class of set functions. We\nempirically investigate the performance of our algorithm, and illustrate the\ndifference between exact and approximate solutions respectively obtained by the\nproposed and existing algorithms in feature selection and discriminative\nstructure learning."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4408v1", 
    "other_authors": "J\u00e9r\u00e9my Barbay, Gonzalo Navarro", 
    "title": "On Compressing Permutations and Adaptive Sorting", 
    "arxiv-id": "1108.4408v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-08-22T19:59:03Z", 
    "summary": "Previous compact representations of permutations have focused on adding a\nsmall index on top of the plain data $<\\pi(1), \\pi(2),...\\pi(n)>$, in order to\nefficiently support the application of the inverse or the iterated permutation.\n  In this paper we initiate the study of techniques that exploit the\ncompressibility of the data itself, while retaining efficient computation of\n$\\pi(i)$ and its inverse.\n  In particular, we focus on exploiting {\\em runs}, which are subsets\n(contiguous or not) of the domain where the permutation is monotonic.\n  Several variants of those types of runs arise in real applications such as\ninverted indexes and suffix arrays.\n  Furthermore, our improved results on compressed data structures for\npermutations also yield better adaptive sorting algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcnc.2011.3402", 
    "link": "http://arxiv.org/pdf/1108.4606v1", 
    "other_authors": "Mong-Jen Kao, D. T. Lee", 
    "title": "Capacitated Domination: Constant Factor Approximation for Planar Graphs", 
    "arxiv-id": "1108.4606v1", 
    "author": "D. T. Lee", 
    "publish": "2011-08-23T14:10:15Z", 
    "summary": "We consider the capacitated domination problem, which models a\nservice-requirement assigning scenario and which is also a generalization of\nthe dominating set problem. In this problem, we are given a graph with three\nparameters defined on the vertex set, which are cost, capacity, and demand. The\nobjective of this problem is to compute a demand assignment of least cost, such\nthat the demand of each vertex is fully-assigned to some of its closed\nneighbours without exceeding the amount of capacity they provide.\n  In this paper, we provide the first constant factor approximation for this\nproblem on planar graphs, based on a new perspective on the hierarchical\nstructure of outer-planar graphs. We believe that this new perspective and\ntechnique can be applied to other capacitated covering problems to help tackle\nvertices of large degrees."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.4983v2", 
    "other_authors": "Justin Ward", 
    "title": "A $(k + 3)/2$-approximation algorithm for monotone submodular   maximization over a $k$-exchange system", 
    "arxiv-id": "1108.4983v2", 
    "author": "Justin Ward", 
    "publish": "2011-08-25T01:28:59Z", 
    "summary": "We consider the problem of maximizing a monotone submodular function in a\n$k$-exchange system. These systems, introduced by Feldman et al., generalize\nthe matroid k-parity problem in a wide class of matroids and capture many other\ncombinatorial optimization problems. Feldman et al. show that a simple\nnon-oblivious local search algorithm attains a $(k + 1)/2$ approximation ratio\nfor the problem of linear maximization in a $k$-exchange system. Here, we\nextend this approach to the case of monotone submodular objective functions. We\ngive a deterministic, non-oblivious local search algorithm that attains an\napproximation ratio of $(k + 3)/2$ for the problem of maximizing a monotone\nsubmodular function in a $k$-exchange system."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.5471v1", 
    "other_authors": "Li Yan, Marek Chrobak", 
    "title": "New Results on the Fault-Tolerant Facility Placement Problem", 
    "arxiv-id": "1108.5471v1", 
    "author": "Marek Chrobak", 
    "publish": "2011-08-27T19:31:25Z", 
    "summary": "We studied the Fault-Tolerant Facility Placement problem (FTFP) which\ngeneralizes the uncapacitated facility location problem (UFL). In FTFP, we are\ngiven a set F of sites at which facilities can be built, and a set C of clients\nwith some demands that need to be satisfied by different facilities. A client\n$j$ has demand $r_j$. Building one facility at a site $i$ incurs a cost $f_i$,\nand connecting one unit of demand from client $j$ to a facility at site\n$i\\in\\fac$ costs $d_{ij}$. $d_{ij}$'s are assumed to form a metric. A feasible\nsolution specifies the number of facilities to be built at each site and the\nway to connect demands from clients to facilities, with the restriction that\ndemands from the same client must go to different facilities. Facilities at the\nsame site are considered different. The goal is to find a solution with minimum\ntotal cost. We gave a 1.7245-approximation algorithm to the FTFP problem. Our\ntechnique is via a reduction to the Fault-Tolerant Facility Location problem,\nin which each client has demand $r_j$ but each site can have at most one\nfacility built."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1108.5525v1", 
    "other_authors": "Manoj Gupta, Yogish Sabharwal, Sandeep Sen", 
    "title": "The update complexity of selection and related problems", 
    "arxiv-id": "1108.5525v1", 
    "author": "Sandeep Sen", 
    "publish": "2011-08-29T12:40:03Z", 
    "summary": "We present a framework for computing with input data specified by intervals,\nrepresenting uncertainty in the values of the input parameters. To compute a\nsolution, the algorithm can query the input parameters that yield more refined\nestimates in form of sub-intervals and the objective is to minimize the number\nof queries. The previous approaches address the scenario where every query\nreturns an exact value. Our framework is more general as it can deal with a\nwider variety of inputs and query responses and we establish interesting\nrelationships between them that have not been investigated previously. Although\nsome of the approaches of the previous restricted models can be adapted to the\nmore general model, we require more sophisticated techniques for the analysis\nand we also obtain improved algorithms for the previous model.\n  We address selection problems in the generalized model and show that there\nexist 2-update competitive algorithms that do not depend on the lengths or\ndistribution of the sub-intervals and hold against the worst case adversary. We\nalso obtain similar bounds on the competitive ratio for the MST problem in\ngraphs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1110.0180v1", 
    "other_authors": "Gleb Novichkov", 
    "title": "An efficient algorithm to find a set of nearest elements in a mesh", 
    "arxiv-id": "1110.0180v1", 
    "author": "Gleb Novichkov", 
    "publish": "2011-10-02T12:32:32Z", 
    "summary": "A linear time algorithm to find a set of nearest elements in a mesh."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.42", 
    "link": "http://arxiv.org/pdf/1110.0583v1", 
    "other_authors": "Ton Kloks, Sheung-Hung Poon, Chin-Ting Ung, Yue-Li Wang", 
    "title": "Algorithms for the strong chromatic index of Halin graphs,   distance-hereditary graphs and maximal outerplanar graphs", 
    "arxiv-id": "1110.0583v1", 
    "author": "Yue-Li Wang", 
    "publish": "2011-10-04T05:43:28Z", 
    "summary": "We show that there exist linear-time algorithms that compute the strong\nchromatic index of Halin graphs, of maximal outerplanar graphs and of\ndistance-hereditary graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.0620v1", 
    "other_authors": "Shinji Imahori, Tomomi Matsui, Ryuhei Miyashiro", 
    "title": "A 2.75-Approximation Algorithm for the Unconstrained Traveling   Tournament Problem", 
    "arxiv-id": "1110.0620v1", 
    "author": "Ryuhei Miyashiro", 
    "publish": "2011-10-04T09:35:21Z", 
    "summary": "A 2.75-approximation algorithm is proposed for the unconstrained traveling\ntournament problem, which is a variant of the traveling tournament problem. For\nthe unconstrained traveling tournament problem, this is the first proposal of\nan approximation algorithm with a constant approximation ratio. In addition,\nthe proposed algorithm yields a solution that meets both the no-repeater and\nmirrored constraints. Computational experiments show that the algorithm\ngenerates solutions of good quality."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.0990v1", 
    "other_authors": "Marco Molinaro, R. Ravi", 
    "title": "The Query-commit Problem", 
    "arxiv-id": "1110.0990v1", 
    "author": "R. Ravi", 
    "publish": "2011-10-05T14:08:33Z", 
    "summary": "In the query-commit problem we are given a graph where edges have distinct\nprobabilities of existing. It is possible to query the edges of the graph, and\nif the queried edge exists then its endpoints are irrevocably matched. The goal\nis to find a querying strategy which maximizes the expected size of the\nmatching obtained. This stochastic matching setup is motivated by applications\nin kidney exchanges and online dating.\n  In this paper we address the query-commit problem from both theoretical and\nexperimental perspectives. First, we show that a simple class of edges can be\nqueried without compromising the optimality of the strategy. This property is\nthen used to obtain in polynomial time an optimal querying strategy when the\ninput graph is sparse. Next we turn our attentions to the kidney exchange\napplication, focusing on instances modeled over real data from existing\nexchange programs. We prove that, as the number of nodes grows, almost every\ninstance admits a strategy which matches almost all nodes. This result supports\nthe intuition that more exchanges are possible on a larger pool of\npatient/donors and gives theoretical justification for unifying the existing\nexchange programs. Finally, we evaluate experimentally different querying\nstrategies over kidney exchange instances. We show that even very simple\nheuristics perform fairly well, being within 1.5% of an optimal clairvoyant\nstrategy, that knows in advance the edges in the graph. In such a\ntime-sensitive application, this result motivates the use of committing\nstrategies."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1064v1", 
    "other_authors": "Prasad Raghavendra, Ning Tan", 
    "title": "Approximating CSPs with Global Cardinality Constraints Using SDP   Hierarchies", 
    "arxiv-id": "1110.1064v1", 
    "author": "Ning Tan", 
    "publish": "2011-10-05T18:31:44Z", 
    "summary": "This work is concerned with approximating constraint satisfaction problems\n(CSPs) with an additional global cardinality constraints. For example, \\maxcut\nis a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find\na cut $S \\cup \\bar S = V$ that maximizes the numberof crossing edges,\n$|E(S,\\bar S)|$. The \\maxbisection problem is a variant of \\maxcut with an\nadditional global constraint that each side of the cut has exactly half the\nvertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like\n\\minbisection and approximating Graph Expansion can be formulated as CSPs with\nglobal constraints.\n  In this work, we formulate a general approach towards approximating CSPs with\nglobal constraints using SDP hierarchies. To demonstrate the approach we\npresent the following results:\n  Using the Lasserre hierarchy, we present an algorithm that runs in time\n$O(n^{poly(1/\\epsilon)})$ that given an instance of \\maxbisection with value\n$1-\\epsilon$, finds a bisection with value $1-O(\\sqrt{\\epsilon})$. This\napproximation is near-optimal (up to constant factors in $O()$) under the\nUnique Games Conjecture.\n  By a computer-assisted proof, we show that the same algorithm also achieves a\n0.85-approximation for \\maxbisection, improving on the previous bound of 0.70\n(note that it is \\uniquegames hard to approximate better than a 0.878 factor).\nThe same algorithm also yields a 0.92-approximation for \\maxtwosat with\ncardinality constraints.\n  For every CSP with a global cardinality constraints, we present a generic\nconversion from integrality gap instances for the Lasserre hierarchy to a {\\it\ndictatorship test} whose soundness is at most integrality gap. Dictatorship\ntesting gadgets are central to hardness results for CSPs, and a generic\nconversion of the above nature lies at the core of the tight Unique Games based\nhardness result for CSPs. \\cite{Raghavendra08}"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1079v1", 
    "other_authors": "Krzysztof Onak, Dana Ron, Michal Rosen, Ronitt Rubinfeld", 
    "title": "A Near-Optimal Sublinear-Time Algorithm for Approximating the Minimum   Vertex Cover Size", 
    "arxiv-id": "1110.1079v1", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2011-10-05T19:27:57Z", 
    "summary": "We give a nearly optimal sublinear-time algorithm for approximating the size\nof a minimum vertex cover in a graph G. The algorithm may query the degree\ndeg(v) of any vertex v of its choice, and for each 1 <= i <= deg(v), it may ask\nfor the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex\ncover in G, the algorithm outputs, with high constant success probability, an\nestimate VC_estimate(G) such that VC_opt(G) <= VC_estimate(G) <= 2 * VC_opt(G)\n+ epsilon*n, where epsilon is a given additive approximation parameter. We\nrefer to such an estimate as a (2,epsilon)-estimate. The query complexity and\nrunning time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg\ndenotes the average vertex degree in the graph. The best previously known\nsublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and\nrunning time O(d^4/epsilon^2), where d is the maximum degree in the graph.\nGiven the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining\nsuch an estimate (with any constant multiplicative factor) due to Parnas and\nRon (TCS 2007), our result is nearly optimal.\n  In the case that the graph is dense, that is, the number of edges is\nTheta(n^2), we consider another model, in which the algorithm may ask, for any\npair of vertices u and v, whether there is an edge between u and v. We show how\nto adapt the algorithm that uses neighbor queries to this model and obtain an\nalgorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex\ncover whose query complexity and running time are ~O(n) * poly(1/epsilon)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1124v1", 
    "other_authors": "Shiyao Chen, Lang Tong, Ting He", 
    "title": "Optimal Deadline Scheduling with Commitment", 
    "arxiv-id": "1110.1124v1", 
    "author": "Ting He", 
    "publish": "2011-10-06T00:49:03Z", 
    "summary": "We consider an online preemptive scheduling problem where jobs with deadlines\narrive sporadically. A commitment requirement is imposed such that the\nscheduler has to either accept or decline a job immediately upon arrival. The\nscheduler's decision to accept an arriving job constitutes a contract with the\ncustomer; if the accepted job is not completed by its deadline as promised, the\nscheduler loses the value of the corresponding job and has to pay an additional\npenalty depending on the amount of unfinished workload. The objective of the\nonline scheduler is to maximize the overall profit, i.e., the total value of\nthe admitted jobs completed before their deadlines less the penalty paid for\nthe admitted jobs that miss their deadlines. We show that the maximum\ncompetitive ratio is $3-2\\sqrt{2}$ and propose a simple online algorithm to\nachieve this competitive ratio. The optimal scheduling includes a threshold\nadmission and a greedy scheduling policies. The proposed algorithm has direct\napplications to the charging of plug-in hybrid electrical vehicles (PHEV) at\ngarages or parking lots."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1194v1", 
    "other_authors": "Maria Chroni, Stavros D. Nikolopoulos", 
    "title": "Efficient Encoding of Watermark Numbers as Reducible Permutation Graphs", 
    "arxiv-id": "1110.1194v1", 
    "author": "Stavros D. Nikolopoulos", 
    "publish": "2011-10-06T09:24:51Z", 
    "summary": "In a software watermarking environment, several graph theoretic watermark\nmethods use numbers as watermark values, where some of these methods encode the\nwatermark numbers as graph structures. In this paper we extended the class of\nerror correcting graphs by proposing an efficient and easily implemented codec\nsystem for encoding watermark numbers as reducible permutation flow-graphs.\nMore precisely, we first present an efficient algorithm which encodes a\nwatermark number $w$ as self-inverting permutation $\\pi^*$ and, then, an\nalgorithm which encodes the self-inverting permutation $\\pi^*$ as a reducible\npermutation flow-graph $F[\\pi^*]$ by exploiting domination relations on the\nelements of $\\pi^*$ and using an efficient DAG representation of $\\pi^*$. The\nwhole encoding process takes O(n) time and space, where $n$ is the binary size\nof the number $w$ or, equivalently, the number of elements of the permutation\n$\\pi^*$. We also propose efficient decoding algorithms which extract the number\n$w$ from the reducible permutation flow-graph $F[\\pi^*]$ within the same time\nand space complexity. The two main components of our proposed codec system,\ni.e., the self-inverting permutation $\\pi^*$ and the reducible permutation\ngraph $F[\\pi^*]$, incorporate important structural properties which make our\nsystem resilient to attacks."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1320v2", 
    "other_authors": "David Eisenstat, Philip Klein, Claire Mathieu", 
    "title": "An efficient polynomial-time approximation scheme for Steiner forest in   planar graphs", 
    "arxiv-id": "1110.1320v2", 
    "author": "Claire Mathieu", 
    "publish": "2011-10-06T16:51:57Z", 
    "summary": "We give an $O(n \\log^3 n)$ approximation scheme for Steiner forest in planar\ngraphs, improving on the previous approximation scheme for this problem, which\nruns in $O(n^{f(\\epsilon)})$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1580v1", 
    "other_authors": "Nikhil Bansal, Niv Buchbinder, Aleksander Madry, Joseph, Naor", 
    "title": "A Polylogarithmic-Competitive Algorithm for the k-Server Problem", 
    "arxiv-id": "1110.1580v1", 
    "author": "Naor", 
    "publish": "2011-10-07T16:39:34Z", 
    "summary": "We give the first polylogarithmic-competitive randomized online algorithm for\nthe $k$-server problem on an arbitrary finite metric space. In particular, our\nalgorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any\nmetric space on n points. Our algorithm improves upon the deterministic\n(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]\nwhenever n is sub-exponential in k."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.1693v1", 
    "other_authors": "Ton Kloks, Chin-Ting Ung, Yue-Li Wang", 
    "title": "On the strong chromatic index and maximum induced matching of   tree-cographs and permutation graphs", 
    "arxiv-id": "1110.1693v1", 
    "author": "Yue-Li Wang", 
    "publish": "2011-10-08T04:04:41Z", 
    "summary": "We show that there exist linear-time algorithms that compute the strong\nchromatic index and a maximum induced matching of tree-cographs when the\ndecomposition tree is a part of the input. We also show that there exists an\nefficient algorithm for the strong chromatic index of permutation graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.2207v3", 
    "other_authors": "Sungjin Im, Viswanath Nagarajan, Ruben van der Zwaan", 
    "title": "Minimum Latency Submodular Cover", 
    "arxiv-id": "1110.2207v3", 
    "author": "Ruben van der Zwaan", 
    "publish": "2011-10-10T21:45:49Z", 
    "summary": "We study the Minimum Latency Submodular Cover problem (MLSC), which consists\nof a metric $(V,d)$ with source $r\\in V$ and $m$ monotone submodular functions\n$f_1, f_2, ..., f_m: 2^V \\rightarrow [0,1]$. The goal is to find a path\noriginating at $r$ that minimizes the total cover time of all functions. This\ngeneralizes well-studied problems, such as Submodular Ranking [AzarG11] and\nGroup Steiner Tree [GKR00]. We give a polynomial time $O(\\log \\frac{1}{\\eps}\n\\cdot \\log^{2+\\delta} |V|)$-approximation algorithm for MLSC, where\n$\\epsilon>0$ is the smallest non-zero marginal increase of any\n$\\{f_i\\}_{i=1}^m$ and $\\delta>0$ is any constant.\n  We also consider the Latency Covering Steiner Tree problem (LCST), which is\nthe special case of \\mlsc where the $f_i$s are multi-coverage functions. This\nis a common generalization of the Latency Group Steiner Tree\n[GuptaNR10a,ChakrabartyS11] and Generalized Min-sum Set Cover [AzarGY09,\nBansalGK10] problems. We obtain an $O(\\log^2|V|)$-approximation algorithm for\nLCST.\n  Finally we study a natural stochastic extension of the Submodular Ranking\nproblem, and obtain an adaptive algorithm with an $O(\\log 1/ \\eps)$\napproximation ratio, which is best possible. This result also generalizes some\npreviously studied stochastic optimization problems, such as Stochastic Set\nCover [GoemansV06] and Shared Filter Evaluation [MunagalaSW07, LiuPRY08]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.2893v1", 
    "other_authors": "Philip Bille, Inge Li Goertz, Hjalte Wedel Vildh\u00f8j, David Kofoed Wind", 
    "title": "String Matching with Variable Length Gaps", 
    "arxiv-id": "1110.2893v1", 
    "author": "David Kofoed Wind", 
    "publish": "2011-10-13T11:13:48Z", 
    "summary": "We consider string matching with variable length gaps. Given a string $T$ and\na pattern $P$ consisting of strings separated by variable length gaps\n(arbitrary strings of length in a specified range), the problem is to find all\nending positions of substrings in $T$ that match $P$. This problem is a basic\nprimitive in computational biology applications. Let $m$ and $n$ be the lengths\nof $P$ and $T$, respectively, and let $k$ be the number of strings in $P$. We\npresent a new algorithm achieving time $O(n\\log k + m +\\alpha)$ and space $O(m\n+ A)$, where $A$ is the sum of the lower bounds of the lengths of the gaps in\n$P$ and $\\alpha$ is the total number of occurrences of the strings in $P$\nwithin $T$. Compared to the previous results this bound essentially achieves\nthe best known time and space complexities simultaneously. Consequently, our\nalgorithm obtains the best known bounds for almost all combinations of $m$,\n$n$, $k$, $A$, and $\\alpha$. Our algorithm is surprisingly simple and\nstraightforward to implement. We also present algorithms for finding and\nencoding the positions of all strings in $P$ for every match of the pattern."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3100v1", 
    "other_authors": "Eyal Even Dar, Mark Sandler", 
    "title": "Telling Two Distributions Apart: a Tight Characterization", 
    "arxiv-id": "1110.3100v1", 
    "author": "Mark Sandler", 
    "publish": "2011-10-14T00:46:23Z", 
    "summary": "We consider the problem of distinguishing between two arbitrary black-box\ndistributions defined over the domain [n], given access to $s$ samples from\nboth. It is known that in the worst case O(n^{2/3}) samples is both necessary\nand sufficient, provided that the distributions have L1 difference of at least\n{\\epsilon}. However, it is also known that in many cases fewer samples suffice.\nWe identify a new parameter, that provides an upper bound on how many samples\nneeded, and present an efficient algorithm that requires the number of samples\nindependent of the domain size. Also for a large subclass of distributions we\nprovide a lower bound, that matches our upper bound up to a poly-logarithmic\nfactor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3381v1", 
    "other_authors": "Gianni Franceschini, Roberto Grossi, S. Muthukrishnan", 
    "title": "Partial Data Compression and Text Indexing via Optimal Suffix   Multi-Selection", 
    "arxiv-id": "1110.3381v1", 
    "author": "S. Muthukrishnan", 
    "publish": "2011-10-15T05:16:18Z", 
    "summary": "Consider an input text string T[1,N] drawn from an unbounded alphabet. We\nstudy partial computation in suffix-based problems for Data Compression and\nText Indexing such as\n  (I) retrieve any segment of K<=N consecutive symbols from the Burrows-Wheeler\ntransform of T, and\n  (II) retrieve any chunk of K<=N consecutive entries of the Suffix Array or\nthe Suffix Tree.\n  Prior literature would take O(N log N) comparisons (and time) to solve these\nproblems by solving the total problem of building the entire Burrows-Wheeler\ntransform or Text Index for T, and performing a post-processing to single out\nthe wanted portion.\n  We introduce a novel adaptive approach to partial computational problems\nabove, and solve both the partial problems in O(K log K + N) comparisons and\ntime, improving the best known running times of O(N log N) for K=o(N).\n  These partial-computation problems are intimately related since they share a\ncommon bottleneck: the suffix multi-selection problem, which is to output the\nsuffixes of rank r_1,r_2,...,r_K under the lexicographic order, where\nr_1<r_2<...<r_K, r_i in [1,N]. Special cases of this problem are well known:\nK=N is the suffix sorting problem that is the workhorse in Stringology with\nhundreds of applications, and K=1 is the recently studied suffix selection.\n  We show that suffix multi-selection can be solved in Theta(N log N -\nsum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,\nr_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0<=j<=K. This is asymptotically\noptimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] for\nmulti-selection on atomic elements (not suffixes). Matching the bound known for\natomic elements for strings is a long running theme and challenge from 70's,\nwhich we achieve for the suffix multi-selection problem. The partial suffix\nproblems as well as the suffix multi-selection problem have many applications."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.3850v1", 
    "other_authors": "Piotr Indyk, Eric Price, David P. Woodruff", 
    "title": "On the Power of Adaptivity in Sparse Recovery", 
    "arxiv-id": "1110.3850v1", 
    "author": "David P. Woodruff", 
    "publish": "2011-10-17T23:35:11Z", 
    "summary": "The goal of (stable) sparse recovery is to recover a $k$-sparse approximation\n$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is\nto recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some\nconstant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or\n$p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ non-adaptive\nmeasurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].\n  In this paper we show that if one is allowed to perform measurements that are\nadaptive, then the number of measurements can be considerably reduced.\nSpecifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k\nlog log (n eps/k))$ measurements that uses $O(log* k \\log \\log (n eps/k))$\nrounds. This is a significant improvement over the best possible non-adaptive\nbound. - A scheme with $m=O((1/eps) k log (k/eps) + k \\log (n/k))$ measurements\nthat uses /two/ rounds. This improves over the best possible non-adaptive\nbound. To the best of our knowledge, these are the first results of this type.\nAs an independent application, we show how to solve the problem of finding a\nduplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using\n$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best\npossible space complexity achievable using a single pass."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4150v1", 
    "other_authors": "Siddharth Barman, Shuchi Chawla", 
    "title": "Traffic-Redundancy Aware Network Design", 
    "arxiv-id": "1110.4150v1", 
    "author": "Shuchi Chawla", 
    "publish": "2011-10-19T00:43:25Z", 
    "summary": "We consider network design problems for information networks where routers\ncan replicate data but cannot alter it. This functionality allows the network\nto eliminate data-redundancy in traffic, thereby saving on routing costs. We\nconsider two problems within this framework and design approximation\nalgorithms.\n  The first problem we study is the traffic-redundancy aware network design\n(RAND) problem. We are given a weighted graph over a single server and many\nclients. The server owns a number of different data packets and each client\ndesires a subset of the packets; the client demand sets form a laminar set\nsystem. Our goal is to connect every client to the source via a single path,\nsuch that the collective cost of the resulting network is minimized. Here the\ntransportation cost over an edge is its weight times times the number of\ndistinct packets that it carries.\n  The second problem is a facility location problem that we call RAFL. Here the\ngoal is to find an assignment from clients to facilities such that the total\ncost of routing packets from the facilities to clients (along unshared paths),\nplus the total cost of \"producing\" one copy of each desired packet at each\nfacility is minimized.\n  We present a constant factor approximation for the RAFL and an O(log P)\napproximation for RAND, where P is the total number of distinct packets. We\nremark that P is always at most the number of different demand sets desired or\nthe number of clients, and is generally much smaller."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4319v2", 
    "other_authors": "Nikhil Bansal, Uriel Feige, Robert Krauthgamer, Konstantin Makarychev, Viswanath Nagarajan, Joseph, Naor, Roy Schwartz", 
    "title": "Min-Max Graph Partitioning and Small Set Expansion", 
    "arxiv-id": "1110.4319v2", 
    "author": "Roy Schwartz", 
    "publish": "2011-10-19T15:51:38Z", 
    "summary": "We study graph partitioning problems from a min-max perspective, in which an\ninput graph on n vertices should be partitioned into k parts, and the objective\nis to minimize the maximum number of edges leaving a single part. The two main\nversions we consider are where the k parts need to be of equal-size, and where\nthey must separate a set of k given terminals. We consider a common\ngeneralization of these two problems, and design for it an $O(\\sqrt{\\log n\\log\nk})$-approximation algorithm. This improves over an $O(\\log^2 n)$ approximation\nfor the second version, and roughly $O(k\\log n)$ approximation for the first\nversion that follows from other previous work. We also give an improved\nO(1)-approximation algorithm for graphs that exclude any fixed minor.\n  Our algorithm uses a new procedure for solving the Small-Set Expansion\nproblem. In this problem, we are given a graph G and the goal is to find a\nnon-empty set $S\\subseteq V$ of size $|S| \\leq \\rho n$ with minimum\nedge-expansion. We give an $O(\\sqrt{\\log{n}\\log{(1/\\rho)}})$ bicriteria\napproximation algorithm for the general case of Small-Set Expansion, and O(1)\napproximation algorithm for graphs that exclude any fixed minor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4350v1", 
    "other_authors": "Javad Doliskani, Eric Schost", 
    "title": "Taking Roots over High Extensions of Finite Fields", 
    "arxiv-id": "1110.4350v1", 
    "author": "Eric Schost", 
    "publish": "2011-10-19T18:33:28Z", 
    "summary": "We present a new algorithm for computing $m$-th roots over the finite field\n$\\F_q$, where $q = p^n$, with $p$ a prime, and $m$ any positive integer. In the\nparticular case $m=2$, the cost of the new algorithm is an expected\n$O(\\M(n)\\log (p) + \\CC(n)\\log(n))$ operations in $\\F_p$, where $\\M(n)$ and\n$\\CC(n)$ are bounds for the cost of polynomial multiplication and modular\npolynomial composition. Known results give $\\M(n) = O(n\\log (n) \\log\\log (n))$\nand $\\CC(n) = O(n^{1.67})$, so our algorithm is subquadratic in $n$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4428v3", 
    "other_authors": "John Iacono", 
    "title": "Improved Upper Bounds for Pairing Heaps", 
    "arxiv-id": "1110.4428v3", 
    "author": "John Iacono", 
    "publish": "2011-10-20T02:43:29Z", 
    "summary": "Pairing heaps are shown to have constant amortized time Insert and Meld, thus\nshowing that pairing heaps have the same amortized runtimes as Fibonacci heaps\nfor all operations but Decrease-key."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4493v1", 
    "other_authors": "Francisco Claude, Gonzalo Navarro", 
    "title": "Improved Grammar-Based Compressed Indexes", 
    "arxiv-id": "1110.4493v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-10-20T11:03:07Z", 
    "summary": "We introduce the first grammar-compressed representation of a sequence that\nsupports searches in time that depends only logarithmically on the size of the\ngrammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar\nof $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of\nthe lengths of the right hands of the rules), a basic grammar-based\nrepresentation of $T$ takes $N\\lg n$ bits of space. Our representation requires\n$2N\\lg n + N\\lg u + \\epsilon\\, n\\lg n + o(N\\lg n)$ bits of space, for any\n$0<\\epsilon \\le 1$. It can find the positions of the $occ$ occurrences of a\npattern of length $m$ in $T$ in $O((m^2/\\epsilon)\\lg (\\frac{\\lg u}{\\lg n})\n+occ\\lg n)$ time, and extract any substring of length $\\ell$ of $T$ in time\n$O(\\ell+h\\lg(N/h))$, where $h$ is the height of the grammar tree."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4604v2", 
    "other_authors": "Hyung-Chan An, Robert Kleinberg, David B. Shmoys", 
    "title": "Improving Christofides' Algorithm for the s-t Path TSP", 
    "arxiv-id": "1110.4604v2", 
    "author": "David B. Shmoys", 
    "publish": "2011-10-20T18:50:21Z", 
    "summary": "We present a deterministic (1+sqrt(5))/2-approximation algorithm for the s-t\npath TSP for an arbitrary metric. Given a symmetric metric cost on n vertices\nincluding two prespecified endpoints, the problem is to find a shortest\nHamiltonian path between the two endpoints; Hoogeveen showed that the natural\nvariant of Christofides' algorithm is a 5/3-approximation algorithm for this\nproblem, and this asymptotically tight bound in fact has been the best\napproximation ratio known until now. We modify this algorithm so that it\nchooses the initial spanning tree based on an optimal solution to the Held-Karp\nrelaxation rather than a minimum spanning tree; we prove this simple but\ncrucial modification leads to an improved approximation ratio, surpassing the\n20-year-old barrier set by the natural Christofides' algorithm variant. Our\nalgorithm also proves an upper bound of (1+sqrt(5))/2 on the integrality gap of\nthe path-variant Held-Karp relaxation. The techniques devised in this paper can\nbe applied to other optimization problems as well: these applications include\nimproved approximation algorithms and improved LP integrality gap upper bounds\nfor the prize-collecting s-t path problem and the unit-weight graphical metric\ns-t path TSP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.4860v2", 
    "other_authors": "Jan Vondrak", 
    "title": "Symmetry and approximability of submodular maximization problems", 
    "arxiv-id": "1110.4860v2", 
    "author": "Jan Vondrak", 
    "publish": "2011-10-21T18:31:38Z", 
    "summary": "A number of recent results on optimization problems involving submodular\nfunctions have made use of the multilinear relaxation of the problem. These\nresults hold typically in the value oracle model, where the objective function\nis accessible via a black box returning f(S) for a given S. We present a\ngeneral approach to deriving inapproximability results in the value oracle\nmodel, based on the notion of symmetry gap. Our main result is that for any\nfixed instance that exhibits a certain symmetry gap in its multilinear\nrelaxation, there is a naturally related class of instances for which a better\napproximation factor than the symmetry gap would require exponentially many\noracle queries. This unifies several known hardness results for submodular\nmaximization, and implies several new ones. In particular, we prove that there\nis no constant-factor approximation for the problem of maximizing a\nnon-negative submodular function over the bases of a matroid. We also provide a\nclosely matching approximation algorithm for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.5236v2", 
    "other_authors": "Philip Bille, Inge Li Goertz, Hjalte Wedel Vildh\u00f8j, S\u00f8ren Vind", 
    "title": "String Indexing for Patterns with Wildcards", 
    "arxiv-id": "1110.5236v2", 
    "author": "S\u00f8ren Vind", 
    "publish": "2011-10-24T13:57:08Z", 
    "summary": "We consider the problem of indexing a string $t$ of length $n$ to report the\noccurrences of a query pattern $p$ containing $m$ characters and $j$ wildcards.\nLet $occ$ be the number of occurrences of $p$ in $t$, and $\\sigma$ the size of\nthe alphabet. We obtain the following results.\n  - A linear space index with query time $O(m+\\sigma^j \\log \\log n + occ)$.\nThis significantly improves the previously best known linear space index by Lam\net al. [ISAAC 2007], which requires query time $\\Theta(jn)$ in the worst case.\n  - An index with query time $O(m+j+occ)$ using space $O(\\sigma^{k^2} n \\log^k\n\\log n)$, where $k$ is the maximum number of wildcards allowed in the pattern.\nThis is the first non-trivial bound with this query time.\n  - A time-space trade-off, generalizing the index by Cole et al. [STOC 2004].\n  We also show that these indexes can be generalized to allow variable length\ngaps in the pattern. Our results are obtained using a novel combination of\nwell-known and new techniques, which could be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1110.6600v2", 
    "other_authors": "Rene Sitters", 
    "title": "The generalized work function algorithm is competitive for the   generalized 2-server problem", 
    "arxiv-id": "1110.6600v2", 
    "author": "Rene Sitters", 
    "publish": "2011-10-30T11:04:36Z", 
    "summary": "The generalized 2-server problem is an online optimization problem where a\nsequence of requests has to be served at minimal cost. Requests arrive one by\none and need to be served instantly by at least one of two servers. We consider\nthe general model where the cost function of the two servers may be different.\nFormally, each server moves in its own metric space and a request consists of\none point in each metric space. It is served by moving one of the two servers\nto its request point. Requests have to be served without knowledge of the\nfuture requests. The objective is to minimize the total traveled distance. The\nspecial case where both servers move on the real line is known as the\nCNN-problem. We show that the generalized work function algorithm is constant\ncompetitive for the generalized 2-server problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0184v3", 
    "other_authors": "Christian Konrad, Fr\u00e9d\u00e9ric Magniez, Claire Mathieu", 
    "title": "Maximum Matching in Semi-Streaming with Few Passes", 
    "arxiv-id": "1112.0184v3", 
    "author": "Claire Mathieu", 
    "publish": "2011-12-01T14:13:45Z", 
    "summary": "In the semi-streaming model, an algorithm receives a stream of edges of a\ngraph in arbitrary order and uses a memory of size $O(n \\mbox{ polylog } n)$,\nwhere $n$ is the number of vertices of a graph. In this work, we present\nsemi-streaming algorithms that perform one or two passes over the input stream\nfor maximum matching with no restrictions on the input graph, and for the\nimportant special case of bipartite graphs that we refer to as maximum\nbipartite matching (MBM). The Greedy matching algorithm performs one pass over\nthe input and outputs a $1/2$ approximation. Whether there is a better one-pass\nalgorithm has been an open question since the appearance of the first paper on\nstreaming algorithms for matching problems in 2005 [Feigenbaum et al., SODA\n2005]. We make the following progress on this problem:\n  In the one-pass setting, we show that there is a deterministic semi-streaming\nalgorithm for MBM with expected approximation factor $1/2+0.005$, assuming that\nedges arrive one by one in (uniform) random order. We extend this algorithm to\ngeneral graphs, and we obtain a $1/2+0.003$ approximation.\n  In the two-pass setting, we do not require the random arrival order\nassumption (the edge stream is in arbitrary order). We present a simple\nrandomized two-pass semi-streaming algorithm for MBM with expected\napproximation factor $1/2 + 0.019$. Furthermore, we discuss a more involved\ndeterministic two-pass semi-streaming algorithm for MBM with approximation\nfactor $1/2 + 0.019$ and a generalization of this algorithm to general graphs\nwith approximation factor $1/2 + 0.0071$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0278v2", 
    "other_authors": "Tian-Ming Bu, Chen Yuan, Peng Zhang", 
    "title": "Computing on Binary Strings", 
    "arxiv-id": "1112.0278v2", 
    "author": "Peng Zhang", 
    "publish": "2011-12-01T19:24:16Z", 
    "summary": "Many problems in Computer Science can be abstracted to the following\nquestion: given a set of objects and rules respectively, which new objects can\nbe produced? In the paper, we consider a succinct version of the question:\ngiven a set of binary strings and several operations like conjunction and\ndisjunction, which new binary strings can be generated? Although it is a\nfundamental problem, to the best of our knowledge, the problem hasn't been\nstudied yet. In this paper, an O(m^2n) algorithm is presented to determine\nwhether a string s is representable by a set W, where n is the number of\nstrings in W and each string has the same length m. However, looking for the\nminimum subset from a set to represent a given string is shown to be NP-hard.\nAlso, finding the smallest subset from a set to represent each string in the\noriginal set is NP-hard. We establishes inapproximability results and\napproximation algorithms for them. In addition, we prove that counting the\nnumber of strings representable is #P-complete. We then explore how the\nproblems change when the operator negation is available. For example, if the\noperator negation can be used, the number is some power of 2. This difference\nmaybe help us understand the problem more profoundly."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0534v1", 
    "other_authors": "Christoph D\u00fcrr, Maurice Queyranne, Frits C. R. Spieksma, Fabrice Talla Nobibon, Gerhard J. Woeginger", 
    "title": "The interval ordering problem", 
    "arxiv-id": "1112.0534v1", 
    "author": "Gerhard J. Woeginger", 
    "publish": "2011-12-02T18:35:06Z", 
    "summary": "For a given set of intervals on the real line, we consider the problem of\nordering the intervals with the goal of minimizing an objective function that\ndepends on the exposed interval pieces (that is, the pieces that are not\ncovered by earlier intervals in the ordering). This problem is motivated by an\napplication in molecular biology that concerns the determination of the\nstructure of the backbone of a protein.\n  We present polynomial-time algorithms for several natural special cases of\nthe problem that cover the situation where the interval boundaries are\nagreeably ordered and the situation where the interval set is laminar. Also the\nbottleneck variant of the problem is shown to be solvable in polynomial time.\nFinally we prove that the general problem is NP-hard, and that the existence of\na constant-factor-approximation algorithm is unlikely."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0784v1", 
    "other_authors": "Michael A. Bender, Jeremy T. Fineman, Seth Gilbert, Robert E. Tarjan", 
    "title": "A New Approach to Incremental Cycle Detection and Related Problems", 
    "arxiv-id": "1112.0784v1", 
    "author": "Robert E. Tarjan", 
    "publish": "2011-12-04T18:12:38Z", 
    "summary": "We consider the problem of detecting a cycle in a directed graph that grows\nby arc insertions, and the related problems of maintaining a topological order\nand the strong components of such a graph. For these problems, we give two\nalgorithms, one suited to sparse graphs, and the other to dense graphs. The\nformer takes the minimum of O(m^{3/2}) and O(mn^{2/3}) time to insert m arcs\ninto an n-vertex graph; the latter takes O(n^2 log(n)) time. Our sparse\nalgorithm is considerably simpler than a previous O(m^{3/2})-time algorithm; it\nis also faster on graphs of sufficient density. The time bound of our dense\nalgorithm beats the previously best time bound of O(n^{5/2}) for dense graphs.\nOur algorithms rely for their efficiency on topologically ordered vertex\nnumberings; bounds on the size of the numbers give bound on running times."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0790v1", 
    "other_authors": "Ran Duan, Seth Pettie, Hsin-Hao Su", 
    "title": "Scaling algorithms for approximate and exact maximum weight matching", 
    "arxiv-id": "1112.0790v1", 
    "author": "Hsin-Hao Su", 
    "publish": "2011-12-04T20:05:24Z", 
    "summary": "The {\\em maximum cardinality} and {\\em maximum weight matching} problems can\nbe solved in time $\\tilde{O}(m\\sqrt{n})$, a bound that has resisted improvement\ndespite decades of research. (Here $m$ and $n$ are the number of edges and\nvertices.) In this article we demonstrate that this \"$m\\sqrt{n}$ barrier\" is\nextremely fragile, in the following sense. For any $\\epsilon>0$, we give an\nalgorithm that computes a $(1-\\epsilon)$-approximate maximum weight matching in\n$O(m\\epsilon^{-1}\\log\\epsilon^{-1})$ time, that is, optimal {\\em linear time}\nfor any fixed $\\epsilon$. Our algorithm is dramatically simpler than the best\nexact maximum weight matching algorithms on general graphs and should be\nappealing in all applications that can tolerate a negligible relative error.\n  Our second contribution is a new {\\em exact} maximum weight matching\nalgorithm for integer-weighted bipartite graphs that runs in time\n$O(m\\sqrt{n}\\log N)$. This improves on the $O(Nm\\sqrt{n})$-time and\n$O(m\\sqrt{n}\\log(nN))$-time algorithms known since the mid 1980s, for $1\\ll\n\\log N \\ll \\log n$. Here $N$ is the maximum integer edge weight."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.0993v1", 
    "other_authors": "Amr Elmasry, Jyrki Katajainen", 
    "title": "Worst-Case Optimal Priority Queues via Extended Regular Counters", 
    "arxiv-id": "1112.0993v1", 
    "author": "Jyrki Katajainen", 
    "publish": "2011-12-05T16:55:27Z", 
    "summary": "We consider the classical problem of representing a collection of priority\nqueues under the operations \\Findmin{}, \\Insert{}, \\Decrease{}, \\Meld{},\n\\Delete{}, and \\Deletemin{}. In the comparison-based model, if the first four\noperations are to be supported in constant time, the last two operations must\ntake at least logarithmic time. Brodal showed that his worst-case efficient\npriority queues achieve these worst-case bounds. Unfortunately, this data\nstructure is involved and the time bounds hide large constants. We describe a\nnew variant of the worst-case efficient priority queues that relies on extended\nregular counters and provides the same asymptotic time and space bounds as the\noriginal. Due to the conceptual separation of the operations on regular\ncounters and all other operations, our data structure is simpler and easier to\ndescribe and understand. Also, the constants in the time and space bounds are\nsmaller. In addition, we give an implementation of our structure on a pointer\nmachine. For our pointer-machine implementation, \\Decrease{} and \\Meld{} are\nasymptotically slower and require $O(\\lg\\lg{n})$ worst-case time, where $n$\ndenotes the number of elements stored in the resulting priority queue."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.1116v6", 
    "other_authors": "Oren Weimann, Raphael Yuster", 
    "title": "Approximating the Diameter of Planar Graphs in Near Linear Time", 
    "arxiv-id": "1112.1116v6", 
    "author": "Raphael Yuster", 
    "publish": "2011-12-05T22:46:04Z", 
    "summary": "We present a $(1+\\epsilon)$-approximation algorithm running in\n$O(f(\\epsilon)\\cdot n \\log^4 n)$ time for finding the diameter of an undirected\nplanar graph with non-negative edge lengths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10479-012-1161-y", 
    "link": "http://arxiv.org/pdf/1112.1136v1", 
    "other_authors": "Siddharth Barman, Seeun Umboh, Shuchi Chawla, David Malec", 
    "title": "Secretary Problems with Convex Costs", 
    "arxiv-id": "1112.1136v1", 
    "author": "David Malec", 
    "publish": "2011-12-06T01:01:44Z", 
    "summary": "We consider online resource allocation problems where given a set of requests\nour goal is to select a subset that maximizes a value minus cost type of\nobjective function. Requests are presented online in random order, and each\nrequest possesses an adversarial value and an adversarial size. The online\nalgorithm must make an irrevocable accept/reject decision as soon as it sees\neach request. The \"profit\" of a set of accepted requests is its total value\nminus a convex cost function of its total size. This problem falls within the\nframework of secretary problems. Unlike previous work in that area, one of the\nmain challenges we face is that the objective function can be positive or\nnegative and we must guard against accepting requests that look good early on\nbut cause the solution to have an arbitrarily large cost as more requests are\naccepted. This requires designing new techniques.\n  We study this problem under various feasibility constraints and present\nonline algorithms with competitive ratios only a constant factor worse than\nthose known in the absence of costs for the same feasibility constraints. We\nalso consider a multi-dimensional version of the problem that generalizes\nmulti-dimensional knapsack within a secretary framework. In the absence of any\nfeasibility constraints, we present an O(l) competitive algorithm where l is\nthe number of dimensions; this matches within constant factors the best known\nratio for multi-dimensional knapsack secretary."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.1444v2", 
    "other_authors": "Xavier Allamigeon", 
    "title": "On the complexity of strongly connected components in directed   hypergraphs", 
    "arxiv-id": "1112.1444v2", 
    "author": "Xavier Allamigeon", 
    "publish": "2011-12-06T23:26:05Z", 
    "summary": "We study the complexity of some algorithmic problems on directed hypergraphs\nand their strongly connected components (SCCs). The main contribution is an\nalmost linear time algorithm computing the terminal strongly connected\ncomponents (i.e. SCCs which do not reach any components but themselves).\n\"Almost linear\" here means that the complexity of the algorithm is linear in\nthe size of the hypergraph up to a factor alpha(n), where alpha is the inverse\nof Ackermann function, and n is the number of vertices. Our motivation to study\nthis problem arises from a recent application of directed hypergraphs to\ncomputational tropical geometry.\n  We also discuss the problem of computing all SCCs. We establish a superlinear\nlower bound on the size of the transitive reduction of the reachability\nrelation in directed hypergraphs, showing that it is combinatorially more\ncomplex than in directed graphs. Besides, we prove a linear time reduction from\nthe well-studied problem of finding all minimal sets among a given family to\nthe problem of computing the SCCs. Only subquadratic time algorithms are known\nfor the former problem. These results strongly suggest that the problem of\ncomputing the SCCs is harder in directed hypergraphs than in directed graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.1945v2", 
    "other_authors": "Suman Kalyan Bera, Shalmoli Gupta, Amit Kumar, Sambuddha Roy", 
    "title": "Approximation Algorithms for Edge Partitioned Vertex Cover Problems", 
    "arxiv-id": "1112.1945v2", 
    "author": "Sambuddha Roy", 
    "publish": "2011-12-08T20:56:07Z", 
    "summary": "We consider a natural generalization of the Partial Vertex Cover problem.\nHere an instance consists of a graph G = (V,E), a positive cost function c: V->\nZ^{+}, a partition $P_1,..., P_r$ of the edge set $E$, and a parameter $k_i$\nfor each partition $P_i$. The goal is to find a minimum cost set of vertices\nwhich cover at least $k_i$ edges from the partition $P_i$. We call this the\nPartition Vertex Cover problem. In this paper, we give matching upper and lower\nbound on the approximability of this problem. Our algorithm is based on a novel\nLP relaxation for this problem. This LP relaxation is obtained by adding\nknapsack cover inequalities to a natural LP relaxation of the problem. We show\nthat this LP has integrality gap of $O(log r)$, where $r$ is the number of sets\nin the partition of the edge set. We also extend our result to more general\nsettings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2143v1", 
    "other_authors": "Robert G\u00f6rke, Andrea Schumm, Dorothea Wagner", 
    "title": "Experiments on Density-Constrained Graph Clustering", 
    "arxiv-id": "1112.2143v1", 
    "author": "Dorothea Wagner", 
    "publish": "2011-12-09T16:43:59Z", 
    "summary": "Clustering a graph means identifying internally dense subgraphs which are\nonly sparsely interconnected. Formalizations of this notion lead to measures\nthat quantify the quality of a clustering and to algorithms that actually find\nclusterings. Since, most generally, corresponding optimization problems are\nhard, heuristic clustering algorithms are used in practice, or other approaches\nwhich are not based on an objective function. In this work we conduct a\ncomprehensive experimental evaluation of the qualitative behavior of greedy\nbottom-up heuristics driven by cut-based objectives and constrained by\nintracluster density, using both real-world data and artificial instances. Our\nstudy documents that a greedy strategy based on local movement is superior to\none based on merging. We further reveal that the former approach generally\noutperforms alternative setups and reference algorithms from the literature in\nterms of its own objective, while a modularity-based algorithm competes\nsurprisingly well. Finally, we exhibit which combinations of cut-based inter-\nand intracluster measures are suitable for identifying a hidden reference\nclustering in synthetic random graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2273v2", 
    "other_authors": "Marek Cygan, Guy Kortsarz, Zeev Nutov", 
    "title": "Steiner Forest Orientation Problems", 
    "arxiv-id": "1112.2273v2", 
    "author": "Zeev Nutov", 
    "publish": "2011-12-10T12:24:13Z", 
    "summary": "We consider connectivity problems with orientation constraints. Given a\ndirected graph $D$ and a collection of ordered node pairs $P$ let $P[D]=\\{(u,v)\n\\in P: D {contains a} uv{-path}}$. In the {\\sf Steiner Forest Orientation}\nproblem we are given an undirected graph $G=(V,E)$ with edge-costs and a set $P\n\\subseteq V \\times V$ of ordered node pairs. The goal is to find a minimum-cost\nsubgraph $H$ of $G$ and an orientation $D$ of $H$ such that $P[D]=P$. We give a\n4-approximation algorithm for this problem.\n  In the {\\sf Maximum Pairs Orientation} problem we are given a graph $G$ and a\nmulti-collection of ordered node pairs $P$ on $V$. The goal is to find an\norientation $D$ of $G$ such that $|P[D]|$ is maximum. Generalizing the result\nof Arkin and Hassin [DAM'02] for $|P|=2$, we will show that for a mixed graph\n$G$ (that may have both directed and undirected edges), one can decide in\n$n^{O(|P|)}$ time whether $G$ has an orientation $D$ with $P[D]=P$ (for\nundirected graphs this problem admits a polynomial time algorithm for any $P$,\nbut it is NP-complete on mixed graphs). For undirected graphs, we will show\nthat one can decide whether $G$ admits an orientation $D$ with $|P[D]| \\geq k$\nin $O(n+m)+2^{O(k\\cdot \\log \\log k)}$ time; hence this decision problem is\nfixed-parameter tractable, which answers an open question from Dorn et al.\n[AMB'11]. We also show that {\\sf Maximum Pairs Orientation} admits ratio\n$O(\\log |P|/\\log\\log |P|)$, which is better than the ratio $O(\\log n/\\log\\log\nn)$ of Gamzu et al. [WABI'10] when $|P|<n$.\n  Finally, we show that the following node-connectivity problem can be solved\nin polynomial time: given a graph $G=(V,E)$ with edge-costs, $s,t \\in V$, and\nan integer $\\ell$, find a min-cost subgraph $H$ of $G$ with an orientation $D$\nsuch that $D$ contains $\\ell$ internally-disjoint $st$-paths, and $\\ell$\ninternally-disjoint $ts$-paths."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.2930v2", 
    "other_authors": "Zachary Friggstad", 
    "title": "Multiple Traveling Salesmen in Asymmetric Metrics", 
    "arxiv-id": "1112.2930v2", 
    "author": "Zachary Friggstad", 
    "publish": "2011-12-13T15:59:58Z", 
    "summary": "We consider some generalizations of the Asymmetric Traveling Salesman Path\nproblem. Suppose we have an asymmetric metric G = (V,A) with two distinguished\nnodes s,t. We are also given a positive integer k. The goal is to find k paths\nof minimum total cost from s to t whose union spans all nodes. We call this the\nk-Person Asymmetric Traveling Salesmen Path problem (k-ATSPP). Our main result\nfor k-ATSPP is a bicriteria approximation that, for some parameter b >= 1 we\nmay choose, finds between k and k + k/b paths of total length O(b log |V|)\ntimes the optimum value of an LP relaxation based on the Held-Karp relaxation\nfor the Traveling Salesman problem. On one extreme this is an O(log\n|V|)-approximation that uses up to 2k paths and on the other it is an O(k log\n|V|)-approximation that uses exactly k paths.\n  Next, we consider the case where we have k pairs of nodes (s_1,t_1), ...,\n(s_k,t_k). The goal is to find an s_i-t_i path for every pair such that each\nnode of G lies on at least one of these paths. Simple approximation algorithms\nare presented for the special cases where the metric is symmetric or where s_i\n= t_i for each i. We also show that the problem can be approximated within a\nfactor O(log n) when k=2. On the other hand, we demonstrate that the general\nproblem cannot be approximated within any bounded ratio unless P = NP."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.3323v1", 
    "other_authors": "Toryn Qwyllyn Klassen, Philipp Woelfel", 
    "title": "Independence of Tabulation-Based Hash Classes", 
    "arxiv-id": "1112.3323v1", 
    "author": "Philipp Woelfel", 
    "publish": "2011-12-14T20:17:50Z", 
    "summary": "A tabulation-based hash function maps a key into d derived characters\nindexing random values in tables that are then combined with bitwise xor\noperations to give the hash. Thorup and Zhang (2004) presented d-wise\nindependent tabulation-based hash classes that use linear maps over finite\nfields to map a key, considered as a vector (a,b), to derived characters. We\nshow that a variant where the derived characters are a+b*i for i=0,..., q-1\n(using integer arithmetic) yielding (2d-1)-wise independence. Our analysis is\nbased on an algebraic property that characterizes k-wise independence of\ntabulation-based hashing schemes, and combines this characterization with a\ngeometric argument. We also prove a non-trivial lower bound on the number of\nderived characters necessary for k-wise independence with our and related hash\nclasses."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.4109v4", 
    "other_authors": "Venkatesan Guruswami, Ali Kemal Sinop", 
    "title": "Approximating Non-Uniform Sparsest Cut via Generalized Spectra", 
    "arxiv-id": "1112.4109v4", 
    "author": "Ali Kemal Sinop", 
    "publish": "2011-12-18T02:41:12Z", 
    "summary": "We give an approximation algorithm for non-uniform sparsest cut with the\nfollowing guarantee: For any $\\epsilon,\\delta \\in (0,1)$, given cost and demand\ngraphs with edge weights $C, D$ respectively, we can find a set $T\\subseteq V$\nwith $\\frac{C(T,V\\setminus T)}{D(T,V\\setminus T)}$ at most\n$\\frac{1+\\epsilon}{\\delta}$ times the optimal non-uniform sparsest cut value,\nin time $2^{r/(\\delta\\epsilon)}\\poly(n)$ provided $\\lambda_r \\ge\n\\Phi^*/(1-\\delta)$. Here $\\lambda_r$ is the $r$'th smallest generalized\neigenvalue of the Laplacian matrices of cost and demand graphs; $C(T,V\\setminus\nT)$ (resp. $D(T,V\\setminus T)$) is the weight of edges crossing the\n$(T,V\\setminus T)$ cut in cost (resp. demand) graph and $\\Phi^*$ is the\nsparsity of the optimal cut. In words, we show that the non-uniform sparsest\ncut problem is easy when the generalized spectrum grows moderately fast. To the\nbest of our knowledge, there were no results based on higher order spectra for\nnon-uniform sparsest cut prior to this work.\n  Even for uniform sparsest cut, the quantitative aspects of our result are\nsomewhat stronger than previous methods. Similar results hold for other\nexpansion measures like edge expansion, normalized cut, and conductance, with\nthe $r$'th smallest eigenvalue of the normalized Laplacian playing the role of\n$\\lambda_r$ in the latter two cases.\n  Our proof is based on an l1-embedding of vectors from a semi-definite program\nfrom the Lasserre hierarchy. The embedded vectors are then rounded to a cut\nusing standard threshold rounding. We hope that the ideas connecting\n$\\ell_1$-embeddings to Lasserre SDPs will find other applications. Another\naspect of the analysis is the adaptation of the column selection paradigm from\nour earlier work on rounding Lasserre SDPs [GS11] to pick a set of edges rather\nthan vertices. This feature is important in order to extend the algorithms to\nnon-uniform sparsest cut."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.4578v1", 
    "other_authors": "Sebastian Kreft, Gonzalo Navarro", 
    "title": "Self-Index based on LZ77 (thesis)", 
    "arxiv-id": "1112.4578v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2011-12-20T06:09:35Z", 
    "summary": "Domains like bioinformatics, version control systems, collaborative editing\nsystems (wiki), and others, are producing huge data collections that are very\nrepetitive. That is, there are few differences between the elements of the\ncollection. This fact makes the compressibility of the collection extremely\nhigh. For example, a collection with all different versions of a Wikipedia\narticle can be compressed up to the 0.1% of its original space, using the\nLempel-Ziv 1977 (LZ77) compression scheme.\n  Many of these repetitive collections handle huge amounts of text data. For\nthat reason, we require a method to store them efficiently, while providing the\nability to operate on them. The most common operations are the extraction of\nrandom portions of the collection and the search for all the occurrences of a\ngiven pattern inside the whole collection.\n  A self-index is a data structure that stores a text in compressed form and\nallows to find the occurrences of a pattern efficiently. On the other hand,\nself-indexes can extract any substring of the collection, hence they are able\nto replace the original text. One of the main goals when using these indexes is\nto store them within main memory.\n  In this thesis we present a scheme for random text extraction from text\ncompressed with a Lempel-Ziv parsing. Additionally, we present a variant of\nLZ77, called LZ-End, that efficiently extracts text using space close to that\nof LZ77.\n  The main contribution of this thesis is the first self-index based on\nLZ77/LZ-End and oriented to repetitive texts, which outperforms the state of\nthe art (the RLCSA self-index) in many aspects. Finally, we present a corpus of\nrepetitive texts, coming from several application domains. We aim at providing\na standard set of texts for research and experimentation, hence this corpus is\npublicly available."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.5153v3", 
    "other_authors": "David P. Woodruff, Qin Zhang", 
    "title": "Tight Bounds for Distributed Functional Monitoring", 
    "arxiv-id": "1112.5153v3", 
    "author": "Qin Zhang", 
    "publish": "2011-12-21T20:56:38Z", 
    "summary": "We resolve several fundamental questions in the area of distributed\nfunctional monitoring, initiated by Cormode, Muthukrishnan, and Yi (SODA,\n2008). In this model there are $k$ sites each tracking their input and\ncommunicating with a central coordinator that continuously maintain an\napproximate output to a function $f$ computed over the union of the inputs. The\ngoal is to minimize the communication.\n  We show the randomized communication complexity of estimating the number of\ndistinct elements up to a $1+\\eps$ factor is $\\tilde{\\Omega}(k/\\eps^2)$,\nimproving the previous $\\Omega(k + 1/\\eps^2)$ bound and matching known upper\nbounds up to a logarithmic factor. For the $p$-th frequency moment $F_p$, $p >\n1$, we improve the previous $\\Omega(k + 1/\\eps^2)$ communication bound to\n$\\tilde{\\Omega}(k^{p-1}/\\eps^2)$. We obtain similar improvements for heavy\nhitters, empirical entropy, and other problems. We also show that we can\nestimate $F_p$, for any $p > 1$, using $\\tilde{O}(k^{p-1}\\poly(\\eps^{-1}))$\ncommunication. This greatly improves upon the previous\n$\\tilde{O}(k^{2p+1}N^{1-2/p} \\poly(\\eps^{-1}))$ bound of Cormode,\nMuthukrishnan, and Yi for general $p$, and their $\\tilde{O}(k^2/\\eps +\nk^{1.5}/\\eps^3)$ bound for $p = 2$. For $p = 2$, our bound resolves their main\nopen question.\n  Our lower bounds are based on new direct sum theorems for approximate\nmajority, and yield significant improvements to problems in the data stream\nmodel, improving the bound for estimating $F_p, p > 2,$ in $t$ passes from\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{2/p} t))$ to\n$\\tilde{\\Omega}(n^{1-2/p}/(\\eps^{4/p} t))$, giving the first bound for\nestimating $F_0$ in $t$ passes of $\\Omega(1/(\\eps^2 t))$ bits of space that\ndoes not use the gap-hamming problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-012-9729-0", 
    "link": "http://arxiv.org/pdf/1112.5396v2", 
    "other_authors": "Saeed Alaei, Mohammad T. Hajiaghayi, Vahid Liaghat, Dan Pei, Barna Saha", 
    "title": "AdCell: Ad Allocation in Cellular Networks", 
    "arxiv-id": "1112.5396v2", 
    "author": "Barna Saha", 
    "publish": "2011-12-22T17:44:55Z", 
    "summary": "With more than four billion usage of cellular phones worldwide, mobile\nadvertising has become an attractive alternative to online advertisements. In\nthis paper, we propose a new targeted advertising policy for Wireless Service\nProviders (WSPs) via SMS or MMS- namely {\\em AdCell}. In our model, a WSP\ncharges the advertisers for showing their ads. Each advertiser has a valuation\nfor specific types of customers in various times and locations and has a limit\non the maximum available budget. Each query is in the form of time and location\nand is associated with one individual customer. In order to achieve a\nnon-intrusive delivery, only a limited number of ads can be sent to each\ncustomer. Recently, new services have been introduced that offer location-based\nadvertising over cellular network that fit in our model (e.g., ShopAlerts by\nAT&T) .\n  We consider both online and offline version of the AdCell problem and develop\napproximation algorithms with constant competitive ratio. For the online\nversion, we assume that the appearances of the queries follow a stochastic\ndistribution and thus consider a Bayesian setting. Furthermore, queries may\ncome from different distributions on different times. This model generalizes\nseveral previous advertising models such as online secretary problem\n\\cite{HKP04}, online bipartite matching \\cite{KVV90,FMMM09} and AdWords\n\\cite{saberi05}. ..."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.5472v3", 
    "other_authors": "Gerth St\u00f8lting Brodal, Casper Kejlberg-Rasmussen", 
    "title": "Cache-Oblivious Implicit Predecessor Dictionaries with the Working Set   Property", 
    "arxiv-id": "1112.5472v3", 
    "author": "Casper Kejlberg-Rasmussen", 
    "publish": "2011-12-22T21:45:16Z", 
    "summary": "In this paper we present an implicit dynamic dictionary with the working-set\nproperty, supporting insert(e) and delete(e) in O(log n) time, predecessor(e)\nin O(log l_{p(e)}) time, successor(e) in O(log l_{s(e)}) time and search(e) in\nO(log min(l_{p(e)},l_{e}, l_{s(e)})) time, where n is the number of elements\nstored in the dictionary, l_{e} is the number of distinct elements searched for\nsince element e was last searched for and p(e) and s(e) are the predecessor and\nsuccessor of e, respectively. The time-bounds are all worst-case. The\ndictionary stores the elements in an array of size n using no additional space.\nIn the cache-oblivious model the log is base B and the cache-obliviousness is\ndue to our black box use of an existing cache-oblivious implicit dictionary.\nThis is the first implicit dictionary supporting predecessor and successor\nsearches in the working-set bound. Previous implicit structures required O(log\nn) time."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.5636v1", 
    "other_authors": "Jan Bul\u00e1nek, Michal Kouck\u00fd, Michael Saks", 
    "title": "Tight lower bounds for online labeling problem", 
    "arxiv-id": "1112.5636v1", 
    "author": "Michael Saks", 
    "publish": "2011-12-23T19:02:01Z", 
    "summary": "We consider the file maintenance problem (also called the online labeling\nproblem) in which n integer items from the set {1,...,r} are to be stored in an\narray of size m >= n. The items are presented sequentially in an arbitrary\norder, and must be stored in the array in sorted order (but not necessarily in\nconsecutive locations in the array). Each new item must be stored in the array\nbefore the next item is received. If r<=m then we can simply store item j in\nlocation j but if r>m then we may have to shift the location of stored items to\nmake space for a newly arrived item. The algorithm is charged each time an item\nis stored in the array, or moved to a new location. The goal is to minimize the\ntotal number of such moves done by the algorithm. This problem is non-trivial\nwhen n=<m<r.\n  In the case that m=Cn for some C>1, algorithms for this problem with cost\nO(log(n)^2) per item have been given [IKR81, Wil92, BCD+02]. When m=n,\nalgorithms with cost O(log(n)^3) per item were given [Zha93, BS07]. In this\npaper we prove lower bounds that show that these algorithms are optimal, up to\nconstant factors. Previously, the only lower bound known for this range of\nparameters was a lower bound of \\Omega(log(n)^2) for the restricted class of\nsmooth algorithms [DSZ05a, Zha93].\n  We also provide an algorithm for the sparse case: If the number of items is\npolylogarithmic in the array size then the problem can be solved in amortized\nconstant time per item."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.6255v1", 
    "other_authors": "Marek Cygan, Marcin Pilipczuk, Micha\u0142 Pilipczuk", 
    "title": "On group feedback vertex set parameterized by the size of the cutset", 
    "arxiv-id": "1112.6255v1", 
    "author": "Micha\u0142 Pilipczuk", 
    "publish": "2011-12-29T09:02:35Z", 
    "summary": "We study the parameterized complexity of a robust generalization of the\nclassical Feedback Vertex Set problem, namely the Group Feedback Vertex Set\nproblem; we are given a graph G with edges labeled with group elements, and the\ngoal is to compute the smallest set of vertices that hits all cycles of G that\nevaluate to a non-null element of the group. This problem generalizes not only\nFeedback Vertex Set, but also Subset Feedback Vertex Set, Multiway Cut and Odd\nCycle Transversal. Completing the results of Guillemot [Discr. Opt. 2011], we\nprovide a fixed-parameter algorithm for the parameterization by the size of the\ncutset only. Our algorithm works even if the group is given as a\npolynomial-time oracle."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1112.6256v2", 
    "other_authors": "Mingfei Li, Christoffer Ma, Li Ning", 
    "title": "(1+epsilon)-Distance Oracle for Planar Labeled Graph", 
    "arxiv-id": "1112.6256v2", 
    "author": "Li Ning", 
    "publish": "2011-12-29T09:23:49Z", 
    "summary": "Given a vertex-labeled graph, each vertex $v$ is attached with a label from a\nset of labels. The vertex-label query desires the length of the shortest path\nfrom the given vertex to the set of vertices with the given label. We show how\nto construct an oracle if the given graph is planar, such that\n$O(\\frac{1}{\\epsilon}n\\log n)$ storing space is needed, and any vertex-label\nquery could be answered in $O(\\frac{1}{\\epsilon}\\log n\\log \\rho)$ time with\nstretch $1+\\epsilon$. $\\rho$ is the radius of the given graph, which is half of\nthe diameter. For the case that $\\rho = O(\\log n)$, we construct an oracle that\nachieves $O(\\log n)$ query time, without changing the order of storing space."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.0749v2", 
    "other_authors": "Gary McGuire, Bastian Tugemann, Gilles Civario", 
    "title": "There is no 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues   Problem", 
    "arxiv-id": "1201.0749v2", 
    "author": "Gilles Civario", 
    "publish": "2012-01-01T19:04:10Z", 
    "summary": "The sudoku minimum number of clues problem is the following question: what is\nthe smallest number of clues that a sudoku puzzle can have? For several years\nit had been conjectured that the answer is 17. We have performed an exhaustive\ncomputer search for 16-clue sudoku puzzles, and did not find any, thus proving\nthat the answer is indeed 17. In this article we describe our method and the\nactual search. As a part of this project we developed a novel way for\nenumerating hitting sets. The hitting set problem is computationally hard; it\nis one of Karp's 21 classic NP-complete problems. A standard backtracking\nalgorithm for finding hitting sets would not be fast enough to search for a\n16-clue sudoku puzzle exhaustively, even at today's supercomputer speeds. To\nmake an exhaustive search possible, we designed an algorithm that allowed us to\nefficiently enumerate hitting sets of a suitable size."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.1157v1", 
    "other_authors": "Krasimir Yordzhev, Ana Markovska", 
    "title": "Method of the Multidimensional Sieve in the Practical Realization of   some Combinatorial Algorithms", 
    "arxiv-id": "1201.1157v1", 
    "author": "Ana Markovska", 
    "publish": "2012-01-05T12:54:30Z", 
    "summary": "Some difficulties regarding the application of the well-known sieve method\nare considered in the case when a practical (program) realization of selecting\nelements, having a particular property among the elements of a set with a\nsufficiently great cardinal number(cardinality). In this paper the problem has\nbeen resolved by using a modified version of the method, utilizing\nmultidimensional arrays. As a theoretical illustration of the method of the\nmultidimensional sieve, the problem of obtaining a single representative of\neach equivalence class with respect to a given relation of equivalence and\nobtaining the cardinality of the respective factor set is considered with\nrelevant mathematical proofs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.1784v1", 
    "other_authors": "St\u00e9phane Martin, Mehdi Ahmed-Nacer, Pascal Urso", 
    "title": "Abstract unordered and ordered trees CRDT", 
    "arxiv-id": "1201.1784v1", 
    "author": "Pascal Urso", 
    "publish": "2012-01-09T14:42:45Z", 
    "summary": "Trees are fundamental data structure for many areas of computer science and\nsystem engineering. In this report, we show how to ensure eventual consistency\nof optimistically replicated trees. In optimistic replication, the different\nreplicas of a distributed system are allowed to diverge but should eventually\nreach the same value if no more mutations occur. A new method to ensure\neventual consistency is to design Conflict-free Replicated Data Types (CRDT).\nIn this report, we design a collection of tree CRDT using existing set CRDTs.\nThe remaining concurrency problems particular to tree data structure are\nresolved using one or two layers of correction algorithm. For each of these\nlayer, we propose different and independent policies. Any combination of set\nCRDT and policies can be constructed, giving to the distributed application\nprogrammer the entire control of the behavior of the shared data in face of\nconcurrent mutations. We also propose to order these trees by adding a\npositioning layer which is also independent to obtain a collection of ordered\ntree CRDTs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2000v1", 
    "other_authors": "Petr Hlineny, Ondrej Moris", 
    "title": "Dynamic Scope-Based Dijkstra's Algorithm", 
    "arxiv-id": "1201.2000v1", 
    "author": "Ondrej Moris", 
    "publish": "2012-01-10T10:11:11Z", 
    "summary": "We briefly report on the current state of a new dynamic algorithm for the\nroute planning problem based on a concept of scope (the static variant\npresented at ESA'11, HM2011A). We first motivate dynamization of the concept of\nscope admissibility, and then we briefly describe a modification of the\nscope-aware query algorithm of HM2011A to dynamic road networks. Finally, we\noutline our future work on this concept."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2501v2", 
    "other_authors": "Haitham Hassanieh, Piotr Indyk, Dina Katabi, Eric Price", 
    "title": "Nearly Optimal Sparse Fourier Transform", 
    "arxiv-id": "1201.2501v2", 
    "author": "Eric Price", 
    "publish": "2012-01-12T08:34:46Z", 
    "summary": "We consider the problem of computing the k-sparse approximation to the\ndiscrete Fourier transform of an n-dimensional signal. We show:\n  * An O(k log n)-time randomized algorithm for the case where the input signal\nhas at most k non-zero Fourier coefficients, and\n  * An O(k log n log(n/k))-time randomized algorithm for general input signals.\n  Both algorithms achieve o(n log n) time, and thus improve over the Fast\nFourier Transform, for any k = o(n). They are the first known algorithms that\nsatisfy this property. Also, if one assumes that the Fast Fourier Transform is\noptimal, the algorithm for the exactly k-sparse case is optimal for any k =\nn^{\\Omega(1)}.\n  We complement our algorithmic results by showing that any algorithm for\ncomputing the sparse Fourier transform of a general signal must use at least\n\\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform\nadaptive sampling."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.2702v1", 
    "other_authors": "Gerth St\u00f8lting Brodal, Alexis C. Kaporis, Apostolos N. Papadopoulos, Spyros Sioutas, Konstantinos Tsakalidis, Kostas Tsichlas", 
    "title": "Dynamic 3-sided Planar Range Queries with Expected Doubly Logarithmic   Time", 
    "arxiv-id": "1201.2702v1", 
    "author": "Kostas Tsichlas", 
    "publish": "2012-01-12T23:00:21Z", 
    "summary": "This work studies the problem of 2-dimensional searching for the 3-sided\nrange query of the form $[a, b]\\times (-\\infty, c]$ in both main and external\nmemory, by considering a variety of input distributions. We present three sets\nof solutions each of which examines the 3-sided problem in both RAM and I/O\nmodel respectively. The presented data structures are deterministic and the\nexpectation is with respect to the input distribution."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.3077v1", 
    "other_authors": "Joseph Yossi Gil, David Allen Scott", 
    "title": "A Bijective String Sorting Transform", 
    "arxiv-id": "1201.3077v1", 
    "author": "David Allen Scott", 
    "publish": "2012-01-15T10:17:36Z", 
    "summary": "Given a string of characters, the Burrows-Wheeler Transform rearranges the\ncharacters in it so as to produce another string of the same length which is\nmore amenable to compression techniques such as move to front, run-length\nencoding, and entropy encoders. We present a variant of the transform which\ngives rise to similar or better compression value, but, unlike the original,\nthe transform we present is bijective, in that the inverse transformation\nexists for all strings. Our experiments indicate that using our variant of the\ntransform gives rise to better compression ratio than the original\nBurrows-Wheeler transform. We also show that both the transform and its inverse\ncan be computed in linear time and consuming linear storage."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.3602v1", 
    "other_authors": "J\u00e9r\u00e9my Barbay, Francisco Claude, Gonzalo Navarro", 
    "title": "Compact Binary Relation Representations with Rich Functionality", 
    "arxiv-id": "1201.3602v1", 
    "author": "Gonzalo Navarro", 
    "publish": "2012-01-17T19:57:11Z", 
    "summary": "Binary relations are an important abstraction arising in many data\nrepresentation problems. The data structures proposed so far to represent them\nsupport just a few basic operations required to fit one particular application.\nWe identify many of those operations arising in applications and generalize\nthem into a wide set of desirable queries for a binary relation representation.\nWe also identify reductions among those operations. We then introduce several\nnovel binary relation representations, some simple and some quite\nsophisticated, that not only are space-efficient but also efficiently support a\nlarge subset of the desired queries."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4206v1", 
    "other_authors": "Ragesh Jaiswal, Amit Kumar, Sandeep Sen", 
    "title": "A simple D^2-sampling based PTAS for k-means and other Clustering   Problems", 
    "arxiv-id": "1201.4206v1", 
    "author": "Sandeep Sen", 
    "publish": "2012-01-20T05:01:48Z", 
    "summary": "Given a set of points $P \\subset \\mathbb{R}^d$, the $k$-means clustering\nproblem is to find a set of $k$ {\\em centers} $C = \\{c_1,...,c_k\\}, c_i \\in\n\\mathbb{R}^d,$ such that the objective function $\\sum_{x \\in P} d(x,C)^2$,\nwhere $d(x,C)$ denotes the distance between $x$ and the closest center in $C$,\nis minimized. This is one of the most prominent objective functions that have\nbeen studied with respect to clustering.\n  $D^2$-sampling \\cite{ArthurV07} is a simple non-uniform sampling technique\nfor choosing points from a set of points. It works as follows: given a set of\npoints $P \\subseteq \\mathbb{R}^d$, the first point is chosen uniformly at\nrandom from $P$. Subsequently, a point from $P$ is chosen as the next sample\nwith probability proportional to the square of the distance of this point to\nthe nearest previously sampled points.\n  $D^2$-sampling has been shown to have nice properties with respect to the\n$k$-means clustering problem. Arthur and Vassilvitskii \\cite{ArthurV07} show\nthat $k$ points chosen as centers from $P$ using $D^2$-sampling gives an\n$O(\\log{k})$ approximation in expectation. Ailon et. al. \\cite{AJMonteleoni09}\nand Aggarwal et. al. \\cite{AggarwalDK09} extended results of \\cite{ArthurV07}\nto show that $O(k)$ points chosen as centers using $D^2$-sampling give $O(1)$\napproximation to the $k$-means objective function with high probability. In\nthis paper, we further demonstrate the power of $D^2$-sampling by giving a\nsimple randomized $(1 + \\epsilon)$-approximation algorithm that uses the\n$D^2$-sampling in its core."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4459v1", 
    "other_authors": "Fatemeh Keshavarz-Kohjerdi, Alireza Bagheri", 
    "title": "An efficient parallel algorithm for the longest path problem in meshes", 
    "arxiv-id": "1201.4459v1", 
    "author": "Alireza Bagheri", 
    "publish": "2012-01-21T10:39:34Z", 
    "summary": "In this paper, first we give a sequential linear-time algorithm for the\nlongest path problem in meshes. This algorithm can be considered as an\nimprovement of [13]. Then based on this sequential algorithm, we present a\nconstant-time parallel algorithm for the problem which can be run on every\nparallel machine."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.4899v2", 
    "other_authors": "Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, Shang-Hua Teng", 
    "title": "Finding Endogenously Formed Communities", 
    "arxiv-id": "1201.4899v2", 
    "author": "Shang-Hua Teng", 
    "publish": "2012-01-24T00:40:37Z", 
    "summary": "A central problem in e-commerce is determining overlapping communities among\nindividuals or objects in the absence of external identification or tagging. We\naddress this problem by introducing a framework that captures the notion of\ncommunities or clusters determined by the relative affinities among their\nmembers. To this end we define what we call an affinity system, which is a set\nof elements, each with a vector characterizing its preference for all other\nelements in the set. We define a natural notion of (potentially overlapping)\ncommunities in an affinity system, in which the members of a given community\ncollectively prefer each other to anyone else outside the community. Thus these\ncommunities are endogenously formed in the affinity system and are\n\"self-determined\" or \"self-certified\" by its members.\n  We provide a tight polynomial bound on the number of self-determined\ncommunities as a function of the robustness of the community. We present a\npolynomial-time algorithm for enumerating these communities. Moreover, we\nobtain a local algorithm with a strong stochastic performance guarantee that\ncan find a community in time nearly linear in the of size the community.\n  Social networks fit particularly naturally within the affinity system\nframework -- if we can appropriately extract the affinities from the relatively\nsparse yet rich information from social networks, our analysis then yields a\nset of efficient algorithms for enumerating self-determined communities in\nsocial networks. In the context of social networks we also connect our analysis\nwith results about $(\\alpha,\\beta)$-clusters introduced by Mishra, Schreiber,\nStanton, and Tarjan \\cite{msst}. In contrast with the polynomial bound we prove\non the number of communities in the affinity system model, we show that there\nexists a family of networks with superpolynomial number of\n$(\\alpha,\\beta)$-clusters."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5030v4", 
    "other_authors": "Guy Even, Moti Medina", 
    "title": "Online Multi-Commodity Flow with High Demands", 
    "arxiv-id": "1201.5030v4", 
    "author": "Moti Medina", 
    "publish": "2012-01-24T16:11:45Z", 
    "summary": "This paper deals with the problem of computing, in an online fashion, a\nmaximum benefit multi-commodity flow (\\ONMCF), where the flow demands may be\nbigger than the edge capacities of the network.\n  We present an online, deterministic, centralized, all-or-nothing, bi-criteria\nalgorithm. The competitive ratio of the algorithm is constant, and the\nalgorithm augments the capacities by at most a logarithmic factor.\n  The algorithm can handle two types of flow requests: (i) low demand requests\nthat must be routed along a path, and (ii) high demand requests that may be\nrouted using a multi-path flow.\n  Two extensions are discussed: requests with known durations and machine\nscheduling."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5513v1", 
    "other_authors": "Aida Ouangraoua, Mathieu Raffinot", 
    "title": "Faster and Simpler Minimal Conflicting Set Identification", 
    "arxiv-id": "1201.5513v1", 
    "author": "Mathieu Raffinot", 
    "publish": "2012-01-26T13:35:33Z", 
    "summary": "Let C be a finite set of N elements and R = r_1,r_2,..., r_m a family of M\nsubsets of C. A subset X of R verifies the Consecutive Ones Property (C1P) if\nthere exists a permutation P of C such that each r_i in X is an interval of P.\nA Minimal Conflicting Set (MCS) S is a subset of R that does not verify the\nC1P, but such that any of its proper subsets does. In this paper, we present a\nnew simpler and faster algorithm to decide if a given element r in R belongs to\nat least one MCS. Our algorithm runs in O(N^2M^2 + NM^7), largely improving the\ncurrent O(M^6N^5 (M+N)^2 log(M+N)) fastest algorithm of [Blin {\\em et al}, CSR\n2011]. The new algorithm is based on an alternative approach considering\nminimal forbidden induced subgraphs of interval graphs instead of Tucker\nmatrices."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.5985v1", 
    "other_authors": "Pascal Berthom\u00e9, Jean-Fran\u00e7ois Lalande, Vincent Levorato", 
    "title": "Implementation of exponential and parametrized algorithms in the AGAPE   project", 
    "arxiv-id": "1201.5985v1", 
    "author": "Vincent Levorato", 
    "publish": "2012-01-28T20:08:26Z", 
    "summary": "This technical report describes the implementation of exact and parametrized\nexponential algorithms, developed during the French ANR Agape during 2010-2012.\nThe developed algorithms are distributed under the CeCILL license and have been\nwritten in Java using the Jung graph library."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.6207v2", 
    "other_authors": "T. Kloks", 
    "title": "k-Probe DH-graphs", 
    "arxiv-id": "1201.6207v2", 
    "author": "T. Kloks", 
    "publish": "2012-01-30T13:51:10Z", 
    "summary": "Let k be a natural number. Let G be a graph and let N_1,...,N_k be k\nindependent sets in G. The graph G is k-probe distance hereditary if G can be\nembedded into a DH-graph by adding edges between vertices that are contained in\nthe same independent set. We show that there exists a polynomial-time algorithm\nto check if a graph G is k-probe distance hereditary."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1201.6421v1", 
    "other_authors": "Ton Kloks", 
    "title": "The black-and-white coloring problem on permutation graphs", 
    "arxiv-id": "1201.6421v1", 
    "author": "Ton Kloks", 
    "publish": "2012-01-31T02:40:54Z", 
    "summary": "Given a graph G and integers b and w. The black-and-white coloring problem\nasks if there exist disjoint sets of vertices B and W with |B|=b and |W|=w such\nthat no vertex in B is adjacent to any vertex in W. In this paper we show that\nthe problem is polynomial when restricted to permutation graphs."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1203.0120v1", 
    "other_authors": "Mita Pal, Soubhik Chakraborty, N. C. Mahanti", 
    "title": "How does the Shift-insertion sort behave when the sorting elements   follow a Normal distribution?", 
    "arxiv-id": "1203.0120v1", 
    "author": "N. C. Mahanti", 
    "publish": "2012-03-01T08:55:06Z", 
    "summary": "The present paper examines the behavior of Shift-insertion sort (insertion\nsort with shifting) for normal distribution inputs and is in continuation of\nour earlier work on this new algorithm for discrete distribution inputs,\nnamely, negative binomial. Shift insertion sort is found more sensitive for\nmain effects but not for all interaction effects compared to conventional\ninsertion sort."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1203.0259v1", 
    "other_authors": "Boris Alexeev, M. Brian Jacokes", 
    "title": "A rearrangement step with potential uses in priority queues", 
    "arxiv-id": "1203.0259v1", 
    "author": "M. Brian Jacokes", 
    "publish": "2012-03-01T18:21:14Z", 
    "summary": "Link-based data structures, such as linked lists and binary search trees,\nhave many well-known rearrangement steps allowing for efficient implementations\nof insertion, deletion, and other operations. We describe a rearrangement\nprimitive designed for link-based, heap-ordered priority queues in the\ncomparison model, such as those similar to Fibonacci heaps or binomial heaps.\n  In its most basic form, the primitive rearranges a collection of heap-ordered\nperfect binary trees. Doing so offers a data structure control on the number of\ntrees involved in such a collection, in particular keeping this number\nlogarithmic in the number of elements. The rearrangement step is free from an\namortized complexity standpoint (using an appropriate potential function)."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1203.0289v3", 
    "other_authors": "Varsha Dani, Valerie King, Mahnush Movahedi, Jared Saia, Mahdi Zamani", 
    "title": "Secure Multi-Party Computation in Large Networks", 
    "arxiv-id": "1203.0289v3", 
    "author": "Mahdi Zamani", 
    "publish": "2012-03-01T20:44:41Z", 
    "summary": "We describe scalable protocols for solving the secure multi-party computation\n(MPC) problem among a large number of parties. We consider both the synchronous\nand the asynchronous communication models. In the synchronous setting, our\nprotocol is secure against a static malicious adversary corrupting less than a\n$1/3$ fraction of the parties. In the asynchronous setting, we allow the\nadversary to corrupt less than a $1/8$ fraction of parties. For any\ndeterministic function that can be computed by an arithmetic circuit with $m$\ngates, both of our protocols require each party to send a number of field\nelements and perform an amount of computation that is $\\tilde{O}(m/n + \\sqrt\nn)$. We also show that our protocols provide perfect and universally-composable\nsecurity.\n  To achieve our asynchronous MPC result, we define the \\emph{threshold\ncounting problem} and present a distributed protocol to solve it in the\nasynchronous setting. This protocol is load balanced, with computation,\ncommunication and latency complexity of $O(\\log{n})$, and can also be used for\ndesigning other load-balanced applications in the asynchronous communication\nmodel."
},{
    "category": "cs.DS", 
    "doi": "10.4230/LIPIcs.STACS.2012.112", 
    "link": "http://arxiv.org/pdf/1203.1250v1", 
    "other_authors": "Olusegun Folorunso, Olufunke R. Vincent, Oluwatimilehin Salako", 
    "title": "An Exploratory Study of Critical Factors Affecting the Efficiency of   Sorting Techniques (Shell, Heap and Treap)", 
    "arxiv-id": "1203.1250v1", 
    "author": "Oluwatimilehin Salako", 
    "publish": "2012-03-03T21:09:42Z", 
    "summary": "The efficiency of sorting techniques has a significant impact on the overall\nefficiency of a program. The efficiency of Shell, Heap and Treap sorting\ntechniques in terms of both running time and memory usage was studied,\nexperiments conducted and results subjected to factor analysis by SPSS. The\nstudy revealed the main factor affecting these sorting techniques was time\ntaken to sort."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcsea.2012.2103", 
    "link": "http://arxiv.org/pdf/1203.1830v2", 
    "other_authors": "Niraj Kumar Singh, Mita Pal, Soubhik Chakraborty", 
    "title": "Partition Sort Revisited: Reconfirming the Robustness in Average Case   and much more!", 
    "arxiv-id": "1203.1830v2", 
    "author": "Soubhik Chakraborty", 
    "publish": "2012-03-08T15:42:04Z", 
    "summary": "In our previous work there was some indication that Partition Sort could be\nhaving a more robust average case O(nlogn) complexity than the popular Quick\nSort. In our first study in this paper, we reconfirm this through computer\nexperiments for inputs from Cauchy distribution for which expectation\ntheoretically does not exist. Additionally, the algorithm is found to be\nsensitive to parameters of the input probability distribution demanding further\ninvestigation on parameterized complexity. The results on this algorithm for\nBinomial inputs in our second study are very encouraging in that direction."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcsea.2012.2103", 
    "link": "http://arxiv.org/pdf/1203.2414v1", 
    "other_authors": "Einollah Pira", 
    "title": "An Optimal Algorithm for Conflict-Free Coloring for Tree of Rings", 
    "arxiv-id": "1203.2414v1", 
    "author": "Einollah Pira", 
    "publish": "2012-03-12T07:20:56Z", 
    "summary": "An optimal algorithm is presented about Conflict-Free Coloring for connected\nsubgraphs of tree of rings. Suppose the number of the rings in the tree is |T|\nand the maximum length of rings is |R|. A presented algorithm in [1] for a Tree\nof rings used O(log|T|.log|R|) colors but this algorithm uses O(log|T|+log|R|)\ncolors. The coloring earned by this algorithm has the unique-min property, that\nis, the unique color is also minimum."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.2538v3", 
    "other_authors": "Kitty Meeks, Alexander Scott", 
    "title": "Spanning trees and the complexity of flood-filling games", 
    "arxiv-id": "1203.2538v3", 
    "author": "Alexander Scott", 
    "publish": "2012-03-12T16:29:42Z", 
    "summary": "We consider problems related to the combinatorial game (Free-)Flood-It, in\nwhich players aim to make a coloured graph monochromatic with the minimum\npossible number of flooding operations. We show that the minimum number of\nmoves required to flood any given graph G is equal to the minimum, taken over\nall spanning trees T of G, of the number of moves required to flood T. This\nresult is then applied to give two polynomial-time algorithms for flood-filling\nproblems. Firstly, we can compute in polynomial time the minimum number of\nmoves required to flood a graph with only a polynomial number of connected\nsubgraphs. Secondly, given any coloured connected graph and a subset of the\nvertices of bounded size, the number of moves required to connect this subset\ncan be computed in polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.2543v2", 
    "other_authors": "H\u00e9lio B. Mac\u00eado Filho, Simone Dantas, Raphael C. S. Machado, Celina M. H. de Figueiredo", 
    "title": "Biclique-colouring verification complexity and biclique-colouring power   graphs", 
    "arxiv-id": "1203.2543v2", 
    "author": "Celina M. H. de Figueiredo", 
    "publish": "2012-03-12T16:45:21Z", 
    "summary": "Biclique-colouring is a colouring of the vertices of a graph in such a way\nthat no maximal complete bipartite subgraph with at least one edge is\nmonochromatic. We show that it is coNP-complete to check whether a given\nfunction that associates a colour to each vertex is a biclique-colouring, a\nresult that justifies the search for structured classes where the\nbiclique-colouring problem could be efficiently solved. We consider\nbiclique-colouring restricted to powers of paths and powers of cycles. We\ndetermine the biclique-chromatic number of powers of paths and powers of\ncycles. The biclique-chromatic number of a power of a path P_{n}^{k} is max(2k\n+ 2 - n, 2) if n >= k + 1 and exactly n otherwise. The biclique-chromatic\nnumber of a power of a cycle C_n^k is at most 3 if n >= 2k + 2 and exactly n\notherwise; we additionally determine the powers of cycles that are\n2-biclique-colourable. All proofs are algorithmic and provide polynomial-time\nbiclique-colouring algorithms for graphs in the investigated classes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3415v5", 
    "other_authors": "Luis A. A. Meira, Vinicius R. M\u00e1ximo, \u00c1lvaro L. Fazenda, Arlindo F. da Concei\u00e7\u00e3o", 
    "title": "acc-Motif Detection Tool", 
    "arxiv-id": "1203.3415v5", 
    "author": "Arlindo F. da Concei\u00e7\u00e3o", 
    "publish": "2012-03-15T16:54:42Z", 
    "summary": "Network motif algorithms have been a topic of research mainly after the\n2002-seminal paper from Milo \\emph{et al}, that provided motifs as a way to\nuncover the basic building blocks of most networks. In Bioinformatics, motifs\nhave been mainly applied in the field of gene regulation networks. This paper\nproposes new algorithms to exactly count isomorphic pattern motifs of sizes 3,\n4 and 5 in directed graphs. Let $G(V,E)$ be a directed graph with $m=|E|$. We\ndescribe an $O({m\\sqrt{m}})$ time complexity algorithm to count isomorphic\npatterns of size 3. In order to count isomorphic patterns of size 4, we propose\nan $O(m^2)$ algorithm. To count patterns with 5 vertices, the algorithm is\n$O(m^2n)$. The new algorithms were implemented and compared with FANMOD and\nKavosh motif detection tools. The experiments show that our algorithms are\nexpressively faster than FANMOD and Kavosh's. We also let our motif-detecting\ntool available in the Internet."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3578v4", 
    "other_authors": "Takuro Fukunaga, Zeev Nutov, R. Ravi", 
    "title": "Iterative rounding approximation algorithms for degree-bounded   node-connectivity network design", 
    "arxiv-id": "1203.3578v4", 
    "author": "R. Ravi", 
    "publish": "2012-03-15T21:35:37Z", 
    "summary": "We consider the problem of finding a minimum edge cost subgraph of a graph\nsatisfying both given node-connectivity requirements and degree upper bounds on\nnodes. We present an iterative rounding algorithm of the biset LP relaxation\nfor this problem. For directed graphs and $k$-out-connectivity requirements\nfrom a root, our algorithm computes a solution that is a 2-approximation on the\ncost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$\nwhere $b(v)$ is the degree upper bound on $v$. For undirected graphs and\nelement-connectivity requirements with maximum connectivity requirement $k$,\nour algorithm computes a solution that is a $4$-approximation on the cost, and\nthe degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These\nratios improve the previous $O(\\log k)$-approximation on the cost and $O(2^k\nb(v))$ approximation on the degrees. Our algorithms can be used to improve\napproximation ratios for other node-connectivity problems such as undirected\n$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected\nrooted $k$-connectivity and subset $k$-connectivity."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3593v1", 
    "other_authors": "Peiji Chen, Wenjing Ma, Srinath Mandalapu, Chandrashekhar Nagarajan, Jayavel Shanmugasundaram, Sergei Vassilvitskii, Erik Vee, Manfai Yu, Jason Zien", 
    "title": "Ad Serving Using a Compact Allocation Plan", 
    "arxiv-id": "1203.3593v1", 
    "author": "Jason Zien", 
    "publish": "2012-03-16T00:31:18Z", 
    "summary": "A large fraction of online display advertising is sold via guaranteed\ncontracts: a publisher guarantees to the advertiser a certain number of user\nvisits satisfying the targeting predicates of the contract. The publisher is\nthen tasked with solving the ad serving problem - given a user visit, which of\nthe thousands of matching contracts should be displayed, so that by the\nexpiration time every contract has obtained the requisite number of user\nvisits. The challenges of the problem come from (1) the sheer size of the\nproblem being solved, with tens of thousands of contracts and billions of user\nvisits, (2) the unpredictability of user behavior, since these contracts are\nsold months ahead of time, when only a forecast of user visits is available and\n(3) the minute amount of resources available online, as an ad server must\nrespond with a matching contract in a fraction of a second.\n  We present a solution to the guaranteed delivery ad serving problem using\n{\\em compact allocation plans}. These plans, computed offline, can be\nefficiently queried by the ad server during an ad call; they are small, using\nonly O(1) space for contract; and are stateless, allowing for distributed\nserving without any central coordination. We evaluate this approach on a real\nset of user visits and guaranteed contracts and show that the compact\nallocation plans are an effective way of solving the guaranteed delivery ad\nserving problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3619v1", 
    "other_authors": "Vijay Bharadwaj, Peiji Chen, Wenjing Ma, Chandrashekhar Nagarajan, John Tomlin, Sergei Vassilvitskii, Erik Vee, Jian Yang", 
    "title": "SHALE: An Efficient Algorithm for Allocation of Guaranteed Display   Advertising", 
    "arxiv-id": "1203.3619v1", 
    "author": "Jian Yang", 
    "publish": "2012-03-16T06:02:29Z", 
    "summary": "Motivated by the problem of optimizing allocation in guaranteed display\nadvertising, we develop an efficient, lightweight method of generating a\ncompact {\\em allocation plan} that can be used to guide ad server decisions.\nThe plan itself uses just O(1) state per guaranteed contract, is robust to\nnoise, and allows us to serve (provably) nearly optimally. The optimization\nmethod we develop is scalable, with a small in-memory footprint, and working in\nlinear time per iteration. It is also \"stop-anytime\", meaning that\ntime-critical applications can stop early and still get a good serving\nsolution. Thus, it is particularly useful for optimizing the large problems\narising in the context of display advertising. We demonstrate the effectiveness\nof our algorithm using actual Yahoo! data."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.3883v1", 
    "other_authors": "Igor S. Sergeev", 
    "title": "A note on the fast power series' exponential", 
    "arxiv-id": "1203.3883v1", 
    "author": "Igor S. Sergeev", 
    "publish": "2012-03-17T18:20:32Z", 
    "summary": "It is shown that the exponential of a complex power series up to order n can\nbe implemented via (23/12+o(1))M(n) binary arithmetic operations over complex\nfield, where M(n) stands for the (smoothed) complexity of multiplication of\npolynomials of degree <n in FFT-model. Yet, it is shown how to raise a power\nseries to a constant power with the complexity (27/8+o(1))M(n)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4117v1", 
    "other_authors": "Martin Dietzfelbinger, Hendrik Peilke, Michael Rink", 
    "title": "A More Reliable Greedy Heuristic for Maximum Matchings in Sparse Random   Graphs", 
    "arxiv-id": "1203.4117v1", 
    "author": "Michael Rink", 
    "publish": "2012-03-19T14:27:00Z", 
    "summary": "We propose a new greedy algorithm for the maximum cardinality matching\nproblem. We give experimental evidence that this algorithm is likely to find a\nmaximum matching in random graphs with constant expected degree c>0,\nindependent of the value of c. This is contrary to the behavior of commonly\nused greedy matching heuristics which are known to have some range of c where\nthey probably fail to compute a maximum matching."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4619v1", 
    "other_authors": "Yossi Azar, Debmalya Panigrahi", 
    "title": "Online Load Balancing on Unrelated Machines with Startup Costs", 
    "arxiv-id": "1203.4619v1", 
    "author": "Debmalya Panigrahi", 
    "publish": "2012-03-20T23:06:10Z", 
    "summary": "Motivated by applications in energy-efficient scheduling in data centers,\nKhuller, Li, and Saha introduced the {\\em machine activation} problem as a\ngeneralization of the classical optimization problems of set cover and load\nbalancing on unrelated machines. In this problem, a set of $n$ jobs have to be\ndistributed among a set of $m$ (unrelated) machines, given the processing time\nof each job on each machine, where each machine has a startup cost. The goal is\nto produce a schedule of minimum total startup cost subject to a constraint\n$\\bf L$ on its makespan. While Khuller {\\em et al} considered the offline\nversion of this problem, a typical scenario in scheduling is one where jobs\narrive online and have to be assigned to a machine immediately on arrival. We\ngive an $(O(\\log (mn)\\log m), O(\\log m))$-competitive randomized online\nalgorithm for this problem, i.e. the schedule produced by our algorithm has a\nmakespan of $O({\\bf L} \\log m)$ with high probability, and a total expected\nstartup cost of $O(\\log (mn)\\log m)$ times that of an optimal offline schedule\nwith makespan $\\bf L$. The competitive ratios of our algorithm are (almost)\noptimal.\n  Our algorithms use the online primal dual framework introduced by Alon {\\em\net al} for the online set cover problem, and subsequently developed further by\nBuchbinder, Naor, and co-authors. To the best of our knowledge, all previous\napplications of this framework have been to linear programs (LPs) with either\npacking or covering constraints. One novelty of our application is that we use\nthis framework for a mixed LP that has both covering and packing constraints.\nWe hope that the algorithmic techniques developed in this paper to\nsimultaneously handle packing and covering constraints will be useful for\nsolving other online optimization problems as well."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4836v1", 
    "other_authors": "Anatolijs Gorbunovs", 
    "title": "On a New Method of Storing a Variable Size Array", 
    "arxiv-id": "1203.4836v1", 
    "author": "Anatolijs Gorbunovs", 
    "publish": "2012-03-21T21:15:44Z", 
    "summary": "This paper introduces a new data structure, log_vector, with the following\nproperties: constant time random access to individual elements; constant time\nelement addition to the end; constant time element removal from the end;\nconstant time empty data structure creation; amortized constant space per\nindividual elements; constant additional space used."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4900v1", 
    "other_authors": "Ashish Goel, Michael Kapralov, Ian Post", 
    "title": "Single pass sparsification in the streaming model with edge deletions", 
    "arxiv-id": "1203.4900v1", 
    "author": "Ian Post", 
    "publish": "2012-03-22T07:45:13Z", 
    "summary": "In this paper we give a construction of cut sparsifiers of Benczur and Karger\nin the {\\em dynamic} streaming setting in a single pass over the data stream.\nPrevious constructions either required multiple passes or were unable to handle\nedge deletions. We use $\\tilde{O}(1/\\e^2)$ time for each stream update and\n$\\tilde{O}(n/\\e^2)$ time to construct a sparsifier. Our $\\e$-sparsifiers have\n$O(n\\log^3 n/\\e^2)$ edges. The main tools behind our result are an application\nof sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivity\ntogether with a novel application of sampling with limited independence and\nsparse recovery to produce the edges of the sparsifier."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.4920v1", 
    "other_authors": "Livio Colussi", 
    "title": "Work function algorithm can forget history without losing   competitiveness", 
    "arxiv-id": "1203.4920v1", 
    "author": "Livio Colussi", 
    "publish": "2012-03-22T08:56:12Z", 
    "summary": "The Work Function Algorithm is the most effective deterministic on-line\nalgorithm for the k-server problem. Koutsoupias and Papadimitriou proved WFA is\n(2k-1) competitive. However the best known implementation of WFA requires time\nO(i^2) to process request r_i and this makes WFA impractical for long sequences\nof requests. The O(i^2) time is spent to compute the work function on the whole\nhistory of past requests. In order to make constant the time to process a\nrequest, Rudec and Menger proposed to restrict the history to a moving window\nof fixed size. However WFA restricted to a moving window loses its\ncompetitiveness. Here we give a condition that allows WFA to forget the whole\nprevious history and restart from scratch without losing competitiveness.\nMoreover for most of the metric spaces of practical interest (finite or bounded\nspaces) there is a constant bound on the length of the history before the\ncondition is verified and this makes O(1) the time to process each request."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.5235v1", 
    "other_authors": "Bang Ye Wu, Jun-Lin Guo, Yue-Li Wang", 
    "title": "A linear time algorithm for the next-to-shortest path problem on   undirected graphs with nonnegative edge lengths", 
    "arxiv-id": "1203.5235v1", 
    "author": "Yue-Li Wang", 
    "publish": "2012-03-23T13:33:47Z", 
    "summary": "For two vertices $s$ and $t$ in a graph $G=(V,E)$, the next-to-shortest path\nis an $st$-path which length is minimum amongst all $st$-paths strictly longer\nthan the shortest path length. In this paper we show that, when the graph is\nundirected and all edge lengths are nonnegative, the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nThis result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to\nallowing zero-length edges."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.6274v1", 
    "other_authors": "Zeev Nutov", 
    "title": "Small $\\ell$-edge-covers in $k$-connected graphs", 
    "arxiv-id": "1203.6274v1", 
    "author": "Zeev Nutov", 
    "publish": "2012-03-28T14:09:49Z", 
    "summary": "Let $G=(V,E)$ be a $k$-edge-connected graph with edge costs $\\{c(e):e \\in\nE\\}$ and let $1 \\leq \\ell \\leq k-1$. We show by a simple and short proof, that\n$G$ contains an $\\ell$-edge cover $I$ such that: $c(I) \\leq \\frac{\\ell}{k}c(E)$\nif $G$ is bipartite, or if $\\ell |V|$ is even, or if $|E| \\geq \\frac{k|V|}{2}\n+\\frac{k}{2\\ell}$; otherwise, $c(I) \\leq (\\frac{\\ell}{k}+\\frac{1}{k|V|})c(E)$.\nThe particular case $\\ell=k-1$ and unit costs already includes a result of\nCheriyan and Thurimella, that $G$ contains a $(k-1)$-edge-cover of size\n$|E|-\\lfloor |V|/2 \\rfloor$. Using our result, we slightly improve the\napproximation ratios for the {\\sf $k$-Connected Subgraph} problem (the\nnode-connectivity version) with uniform and $\\beta$-metric costs. We then\nconsider the dual problem of finding a spanning subgraph of maximum\nconnectivity $k^*$ with a prescribed number of edges. We give an algorithm that\ncomputes a $(k^*-1)$-connected subgraph, which is tight, since the problem is\nNP-hard."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1203.6695v2", 
    "other_authors": "Umang Bhaskar, Lisa Fleischer", 
    "title": "Online Mixed Packing and Covering", 
    "arxiv-id": "1203.6695v2", 
    "author": "Lisa Fleischer", 
    "publish": "2012-03-30T01:37:11Z", 
    "summary": "In many problems, the inputs arrive over time, and must be dealt with\nirrevocably when they arrive. Such problems are online problems. A common\nmethod of solving online problems is to first solve the corresponding linear\nprogram, and then round the fractional solution online to obtain an integral\nsolution.\n  We give algorithms for solving linear programs with mixed packing and\ncovering constraints online. We first consider mixed packing and covering\nlinear programs, where packing constraints are given offline and covering\nconstraints are received online. The objective is to minimize the maximum\nmultiplicative factor by which any packing constraint is violated, while\nsatisfying the covering constraints. No prior sublinear competitive algorithms\nare known for this problem. We give the first such --- a\npolylogarithmic-competitive algorithm for solving mixed packing and covering\nlinear programs online. We also show a nearly tight lower bound.\n  Our techniques for the upper bound use an exponential penalty function in\nconjunction with multiplicative updates. While exponential penalty functions\nare used previously to solve linear programs offline approximately, offline\nalgorithms know the constraints beforehand and can optimize greedily. In\ncontrast, when constraints arrive online, updates need to be more complex.\n  We apply our techniques to solve two online fixed-charge problems with\ncongestion. These problems are motivated by applications in machine scheduling\nand facility location. The linear program for these problems is more\ncomplicated than mixed packing and covering, and presents unique challenges. We\nshow that our techniques combined with a randomized rounding procedure give\npolylogarithmic-competitive integral solutions. These problems generalize\nonline set-cover, for which there is a polylogarithmic lower bound. Hence, our\nresults are close to tight."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.0072v1", 
    "other_authors": "Shoshana Marcus, Dina Sokol", 
    "title": "Dynamic 2D Dictionary Matching in Small Space", 
    "arxiv-id": "1302.0072v1", 
    "author": "Dina Sokol", 
    "publish": "2013-02-01T04:14:08Z", 
    "summary": "The dictionary matching problem preprocesses a set of patterns and finds all\noccurrences of each of the patterns in a text when it is provided. We focus on\nthe dynamic setting, in which patterns can be inserted to and removed from the\ndictionary, without reprocessing the entire dictionary. This article presents\nthe first algorithm that performs \\emph{dynamic} dictionary matching on\ntwo-dimensional data within small space. The time complexity of our algorithm\nis almost linear. The only slowdown is incurred by querying the compressed\nself-index that replaces the dictionary. The dictionary is updated in time\nproportional to the size of the pattern that is being inserted to or removed\nfrom the dictionary. Our algorithm is suitable for rectangular patterns that\nare of uniform size in one dimension."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.0892v1", 
    "other_authors": "Mark Braverman, Gal Oshri", 
    "title": "Search using queries on indistinguishable items", 
    "arxiv-id": "1302.0892v1", 
    "author": "Gal Oshri", 
    "publish": "2013-02-04T22:34:59Z", 
    "summary": "We investigate the problem of determining a set S of k indistinguishable\nintegers in the range [1,n]. The algorithm is allowed to query an integer $q\\in\n[1,n]$, and receive a response comparing this integer to an integer randomly\nchosen from S. The algorithm has no control over which element of S the query q\nis compared to. We show tight bounds for this problem. In particular, we show\nthat in the natural regime where $k\\le n$, the optimal number of queries to\nattain $n^{-\\Omega(1)}$ error probability is $\\Theta(k^3 \\log n)$. In the\nregime where $k>n$, the optimal number of queries is $\\Theta(n^2 k \\log n)$.\n  Our main technical tools include the use of information theory to derive the\nlower bounds, and the application of noisy binary search in the spirit of\nFeige, Raghavan, Peleg, and Upfal (1994). In particular, our lower bound\ntechnique is likely to be applicable in other situations that involve search\nunder uncertainty."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.1064v2", 
    "other_authors": "Juha K\u00e4rkk\u00e4inen, Dominik Kempa, Simon J. Puglisi", 
    "title": "Lightweight Lempel-Ziv Parsing", 
    "arxiv-id": "1302.1064v2", 
    "author": "Simon J. Puglisi", 
    "publish": "2013-02-05T15:34:53Z", 
    "summary": "We introduce a new approach to LZ77 factorization that uses O(n/d) words of\nworking space and O(dn) time for any d >= 1 (for polylogarithmic alphabet\nsizes). We also describe carefully engineered implementations of alternative\napproaches to lightweight LZ77 factorization. Extensive experiments show that\nthe new algorithm is superior in most cases, particularly at the lowest memory\nlevels and for highly repetitive data. As a part of the algorithm, we describe\nnew methods for computing matching statistics which may be of independent\ninterest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.1948v1", 
    "other_authors": "Sanjoy Dasgupta, Kaushik Sinha", 
    "title": "Randomized partition trees for exact nearest neighbor search", 
    "arxiv-id": "1302.1948v1", 
    "author": "Kaushik Sinha", 
    "publish": "2013-02-08T05:40:38Z", 
    "summary": "The k-d tree was one of the first spatial data structures proposed for\nnearest neighbor search. Its efficacy is diminished in high-dimensional spaces,\nbut several variants, with randomization and overlapping cells, have proved to\nbe successful in practice. We analyze three such schemes. We show that the\nprobability that they fail to find the nearest neighbor, for any data set and\nany query point, is directly related to a simple potential function that\ncaptures the difficulty of the point configuration. We then bound this\npotential function in two situations of interest: the first, when data come\nfrom a doubling measure, and the second, when the data are documents from a\ntopic model."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.2127v3", 
    "other_authors": "Jochen K\u00f6nemann, Sina Sadeghian, Laura Sanit\u00e0", 
    "title": "An LMP O(log n)-Approximation Algorithm for Node Weighted Prize   Collecting Steiner Tree", 
    "arxiv-id": "1302.2127v3", 
    "author": "Laura Sanit\u00e0", 
    "publish": "2013-02-08T19:41:06Z", 
    "summary": "In the node-weighted prize-collecting Steiner tree problem (NW-PCST) we are\ngiven an undirected graph $G=(V,E)$, non-negative costs $c(v)$ and penalties\n$\\pi(v)$ for each $v \\in V$. The goal is to find a tree $T$ that minimizes the\ntotal cost of the vertices spanned by $T$ plus the total penalty of vertices\nnot in $T$. This problem is well-known to be set-cover hard to approximate.\nMoss and Rabani (STOC'01) presented a primal-dual\nLagrangean-multiplier-preserving $O(\\ln |V|)$-approximation algorithm for this\nproblem. We show a serious problem with the algorithm, and present a new,\nfundamentally different primal-dual method achieving the same performance\nguarantee. Our algorithm introduces several novel features to the primal-dual\nmethod that may be of independent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.2137v1", 
    "other_authors": "Edith Cohen, Graham Cormode, Nick Duffield, Carsten Lund", 
    "title": "On the Tradeoff between Stability and Fit", 
    "arxiv-id": "1302.2137v1", 
    "author": "Carsten Lund", 
    "publish": "2013-02-08T20:16:46Z", 
    "summary": "In computing, as in many aspects of life, changes incur cost. Many\noptimization problems are formulated as a one-time instance starting from\nscratch. However, a common case that arises is when we already have a set of\nprior assignments, and must decide how to respond to a new set of constraints,\ngiven that each change from the current assignment comes at a price. That is,\nwe would like to maximize the fitness or efficiency of our system, but we need\nto balance it with the changeout cost from the previous state.\n  We provide a precise formulation for this tradeoff and analyze the resulting\n{\\em stable extensions} of some fundamental problems in measurement and\nanalytics. Our main technical contribution is a stable extension of PPS\n(probability proportional to size) weighted random sampling, with applications\nto monitoring and anomaly detection problems. We also provide a general\nframework that applies to top-$k$, minimum spanning tree, and assignment. In\nboth cases, we are able to provide exact solutions, and discuss efficient\nincremental algorithms that can find new solutions as the input changes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.2184v2", 
    "other_authors": "Glencora Borradaile, Philip Klein", 
    "title": "The two-edge connectivity survivable-network design problem in planar   graphs", 
    "arxiv-id": "1302.2184v2", 
    "author": "Philip Klein", 
    "publish": "2013-02-09T00:09:51Z", 
    "summary": "Consider the following problem: given a graph with edge costs and a subset Q\nof vertices, find a minimum-cost subgraph in which there are two edge-disjoint\npaths connecting every pair of vertices in Q. The problem is a\nfailure-resilient analog of the Steiner tree problem arising, for example, in\ntelecommunications applications. We study a more general mixed-connectivity\nformulation, also employed in telecommunications optimization. Given a number\n(or requirement) r(v) in {0, 1, 2} for each vertex v in the graph, find a\nminimum-cost subgraph in which there are min{r(u), r(v)} edge-disjoint u-to-v\npaths for every pair u, v of vertices.\n  We address the problem in planar graphs, considering a popular relaxation in\nwhich the solution is allowed to use multiple copies of the input-graph edges\n(paying separately for each copy). The problem is max SNP-hard in general\ngraphs and strongly NP-hard in planar graphs. We give the first polynomial-time\napproximation scheme in planar graphs. The running time is O(n log n).\n  Under the additional restriction that the requirements are only non-zero for\nvertices on the boundary of a single face of a planar graph, we give a\npolynomial-time algorithm to find the optimal solution."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.2551v3", 
    "other_authors": "Marcin Mucha, Maxim Sviridenko", 
    "title": "No-Wait Flowshop Scheduling is as Hard as Asymmetric Traveling Salesman   Problem", 
    "arxiv-id": "1302.2551v3", 
    "author": "Maxim Sviridenko", 
    "publish": "2013-02-11T17:44:20Z", 
    "summary": "In this paper we study the classical no-wait flowshop scheduling problem with\nmakespan objective (F|no-wait|C_max in the standard three-field notation). This\nproblem is well-known to be a special case of the asymmetric traveling salesman\nproblem (ATSP) and as such has an approximation algorithm with logarithmic\nperformance guarantee. In this work we show a reverse connection, we show that\nany polynomial time \\alpha-approximation algorithm for the no-wait flowshop\nscheduling problem with makespan objective implies the existence of a\npolynomial-time \\alpha(1+\\epsilon)-approximation algorithm for the ATSP, for\nany \\epsilon>0. This in turn implies that all non-approximability results for\nthe ATSP (current or future) will carry over to its special case. In\nparticular, it follows that no-wait flowshop problem is APX-hard, which is the\nfirst non-approximability result for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.2805v1", 
    "other_authors": "Dennis Komm, Rastislav Kr\u00e1lovi\u010d, Richard Kr\u00e1lovi\u010d, Tobias M\u00f6mke", 
    "title": "Randomized online computation with high probability guarantees", 
    "arxiv-id": "1302.2805v1", 
    "author": "Tobias M\u00f6mke", 
    "publish": "2013-02-12T14:29:05Z", 
    "summary": "We study the relationship between the competitive ratio and the tail\ndistribution of randomized online minimization problems. To this end, we define\na broad class of online problems that includes some of the well-studied\nproblems like paging, k-server and metrical task systems on finite metrics, and\nshow that for these problems it is possible to obtain, given an algorithm with\nconstant expected competitive ratio, another algorithm that achieves the same\nsolution quality up to an arbitrarily small constant error a with high\nprobability; the \"high probability\" statement is in terms of the optimal cost.\nFurthermore, we show that our assumptions are tight in the sense that removing\nany of them allows for a counterexample to the theorem. In addition, there are\nexamples of other problems not covered by our definition, where similar high\nprobability results can be obtained."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3035v1", 
    "other_authors": "Bj\u00f6rn Hlava", 
    "title": "Yet another approach to the Maximum Flow", 
    "arxiv-id": "1302.3035v1", 
    "author": "Bj\u00f6rn Hlava", 
    "publish": "2013-02-13T10:06:17Z", 
    "summary": "I introduce a new approach to the maximum flow problem by a simple algorithm\nwith a slightly better runtime. This approach is based on sorting arcs insight\nof vertices on a residual graph. This new approach leads to an O(mn^0.5) time\nbound for a network with n vertices and m arcs.\n  Category: Algorithms, Graph Theory and maximum flows"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3145v2", 
    "other_authors": "Zachary Friggstad, Anupam Gupta, Mohit Singh", 
    "title": "An Improved Integrality Gap for Asymmetric TSP Paths", 
    "arxiv-id": "1302.3145v2", 
    "author": "Mohit Singh", 
    "publish": "2013-02-13T15:49:23Z", 
    "summary": "The Asymmetric Traveling Salesperson Path Problem (ATSPP) is one where, given\nan asymmetric metric space $(V,d)$ with specified vertices s and t, the goal is\nto find an s-t path of minimum length that passes through all the vertices in\nV.\n  This problem is closely related to the Asymmetric TSP (ATSP), which seeks to\nfind a tour (instead of an $s-t$ path) visiting all the nodes: for ATSP, a\n$\\rho$-approximation guarantee implies an $O(\\rho)$-approximation for ATSPP.\nHowever, no such connection is known for the integrality gaps of the linear\nprogramming relaxations for these problems: the current-best approximation\nalgorithm for ATSPP is $O(\\log n/\\log\\log n)$, whereas the best bound on the\nintegrality gap of the natural LP relaxation (the subtour elimination LP) for\nATSPP is $O(\\log n)$.\n  In this paper, we close this gap, and improve the current best bound on the\nintegrality gap from $O(\\log n)$ to $O(\\log n/\\log\\log n)$. The resulting\nalgorithm uses the structure of narrow $s$-$t$ cuts in the LP solution to\nconstruct a (random) tree spanning tree that can be cheaply augmented to\ncontain an Eulerian $s$-$t$ walk.\n  We also build on a result of Oveis Gharan and Saberi and show a strong form\nof Goddyn's conjecture about thin spanning trees implies the integrality gap of\nthe subtour elimination LP relaxation for ATSPP is bounded by a constant.\nFinally, we give a simpler family of instances showing the integrality gap of\nthis LP is at least 2."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3347v1", 
    "other_authors": "Johannes Fischer, Pawel Gawrychowski", 
    "title": "Alphabet-Dependent String Searching with Wexponential Search Trees", 
    "arxiv-id": "1302.3347v1", 
    "author": "Pawel Gawrychowski", 
    "publish": "2013-02-14T09:29:09Z", 
    "summary": "It is widely assumed that $O(m+\\lg \\sigma)$ is the best one can do for\nfinding a pattern of length $m$ in a compacted trie storing strings over an\nalphabet of size $\\sigma$, if one insists on linear-size data structures and\ndeterministic worst-case running times [Cole et al., ICALP'06]. In this\narticle, we first show that a rather straightforward combination of well-known\nideas yields $O(m+\\lg\\lg \\sigma)$ deterministic worst-case searching time for\nstatic tries.\n  Then we move on to dynamic tries, where we achieve a worst-case bound of\n$O(m+\\frac{\\lg^{2}\\lg\\sigma}{\\lg\\lg\\lg\\sigma})$ per query or update, which\nshould again be compared to the previously known $O(m+\\lg\\sigma)$ deterministic\nworst-case bounds [Cole et al., ICALP'06], and to the alphabet\n\\emph{in}dependent $O(m+\\sqrt{\\lg n/\\lg\\lg n})$ deterministic worst-case bounds\n[Andersson and Thorup, SODA'01], where $n$ is the number of nodes in the trie.\nThe basis of our update procedure is a weighted variant of exponential search\ntrees which, while simple, might be of independent interest.\n  As one particular application, the above bounds (static and dynamic) apply to\nsuffix trees. There, an update corresponds to pre- or appending a letter to the\ntext, and an additional goal is to do the updates quicker than rematching\nentire suffixes. We show how to do this in $O(\\lg\\lg n +\n\\frac{\\lg^{2}\\lg\\sigma}{\\lg\\lg\\lg\\sigma})$ time, which improves the previously\nknown $O(\\lg n)$ bound [Amir et al., SPIRE'05]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3404v1", 
    "other_authors": "Tommi Larjomaa, Alexandru Popa", 
    "title": "The min-max edge q-coloring problem", 
    "arxiv-id": "1302.3404v1", 
    "author": "Alexandru Popa", 
    "publish": "2013-02-14T14:07:55Z", 
    "summary": "In this paper we introduce and study a new problem named \\emph{min-max edge\n$q$-coloring} which is motivated by applications in wireless mesh networks. The\ninput of the problem consists of an undirected graph and an integer $q$. The\ngoal is to color the edges of the graph with as many colors as possible such\nthat: (a) any vertex is incident to at most $q$ different colors, and (b) the\nmaximum size of a color group (i.e. set of edges identically colored) is\nminimized. We show the following results: 1. Min-max edge $q$-coloring is\nNP-hard, for any $q \\ge 2$. 2. A polynomial time exact algorithm for min-max\nedge $q$-coloring on trees. 3. Exact formulas of the optimal solution for\ncliques and almost tight bounds for bicliques and hypergraphs. 4. A non-trivial\nlower bound of the optimal solution with respect to the average degree of the\ngraph. 5. An approximation algorithm for planar graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3417v1", 
    "other_authors": "Reut Levi, Dana Ron", 
    "title": "A Quasi-Polynomial Time Partition Oracle for Graphs with an Excluded   Minor", 
    "arxiv-id": "1302.3417v1", 
    "author": "Dana Ron", 
    "publish": "2013-02-14T14:51:29Z", 
    "summary": "Motivated by the problem of testing planarity and related properties, we\nstudy the problem of designing efficient {\\em partition oracles}. A {\\em\npartition oracle} is a procedure that, given access to the incidence lists\nrepresentation of a bounded-degree graph $G= (V,E)$ and a parameter $\\eps$,\nwhen queried on a vertex $v\\in V$, returns the part (subset of vertices) which\n$v$ belongs to in a partition of all graph vertices. The partition should be\nsuch that all parts are small, each part is connected, and if the graph has\ncertain properties, the total number of edges between parts is at most $\\eps\n|V|$. In this work we give a partition oracle for graphs with excluded minors\nwhose query complexity is quasi-polynomial in $1/\\eps$, thus improving on the\nresult of Hassidim et al. ({\\em Proceedings of FOCS 2009}) who gave a partition\noracle with query complexity exponential in $1/\\eps$. This improvement implies\ncorresponding improvements in the complexity of testing planarity and other\nproperties that are characterized by excluded minors as well as sublinear-time\napproximation algorithms that work under the promise that the graph has an\nexcluded minor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3720v1", 
    "other_authors": "Guillaume Aupy, Anne Benoit, Rami Melhem, Paul Renaud-Goud, Yves Robert", 
    "title": "Energy-aware checkpointing of divisible tasks with soft or hard   deadlines", 
    "arxiv-id": "1302.3720v1", 
    "author": "Yves Robert", 
    "publish": "2013-02-15T10:42:53Z", 
    "summary": "In this paper, we aim at minimizing the energy consumption when executing a\ndivisible workload under a bound on the total execution time, while resilience\nis provided through checkpointing. We discuss several variants of this\nmulti-criteria problem. Given the workload, we need to decide how many chunks\nto use, what are the sizes of these chunks, and at which speed each chunk is\nexecuted. Furthermore, since a failure may occur during the execution of a\nchunk, we also need to decide at which speed a chunk should be re-executed in\nthe event of a failure. The goal is to minimize the expectation of the total\nenergy consumption, while enforcing a deadline on the execution time, that\nshould be met either in expectation (soft deadline), or in the worst case (hard\ndeadline). For each problem instance, we propose either an exact solution, or a\nfunction that can be optimized numerically. The different models are then\ncompared through an extensive set of experiments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3763v1", 
    "other_authors": "Marek Cygan, Marcin Pilipczuk", 
    "title": "Faster exponential-time algorithms in graphs of bounded average degree", 
    "arxiv-id": "1302.3763v1", 
    "author": "Marcin Pilipczuk", 
    "publish": "2013-02-15T14:41:00Z", 
    "summary": "We first show that the Traveling Salesman Problem in an n-vertex graph with\naverage degree bounded by d can be solved in O*(2^{(1-\\eps_d)n}) time and\nexponential space for a constant \\eps_d depending only on d, where the\nO*-notation suppresses factors polynomial in the input size. Thus, we\ngeneralize the recent results of Bjorklund et al. [TALG 2012] on graphs of\nbounded degree.\n  Then, we move to the problem of counting perfect matchings in a graph. We\nfirst present a simple algorithm for counting perfect matchings in an n-vertex\ngraph in O*(2^{n/2}) time and polynomial space; our algorithm matches the\ncomplexity bounds of the algorithm of Bjorklund [SODA 2012], but relies on\ninclusion-exclusion principle instead of algebraic transformations. Building\nupon this result, we show that the number of perfect matchings in an n-vertex\ngraph with average degree bounded by d can be computed in\nO*(2^{(1-\\eps_{2d})n/2}) time and exponential space, where \\eps_{2d} is the\nconstant obtained by us for the Traveling Salesman Problem in graphs of average\ndegree at most 2d.\n  Moreover we obtain a simple algorithm that counts the number of perfect\nmatchings in an n-vertex bipartite graph of average degree at most d in\nO*(2^{(1-1/(3.55d))n/2}) time, improving and simplifying the recent result of\nIzumi and Wadayama [FOCS 2012]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3889v1", 
    "other_authors": "Mohammad M. Karbasioun, Gennady Shaikhet, Evangelos Kranakis, Ioannis Lambadaris", 
    "title": "Power Strip Packing of Malleable Demands in Smart Grid", 
    "arxiv-id": "1302.3889v1", 
    "author": "Ioannis Lambadaris", 
    "publish": "2013-02-15T21:12:30Z", 
    "summary": "We consider a problem of supplying electricity to a set of $\\mathcal{N}$\ncustomers in a smart-grid framework. Each customer requires a certain amount of\nelectrical energy which has to be supplied during the time interval $[0,1]$. We\nassume that each demand has to be supplied without interruption, with possible\nduration between $\\ell$ and $r$, which are given system parameters ($\\ell\\le\nr$). At each moment of time, the power of the grid is the sum of all the\nconsumption rates for the demands being supplied at that moment. Our goal is to\nfind an assignment that minimizes the {\\it power peak} - maximal power over\n$[0,1]$ - while satisfying all the demands. To do this first we find the lower\nbound of optimal power peak. We show that the problem depends on whether or not\nthe pair $\\ell, r$ belongs to a \"good\" region $\\mathcal{G}$. If it does - then\nan optimal assignment almost perfectly \"fills\" the rectangle $time \\times power\n= [0,1] \\times [0, A]$ with $A$ being the sum of all the energy demands - thus\nachieving an optimal power peak $A$. Conversely, if $\\ell, r$ do not belong to\n$\\mathcal{G}$, we identify the lower bound $\\bar{A} >A$ on the optimal value of\npower peak and introduce a simple linear time algorithm that almost perfectly\narranges all the demands in a rectangle $[0, A /\\bar{A}] \\times [0, \\bar{A}]$\nand show that it is asymptotically optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.3946v1", 
    "other_authors": "Lin Chen, Deshi Ye, Guochuan Zhang", 
    "title": "Approximating the optimal competitive ratio for an ancient online   scheduling problem", 
    "arxiv-id": "1302.3946v1", 
    "author": "Guochuan Zhang", 
    "publish": "2013-02-16T08:32:11Z", 
    "summary": "We consider the classical online scheduling problem P||C_{max} in which jobs\nare released over list and provide a nearly optimal online algorithm. More\nprecisely, an online algorithm whose competitive ratio is at most (1+\\epsilon)\ntimes that of an optimal online algorithm could be achieved in polynomial time,\nwhere m, the number of machines, is a part of the input. It substantially\nimproves upon the previous results by almost closing the gap between the\ncurrently best known lower bound of 1.88 (Rudin, Ph.D thesis, 2001) and the\nbest known upper bound of 1.92 (Fleischer, Wahl, Journal of Scheduling, 2000).\nIt has been known by folklore that an online problem could be viewed as a game\nbetween an adversary and the online player. Our approach extensively explores\nsuch a structure and builds up a completely new framework to show that, for the\nonline over list scheduling problem, given any \\epsilon>0, there exists a\nuniform threshold K which is polynomial in m such that if the competitive ratio\nof an online algorithm is \\rho<=2, then there exists a list of at most K jobs\nto enforce the online algorithm to achieve a competitive ratio of at least\n\\rho-O(\\epsilon). Our approach is substantially different from that of Gunther\net al. (Gunther et al., SODA 2013), in which an approximation scheme for online\nover time scheduling problems is given, where the number of machines is fixed.\nOur method could also be extended to several related online over list\nscheduling models."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4016v3", 
    "other_authors": "Gregory Kucherov, Yakov Nekrich", 
    "title": "Full-fledged Real-Time Indexing for Constant Size Alphabets", 
    "arxiv-id": "1302.4016v3", 
    "author": "Yakov Nekrich", 
    "publish": "2013-02-17T00:01:20Z", 
    "summary": "In this paper we describe a data structure that supports pattern matching\nqueries on a dynamically arriving text over an alphabet ofconstant size. Each\nnew symbol can be prepended to $T$ in O(1) worst-case time. At any moment, we\ncan report all occurrences of a pattern $P$ in the current text in $O(|P|+k)$\ntime, where $|P|$ is the length of $P$ and $k$ is the number of occurrences.\nThis resolves, under assumption of constant-size alphabet, a long-standing open\nproblem of existence of a real-time indexing method for string matching (see\n\\cite{AmirN08})."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4064v1", 
    "other_authors": "Jinil Kim, Peter Eades, Rudolf Fleischer, Seok-Hee Hong, Costas S. Iliopoulos, Kunsoo Park, Simon J. Puglisi, Takeshi Tokuyama", 
    "title": "Order Preserving Matching", 
    "arxiv-id": "1302.4064v1", 
    "author": "Takeshi Tokuyama", 
    "publish": "2013-02-17T13:06:42Z", 
    "summary": "We introduce a new string matching problem called order-preserving matching\non numeric strings where a pattern matches a text if the text contains a\nsubstring whose relative orders coincide with those of the pattern.\nOrder-preserving matching is applicable to many scenarios such as stock price\nanalysis and musical melody matching in which the order relations should be\nmatched instead of the strings themselves. Solving order-preserving matching\nhas to do with representations of order relations of a numeric string. We\ndefine prefix representation and nearest neighbor representation, which lead to\nefficient algorithms for order-preserving matching. We present efficient\nalgorithms for single and multiple pattern cases. For the single pattern case,\nwe give an O(n log m) time algorithm and optimize it further to obtain O(n + m\nlog m) time. For the multiple pattern case, we give an O(n log m) time\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4213v1", 
    "other_authors": "Klaus Jansen, Kim-Manuel Klein", 
    "title": "A Robust AFPTAS for Online Bin Packing with Polynomial Migration", 
    "arxiv-id": "1302.4213v1", 
    "author": "Kim-Manuel Klein", 
    "publish": "2013-02-18T10:23:00Z", 
    "summary": "In this paper we develop general LP and ILP techniques to find an approximate\nsolution with improved objective value close to an existing solution. The task\nof improving an approximate solution is closely related to a classical theorem\nof Cook et al. in the sensitivity analysis for LPs and ILPs. This result is\noften applied in designing robust algorithms for online problems. We apply our\nnew techniques to the online bin packing problem, where it is allowed to\nreassign a certain number of items, measured by the migration factor. The\nmigration factor is defined by the total size of reassigned items divided by\nthe size of the arriving item. We obtain a robust asymptotic fully polynomial\ntime approximation scheme (AFPTAS) for the online bin packing problem with\nmigration factor bounded by a polynomial in $\\frac{1}{\\epsilon}$. This answers\nan open question stated by Epstein and Levin in the affirmative. As a byproduct\nwe prove an approximate variant of the sensitivity theorem by Cook at el. for\nlinear programs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4216v2", 
    "other_authors": "Karl Bringmann, Benjamin Doerr, Adrian Neumann, Jakub Sliacan", 
    "title": "Online Checkpointing with Improved Worst-Case Guarantees", 
    "arxiv-id": "1302.4216v2", 
    "author": "Jakub Sliacan", 
    "publish": "2013-02-18T10:36:08Z", 
    "summary": "In the online checkpointing problem, the task is to continuously maintain a\nset of k checkpoints that allow to rewind an ongoing computation faster than by\na full restart. The only operation allowed is to replace an old checkpoint by\nthe current state. Our aim are checkpoint placement strategies that minimize\nrewinding cost, i.e., such that at all times T when requested to rewind to some\ntime t <= T the number of computation steps that need to be redone to get to t\nfrom a checkpoint before t is as small as possible. In particular, we want that\nthe closest checkpoint earlier than t is not further away from t than q_k times\nthe ideal distance T / (k+1), where q_k is a small constant.\n  Improving over earlier work showing 1 + 1/k <= q_k <= 2, we show that q_k can\nbe chosen asymptotically less than 2. We present algorithms with asymptotic\ndiscrepancy q_k <= 1.59 + o(1) valid for all k and q_k <= ln(4) + o(1) <= 1.39\n+ o(1) valid for k being a power of two. Experiments indicate the uniform bound\np_k <= 1.7 for all k. For small k, we show how to use a linear programming\napproach to compute good checkpointing algorithms. This gives discrepancies of\nless than 1.55 for all k < 60.\n  We prove the first lower bound that is asymptotically more than one, namely\nq_k >= 1.30 - o(1). We also show that optimal algorithms (yielding the infimum\ndiscrepancy) exist for all k."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4347v1", 
    "other_authors": "Maxim Sviridenko, Justin Ward", 
    "title": "Large Neighborhood Local Search for the Maximum Set Packing Problem", 
    "arxiv-id": "1302.4347v1", 
    "author": "Justin Ward", 
    "publish": "2013-02-18T16:48:00Z", 
    "summary": "In this paper we consider the classical maximum set packing problem where set\ncardinality is upper bounded by $k$. We show how to design a variant of a\npolynomial-time local search algorithm with performance guarantee $(k+2)/3$.\nThis local search algorithm is a special case of a more general procedure that\nallows to swap up to $\\Theta(\\log n)$ elements per iteration. We also design\nproblem instances with locality gap $k/3$ even for a wide class of exponential\ntime local search procedures, which can swap up to $cn$ elements for a constant\n$c$. This shows that our analysis of this class of algorithms is almost tight."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.4426v1", 
    "other_authors": "Fatemeh Rajabi-Alni", 
    "title": "A new algorithm for Many to Many Matching with Demands and Capacities", 
    "arxiv-id": "1302.4426v1", 
    "author": "Fatemeh Rajabi-Alni", 
    "publish": "2013-02-18T20:54:05Z", 
    "summary": "Let A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t} with s+r=n, the many to many\npoint matching with demands and capacities matches each point a_i in A to at\nleast alpha_i and at most alpha_i points in B, and each point b_j in B to at\nleast beta_j and at most beta_j points in A for all 1 <= i <= s and 1 <= j <=\nt. In this paper, we present an O(n^4) time and O(n) space algorithm for this\nproblem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5127v3", 
    "other_authors": "Mikkel Thorup", 
    "title": "On the k-Independence Required by Linear Probing and Minwise   Independence", 
    "arxiv-id": "1302.5127v3", 
    "author": "Mikkel Thorup", 
    "publish": "2013-02-20T21:25:32Z", 
    "summary": "We show that linear probing requires 5-independent hash functions for\nexpected constant-time performance, matching an upper bound of [Pagh et al.\nSTOC'07]. More precisely, we construct a 4-independent hash functions yielding\nexpected logarithmic search time.\n  For (1+{\\epsilon})-approximate minwise independence, we show that \\Omega(log\n1/{\\epsilon})-independent hash functions are required, matching an upper bound\nof [Indyk, SODA'99].\n  We also show that the very fast 2-independent multiply-shift scheme of\nDietzfelbinger [STACS'96] fails badly in both applications."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5401v1", 
    "other_authors": "Merav Parter, David Peleg", 
    "title": "Sparse Fault-Tolerant BFS Trees", 
    "arxiv-id": "1302.5401v1", 
    "author": "David Peleg", 
    "publish": "2013-02-21T20:19:31Z", 
    "summary": "This paper addresses the problem of designing a sparse {\\em fault-tolerant}\nBFS tree, or {\\em FT-BFS tree} for short, namely, a sparse subgraph $T$ of the\ngiven network $G$ such that subsequent to the failure of a single edge or\nvertex, the surviving part $T'$ of $T$ still contains a BFS spanning tree for\n(the surviving part of) $G$. Our main results are as follows. We present an\nalgorithm that for every $n$-vertex graph $G$ and source node $s$ constructs a\n(single edge failure) FT-BFS tree rooted at $s$ with $O(n \\cdot\n\\min\\{\\Depth(s), \\sqrt{n}\\})$ edges, where $\\Depth(s)$ is the depth of the BFS\ntree rooted at $s$. This result is complemented by a matching lower bound,\nshowing that there exist $n$-vertex graphs with a source node $s$ for which any\nedge (or vertex) FT-BFS tree rooted at $s$ has $\\Omega(n^{3/2})$ edges. We then\nconsider {\\em fault-tolerant multi-source BFS trees}, or {\\em FT-MBFS trees}\nfor short, aiming to provide (following a failure) a BFS tree rooted at each\nsource $s\\in S$ for some subset of sources $S\\subseteq V$. Again, tight bounds\nare provided, showing that there exists a poly-time algorithm that for every\n$n$-vertex graph and source set $S \\subseteq V$ of size $\\sigma$ constructs a\n(single failure) FT-MBFS tree $T^*(S)$ from each source $s_i \\in S$, with\n$O(\\sqrt{\\sigma} \\cdot n^{3/2})$ edges, and on the other hand there exist\n$n$-vertex graphs with source sets $S \\subseteq V$ of cardinality $\\sigma$, on\nwhich any FT-MBFS tree from $S$ has $\\Omega(\\sqrt{\\sigma}\\cdot n^{3/2})$ edges.\nFinally, we propose an $O(\\log n)$ approximation algorithm for constructing\nFT-BFS and FT-MBFS structures. The latter is complemented by a hardness result\nstating that there exists no $\\Omega(\\log n)$ approximation algorithm for these\nproblems under standard complexity assumptions."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5445v1", 
    "other_authors": "Anupam Gupta, Viswanath Nagarajan, Vijay V. Vazirani", 
    "title": "Thrifty Algorithms for Multistage Robust Optimization", 
    "arxiv-id": "1302.5445v1", 
    "author": "Vijay V. Vazirani", 
    "publish": "2013-02-21T22:34:33Z", 
    "summary": "We consider a class of multi-stage robust covering problems, where additional\ninformation is revealed about the problem instance in each stage, but the cost\nof taking actions increases. The dilemma for the decision-maker is whether to\nwait for additional information and risk the inflation, or to take early\nactions to hedge against rising costs. We study the \"k-robust\" uncertainty\nmodel: in each stage i = 0, 1,...,T, the algorithm is shown some subset of size\nk_i that completely contains the eventual demands to be covered; here k_1 > k_2\n>...> k_T which ensures increasing information over time. The goal is to\nminimize the cost incurred in the worst-case possible sequence of revelations.\n  For the multistage k-robust set cover problem, we give an O(log m + log\nn)-approximation algorithm, nearly matching the \\Omega(log n + log m/loglog m)\nhardness of approximation even for T=2 stages. Moreover, our algorithm has a\nuseful \"thrifty\" property: it takes actions on just two stages. We show similar\nthrifty algorithms for multi-stage k-robust Steiner tree, Steiner forest, and\nminimum-cut. For these problems our approximation guarantees are O(min{T, log\nn, log L_{max}), where L_{max} is the maximum inflation over all the stages. We\nconjecture that these problems also admit O(1)-approximate thrifty algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5611v1", 
    "other_authors": "Julian Arz, Dennis Luxen, Peter Sanders", 
    "title": "Transit Node Routing Reconsidered", 
    "arxiv-id": "1302.5611v1", 
    "author": "Peter Sanders", 
    "publish": "2013-02-22T14:43:06Z", 
    "summary": "Transit Node Routing (TNR) is a fast and exact distance oracle for road\nnetworks. We show several new results for TNR. First, we give a surprisingly\nsimple implementation fully based on Contraction Hierarchies that speeds up\npreprocessing by an order of magnitude approaching the time for just finding a\nCH (which alone has two orders of magnitude larger query time). We also develop\na very effective purely graph theoretical locality filter without any\ncompromise in query times. Finally, we show that a specialization to the online\nmany-to-one (or one-to-many) shortest path further speeds up query time by an\norder of magnitude. This variant even has better query time than the fastest\nknown previous methods which need much more space."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5820v1", 
    "other_authors": "Songjian Lu, Xinghua Lu", 
    "title": "An exact algorithm with the time complexity of $O^*(1.299^m)$ for the   weighed mutually exclusive set cover problem", 
    "arxiv-id": "1302.5820v1", 
    "author": "Xinghua Lu", 
    "publish": "2013-02-23T15:55:48Z", 
    "summary": "In this paper, we will introduce an exact algorithm with a time complexity of\n$O^*(1.299^m)$ for the {\\sc weighted mutually exclusive set cover} problem,\nwhere $m$ is the number of subsets in the problem. This problem has important\napplications in recognizing mutation genes that cause different cancer\ndiseases."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5871v1", 
    "other_authors": "S. Kapoor, M. Sarwat", 
    "title": "The Budgeted Transportation Problem", 
    "arxiv-id": "1302.5871v1", 
    "author": "M. Sarwat", 
    "publish": "2013-02-24T05:08:27Z", 
    "summary": "Consider a transportation problem with sets of sources and sinks. There are\nprofits and prices on the edges. The goal is to maximize the profit while\nmeeting the following constraints; the total flow going out of a source must\nnot exceed its capacity and the total price of the incoming flow on a sink must\nnot exceed its budget. This problem is closely related to the generalized flow\nproblem.\n  We propose an auction based primal dual approximation algorithm to solve the\nproblem. The complexity is $O(\\epsilon^{-1}(n^2+ n\\log{m})m\\log U)$ where $n$\nis the number of sources, $m$ is the number of sinks, $U$ is the ratio of the\nmaximum profit/price to the minimum profit/price.\n  We also show how to generalize the scheme to solve a more general version of\nthe problem, where there are edge capacities and/or the profit function is\nconcave and piece-wise linear. The complexity of the algorithm depends on the\nnumber of linear segments, termed ${\\cal L}$, of the profit function."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.5913v1", 
    "other_authors": "Anupam Gupta, Viswanath Nagarajan", 
    "title": "A Stochastic Probing Problem with Applications", 
    "arxiv-id": "1302.5913v1", 
    "author": "Viswanath Nagarajan", 
    "publish": "2013-02-24T14:48:37Z", 
    "summary": "We study a general stochastic probing problem defined on a universe V, where\neach element e in V is \"active\" independently with probability p_e. Elements\nhave weights {w_e} and the goal is to maximize the weight of a chosen subset S\nof active elements. However, we are given only the p_e values-- to determine\nwhether or not an element e is active, our algorithm must probe e. If element e\nis probed and happens to be active, then e must irrevocably be added to the\nchosen set S; if e is not active then it is not included in S. Moreover, the\nfollowing conditions must hold in every random instantiation: (1) the set Q of\nprobed elements satisfy an \"outer\" packing constraint, and (2) the set S of\nchosen elements satisfy an \"inner\" packing constraint.\n  The kinds of packing constraints we consider are intersections of matroids\nand knapsacks. Our results provide a simple and unified view of results in\nstochastic matching and Bayesian mechanism design, and can also handle more\ngeneral constraints. As an application, we obtain the first polynomial-time\n$\\Omega(1/k)$-approximate \"Sequential Posted Price Mechanism\" under k-matroid\nintersection feasibility constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.6641v3", 
    "other_authors": "John Iacono", 
    "title": "Why some heaps support constant-amortized-time decrease-key operations,   and others do not", 
    "arxiv-id": "1302.6641v3", 
    "author": "John Iacono", 
    "publish": "2013-02-27T01:52:21Z", 
    "summary": "A lower bound is presented which shows that a class of heap algorithms in the\npointer model with only heap pointers must spend Omega(log log n / log log log\nn) amortized time on the decrease-key operation (given O(log n) amortized-time\nextract-min). Intuitively, this bound shows the key to having O(1)-time\ndecrease-key is the ability to sort O(log n) items in O(log n) time; Fibonacci\nheaps [M.L. Fredman and R. E. Tarjan. J. ACM 34(3):596-615 (1987)] do this\nthrough the use of bucket sort. Our lower bound also holds no matter how much\ndata is augmented; this is in contrast to the lower bound of Fredman [J. ACM\n46(4):473-501 (1999)] who showed a tradeoff between the number of augmented\nbits and the amortized cost of decrease-key. A new heap data structure, the\nsort heap, is presented. This heap is a simplification of the heap of Elmasry\n[SODA 2009: 471-476] and shares with it a O(log log n) amortized-time\ndecrease-key, but with a straightforward implementation such that our lower\nbound holds. Thus a natural model is presented for a pointer-based heap such\nthat the amortized runtime of a self-adjusting structure and amortized lower\nasymptotic bounds for decrease-key differ by but a O(log log log n) factor."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.6666v1", 
    "other_authors": "Yan Huang, Ruoming Jin, Favyen Bastani, Xiaoyang Sean Wang", 
    "title": "Large Scale Real-time Ridesharing with Service Guarantee on Road   Networks", 
    "arxiv-id": "1302.6666v1", 
    "author": "Xiaoyang Sean Wang", 
    "publish": "2013-02-27T05:41:49Z", 
    "summary": "The mean occupancy rates of personal vehicle trips in the United States is\nonly 1.6 persons per vehicle mile. Urban traffic gridlock is a familiar scene.\nRidesharing has the potential to solve many environmental, congestion, and\nenergy problems. In this paper, we introduce the problem of large scale\nreal-time ridesharing with service guarantee on road networks. Servers and trip\nrequests are dynamically matched while waiting time and service time\nconstraints of trips are satisfied. We first propose two basic algorithms: a\nbranch-and-bound algorithm and an integer programing algorithm. However, these\nalgorithm structures do not adapt well to the dynamic nature of the ridesharing\nproblem. Thus, we then propose a kinetic tree algorithm capable of better\nscheduling dynamic requests and adjusting routes on-the-fly. We perform\nexperiments on a large real taxi dataset from Shanghai. The results show that\nthe kinetic tree algorithm is faster than other algorithms in response time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00224-013-9482-z", 
    "link": "http://arxiv.org/pdf/1302.6863v3", 
    "other_authors": "Jakub Gajarsk\u00fd, Petr Hlin\u011bn\u00fd, Jan Obdr\u017e\u00e1lek, Sebastian Ordyniak, Felix Reidl, Peter Rossmanith, Fernando S\u00e1nchez Villaamil, Somnath Sikdar", 
    "title": "Kernelization Using Structural Parameters on Sparse Graph Classes", 
    "arxiv-id": "1302.6863v3", 
    "author": "Somnath Sikdar", 
    "publish": "2013-02-27T14:49:43Z", 
    "summary": "Meta-theorems for polynomial (linear) kernels have been the subject of\nintensive research in parameterized complexity. Heretofore, meta-theorems for\nlinear kernels exist on graphs of bounded genus, $H$-minor-free graphs, and\n$H$-topological-minor-free graphs. To the best of our knowledge, no\nmeta-theorems for polynomial kernels are known for any larger sparse graph\nclasses; e.g., for classes of bounded expansion or for nowhere dense ones. In\nthis paper we prove such meta-theorems for the two latter cases. More\nspecifically, we show that graph problems that have finite integer index (FII)\nhave linear kernels on graphs of bounded expansion when parameterized by the\nsize of a modulator to constant-treedepth graphs. For nowhere dense graph\nclasses, our result yields almost-linear kernels. While our parameter may seem\nrather strong, we argue that a linear kernelization result on graphs of bounded\nexpansion with a weaker parameter (than treedepth modulator) would fail to\ninclude some of the problems covered by our framework. Moreover, we only\nrequire the problems to have FII on graphs of constant treedepth. This allows\nus to prove linear kernels for problems such as Longest Path/Cycle, Exact\n$s,t$-Path, Treewidth, and Pathwidth, which do not have FII on general graphs\n(and the first two not even on bounded treewidth graphs)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1302.6900v2", 
    "other_authors": "Manuel Schmitt, Rolf Wanka", 
    "title": "Exploiting Independent Subformulas: A Faster Approximation Scheme for   #k-SAT", 
    "arxiv-id": "1302.6900v2", 
    "author": "Rolf Wanka", 
    "publish": "2013-02-27T16:19:18Z", 
    "summary": "We present an improvement on Thurley's recent randomized approximation scheme\nfor #k-SAT where the task is to count the number of satisfying truth\nassignments of a Boolean function {\\Phi} given as an n-variable k-CNF. We\nintroduce a novel way to identify independent substructures of {\\Phi} and can\ntherefore reduce the size of the search space considerably. Our randomized\nalgorithm works for any k. For #3-SAT, it runs in time\nO(\\epsilon^{-2}*1.51426^n), for #4-SAT, it runs in time\nO(\\epsilon^{-2}*1.60816^n), with error bound \\epsilon."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1302.6914v1", 
    "other_authors": "John Howat, John Iacono, Pat Morin", 
    "title": "The Fresh-Finger Property", 
    "arxiv-id": "1302.6914v1", 
    "author": "Pat Morin", 
    "publish": "2013-02-27T16:40:11Z", 
    "summary": "The unified property roughly states that searching for an element is fast\nwhen the current access is close to a recent access. Here, \"close\" refers to\nrank distance measured among all elements stored by the dictionary. We show\nthat distance need not be measured this way: in fact, it is only necessary to\nconsider a small working-set of elements to measure this rank distance. This\nresults in a data structure with access time that is an improvement upon those\noffered by the unified property for many query sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1302.7014v5", 
    "other_authors": "Jiayang Jiang, Michael Mitzenmacher, Justin Thaler", 
    "title": "Parallel Peeling Algorithms", 
    "arxiv-id": "1302.7014v5", 
    "author": "Justin Thaler", 
    "publish": "2013-02-27T22:03:08Z", 
    "summary": "The analysis of several algorithms and data structures can be framed as a\npeeling process on a random hypergraph: vertices with degree less than k are\nremoved until there are no vertices of degree less than k left. The remaining\nhypergraph is known as the k-core. In this paper, we analyze parallel peeling\nprocesses, where in each round, all vertices of degree less than k are removed.\nIt is known that, below a specific edge density threshold, the k-core is empty\nwith high probability. We show that, with high probability, below this\nthreshold, only (log log n)/log(k-1)(r-1) + O(1) rounds of peeling are needed\nto obtain the empty k-core for r-uniform hypergraphs. Interestingly, we show\nthat above this threshold, Omega(log n) rounds of peeling are required to find\nthe non-empty k-core. Since most algorithms and data structures aim to peel to\nan empty k-core, this asymmetry appears fortunate. We verify the theoretical\nresults both with simulation and with a parallel implementation using graphics\nprocessing units (GPUs). Our implementation provides insights into how to\nstructure parallel peeling algorithms for efficiency in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1302.7278v2", 
    "other_authors": "Kamil Salikhov, Gustavo Sacomoto, Gregory Kucherov", 
    "title": "Using cascading Bloom filters to improve the memory usage for de Brujin   graphs", 
    "arxiv-id": "1302.7278v2", 
    "author": "Gregory Kucherov", 
    "publish": "2013-02-28T18:35:21Z", 
    "summary": "De Brujin graphs are widely used in bioinformatics for processing\nnext-generation sequencing data. Due to a very large size of NGS datasets, it\nis essential to represent de Bruijn graphs compactly, and several approaches to\nthis problem have been proposed recently. In this work, we show how to reduce\nthe memory required by the algorithm of [3] that represents de Brujin graphs\nusing Bloom filters. Our method requires 30% to 40% less memory with respect to\nthe method of [3], with insignificant impact to construction time. At the same\ntime, our experiments showed a better query time compared to [3]. This is, to\nour knowledge, the best practical representation for de Bruijn graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1303.0270v1", 
    "other_authors": "Crysttian Arantes Paix\u00e3o, Fl\u00e1vio Code\u00e7o Coelho", 
    "title": "Computable Compressed Matrices", 
    "arxiv-id": "1303.0270v1", 
    "author": "Fl\u00e1vio Code\u00e7o Coelho", 
    "publish": "2013-03-01T20:34:41Z", 
    "summary": "The biggest cost of computing with large matrices in any modern computer is\nrelated to memory latency and bandwidth. The average latency of modern RAM\nreads is 150 times greater than a clock step of the processor. Throughput is a\nlittle better but still 25 times slower than the CPU can consume. The\napplication of bitstring compression allows for larger matrices to be moved\nentirely to the cache memory of the computer, which has much better latency and\nbandwidth (average latency of L1 cache is 3 to 4 clock steps). This allows for\nmassive performance gains as well as the ability to simulate much larger models\nefficiently. In this work, we propose a methodology to compress matrices in\nsuch a way that they retain their mathematical properties. Considerable\ncompression of the data is also achieved in the process Thus allowing for the\ncomputation of much larger linear problems within the same memory constraints\nwhen compared with the traditional representation of matrices."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.02.013", 
    "link": "http://arxiv.org/pdf/1303.0328v6", 
    "other_authors": "Ernst W. Mayer", 
    "title": "Efficient long division via Montgomery multiply", 
    "arxiv-id": "1303.0328v6", 
    "author": "Ernst W. Mayer", 
    "publish": "2013-03-02T01:00:44Z", 
    "summary": "We present a novel right-to-left long division algorithm based on the\nMontgomery modular multiply, consisting of separate highly efficient loops with\nsimply carry structure for computing first the remainder (x mod q) and then the\nquotient floor(x/q). These loops are ideally suited for the case where x\noccupies many more machine words than the divide modulus q, and are strictly\nlinear time in the \"bitsize ratio\" lg(x)/lg(q). For the paradigmatic\nperformance test of multiword dividend and single 64-bit-word divisor,\nexploitation of the inherent data-parallelism of the algorithm effectively\nmitigates the long latency of hardware integer MUL operations, as a result of\nwhich we are able to achieve respective costs for remainder-only and full-DIV\n(remainder and quotient) of 6 and 12.5 cycles per dividend word on the Intel\nCore 2 implementation of the x86_64 architecture, in single-threaded execution\nmode. We further describe a simple \"bit-doubling modular inversion\" scheme,\nwhich allows the entire iterative computation of the mod-inverse required by\nthe Montgomery multiply at arbitrarily large precision to be performed with\ncost less than that of a single Newtonian iteration performed at the full\nprecision of the final result. We also show how the Montgomery-multiply-based\npowering can be efficiently used in Mersenne and Fermat-number trial\nfactorization via direct computation of a modular inverse power of 2, without\nany need for explicit radix-mod scalings."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.0377v1", 
    "other_authors": "Gunjan Kumar, Saswata Shannigrahi", 
    "title": "New Online Algorithm for Dynamic Speed Scaling with Sleep State", 
    "arxiv-id": "1303.0377v1", 
    "author": "Saswata Shannigrahi", 
    "publish": "2013-03-02T11:38:17Z", 
    "summary": "In this paper, we consider an energy-efficient scheduling problem where $n$\njobs $J_1, J_2, ..., J_n$ need to be executed such that the total energy usage\nof these jobs is minimized while ensuring that each job is finished within it's\ndeadline. We work in an online setting where a job is known only at it's\narrival time, along with it's processing volume and deadline. In such a\nsetting, the currently best-known algorithm by Han et al. \\cite{han} provides a\ncompetitive ratio max $\\{4, 2 + {\\alpha}^{\\alpha}\\}$ of energy usage. In this\npaper, we present a new online algorithm SqOA which provides a competitive\nratio max $\\{4, 2 + (2-1/{\\alpha})^\\alpha 2^{\\alpha-1}\\}$ of energy usage. For\n$\\alpha \\geq 3$, the competitive ratio of our algorithm is better than that of\nany other existing algorithms for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.0422v1", 
    "other_authors": "Ahmet Erdem Sariyuce, Kamer Kaya, Erik Saule, Umit V. Catalyurek", 
    "title": "Incremental Algorithms for Network Management and Analysis based on   Closeness Centrality", 
    "arxiv-id": "1303.0422v1", 
    "author": "Umit V. Catalyurek", 
    "publish": "2013-03-02T19:49:32Z", 
    "summary": "Analyzing networks requires complex algorithms to extract meaningful\ninformation. Centrality metrics have shown to be correlated with the importance\nand loads of the nodes in network traffic. Here, we are interested in the\nproblem of centrality-based network management. The problem has many\napplications such as verifying the robustness of the networks and controlling\nor improving the entity dissemination. It can be defined as finding a small set\nof topological network modifications which yield a desired closeness centrality\nconfiguration. As a fundamental building block to tackle that problem, we\npropose incremental algorithms which efficiently update the closeness\ncentrality values upon changes in network topology, i.e., edge insertions and\ndeletions. Our algorithms are proven to be efficient on many real-life\nnetworks, especially on small-world networks, which have a small diameter and a\nspike-shaped shortest distance distribution. In addition to closeness\ncentrality, they can also be a great arsenal for the shortest-path-based\nmanagement and analysis of the networks. We experimentally validate the\nefficiency of our algorithms on large networks and show that they update the\ncloseness centrality values of the temporal DBLP-coauthorship network of 1.2\nmillion users 460 times faster than it would take to compute them from scratch.\nTo the best of our knowledge, this is the first work which can yield practical\nlarge-scale network management based on closeness centrality values."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.0726v2", 
    "other_authors": "Amol Deshpande, Lisa Hellerstein, Devorah Kletenik", 
    "title": "Approximation Algorithms for Stochastic Boolean Function Evaluation and   Stochastic Submodular Set Cover", 
    "arxiv-id": "1303.0726v2", 
    "author": "Devorah Kletenik", 
    "publish": "2013-03-04T15:15:36Z", 
    "summary": "Stochastic Boolean Function Evaluation is the problem of determining the\nvalue of a given Boolean function f on an unknown input x, when each bit of x_i\nof x can only be determined by paying an associated cost c_i. The assumption is\nthat x is drawn from a given product distribution, and the goal is to minimize\nthe expected cost. This problem has been studied in Operations Research, where\nit is known as \"sequential testing\" of Boolean functions. It has also been\nstudied in learning theory in the context of learning with attribute costs. We\nconsider the general problem of developing approximation algorithms for\nStochastic Boolean Function Evaluation. We give a 3-approximation algorithm for\nevaluating Boolean linear threshold formulas. We also present an approximation\nalgorithm for evaluating CDNF formulas (and decision trees) achieving a factor\nof O(log kd), where k is the number of terms in the DNF formula, and d is the\nnumber of clauses in the CNF formula. In addition, we present approximation\nalgorithms for simultaneous evaluation of linear threshold functions, and for\nranking of linear functions.\n  Our function evaluation algorithms are based on reductions to the Stochastic\nSubmodular Set Cover (SSSC) problem. This problem was introduced by Golovin and\nKrause. They presented an approximation algorithm for the problem, called\nAdaptive Greedy. Our main technical contribution is a new approximation\nalgorithm for the SSSC problem, which we call Adaptive Dual Greedy. It is an\nextension of the Dual Greedy algorithm for Submodular Set Cover due to Fujito,\nwhich is a generalization of Hochbaum's algorithm for the classical Set Cover\nProblem. We also give a new bound on the approximation achieved by the Adaptive\nGreedy algorithm of Golovin and Krause."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1016v2", 
    "other_authors": "Joshua Wang", 
    "title": "Space-Efficient Las Vegas Algorithms for K-SUM", 
    "arxiv-id": "1303.1016v2", 
    "author": "Joshua Wang", 
    "publish": "2013-03-05T12:32:10Z", 
    "summary": "Using hashing techniques, this paper develops a family of space-efficient Las\nVegas randomized algorithms for $k$-SUM problems. This family includes an\nalgorithm that can solve 3-SUM in $O(n^2)$ time and $O(\\sqrt{n})$ space. It\nalso establishes a new time-space upper bound for SUBSET-SUM, which can be\nsolved by a Las Vegas algorithm in $O^*(2^{(1-\\sqrt{\\8/9\\beta})n})$ time and\n$O^*(2^{\\beta n})$ space, for any $\\beta \\in [0, \\9/32]$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1559v4", 
    "other_authors": "G. Ausiello, P. G. Franciosa, G. F. Italiano, A. Ribichini", 
    "title": "On Resilient Graph Spanners", 
    "arxiv-id": "1303.1559v4", 
    "author": "A. Ribichini", 
    "publish": "2013-03-06T22:17:18Z", 
    "summary": "We introduce and investigate a new notion of resilience in graph spanners.\nLet $S$ be a spanner of a graph $G$. Roughly speaking, we say that a spanner\n$S$ is resilient if all its point-to-point distances are resilient to edge\nfailures. Namely, whenever any edge in $G$ fails, then as a consequence of this\nfailure all distances do not degrade in $S$ substantially more than in $G$\n(i.e., the relative distance increases in $S$ are very close to those in the\nunderlying graph $G$). In this paper we show that sparse resilient spanners\nexist, and that they can be computed efficiently."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1643v1", 
    "other_authors": "N. S. Narayanaswamy, R. Subashini", 
    "title": "$d$-COS-R is FPT via Interval Deletion", 
    "arxiv-id": "1303.1643v1", 
    "author": "R. Subashini", 
    "publish": "2013-03-07T10:44:42Z", 
    "summary": "A binary matrix $M$ has the Consecutive Ones Property (COP) if there exists a\npermutation of columns that arranges the ones consecutively in all the rows.\nGiven a matrix, the $d$-COS-R problem is to determine if there exists a set of\nat most $d$ rows whose deletion results in a matrix with COP. We consider the\nparameterized complexity of this problem with respect to the number $d$ of rows\nto be deleted as the parameter. The closely related Interval Deletion problem\nhas recently shown to be FPT [Y. Cao and D. Marx, Interval Deletion is\nFixed-Parameter Tractable, arXiv:1211.5933 [cs.DS],2012]. In this work, we\ndescribe a recursive depth-bounded search tree algorithm in which the problems\nat the leaf-level are solved as instances of Interval Deletion. The running\ntime of the algorithm is dominated by the running time of Interval Deletion,\nand therefore we show that $d$-COS-R is fixed-parameter tractable and has a\nrun-time of $O^*(10^d)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1671v1", 
    "other_authors": "R. Krithika, N. S. Narayanaswamy", 
    "title": "Another Disjoint Compression Algorithm for OCT", 
    "arxiv-id": "1303.1671v1", 
    "author": "N. S. Narayanaswamy", 
    "publish": "2013-03-07T12:58:35Z", 
    "summary": "We describe an elegant O*(2^k) algorithm for the disjoint compression problem\nfor Odd Cycle Transversal based on a reduction to Above Guarantee Vertex Cover.\nWe believe that this algorithm refines the understanding of the Odd Cycle\nTransversal algorithm by Reed, Smith and Vetta."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1872v1", 
    "other_authors": "Lei Wang, Xiaodong Wang, Yingjie Wu, Daxin Zhu", 
    "title": "An Efficient Dynamic Programming Algorithm for the Generalized LCS   Problem with Multiple Substring Exclusion Constrains", 
    "arxiv-id": "1303.1872v1", 
    "author": "Daxin Zhu", 
    "publish": "2013-03-08T02:15:59Z", 
    "summary": "In this paper, we consider a generalized longest common subsequence problem\nwith multiple substring exclusion constrains. For the two input sequences $X$\nand $Y$ of lengths $n$ and $m$, and a set of $d$ constrains $P=\\{P_1,...,P_d\\}$\nof total length $r$, the problem is to find a common subsequence $Z$ of $X$ and\n$Y$ excluding each of constrain string in $P$ as a substring and the length of\n$Z$ is maximized. The problem was declared to be NP-hard\\cite{1}, but we\nfinally found that this is not true. A new dynamic programming solution for\nthis problem is presented in this paper. The correctness of the new algorithm\nis proved. The time complexity of our algorithm is $O(nmr)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.1912v1", 
    "other_authors": "Nicole Megow, Andreas Wiese", 
    "title": "Competitive-Ratio Approximation Schemes for Minimizing the Makespan in   the Online-List Model", 
    "arxiv-id": "1303.1912v1", 
    "author": "Andreas Wiese", 
    "publish": "2013-03-08T08:52:20Z", 
    "summary": "We consider online scheduling on multiple machines for jobs arriving\none-by-one with the objective of minimizing the makespan. For any number of\nidentical parallel or uniformly related machines, we provide a\ncompetitive-ratio approximation scheme that computes an online algorithm whose\ncompetitive ratio is arbitrarily close to the best possible competitive ratio.\nWe also determine this value up to any desired accuracy. This is the first\napplication of competitive-ratio approximation schemes in the online-list\nmodel. The result proves the applicability of the concept in different online\nmodels. We expect that it fosters further research on other online problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.2514v2", 
    "other_authors": "Wojciech Wawrzyniak", 
    "title": "A local constant-factor approximation algorithm for MDS problem in   anonymous network", 
    "arxiv-id": "1303.2514v2", 
    "author": "Wojciech Wawrzyniak", 
    "publish": "2013-03-11T13:32:14Z", 
    "summary": "In research on distributed local algorithms it is commonly assumed that each\nvertex has a unique identifier in the entire graph. However, it turns out that\nin case of certain classes of graphs (for example not lift-closed bounded\ndegree graphs) identifiers are unnecessary and only a port ordering is needed.\nOne of the open issues was whether identifiers are essential in planar graphs.\nIn this paper, we answer this question and we propose an algorithm which\nreturns constant approximation of the MDS problem in CONGEST model. The\nalgorithm doesn't use any additional information about the structure of the\ngraph and the nodes don't have unique identifiers. We hope that this paper will\nbe very helpful as a hint for further comparisons of the unique identifier\nmodel and the model with only a port numbering in other classes of graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.2730v1", 
    "other_authors": "Luca Trevisan", 
    "title": "Is Cheeger-type Approximation Possible for Nonuniform Sparsest Cut?", 
    "arxiv-id": "1303.2730v1", 
    "author": "Luca Trevisan", 
    "publish": "2013-03-12T00:16:54Z", 
    "summary": "In the {\\em nonuniform sparsest cut} problem, given two undirected graphs $G$\nand $H$ over the same set of vertices $V$, we want to find a cut $(S,V-S)$ that\nminimizes the ratio between the fraction of $G$-edges that are cut and the\nfraction of $H$-edges that are cut. The ratio (which is at most 1 in an optimal\nsolution) is called the {\\em sparsity} of the cut. In the {\\em uniform sparsest\ncut} problem, $H$ is a clique over $V$. If $G$ is regular, it is possible to\nfind a solution to the uniform sparsest cut of cost $O(\\sqrt{opt})$ in nearly\nlinear time. Is such an approximation, which we call \"Cheege-type\"\napproximation, achievable in the non-uniform case?\n  We show that the answer is negative, assuming the Unique Games Conjecture,\nfor general H. Furthermore, the Leighton-Rao linear programming relaxation and\nthe spectral relaxation fail to find such an approximation even if $H$ is a\nclique over a subset of vertices. Using semidefinite programming, however, we\ncan find Cheeger-type approximations in polynomial time whenever the adjacency\nmatrix of $H$ has rank 1. (This includes the cases in which $H$ is a clique\nover a subset of vertices.)"
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.2772v1", 
    "other_authors": "Richard P. Brent", 
    "title": "Further analysis of the binary Euclidean algorithm", 
    "arxiv-id": "1303.2772v1", 
    "author": "Richard P. Brent", 
    "publish": "2013-03-12T03:57:58Z", 
    "summary": "The binary Euclidean algorithm is a variant of the classical Euclidean\nalgorithm. It avoids multiplications and divisions, except by powers of two, so\nis potentially faster than the classical algorithm on a binary machine. We\ndescribe the binary algorithm and consider its average case behaviour. In\nparticular, we correct some errors in the literature, discuss some results of\nVall\\'ee, and describe a numerical computation which supports a conjecture of\nVall\\'ee."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.2920v2", 
    "other_authors": "Neal E. Young", 
    "title": "Approximating 1-dimensional TSP Requires Omega(n log n) Comparisons", 
    "arxiv-id": "1303.2920v2", 
    "author": "Neal E. Young", 
    "publish": "2013-03-12T15:38:48Z", 
    "summary": "We give a short proof that any comparison-based n^(1-epsilon)-approximation\nalgorithm for the 1-dimensional Traveling Salesman Problem (TSP) requires\nOmega(n log n) comparisons."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.2963v1", 
    "other_authors": "Tobias M\u00f6mke", 
    "title": "A Competitive Ratio Approximation Scheme for the k-Server Problem in   Fixed Finite Metrics", 
    "arxiv-id": "1303.2963v1", 
    "author": "Tobias M\u00f6mke", 
    "publish": "2013-03-12T17:38:35Z", 
    "summary": "We show how to restrict the analysis of a class of online problems that\nincludes the $k$-server problem in finite metrics such that we only have to\nconsider finite sequences of request. When applying the restrictions, both the\noptimal offline solutions and the best possible deterministic or randomized\nonline solutions only differ by at most an arbitrarily small constant factor\nfrom the corresponding solutions without restrictions. Furthermore, we show how\nto obtain an algorithm with best possible deterministic or randomized\ncompetitive ratio for the restricted setup. Thus, for each fixed finite metrics\nour result qualifies as a competitive ratio approximation scheme as defined by\nG\\\"unther et al."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.3386v1", 
    "other_authors": "Noa Avigdor-elgrabli, Yuval Rabani", 
    "title": "An Optimal Randomized Online Algorithm for Reordering Buffer Management", 
    "arxiv-id": "1303.3386v1", 
    "author": "Yuval Rabani", 
    "publish": "2013-03-14T09:47:49Z", 
    "summary": "We give an $O(\\log\\log k)$-competitive randomized online algorithm for\nreordering buffer management, where $k$ is the buffer size. Our bound matches\nthe lower bound of Adamaszek et al. (STOC 2011). Our algorithm has two stages\nwhich are executed online in parallel. The first stage computes\ndeterministically a feasible fractional solution to an LP relaxation for\nreordering buffer management. The second stage \"rounds\" using randomness the\nfractional solution. The first stage is based on the online primal-dual schema,\ncombined with a dual fitting argument. As multiplicative weights steps and dual\nfitting steps are interleaved and in some sense conflicting, combining them is\nchallenging. We also note that we apply the primal-dual schema to a relaxation\nwith mixed packing and covering constraints. We pay the $O(\\log\\log k)$\ncompetitive factor for the gap between the computed LP solution and the optimal\nLP solution. The second stage gives an online algorithm that converts the LP\nsolution to an integral solution, while increasing the cost by an O(1) factor.\nThis stage generalizes recent results that gave a similar approximation factor\nfor rounding the LP solution, albeit using an offline rounding algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.3564v1", 
    "other_authors": "Elaheh Fata, Stephen L. Smith, Shreyas Sundaram", 
    "title": "Distributed Dominating Sets on Grids", 
    "arxiv-id": "1303.3564v1", 
    "author": "Shreyas Sundaram", 
    "publish": "2013-03-14T19:33:26Z", 
    "summary": "This paper presents a distributed algorithm for finding near optimal\ndominating sets on grids. The basis for this algorithm is an existing\ncentralized algorithm that constructs dominating sets on grids. The size of the\ndominating set provided by this centralized algorithm is upper-bounded by\n$\\lceil\\frac{(m+2)(n+2)}{5}\\rceil$ for $m\\times n$ grids and its difference\nfrom the optimal domination number of the grid is upper-bounded by five. Both\nthe centralized and distributed algorithms are generalized for the $k$-distance\ndominating set problem, where all grid vertices are within distance $k$ of the\nvertices in the dominating set."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.3945v1", 
    "other_authors": "Toshiya Tanaka, Tomohiro I, Shunsuke Inenaga, Hideo Bannai, Masayuki Takeda", 
    "title": "Computing convolution on grammar-compressed text", 
    "arxiv-id": "1303.3945v1", 
    "author": "Masayuki Takeda", 
    "publish": "2013-03-16T05:07:24Z", 
    "summary": "The convolution between a text string $S$ of length $N$ and a pattern string\n$P$ of length $m$ can be computed in $O(N \\log m)$ time by FFT. It is known\nthat various types of approximate string matching problems are reducible to\nconvolution. In this paper, we assume that the input text string is given in a\ncompressed form, as a \\emph{straight-line program (SLP)}, which is a context\nfree grammar in the Chomsky normal form that derives a single string. Given an\nSLP $\\mathcal{S}$ of size $n$ describing a text $S$ of length $N$, and an\nuncompressed pattern $P$ of length $m$, we present a simple $O(nm \\log m)$-time\nalgorithm to compute the convolution between $S$ and $P$. We then show that\nthis can be improved to $O(\\min\\{nm, N-\\alpha\\} \\log m)$ time, where $\\alpha\n\\geq 0$ is a value that represents the amount of redundancy that the SLP\ncaptures with respect to the length-$m$ substrings. The key of the improvement\nis our new algorithm that computes the convolution between a trie of size $r$\nand a pattern string $P$ of length $m$ in $O(r \\log m)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.4031v1", 
    "other_authors": "Fatemeh Rajabi-Alni", 
    "title": "Two exact algorithms for the generalized assignment problem", 
    "arxiv-id": "1303.4031v1", 
    "author": "Fatemeh Rajabi-Alni", 
    "publish": "2013-03-17T02:57:16Z", 
    "summary": "Let A={a_1,a_2,...,a_s} and B={b_1,b_2,...,b_t} be two sets of objects with\ns+r=n, the generalized assignment problem assigns each element a_i in A to at\nleast alpha_i and at most alpha '_i elements in B, and each element b_j in B to\nat least beta_j and at most beta '_j elements in A for all 1 <= i <= s and 1 <=\nj <= t. In this paper, we present an O(n^4) time and O(n) space algorithm for\nthis problem using the well known Hungarian algorithm. We also present an\nO(n^3) algorithm for a special case of the generalized assignment, called the\nlimited-capacity assignment problem, where alpha_i,beta_j=1 for all i,j."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.4177v1", 
    "other_authors": "Igor S. Sergeev", 
    "title": "A relation between additive and multiplicative complexity of Boolean   functions", 
    "arxiv-id": "1303.4177v1", 
    "author": "Igor S. Sergeev", 
    "publish": "2013-03-18T08:02:55Z", 
    "summary": "In the present note we prove an asymptotically tight relation between\nadditive and multiplicative complexity of Boolean functions with respect to\nimplementation by circuits over the basis {+,*,1}."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.4632v1", 
    "other_authors": "Paulo Shakarian, V. S. Subrahmanian", 
    "title": "Geospatial Optimization Problems", 
    "arxiv-id": "1303.4632v1", 
    "author": "V. S. Subrahmanian", 
    "publish": "2013-03-19T15:02:55Z", 
    "summary": "There are numerous applications which require the ability to take certain\nactions (e.g. distribute money, medicines, people etc.) over a geographic\nregion. A disaster relief organization must allocate people and supplies to\nparts of a region after a disaster. A public health organization must allocate\nlimited vaccine to people across a region. In both cases, the organization is\ntrying to optimize something (e.g. minimize expected number of people with a\ndisease). We introduce \"geospatial optimization problems\" (GOPs) where an\norganization has limited resources and budget to take actions in a geographic\narea. The actions result in one or more properties changing for one or more\nlocations. There are also certain constraints on the combinations of actions\nthat can be taken. We study two types of GOPs - goal-based and\nbenefit-maximizing (GBGOP and BMGOP respectively). A GBGOP ensures that certain\nproperties must be true at specified locations after the actions are taken\nwhile a BMGOP optimizes a linear benefit function. We show both problems to be\nNP-hard (with membership in NP for the associated decision problems).\nAdditionally, we prove limits on approximation for both problems. We present\ninteger programs for both GOPs that provide exact solutions. We also correctly\nreduce the number of variables in for the GBGOP integer constraints. For BMGOP,\nwe present the BMGOP-Compute algorithm that runs in PTIME and provides a\nreasonable approximation guarantee in most cases."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.5217v3", 
    "other_authors": "Martin Aum\u00fcller, Martin Dietzfelbinger", 
    "title": "Optimal Partitioning for Dual-Pivot Quicksort", 
    "arxiv-id": "1303.5217v3", 
    "author": "Martin Dietzfelbinger", 
    "publish": "2013-03-21T10:20:51Z", 
    "summary": "Dual-pivot quicksort refers to variants of classical quicksort where in the\npartitioning step two pivots are used to split the input into three segments.\nThis can be done in different ways, giving rise to different algorithms.\nRecently, a dual-pivot algorithm proposed by Yaroslavskiy received much\nattention, because a variant of it replaced the well-engineered quicksort\nalgorithm in Sun's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed\nthis algorithm and showed that on average it uses 1.9n ln n + O(n) comparisons\nto sort an input of size n, beating standard quicksort, which uses 2n ln n +\nO(n) comparisons. We introduce a model that captures all dual-pivot algorithms,\ngive a unified analysis, and identify new dual-pivot algorithms that minimize\nthe average number of key comparisons among all possible algorithms up to a\nlinear term. This minimum is 1.8n ln n + O(n). For the case that the pivots are\nchosen from a small sample, we include a comparison of dual-pivot quicksort and\nclassical quicksort. Specifically, we show that dual-pivot quicksort benefits\nfrom a skewed choice of pivots. We experimentally evaluate our algorithms and\ncompare them to Yaroslavskiy's algorithm and the recently described three-pivot\nquicksort algorithm of Kushagra et al. (ALENEX 2014)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.5479v2", 
    "other_authors": "Mikkel Thorup", 
    "title": "Bottom-k and Priority Sampling, Set Similarity and Subset Sums with   Minimal Independence", 
    "arxiv-id": "1303.5479v2", 
    "author": "Mikkel Thorup", 
    "publish": "2013-03-21T22:18:08Z", 
    "summary": "We consider bottom-k sampling for a set X, picking a sample S_k(X) consisting\nof the k elements that are smallest according to a given hash function h. With\nthis sample we can estimate the relative size f=|Y|/|X| of any subset Y as\n|S_k(X) intersect Y|/k. A standard application is the estimation of the Jaccard\nsimilarity f=|A intersect B|/|A union B| between sets A and B. Given the\nbottom-k samples from A and B, we construct the bottom-k sample of their union\nas S_k(A union B)=S_k(S_k(A) union S_k(B)), and then the similarity is\nestimated as |S_k(A union B) intersect S_k(A) intersect S_k(B)|/k.\n  We show here that even if the hash function is only 2-independent, the\nexpected relative error is O(1/sqrt(fk)). For fk=Omega(1) this is within a\nconstant factor of the expected relative error with truly random hashing.\n  For comparison, consider the classic approach of kxmin-wise where we use k\nhash independent functions h_1,...,h_k, storing the smallest element with each\nhash function. For kxmin-wise there is an at least constant bias with constant\nindependence, and it is not reduced with larger k. Recently Feigenblat et al.\nshowed that bottom-k circumvents the bias if the hash function is 8-independent\nand k is sufficiently large. We get down to 2-independence for any k. Our\nresult is based on a simply union bound, transferring generic concentration\nbounds for the hashing scheme to the bottom-k sample, e.g., getting stronger\nprobability error bounds with higher independence.\n  For weighted sets, we consider priority sampling which adapts efficiently to\nthe concrete input weights, e.g., benefiting strongly from heavy-tailed input.\nThis time, the analysis is much more involved, but again we show that generic\nconcentration bounds can be applied."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2015.05.045", 
    "link": "http://arxiv.org/pdf/1303.5481v1", 
    "other_authors": "Mugurel Ionut Andreica", 
    "title": "Novel O(H(N)+N/H(N)) Algorithmic Techniques for Several Types of Queries   and Updates on Rooted Trees and Lists", 
    "arxiv-id": "1303.5481v1", 
    "author": "Mugurel Ionut Andreica", 
    "publish": "2013-03-21T22:41:52Z", 
    "summary": "In this paper we present novel algorithmic techniques with a O(H(N)+N/H(N))\ntime complexity for performing several types of queries and updates on general\nrooted trees, binary search trees and lists of size N. For rooted trees we\nintroduce a new compressed super-node tree representation which can be used for\nefficiently addressing a wide range of applications. For binary search trees we\ndiscuss the idea of globally rebuilding the entire tree in a fully balanced\nmanner whenever the height of the tree exceeds the value of a conveniently\nchosen function of the number of tree nodes. In the end of the paper we\nintroduce the H-list data structure which supports concatenation, split and\nseveral types of queries. Note that when choosing H(N)=sqrt(N) we obtain\nO(H(N)+N/H(N))=O(sqrt(N))."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.orl.2014.02.004", 
    "link": "http://arxiv.org/pdf/1303.6071v2", 
    "other_authors": "Jian Li, Tianlin Shi", 
    "title": "A Fully Polynomial-Time Approximation Scheme for Approximating a Sum of   Random Variables", 
    "arxiv-id": "1303.6071v2", 
    "author": "Tianlin Shi", 
    "publish": "2013-03-25T10:09:00Z", 
    "summary": "Given $n$ independent random variables $X_1, X_2, ..., X_n$ and an integer\n$C$, we study the fundamental problem of computing the probability that the sum\n$X=X_1+X_2+...+X_n$ is at most $C$. We assume that each random variable $X_i$\nis implicitly given by an oracle which, given an input value $k$, returns the\nprobability $X_i\\leq k$. We give the first deterministic fully polynomial-time\napproximation scheme (FPTAS) to estimate the probability up to a relative error\nof $1\\pm \\epsilon$. Our algorithm is based on the idea developed for\napproximately counting knapsack solutions in [Gopalan et al. FOCS11]."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.orl.2014.02.004", 
    "link": "http://arxiv.org/pdf/1303.6481v1", 
    "other_authors": "Simon Gog, Alistair Moffat, J. Shane Culpepper, Andrew Turpin, Anthony Wirth", 
    "title": "Large-Scale Pattern Search Using Reduced-Space On-Disk Suffix Arrays", 
    "arxiv-id": "1303.6481v1", 
    "author": "Anthony Wirth", 
    "publish": "2013-03-26T13:28:31Z", 
    "summary": "The suffix array is an efficient data structure for in-memory pattern search.\nSuffix arrays can also be used for external-memory pattern search, via\ntwo-level structures that use an internal index to identify the correct block\nof suffix pointers. In this paper we describe a new two-level suffix\narray-based index structure that requires significantly less disk space than\nprevious approaches. Key to the saving is the use of disk blocks that are based\non prefixes rather than the more usual uniform-sampling approach, allowing\nreductions between blocks and subparts of other blocks. We also describe a new\nin-memory structure based on a condensed BWT string, and show that it allows\ncommon patterns to be resolved without access to the text. Experiments using 64\nGB of English web text and a laptop computer with just 4 GB of main memory\ndemonstrate the speed and versatility of the new approach. For this data the\nindex is around one- third the size of previous two-level mechanisms; and the\nmemory footprint of as little as 1% of the text size means that queries can be\nprocessed more quickly than is possible with a compact FM-INDEX."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1303.6867v2", 
    "other_authors": "Bang Ye Wu, Li-Hsuan Chen", 
    "title": "Parameterized algorithms for the 2-clustering problem with minimum sum   and minimum sum of squares objective functions", 
    "arxiv-id": "1303.6867v2", 
    "author": "Li-Hsuan Chen", 
    "publish": "2013-03-27T15:57:58Z", 
    "summary": "In the {\\sc Min-Sum 2-Clustering} problem, we are given a graph and a\nparameter $k$, and the goal is to determine if there exists a 2-partition of\nthe vertex set such that the total conflict number is at most $k$, where the\nconflict number of a vertex is the number of its non-neighbors in the same\ncluster and neighbors in the different cluster. The problem is equivalent to\n{\\sc 2-Cluster Editing} and {\\sc 2-Correlation Clustering} with an additional\nmultiplicative factor two in the cost function. In this paper we show an\nalgorithm for {\\sc Min-Sum 2-Clustering} with time complexity $O(n\\cdot\n2.619^{r/(1-4r/n)}+n^3)$, where $n$ is the number of vertices and $r=k/n$.\nParticularly, the time complexity is $O^*(2.619^{k/n})$ for $k\\in o(n^2)$ and\npolynomial for $k\\in O(n\\log n)$, which implies that the problem can be solved\nin subexponential time for $k\\in o(n^2)$. We also design a parameterized\nalgorithm for a variant in which the cost is the sum of the squared\nconflict-numbers. For $k\\in o(n^3)$, the algorithm runs in subexponential\n$O(n^3\\cdot 5.171^{\\theta})$ time, where $\\theta=\\sqrt{k/n}$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1303.6872v1", 
    "other_authors": "Maxime Crochemore, Costas S. Iliopoulos, Tomasz Kociumaka, Marcin Kubica, Alessio Langiu, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, Tomasz Walen", 
    "title": "Order-Preserving Suffix Trees and Their Algorithmic Applications", 
    "arxiv-id": "1303.6872v1", 
    "author": "Tomasz Walen", 
    "publish": "2013-03-27T16:13:03Z", 
    "summary": "Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted\nto Theor. Comp. Sci.) introduced order-preserving pattern matching. In this\nproblem we are looking for consecutive substrings of the text that have the\nsame \"shape\" as a given pattern. These results include a linear-time\norder-preserving pattern matching algorithm for polynomially-bounded alphabet\nand an extension of this result to pattern matching with multiple patterns. We\nmake one step forward in the analysis and give an\n$O(\\frac{n\\log{n}}{\\log\\log{n}})$ time randomized algorithm constructing suffix\ntrees in the order-preserving setting. We show a number of applications of\norder-preserving suffix trees to identify patterns and repetitions in time\nseries."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1303.7462v7", 
    "other_authors": "James Smith", 
    "title": "Concur: An Algorithm for Merging Concurrent Changes without Conflicts", 
    "arxiv-id": "1303.7462v7", 
    "author": "James Smith", 
    "publish": "2013-03-29T19:15:04Z", 
    "summary": "Suppose you and I are both editing a document. You make and change and I make\na change, concurrently. Now if we want to still be seeing the same document\nthen I need to apply your change after mine and you mine after yours. But we\ncan't just apply them willy-nilly. I must amend yours somehow and you mine. If\nmy change is written D, yours d, and the amended changes D.d and d.D, we get\n*D*D.d=*d*d.D as long as applying is written * and we don't care about what\nwe're applying the changes to. We start by proving this identity for single\nchanges and finish by proving it for many."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.0085v1", 
    "other_authors": "Yinglei Song", 
    "title": "An Improved Parameterized Algorithm for the Independent Feedback Vertex   Set Problem", 
    "arxiv-id": "1308.0085v1", 
    "author": "Yinglei Song", 
    "publish": "2013-08-01T03:26:10Z", 
    "summary": "In this paper, we develop a new parameterized algorithm for the {\\sc\nIndependent Feedback Vertex Set} (IFVS) problem. Given a graph $G=(V,E)$, the\ngoal of the problem is to determine whether there exists a vertex subset\n$F\\subseteq V$ such that $V-F$ induces a forest in $G$ and $F$ is an\nindependent set. We show that there exists a parameterized algorithm that can\ndetermine whether a graph contains an IFVS of size $k$ or not in time\n$O(4^kn^{2})$. To our best knowledge, this result improves the known upper\nbound for this problem, which is $O^{*}(5^{k}n^{O(1)})$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.0776v2", 
    "other_authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", 
    "title": "Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier   and Derandomization", 
    "arxiv-id": "1308.0776v2", 
    "author": "Danupon Nanongkai", 
    "publish": "2013-08-04T04:11:38Z", 
    "summary": "We study dynamic $(1+\\epsilon)$-approximation algorithms for the all-pairs\nshortest paths problem in unweighted undirected $n$-node $m$-edge graphs under\nedge deletions. The fastest algorithm for this problem is a randomized\nalgorithm with a total update time of $\\tilde O(mn/\\epsilon)$ and constant\nquery time by Roditty and Zwick [FOCS 2004]. The fastest deterministic\nalgorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total\nupdate time of $O(mn^2)$ and constant query time. We improve these results as\nfollows: (1) We present an algorithm with a total update time of $\\tilde\nO(n^{5/2}/\\epsilon)$ and constant query time that has an additive error of $2$\nin addition to the $1+\\epsilon$ multiplicative error. This beats the previous\n$\\tilde O(mn/\\epsilon)$ time when $m=\\Omega(n^{3/2})$. Note that the additive\nerror is unavoidable since, even in the static case, an $O(n^{3-\\delta})$-time\n(a so-called truly subcubic) combinatorial algorithm with $1+\\epsilon$\nmultiplicative error cannot have an additive error less than $2-\\epsilon$,\nunless we make a major breakthrough for Boolean matrix multiplication [Dor et\nal. FOCS 1996] and many other long-standing problems [Vassilevska Williams and\nWilliams FOCS 2010]. The algorithm can also be turned into a\n$(2+\\epsilon)$-approximation algorithm (without an additive error) with the\nsame time guarantees, improving the recent $(3+\\epsilon)$-approximation\nalgorithm with $\\tilde O(n^{5/2+O(\\sqrt{\\log{(1/\\epsilon)}/\\log n})})$ running\ntime of Bernstein and Roditty [SODA 2011] in terms of both approximation and\ntime guarantees. (2) We present a deterministic algorithm with a total update\ntime of $\\tilde O(mn/\\epsilon)$ and a query time of $O(\\log\\log n)$. The\nalgorithm has a multiplicative error of $1+\\epsilon$ and gives the first\nimproved deterministic algorithm since 1981. It also answers an open question\nraised by Bernstein [STOC 2013]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.0907v1", 
    "other_authors": "Shailesh Vaya", 
    "title": "The complexity of resolving conflicts on MAC", 
    "arxiv-id": "1308.0907v1", 
    "author": "Shailesh Vaya", 
    "publish": "2013-08-05T08:58:32Z", 
    "summary": "We consider the fundamental problem of multiple stations competing to\ntransmit on a multiple access channel (MAC). We are given $n$ stations out of\nwhich at most $d$ are active and intend to transmit a message to other stations\nusing MAC. All stations are assumed to be synchronized according to a time\nclock. If $l$ stations node transmit in the same round, then the MAC provides\nthe feedback whether $l=0$, $l=2$ (collision occurred) or $l=1$. When $l=1$,\nthen a single station is indeed able to successfully transmit a message, which\nis received by all other nodes. For the above problem the active stations have\nto schedule their transmissions so that they can singly, transmit their\nmessages on MAC, based only on the feedback received from the MAC in previous\nround.\n  For the above problem it was shown in [Greenberg, Winograd, {\\em A Lower\nbound on the Time Needed in the Worst Case to Resolve Conflicts\nDeterministically in Multiple Access Channels}, Journal of ACM 1985] that every\ndeterministic adaptive algorithm should take $\\Omega(d (\\lg n)/(\\lg d))$ rounds\nin the worst case. The fastest known deterministic adaptive algorithm requires\n$O(d \\lg n)$ rounds. The gap between the upper and lower bound is $O(\\lg d)$\nround. It is substantial for most values of $d$: When $d = $ constant and $d\n\\in O(n^{\\epsilon})$ (for any constant $\\epsilon \\leq 1$, the lower bound is\nrespectively $O(\\lg n)$ and O(n), which is trivial in both cases. Nevertheless,\nthe above lower bound is interesting indeed when $d \\in$ poly($\\lg n$). In this\nwork, we present a novel counting argument to prove a tight lower bound of\n$\\Omega(d \\lg n)$ rounds for all deterministic, adaptive algorithms, closing\nthis long standing open question.}"
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.1068v1", 
    "other_authors": "Rajesh Chitnis, Laszlo Egri, Daniel Marx", 
    "title": "List H-Coloring a Graph by Removing Few Vertices", 
    "arxiv-id": "1308.1068v1", 
    "author": "Daniel Marx", 
    "publish": "2013-08-05T18:56:38Z", 
    "summary": "In the deletion version of the list homomorphism problem, we are given graphs\nG and H, a list L(v) that is a subset of V(H) for each vertex v of G, and an\ninteger k. The task is to decide whether there exists a subset W of V(G) of\nsize at most k such that there is a homomorphism from G \\ W to H respecting the\nlists. We show that DL-Hom(H), parameterized by k and |H|, is fixed-parameter\ntractable for any (P6, C6)-free bipartite graph H; already for this restricted\nclass of graphs, the problem generalizes Vertex Cover, Odd Cycle Transversal,\nand Vertex Multiway Cut parameterized by the size of the cutset and the number\nof terminals. We conjecture that DL-Hom(H) is fixed-parameter tractable for the\nclass of graphs H for which the list homomorphism problem (without deletions)\nis polynomial-time solvable; by a result of Feder, Hell and Huang (1999), a\ngraph H belongs to this class precisely if it is a bipartite graph whose\ncomplement is a circular arc graph. We show that this conjecture is equivalent\nto the fixed-parameter tractability of a single fairly natural satisfiability\nproblem, Clause Deletion Chain-SAT."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.1351v3", 
    "other_authors": "Davis Issac, Ragesh Jaiswal", 
    "title": "An $O^*(1.0821^n)$-Time Algorithm for Computing Maximum Independent Set   in Graphs with Bounded Degree 3", 
    "arxiv-id": "1308.1351v3", 
    "author": "Ragesh Jaiswal", 
    "publish": "2013-08-06T17:08:50Z", 
    "summary": "We give an $O^*(1.0821^n)$-time, polynomial space algorithm for computing\nMaximum Independent Set in graphs with bounded degree 3. This improves all the\nprevious running time bounds known for the problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.1385v1", 
    "other_authors": "Cynthia Dwork, Aleksandar Nikolov, Kunal Talwar", 
    "title": "Efficient Algorithms for Privately Releasing Marginals via Convex   Relaxations", 
    "arxiv-id": "1308.1385v1", 
    "author": "Kunal Talwar", 
    "publish": "2013-08-06T19:29:17Z", 
    "summary": "Consider a database of $n$ people, each represented by a bit-string of length\n$d$ corresponding to the setting of $d$ binary attributes. A $k$-way marginal\nquery is specified by a subset $S$ of $k$ attributes, and a $|S|$-dimensional\nbinary vector $\\beta$ specifying their values. The result for this query is a\ncount of the number of people in the database whose attribute vector restricted\nto $S$ agrees with $\\beta$.\n  Privately releasing approximate answers to a set of $k$-way marginal queries\nis one of the most important and well-motivated problems in differential\nprivacy. Information theoretically, the error complexity of marginal queries is\nwell-understood: the per-query additive error is known to be at least\n$\\Omega(\\min\\{\\sqrt{n},d^{\\frac{k}{2}}\\})$ and at most\n$\\tilde{O}(\\min\\{\\sqrt{n} d^{1/4},d^{\\frac{k}{2}}\\})$. However, no polynomial\ntime algorithm with error complexity as low as the information theoretic upper\nbound is known for small $n$. In this work we present a polynomial time\nalgorithm that, for any distribution on marginal queries, achieves average\nerror at most $\\tilde{O}(\\sqrt{n} d^{\\frac{\\lceil k/2 \\rceil}{4}})$. This error\nbound is as good as the best known information theoretic upper bounds for\n$k=2$. This bound is an improvement over previous work on efficiently releasing\nmarginals when $k$ is small and when error $o(n)$ is desirable. Using private\nboosting we are also able to give nearly matching worst-case error bounds.\n  Our algorithms are based on the geometric techniques of Nikolov, Talwar, and\nZhang. The main new ingredients are convex relaxations and careful use of the\nFrank-Wolfe algorithm for constrained convex minimization. To design our\nrelaxations, we rely on the Grothendieck inequality from functional analysis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.1556v1", 
    "other_authors": "Yinglei Song", 
    "title": "On the Independent Set and Common Subgraph Problems in Random Graphs", 
    "arxiv-id": "1308.1556v1", 
    "author": "Yinglei Song", 
    "publish": "2013-08-07T12:55:41Z", 
    "summary": "In this paper, we develop efficient exact and approximate algorithms for\ncomputing a maximum independent set in random graphs. In a random graph $G$,\neach pair of vertices are joined by an edge with a probability $p$, where $p$\nis a constant between $0$ and $1$. We show that, a maximum independent set in a\nrandom graph that contains $n$ vertices can be computed in expected computation\ntime $2^{O(\\log_{2}^{2}{n})}$. Using techniques based on enumeration, we\ndevelop an algorithm that can find a largest common subgraph in two random\ngraphs in $n$ and $m$ vertices ($m \\leq n$) in expected computation time\n$2^{O(n^{\\frac{1}{2}}\\log_{2}^{\\frac{5}{3}}{n})}$. In addition, we show that,\nwith high probability, the parameterized independent set problem is fixed\nparameter tractable in random graphs and the maximum independent set in a\nrandom graph in $n$ vertices can be approximated within a ratio of\n$\\frac{2n}{2^{\\sqrt{\\log_{2}{n}}}}$ in expected polynomial time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.1568v1", 
    "other_authors": "Urakov, Timeryaev", 
    "title": "All-Pairs Shortest Paths Algorithm for High-dimensional Sparse Graphs", 
    "arxiv-id": "1308.1568v1", 
    "author": "Timeryaev", 
    "publish": "2013-08-07T13:32:38Z", 
    "summary": "Here the All-pairs shortest path problem on weighted undirected sparse graphs\nis being considered. For the problem considered, we propose ``disassembly and\nassembly of a graph'' algorithm which uses a solution of the problem on a\nsmall-dimensional graph to obtain the solution for the given graph. The\nproposed algorithm has been compared to one of the fastest classic algorithms\non data from an open public source."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.2400v1", 
    "other_authors": "Niraj Kumar Singh, Soubhik Chakraborty, Dheeresh Kumar Mallick", 
    "title": "An Adaptable Fast Matrix Multiplication Algorithm, Going Beyond the Myth   of Decimal War", 
    "arxiv-id": "1308.2400v1", 
    "author": "Dheeresh Kumar Mallick", 
    "publish": "2013-08-11T14:55:14Z", 
    "summary": "In this paper we present an adaptable fast matrix multiplication (AFMM)\nalgorithm, for two nxn dense matrices which computes the product matrix with\naverage complexity Tavg(n) = d1d2n3 with the acknowledgement that the average\ncount is obtained for addition as the basic operation rather than\nmultiplication which is probably the unquestionable choice for basic operation\nin existing matrix multiplication algorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.2599v4", 
    "other_authors": "Gregory Gutin, Magnus Wahlstrom, Anders Yeo", 
    "title": "Parameterized Rural Postman Problem", 
    "arxiv-id": "1308.2599v4", 
    "author": "Anders Yeo", 
    "publish": "2013-08-12T15:45:58Z", 
    "summary": "The Directed Rural Postman Problem (DRPP) can be formulated as follows: given\na strongly connected directed multigraph $D=(V,A)$ with nonnegative integral\nweights on the arcs, a subset $R$ of $A$ and a nonnegative integer $\\ell$,\ndecide whether $D$ has a closed directed walk containing every arc of $R$ and\nof total weight at most $\\ell$. Let $k$ be the number of weakly connected\ncomponents in the the subgraph of $D$ induced by $R$. Sorge et al. (2012) ask\nwhether the DRPP is fixed-parameter tractable (FPT) when parameterized by $k$,\ni.e., whether there is an algorithm of running time $O^*(f(k))$ where $f$ is a\nfunction of $k$ only and the $O^*$ notation suppresses polynomial factors.\nSorge et al. (2012) note that this question is of significant practical\nrelevance and has been open for more than thirty years. Using an algebraic\napproach, we prove that DRPP has a randomized algorithm of running time\n$O^*(2^k)$ when $\\ell$ is bounded by a polynomial in the number of vertices in\n$D$. We also show that the same result holds for the undirected version of\nDRPP, where $D$ is a connected undirected multigraph."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.2891v2", 
    "other_authors": "N. A. Carella", 
    "title": "Deterministic Integer Factorization Algorithms", 
    "arxiv-id": "1308.2891v2", 
    "author": "N. A. Carella", 
    "publish": "2013-08-05T22:06:03Z", 
    "summary": "This note presents a deterministic integer factorization algorithm of running\ntime complexity O(N^(1/6+e)), e > 0. This improves the current performances of\ndeterministic integer factorization algorithms rated at O(N^(1/4+e)) arithmetic\noperations. Equivalently, given the least (log N)/6 bits of a factor of N = pq,\nthe algorithm factors the integer in polynomial time O(log(N)^c), c > 0\nconstant."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3326v1", 
    "other_authors": "Gelin Zhou", 
    "title": "Sorted Range Reporting Revisited", 
    "arxiv-id": "1308.3326v1", 
    "author": "Gelin Zhou", 
    "publish": "2013-08-15T07:38:16Z", 
    "summary": "We consider the two-dimensional sorted range reporting problem. Our data\nstructure requires O(n lglg n) words of space and O(lglg n + k lglg n) query\ntime, where k is the number of points in the query range. This data structure\nimproves a recent result of Nekrich and Navarro [8] by a factor of O(lglg n) in\nquery time, and matches the state of the art for unsorted range reporting [1]."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3336v5", 
    "other_authors": "Jakub \u0141\u0105cki, Jakub O\u0107wieja, Marcin Pilipczuk, Piotr Sankowski, Anna Zych", 
    "title": "The Power of Dynamic Distance Oracles: Efficient Dynamic Algorithms for   the Steiner Tree", 
    "arxiv-id": "1308.3336v5", 
    "author": "Anna Zych", 
    "publish": "2013-08-15T08:32:39Z", 
    "summary": "In this paper we study the Steiner tree problem over a dynamic set of\nterminals. We consider the model where we are given an $n$-vertex graph\n$G=(V,E,w)$ with positive real edge weights, and our goal is to maintain a tree\nwhich is a good approximation of the minimum Steiner tree spanning a terminal\nset $S \\subseteq V$, which changes over time. The changes applied to the\nterminal set are either terminal additions (incremental scenario), terminal\nremovals (decremental scenario), or both (fully dynamic scenario). Our task\nhere is twofold. We want to support updates in sublinear $o(n)$ time, and keep\nthe approximation factor of the algorithm as small as possible. We show that we\ncan maintain a $(6+\\varepsilon)$-approximate Steiner tree of a general graph in\n$\\tilde{O}(\\sqrt{n} \\log D)$ time per terminal addition or removal. Here, $D$\ndenotes the stretch of the metric induced by $G$. For planar graphs we achieve\nthe same running time and the approximation ratio of $(2+\\varepsilon)$.\nMoreover, we show faster algorithms for incremental and decremental scenarios.\nFinally, we show that if we allow higher approximation ratio, even more\nefficient algorithms are possible. In particular we show a polylogarithmic time\n$(4+\\varepsilon)$-approximate algorithm for planar graphs.\n  One of the main building blocks of our algorithms are dynamic distance\noracles for vertex-labeled graphs, which are of independent interest. We also\nimprove and use the online algorithms for the Steiner tree problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3405v1", 
    "other_authors": "Matthias Poloczek, David P. Williamson, Anke van Zuylen", 
    "title": "On Some Recent MAX SAT Approximation Algorithms", 
    "arxiv-id": "1308.3405v1", 
    "author": "Anke van Zuylen", 
    "publish": "2013-08-15T14:20:51Z", 
    "summary": "Recently a number of randomized 3/4-approximation algorithms for MAX SAT have\nbeen proposed that all work in the same way: given a fixed ordering of the\nvariables, the algorithm makes a random assignment to each variable in\nsequence, in which the probability of assigning each variable true or false\ndepends on the current set of satisfied (or unsatisfied) clauses. To our\nknowledge, the first such algorithm was proposed by Poloczek and Schnitger; Van\nZuylen subsequently gave an algorithm that set the probabilities differently\nand had a simpler analysis. She also set up a framework for deriving such\nalgorithms. Buchbinder, Feldman, Naor, and Schwartz, as a special case of their\nwork on maximizing submodular functions, also give a randomized\n3/4-approximation algorithm for MAX SAT with the same structure as these\nprevious algorithms. In this note we give a gloss on the Buchbinder et al.\nalgorithm that makes it even simpler, and show that in fact it is equivalent to\nthe previous algorithm of Van Zuylen. We also show how it extends to a\ndeterministic LP rounding algorithm; such an algorithm was also given by Van\nZuylen."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3466v3", 
    "other_authors": "Petra Berenbrink, Funda Erg\u00fcn, Frederik Mallmann-Trenn, Erfan Sadeqi Azer", 
    "title": "Palindrome Recognition In The Streaming Model", 
    "arxiv-id": "1308.3466v3", 
    "author": "Erfan Sadeqi Azer", 
    "publish": "2013-08-15T18:04:24Z", 
    "summary": "In the Palindrome Problem one tries to find all palindromes (palindromic\nsubstrings) in a given string. A palindrome is defined as a string which reads\nforwards the same as backwards, e.g., the string \"racecar\". A related problem\nis the Longest Palindromic Substring Problem in which finding an arbitrary one\nof the longest palindromes in the given string suffices. We regard the\nstreaming version of both problems. In the streaming model the input arrives\nover time and at every point in time we are only allowed to use sublinear\nspace. The main algorithms in this paper are the following: The first one is a\none-pass randomized algorithm that solves the Palindrome Problem. It has an\nadditive error and uses $O(\\sqrt n$) space. The second algorithm is a two-pass\nalgorithm which determines the exact locations of all longest palindromes. It\nuses the first algorithm as the first pass. The third algorithm is again a\none-pass randomized algorithm, which solves the Longest Palindromic Substring\nProblem. It has a multiplicative error using only $O(\\log(n))$ space. We also\ngive two variants of the first algorithm which solve other related practical\nproblems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3520v1", 
    "other_authors": "Rajesh Chitnis, MohammadTaghi Hajiaghayi, Guy Kortsarz", 
    "title": "Fixed-Parameter and Approximation Algorithms: A New Look", 
    "arxiv-id": "1308.3520v1", 
    "author": "Guy Kortsarz", 
    "publish": "2013-08-15T22:17:59Z", 
    "summary": "A Fixed-Parameter Tractable (\\FPT) $\\rho$-approximation algorithm for a\nminimization (resp. maximization) parameterized problem $P$ is an FPT algorithm\nthat, given an instance $(x, k)\\in P$ computes a solution of cost at most $k\n\\cdot \\rho(k)$ (resp. $k/\\rho(k)$) if a solution of cost at most (resp. at\nleast) $k$ exists; otherwise the output can be arbitrary. For well-known\nintractable problems such as the W[1]-hard {Clique} and W[2]-hard {Set Cover}\nproblems, the natural question is whether we can get any \\FPT-approximation. It\nis widely believed that both {Clique} and {Set-Cover} admit no FPT\n$\\rho$-approximation algorithm, for any increasing function $\\rho$. Assuming\nstandard conjectures such as the Exponential Time Hypothesis (ETH)\n\\cite{eth-paturi} and the Projection Games Conjecture (PGC) \\cite{r3}, we make\nthe first progress towards proving this conjecture by showing that\n  1. Under the ETH and PGC, there exist constants $F_1, F_2 >0$ such that the\n{Set Cover} problem does not admit an FPT approximation algorithm with ratio\n$k^{F_1}$ in $2^{k^{F_2}}\\cdot \\text{poly}(N,M)$ time, where $N$ is the size of\nthe universe and $M$ is the number of sets.\n  2. Unless $\\NP\\subseteq \\SUBEXP$, for every $1> \\delta > 0$ there exists a\nconstant $F(\\delta)>0$ such that {Clique} has no FPT cost approximation with\nratio $k^{1-\\delta}$ in $2^{k^{F}}\\cdot \\text{poly}(n)$ time, where $n$ is the\nnumber of vertices in the graph.\n  In the second part of the paper we consider various W[1]-hard problems such\nas {\\dst}, {\\dsf}, Directed Steiner Network and {\\mec}. For all these problem\nwe give polynomial time $f(\\text{OPT})$-approximation algorithms for some small\nfunction $f$ (the largest approximation ratio we give is $\\text{OPT}^2$)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.3822v3", 
    "other_authors": "Steven M. Kearns", 
    "title": "Sublinear Matching With Finite Automata Using Reverse Suffix Scanning", 
    "arxiv-id": "1308.3822v3", 
    "author": "Steven M. Kearns", 
    "publish": "2013-08-18T01:45:04Z", 
    "summary": "We give algorithms to accelerate the computation of deterministic finite\nautomata (DFA) by calculating the state of a DFA n positions ahead utilizing a\nreverse scan of the next n characters. Often this requires scanning fewer than\nn characters resulting in a fraction of the input being skipped and a\ncommensurate increase in processing speed. The skipped fraction is > 80% in\nseveral of our examples. We introduce offsetting finite automata (OFA) to\nencode the accelerated computation. OFA generalize DFA by adding an integer\noffset to the current input index at each state transition. We give algorithms\nfor constructing an OFA that accepts the same language as a DFA while possibly\nskipping input, and for matching with an OFA. Compared to previous algorithms\nthat attempt to skip some of the input, the new matching algorithm can skip\nmore often and can skip farther. In the worst case the new matching algorithm\nscans the same number of characters as a simple forward scan, whereas previous\napproaches often scan more, so the new algorithm can be used as a reliable\nreplacement for the simple forward scan. Additionally, the new algorithm adapts\nto available memory and time constraints."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.4469v1", 
    "other_authors": "Craig Dillabaugh", 
    "title": "External Memory Algorithms For Path Traversal in Graphs", 
    "arxiv-id": "1308.4469v1", 
    "author": "Craig Dillabaugh", 
    "publish": "2013-08-21T01:56:50Z", 
    "summary": "This thesis presents a number of results related to path traversal in trees\nand graphs. In particular, we focus on data structures which allow such\ntraversals to be performed efficiently in the external memory setting. In\naddition, for trees and planar graphs the data structures we present are\nsuccinct. Our tree structures permit efficient bottom-up path traversal in\nrooted trees of arbitrary degree and efficient top-down path traversal in\nbinary trees. In the graph setting, we permit efficient traversal of an\narbitrary path in bounded degree planar graphs. Our data structures for both\ntrees and graphs match or slightly improve current best results for external\nmemory path traversal in these settings while at the same time improving space\nbounds due to the succinct nature of our data structures. Employing our path\ntraversal structure for bounded degree planar graphs, we describe a number of\nuseful applications of this technique for triangular meshes in R^2. As an\nextension of the R^2 representation for triangular meshes we also present an\nefficient external memory representation for well-shaped tetrahedral meshes in\nR^3. The external memory representation we present is based on a partitioning\nscheme that matches the current best-known results for well-shaped tetrahedral\nmeshes. We describe applications of path traversal in tetrahedral meshes which\nare made efficient in the external memory setting using our structure. Finally,\nwe present a result on using jump-and-walk point location in well-shaped meshes\nin both R^2 and R^3. We demonstrate that, given an approximate nearest\nneighbour from among the vertices of a mesh, locating the simplex containing\nthe query point involves a constant length walk (path traversal) in the mesh."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.4534v4", 
    "other_authors": "P. Biro, D. F. Manlove, I. McBride", 
    "title": "The Hospitals / Residents Problem with Couples: Complexity and Integer   Programming Models", 
    "arxiv-id": "1308.4534v4", 
    "author": "I. McBride", 
    "publish": "2013-08-21T10:58:03Z", 
    "summary": "The Hospitals / Residents problem with Couples (HRC) is a generalisation of\nthe classical Hospitals / Resident problem (HR) that is important in practical\napplications because it models the case where couples submit joint preference\nlists over pairs of (typically geographically close) hospitals. In this paper\nwe give a new NP-completeness result for the problem of deciding whether a\nstable matching exists, in highly restricted instances of HRC. Further, we\npresent an Integer Programming (IP) model for HRC and extend it the case where\npreference lists can include ties. Also, we describe an empirical study of an\nIP model or HRC and its extension to the case where preference lists can\ninclude ties. This model was applied to randomly generated instances and also\nreal-world instances arising from previous matching runs of the Scottish\nFoundation Allocation Scheme, used to allocate junior doctors to hospitals in\nScotland."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.5444v1", 
    "other_authors": "Rad Niazadeh, Robert D. Kleinberg", 
    "title": "A Unified Approach to Online Allocation Algorithms via Randomized Dual   Fitting", 
    "arxiv-id": "1308.5444v1", 
    "author": "Robert D. Kleinberg", 
    "publish": "2013-08-25T20:14:08Z", 
    "summary": "We present a unified framework for designing and analyzing algorithms for\nonline budgeted allocation problems (including online matching) and their\ngeneralization, the Online Generalized Assignment Problem (OnGAP). These\nproblems have been intensively studied as models of how to allocate impressions\nfor online advertising. In contrast to previous analyses of online budgeted\nallocation algorithms (the so-called \"balance\" or \"water-filling\" family of\nalgorithms) our analysis is based on the method of randomized dual fitting,\nanalogous to the recent analysis of the RANKING algorithm for online matching\ndue to Devanur et al. Our main contribution is thus to provide a unified method\nof proof that simultaneously derives the optimal competitive ratio bounds for\nonline matching and online fractional budgeted allocation. The same method of\nproof also supplies $(1-1/e)$ competitive ratio bounds for greedy algorithms\nfor both problems, in the random order arrival model; this simplifies existing\nanalyses of greedy online allocation algorithms with random order of arrivals,\nwhile also strengthening them to apply to a larger family of greedy algorithms.\nFinally, for the more general OnGAP problem, we show that no algorithm can be\nconstant-competitive; instead we present an algorithm whose competitive ratio\ndepends logarithmically on a certain parameter of the problem instance, and we\nshow that this dependence cannot be improved."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.6145v1", 
    "other_authors": "Lars F. Bonnichsen, Sven Karlsson, Christian W. Probst", 
    "title": "ELB-Trees, An Efficient and Lock-free B-tree Derivative", 
    "arxiv-id": "1308.6145v1", 
    "author": "Christian W. Probst", 
    "publish": "2013-08-28T12:45:42Z", 
    "summary": "This technical report is an extension of the paper of the same title, which\nis to appear at MUCOCOS'13. The technical report proves correctness of the\nELB-trees operations' semantics and that the operations are lock-free."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.6203v1", 
    "other_authors": "Harris V. Georgiou", 
    "title": "Adaptive detection and severity level characterization algorithm for   Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) via oximetry signal   analysis", 
    "arxiv-id": "1308.6203v1", 
    "author": "Harris V. Georgiou", 
    "publish": "2013-08-28T16:23:46Z", 
    "summary": "In this paper, an abstract definition and formal specification is presented\nfor the task of adaptive-threshold OSAHS events detection and severity\ncharacterization. Specifically, a low-level pseudocode is designed for the\nalgorithm of raw oximetry signal pre-processing, calculation of the 'drop' and\n'rise' frames in the related time series, detection of valid apnea/hypopnea\nevents via SpO2 saturation level tracking, as well as calculation of\ncorresponding event rates for OSAHS severity characterization. The designed\nalgorithm can be used as the first module in a machine learning application\nwhere these data can be used as inputs or encoded into higher-level statistics\n(features) for pattern classifiers, in the context of computer-aided or fully\nautomated diagnosis of OSAHS and related pathologies."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.6509v2", 
    "other_authors": "Pawel Gawrychowski, Damian Straszak", 
    "title": "Beating O(nm) in approximate LZW-compressed pattern matching", 
    "arxiv-id": "1308.6509v2", 
    "author": "Damian Straszak", 
    "publish": "2013-08-29T16:16:45Z", 
    "summary": "Given an LZW/LZ78 compressed text, we want to find an approximate occurrence\nof a given pattern of length m. The goal is to achieve time complexity\ndepending on the size n of the compressed representation of the text instead of\nits length. We consider two specific definitions of approximate matching,\nnamely the Hamming distance and the edit distance, and show how to achieve\nO(nm^0.5k^2) and O(nm^0.5k^3) running time, respectively, where k is the bound\non the distance. Both algorithms use just linear space. Even for very small\nvalues of k, the best previously known solutions required O(nm) time. Our main\ncontribution is applying a periodicity-based argument in a way that is\ncomputationally effective even if we need to operate on a compressed\nrepresentation of a string, while the previous solutions were either based on a\ndynamic programming, or a black-box application of tools developed for\nuncompressed strings."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.6635v1", 
    "other_authors": "Rui Ferreira", 
    "title": "Efficiently Listing Combinatorial Patterns in Graphs", 
    "arxiv-id": "1308.6635v1", 
    "author": "Rui Ferreira", 
    "publish": "2013-08-30T02:10:19Z", 
    "summary": "Graphs are extremely versatile and ubiquitous mathematical structures with\npotential to model a wide range of domains. For this reason, graph problems\nhave been of interest since the early days of computer science. Some of these\nproblems consider substructures of a graph that have certain properties. These\nsubstructures of interest, generally called patterns, are often meaningful in\nthe domain being modeled. Classic examples of patterns include spanning trees,\ncycles and subgraphs.\n  This thesis focuses on the topic of explicitly listing all the patterns\nexisting in an input graph. One of the defining features of this problem is\nthat the number of patterns is frequently exponential on the size of the input\ngraph. Thus, the time complexity of listing algorithms is parameterized by the\nsize of the output.\n  The main contribution of this work is the presentation of optimal algorithms\nfor four different problems of listing patterns in graphs, namely the listing\nof k-subtrees, k-subgraphs, st-paths and cycles. The algorithms presented are\nframed within the same generic approach, based in a recursive partition of the\nsearch space that divides the problem into subproblems. The key to an efficient\nimplementation of this approach is to avoid recursing into subproblems that do\nnot list any patterns. With this goal in sight, a dynamic data structure,\ncalled the certificate, is introduced and maintained throughout the recursion.\nMoreover, properties of the recursion tree and lower bounds on the number of\npatterns are used to amortize the cost of the algorithm on the size of the\noutput."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1308.6711v1", 
    "other_authors": "Michael T. Goodrich, Pawe\u0142 Pszona", 
    "title": "Streamed Graph Drawing and the File Maintenance Problem", 
    "arxiv-id": "1308.6711v1", 
    "author": "Pawe\u0142 Pszona", 
    "publish": "2013-08-30T11:10:39Z", 
    "summary": "In streamed graph drawing, a planar graph, G, is given incrementally as a\ndata stream and a straight-line drawing of G must be updated after each new\nedge is released. To preserve the mental map, changes to the drawing should be\nminimized after each update, and Binucci et al.show that exponential area is\nnecessary and sufficient for a number of streamed graph drawings for trees if\nedges are not allowed to move at all. We show that a number of streamed graph\ndrawings can, in fact, be done with polynomial area, including planar streamed\ngraph drawings of trees, tree-maps, and outerplanar graphs, if we allow for a\nsmall number of coordinate movements after each update. Our algorithms involve\nan interesting connection to a classic algorithmic problem - the file\nmaintenance problem - and we also give new algorithms for this problem in a\nframework where bulk memory moves are allowed."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00453-014-9874-8", 
    "link": "http://arxiv.org/pdf/1402.1076v3", 
    "other_authors": "Aaron Bohy, V\u00e9ronique Bruy\u00e8re, Jean-Fran\u00e7ois Raskin", 
    "title": "Symblicit algorithms for optimal strategy synthesis in monotonic Markov   decision processes (extended version)", 
    "arxiv-id": "1402.1076v3", 
    "author": "Jean-Fran\u00e7ois Raskin", 
    "publish": "2014-02-05T16:22:41Z", 
    "summary": "When treating Markov decision processes (MDPs) with large state spaces, using\nexplicit representations quickly becomes unfeasible. Lately, Wimmer et al. have\nproposed a so-called symblicit algorithm for the synthesis of optimal\nstrategies in MDPs, in the quantitative setting of expected mean-payoff. This\nalgorithm, based on the strategy iteration algorithm of Howard and Veinott,\nefficiently combines symbolic and explicit data structures, and uses binary\ndecision diagrams as symbolic representation. The aim of this paper is to show\nthat the new data structure of pseudo-antichains (an extension of antichains)\nprovides another interesting alternative, especially for the class of monotonic\nMDPs. We design efficient pseudo-antichain based symblicit algorithms (with\nopen source implementations) for two quantitative settings: the expected\nmean-payoff and the stochastic shortest path. For two practical applications\ncoming from automated planning and LTL synthesis, we report promising\nexperimental results w.r.t. both the run time and the memory consumption."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2486001.2486009", 
    "link": "http://arxiv.org/pdf/1402.1194v1", 
    "other_authors": "G\u00e1bor R\u00e9tv\u00e1ri, J\u00e1nos Tapolcai, Attila K\u0151r\u00f6si, Andr\u00e1s Majd\u00e1n, Zal\u00e1n Heszberger", 
    "title": "Compressing IP Forwarding Tables: Towards Entropy Bounds and Beyond", 
    "arxiv-id": "1402.1194v1", 
    "author": "Zal\u00e1n Heszberger", 
    "publish": "2014-02-05T21:45:26Z", 
    "summary": "Lately, there has been an upsurge of interest in compressed data structures,\naiming to pack ever larger quantities of information into constrained memory\nwithout sacrificing the efficiency of standard operations, like random access,\nsearch, or update. The main goal of this paper is to demonstrate how data\ncompression can benefit the networking community, by showing how to squeeze the\nIP Forwarding Information Base (FIB), the giant table consulted by IP routers\nto make forwarding decisions, into information-theoretical entropy bounds, with\nessentially zero cost on longest prefix match and FIB update. First, we adopt\nthe state-of-the-art in compressed data structures, yielding a static\nentropy-compressed FIB representation with asymptotically optimal lookup. Then,\nwe re-design the venerable prefix tree, used commonly for IP lookup for at\nleast 20 years in IP routers, to also admit entropy bounds and support lookup\nin optimal time and update in nearly optimal time. Evaluations on a Linux\nkernel prototype indicate that our compressors encode a FIB comprising more\nthan 440K prefixes to just about 100--400 KBytes of memory, with a threefold\nincrease in lookup throughput and no penalty on FIB updates.\n  This technical report contains a number of important corrections and\nrevisions to the original manuscript."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2486001.2486009", 
    "link": "http://arxiv.org/pdf/1402.2097v1", 
    "other_authors": "Gary Benson, Avivit Levy, Riva Shalom", 
    "title": "Longest Common Subsequence in k-length substrings", 
    "arxiv-id": "1402.2097v1", 
    "author": "Riva Shalom", 
    "publish": "2014-02-10T10:54:52Z", 
    "summary": "In this paper we define a new problem, motivated by computational biology,\n$LCSk$ aiming at finding the maximal number of $k$ length $substrings$,\nmatching in both input strings while preserving their order of appearance. The\ntraditional LCS definition is a special case of our problem, where $k = 1$. We\nprovide an algorithm, solving the general case in $O(n^2)$ time, where $n$ is\nthe length of the input strings, equaling the time required for the special\ncase of $k=1$. The space requirement of the algorithm is $O(kn)$. %, however,\nin order to enable %backtracking of the solution, $O(n^2)$ space is needed.\n  We also define a complementary $EDk$ distance measure and show that\n$EDk(A,B)$ can be computed in $O(nm)$ time and $O(km)$ space, where $m$, $n$\nare the lengths of the input sequences $A$ and $B$ respectively."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2486001.2486009", 
    "link": "http://arxiv.org/pdf/1402.2136v3", 
    "other_authors": "Leo van Iersel, Steven Kelk, Nela Leki\u0107, Chris Whidden, Norbert Zeh", 
    "title": "Hybridization Number on Three Rooted Binary Trees is EPT", 
    "arxiv-id": "1402.2136v3", 
    "author": "Norbert Zeh", 
    "publish": "2014-02-10T13:24:51Z", 
    "summary": "Phylogenetic networks are leaf-labelled directed acyclic graphs that are used\nto describe non-treelike evolutionary histories and are thus a generalization\nof phylogenetic trees. The hybridization number of a phylogenetic network is\nthe sum of all indegrees minus the number of nodes plus one. The Hybridization\nNumber problem takes as input a collection of phylogenetic trees and asks to\nconstruct a phylogenetic network that contains an embedding of each of the\ninput trees and has a smallest possible hybridization number. We present an\nalgorithm for the Hybridization Number problem on three binary trees on $n$\nleaves, which runs in time $O(c^k poly(n))$, with $k$ the hybridization number\nof an optimal network and $c$ a constant. For two trees, an algorithm with\nrunning time $O(3.18^k n)$ was proposed before whereas an algorithm with\nrunning time $O(c^k poly(n))$ had prior to this article remained elusive for\nmore than two trees. The algorithm for two trees uses the close connection to\nacyclic agreement forests to achieve a linear exponent in the running time,\nwhile previous algorithms for more than two trees (explicitly or implicitly)\nrelied on a brute force search through all possible underlying network\ntopologies, leading to running times that are not $O(c^k poly(n))$ for any $c$.\nThe connection to acyclic agreement forests is much weaker for more than two\ntrees, so even given the right agreement forest, reconstructing the network\nposes major challenges. We prove novel structural results that allow us to\nreconstruct a network without having to guess the underlying topology. Our\ntechniques generalize to more than three input trees with the exception of one\nkey lemma that maps nodes in the network to tree nodes and, thus, minimizes the\namount of guessing involved in constructing the network. The main open problem\ntherefore is to establish a similar mapping for more than three trees."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2486001.2486009", 
    "link": "http://arxiv.org/pdf/1402.2137v1", 
    "other_authors": "Gregory Gutin, Mark Jones, Bin Sheng, Magnus Wahlstrom", 
    "title": "Parameterized Directed $k$-Chinese Postman Problem and $k$ Arc-Disjoint   Cycles Problem on Euler Digraphs", 
    "arxiv-id": "1402.2137v1", 
    "author": "Magnus Wahlstrom", 
    "publish": "2014-02-10T13:25:18Z", 
    "summary": "In the Directed $k$-Chinese Postman Problem ($k$-DCPP), we are given a\nconnected weighted digraph $G$ and asked to find $k$ non-empty closed directed\nwalks covering all arcs of $G$ such that the total weight of the walks is\nminimum. Gutin, Muciaccia and Yeo (Theor. Comput. Sci. 513 (2013) 124--128)\nasked for the parameterized complexity of $k$-DCPP when $k$ is the parameter.\nWe prove that the $k$-DCPP is fixed-parameter tractable.\n  We also consider a related problem of finding $k$ arc-disjoint directed\ncycles in an Euler digraph, parameterized by $k$. Slivkins (ESA 2003) showed\nthat this problem is W[1]-hard for general digraphs. Generalizing another\nresult by Slivkins, we prove that the problem is fixed-parameter tractable for\nEuler digraphs. The corresponding problem on vertex-disjoint cycles in Euler\ndigraphs remains W[1]-hard even for Euler digraphs."
},{
    "category": "cs.DS", 
    "doi": "10.1145/2486001.2486009", 
    "link": "http://arxiv.org/pdf/1402.2508v1", 
    "other_authors": "Steffen G\u00f6rzig", 
    "title": "Data Compaction - Compression without Decompression", 
    "arxiv-id": "1402.2508v1", 
    "author": "Steffen G\u00f6rzig", 
    "publish": "2014-02-11T14:48:27Z", 
    "summary": "Data compaction is a new approach for lossless and lossy compression of\nread-only array data. The biggest advantage over existing approaches is the\npossibility to access compressed data without any decompression. This makes\ndata compaction most suitable for systems that could currently not apply\ncompression techniques due to real-time or memory constraints. This is true for\nthe majority of all computers, i.e. a wide range of embedded systems."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.2710v1", 
    "other_authors": "L. Wang, R. C. de Lamare, M. Haardt", 
    "title": "Direction Finding Algorithms with Joint Iterative Subspace Optimization", 
    "arxiv-id": "1402.2710v1", 
    "author": "M. Haardt", 
    "publish": "2014-02-12T01:13:12Z", 
    "summary": "In this paper, a reduced-rank scheme with joint iterative optimization is\npresented for direction of arrival estimation. A rank-reduction matrix and an\nauxiliary reduced-rank parameter vector are jointly optimized to calculate the\noutput power with respect to each scanning angle. Subspace algorithms to\nestimate the rank-reduction matrix and the auxiliary vector are proposed.\nSimulations are performed to show that the proposed algorithms achieve an\nenhanced performance over existing algorithms in the studied scenarios."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.2712v3", 
    "other_authors": "Jiamou Liu, Kostya Ross", 
    "title": "Dynamic Partial Sorting", 
    "arxiv-id": "1402.2712v3", 
    "author": "Kostya Ross", 
    "publish": "2014-02-12T01:36:36Z", 
    "summary": "The dynamic partial sorting problem asks for an algorithm that maintains\nlists of numbers under the link, cut and change value operations, and queries\nthe sorted sequence of the $k$ least numbers in one of the lists. We first\nsolve the problem in $O(k\\log (n))$ time for queries and $O(\\log (n))$ time for\nupdates using the tournament tree data structure, where $n$ is the number of\nelements in the lists. We then introduce a layered tournament tree data\nstructure and solve the same problem in $O(\\log_\\varphi^* (n) k\\log (k))$ time\nfor queries and $O\\left(\\log (n)\\cdot\\log^2\\log (n)\\right)$ for updates, where\n$\\varphi$ is the golden ratio and $\\log_\\varphi^*(n)$ is the iterated\nlogarithmic function with base $\\varphi$."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.2741v1", 
    "other_authors": "Dimitris Papamichail, Thomas Caputi, Georgios Papamichail", 
    "title": "The Level Ancestor Problem in Practice", 
    "arxiv-id": "1402.2741v1", 
    "author": "Georgios Papamichail", 
    "publish": "2014-02-12T05:37:11Z", 
    "summary": "Given a rooted tree T, the level ancestor problem is the problem of answering\nqueries of the form LA(v, d), which identify the level d ancestor of a node v\nin the tree. Several algorithms of varied complexity have been proposed in the\nliterature, including optimal solutions that preprocess T in linear bounded\ntime and proceed to answer queries in constant bounded time. Despite its\nsignificance and numerous applications, to date there have been no comparative\nstudies of the performance of these algorithms and no implementations are\nwidely available. In our experimental study we have implemented and compared\nseveral solutions for the level ancestor problem, including two optimal\nalgorithms, and examine their space requirements and time performance."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.2760v2", 
    "other_authors": "J\u00e9r\u00e9mie Chalopin, Yoann Dieudonn\u00e9, Arnaud Labourel, Andrzej Pelc", 
    "title": "Rendezvous in Networks in Spite of Delay Faults", 
    "arxiv-id": "1402.2760v2", 
    "author": "Andrzej Pelc", 
    "publish": "2014-02-12T08:34:07Z", 
    "summary": "Two mobile agents, starting from different nodes of an unknown network, have\nto meet at the same node. Agents move in synchronous rounds using a\ndeterministic algorithm. Each agent has a different label, which it can use in\nthe execution of the algorithm, but it does not know the label of the other\nagent. Agents do not know any bound on the size of the network. In each round\nan agent decides if it remains idle or if it wants to move to one of the\nadjacent nodes. Agents are subject to delay faults: if an agent incurs a fault\nin a given round, it remains in the current node, regardless of its decision.\nIf it planned to move and the fault happened, the agent is aware of it. We\nconsider three scenarios of fault distribution: random (independently in each\nround and for each agent with constant probability 0 < p < 1), unbounded adver-\nsarial (the adversary can delay an agent for an arbitrary finite number of\nconsecutive rounds) and bounded adversarial (the adversary can delay an agent\nfor at most c consecutive rounds, where c is unknown to the agents). The\nquality measure of a rendezvous algorithm is its cost, which is the total\nnumber of edge traversals. For random faults, we show an algorithm with cost\npolynomial in the size n of the network and polylogarithmic in the larger label\nL, which achieves rendezvous with very high probability in arbitrary networks.\nBy contrast, for unbounded adversarial faults we show that rendezvous is not\nfeasible, even in the class of rings. Under this scenario we give a rendezvous\nalgorithm with cost O(nl), where l is the smaller label, working in arbitrary\ntrees, and we show that \\Omega(l) is the lower bound on rendezvous cost, even\nfor the two-node tree. For bounded adversarial faults, we give a rendezvous\nalgorithm working for arbitrary networks, with cost polynomial in n, and\nlogarithmic in the bound c and in the larger label L."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.2782v2", 
    "other_authors": "Roland Glantz, Henning Meyerhenke, Christian Schulz", 
    "title": "Tree-based Coarsening and Partitioning of Complex Networks", 
    "arxiv-id": "1402.2782v2", 
    "author": "Christian Schulz", 
    "publish": "2014-02-12T10:53:01Z", 
    "summary": "Many applications produce massive complex networks whose analysis would\nbenefit from parallel processing. Parallel algorithms, in turn, often require a\nsuitable network partition. For solving optimization tasks such as graph\npartitioning on large networks, multilevel methods are preferred in practice.\nYet, complex networks pose challenges to established multilevel algorithms, in\nparticular to their coarsening phase.\n  One way to specify a (recursive) coarsening of a graph is to rate its edges\nand then contract the edges as prioritized by the rating. In this paper we (i)\ndefine weights for the edges of a network that express the edges' importance\nfor connectivity, (ii) compute a minimum weight spanning tree $T^m$ with\nrespect to these weights, and (iii) rate the network edges based on the\nconductance values of $T^m$'s fundamental cuts. To this end, we also (iv)\ndevelop the first optimal linear-time algorithm to compute the conductance\nvalues of \\emph{all} fundamental cuts of a given spanning tree. We integrate\nthe new edge rating into a leading multilevel graph partitioner and equip the\nlatter with a new greedy postprocessing for optimizing the maximum\ncommunication volume (MCV). Experiments on bipartitioning frequently used\nbenchmark networks show that the postprocessing already reduces MCV by 11.3%.\nOur new edge rating further reduces MCV by 10.3% compared to the previously\nbest rating with the postprocessing in place for both ratings. In total, with a\nmodest increase in running time, our new approach reduces the MCV of complex\nnetwork partitions by 20.4%."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3036v1", 
    "other_authors": "J. David Morgenthaler, T. C. Hu", 
    "title": "Optimal Alphabetic Ternary Trees", 
    "arxiv-id": "1402.3036v1", 
    "author": "T. C. Hu", 
    "publish": "2014-02-13T06:00:59Z", 
    "summary": "We give a new algorithm to construct optimal alphabetic ternary trees, where\nevery internal node has at most three children. This algorithm generalizes the\nclassic Hu-Tucker algorithm, though the overall computational complexity has\nyet to be determined."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3444v3", 
    "other_authors": "Francesco Silvestri", 
    "title": "Subgraph Enumeration in Massive Graphs", 
    "arxiv-id": "1402.3444v3", 
    "author": "Francesco Silvestri", 
    "publish": "2014-02-14T12:01:47Z", 
    "summary": "We consider the problem of enumerating all instances of a given pattern graph\nin a large data graph. Our focus is on determining the input/output (I/O)\ncomplexity of this problem. Let $E$ be the number of edges in the data graph,\n$k=O(1)$ be the number of vertices in the pattern graph, $B$ be the block\nlength, and $M$ be the main memory size. The main results of the paper are two\nalgorithms that enumerate all instances of the pattern graph. The first one is\na deterministic algorithm that exploits a suitable independent set of the\npattern graph of size $1\\leq s \\leq k/2$ and requires\n$O\\left(E^{k-s}/\\left(BM^{k-s-1}\\right)\\right)$ I/Os. The second algorithm is a\nrandomized algorithm that enumerates all instances in\n$O\\left(E^{k/2}/\\left(BM^{k/2-1}\\right)\\right)$ expected I/Os; the same bound\nalso applies with high probability under some assumptions. A lower bound shows\nthat the deterministic algorithm is optimal for some pattern graphs with\n$s=k/2$ (e.g., paths and cycles of even length, meshes of even side), while the\nrandomized algorithm is optimal for a wide class of pattern graphs, called Alon\nclass (e.g., cliques, cycles and every graph with a perfect matching)."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3472v1", 
    "other_authors": "Ivan Bliznets, Fedor V. Fomin, Marcin Pilipczuk, Micha\u0142 Pilipczuk", 
    "title": "A subexponential parameterized algorithm for Proper Interval Completion", 
    "arxiv-id": "1402.3472v1", 
    "author": "Micha\u0142 Pilipczuk", 
    "publish": "2014-02-13T20:03:41Z", 
    "summary": "In the Proper Interval Completion problem we are given a graph G and an\ninteger k, and the task is to turn G using at most k edge additions into a\nproper interval graph, i.e., a graph admitting an intersection model of\nequal-length intervals on a line. The study of Proper Interval Completion from\nthe viewpoint of parameterized complexity has been initiated by Kaplan, Shamir\nand Tarjan [FOCS 1994; SIAM J. Comput. 1999], who showed an algorithm for the\nproblem working in $O(16^k (n + m))$ time. In this paper we present an\nalgorithm with running time $k^{O(k^{2/3})} + O(nm(kn + m))$, which is the\nfirst subexponential parameterized algorithm for Proper Interval Completion."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3473v3", 
    "other_authors": "Ivan Bliznets, Fedor V. Fomin, Marcin Pilipczuk, Micha\u0142 Pilipczuk", 
    "title": "A subexponential parameterized algorithm for Interval Completion", 
    "arxiv-id": "1402.3473v3", 
    "author": "Micha\u0142 Pilipczuk", 
    "publish": "2014-02-13T20:06:50Z", 
    "summary": "In the Interval Completion problem we are given a graph G and an integer k,\nand the task is to turn G using at most k edge additions into an interval\ngraph, i.e., a graph admitting an intersection model of intervals on a line.\nMotivated by applications in sparse matrix multiplication and molecular\nbiology, Kaplan, Shamir and Tarjan [FOCS 1994; SIAM J. Comput. 1999] asked for\na fixed-parameter algorithm solving this problem. This question was answer\naffirmatively more than a decade later by Villanger at el. [STOC 2007; SIAM J.\nComput. 2009], who presented an algorithm with running time $O(k^{2k}n^3m)$. We\ngive the first subexponential parameterized algorithm solving Interval\nCompletion in time $k^{O(\\sqrt{k})} n^{O(1)}$. This adds Interval Completion to\na very small list of parameterized graph modification problems solvable in\nsubexponential time."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3547v2", 
    "other_authors": "Hadas Shachnai, Meirav Zehavi", 
    "title": "Representative Families: A Unified Tradeoff-Based Approach", 
    "arxiv-id": "1402.3547v2", 
    "author": "Meirav Zehavi", 
    "publish": "2014-02-14T18:32:15Z", 
    "summary": "Let $M=(E,{\\cal I})$ be a matroid, and let $\\cal S$ be a family of subsets of\nsize $p$ of $E$. A subfamily $\\widehat{\\cal S}\\subseteq{\\cal S}$ represents\n${\\cal S}$ if for every pair of sets $X\\in{\\cal S}$ and $Y\\subseteq E\\setminus\nX$ such that $X\\cup Y\\in{\\cal I}$, there is a set $\\widehat{X}\\in\\widehat{\\cal\nS}$ disjoint from $Y$ such that $\\widehat{X}\\cup Y\\in{\\cal I}$. Fomin et al.\n(Proc. ACM-SIAM Symposium on Discrete Algorithms, 2014) introduced a powerful\ntechnique for fast computation of representative families for uniform matroids.\nIn this paper, we show that this technique leads to a unified approach for\nsubstantially improving the running times of parameterized algorithms for some\nclassic problems. This includes, among others, $k$-Partial Cover, $k$-Internal\nOut-Branching, and Long Directed Cycle. Our approach exploits an interesting\ntradeoff between running time and the size of the representative families."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3609v1", 
    "other_authors": "Reut Levi, Dana Ron, Ronitt Rubinfeld", 
    "title": "Local Algorithms for Sparse Spanning Graphs", 
    "arxiv-id": "1402.3609v1", 
    "author": "Ronitt Rubinfeld", 
    "publish": "2014-02-14T21:36:27Z", 
    "summary": "We initiate the study of the problem of designing sublinear-time ({\\em\nlocal\\/}) algorithms that, given an edge $(u,v)$ in a connected graph\n$G=(V,E)$, decide whether $(u,v)$ belongs to a sparse spanning graph $G' =\n(V,E')$ of $G$. Namely, $G'$ should be connected and $|E'|$ should be upper\nbounded by $(1+\\epsilon)|V|$ for a given parameter $\\epsilon > 0$. To this end\nthe algorithms may query the incidence relation of the graph $G$, and we seek\nalgorithms whose query complexity and running time (per given edge $(u,v)$) is\nas small as possible. Such an algorithm may be randomized but (for a fixed\nchoice of its random coins) its decision on different edges in the graph should\nbe consistent with the same spanning graph $G'$ and independent of the order of\nqueries.\n  We first show that for general (bounded-degree) graphs, the query complexity\nof any such algorithm must be $\\Omega(\\sqrt{|V|})$. This lower bound holds for\ngraphs that have high expansion. We then turn to design and analyze algorithms\nboth for graphs with high expansion (obtaining a result that roughly matches\nthe lower bound) and for graphs that are (strongly) non-expanding (obtaining\nresults in which the complexity does not depend on $|V|$). The complexity of\nthe problem for graphs that do not fall into these two categories is left as an\nopen question."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3782v1", 
    "other_authors": "Eric Angel, Evripidis Bampis, Vincent Chau, Nguyen Kim Thang", 
    "title": "Throughput Maximization in Multiprocessor Speed-Scaling", 
    "arxiv-id": "1402.3782v1", 
    "author": "Nguyen Kim Thang", 
    "publish": "2014-02-16T10:15:47Z", 
    "summary": "We are given a set of $n$ jobs that have to be executed on a set of $m$\nspeed-scalable machines that can vary their speeds dynamically using the energy\nmodel introduced in [Yao et al., FOCS'95]. Every job $j$ is characterized by\nits release date $r_j$, its deadline $d_j$, its processing volume $p_{i,j}$ if\n$j$ is executed on machine $i$ and its weight $w_j$. We are also given a budget\nof energy $E$ and our objective is to maximize the weighted throughput, i.e.\nthe total weight of jobs that are completed between their respective release\ndates and deadlines. We propose a polynomial-time approximation algorithm where\nthe preemption of the jobs is allowed but not their migration. Our algorithm\nuses a primal-dual approach on a linearized version of a convex program with\nlinear constraints. Furthermore, we present two optimal algorithms for the\nnon-preemptive case where the number of machines is bounded by a fixed\nconstant. More specifically, we consider: {\\em (a)} the case of identical\nprocessing volumes, i.e. $p_{i,j}=p$ for every $i$ and $j$, for which we\npresent a polynomial-time algorithm for the unweighted version, which becomes a\npseudopolynomial-time algorithm for the weighted throughput version, and {\\em\n(b)} the case of agreeable instances, i.e. for which $r_i \\le r_j$ if and only\nif $d_i \\le d_j$, for which we present a pseudopolynomial-time algorithm. Both\nalgorithms are based on a discretization of the problem and the use of dynamic\nprogramming."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3796v3", 
    "other_authors": "Guy Even, Moti Medina, Dana Ron", 
    "title": "Best of Two Local Models: Local Centralized and Local Distributed   Algorithms", 
    "arxiv-id": "1402.3796v3", 
    "author": "Dana Ron", 
    "publish": "2014-02-16T13:09:06Z", 
    "summary": "We consider two models of computation: centralized local algorithms and local\ndistributed algorithms. Algorithms in one model are adapted to the other model\nto obtain improved algorithms.\n  Distributed vertex coloring is employed to design improved centralized local\nalgorithms for: maximal independent set, maximal matching, and an approximation\nscheme for maximum (weighted) matching over bounded degree graphs. The\nimprovement is threefold: the algorithms are deterministic, stateless, and the\nnumber of probes grows polynomially in $\\log^* n$, where $n$ is the number of\nvertices of the input graph.\n  The recursive centralized local improvement technique by Nguyen and\nOnak~\\cite{onak2008} is employed to obtain an improved distributed\napproximation scheme for maximum (weighted) matching. The improvement is\ntwofold: we reduce the number of rounds from $O(\\log n)$ to $O(\\log^*n)$ for a\nwide range of instances and, our algorithms are deterministic rather than\nrandomized."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3851v2", 
    "other_authors": "Ioannis Koutis", 
    "title": "Simple parallel and distributed algorithms for spectral graph   sparsification", 
    "arxiv-id": "1402.3851v2", 
    "author": "Ioannis Koutis", 
    "publish": "2014-02-16T22:28:54Z", 
    "summary": "We describe a simple algorithm for spectral graph sparsification, based on\niterative computations of weighted spanners and uniform sampling. Leveraging\nthe algorithms of Baswana and Sen for computing spanners, we obtain the first\ndistributed spectral sparsification algorithm. We also obtain a parallel\nalgorithm with improved work and time guarantees. Combining this algorithm with\nthe parallel framework of Peng and Spielman for solving symmetric diagonally\ndominant linear systems, we get a parallel solver which is much closer to being\npractical and significantly more efficient in terms of the total work."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.3909v1", 
    "other_authors": "Fedor V. Fomin, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh", 
    "title": "Representative Sets of Product Families", 
    "arxiv-id": "1402.3909v1", 
    "author": "Saket Saurabh", 
    "publish": "2014-02-17T06:55:42Z", 
    "summary": "A subfamily ${\\cal F}'$ of a set family ${\\cal F}$ is said to $q$-{\\em\nrepresent} ${\\cal F}$ if for every $A \\in {\\cal F}$ and $B$ of size $q$ such\nthat $A \\cap B = \\emptyset$ there exists a set $A' \\in {\\cal F}'$ such that $A'\n\\cap B = \\emptyset$. In this paper, we consider the efficient computation of\n$q$-representative sets for {\\em product} families ${\\cal F}$. A family ${\\cal\nF}$ is a product family if there exist families ${\\cal A}$ and ${\\cal B}$ such\nthat ${\\cal F} = \\{A \\cup B~:~A \\in {\\cal A}, B \\in {\\cal B}, A \\cap B =\n\\emptyset\\}$. Our main technical contribution is an algorithm which given\n${\\cal A}$, ${\\cal B}$ and $q$ computes a $q$-representative family ${\\cal F}'$\nof ${\\cal F}$. The running time of our algorithm is sublinear in $|{\\cal F}|$\nfor many choices of ${\\cal A}$, ${\\cal B}$ and $q$ which occur naturally in\nseveral dynamic programming algorithms. We also give an algorithm for the\ncomputation of $q$-representative sets for product families ${\\cal F}$ in the\nmore general setting where $q$-representation also involves independence in a\nmatroid in addition to disjointness. This algorithm considerably outperforms\nthe naive approach where one first computes ${\\cal F}$ from ${\\cal A}$ and\n${\\cal B}$, and then computes the $q$-representative family ${\\cal F}'$ from\n${\\cal F}$.\n  We give two applications of our new algorithms for computing\n$q$-representative sets for product families. The first is a\n$3.8408^{k}n^{O(1)}$ deterministic algorithm for the Multilinear Monomial\nDetection ($k$-MlD) problem. The second is a significant improvement of\ndeterministic dynamic programming algorithms for \"connectivity problems\" on\ngraphs of bounded treewidth."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.4037v2", 
    "other_authors": "Sampath Kannan, Claire Mathieu, Hang Zhou", 
    "title": "Near-Linear Query Complexity for Graph Inference", 
    "arxiv-id": "1402.4037v2", 
    "author": "Hang Zhou", 
    "publish": "2014-02-17T15:44:34Z", 
    "summary": "How efficiently can we find an unknown graph using distance or shortest path\nqueries between its vertices? Let $G = (V,E)$ be an unweighted, connected graph\nof bounded degree. The edge set $E$ is initially unknown, and the graph can be\naccessed using a \\emph{distance oracle}, which receives a pair of vertices\n$(u,v)$ and returns the distance between $u$ and $v$. In the\n\\emph{verification} problem, we are given a hypothetical graph $\\hat G =\n(V,\\hat E)$ and want to check whether $G$ is equal to $\\hat G$. We analyze a\nnatural greedy algorithm and prove that it uses $n^{1+o(1)}$ distance queries.\nIn the more difficult \\emph{reconstruction} problem, $\\hat G$ is not given, and\nthe goal is to find the graph $G$. If the graph can be accessed using a\n\\emph{shortest path oracle}, which returns not just the distance but an actual\nshortest path between $u$ and $v$, we show that extending the idea of greedy\ngives a reconstruction algorithm that uses $n^{1+o(1)}$ shortest path queries.\nWhen the graph has bounded treewidth, we further bound the query complexity of\nthe greedy algorithms for both problems by $\\tilde O(n)$. When the graph is\nchordal, we provide a randomized algorithm for reconstruction using $\\tilde\nO(n)$ distance queries."
},{
    "category": "cs.DS", 
    "doi": "10.1109/TAES.2014.120395", 
    "link": "http://arxiv.org/pdf/1402.4111v2", 
    "other_authors": "Vincent Cohen-Addad, Zhentao Li, Claire Mathieu, Ioannis Millis", 
    "title": "Energy-efficient algorithms for non-preemptive speed-scaling", 
    "arxiv-id": "1402.4111v2", 
    "author": "Ioannis Millis", 
    "publish": "2014-02-17T20:23:04Z", 
    "summary": "We improve complexity bounds for energy-efficient speed scheduling problems\nfor both the single processor and multi-processor cases. Energy conservation\nhas become a major concern, so revisiting traditional scheduling problems to\ntake into account the energy consumption has been part of the agenda of the\nscheduling community for the past few years.\n  We consider the energy minimizing speed scaling problem introduced by Yao et\nal. where we wish to schedule a set of jobs, each with a release date, deadline\nand work volume, on a set of identical processors. The processors may change\nspeed as a function of time and the energy they consume is the $\\alpha$th power\nof its speed. The objective is then to find a feasible schedule which minimizes\nthe total energy used.\n  We show that in the setting with an arbitrary number of processors where all\nwork volumes are equal, there is a $2(1+\\varepsilon)(5(1+\\varepsilon))^{\\alpha\n-1}\\tilde{B}_{\\alpha}=O_{\\alpha}(1)$ approximation algorithm, where\n$\\tilde{B}_{\\alpha}$ is the generalized Bell number. This is the first constant\nfactor algorithm for this problem. This algorithm extends to general unequal\nprocessor-dependent work volumes, up to losing a factor of\n$(\\frac{(1+r)r}{2})^{\\alpha}$ in the approximation, where $r$ is the maximum\nratio between two work volumes. We then show this latter problem is APX-hard,\neven in the special case when all release dates and deadlines are equal and $r$\nis 4.\n  In the single processor case, we introduce a new linear programming\nformulation of speed scaling and prove that its integrality gap is at most\n$12^{\\alpha -1}$. As a corollary, we obtain a $(12(1+\\varepsilon))^{\\alpha -1}$\napproximation algorithm where there is a single processor, improving on the\nprevious best bound of $2^{\\alpha-1}(1+\\varepsilon)^{\\alpha}\\tilde{B}_{\\alpha}$\nwhen $\\alpha \\ge 25$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10951-015-0436-y", 
    "link": "http://arxiv.org/pdf/1402.4178v2", 
    "other_authors": "Enrico Angelelli, Thomas Kalinowski, Reena Kapoor, Martin W. P. Savelsbergh", 
    "title": "A reclaimer scheduling problem arising in coal stockyard management", 
    "arxiv-id": "1402.4178v2", 
    "author": "Martin W. P. Savelsbergh", 
    "publish": "2014-02-17T23:23:05Z", 
    "summary": "We study a number of variants of an abstract scheduling problem inspired by\nthe scheduling of reclaimers in the stockyard of a coal export terminal. We\nanalyze the complexity of each of the variants, providing complexity proofs for\nsome and polynomial algorithms for others. For one, especially interesting\nvariant, we also develop a constant factor approximation algorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10951-015-0436-y", 
    "link": "http://arxiv.org/pdf/1402.4370v1", 
    "other_authors": "Pinyan Lu, Menghui Wang, Chihao Zhang", 
    "title": "FPTAS for Weighted Fibonacci Gates and Its Applications", 
    "arxiv-id": "1402.4370v1", 
    "author": "Chihao Zhang", 
    "publish": "2014-02-18T15:15:37Z", 
    "summary": "Fibonacci gate problems have severed as computation primitives to solve other\nproblems by holographic algorithm and play an important role in the dichotomy\nof exact counting for Holant and CSP frameworks. We generalize them to weighted\ncases and allow each vertex function to have different parameters, which is a\nmuch boarder family and #P-hard for exactly counting. We design a fully\npolynomial-time approximation scheme (FPTAS) for this generalization by\ncorrelation decay technique. This is the first deterministic FPTAS for\napproximate counting in the general Holant framework without a degree bound. We\nalso formally introduce holographic reduction in the study of approximate\ncounting and these weighted Fibonacci gate problems serve as computation\nprimitives for approximate counting. Under holographic reduction, we obtain\nFPTAS for other Holant problems and spin problems. One important application is\ndeveloping an FPTAS for a large range of ferromagnetic two-state spin systems.\nThis is the first deterministic FPTAS in the ferromagnetic range for two-state\nspin systems without a degree bound. Besides these algorithms, we also develop\nseveral new tools and techniques to establish the correlation decay property,\nwhich are applicable in other problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-18263-6_12", 
    "link": "http://arxiv.org/pdf/1402.4722v5", 
    "other_authors": "Guilherme D. da Fonseca, Vin\u00edcius G. Pereira de S\u00e1, Celina M. H. de Figueiredo", 
    "title": "Shifting coresets: obtaining linear-time approximations for unit disk   graphs and other geometric intersection graphs", 
    "arxiv-id": "1402.4722v5", 
    "author": "Celina M. H. de Figueiredo", 
    "publish": "2014-02-19T16:30:04Z", 
    "summary": "Numerous approximation algorithms for problems on unit disk graphs have been\nproposed in the literature, exhibiting a sharp trade-off between running times\nand approximation ratios. We introduce a variation of the known shifting\nstrategy that allows us to obtain linear-time constant-factor approximation\nalgorithms for such problems. To illustrate the applicability of the proposed\nvariation, we obtain results for three well-known optimization problems. Among\nsuch results, the proposed method yields linear-time (4+eps)-approximation for\nthe maximum-weight independent set and the minimum dominating set of unit disk\ngraphs, thus bringing significant performance improvements when compared to\nprevious algorithms that achieve the same approximation ratios. Finally, we use\naxis-aligned rectangles to illustrate that the same method may be used to\nderive linear-time approximations for problems on other geometric intersection\ngraph classes."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-18263-6_12", 
    "link": "http://arxiv.org/pdf/1402.4994v2", 
    "other_authors": "Fabian Fuchs, Dorothea Wagner", 
    "title": "Arbitrary Transmission Power in the SINR Model: Local Broadcasting,   Coloring and MIS", 
    "arxiv-id": "1402.4994v2", 
    "author": "Dorothea Wagner", 
    "publish": "2014-02-20T13:37:45Z", 
    "summary": "In the light of energy conservation and the expansion of existing networks,\nwireless networks face the challenge of nodes with heterogeneous transmission\npower. However, for more realistic models of wireless communication only few\nalgorithmic results are known. In this paper we consider nodes with arbitrary,\npossibly variable, transmission power in the so-called physical or SINR model.\nOur first result is a bound on the probabilistic interference from all\nsimultaneously transmitting nodes on receivers. This result implies that\ncurrent local broadcasting algorithms can be generalized to the case of\nnon-uniform transmission power with minor changes. The algorithms run in\n$\\O(\\Gamma^{2} \\Delta \\log n)$ time slots if the maximal degree $\\Delta$ is\nknown, and $\\O((\\Delta + \\log n)\\Gamma^{2} \\log n)$ otherwise, where $\\Gamma$\nis the ratio between the maximal and the minimal transmission range. The broad\napplicability of our result on bounding the interference is further\nhighlighted, by generalizing a distributed coloring algorithm to this setting."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-18263-6_12", 
    "link": "http://arxiv.org/pdf/1402.5492v2", 
    "other_authors": "Pooya Davoodi, Jeremy T. Fineman, John Iacono, \u00d6zg\u00fcr \u00d6zkan", 
    "title": "Cache-Oblivious Persistence", 
    "arxiv-id": "1402.5492v2", 
    "author": "\u00d6zg\u00fcr \u00d6zkan", 
    "publish": "2014-02-22T08:08:23Z", 
    "summary": "Partial persistence is a general transformation that takes a data structure\nand allows queries to be executed on any past state of the structure. The\ncache-oblivious model is the leading model of a modern multi-level memory\nhierarchy.We present the first general transformation for making\ncache-oblivious model data structures partially persistent."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-18263-6_12", 
    "link": "http://arxiv.org/pdf/1402.5613v1", 
    "other_authors": "Bo Peng, Zhipeng Lu, T. C. E. Cheng", 
    "title": "A Tabu Search/Path Relinking Algorithm to Solve the Job Shop Scheduling   Problem", 
    "arxiv-id": "1402.5613v1", 
    "author": "T. C. E. Cheng", 
    "publish": "2014-02-23T14:46:04Z", 
    "summary": "We present an algorithm that incorporates a tabu search procedure into the\nframework of path relinking to tackle the job shop scheduling problem (JSP).\nThis tabu search/path relinking (TS/PR) algorithm comprises several\ndistinguishing features, such as a specific relinking procedure and a reference\nsolution determination method. To test the performance of TS/PR, we apply it to\ntackle almost all of the benchmark JSP instances available in the literature.\nThe test results show that TS/PR obtains competitive results compared with\nstate-of-the-art algorithms for JSP in the literature, demonstrating its\nefficacy in terms of both solution quality and computational efficiency. In\nparticular, TS/PR is able to improve the upper bounds for 49 out of the 205\ntested instances and it solves a challenging instance that has remained\nunsolved for over 20 years."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1402.5769v1", 
    "other_authors": "Tomomi Matsui, Noriyoshi Sukegawa, Atsushi Miyauchi", 
    "title": "Fractional programming formulation for the vertex coloring problem", 
    "arxiv-id": "1402.5769v1", 
    "author": "Atsushi Miyauchi", 
    "publish": "2014-02-24T10:01:07Z", 
    "summary": "We devise a new formulation for the vertex coloring problem. Different from\nother formulations, decision variables are associated with the pairs of\nvertices. Consequently, colors will be distinguishable. Although the objective\nfunction is fractional, it can be replaced by a piece-wise linear convex\nfunction. Numerical experiments show that our formulation has significantly\ngood performance for dense graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.0217v1", 
    "other_authors": "Neal E. Young", 
    "title": "A Bound on the Sum of Weighted Pairwise Distances of Points Constrained   to Balls", 
    "arxiv-id": "1007.0217v1", 
    "author": "Neal E. Young", 
    "publish": "2010-07-01T17:09:00Z", 
    "summary": "We consider the problem of choosing Euclidean points to maximize the sum of\ntheir weighted pairwise distances, when each point is constrained to a ball\ncentered at the origin. We derive a dual minimization problem and show strong\nduality holds (i.e., the resulting upper bound is tight) when some locally\noptimal configuration of points is affinely independent. We sketch a polynomial\ntime algorithm for finding a near-optimal set of points."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.0372v1", 
    "other_authors": "Benjamin Doerr, Marvin K\u00fcnnemann, Magnus Wahlstr\u00f6m", 
    "title": "Randomized Rounding for Routing and Covering Problems: Experiments and   Improvements", 
    "arxiv-id": "1007.0372v1", 
    "author": "Magnus Wahlstr\u00f6m", 
    "publish": "2010-07-02T14:22:26Z", 
    "summary": "Following previous theoretical work by Srinivasan (FOCS 2001) and the first\nauthor (STACS 2006) and a first experimental evaluation on random instances\n(ALENEX 2009), we investigate how the recently developed different approaches\nto generate randomized roundings satisfying disjoint cardinality constraints\nbehave when used in two classical algorithmic problems, namely low-congestion\nrouting in networks and max-coverage problems in hypergraphs.\n  We generally find that all randomized rounding algorithms work well, much\nbetter than what is guaranteed by existing theoretical work. The derandomized\nversions produce again significantly better rounding errors, with running times\nstill negligible compared to the one for solving the corresponding LP. It thus\nseems worth preferring them over the randomized variants.\n  The data created in these experiments lets us propose and investigate the\nfollowing new ideas. For the low-congestion routing problems, we suggest to\nsolve a second LP, which yields the same congestion, but aims at producing a\nsolution that is easier to round. Experiments show that this reduces the\nrounding errors considerably, both in combination with randomized and\nderandomized rounding.\n  For the max-coverage instances, we generally observe that the greedy\nheuristics also performs very good. We develop a strengthened method of\nderandomized rounding, and a simple greedy/rounding hybrid approach using\ngreedy and LP-based rounding elements, and observe that both these improvements\nyield again better solutions than both earlier approaches on their own.\n  For unit disk max-domination, we also develop a PTAS. Contrary to all other\nalgorithms investigated, it performs not much better in experiments than in\ntheory; thus, unless extremely good solutions are to be obtained with huge\ncomputational resources, greedy, LP-based rounding or hybrid approaches are\npreferable."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.0501v1", 
    "other_authors": "Glenn Langford", 
    "title": "An Improved Neighbourhood for the Traveling Tournament Problem", 
    "arxiv-id": "1007.0501v1", 
    "author": "Glenn Langford", 
    "publish": "2010-07-03T16:17:35Z", 
    "summary": "The Traveling Tournament Problem (TTP) is a challenging combinatorial\noptimization problem that has attracted the interest of researchers around the\nworld. This paper proposes an improved search neighbourhood for the TTP that\nhas been tested in a simulated annealing context. The neighbourhood encompasses\nboth feasible and infeasible schedules, and can be generated efficiently. For\nthe largest TTP challenge problems with up to 40 teams, solutions found using\nthis neighbourhood are the best currently known, and for smaller problems with\n10 teams, three solutions found were subsequently proven optimal."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.1161v1", 
    "other_authors": "Andreas Bj\u00f6rklund, Thore Husfeldt, Petteri Kaski, Mikko Koivisto", 
    "title": "Narrow sieves for parameterized paths and packings", 
    "arxiv-id": "1007.1161v1", 
    "author": "Mikko Koivisto", 
    "publish": "2010-07-07T15:08:09Z", 
    "summary": "We present randomized algorithms for some well-studied, hard combinatorial\nproblems: the k-path problem, the p-packing of q-sets problem, and the\nq-dimensional p-matching problem. Our algorithms solve these problems with high\nprobability in time exponential only in the parameter (k, p, q) and using\npolynomial space; the constant bases of the exponentials are significantly\nsmaller than in previous works. For example, for the k-path problem the\nimprovement is from 2 to 1.66. We also show how to detect if a d-regular graph\nadmits an edge coloring with $d$ colors in time within a polynomial factor of\nO(2^{(d-1)n/2}).\n  Our techniques build upon and generalize some recently published ideas by I.\nKoutis (ICALP 2009), R. Williams (IPL 2009), and A. Bj\\\"orklund (STACS 2010,\nFOCS 2010)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.1166v2", 
    "other_authors": "Konstantin Kutzkov, Dominik Scheder", 
    "title": "Using CSP To Improve Deterministic 3-SAT", 
    "arxiv-id": "1007.1166v2", 
    "author": "Dominik Scheder", 
    "publish": "2010-07-07T15:25:27Z", 
    "summary": "We show how one can use certain deterministic algorithms for higher-value\nconstraint satisfaction problems (CSPs) to speed up deterministic local search\nfor 3-SAT. This way, we improve the deterministic worst-case running time for\n3-SAT to O(1.439^n)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2014.06.010", 
    "link": "http://arxiv.org/pdf/1007.1271v1", 
    "other_authors": "Gagan Aggarwal, Gagan Goel, Chinmay Karande, Aranyak Mehta", 
    "title": "Online Vertex-Weighted Bipartite Matching and Single-bid Budgeted   Allocations", 
    "arxiv-id": "1007.1271v1", 
    "author": "Aranyak Mehta", 
    "publish": "2010-07-08T01:04:12Z", 
    "summary": "We study the following vertex-weighted online bipartite matching problem:\n$G(U, V, E)$ is a bipartite graph. The vertices in $U$ have weights and are\nknown ahead of time, while the vertices in $V$ arrive online in an arbitrary\norder and have to be matched upon arrival. The goal is to maximize the sum of\nweights of the matched vertices in $U$. When all the weights are equal, this\nreduces to the classic \\emph{online bipartite matching} problem for which Karp,\nVazirani and Vazirani gave an optimal $\\left(1-\\frac{1}{e}\\right)$-competitive\nalgorithm in their seminal work~\\cite{KVV90}. Our main result is an optimal\n$\\left(1-\\frac{1}{e}\\right)$-competitive randomized algorithm for general\nvertex weights. We use \\emph{random perturbations} of weights by appropriately\nchosen multiplicative factors. Our solution constitutes the first known\ngeneralization of the algorithm in~\\cite{KVV90} in this model and provides new\ninsights into the role of randomization in online allocation problems. It also\neffectively solves the problem of \\emph{online budgeted allocations}\n\\cite{MSVV05} in the case when an agent makes the same bid for any desired\nitem, even if the bid is comparable to his budget - complementing the results\nof \\cite{MSVV05, BJN07} which apply when the bids are much smaller than the\nbudgets."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00291", 
    "link": "http://arxiv.org/pdf/1007.1484v1", 
    "other_authors": "Erin Chambers, David Eppstein", 
    "title": "Flows in One-Crossing-Minor-Free Graphs", 
    "arxiv-id": "1007.1484v1", 
    "author": "David Eppstein", 
    "publish": "2010-07-08T23:24:59Z", 
    "summary": "We study the maximum flow problem in directed H-minor-free graphs where H can\nbe drawn in the plane with one crossing. If a structural decomposition of the\ngraph as a clique-sum of planar graphs and graphs of constant complexity is\ngiven, we show that a maximum flow can be computed in O(n log n) time. In\nparticular, maximum flows in directed K_{3,3}-minor-free graphs and directed\nK_5-minor-free graphs can be computed in O(n log n) time without additional\nassumptions."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00291", 
    "link": "http://arxiv.org/pdf/1007.1535v3", 
    "other_authors": "Marcin Bienkowski", 
    "title": "An Optimal Lower Bound for Buffer Management in Multi-Queue Switches", 
    "arxiv-id": "1007.1535v3", 
    "author": "Marcin Bienkowski", 
    "publish": "2010-07-09T09:05:38Z", 
    "summary": "In the online packet buffering problem (also known as the unweighted FIFO\nvariant of buffer management), we focus on a single network packet switching\ndevice with several input ports and one output port. This device forwards\nunit-size, unit-value packets from input ports to the output port. Buffers\nattached to input ports may accumulate incoming packets for later transmission;\nif they cannot accommodate all incoming packets, their excess is lost. A packet\nbuffering algorithm has to choose from which buffers to transmit packets in\norder to minimize the number of lost packets and thus maximize the throughput.\n  We present a tight lower bound of e/(e-1) ~ 1.582 on the competitive ratio of\nthe throughput maximization, which holds even for fractional or randomized\nalgorithms. This improves the previously best known lower bound of 1.4659 and\nmatches the performance of the algorithm Random Schedule. Our result\ncontradicts the claimed performance of the algorithm Random Permutation; we\npoint out a flaw in its original analysis."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00291", 
    "link": "http://arxiv.org/pdf/1007.1632v1", 
    "other_authors": "Shayan Oveis Gharan, Jan Vondr\u00e1k", 
    "title": "Submodular Maximization by Simulated Annealing", 
    "arxiv-id": "1007.1632v1", 
    "author": "Jan Vondr\u00e1k", 
    "publish": "2010-07-09T17:31:43Z", 
    "summary": "We consider the problem of maximizing a nonnegative (possibly non-monotone)\nsubmodular set function with or without constraints. Feige et al. [FOCS'07]\nshowed a 2/5-approximation for the unconstrained problem and also proved that\nno approximation better than 1/2 is possible in the value oracle model.\nConstant-factor approximation was also given for submodular maximization\nsubject to a matroid independence constraint (a factor of 0.309 Vondrak\n[FOCS'09]) and for submodular maximization subject to a matroid base\nconstraint, provided that the fractional base packing number is at least 2 (a\n1/4-approximation, Vondrak [FOCS'09]).\n  In this paper, we propose a new algorithm for submodular maximization which\nis based on the idea of {\\em simulated annealing}. We prove that this algorithm\nachieves improved approximation for two problems: a 0.41-approximation for\nunconstrained submodular maximization, and a 0.325-approximation for submodular\nmaximization subject to a matroid independence constraint.\n  On the hardness side, we show that in the value oracle model it is impossible\nto achieve a 0.478-approximation for submodular maximization subject to a\nmatroid independence constraint, or a 0.394-approximation subject to a matroid\nbase constraint in matroids with two disjoint bases. Even for the special case\nof cardinality constraint, we prove it is impossible to achieve a\n0.491-approximation. (Previously it was conceivable that a 1/2-approximation\nexists for these problems.) It is still an open question whether a\n1/2-approximation is possible for unconstrained submodular maximization."
},{
    "category": "cs.DS", 
    "doi": "10.7155/jgaa.00291", 
    "link": "http://arxiv.org/pdf/1007.1673v2", 
    "other_authors": "Vahideh H. Manshadi, Shayan Oveis Gharan, Amin Saberi", 
    "title": "Online Stochastic Matching: Online Actions Based on Offline Statistics", 
    "arxiv-id": "1007.1673v2", 
    "author": "Amin Saberi", 
    "publish": "2010-07-09T20:40:42Z", 
    "summary": "We consider the online stochastic matching problem proposed by Feldman et al.\n[FMMM09] as a model of display ad allocation. We are given a bipartite graph;\none side of the graph corresponds to a fixed set of bins and the other side\nrepresents the set of possible ball types. At each time step, a ball is sampled\nindependently from the given distribution and it needs to be matched upon its\narrival to an empty bin. The goal is to maximize the number of allocations.\n  We present an online algorithm for this problem with a competitive ratio of\n0.702. Before our result, algorithms with a competitive ratio better than\n$1-1/e$ were known under the assumption that the expected number of arriving\nballs of each type is integral. A key idea of the algorithm is to collect\nstatistics about the decisions of the optimum offline solution using Monte\nCarlo sampling and use those statistics to guide the decisions of the online\nalgorithm. We also show that our algorithm achieves a competitive ratio of\n0.705 when the rates are integral.\n  On the hardness side, we prove that no online algorithm can have a\ncompetitive ratio better than 0.823 under the known distribution model (and\nhenceforth under the permutation model). This improves upon the 5/6 hardness\nresult proved by Goel and Mehta \\cite{GM08} for the permutation model."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2140v1", 
    "other_authors": "Michel X. Goemans, Jos\u00e9 A. Soto", 
    "title": "Symmetric Submodular Function Minimization Under Hereditary Family   Constraints", 
    "arxiv-id": "1007.2140v1", 
    "author": "Jos\u00e9 A. Soto", 
    "publish": "2010-07-13T16:46:51Z", 
    "summary": "We present an efficient algorithm to find non-empty minimizers of a symmetric\nsubmodular function over any family of sets closed under inclusion. This for\nexample includes families defined by a cardinality constraint, a knapsack\nconstraint, a matroid independence constraint, or any combination of such\nconstraints. Our algorithm make $O(n^3)$ oracle calls to the submodular\nfunction where $n$ is the cardinality of the ground set. In contrast, the\nproblem of minimizing a general submodular function under a cardinality\nconstraint is known to be inapproximable within $o(\\sqrt{n/\\log n})$ (Svitkina\nand Fleischer [2008]).\n  The algorithm is similar to an algorithm of Nagamochi and Ibaraki [1998] to\nfind all nontrivial inclusionwise minimal minimizers of a symmetric submodular\nfunction over a set of cardinality $n$ using $O(n^3)$ oracle calls. Their\nprocedure in turn is based on Queyranne's algorithm [1998] to minimize a\nsymmetric submodular"
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2152v1", 
    "other_authors": "Jos\u00e9 A. Soto", 
    "title": "Matroid Secretary Problem in the Random Assignment Model", 
    "arxiv-id": "1007.2152v1", 
    "author": "Jos\u00e9 A. Soto", 
    "publish": "2010-07-13T17:09:33Z", 
    "summary": "In the Matroid Secretary Problem, introduced by Babaioff et al. [SODA 2007],\nthe elements of a given matroid are presented to an online algorithm in random\norder. When an element is revealed, the algorithm learns its weight and decides\nwhether or not to select it under the restriction that the selected elements\nform an independent set in the matroid. The objective is to maximize the total\nweight of the chosen elements. In the most studied version of this problem, the\nalgorithm has no information about the weights beforehand. We refer to this as\nthe zero information model. In this paper we study a different model, also\nproposed by Babaioff et al., in which the relative order of the weights is\nrandom in the matroid. To be precise, in the random assignment model, an\nadversary selects a collection of weights that are randomly assigned to the\nelements of the matroid. Later, the elements are revealed to the algorithm in a\nrandom order independent of the assignment.\n  Our main result is the first constant competitive algorithm for the matroid\nsecretary problem in the random assignment model. This solves an open question\nof Babaioff et al. Our algorithm achieves a competitive ratio of $2e^2/(e-1)$.\nIt exploits the notion of principal partition of a matroid, its decomposition\ninto uniformly dense minors, and a $2e$-competitive algorithm for uniformly\ndense matroids we also develop. As additional results, we present simple\nconstant competitive algorithms in the zero information model for various\nclasses of matroids including cographic, low density and the case when every\nelement is in a small cocircuit. In the same model, we also give a\n$ke$-competitive algorithm for $k$-column sparse linear matroids, and a new\n$O(\\log r)$-competitive algorithm for general matroids of rank $r$ which only\nuses the relative order of the weights seen and not their numerical value, as\npreviously needed."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2216v1", 
    "other_authors": "Virginia Vassilevska Williams", 
    "title": "Faster Replacement Paths", 
    "arxiv-id": "1007.2216v1", 
    "author": "Virginia Vassilevska Williams", 
    "publish": "2010-07-13T22:06:16Z", 
    "summary": "The replacement paths problem for directed graphs is to find for given nodes\ns and t and every edge e on the shortest path between them, the shortest path\nbetween s and t which avoids e. For unweighted directed graphs on n vertices,\nthe best known algorithm runtime was \\tilde{O}(n^{2.5}) by Roditty and Zwick.\nFor graphs with integer weights in {-M,...,M}, Weimann and Yuster recently\nshowed that one can use fast matrix multiplication and solve the problem in\nO(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent\n\\omega of matrix multiplication is 2.\n  We improve both of these algorithms. Our new algorithm also relies on fast\nmatrix multiplication and runs in O(M n^{\\omega} polylog(n)) time if \\omega>2\nand O(n^{2+\\eps}) for any \\eps>0 if \\omega=2. Our result shows that, at least\nfor small integer weights, the replacement paths problem in directed graphs may\nbe easier than the related all pairs shortest paths problem in directed graphs,\nas the current best runtime for the latter is \\Omega(n^{2.5}) time even if\n\\omega=2."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2365v1", 
    "other_authors": "John Byers, Brent Heeringa, Michael Mitzenmacher, Georgios Zervas", 
    "title": "Heapable Sequences and Subsequences", 
    "arxiv-id": "1007.2365v1", 
    "author": "Georgios Zervas", 
    "publish": "2010-07-14T16:02:37Z", 
    "summary": "Let us call a sequence of numbers heapable if they can be sequentially\ninserted to form a binary tree with the heap property, where each insertion\nsubsequent to the first occurs at a leaf of the tree, i.e. below a previously\nplaced number. In this paper we consider a variety of problems related to\nheapable sequences and subsequences that do not appear to have been studied\npreviously. Our motivation for introducing these concepts is two-fold. First,\nsuch problems correspond to natural extensions of the well-known secretary\nproblem for hiring an organization with a hierarchical structure. Second, from\na purely combinatorial perspective, our problems are interesting variations on\nsimilar longest increasing subsequence problems, a problem paradigm that has\nled to many deep mathematical connections.\n  We provide several basic results. We obtain an efficient algorithm for\ndetermining the heapability of a sequence, and also prove that the question of\nwhether a sequence can be arranged in a complete binary heap is NP-hard.\nRegarding subsequences we show that, with high probability, the longest\nheapable subsequence of a random permutation of n numbers has length (1 - o(1))\nn, and a subsequence of length (1 - o(1)) n can in fact be found online with\nhigh probability. We similarly show that for a random permutation a subsequence\nthat yields a complete heap of size \\alpha n for a constant \\alpha can be found\nwith high probability. Our work highlights the interesting structure underlying\nthis class of subsequence problems, and we leave many further interesting\nvariations open for future work."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2503v1", 
    "other_authors": "Yossi Azar, Iftah Gamzu", 
    "title": "Ranking with Submodular Valuations", 
    "arxiv-id": "1007.2503v1", 
    "author": "Iftah Gamzu", 
    "publish": "2010-07-15T08:42:04Z", 
    "summary": "We study the problem of ranking with submodular valuations. An instance of\nthis problem consists of a ground set $[m]$, and a collection of $n$ monotone\nsubmodular set functions $f^1, \\ldots, f^n$, where each $f^i: 2^{[m]} \\to R_+$.\nAn additional ingredient of the input is a weight vector $w \\in R_+^n$. The\nobjective is to find a linear ordering of the ground set elements that\nminimizes the weighted cover time of the functions. The cover time of a\nfunction is the minimal number of elements in the prefix of the linear ordering\nthat form a set whose corresponding function value is greater than a unit\nthreshold value.\n  Our main contribution is an $O(\\ln(1 / \\epsilon))$-approximation algorithm\nfor the problem, where $\\epsilon$ is the smallest non-zero marginal value that\nany function may gain from some element. Our algorithm orders the elements\nusing an adaptive residual updates scheme, which may be of independent\ninterest. We also prove that the problem is $\\Omega(\\ln(1 / \\epsilon))$-hard to\napproximate, unless P = NP. This implies that the outcome of our algorithm is\noptimal up to constant factors."
},{
    "category": "cs.DS", 
    "doi": "10.1137/120891502", 
    "link": "http://arxiv.org/pdf/1007.2618v2", 
    "other_authors": "Bin Fu, Yunhui Fu", 
    "title": "Sublinear Time Motif Discovery from Multiple Sequences", 
    "arxiv-id": "1007.2618v2", 
    "author": "Yunhui Fu", 
    "publish": "2010-07-15T17:09:54Z", 
    "summary": "A natural probabilistic model for motif discovery has been used to\nexperimentally test the quality of motif discovery programs. In this model,\nthere are $k$ background sequences, and each character in a background sequence\nis a random character from an alphabet $\\Sigma$. A motif $G=g_1g_2...g_m$ is a\nstring of $m$ characters. Each background sequence is implanted a\nprobabilistically generated approximate copy of $G$. For a probabilistically\ngenerated approximate copy $b_1b_2...b_m$ of $G$, every character $b_i$ is\nprobabilistically generated such that the probability for $b_i\\neq g_i$ is at\nmost $\\alpha$. We develop three algorithms that under the probabilistic model\ncan find the implanted motif with high probability via a tradeoff between\ncomputational time and the probability of mutation. The methods developed in\nthis paper have been used in the software implementation. We observed some\nencouraging results that show improved performance for motif detection compared\nwith other softwares."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.2671v1", 
    "other_authors": "Artem Chebotko, Bin Fu", 
    "title": "XML Reconstruction View Selection in XML Databases: Complexity Analysis   and Approximation Scheme", 
    "arxiv-id": "1007.2671v1", 
    "author": "Bin Fu", 
    "publish": "2010-07-15T22:34:59Z", 
    "summary": "Query evaluation in an XML database requires reconstructing XML subtrees\nrooted at nodes found by an XML query. Since XML subtree reconstruction can be\nexpensive, one approach to improve query response time is to use reconstruction\nviews - materialized XML subtrees of an XML document, whose nodes are\nfrequently accessed by XML queries. For this approach to be efficient, the\nprincipal requirement is a framework for view selection. In this work, we are\nthe first to formalize and study the problem of XML reconstruction view\nselection. The input is a tree $T$, in which every node $i$ has a size $c_i$\nand profit $p_i$, and the size limitation $C$. The target is to find a subset\nof subtrees rooted at nodes $i_1,\\cdots, i_k$ respectively such that\n$c_{i_1}+\\cdots +c_{i_k}\\le C$, and $p_{i_1}+\\cdots +p_{i_k}$ is maximal.\nFurthermore, there is no overlap between any two subtrees selected in the\nsolution. We prove that this problem is NP-hard and present a fully\npolynomial-time approximation scheme (FPTAS) as a solution."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3036v2", 
    "other_authors": "Marek Adamczyk", 
    "title": "Greedy algorithm for stochastic matching is a 2-approximation", 
    "arxiv-id": "1007.3036v2", 
    "author": "Marek Adamczyk", 
    "publish": "2010-07-18T20:23:19Z", 
    "summary": "Motivated by applications in online dating and kidney exchange, the\nstochastic matching problem was introduced by Chen, Immorlica, Karlin, Mahdian\nand Rudra (2009). They have proven a 4-approximation of a simple greedy\nstrategy, but conjectured that it is in fact a 2-approximation. In this paper\nwe confirm this hypothesis."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3157v1", 
    "other_authors": "John Alexandris, Gregory Karagiorgos 'and' Ioannis Stavrakakis", 
    "title": "Enhanced Random Walk with Choice: An Empirical Study", 
    "arxiv-id": "1007.3157v1", 
    "author": "Gregory Karagiorgos 'and' Ioannis Stavrakakis", 
    "publish": "2010-07-19T14:35:50Z", 
    "summary": "The random walk with choice is a well known variation to the random walk that\nfirst selects a subset of $d$ neighbours nodes and then decides to move to the\nnode which maximizes the value of a certain metric; this metric captures the\nnumber of (past) visits of the walk to the node. In this paper we propose an\nenhancement to the random walk with choice by considering a new metric that\ncaptures not only the actual visits to a given node, but also the intensity of\nthe visits to the neighbourhood of the node. We compare the random walk with\nchoice with its enhanced counterpart. Simulation results show a significant\nimprovement in cover time, maximum node load and load balancing, mainly in\nrandom geometric graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3292v1", 
    "other_authors": "Yuichi Yoshida", 
    "title": "Lower Bounds on Query Complexity for Testing Bounded-Degree CSPs", 
    "arxiv-id": "1007.3292v1", 
    "author": "Yuichi Yoshida", 
    "publish": "2010-07-19T21:44:55Z", 
    "summary": "In this paper, we consider lower bounds on the query complexity for testing\nCSPs in the bounded-degree model.\n  First, for any ``symmetric'' predicate $P:{0,1}^{k} \\to {0,1}$ except \\equ\nwhere $k\\geq 3$, we show that every (randomized) algorithm that distinguishes\nsatisfiable instances of CSP(P) from instances $(|P^{-1}(0)|/2^k-\\epsilon)$-far\nfrom satisfiability requires $\\Omega(n^{1/2+\\delta})$ queries where $n$ is the\nnumber of variables and $\\delta>0$ is a constant that depends on $P$ and\n$\\epsilon$. This breaks a natural lower bound $\\Omega(n^{1/2})$, which is\nobtained by the birthday paradox. We also show that every one-sided error\ntester requires $\\Omega(n)$ queries for such $P$. These results are hereditary\nin the sense that the same results hold for any predicate $Q$ such that\n$P^{-1}(1) \\subseteq Q^{-1}(1)$. For EQU, we give a one-sided error tester\nwhose query complexity is $\\tilde{O}(n^{1/2})$. Also, for 2-XOR (or,\nequivalently E2LIN2), we show an $\\Omega(n^{1/2+\\delta})$ lower bound for\ndistinguishing instances between $\\epsilon$-close to and $(1/2-\\epsilon)$-far\nfrom satisfiability.\n  Next, for the general k-CSP over the binary domain, we show that every\nalgorithm that distinguishes satisfiable instances from instances\n$(1-2k/2^k-\\epsilon)$-far from satisfiability requires $\\Omega(n)$ queries. The\nmatching NP-hardness is not known, even assuming the Unique Games Conjecture or\nthe $d$-to-$1$ Conjecture. As a corollary, for Maximum Independent Set on\ngraphs with $n$ vertices and a degree bound $d$, we show that every\napproximation algorithm within a factor $d/\\poly\\log d$ and an additive error\nof $\\epsilon n$ requires $\\Omega(n)$ queries. Previously, only super-constant\nlower bounds were known."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3415v1", 
    "other_authors": "Yakov Nekrich", 
    "title": "Searching in Dynamic Catalogs on a Tree", 
    "arxiv-id": "1007.3415v1", 
    "author": "Yakov Nekrich", 
    "publish": "2010-07-20T13:28:50Z", 
    "summary": "In this paper we consider the following modification of the iterative search\nproblem. We are given a tree $T$, so that a dynamic catalog $C(v)$ is\nassociated with every tree node $v$. For any $x$ and for any node-to-root path\n$\\pi$ in $T$, we must find the predecessor of $x$ in $\\cup_{v\\in \\pi} C(v)$. We\npresent a linear space dynamic data structure that supports such queries in\n$O(t(n)+|\\pi|)$ time, where $t(n)$ is the time needed to search in one catalog\nand $|\\pi|$ denotes the number of nodes on path $\\pi$. We also consider the\nreporting variant of this problem, in which for any $x_1$, $x_2$ and for any\npath $\\pi'$ all elements of $\\cup_{v\\in \\pi'} (C(v)\\cap [x_1,x_2])$ must be\nreported; here $\\pi'$ denotes a path between an arbitrary node $v_0$ and its\nancestor $v_1$. We show that such queries can be answered in $O(t(n)+|\\pi'|+\nk)$ time, where $k$ is the number of elements in the answer. To illustrate\napplications of our technique, we describe the first dynamic data structures\nfor the stabbing-max problem, the horizontal point location problem, and the\northogonal line-segment intersection problem with optimal $O(\\log n/\\log \\log\nn)$ query time and poly-logarithmic update time."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3611v2", 
    "other_authors": "Jaroslaw Byrka, MohammadReza Ghodsi, Aravind Srinivasan", 
    "title": "LP-rounding algorithms for facility-location problems", 
    "arxiv-id": "1007.3611v2", 
    "author": "Aravind Srinivasan", 
    "publish": "2010-07-21T10:48:52Z", 
    "summary": "We study LP-rounding approximation algorithms for metric uncapacitated\nfacility-location problems. We first give a new analysis for the algorithm of\nChudak and Shmoys, which differs from the analysis of Byrka and Aardal in that\nnow we do not need any bound based on the solution to the dual LP program.\nBesides obtaining the optimal bifactor approximation as do Byrka and Aardal, we\ncan now also show that the algorithm with scaling parameter equaling 1.58 is,\nin fact, an 1.58-approximation algorithm. More importantly, we suggest an\napproach based on additional randomization and analyses such as ours, which\ncould achieve or approach the conjectured optimal 1.46...--approximation for\nthis basic problem.\n  Next, using essentially the same techniques, we obtain improved approximation\nalgorithms in the 2-stage stochastic variant of the problem, where we must open\na subset of facilities having only stochastic information about the future\ndemand from the clients. For this problem we obtain a 2.2975-approximation\nalgorithm in the standard setting, and a 2.4957-approximation in the more\nrestricted, per-scenario setting.\n  We then study robust fault-tolerant facility location, introduced by Chechik\nand Peleg: solutions here are designed to provide low connection cost in case\nof failure of up to $k$ facilities. Chechik and Peleg gave a 6.5-approximation\nalgorithm for $k=1$ and a ($7.5k + 1.5$)-approximation algorithm for general\n$k$. We improve this to an LP-rounding $(k+5+4/k)$-approximation algorithm. We\nalso observe that in case of oblivious failures the expected approximation\nratio can be reduced to $k + 1.5$, and that the integrality gap of the natural\nLP-relaxation of the problem is at least $k + 1$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17461-2_8", 
    "link": "http://arxiv.org/pdf/1007.3747v1", 
    "other_authors": "Benjamin Moseley", 
    "title": "Scheduling to Minimize Energy and Flow Time in Broadcast Scheduling", 
    "arxiv-id": "1007.3747v1", 
    "author": "Benjamin Moseley", 
    "publish": "2010-07-21T20:06:13Z", 
    "summary": "In this paper we initiate the study of minimizing power consumption in the\nbroadcast scheduling model. In this setting there is a wireless transmitter.\nOver time requests arrive at the transmitter for pages of information. Multiple\nrequests may be for the same page. When a page is transmitted, all requests for\nthat page receive the transmission simulteneously. The speed the transmitter\nsends data at can be dynamically scaled to conserve energy. We consider the\nproblem of minimizing flow time plus energy, the most popular scheduling metric\nconsidered in the standard scheduling model when the scheduler is energy aware.\nWe will assume that the power consumed is modeled by an arbitrary convex\nfunction. For this problem there is a $\\Omega(n)$ lower bound. Due to the lower\nbound, we consider the resource augmentation model of Gupta \\etal\n\\cite{GuptaKP10}. Using resource augmentation, we give a scalable algorithm.\nOur result also gives a scalable non-clairvoyant algorithm for minimizing\nweighted flow time plus energy in the standard scheduling model."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1007.4011v1", 
    "other_authors": "Sylvain Guillemot, Christophe Paul, Anthony Perez", 
    "title": "On the (non-)existence of polynomial kernels for Pl-free edge   modification problems", 
    "arxiv-id": "1007.4011v1", 
    "author": "Anthony Perez", 
    "publish": "2010-07-22T21:14:40Z", 
    "summary": "Given a graph G = (V,E) and an integer k, an edge modification problem for a\ngraph property P consists in deciding whether there exists a set of edges F of\nsize at most k such that the graph H = (V,E \\vartriangle F) satisfies the\nproperty P. In the P edge-completion problem, the set F of edges is constrained\nto be disjoint from E; in the P edge-deletion problem, F is a subset of E; no\nconstraint is imposed on F in the P edge-edition problem. A number of\noptimization problems can be expressed in terms of graph modification problems\nwhich have been extensively studied in the context of parameterized complexity.\nWhen parameterized by the size k of the edge set F, it has been proved that if\nP is an hereditary property characterized by a finite set of forbidden induced\nsubgraphs, then the three P edge-modification problems are FPT. It was then\nnatural to ask whether these problems also admit a polynomial size kernel.\nUsing recent lower bound techniques, Kratsch and Wahlstr\u007fom answered this\nquestion negatively. However, the problem remains open on many natural graph\nclasses characterized by forbidden induced subgraphs. Kratsch and Wahlstr\u007fom\nasked whether the result holds when the forbidden subgraphs are paths or cycles\nand pointed out that the problem is already open in the case of P4-free graphs\n(i.e. cographs). This paper provides positive and negative results in that line\nof research. We prove that parameterized cograph edge modification problems\nhave cubic vertex kernels whereas polynomial kernels are unlikely to exist for\nthe Pl-free and Cl-free edge-deletion problems for large enough l."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1007.4191v1", 
    "other_authors": "Daniel M. Kane, Jelani Nelson, Ely Porat, David P. Woodruff", 
    "title": "Fast Moment Estimation in Data Streams in Optimal Space", 
    "arxiv-id": "1007.4191v1", 
    "author": "David P. Woodruff", 
    "publish": "2010-07-23T19:31:06Z", 
    "summary": "We give a space-optimal algorithm with update time\nO(log^2(1/eps)loglog(1/eps)) for (1+eps)-approximating the pth frequency\nmoment, 0 < p < 2, of a length-n vector updated in a data stream. This provides\na nearly exponential improvement in the update time complexity over the\nprevious space-optimal algorithm of [Kane-Nelson-Woodruff, SODA 2010], which\nhad update time Omega(1/eps^2)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1007.5318v3", 
    "other_authors": "Vladimir Pestov", 
    "title": "Intrinsic Dimensionality", 
    "arxiv-id": "1007.5318v3", 
    "author": "Vladimir Pestov", 
    "publish": "2010-07-29T20:07:49Z", 
    "summary": "This entry for the SIGSPATIAL Special July 2010 issue on Similarity Searching\nin Metric Spaces discusses the notion of intrinsic dimensionality of data in\nthe context of similarity search."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1007.5406v1", 
    "other_authors": "Markus Lohrey, Sebastian Maneth, Roy Mennicke", 
    "title": "Tree structure compression with RePair", 
    "arxiv-id": "1007.5406v1", 
    "author": "Roy Mennicke", 
    "publish": "2010-07-30T10:14:21Z", 
    "summary": "In this work we introduce a new linear time compression algorithm, called\n\"Re-pair for Trees\", which compresses ranked ordered trees using linear\nstraight-line context-free tree grammars. Such grammars generalize\nstraight-line context-free string grammars and allow basic tree operations,\nlike traversal along edges, to be executed without prior decompression. Our\nalgorithm can be considered as a generalization of the \"Re-pair\" algorithm\ndeveloped by N. Jesper Larsson and Alistair Moffat in 2000. The latter\nalgorithm is a dictionary-based compression algorithm for strings. We also\nintroduce a succinct coding which is specialized in further compressing the\ngrammars generated by our algorithm. This is accomplished without loosing the\nability do directly execute queries on this compressed representation of the\ninput tree. Finally, we compare the grammars and output files generated by a\nprototype of the Re-pair for Trees algorithm with those of similar compression\nalgorithms. The obtained results show that that our algorithm outperforms its\ncompetitors in terms of compression ratio, runtime and memory usage."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1007.5475v1", 
    "other_authors": "Christian Gla\u00dfer, Christian Reitwie\u00dfner, Maximilian Witek", 
    "title": "Balanced Combinations of Solutions in Multi-Objective Optimization", 
    "arxiv-id": "1007.5475v1", 
    "author": "Maximilian Witek", 
    "publish": "2010-07-30T15:25:17Z", 
    "summary": "For every list of integers x_1, ..., x_m there is some j such that x_1 + ...\n+ x_j - x_{j+1} - ... - x_m \\approx 0. So the list can be nearly balanced and\nfor this we only need one alternation between addition and subtraction. But\nwhat if the x_i are k-dimensional integer vectors? Using results from\ntopological degree theory we show that balancing is still possible, now with k\nalternations.\n  This result is useful in multi-objective optimization, as it allows a\npolynomial-time computable balance of two alternatives with conflicting costs.\nThe application to two multi-objective optimization problems yields the\nfollowing results:\n  - A randomized 1/2-approximation for multi-objective maximum asymmetric\ntraveling salesman, which improves and simplifies the best known approximation\nfor this problem.\n  - A deterministic 1/2-approximation for multi-objective maximum weighted\nsatisfiability."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1104.0733v1", 
    "other_authors": "Wei Ren, Qing Zhao", 
    "title": "A Note on: `Algorithms for Connected Set Cover Problem and   Fault-Tolerant Connected Set Cover Problem'", 
    "arxiv-id": "1104.0733v1", 
    "author": "Qing Zhao", 
    "publish": "2011-04-05T04:57:44Z", 
    "summary": "A flaw in the greedy approximation algorithm proposed by Zhang et al. for\nminimum connected set cover problem is corrected, and a stronger result on the\napproximation ratio of the modified greedy algorithm is established. The\nresults are now consistent with the existing results on connected dominating\nset problem which is a special case of the minimum connected set cover problem."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-17493-3_15", 
    "link": "http://arxiv.org/pdf/1104.0739v1", 
    "other_authors": "Ran Gelles, Amit Sahai", 
    "title": "Potent Tree Codes and their applications: Coding for Interactive   Communication, revisited", 
    "arxiv-id": "1104.0739v1", 
    "author": "Amit Sahai", 
    "publish": "2011-04-05T06:10:01Z", 
    "summary": "We study the fundamental problem of reliable interactive communication over a\nnoisy channel. In a breakthrough sequence of papers published in 1992 and 1993,\nSchulman gave non-constructive proofs of the existence of general methods to\nemulate any two-party interactive protocol such that: (1) the emulation\nprotocol takes a constant-factor longer than the original protocol, and (2) if\nthe emulation protocol is executed over a noisy channel, then the probability\nthat the emulation protocol fails is exponentially small in the total length of\nthe protocol. Unfortunately, Schulman's emulation procedures either only work\nin a model with a large amount of shared randomness, or are non-constructive in\nthat they rely on the existence of good tree codes. The only known proofs of\nthe existence of good tree codes are non-constructive, and finding an explicit\nconstruction remains an important open problem. Indeed, randomly generated tree\ncodes are not good tree codes with overwhelming probability.\n  In this work, we revisit the problem of reliable interactive communication,\nand obtain the following results: We introduce a new notion of goodness for a\ntree code, and define the notion of a potent tree code. We believe that this\nnotion is of independent interest. We prove the correctness of an explicit\nemulation procedure based on any potent tree code. We show that a randomly\ngenerated tree code (with suitable constant alphabet size) is a potent tree\ncode with overwhelming probability. Furthermore we are able to partially\nderandomize this result using only O(n) random bits, where $n$ is the depth of\nthe tree.\n  These results allow us to obtain the first fully explicit emulation procedure\nfor reliable interactive communication over noisy channels with a constant\ncommunication overhead, and exponentially small failure probability."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.0919v2", 
    "other_authors": "Benjamin A. Burton, Mathias Hiron", 
    "title": "Locating regions in a sequence under density constraints", 
    "arxiv-id": "1104.0919v2", 
    "author": "Mathias Hiron", 
    "publish": "2011-04-05T19:42:00Z", 
    "summary": "Several biological problems require the identification of regions in a\nsequence where some feature occurs within a target density range: examples\nincluding the location of GC-rich regions, identification of CpG islands, and\nsequence matching. Mathematically, this corresponds to searching a string of 0s\nand 1s for a substring whose relative proportion of 1s lies between given lower\nand upper bounds. We consider the algorithmic problem of locating the longest\nsuch substring, as well as other related problems (such as finding the shortest\nsubstring or a maximal set of disjoint substrings). For locating the longest\nsuch substring, we develop an algorithm that runs in O(n) time, improving upon\nthe previous best-known O(n log n) result. For the related problems we develop\nO(n log log n) algorithms, again improving upon the best-known O(n log n)\nresults. Practical testing verifies that our new algorithms enjoy significantly\nsmaller time and memory footprints, and can process sequences that are orders\nof magnitude longer as a result."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1044v2", 
    "other_authors": "Ming Lam Leung", 
    "title": "Fixed Parameter Tractable Algorithm for Firefighting Problem", 
    "arxiv-id": "1104.1044v2", 
    "author": "Ming Lam Leung", 
    "publish": "2011-04-06T09:33:42Z", 
    "summary": "The firefighter problem is defined as below. A fire initially breaks out at a\nvertex r on a graph G. In each step, a firefighter chooses to protect one\nvertex, which is not yet burnt. And the fire spreads out to its unprotected\nneighboring vertices afterwards. The objective of the problem is to choose a\nsequence of vertices to protect, in order to save maximum number of vertices\nfrom the fire.\n  In this paper, we will introduce a parameter k into the firefighter problem\nand give several FPT algorithms using a random separation technique of Cai,\nChan and Chan. We will prove firefighter problem is FPT on general graph if we\ntake total number of vertices burnt to be a parameter. If we parameterize the\nnumber of protected vertices, we discover several FPT algorithms of the\nfirefighter problem on degree bounded graph and unicyclic graph. Furthermore,\nwe also study the firefighter problem on weighted and valued graph, and the\nproblem with multiple fire sources on degree-bounded graph."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1330v5", 
    "other_authors": "Guy Even, Yakov Matsri, Moti Medina", 
    "title": "Multi-Hop Routing and Scheduling in Wireless Networks in the SINR model", 
    "arxiv-id": "1104.1330v5", 
    "author": "Moti Medina", 
    "publish": "2011-04-07T13:44:46Z", 
    "summary": "We present an algorithm for multi-hop routing and scheduling of requests in\nwireless networks in the \\sinr\\ model. The goal of our algorithm is to maximize\nthe throughput or maximize the minimum ratio between the flow and the demand.\n  Our algorithm partitions the links into buckets. Every bucket consists of a\nset of links that have nearly equivalent reception powers. We denote the number\nof nonempty buckets by $\\sigdiv$. Our algorithm obtains an approximation ratio\nof $O(\\sigdiv \\cdot \\log n)$, where $n$ denotes the number of nodes. For the\ncase of linear powers $\\sigdiv =1$, hence the approximation ratio of the\nalgorithm is $O(\\log n)$. This is the first practical approximation algorithm\nfor linear powers with an approximation ratio that depends only on $n$ (and not\non the max-to-min distance ratio).\n  If the transmission power of each link is part of the input (and arbitrary),\nthen $\\sigdiv = O(\\log\\Gamma + \\log \\Delta)$, where $\\Gamma$ denotes the ratio\nof the max-to-min power, and $\\Delta$ denotes the ratio of the max-to-min\ndistance. Hence, the approximation ratio is $O(\\log n \\cdot (\\log\\Gamma + \\log\n\\Delta))$.\n  Finally, we consider the case that the algorithm needs to assign powers to\neach link in a range $[\\pmin,\\pmax]$. An extension of the algorithm to this\ncase achieves an approximation ratio of $O[(\\log n + \\log \\log \\Gamma) \\cdot\n(\\log\\Gamma + \\log \\Delta)]$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1377v1", 
    "other_authors": "Ronitt Rubinfeld, Gil Tamir, Shai Vardi, Ning Xie", 
    "title": "Fast Local Computation Algorithms", 
    "arxiv-id": "1104.1377v1", 
    "author": "Ning Xie", 
    "publish": "2011-04-07T16:55:23Z", 
    "summary": "For input $x$, let $F(x)$ denote the set of outputs that are the \"legal\"\nanswers for a computational problem $F$. Suppose $x$ and members of $F(x)$ are\nso large that there is not time to read them in their entirety. We propose a\nmodel of {\\em local computation algorithms} which for a given input $x$,\nsupport queries by a user to values of specified locations $y_i$ in a legal\noutput $y \\in F(x)$. When more than one legal output $y$ exists for a given\n$x$, the local computation algorithm should output in a way that is consistent\nwith at least one such $y$. Local computation algorithms are intended to\ndistill the common features of several concepts that have appeared in various\nalgorithmic subfields, including local distributed computation, local\nalgorithms, locally decodable codes, and local reconstruction.\n  We develop a technique, based on known constructions of small sample spaces\nof $k$-wise independent random variables and Beck's analysis in his algorithmic\napproach to the Lov{\\'{a}}sz Local Lemma, which under certain conditions can be\napplied to construct local computation algorithms that run in {\\em\npolylogarithmic} time and space. We apply this technique to maximal independent\nset computations, scheduling radio network broadcasts, hypergraph coloring and\nsatisfying $k$-SAT formulas."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1601v5", 
    "other_authors": "Gregory Kucherov", 
    "title": "On-line construction of position heaps", 
    "arxiv-id": "1104.1601v5", 
    "author": "Gregory Kucherov", 
    "publish": "2011-04-08T15:46:44Z", 
    "summary": "We propose a simple linear-time on-line algorithm for constructing a position\nheap for a string [Ehrenfeucht et al, 2011]. Our definition of position heap\ndiffers slightly from the one proposed in [Ehrenfeucht et al, 2011] in that it\nconsiders the suffixes ordered from left to right. Our construction is based on\nclassic suffix pointers and resembles the Ukkonen's algorithm for suffix trees\n[Ukkonen, 1995]. Using suffix pointers, the position heap can be extended into\nthe augmented position heap that allows for a linear-time string matching\nalgorithm [Ehrenfeucht et al, 2011]."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1822v1", 
    "other_authors": "Eduardo Hwang", 
    "title": "Dimensionality Decrease Heuristics for NP Complete Problems", 
    "arxiv-id": "1104.1822v1", 
    "author": "Eduardo Hwang", 
    "publish": "2011-04-11T01:04:05Z", 
    "summary": "The vast majority of scientific community believes that P!=NP, with countless\nsupporting arguments. The number of people who believe otherwise probably\namounts to as few as those opposing the 2nd Law of Thermodynamics. But isn't\nnature elegant enough, not to resource to brute-force search? In this article,\na novel concept of dimensionality is presented, which may lead to a more\nefficient class of heuristic implementations to solve NP complete problems.\nThus, broadening the universe of man-machine tractable problems.\nDimensionality, as defined here, will be a closer analog of strain energy in\nnature."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.1852v1", 
    "other_authors": "Tony T. Lee, Yujie Wan, Hao Guan", 
    "title": "Randomized $\u0394$-Edge-Coloring via Quaternion of Complex Colors", 
    "arxiv-id": "1104.1852v1", 
    "author": "Hao Guan", 
    "publish": "2011-04-11T06:25:40Z", 
    "summary": "This paper explores the application of a new algebraic method of color\nexchanges to the edge coloring of simple graphs. Vizing's theorem states that\nthe edge coloring of a simple graph $G$ requires either $\\Delta$ or $\\Delta+1$\ncolors, where $\\Delta$ is the maximum vertex degree of $G$. Holyer proved that\nit is {\\bf NP}-complete to decide whether $G$ is $\\Delta$-edge-colorable even\nfor cubic graphs. By introducing the concept of complex colors, we show that\nthe color-exchange operation follows the same multiplication rules as\nquaternion. An initially $\\Delta$-edge-colored graph $G$ allows\nvariable-colored edges, which can be eliminated by color exchanges in a manner\nsimilar to variable eliminations in solving systems of linear equations. The\nproblem is solved if all variables are eliminated and a properly\n$\\Delta$-edge-colored graph is reached. For a randomly generated graph $G$, we\nprove that our algorithm returns a proper $\\Delta$-edge-coloring with a\nprobability of at least 1/2 in $O(\\Delta|V||E|^5)$ time if $G$ is\n$\\Delta$-edge-colorable. Otherwise, the algorithm halts in polynomial time and\nsignals the impossibility of a solution, meaning that the chromatic index of\n$G$ probably equals $\\Delta+1$. Animations of the edge-coloring algorithms\nproposed in this paper are posted at YouTube\nhttp://www.youtube.com/watch?v=KMnj4UMYl7k."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2076v1", 
    "other_authors": "Malik Magdon-Ismail", 
    "title": "A Note On Estimating the Spectral Norm of A Matrix Efficiently", 
    "arxiv-id": "1104.2076v1", 
    "author": "Malik Magdon-Ismail", 
    "publish": "2011-04-11T22:06:19Z", 
    "summary": "We give an efficient algorithm which can obtain a relative error\napproximation to the spectral norm of a matrix, combining the power iteration\nmethod with some techniques from matrix reconstruction which use random\nsampling."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2230v1", 
    "other_authors": "Fedor V. Fomin, Yngve Villanger", 
    "title": "Subexponential Parameterized Algorithm for Minimum Fill-in", 
    "arxiv-id": "1104.2230v1", 
    "author": "Yngve Villanger", 
    "publish": "2011-04-12T14:39:13Z", 
    "summary": "The Minimum Fill-in problem is to decide if a graph can be triangulated by\nadding at most k edges. Kaplan, Shamir, and Tarjan [FOCS 1994] have shown that\nthe problem is solvable in time O(2^(O(k)) + k2 * nm) on graphs with n vertices\nand m edges and thus is fixed parameter tractable. Here, we give the first\nsubexponential parameterized algorithm solving Minimum Fill-in in time\nO(2^(O(\\sqrt{k} log k)) + k2 * nm). This substantially lower the complexity of\nthe problem. Techniques developed for Minimum Fill-in can be used to obtain\nsubexponential parameterized algorithms for several related problems including\nMinimum Chain Completion, Chordal Graph Sandwich, and Triangulating Colored\nGraph."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2275v5", 
    "other_authors": "Frank Kammer, Torsten Tholey", 
    "title": "Approximate Tree Decompositions of Planar Graphs in Linear Time", 
    "arxiv-id": "1104.2275v5", 
    "author": "Torsten Tholey", 
    "publish": "2011-04-12T17:12:46Z", 
    "summary": "Many algorithms have been developed for NP-hard problems on graphs with small\ntreewidth $k$. For example, all problems that are expressable in linear\nextended monadic second order can be solved in linear time on graphs of bounded\ntreewidth. It turns out that the bottleneck of many algorithms for NP-hard\nproblems is the computation of a tree decomposition of width $O(k)$. In\nparticular, by the bidimensional theory, there are many linear extended monadic\nsecond order problems that can be solved on $n$-vertex planar graphs with\ntreewidth $k$ in a time linear in $n$ and subexponential in $k$ if a tree\ndecomposition of width $O(k)$ can be found in such a time.\n  We present the first algorithm that, on $n$-vertex planar graphs with\ntreewidth $k$, finds a tree decomposition of width $O(k)$ in such a time. In\nmore detail, our algorithm has a running time of $O(n k^2 \\log k)$. We show the\nresult as a special case of a result concerning so-called weighted treewidth of\nweighted graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2315v3", 
    "other_authors": "Kook Jin Ahn, Sudipto Guha", 
    "title": "Linear Programming in the Semi-streaming Model with Application to the   Maximum Matching Problem", 
    "arxiv-id": "1104.2315v3", 
    "author": "Sudipto Guha", 
    "publish": "2011-04-12T19:51:47Z", 
    "summary": "In this paper, we study linear programming based approaches to the maximum\nmatching problem in the semi-streaming model. The semi-streaming model has\ngained attention as a model for processing massive graphs as the importance of\nsuch graphs has increased. This is a model where edges are streamed-in in an\nadversarial order and we are allowed a space proportional to the number of\nvertices in a graph.\n  In recent years, there has been several new results in this semi-streaming\nmodel. However broad techniques such as linear programming have not been\nadapted to this model. We present several techniques to adapt and optimize\nlinear programming based approaches in the semi-streaming model with an\napplication to the maximum matching problem. As a consequence, we improve\n(almost) all previous results on this problem, and also prove new results on\ninteresting variants."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2799v1", 
    "other_authors": "John Iacono, Mihai P\u01cetra\u015fcu", 
    "title": "Using Hashing to Solve the Dictionary Problem (In External Memory)", 
    "arxiv-id": "1104.2799v1", 
    "author": "Mihai P\u01cetra\u015fcu", 
    "publish": "2011-04-14T15:16:32Z", 
    "summary": "We consider the dictionary problem in external memory and improve the update\ntime of the well-known buffer tree by roughly a logarithmic factor. For any\n\\lambda >= max {lg lg n, log_{M/B} (n/B)}, we can support updates in time\nO(\\lambda / B) and queries in sublogarithmic time, O(log_\\lambda n). We also\npresent a lower bound in the cell-probe model showing that our data structure\nis optimal.\n  In the RAM, hash tables have been used to solve the dictionary problem faster\nthan binary search for more than half a century. By contrast, our data\nstructure is the first to beat the comparison barrier in external memory. Ours\nis also the first data structure to depart convincingly from the indivisibility\nparadigm."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.2882v1", 
    "other_authors": "Liam Roditty, Virginia Vassilevska Williams", 
    "title": "Minimum Weight Cycles and Triangles: Equivalences and Algorithms", 
    "arxiv-id": "1104.2882v1", 
    "author": "Virginia Vassilevska Williams", 
    "publish": "2011-04-14T19:32:10Z", 
    "summary": "We consider the fundamental algorithmic problem of finding a cycle of minimum\nweight in a weighted graph. In particular, we show that the minimum weight\ncycle problem in an undirected n-node graph with edge weights in {1,...,M} or\nin a directed n-node graph with edge weights in {-M,..., M} and no negative\ncycles can be efficiently reduced to finding a minimum weight triangle in an\nTheta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking,\nour reductions imply the following surprising phenomenon: a minimum cycle with\nan arbitrary number of weighted edges can be \"encoded\" using only three edges\nwithin roughly the same weight interval! This resolves a longstanding open\nproblem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].\n  A direct consequence of our efficient reductions are O (Mn^{omega})-time\nalgorithms using fast matrix multiplication (FMM) for finding a minimum weight\ncycle in both undirected graphs with integral weights from the interval [1,M]\nand directed graphs with integral weights from the interval [-M,M]. The latter\nseems to reveal a strong separation between the all pairs shortest paths (APSP)\nproblem and the minimum weight cycle problem in directed graphs as the fastest\nknown APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J.\nACM 2002].\n  In contrast, when only combinatorial algorithms are allowed (that is, without\nFMM) the only known solution to minimum weight cycle is by computing APSP.\nInterestingly, any separation between the two problems in this case would be an\namazing breakthrough as by a recent paper by Vassilevska W. and Williams\n[FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycle\nimmediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3045v1", 
    "other_authors": "Sebastien Collette, John Iacono, Stefan Langerman", 
    "title": "Confluent Persistence Revisited", 
    "arxiv-id": "1104.3045v1", 
    "author": "Stefan Langerman", 
    "publish": "2011-04-15T13:13:05Z", 
    "summary": "It is shown how to enhance any data structure in the pointer model to make it\nconfluently persistent, with efficient query and update times and limited space\noverhead. Updates are performed in $O(\\log n)$ amortized time, and following a\npointer takes $O(\\log c \\log n)$ time where $c$ is the in-degree of a node in\nthe data structure. In particular, this proves that confluent persistence can\nbe achieved at a logarithmic cost in the bounded in-degree model used widely in\nprevious work. This is a $O(n/\\log n)$-factor improvement over the previous\nknown transform to make a data structure confluently persistent."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3090v1", 
    "other_authors": "Tobias M\u00f6mke, Ola Svensson", 
    "title": "Approximating Graphic TSP by Matchings", 
    "arxiv-id": "1104.3090v1", 
    "author": "Ola Svensson", 
    "publish": "2011-04-15T15:40:56Z", 
    "summary": "We present a framework for approximating the metric TSP based on a novel use\nof matchings. Traditionally, matchings have been used to add edges in order to\nmake a given graph Eulerian, whereas our approach also allows for the removal\nof certain edges leading to a decreased cost.\n  For the TSP on graphic metrics (graph-TSP), the approach yields a\n1.461-approximation algorithm with respect to the Held-Karp lower bound. For\ngraph-TSP restricted to a class of graphs that contains degree three bounded\nand claw-free graphs, we show that the integrality gap of the Held-Karp\nrelaxation matches the conjectured ratio 4/3. The framework allows for\ngeneralizations in a natural way and also leads to a 1.586-approximation\nalgorithm for the traveling salesman path problem on graphic metrics where the\nstart and end vertices are prespecified."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3100v1", 
    "other_authors": "Stefan Kiefer, Andrzej Murawski, Jo\u00ebl Ouaknine, James Worrell, Lijun Zhang", 
    "title": "On Stabilization in Herman's Algorithm", 
    "arxiv-id": "1104.3100v1", 
    "author": "Lijun Zhang", 
    "publish": "2011-04-15T16:08:59Z", 
    "summary": "Herman's algorithm is a synchronous randomized protocol for achieving\nself-stabilization in a token ring consisting of N processes. The interaction\nof tokens makes the dynamics of the protocol very difficult to analyze. In this\npaper we study the expected time to stabilization in terms of the initial\nconfiguration. It is straightforward that the algorithm achieves stabilization\nalmost surely from any initial configuration, and it is known that the\nworst-case expected time to stabilization (with respect to the initial\nconfiguration) is Theta(N^2). Our first contribution is to give an upper bound\nof 0.64 N^2 on the expected stabilization time, improving on previous upper\nbounds and reducing the gap with the best existing lower bound. We also\nintroduce an asynchronous version of the protocol, showing a similar O(N^2)\nconvergence bound in this case. Assuming that errors arise from the corruption\nof some number k of bits, where k is fixed independently of the size of the\nring, we show that the expected time to stabilization is O(N). This reveals a\nhitherto unknown and highly desirable property of Herman's algorithm: it\nrecovers quickly from bounded errors. We also show that if the initial\nconfiguration arises by resetting each bit independently and uniformly at\nrandom, then stabilization is significantly faster than in the worst case."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3119v2", 
    "other_authors": "Alfons Laarman, Jaco van de Pol, Michael Weber", 
    "title": "Parallel Recursive State Compression for Free", 
    "arxiv-id": "1104.3119v2", 
    "author": "Michael Weber", 
    "publish": "2011-04-15T17:59:12Z", 
    "summary": "This paper focuses on reducing memory usage in enumerative model checking,\nwhile maintaining the multi-core scalability obtained in earlier work. We\npresent a tree-based multi-core compression method, which works by leveraging\nsharing among sub-vectors of state vectors.\n  An algorithmic analysis of both worst-case and optimal compression ratios\nshows the potential to compress even large states to a small constant on\naverage (8 bytes). Our experiments demonstrate that this holds up in practice:\nthe median compression ratio of 279 measured experiments is within 17% of the\noptimum for tree compression, and five times better than the median compression\nratio of SPIN's COLLAPSE compression.\n  Our algorithms are implemented in the LTSmin tool, and our experiments show\nthat for model checking, multi-core tree compression pays its own way: it comes\nvirtually without overhead compared to the fastest hash table-based methods."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3153v1", 
    "other_authors": "Michalis Christou, Maxime Crochemore, Costas S. Iliopoulos, Marcin Kubica, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, Bartosz Szreder, Tomasz Walen", 
    "title": "Efficient Seeds Computation Revisited", 
    "arxiv-id": "1104.3153v1", 
    "author": "Tomasz Walen", 
    "publish": "2011-04-15T20:29:30Z", 
    "summary": "The notion of the cover is a generalization of a period of a string, and\nthere are linear time algorithms for finding the shortest cover. The seed is a\nmore complicated generalization of periodicity, it is a cover of a superstring\nof a given string, and the shortest seed problem is of much higher algorithmic\ndifficulty. The problem is not well understood, no linear time algorithm is\nknown. In the paper we give linear time algorithms for some of its versions ---\ncomputing shortest left-seed array, longest left-seed array and checking for\nseeds of a given length. The algorithm for the last problem is used to compute\nthe seed array of a string (i.e., the shortest seeds for all the prefixes of\nthe string) in $O(n^2)$ time. We describe also a simpler alternative algorithm\ncomputing efficiently the shortest seeds. As a by-product we obtain an\n$O(n\\log{(n/m)})$ time algorithm checking if the shortest seed has length at\nleast $m$ and finding the corresponding seed. We also correct some important\ndetails missing in the previously known shortest-seed algorithm (Iliopoulos et\nal., 1996)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3677v1", 
    "other_authors": "Pinar Heggernes, Pim van 't Hof, Benjamin L\u00e9v\u00eaque, Daniel Lokshtanov, Christophe Paul", 
    "title": "Contracting Graphs to Paths and Trees", 
    "arxiv-id": "1104.3677v1", 
    "author": "Christophe Paul", 
    "publish": "2011-04-19T08:49:35Z", 
    "summary": "Vertex deletion and edge deletion problems play a central role in\nParameterized Complexity. Examples include classical problems like Feedback\nVertex Set, Odd Cycle Transversal, and Chordal Deletion. Interestingly, the\nstudy of edge contraction problems of this type from a parameterized\nperspective has so far been left largely unexplored. We consider two basic edge\ncontraction problems, which we call Path-Contractibility and\nTree-Contractibility. Both problems take an undirected graph $G$ and an integer\n$k$ as input, and the task is to determine whether we can obtain a path or an\nacyclic graph, respectively, by contracting at most $k$ edges of $G$. Our main\ncontribution is an algorithm with running time $4^{k+O(\\log^2 k)} + n^{O(1)}$\nfor Path-Contractibility and an algorithm with running time $4.88^k n^{O(1)}$\nfor Tree-Contractibility, based on a novel application of the color coding\ntechnique of Alon, Yuster and Zwick. Furthermore, we show that\nPath-Contractibility has a kernel with at most $5k+3$ vertices, while\nTree-Contractibility does not have a polynomial kernel unless coNP $\\subseteq$\nNP/poly. We find the latter result surprising, because of the strong connection\nbetween Tree-Contractibility and Feedback Vertex Set, which is known to have a\nvertex kernel with size $O(k^2)$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.3923v4", 
    "other_authors": "Bundit Laekhanukit", 
    "title": "An improved approximation algorithm for the minimum-cost subset   k-connected subgraph problem", 
    "arxiv-id": "1104.3923v4", 
    "author": "Bundit Laekhanukit", 
    "publish": "2011-04-20T02:14:05Z", 
    "summary": "The minimum-cost subset $k$-connected subgraph problem is a cornerstone\nproblem in the area of network design with vertex connectivity requirements. In\nthis problem, we are given a graph $G=(V,E)$ with costs on edges and a set of\nterminals $T$. The goal is to find a minimum cost subgraph such that every pair\nof terminals are connected by $k$ openly (vertex) disjoint paths. In this\npaper, we present an approximation algorithm for the subset $k$-connected\nsubgraph problem which improves on the previous best approximation guarantee of\n$O(k^2\\log{k})$ by Nutov (FOCS 2009). Our approximation guarantee,\n$\\alpha(|T|)$, depends upon the number of terminals: [\\alpha(|T|) \\ \\ =\\ \\\nO(|T|^2) & if |T| < 2k O(k \\log^2 k) & if 2k\\le |T| < k^2 O(k \\log k) & if |T|\n\\ge k^2]\n  So, when the number of terminals is {\\em large enough}, the approximation\nguarantee improves significantly. Moreover, we show that, given an\napproximation algorithm for $|T|=k$, we can obtain almost the same\napproximation guarantee for any instances with $|T|> k$. This suggests that the\nhardest instances of the problem are when $|T|\\approx k$."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.4058v1", 
    "other_authors": "Kook Jin Ahn, Sudipto Guha", 
    "title": "Laminar Families and Metric Embeddings: Non-bipartite Maximum Matching   Problem in the Semi-Streaming Model", 
    "arxiv-id": "1104.4058v1", 
    "author": "Sudipto Guha", 
    "publish": "2011-04-20T15:39:28Z", 
    "summary": "In this paper, we study the non-bipartite maximum matching problem in the\nsemi-streaming model. The maximum matching problem in the semi-streaming model\nhas received a significant amount of attention lately. While the problem has\nbeen somewhat well solved for bipartite graphs, the known algorithms for\nnon-bipartite graphs use $2^{\\frac1\\epsilon}$ passes or $n^{\\frac1\\epsilon}$\ntime to compute a $(1-\\epsilon)$ approximation. In this paper we provide the\nfirst FPTAS (polynomial in $n,\\frac1\\epsilon$) for the problem which is\nefficient in both the running time and the number of passes. We also show that\nwe can estimate the size of the matching in $O(\\frac1\\epsilon)$ passes using\nslightly superlinear space.\n  To achieve both results, we use the structural properties of the matching\npolytope such as the laminarity of the tight sets and total dual integrality.\nThe algorithms are iterative, and are based on the fractional packing and\ncovering framework. However the formulations herein require exponentially many\nvariables or constraints. We use laminarity, metric embeddings and graph\nsparsification to reduce the space required by the algorithms in between and\nacross the iterations. This is the first use of these ideas in the\nsemi-streaming model to solve a combinatorial optimization problem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110830605", 
    "link": "http://arxiv.org/pdf/1104.4203v1", 
    "other_authors": "Pawel Gawrychowski", 
    "title": "Pattern matching in Lempel-Ziv compressed strings: fast, simple, and   deterministic", 
    "arxiv-id": "1104.4203v1", 
    "author": "Pawel Gawrychowski", 
    "publish": "2011-04-21T08:39:37Z", 
    "summary": "Countless variants of the Lempel-Ziv compression are widely used in many\nreal-life applications. This paper is concerned with a natural modification of\nthe classical pattern matching problem inspired by the popularity of such\ncompression methods: given an uncompressed pattern s[1..m] and a Lempel-Ziv\nrepresentation of a string t[1..N], does s occur in t? Farach and Thorup gave a\nrandomized O(nlog^2(N/n)+m) time solution for this problem, where n is the size\nof the compressed representation of t. We improve their result by developing a\nfaster and fully deterministic O(nlog(N/n)+m) time algorithm with the same\nspace complexity. Note that for highly compressible texts, log(N/n) might be of\norder n, so for such inputs the improvement is very significant. A (tiny)\nfragment of our method can be used to give an asymptotically optimal solution\nfor the substring hashing problem considered by Farach and Muthukrishnan."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4279v2", 
    "other_authors": "Sebastian Ordyniak, Daniel Paulusma, Stefan Szeider", 
    "title": "Satisfiability of Acyclic and Almost Acyclic CNF Formulas", 
    "arxiv-id": "1104.4279v2", 
    "author": "Stefan Szeider", 
    "publish": "2011-04-21T15:01:02Z", 
    "summary": "We show that the Satisfiability (SAT) problem for CNF formulas with\n{\\beta}-acyclic hypergraphs can be solved in polynomial time by using a special\ntype of Davis-Putnam resolution in which each resolvent is a subset of a parent\nclause. We extend this class to CNF formulas for which this type of\nDavis-Putnam resolution still applies and show that testing membership in this\nclass is NP-complete. We compare the class of {\\beta}-acyclic formulas and this\nsuperclass with a number of known polynomial formula classes. We then study the\nparameterized complexity of SAT for \"almost\" {\\beta}-acyclic instances, using\nas parameter the formula's distance from being {\\beta}-acyclic. As distance we\nuse the size of a smallest strong backdoor set and the {\\beta}-hypertree width.\nAs a by-product we obtain the W[1]-hardness of SAT parameterized by the\n(undirected) clique-width of the incidence graph, which disproves a conjecture\nby Fischer, Makowsky, and Ravve."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4353v1", 
    "other_authors": "D. Belazzougui, A. C. Kaporis, P. G. Spirakis", 
    "title": "Random input helps searching predecessors", 
    "arxiv-id": "1104.4353v1", 
    "author": "P. G. Spirakis", 
    "publish": "2011-04-21T20:25:03Z", 
    "summary": "We solve the dynamic Predecessor Problem with high probability (whp) in\nconstant time, using only $n^{1+\\delta}$ bits of memory, for any constant\n$\\delta > 0$. The input keys are random wrt a wider class of the well studied\nand practically important class of $(f_1, f_2)$-smooth distributions introduced\nin \\cite{and:mat}. It achieves O(1) whp amortized time. Its worst-case time is\n$O(\\sqrt{\\frac{\\log n}{\\log \\log n}})$. Also, we prove whp $O(\\log \\log \\log\nn)$ time using only $n^{1+ \\frac{1}{\\log \\log n}}= n^{1+o(1)}$ bits. Finally,\nwe show whp $O(\\log \\log n)$ time using O(n) space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4471v1", 
    "other_authors": "Sebastian B\u00f6cker", 
    "title": "Towards a Data Reduction for the Minimum Flip Supertree Problem", 
    "arxiv-id": "1104.4471v1", 
    "author": "Sebastian B\u00f6cker", 
    "publish": "2011-04-22T15:48:15Z", 
    "summary": "In computational phylogenetics, the problem of constructing a supertree of a\ngiven set of rooted input trees can be formalized in different ways, to cope\nwith contradictory information in the input. We consider the Minimum Flip\nSupertree problem, where the input trees are transformed into a 0/1/?-matrix,\nsuch that each row represents a taxon, and each column represents an inner node\nof one of the input trees. Our goal is to find a perfect phylogeny for the\ninput matrix requiring a minimum number of 0/1-flips, that is, corrections of\n0/1-entries in the matrix. The problem is known to be NP-complete. Here, we\npresent a parameterized data reduction with polynomial running time. The data\nreduction guarantees that the reduced instance has a solution if and only if\nthe original instance has a solution. We then make our data reduction\nparameter-independent by using upper bounds. This allows us to preprocess an\ninstance, and to solve the reduced instance with an arbitrary method. Different\nfrom an existing data reduction for the consensus tree problem, our reduction\nallows us to draw conclusions about certain entries in the matrix. We have\nimplemented and evaluated our data reduction. Unfortunately, we find that the\nMinimum Flip Supertree problem is also hard in practice: The amount of\ninformation that can be derived during data reduction diminishes as instances\nget more \"complicated\", and running times for \"complicated\" instances quickly\nbecome prohibitive. Still, our method offers another route of attack for this\nrelevant phylogenetic problem."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4552v1", 
    "other_authors": "Sumit Ganguly", 
    "title": "Polynomial Estimators for High Frequency Moments", 
    "arxiv-id": "1104.4552v1", 
    "author": "Sumit Ganguly", 
    "publish": "2011-04-23T12:32:22Z", 
    "summary": "We present an algorithm for computing $F_p$, the $p$th moment of an\n$n$-dimensional frequency vector of a data stream, for $2 < p < \\log (n) $, to\nwithin $1\\pm \\epsilon$ factors, $\\epsilon \\in [n^{-1/p},1]$ with high constant\nprobability. Let $m$ be the number of stream records and $M$ be the largest\nmagnitude of a stream update.\n  The algorithm uses space in bits $$ O(p^2\\epsilon^{-2}n^{1-2/p}E(p,n) \\log\n(n) \\log (nmM)/\\min(\\log (n),\\epsilon^{4/p-2}))$$ where, $E(p,n) =\n(1-2/p)^{-1}(1-n^{-4(1-2/p})$. Here $E(p,n)$ is $ O(1)$ for $p = 2+\\Omega(1)$\nand $ O(\\log n)$ for $p = 2 + O(1/\\log (n)$. This improves upon the space\nrequired by current algorithms\n\\cite{iw:stoc05,bgks:soda06,ako:arxiv10,bo:arxiv10} by a factor of at least\n$\\Omega(\\epsilon^{-4/p} \\min(\\log (n), \\epsilon^{4/p-2}))$. The update time is\n$O(\\log (n))$. We use a new technique for designing estimators for functions of\nthe form $\\psi(\\expect{X})$, where, $X$ is a random variable and $\\psi$ is a\nsmooth function, based on a low-degree Taylor polynomial expansion of\n$\\psi(\\expect{X})$ around an estimate of $\\expect{X}$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4669v2", 
    "other_authors": "Michelangelo Grigni, Hao-Hsiang Hung", 
    "title": "Finding Light Spanners in Bounded Pathwidth Graphs", 
    "arxiv-id": "1104.4669v2", 
    "author": "Hao-Hsiang Hung", 
    "publish": "2011-04-25T00:59:46Z", 
    "summary": "Given an edge-weighted graph $G$ and $\\epsilon>0$, a $(1+\\epsilon)$-spanner\nis a spanning subgraph $G'$ whose shortest path distances approximate those of\n$G$ within a $(1+\\epsilon)$ factor. If $G$ is from certain minor-closed graph\nfamilies (at least bounded genus graphs and apex graphs), then we know that\nlight spanners exist. That is, we can compute a $(1+\\epsilon)$-spanner $G'$\nwith total edge weight at most a constant times the weight of a minimum\nspanning tree. This constant may depend on $\\epsilon$ and the graph family, but\nnot on the particular graph $G$ nor on its edge weighting. For weighted graphs\nfrom several minor-closed graph families, the existence of light spanners has\nbeen essential in the design of approximation schemes for the metric TSP (the\ntraveling salesman problem) and some similar problems. In this paper we make\nsome progress towards the conjecture that light spanners exist for every\nminor-closed graph family. In particular, we show that they exist for graphs\nwith bounded pathwidth. We do this via the construction of light enough\nmonotone spanning trees in such graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2012.12.039", 
    "link": "http://arxiv.org/pdf/1104.4890v1", 
    "other_authors": "Jakub \u0141\u0105cki, Piotr Sankowski", 
    "title": "Min-cuts and Shortest Cycles in Planar Graphs in O(n log log n) Time", 
    "arxiv-id": "1104.4890v1", 
    "author": "Piotr Sankowski", 
    "publish": "2011-04-26T11:29:22Z", 
    "summary": "We present a deterministic O(n log log n) time algorithm for finding shortest\ncycles and minimum cuts in planar graphs. The algorithm improves the previously\nknown fastest algorithm by Italiano et al. in STOC'11 by a factor of log n.\nThis speedup is obtained through the use of dense distance graphs combined with\na divide-and-conquer approach."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1104.4892v2", 
    "other_authors": "Hsien-Chih Chang, Hsueh-I Lu", 
    "title": "Computing the Girth of a Planar Graph in Linear Time", 
    "arxiv-id": "1104.4892v2", 
    "author": "Hsueh-I Lu", 
    "publish": "2011-04-26T11:44:19Z", 
    "summary": "The girth of a graph is the minimum weight of all simple cycles of the graph.\nWe study the problem of determining the girth of an n-node unweighted\nundirected planar graph. The first non-trivial algorithm for the problem, given\nby Djidjev, runs in O(n^{5/4} log n) time. Chalermsook, Fakcharoenphol, and\nNanongkai reduced the running time to O(n log^2 n). Weimann and Yuster further\nreduced the running time to O(n log n). In this paper, we solve the problem in\nO(n) time."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1104.5111v1", 
    "other_authors": "Martin Dietzfelbinger, Michael Mitzenmacher, Michael Rink", 
    "title": "Cuckoo Hashing with Pages", 
    "arxiv-id": "1104.5111v1", 
    "author": "Michael Rink", 
    "publish": "2011-04-27T10:32:34Z", 
    "summary": "Although cuckoo hashing has significant applications in both theoretical and\npractical settings, a relevant downside is that it requires lookups to multiple\nlocations. In many settings, where lookups are expensive, cuckoo hashing\nbecomes a less compelling alternative. One such standard setting is when memory\nis arranged in large pages, and a major cost is the number of page accesses. We\npropose the study of cuckoo hashing with pages, advocating approaches where\neach key has several possible locations, or cells, on a single page, and\nadditional choices on a second backup page. We show experimentally that with k\ncell choices on one page and a single backup cell choice, one can achieve\nnearly the same loads as when each key has k+1 random cells to choose from,\nwith most lookups requiring just one page access, even when keys are placed\nonline using a simple algorithm. While our results are currently experimental,\nthey suggest several interesting new open theoretical questions for cuckoo\nhashing with pages."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1104.5400v3", 
    "other_authors": "Ely Porat, Bar Shalem", 
    "title": "A cuckoo hashing variant with improved memory utilization and insertion   time", 
    "arxiv-id": "1104.5400v3", 
    "author": "Bar Shalem", 
    "publish": "2011-04-28T14:18:26Z", 
    "summary": "Cuckoo hashing [4] is a multiple choice hashing scheme in which each item can\nbe placed in multiple locations, and collisions are resolved by moving items to\ntheir alternative locations. In the classical implementation of two-way cuckoo\nhashing, the memory is partitioned into contiguous disjoint fixed-size buckets.\nEach item is hashed to two buckets, and may be stored in any of the positions\nwithin those buckets. Ref. [2] analyzed a variation in which the buckets are\ncontiguous and overlap. However, many systems retrieve data from secondary\nstorage in same-size blocks called pages. Fetching a page is a relatively\nexpensive process; but once a page is fetched, its contents can be accessed\norders of magnitude faster. We utilize this property of memory retrieval,\npresenting a variant of cuckoo hashing incorporating the following constraint:\neach bucket must be fully contained in a single page, but buckets are not\nnecessarily contiguous. Empirical results show that this modification increases\nmemory utilization and decreases the number of iterations required to insert an\nitem. If each item is hashed to two buckets of capacity two, the page size is\n8, and each bucket is fully contained in a single page, the memory utilization\nequals 89.71% in the classical contiguous disjoint bucket variant, 93.78% in\nthe contiguous overlapping bucket variant, and increases to 97.46% in our new\nnon-contiguous bucket variant. When the memory utilization is 92% and we use\nbreadth first search to look for a vacant position, the number of iterations\nrequired to insert a new item is dramatically reduced from 545 in the\ncontiguous overlapping buckets variant to 52 in our new non-contiguous bucket\nvariant. In addition to the empirical results, we present a theoretical lower\nbound on the memory utilization of our variation as a function of the page\nsize."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1104.5533v2", 
    "other_authors": "Elaine Angelino, Michael T. Goodrich, Michael Mitzenmacher, Justin Thaler", 
    "title": "External-Memory Multimaps", 
    "arxiv-id": "1104.5533v2", 
    "author": "Justin Thaler", 
    "publish": "2011-04-29T01:55:00Z", 
    "summary": "Many data structures support dictionaries, also known as maps or associative\narrays, which store and manage a set of key-value pairs. A \\emph{multimap} is\ngeneralization that allows multiple values to be associated with the same key.\nFor example, the inverted file data structure that is used prevalently in the\ninfrastructure supporting search engines is a type of multimap, where words are\nused as keys and document pointers are used as values. We study the multimap\nabstract data type and how it can be implemented efficiently online in external\nmemory frameworks, with constant expected I/O performance. The key technique\nused to achieve our results is a combination of cuckoo hashing using buckets\nthat hold multiple items with a multiqueue implementation to cope with varying\nnumbers of values per key. Our external-memory results are for the standard\ntwo-level memory model."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1104.5557v3", 
    "other_authors": "Michael W. Mahoney", 
    "title": "Randomized algorithms for matrices and data", 
    "arxiv-id": "1104.5557v3", 
    "author": "Michael W. Mahoney", 
    "publish": "2011-04-29T06:41:53Z", 
    "summary": "Randomized algorithms for very large matrix problems have received a great\ndeal of attention in recent years. Much of this work was motivated by problems\nin large-scale data analysis, and this work was performed by individuals from\nmany different research communities. This monograph will provide a detailed\noverview of recent work on the theory of randomized matrix algorithms as well\nas the application of those ideas to the solution of practical problems in\nlarge-scale data analysis. An emphasis will be placed on a few simple core\nideas that underlie not only recent theoretical advances but also the\nusefulness of these tools in large-scale data applications. Crucial in this\ncontext is the connection with the concept of statistical leverage. This\nconcept has long been used in statistical regression diagnostics to identify\noutliers; and it has recently proved crucial in the development of improved\nworst-case matrix algorithms that are also amenable to high-quality numerical\nimplementation and that are useful to domain scientists. Randomized methods\nsolve problems such as the linear least-squares problem and the low-rank matrix\napproximation problem by constructing and operating on a randomized sketch of\nthe input matrix. Depending on the specifics of the situation, when compared\nwith the best previously-existing deterministic algorithms, the resulting\nrandomized algorithms have worst-case running time that is asymptotically\nfaster; their numerical implementations are faster in terms of clock-time; or\nthey can be implemented in parallel computing environments where existing\nnumerical algorithms fail to run at all. Numerous examples illustrating these\nobservations will be described in detail."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.0219v1", 
    "other_authors": "Iftah Gamzu, Moti Medina", 
    "title": "Improved Approximation for Orienting Mixed Graphs", 
    "arxiv-id": "1204.0219v1", 
    "author": "Moti Medina", 
    "publish": "2012-04-01T15:14:26Z", 
    "summary": "An instance of the maximum mixed graph orientation problem consists of a\nmixed graph and a collection of source-target vertex pairs. The objective is to\norient the undirected edges of the graph so as to maximize the number of pairs\nthat admit a directed source-target path. This problem has recently arisen in\nthe study of biological networks, and it also has applications in communication\nnetworks.\n  In this paper, we identify an interesting local-to-global orientation\nproperty. This property enables us to modify the best known algorithms for\nmaximum mixed graph orientation and some of its special structured instances,\ndue to Elberfeld et al. (CPM '11), and obtain improved approximation ratios. We\nfurther proceed by developing an algorithm that achieves an even better\napproximation guarantee for the general setting of the problem. Finally, we\nstudy several well-motivated variants of this orientation problem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.0220v1", 
    "other_authors": "Youssef Bassil, Aziz Barbar", 
    "title": "Sequential & Parallel Algorithms for Big-Integer Numbers Subtraction", 
    "arxiv-id": "1204.0220v1", 
    "author": "Aziz Barbar", 
    "publish": "2012-04-01T15:31:31Z", 
    "summary": "Many emerging computer applications require the processing of large numbers,\nlarger than what a CPU can handle. In fact, the top of the line PCs can only\nmanipulate numbers not longer than 32 bits or 64 bits. This is due to the size\nof the registers and the data-path inside the CPU. As a result, performing\narithmetic operations such as subtraction on big-integer numbers is to some\nextend limited. Different algorithms were designed in an attempt to solve this\nproblem; they all operate on big-integer numbers by first converting them into\na binary representation then performing bitwise operations on single bits. Such\nalgorithms are of complexity O(n) where n is the total number of bits in each\noperand. This paper proposes two new algorithms for performing arithmetic\nsubtraction on big-integer numbers. The two algorithms are different in that\none is sequential while the other is parallel. The similarity between them is\nthat both follow the same concept of dividing the big-integer inputs into\nseveral blocks or tokens of 60 bits (18 digits) each; thus reducing the input\nsize n in O(n) by a factor of 60. Subtraction of corresponding tokens, one from\neach operand, is performed as humans perform subtraction, using a pencil and a\npaper in the decimal system. Both algorithms are to be implemented using MS\nC#.NET 2005 and tested over a multiple processor system. Further studies can be\ndone on other arithmetic operations such as addition and multiplication."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.0232v1", 
    "other_authors": "Youssef Bassil, Aziz Barbar", 
    "title": "Sequential and Parallel Algorithms for the Addition of Big-Integer   Numbers", 
    "arxiv-id": "1204.0232v1", 
    "author": "Aziz Barbar", 
    "publish": "2012-04-01T16:12:07Z", 
    "summary": "Today's PCs can directly manipulate numbers not longer than 64 bits because\nthe size of the CPU registers and the data-path are limited. Consequently,\narithmetic operations such as addition, can only be performed on numbers of\nthat length. To solve the problem of computation on big-integer numbers,\ndifferent algorithms were developed. However, these algorithms are considerably\nslow because they operate on individual bits; and are only designed to run over\nsingle-processor computers. In this paper, two algorithms for handling\narithmetic addition on big-integer numbers are presented. The first algorithm\nis sequential while the second is parallel. Both algorithms, unlike existing\nones, perform addition on blocks or tokens of 60 bits (18 digits), and thus\nboosting the execution time by a factor of 60."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.0897v2", 
    "other_authors": "Elisabeth G\u00fcnther, Olaf Maurer, Nicole Megow, Andreas Wiese", 
    "title": "A New Approach to Online Scheduling: Approximating the Optimal   Competitive Ratio", 
    "arxiv-id": "1204.0897v2", 
    "author": "Andreas Wiese", 
    "publish": "2012-04-04T09:17:22Z", 
    "summary": "We propose a new approach to competitive analysis in online scheduling by\nintroducing the novel concept of competitive-ratio approximation schemes. Such\na scheme algorithmically constructs an online algorithm with a competitive\nratio arbitrarily close to the best possible competitive ratio for any online\nalgorithm. We study the problem of scheduling jobs online to minimize the\nweighted sum of completion times on parallel, related, and unrelated machines,\nand we derive both deterministic and randomized algorithms which are almost\nbest possible among all online algorithms of the respective settings. We also\ngeneralize our techniques to arbitrary monomial cost functions and apply them\nto the makespan objective. Our method relies on an abstract characterization of\nonline algorithms combined with various simplifications and transformations. We\nalso contribute algorithmic means to compute the actual value of the best\npossi- ble competitive ratio up to an arbitrary accuracy. This strongly\ncontrasts all previous manually obtained competitiveness results for algorithms\nand, most importantly, it reduces the search for the optimal com- petitive\nratio to a question that a computer can answer. We believe that our concept can\nalso be applied to many other problems and yields a new perspective on online\nalgorithms in general."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1025v2", 
    "other_authors": "Michael Kapralov, Ian Post, Jan Vondrak", 
    "title": "Online submodular welfare maximization: Greedy is optimal", 
    "arxiv-id": "1204.1025v2", 
    "author": "Jan Vondrak", 
    "publish": "2012-04-04T18:43:31Z", 
    "summary": "We prove that no online algorithm (even randomized, against an oblivious\nadversary) is better than 1/2-competitive for welfare maximization with\ncoverage valuations, unless $NP = RP$. Since the Greedy algorithm is known to\nbe 1/2-competitive for monotone submodular valuations, of which coverage is a\nspecial case, this proves that Greedy provides the optimal competitive ratio.\nOn the other hand, we prove that Greedy in a stochastic setting with\ni.i.d.items and valuations satisfying diminishing returns is\n$(1-1/e)$-competitive, which is optimal even for coverage valuations, unless\n$NP=RP$. For online budget-additive allocation, we prove that no algorithm can\nbe 0.612-competitive with respect to a natural LP which has been used\npreviously for this problem."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1082v2", 
    "other_authors": "Amotz Bar-Noy, Ben Baumer, Dror Rawitz", 
    "title": "Set It and Forget It: Approximating the Set Once Strip Cover Problem", 
    "arxiv-id": "1204.1082v2", 
    "author": "Dror Rawitz", 
    "publish": "2012-04-04T21:31:00Z", 
    "summary": "We consider the Set Once Strip Cover problem, in which n wireless sensors are\ndeployed over a one-dimensional region. Each sensor has a fixed battery that\ndrains in inverse proportion to a radius that can be set just once, but\nactivated at any time. The problem is to find an assignment of radii and\nactivation times that maximizes the length of time during which the entire\nregion is covered. We show that this problem is NP-hard. Second, we show that\nRoundRobin, the algorithm in which the sensors simply take turns covering the\nentire region, has a tight approximation guarantee of 3/2 in both Set Once\nStrip Cover and the more general Strip Cover problem, in which each radius may\nbe set finitely-many times. Moreover, we show that the more general class of\nduty cycle algorithms, in which groups of sensors take turns covering the\nentire region, can do no better. Finally, we give an optimal O(n^2 log n)-time\nalgorithm for the related Set Radius Strip Cover problem, in which all sensors\nmust be activated immediately."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1136v3", 
    "other_authors": "Adrian Kosowski", 
    "title": "Faster Walks in Graphs: A $\\tilde O(n^2)$ Time-Space Trade-off for   Undirected s-t Connectivity", 
    "arxiv-id": "1204.1136v3", 
    "author": "Adrian Kosowski", 
    "publish": "2012-04-05T07:47:57Z", 
    "summary": "In this paper, we make use of the Metropolis-type walks due to Nonaka et al.\n(2010) to provide a faster solution to the $S$-$T$-connectivity problem in\nundirected graphs (USTCON). As our main result, we propose a family of\nrandomized algorithms for USTCON which achieves a time-space product of $S\\cdot\nT = \\tilde O(n^2)$ in graphs with $n$ nodes and $m$ edges (where the $\\tilde\nO$-notation disregards poly-logarithmic terms). This improves the previously\nbest trade-off of $\\tilde O(n m)$, due to Feige (1995). Our algorithm consists\nin deploying several short Metropolis-type walks, starting from landmark nodes\ndistributed using the scheme of Broder et al. (1994) on a modified input graph.\nIn particular, we obtain an algorithm running in time $\\tilde O(n+m)$ which is,\nin general, more space-efficient than both BFS and DFS. We close the paper by\nshowing how to fine-tune the Metropolis-type walk so as to match the\nperformance parameters (e.g., average hitting time) of the unbiased random walk\nfor any graph, while preserving a worst-case bound of $\\tilde O(n^2)$ on cover\ntime."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1215v1", 
    "other_authors": "Travis Gagie", 
    "title": "On the Value of Multiple Read/Write Streams for Data Compression", 
    "arxiv-id": "1204.1215v1", 
    "author": "Travis Gagie", 
    "publish": "2012-04-05T13:07:38Z", 
    "summary": "We study whether, when restricted to using polylogarithmic memory and\npolylogarithmic passes, we can achieve qualitatively better data compression\nwith multiple read/write streams than we can with only one. We first show how\nwe can achieve universal compression using only one pass over one stream. We\nthen show that one stream is not sufficient for us to achieve good\ngrammar-based compression. Finally, we show that two streams are necessary and\nsufficient for us to achieve entropy-only bounds."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1616v2", 
    "other_authors": "Marek Cygan, Harold N. Gabow, Piotr Sankowski", 
    "title": "Algorithmic Applications of Baur-Strassen's Theorem: Shortest Cycles,   Diameter and Matchings", 
    "arxiv-id": "1204.1616v2", 
    "author": "Piotr Sankowski", 
    "publish": "2012-04-07T09:54:25Z", 
    "summary": "Consider a directed or an undirected graph with integral edge weights from\nthe set [-W, W], that does not contain negative weight cycles. In this paper,\nwe introduce a general framework for solving problems on such graphs using\nmatrix multiplication. The framework is based on the usage of Baur-Strassen's\ntheorem and of Strojohann's determinant algorithm. It allows us to give new and\nsimple solutions to the following problems:\n  * Finding Shortest Cycles -- We give a simple \\tilde{O}(Wn^{\\omega}) time\nalgorithm for finding shortest cycles in undirected and directed graphs. For\ndirected graphs (and undirected graphs with non-negative weights) this matches\nthe time bounds obtained in 2011 by Roditty and Vassilevska-Williams. On the\nother hand, no algorithm working in \\tilde{O}(Wn^{\\omega}) time was previously\nknown for undirected graphs with negative weights. Furthermore our algorithm\nfor a given directed or undirected graph detects whether it contains a negative\nweight cycle within the same running time.\n  * Computing Diameter and Radius -- We give a simple \\tilde{O}(Wn^{\\omega})\ntime algorithm for computing a diameter and radius of an undirected or directed\ngraphs. To the best of our knowledge no algorithm with this running time was\nknown for undirected graphs with negative weights.\n  * Finding Minimum Weight Perfect Matchings -- We present an\n\\tilde{O}(Wn^{\\omega}) time algorithm for finding minimum weight perfect\nmatchings in undirected graphs. This resolves an open problem posted by\nSankowski in 2006, who presented such an algorithm but only in the case of\nbipartite graphs.\n  In order to solve minimum weight perfect matching problem we develop a novel\ncombinatorial interpretation of the dual solution which sheds new light on this\nproblem. Such a combinatorial interpretation was not know previously, and is of\nindependent interest."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1957v2", 
    "other_authors": "J. Ian Munro, Patrick K. Nicholson", 
    "title": "Succinct Posets", 
    "arxiv-id": "1204.1957v2", 
    "author": "Patrick K. Nicholson", 
    "publish": "2012-04-09T19:38:58Z", 
    "summary": "We describe an algorithm for compressing a partially ordered set, or\n\\emph{poset}, so that it occupies space matching the information theory lower\nbound (to within lower order terms), in the worst case. Using this algorithm,\nwe design a succinct data structure for representing a poset that, given two\nelements, can report whether one precedes the other in constant time. This is\nequivalent to succinctly representing the transitive closure graph of the\nposet, and we note that the same method can also be used to succinctly\nrepresent the transitive reduction graph. For an $n$ element poset, the data\nstructure occupies $n^2/4 + o(n^2)$ bits, in the worst case, which is roughly\nhalf the space occupied by an upper triangular matrix. Furthermore, a slight\nextension to this data structure yields a succinct oracle for reachability in\narbitrary directed graphs. Thus, using roughly a quarter of the space required\nto represent an arbitrary directed graph, reachability queries can be supported\nin constant time."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.1958v2", 
    "other_authors": "Qingxuan Yang, John Ellis, Khalegh Mamakani, Frank Ruskey", 
    "title": "Parallel and sequential in-place permuting and perfect shuffling using   involutions", 
    "arxiv-id": "1204.1958v2", 
    "author": "Frank Ruskey", 
    "publish": "2012-04-09T19:39:27Z", 
    "summary": "We show that any permutation of ${1,2,...,N}$ can be written as the product\nof two involutions. As a consequence, any permutation of the elements of an\narray can be performed in-place in parallel in time O(1). In the case where the\npermutation is the $k$-way perfect shuffle we develop two methods for\nefficiently computing such a pair of involutions.\n  The first method works whenever $N$ is a power of $k$; in this case the time\nis O(N) and space $O(\\log^2 N)$. The second method applies to the general case\nwhere $N$ is a multiple of $k$; here the time is $O(N \\log N)$ and the space is\n$O(\\log^2 N)$. If $k=2$ the space usage of the first method can be reduced to\n$O(\\log N)$ on a machine that has a SADD (population count) instruction."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2021v3", 
    "other_authors": "Shayan Oveis Gharan, Luca Trevisan", 
    "title": "Approximating the Expansion Profile and Almost Optimal Local Graph   Clustering", 
    "arxiv-id": "1204.2021v3", 
    "author": "Luca Trevisan", 
    "publish": "2012-04-10T01:06:25Z", 
    "summary": "Spectral partitioning is a simple, nearly-linear time, algorithm to find\nsparse cuts, and the Cheeger inequalities provide a worst-case guarantee for\nthe quality of the approximation found by the algorithm. Local graph\npartitioning algorithms [ST08,ACL06,AP09] run in time that is nearly linear in\nthe size of the output set, and their approximation guarantee is worse than the\nguarantee provided by the Cheeger inequalities by a polylogarithmic\n$\\log^{\\Omega(1)} n$ factor. It has been a long standing open problem to design\na local graph clustering algorithm with an approximation guarantee close to the\nguarantee of the Cheeger inequalities and with a running time nearly linear in\nthe size of the output.\n  In this paper we solve this problem; we design an algorithm with the same\nguarantee (up to a constant factor) as the Cheeger inequality, that runs in\ntime slightly super linear in the size of the output. This is the first\nsublinear (in the size of the input) time algorithm with almost the same\nguarantee as the Cheeger's inequality. As a byproduct of our results, we prove\na bicriteria approximation algorithm for the expansion profile of any graph.\nLet $\\phi(\\gamma) = \\min_{\\mu(S) \\leq \\gamma}\\phi(S)$. There is a polynomial\ntime algorithm that, for any $\\gamma,\\epsilon>0$, finds a set $S$ of measure\n$\\mu(S)\\leq 2\\gamma^{1+\\epsilon}$, and expansion $\\phi(S)\\leq\n\\sqrt{2\\phi(\\gamma)/\\epsilon}$. Our proof techniques also provide a simpler\nproof of the structural result of Arora, Barak, Steurer [ABS10], that can be\napplied to irregular graphs.\n  Our main technical tool is that for any set $S$ of vertices of a graph, a\nlazy $t$-step random walk started from a randomly chosen vertex of $S$, will\nremain entirely inside $S$ with probability at least $(1-\\phi(S)/2)^t$. This\nitself provides a new lower bound to the uniform mixing time of any finite\nstates reversible markov chain."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2131v1", 
    "other_authors": "Michael Rink", 
    "title": "On Thresholds for the Appearance of 2-cores in Mixed Hypergraphs", 
    "arxiv-id": "1204.2131v1", 
    "author": "Michael Rink", 
    "publish": "2012-04-10T12:56:42Z", 
    "summary": "We study thresholds for the appearance of a 2-core in random hypergraphs that\nare a mixture of a constant number of random uniform hypergraphs each with a\nlinear number of edges but with different edge sizes. For the case of two\noverlapping hypergraphs we give a solution for the optimal (expected) number of\nedges of each size such that the 2-core threshold for the resulting mixed\nhypergraph is maximized. We show that for adequate edge sizes this threshold\nexceeds the maximum 2-core threshold for any random uniform hypergraph, which\ncan be used to improve the space utilization of several data structures that\nrely on this parameter."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2136v2", 
    "other_authors": "Jeremiah Blocki, Avrim Blum, Anupam Datta, Or Sheffet", 
    "title": "The Johnson-Lindenstrauss Transform Itself Preserves Differential   Privacy", 
    "arxiv-id": "1204.2136v2", 
    "author": "Or Sheffet", 
    "publish": "2012-04-10T13:11:47Z", 
    "summary": "This paper proves that an \"old dog\", namely -- the classical\nJohnson-Lindenstrauss transform, \"performs new tricks\" -- it gives a novel way\nof preserving differential privacy. We show that if we take two databases, $D$\nand $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) all\nsingular values of $D$ and $D'$ are sufficiently large, then multiplying either\n$D$ or $D'$ with a vector of iid normal Gaussians yields two statistically\nclose distributions in the sense of differential privacy. Furthermore, a small,\ndeterministic and \\emph{public} alteration of the input is enough to assert\nthat all singular values of $D$ are large.\n  We apply the Johnson-Lindenstrauss transform to the task of approximating\ncut-queries: the number of edges crossing a $(S,\\bar S)$-cut in a graph. We\nshow that the JL transform allows us to \\emph{publish a sanitized graph} that\npreserves edge differential privacy (where two graphs are neighbors if they\ndiffer on a single edge) while adding only $O(|S|/\\epsilon)$ random noise to\nany given query (w.h.p). Comparing the additive noise of our algorithm to\nexisting algorithms for answering cut-queries in a differentially private\nmanner, we outperform all others on small cuts ($|S| = o(n)$).\n  We also apply our technique to the task of estimating the variance of a given\nmatrix in any given direction. The JL transform allows us to \\emph{publish a\nsanitized covariance matrix} that preserves differential privacy w.r.t bounded\nchanges (each row in the matrix can change by at most a norm-1 vector) while\nadding random noise of magnitude independent of the size of the matrix (w.h.p).\nIn contrast, existing algorithms introduce an error which depends on the matrix\ndimensions."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2844v1", 
    "other_authors": "Julia Chuzhoy", 
    "title": "On Vertex Sparsifiers with Steiner Nodes", 
    "arxiv-id": "1204.2844v1", 
    "author": "Julia Chuzhoy", 
    "publish": "2012-04-12T21:37:48Z", 
    "summary": "Given an undirected graph $G=(V,E)$ with edge capacities $c_e\\geq 1$ for\n$e\\in E$ and a subset $T$ of $k$ vertices called terminals, we say that a graph\n$H$ is a quality-$q$ cut sparsifier for $G$ iff $T\\subseteq V(H)$, and for any\npartition $(A,B)$ of $T$, the values of the minimum cuts separating $A$ and $B$\nin graphs $G$ and $H$ are within a factor $q$ from each other. We say that $H$\nis a quality-$q$ flow sparsifier for $G$ iff $T\\subseteq V(H)$, and for any set\n$D$ of demands over the terminals, the values of the minimum edge congestion\nincurred by fractionally routing the demands in $D$ in graphs $G$ and $H$ are\nwithin a factor $q$ from each other.\n  So far vertex sparsifiers have been studied in a restricted setting where the\nsparsifier $H$ is not allowed to contain any non-terminal vertices, that is\n$V(H)=T$. For this setting, efficient algorithms are known for constructing\nquality-$O(\\log k/\\log\\log k)$ cut and flow vertex sparsifiers, as well as a\nlower bound of $\\tilde{\\Omega}(\\sqrt{\\log k})$ on the quality of any flow or\ncut sparsifier.\n  We study flow and cut sparsifiers in the more general setting where Steiner\nvertices are allowed, that is, we no longer require that $V(H)=T$. We show\nalgorithms to construct constant-quality cut sparsifiers of size $O(C^3)$ in\ntime $\\poly(n)\\cdot 2^C$, and constant-quality flow sparsifiers of size\n$C^{O(\\log\\log C)}$ in time $n^{O(\\log C)}\\cdot 2^C$, where $C$ is the total\ncapacity of the edges incident on the terminals."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2933v1", 
    "other_authors": "Stanley P. Y. Fung, Chung Keung Poon, Feifeng Zheng", 
    "title": "Improved Randomized Online Scheduling of Intervals and Jobs", 
    "arxiv-id": "1204.2933v1", 
    "author": "Feifeng Zheng", 
    "publish": "2012-04-13T09:43:27Z", 
    "summary": "We study the online preemptive scheduling of intervals and jobs (with\nrestarts). Each interval or job has an arrival time, a deadline, a length and a\nweight. The objective is to maximize the total weight of completed intervals or\njobs. While the deterministic case for intervals was settled a long time ago,\nthe randomized case remains open. In this paper we first give a 2-competitive\nrandomized algorithm for the case of equal length intervals. The algorithm is\nbarely random in the sense that it randomly chooses between two deterministic\nalgorithms at the beginning and then sticks with it thereafter. Then we extend\nthe algorithm to cover several other cases of interval scheduling including\nmonotone instances, C-benevolent instances and D-benevolent instances, giving\nthe same competitive ratio. These algorithms are surprisingly simple but have\nthe best competitive ratio against all previous (fully or barely) randomized\nalgorithms. Next we extend the idea to give a 3-competitive algorithm for equal\nlength jobs. Finally, we prove a lower bound of 2 on the competitive ratio of\nall barely random algorithms that choose between two deterministic algorithms\nfor scheduling equal length intervals (and hence jobs)."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.2955v2", 
    "other_authors": "Meng Zhang, Yi Zhang, Jijun Tang", 
    "title": "Label-Guided Graph Exploration with Adjustable Ratio of Labels", 
    "arxiv-id": "1204.2955v2", 
    "author": "Jijun Tang", 
    "publish": "2012-04-13T11:19:06Z", 
    "summary": "The graph exploration problem is to visit all the nodes of a connected graph\nby a mobile entity, e.g., a robot. The robot has no a priori knowledge of the\ntopology of the graph or of its size. Cohen et al. \\cite{Ilcinkas08} introduced\nlabel guided graph exploration which allows the system designer to add short\nlabels to the graph nodes in a preprocessing stage; these labels can guide the\nrobot in the exploration of the graph. In this paper, we address the problem of\nadjustable 1-bit label guided graph exploration. We focus on the labeling\nschemes that not only enable a robot to explore the graph but also allow the\nsystem designer to adjust the ratio of the number of different labels. This\nflexibility is necessary when maintaining different labels may have different\ncosts or when the ratio is pre-specified. We present 1-bit labeling (two\ncolors, namely black and white) schemes for this problem along with a labeling\nalgorithm for generating the required labels. Given an $n$-node graph and a\nrational number $\\rho$, we can design a 1-bit labeling scheme such that\n$n/b\\geq \\rho$ where $b$ is the number of nodes labeled black. The robot uses\n$O(\\rho\\log\\Delta)$ bits of memory for exploring all graphs of maximum degree\n$\\Delta$. The exploration is completed in time\n$O(n\\Delta^{\\frac{16\\rho+7}{3}}/\\rho+\\Delta^{\\frac{40\\rho+10}{3}})$. Moreover,\nour labeling scheme can work on graphs containing loops and multiple edges,\nwhile that of Cohen et al. focuses on simple graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110832033", 
    "link": "http://arxiv.org/pdf/1204.3113v1", 
    "other_authors": "Carlos Eduardo Ferreira, \u00c1lvaro Junio Pereira Franco", 
    "title": "Algorithms for Junctions in Directed Acyclic Graphs", 
    "arxiv-id": "1204.3113v1", 
    "author": "\u00c1lvaro Junio Pereira Franco", 
    "publish": "2012-04-13T21:43:31Z", 
    "summary": "Given a pair of distinct vertices u, v in a graph G, we say that s is a\njunction of u, v if there are in G internally vertex disjoint directed paths\nfrom s to u and from s to v. We show how to characterize junctions in directed\nacyclic graphs. We also consider the two problems in the following and derive\nefficient algorithms to solve them. Given a directed acyclic graph G and a\nvertex s in G, how can we find all pairs of vertices of G such that s is a\njunction of them? And given a directed acyclic graph G and k pairs of vertices\nof G, how can we preprocess G such that all junctions of k given pairs of\nvertices could be listed quickly? All junctions of k pairs problem arises in an\napplication in Anthropology and we apply our algorithm to find such junctions\non kinship networks of some brazilian indian ethnic groups."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.3488v3", 
    "other_authors": "Guilherme D. da Fonseca, Celina M. H. de Figueiredo, Vin\u00edcius G. P. de S\u00e1, Raphael Machado", 
    "title": "Efficient sub-5 approximations for minimum dominating sets in unit disk   graphs", 
    "arxiv-id": "1204.3488v3", 
    "author": "Raphael Machado", 
    "publish": "2012-04-16T13:59:38Z", 
    "summary": "A unit disk graph is the intersection graph of n congruent disks in the\nplane. Dominating sets in unit disk graphs are widely studied due to their\napplication in wireless ad-hoc networks. Because the minimum dominating set\nproblem for unit disk graphs is NP-hard, numerous approximation algorithms have\nbeen proposed in the literature, including some PTAS. However, since the\nproposal of a linear-time 5-approximation algorithm in 1995, the lack of\nefficient algorithms attaining better approximation factors has aroused\nattention. We introduce a linear-time O(n+m) approximation algorithm that takes\nthe usual adjacency representation of the graph as input and outputs a\n44/9-approximation. This approximation factor is also attained by a second\nalgorithm, which takes the geometric representation of the graph as input and\nruns in O(n log n) time regardless of the number of edges. Additionally, we\npropose a 43/9-approximation which can be obtained in O(n^2 m) time given only\nthe graph's adjacency representation. It is noteworthy that the dominating sets\nobtained by our algorithms are also independent sets."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4047v2", 
    "other_authors": "Joan Boyar, Sushmita Gupta, Kim S. Larsen", 
    "title": "Access Graphs Results for LRU versus FIFO under Relative Worst Order   Analysis", 
    "arxiv-id": "1204.4047v2", 
    "author": "Kim S. Larsen", 
    "publish": "2012-04-18T11:19:01Z", 
    "summary": "Access graphs, which have been used previously in connection with competitive\nanalysis to model locality of reference in paging, are considered in connection\nwith relative worst order analysis. In this model, FWF is shown to be strictly\nworse than both LRU and FIFO on any access graph. LRU is shown to be strictly\nbetter than FIFO on paths and cycles, but they are incomparable on some\nfamilies of graphs which grow with the length of the sequences."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4159v2", 
    "other_authors": "Glencora Borradaile, Seth Pettie, Christian Wulff-Nilsen", 
    "title": "Connectivity Oracles for Planar Graphs", 
    "arxiv-id": "1204.4159v2", 
    "author": "Christian Wulff-Nilsen", 
    "publish": "2012-04-18T18:46:35Z", 
    "summary": "We consider dynamic subgraph connectivity problems for planar graphs. In this\nmodel there is a fixed underlying planar graph, where each edge and vertex is\neither \"off\" (failed) or \"on\" (recovered). We wish to answer connectivity\nqueries with respect to the \"on\" subgraph. The model has two natural variants,\none in which there are $d$ edge/vertex failures that precede all connectivity\nqueries, and one in which failures/recoveries and queries are intermixed.\n  We present a $d$-failure connectivity oracle for planar graphs that processes\nany $d$ edge/vertex failures in $sort(d,n)$ time so that connectivity queries\ncan be answered in $pred(d,n)$ time. (Here $sort$ and $pred$ are the time for\ninteger sorting and integer predecessor search over a subset of $[n]$ of size\n$d$.) Our algorithm has two discrete parts. The first is an algorithm tailored\nto triconnected planar graphs. It makes use of Barnette's theorem, which states\nthat every triconnected planar graph contains a degree-3 spanning tree. The\nsecond part is a generic reduction from general (planar) graphs to triconnected\n(planar) graphs. Our algorithm is, moreover, provably optimal. An implication\nof Patrascu and Thorup's lower bound on predecessor search is that no\n$d$-failure connectivity oracle (even on trees) can beat $pred(d,n)$ query\ntime.\n  We extend our algorithms to the subgraph connectivity model where edge/vertex\nfailures (but no recoveries) are intermixed with connectivity queries. In\ntriconnected planar graphs each failure and query is handled in $O(\\log n)$\ntime (amortized), whereas in general planar graphs both bounds become $O(\\log^2\nn)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4431v1", 
    "other_authors": "Martin Aum\u00fcller, Martin Dietzfelbinger, Philipp Woelfel", 
    "title": "Explicit and Efficient Hash Families Suffice for Cuckoo Hashing with a   Stash", 
    "arxiv-id": "1204.4431v1", 
    "author": "Philipp Woelfel", 
    "publish": "2012-04-19T18:40:58Z", 
    "summary": "It is shown that for cuckoo hashing with a stash as proposed by Kirsch,\nMitzenmacher, and Wieder (2008) families of very simple hash functions can be\nused, maintaining the favorable performance guarantees: with stash size $s$ the\nprobability of a rehash is $O(1/n^{s+1})$, and the evaluation time is $O(s)$.\nInstead of the full randomness needed for the analysis of Kirsch et al. and of\nKutzelnigg (2010) (resp. $\\Theta(\\log n)$-wise independence for standard cuckoo\nhashing) the new approach even works with 2-wise independent hash families as\nbuilding blocks. Both construction and analysis build upon the work of\nDietzfelbinger and Woelfel (2003). The analysis, which can also be applied to\nthe fully random case, utilizes a graph counting argument and is much simpler\nthan previous proofs. As a byproduct, an algorithm for simulating uniform\nhashing is obtained. While it requires about twice as much space as the most\nspace efficient solutions, it is attractive because of its simple and direct\nstructure."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4526v4", 
    "other_authors": "Yuval Filmus, Justin Ward", 
    "title": "A Tight Combinatorial Algorithm for Submodular Maximization Subject to a   Matroid Constraint", 
    "arxiv-id": "1204.4526v4", 
    "author": "Justin Ward", 
    "publish": "2012-04-20T03:42:03Z", 
    "summary": "We present an optimal, combinatorial 1-1/e approximation algorithm for\nmonotone submodular optimization over a matroid constraint. Compared to the\ncontinuous greedy algorithm (Calinescu, Chekuri, Pal and Vondrak, 2008), our\nalgorithm is extremely simple and requires no rounding. It consists of the\ngreedy algorithm followed by local search. Both phases are run not on the\nactual objective function, but on a related non-oblivious potential function,\nwhich is also monotone submodular. Our algorithm runs in randomized time\nO(n^8u), where n is the rank of the given matroid and u is the size of its\nground set. We additionally obtain a 1-1/e-eps approximation algorithm running\nin randomized time O (eps^-3n^4u). For matroids in which n = o(u), this\nimproves on the runtime of the continuous greedy algorithm. The improvement is\ndue primarily to the time required by the pipage rounding phase, which we avoid\naltogether. Furthermore, the independence of our algorithm from pipage rounding\ntechniques suggests that our general approach may be helpful in contexts such\nas monotone submodular maximization subject to multiple matroid constraints.\n  Our approach generalizes to the case where the monotone submodular function\nhas restricted curvature. For any curvature c, we adapt our algorithm to\nproduce a (1-e^-c)/c approximation. This result complements results of Vondrak\n(2008), who has shown that the continuous greedy algorithm produces a\n(1-e^-c)/c approximation when the objective function has curvature c. He has\nalso proved that achieving any better approximation ratio is impossible in the\nvalue oracle model."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4666v1", 
    "other_authors": "Tsz Chiu Kwok, Lap Chi Lau", 
    "title": "Finding Small Sparse Cuts Locally by Random Walk", 
    "arxiv-id": "1204.4666v1", 
    "author": "Lap Chi Lau", 
    "publish": "2012-04-20T16:16:01Z", 
    "summary": "We study the problem of finding a small sparse cut in an undirected graph.\nGiven an undirected graph G=(V,E) and a parameter k <= |E|, the small sparsest\ncut problem is to find a subset of vertices S with minimum conductance among\nall sets with volume at most k. Using ideas developed in local graph\npartitioning algorithms, we obtain the following bicriteria approximation\nalgorithms for the small sparsest cut problem:\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance\nO(\\sqrt{\\phi/\\epsilon}) and vol(S) <= k^{1+\\epsilon} for any \\epsilon > 1/k.\n  - If there is a subset U with conductance \\phi and vol(U) <= k, then there is\na polynomial time algorithm to find a set S with conductance O(\\sqrt{\\phi\nln(k)/\\epsilon}) and vol(S) <= (1+\\epsilon)k for any \\epsilon > 2ln(k)/k.\n  These algorithms can be implemented locally using truncated random walk, with\nrunning time almost linear to the output size. This provides a local graph\npartitioning algorithm with a better conductance guarantee when k is sublinear."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4688v3", 
    "other_authors": "Ryan O'Donnell, David Witmer", 
    "title": "Markov chain methods for small-set expansion", 
    "arxiv-id": "1204.4688v3", 
    "author": "David Witmer", 
    "publish": "2012-04-20T17:52:57Z", 
    "summary": "Consider a finite irreducible Markov chain with invariant distribution $\\pi$.\nWe use the inner product induced by $\\pi$ and the associated heat operator to\nsimplify and generalize some results related to graph partitioning and the\nsmall-set expansion problem. For example, Steurer showed a tight connection\nbetween the number of small eigenvalues of a graph's Laplacian and the\nexpansion of small sets in that graph. We give a simplified proof which\ngeneralizes to the nonregular, directed case. This result implies an\napproximation algorithm for an \"analytic\" version of the Small-Set Expansion\nProblem, which, in turn, immediately gives an approximation algorithm for\nSmall-Set Expansion. We also give a simpler proof of a lower bound on the\nprobability that a random walk stays within a set; this result was used in some\nrecent works on finding small sparse cuts."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4765v1", 
    "other_authors": "Julius D'souza", 
    "title": "String Trees", 
    "arxiv-id": "1204.4765v1", 
    "author": "Julius D'souza", 
    "publish": "2012-04-21T00:36:28Z", 
    "summary": "A string-like compact data structure for unlabelled rooted trees is given\nusing 2n bits."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4835v1", 
    "other_authors": "Arash Farzan, J. Ian Munro, Rajeev Raman", 
    "title": "Succinct Indices for Range Queries with applications to Orthogonal Range   Maxima", 
    "arxiv-id": "1204.4835v1", 
    "author": "Rajeev Raman", 
    "publish": "2012-04-21T19:36:06Z", 
    "summary": "We consider the problem of preprocessing $N$ points in 2D, each endowed with\na priority, to answer the following queries: given a axis-parallel rectangle,\ndetermine the point with the largest priority in the rectangle. Using the ideas\nof the \\emph{effective entropy} of range maxima queries and \\emph{succinct\nindices} for range maxima queries, we obtain a structure that uses O(N) words\nand answers the above query in $O(\\log N \\log \\log N)$ time. This is a direct\nimprovement of Chazelle's result from FOCS 1985 for this problem -- Chazelle\nrequired $O(N/\\epsilon)$ words to answer queries in $O((\\log N)^{1+\\epsilon})$\ntime for any constant $\\epsilon > 0$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.4880v1", 
    "other_authors": "Fedor V. Fomin, Saket Saurabh, Yngve Villanger", 
    "title": "A Polynomial kernel for Proper Interval Vertex Deletion", 
    "arxiv-id": "1204.4880v1", 
    "author": "Yngve Villanger", 
    "publish": "2012-04-22T11:24:42Z", 
    "summary": "It is known that the problem of deleting at most k vertices to obtain a\nproper interval graph (Proper Interval Vertex Deletion) is fixed parameter\ntractable. However, whether the problem admits a polynomial kernel or not was\nopen. Here, we answers this question in affirmative by obtaining a polynomial\nkernel for Proper Interval Vertex Deletion. This resolves an open question of\nvan Bevern, Komusiewicz, Moser, and Niedermeier."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.5023v1", 
    "other_authors": "Niraj Kumar Singh, Mita Pal, Soubhik Chakraborty", 
    "title": "The Parameterized Complexity Analysis of Partition Sort for Negative   Binomial Distribution Inputs", 
    "arxiv-id": "1204.5023v1", 
    "author": "Soubhik Chakraborty", 
    "publish": "2012-04-23T10:53:16Z", 
    "summary": "The present paper makes a study on Partition sort algorithm for negative\nbinomial inputs. Comparing the results with those for binomial inputs in our\nprevious work, we find that this algorithm is sensitive to parameters of both\ndistributions. But the main effects as well as the interaction effects\ninvolving these parameters and the input size are more significant for negative\nbinomial case."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.5083v1", 
    "other_authors": "Niraj Kumar Singh, Soubhik Chakraborty", 
    "title": "Smart Sort: Design and Analysis of a Fast, Efficient and Robust   Comparison Based Internal Sort Algorithm", 
    "arxiv-id": "1204.5083v1", 
    "author": "Soubhik Chakraborty", 
    "publish": "2012-04-23T15:22:43Z", 
    "summary": "Smart Sort algorithm is a \"smart\" fusion of heap construction procedures (of\nHeap sort algorithm) into the conventional \"Partition\" function (of Quick sort\nalgorithm) resulting in a robust version of Quick sort algorithm. We have also\nperformed empirical analysis of average case behavior of our proposed algorithm\nalong with the necessary theoretical analysis for best and worst cases. Its\nperformance was checked against some standard probability distributions, both\nuniform and non-uniform, like Binomial, Poisson, Discrete & Continuous Uniform,\nExponential, and Standard Normal. The analysis exhibited the desired robustness\ncoupled with excellent performance of our algorithm. Although this paper\nassumes the static partition ratios, its dynamic version is expected to yield\nstill better results."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.5229v2", 
    "other_authors": "Tsvi Kopelowitz, Nimrod Talmon", 
    "title": "Selection in the Presence of Memory Faults, with Applications to   In-place Resilient Sorting", 
    "arxiv-id": "1204.5229v2", 
    "author": "Nimrod Talmon", 
    "publish": "2012-04-23T23:08:38Z", 
    "summary": "The selection problem, where one wishes to locate the $k^{th}$ smallest\nelement in an unsorted array of size $n$, is one of the basic problems studied\nin computer science. The main focus of this work is designing algorithms for\nsolving the selection problem in the presence of memory faults. These can\nhappen as the result of cosmic rays, alpha particles, or hardware failures.\n  Specifically, the computational model assumed here is a faulty variant of the\nRAM model (abbreviated as FRAM), which was introduced by Finocchi and Italiano.\nIn this model, the content of memory cells might get corrupted adversarially\nduring the execution, and the algorithm is given an upper bound $\\delta$ on the\nnumber of corruptions that may occur.\n  The main contribution of this work is a deterministic resilient selection\nalgorithm with optimal O(n) worst-case running time. Interestingly, the running\ntime does not depend on the number of faults, and the algorithm does not need\nto know $\\delta$.\n  The aforementioned resilient selection algorithm can be used to improve the\ncomplexity bounds for resilient $k$-d trees developed by Gieseke, Moruz and\nVahrenhold. Specifically, the time complexity for constructing a $k$-d tree is\nimproved from $O(n\\log^2 n + \\delta^2)$ to $O(n \\log n)$.\n  Besides the deterministic algorithm, a randomized resilient selection\nalgorithm is developed, which is simpler than the deterministic one, and has\n$O(n + \\alpha)$ expected time complexity and O(1) space complexity (i.e., is\nin-place). This algorithm is used to develop the first resilient sorting\nalgorithm that is in-place and achieves optimal $O(n\\log n + \\alpha\\delta)$\nexpected running time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.tcs.2014.01.023", 
    "link": "http://arxiv.org/pdf/1204.5442v1", 
    "other_authors": "Qin Xin", 
    "title": "Faster Treasure Hunt and Better Strongly Universal Exploration Sequences", 
    "arxiv-id": "1204.5442v1", 
    "author": "Qin Xin", 
    "publish": "2012-04-24T17:33:56Z", 
    "summary": "In this paper, we investigate the explicit deterministic treasure hunt\nproblem in a $n$-vertex network. This problem was firstly introduced by Ta-Shma\nand Zwick in \\cite{TZ07} [SODA'07]. Note also it is a variant of the well known\nrendezvous problem in which one of the robot (the treasure) is always\nstationary. In this paper, we propose an $O(n^{c(1+\\frac{1}{\\lambda})})$-time\nalgorithm for the treasure hunt problem, which significantly improves the\ncurrently best known result of running time $O(n^{2c})$ in \\cite{TZ07}, where\n$c$ is a constant induced from the construction of an universal exploration\nsequence in \\cite{R05,TZ07}, and $\\lambda \\gg 1$ is an arbitrary large, but\nfixed, integer constant. The treasure hunt problem also motivates the study of\nstrongly universal exploration sequences. In this paper, we also propose a much\nbetter explicit construction for strongly universal exploration sequences\ncompared to the one in \\cite{TZ07}."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5500v2", 
    "other_authors": "Peter Lofgren", 
    "title": "On the Complexity of the Monte Carlo Method for Incremental PageRank", 
    "arxiv-id": "1204.5500v2", 
    "author": "Peter Lofgren", 
    "publish": "2012-04-24T21:35:32Z", 
    "summary": "This note extends the analysis of incremental PageRank in [B. Bahmani, A.\nChowdhury, and A. Goel. Fast Incremental and Personalized PageRank. VLDB 2011].\nIn that work, the authors prove a running time of $O(\\frac{nR}{\\epsilon^2}\n\\ln(m))$ to keep PageRank updated over $m$ edge arrivals in a graph with $n$\nnodes when the algorithm stores $R$ random walks per node and the PageRank\nteleport probability is $\\epsilon$. To prove this running time, they assume\nthat edges arrive in a random order, and leave it to future work to extend\ntheir running time guarantees to adversarial edge arrival. In this note, we\nshow that the random edge order assumption is necessary by exhibiting a graph\nand adversarial edge arrival order in which the running time is $\\Omega \\left(R\nn m^{\\lg{\\frac{3}{2}(1-\\epsilon)}}\\right)$. More generally, for any integer $d\n\\geq 2$, we construct a graph and adversarial edge order in which the running\ntime is $\\Omega \\left(R n m^{\\log_d(H_d (1-\\epsilon))}\\right)$, where $H_d$ is\nthe $d$th harmonic number."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5524v3", 
    "other_authors": "Jun'ichi Yamamoto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", 
    "title": "Time and Space Efficient Lempel-Ziv Factorization based on Run Length   Encoding", 
    "arxiv-id": "1204.5524v3", 
    "author": "Masayuki Takeda", 
    "publish": "2012-04-25T01:07:13Z", 
    "summary": "We propose a new approach for calculating the Lempel-Ziv factorization of a\nstring, based on run length encoding (RLE). We present a conceptually simple\noff-line algorithm based on a variant of suffix arrays, as well as an on-line\nalgorithm based on a variant of directed acyclic word graphs (DAWGs). Both\nalgorithms run in $O(N+n\\log n)$ time and O(n) extra space, where N is the size\nof the string, $n\\leq N$ is the number of RLE factors. The time dependency on N\nis only in the conversion of the string to RLE, which can be computed very\nefficiently in O(N) time and O(1) extra space (excluding the output). When the\nstring is compressible via RLE, i.e., $n = o(N)$, our algorithms are, to the\nbest of our knowledge, the first algorithms which require only o(N) extra space\nwhile running in $o(N\\log N)$ time."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5613v1", 
    "other_authors": "Paul Bonsma", 
    "title": "Rerouting shortest paths in planar graphs", 
    "arxiv-id": "1204.5613v1", 
    "author": "Paul Bonsma", 
    "publish": "2012-04-25T10:59:41Z", 
    "summary": "A rerouting sequence is a sequence of shortest st-paths such that consecutive\npaths differ in one vertex. We study the the Shortest Path Rerouting Problem,\nwhich asks, given two shortest st-paths P and Q in a graph G, whether a\nrerouting sequence exists from P to Q. This problem is PSPACE-hard in general,\nbut we show that it can be solved in polynomial time if G is planar. To this\nend, we introduce a dynamic programming method for reconfiguration problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5801v6", 
    "other_authors": "J\u00e9r\u00e9my Barbay", 
    "title": "Optimal Prefix Free Code in Linear Time", 
    "arxiv-id": "1204.5801v6", 
    "author": "J\u00e9r\u00e9my Barbay", 
    "publish": "2012-04-26T01:27:44Z", 
    "summary": "We describe an algorithm computing an optimal prefix free code from $N$\nunsorted positive integer weights in time linear in the number of machine words\nholding those weights. This algorithm takes advantage of common non-algebraic\ninstructions, and of specific results on optimal prefix free codes. This result\nimproves over the state of the art complexities of $O(N\\lg N)$ in the algebraic\ndecision tree model and $O(N\\lg\\lg N)$ in the RAM model for the computation of\nHuffman's codes, a landmark in compression and coding since 1952."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5823v1", 
    "other_authors": "Siddharth Barman, Shuchi Chawla, Seeun Umboh", 
    "title": "A Bicriteria Approximation for the Reordering Buffer Problem", 
    "arxiv-id": "1204.5823v1", 
    "author": "Seeun Umboh", 
    "publish": "2012-04-26T03:33:41Z", 
    "summary": "In the reordering buffer problem (RBP), a server is asked to process a\nsequence of requests lying in a metric space. To process a request the server\nmust move to the corresponding point in the metric. The requests can be\nprocessed slightly out of order; in particular, the server has a buffer of\ncapacity k which can store up to k requests as it reads in the sequence. The\ngoal is to reorder the requests in such a manner that the buffer constraint is\nsatisfied and the total travel cost of the server is minimized. The RBP arises\nin many applications that require scheduling with a limited buffer capacity,\nsuch as scheduling a disk arm in storage systems, switching colors in paint\nshops of a car manufacturing plant, and rendering 3D images in computer\ngraphics.\n  We study the offline version of RBP and develop bicriteria approximations.\nWhen the underlying metric is a tree, we obtain a solution of cost no more than\n9OPT using a buffer of capacity 4k + 1 where OPT is the cost of an optimal\nsolution with buffer capacity k. Constant factor approximations were known\npreviously only for the uniform metric (Avigdor-Elgrabli et al., 2012). Via\nrandomized tree embeddings, this implies an O(log n) approximation to cost and\nO(1) approximation to buffer size for general metrics. Previously the best\nknown algorithm for arbitrary metrics by Englert et al. (2007) provided an\nO(log^2 k log n) approximation without violating the buffer constraint."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.5834v2", 
    "other_authors": "Antonio Blanca, Milena Mihail", 
    "title": "Efficient Generation \u03b5-close to G(n,p) and Generalizations", 
    "arxiv-id": "1204.5834v2", 
    "author": "Milena Mihail", 
    "publish": "2012-04-26T06:06:52Z", 
    "summary": "We give an efficient algorithm to generate a graph from a distribution\n$\\epsilon$-close to $G(n,p)$, in the sense of total variation distance. In\nparticular, if $p$ is represented with $O(\\log n)$-bit accuracy, then, with\nhigh probability, the running time is linear in the expected number of edges of\nthe output graph (up to poly-logarithmic factors). All our running times\ninclude the complexity of the arithmetic involved in the corresponding\nalgorithms. Previous standard methods for exact $G(n,p)$ sampling (see e.g.\nBatagelj and Brandes, 2005) achieve similar running times, however, under the\nassumption that performing real number arithmetic with arbitrary accuracy takes\nconstant time. We note that the actual accuracy required by these methods is\nO(n)-bit per step, which results in quadratic running times.\n  The main idea of our $G(n,p)$ generation algorithm is a Metropolis Markov\nchain to sample $\\epsilon$-close from the binomial distribution. This is a new\nmethod for sampling from the binomial distribution: it is of separate interest\nand may find other useful applications. Our analysis accounts for all necessary\nbit-accuracy and arithmetic, and our running times are comparable to known\nmethods for exact binomial sampling.\n  We further obtain efficient generation algorithms for random graphs with\ngiven arbitrary degree distributions, Inhomogeneous Random Graphs when the\nkernel function is the inner product, and Stochastic Kronecker Graphs. To the\nbest our knowledge, our work can be viewed as the first effort to simulate\nefficient generation of graphs from classical random graph models, while taking\ninto account implementational considerations as fundamental computational\naspects, and quantifying the tradeoff between accuracy and running time in a\nway that can be useful in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1204.6070v2", 
    "other_authors": "James Abello, Pavel Klav\u00edk, Jan Kratochv\u00edl, Tom\u00e1\u0161 Vysko\u010dil", 
    "title": "MSOL Restricted Contractibility to Planar Graphs", 
    "arxiv-id": "1204.6070v2", 
    "author": "Tom\u00e1\u0161 Vysko\u010dil", 
    "publish": "2012-04-26T22:46:55Z", 
    "summary": "We study the computational complexity of graph planarization via edge\ncontraction. The problem CONTRACT asks whether there exists a set $S$ of at\nmost $k$ edges that when contracted produces a planar graph. We work with a\nmore general problem called $P$-RESTRICTEDCONTRACT in which $S$, in addition,\nis required to satisfy a fixed MSOL formula $P(S,G)$. We give an FPT algorithm\nin time $O(n^2 f(k))$ which solves $P$-RESTRICTEDCONTRACT, where $P(S,G)$ is\n(i) inclusion-closed and (ii) inert contraction-closed (where inert edges are\nthe edges non-incident to any inclusion minimal solution $S$).\n  As a specific example, we can solve the $\\ell$-subgraph contractibility\nproblem in which the edges of a set $S$ are required to form disjoint connected\nsubgraphs of size at most $\\ell$. This problem can be solved in time $O(n^2\nf'(k,\\ell))$ using the general algorithm. We also show that for $\\ell \\ge 2$\nthe problem is NP-complete."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.0043v2", 
    "other_authors": "Bundit Laekhanukit, Adrian Vetta, Gordon Wilfong", 
    "title": "Routing Regardless of Network Stability", 
    "arxiv-id": "1207.0043v2", 
    "author": "Gordon Wilfong", 
    "publish": "2012-06-30T04:35:52Z", 
    "summary": "We examine the effectiveness of packet routing in this model for the broad\nclass next-hop preferences with filtering. Here each node v has a filtering\nlist D(v) consisting of nodes it does not want its packets to route through.\nAcceptable paths (those that avoid nodes in the filtering list) are ranked\naccording to the next-hop, that is, the neighbour of v that the path begins\nwith. On the negative side, we present a strong inapproximability result. For\nfiltering lists of cardinality at most one, given a network in which an\nequilibrium is guaranteed to exist, it is NP-hard to approximate the maximum\nnumber of packets that can be routed to within a factor of O(n^{1-\\epsilon}),\nfor any constant \\epsilon >0. On the positive side, we give algorithms to show\nthat in two fundamental cases every packet will eventually route with\nprobability one. The first case is when each node's filtering list contains\nonly itself, that is, D(v)={v}. Moreover, with positive probability every\npacket will be routed before the control plane reaches an equilibrium. The\nsecond case is when all the filtering lists are empty, that is,\n$\\mathcal{D}(v)=\\emptyset$. Thus, with probability one packets will route even\nwhen the nodes don't care if their packets cycle! Furthermore, with probability\none every packet will route even when the control plane has em no equilibrium\nat all."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.0271v3", 
    "other_authors": "Shuxin Cai, Wenguo Yang, Yaohua Tang", 
    "title": "Approximating Soft-Capacitated Facility Location Problem With   Uncertainty", 
    "arxiv-id": "1207.0271v3", 
    "author": "Yaohua Tang", 
    "publish": "2012-07-02T03:22:06Z", 
    "summary": "We first show that a better analysis of the algorithm for The Two-Sage\nStochastic Facility Location Problem from Srinivasan \\cite{sri07} and the\nalgorithm for The Robust Fault Tolerant Facility Location Problem from Byrka et\nal \\cite{bgs10} can render improved approximation factors of 2.206 and \\alpha+4\nwhere \\alpha is the maximum number an adversary can close, respectively, and\nwhich are the best ratios so far.\n  We then present new models for the soft-capacitated facility location problem\nwith uncertainty and design constant factor approximation algorithms to solve\nthem. We devise the stochastic and robust approaches to handle the uncertainty\nincorporated into the original model. Explicitly, in this paper we propose two\nnew problem, named The 2-Stage Soft-Capacitated Facility Location Problem and\nThe Robust Soft-Capacitated Facility Location Problem respectively, and present\nconstant factor approximation algorithms for them both. Our method uses\nreductions between facility location problems and linear-cost models, the\nrandomized thresholding technique of Srinivasan \\cite{sri07} and the filtering\nand clustering technique of Byrka et al \\cite{bgs10}."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.0316v1", 
    "other_authors": "Angsheng Li, Peng Zhang", 
    "title": "Algorithmic Aspects of Homophyly of Networks", 
    "arxiv-id": "1207.0316v1", 
    "author": "Peng Zhang", 
    "publish": "2012-07-02T09:31:32Z", 
    "summary": "We investigate the algorithmic problems of the {\\it homophyly phenomenon} in\nnetworks. Given an undirected graph $G = (V, E)$ and a vertex coloring $c\n\\colon V \\rightarrow {1, 2, ..., k}$ of $G$, we say that a vertex $v\\in V$ is\n{\\it happy} if $v$ shares the same color with all its neighbors, and {\\it\nunhappy}, otherwise, and that an edge $e\\in E$ is {\\it happy}, if its two\nendpoints have the same color, and {\\it unhappy}, otherwise. Supposing $c$ is a\n{\\it partial vertex coloring} of $G$, we define the Maximum Happy Vertices\nproblem (MHV, for short) as to color all the remaining vertices such that the\nnumber of happy vertices is maximized, and the Maximum Happy Edges problem\n(MHE, for short) as to color all the remaining vertices such that the number of\nhappy edges is maximized.\n  Let $k$ be the number of colors allowed in the problems. We show that both\nMHV and MHE can be solved in polynomial time if $k = 2$, and that both MHV and\nMHE are NP-hard if $k \\geq 3$. We devise a $\\max {1/k,\n\\Omega(\\Delta^{-3})}$-approximation algorithm for the MHV problem, where\n$\\Delta$ is the maximum degree of vertices in the input graph, and a\n1/2-approximation algorithm for the MHE problem. This is the first theoretical\nprogress of these two natural and fundamental new problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.0370v1", 
    "other_authors": "Abderrahmane Euldji, Abderrahim Tienti, Amine Boudghene Stambouli", 
    "title": "A new path algorithm for the weighted multi-graphs WMGPA: application to   the Direct Topological Method", 
    "arxiv-id": "1207.0370v1", 
    "author": "Amine Boudghene Stambouli", 
    "publish": "2012-07-02T13:20:45Z", 
    "summary": "The aim of this paper is to present an algorithm which gives all the possible\npaths that start from a specific node to another of a weighted multi-graph.\nThis algorithm is intended to be applied for the direct topological method."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.0892v1", 
    "other_authors": "T-H. Hubert Chan, Mingfei Li, Li Ning", 
    "title": "Incubators vs Zombies: Fault-Tolerant, Short, Thin and Lanky Spanners   for Doubling Metrics", 
    "arxiv-id": "1207.0892v1", 
    "author": "Li Ning", 
    "publish": "2012-07-04T03:58:39Z", 
    "summary": "Recently Elkin and Solomon gave a construction of spanners for doubling\nmetrics that has constant maximum degree, hop-diameter O(log n) and lightness\nO(log n) (i.e., weight O(log n)w(MST). This resolves a long standing conjecture\nproposed by Arya et al. in a seminal STOC 1995 paper.\n  However, Elkin and Solomon's spanner construction is extremely complicated;\nwe offer a simple alternative construction that is very intuitive and is based\non the standard technique of net tree with cross edges. Indeed, our approach\ncan be readily applied to our previous construction of k-fault tolerant\nspanners (ICALP 2012) to achieve k-fault tolerance, maximum degree O(k^2),\nhop-diameter O(log n) and lightness O(k^3 log n)."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1135v1", 
    "other_authors": "Philip Bille, Inge Li G\u00f8rtz, Tsvi Kopelowitz, Benjamin Sach, Hjalte Wedel Vildh\u00f8j", 
    "title": "Sparse Suffix Tree Construction with Small Space", 
    "arxiv-id": "1207.1135v1", 
    "author": "Hjalte Wedel Vildh\u00f8j", 
    "publish": "2012-07-04T22:41:51Z", 
    "summary": "We consider the problem of constructing a sparse suffix tree (or suffix\narray) for $b$ suffixes of a given text $T$ of size $n$, using only $O(b)$\nwords of space during construction time. Breaking the naive bound of\n$\\Omega(nb)$ time for this problem has occupied many algorithmic researchers\nsince a different structure, the (evenly spaced) sparse suffix tree, was\nintroduced by K{\\\"a}rkk{\\\"a}inen and Ukkonen in 1996. While in the evenly\nspaced sparse suffix tree the suffixes considered must be evenly spaced in $T$,\nhere there is no constraint on the locations of the suffixes.\n  We show that the sparse suffix tree can be constructed in $O(n\\log^2b)$ time.\nTo achieve this we develop a technique, which may be of independent interest,\nthat allows to efficiently answer $b$ longest common prefix queries on suffixes\nof $T$, using only $O(b)$ space. We expect that this technique will prove\nuseful in many other applications in which space usage is a concern.\nFurthermore, additional tradeoffs between the space usage and the construction\ntime are given."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1141v2", 
    "other_authors": "Nicolaos Matsakis", 
    "title": "The Longest Queue Drop Policy for Shared-Memory Switches is   1.5-competitive", 
    "arxiv-id": "1207.1141v2", 
    "author": "Nicolaos Matsakis", 
    "publish": "2012-07-04T23:22:31Z", 
    "summary": "We consider the Longest Queue Drop memory management policy in shared-memory\nswitches consisting of $N$ output ports. The shared memory of size $M\\geq N$\nmay have an arbitrary number of input ports. Each packet may be admitted by any\nincoming port, but must be destined to a specific output port and each output\nport may be used by only one queue. The Longest Queue Drop policy is a natural\nonline strategy used in directing the packet flow in buffering problems.\nAccording to this policy and assuming unit packet values and cost of\ntransmission, every incoming packet is accepted, whereas if the shared memory\nbecomes full, one or more packets belonging to the longest queue are preempted,\nin order to make space for the newly arrived packets. It was proved in 2001\n[Hahne et al., SPAA '01] that the Longest Queue Drop policy is 2-competitive\nand at least $\\sqrt{2}$-competitive. It remained an open question whether a\n(2-\\epsilon) upper bound for the competitive ratio of this policy could be\nshown, for any positive constant \\epsilon. We show that the Longest Queue Drop\nonline policy is 1.5-competitive."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1277v2", 
    "other_authors": "Ofer Neiman, Shay Solomon", 
    "title": "Simple Deterministic Algorithms for Fully Dynamic Maximal Matching", 
    "arxiv-id": "1207.1277v2", 
    "author": "Shay Solomon", 
    "publish": "2012-07-05T14:50:31Z", 
    "summary": "A maximal matching can be maintained in fully dynamic (supporting both\naddition and deletion of edges) $n$-vertex graphs using a trivial deterministic\nalgorithm with a worst-case update time of O(n). No deterministic algorithm\nthat outperforms the na\\\"{\\i}ve O(n) one was reported up to this date. The only\nprogress in this direction is due to Ivkovi\\'{c} and Lloyd \\cite{IL93}, who in\n1993 devised a deterministic algorithm with an \\emph{amortized} update time of\n$O((n+m)^{\\sqrt{2}/2})$, where $m$ is the number of edges.\n  In this paper we show the first deterministic fully dynamic algorithm that\noutperforms the trivial one. Specifically, we provide a deterministic\n\\emph{worst-case} update time of $O(\\sqrt{m})$. Moreover, our algorithm\nmaintains a matching which is in fact a 3/2-approximate maximum cardinality\nmatching (MCM). We remark that no fully dynamic algorithm for maintaining\n$(2-\\eps)$-approximate MCM improving upon the na\\\"{\\i}ve O(n) was known prior\nto this work, even allowing amortized time bounds and \\emph{randomization}.\n  For low arboricity graphs (e.g., planar graphs and graphs excluding fixed\nminors), we devise another simple deterministic algorithm with\n\\emph{sub-logarithmic update time}. Specifically, it maintains a fully dynamic\nmaximal matching with amortized update time of $O(\\log n/\\log \\log n)$. This\nresult addresses an open question of Onak and Rubinfeld \\cite{OR10}.\n  We also show a deterministic algorithm with optimal space usage, that for\narbitrary graphs maintains a maximal matching in amortized $O(\\sqrt{m})$ time,\nand uses only $O(n+m)$ space."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1307v1", 
    "other_authors": "Michalis Christou, Maxime Crochemore, Costas S. Iliopoulos", 
    "title": "Identifying all abelian periods of a string in quadratic time and   relevant problems", 
    "arxiv-id": "1207.1307v1", 
    "author": "Costas S. Iliopoulos", 
    "publish": "2012-07-05T17:43:50Z", 
    "summary": "Abelian periodicity of strings has been studied extensively over the last\nyears. In 2006 Constantinescu and Ilie defined the abelian period of a string\nand several algorithms for the computation of all abelian periods of a string\nwere given. In contrast to the classical period of a word, its abelian version\nis more flexible, factors of the word are considered the same under any\ninternal permutation of their letters. We show two O(|y|^2) algorithms for the\ncomputation of all abelian periods of a string y. The first one maps each\nletter to a suitable number such that each factor of the string can be\nidentified by the unique sum of the numbers corresponding to its letters and\nhence abelian periods can be identified easily. The other one maps each letter\nto a prime number such that each factor of the string can be identified by the\nunique product of the numbers corresponding to its letters and so abelian\nperiods can be identified easily. We also define weak abelian periods on\nstrings and give an O(|y|log(|y|)) algorithm for their computation, together\nwith some other algorithms for more basic problems."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1371v1", 
    "other_authors": "Shuchi Chawla, Cynthia Dwork, Frank McSherry, Kunal Talwar", 
    "title": "On Privacy-Preserving Histograms", 
    "arxiv-id": "1207.1371v1", 
    "author": "Kunal Talwar", 
    "publish": "2012-07-04T16:06:07Z", 
    "summary": "We advance the approach initiated by Chawla et al. for sanitizing (census)\ndata so as to preserve the privacy of respondents while simultaneously\nextracting \"useful\" statistical information. First, we extend the scope of\ntheir techniques to a broad and rich class of distributions, specifically,\nmixtures of highdimensional balls, spheres, Gaussians, and other \"nice\"\ndistributions. Second, we randomize the histogram constructions to preserve\nspatial characteristics of the data, allowing us to approximate various\nquantities of interest, e.g., cost of the minimum spanning tree on the data, in\na privacy-preserving fashion."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1668v1", 
    "other_authors": "Michael Elkin, Shay Solomon", 
    "title": "Fast Constructions of Light-Weight Spanners for General Graphs", 
    "arxiv-id": "1207.1668v1", 
    "author": "Shay Solomon", 
    "publish": "2012-07-06T15:46:23Z", 
    "summary": "To our knowledge, there are only two known algorithms for constructing sparse\nand light spanners for general graphs. One of them is the greedy algorithm of\nAlth$\\ddot{o}$fer et al. \\cite{ADDJS93}, analyzed by Chandra et al. in SoCG'92.\nThe greedy algorithm consructs, for every \\emph{weighted} undirected $n$-vertex\n$m$-edge graph $G = (V,E)$ and any integer $k \\ge 1$, a $(2k-1)$-spanner with\n$O(n^{1 + 1/k})$ edges and weight $O(k \\cdot n^{(1+\\eps)/k}) \\cdot\n\\omega(MST(G))$, for any $\\eps > 0$. The drawback of the greedy algorithm is\nthat it requires $O(m \\cdot (n^{1 + 1/k} + n \\cdot \\log n))$ time. The other\nalgorithm is due to Awerbuch et al. \\cite{ABP91}. It constructs $O(k)$-spanners\nwith $O(k \\cdot n^{1 + 1/k} \\cdot \\Lambda)$ edges, weight $O(k^2 \\cdot n^{1/k}\n\\cdot \\Lambda) \\cdot \\omega(MST(G))$, within time $O(m \\cdot k \\cdot n^{1/k}\n\\cdot \\Lambda)$, where $\\Lambda$ is the logarithm of the aspect ratio of the\ngraph. The running time of both these algorithms is unsatisfactory. Moreover,\nthe usually faster algorithm of \\cite{ABP91} pays for the speedup by\nsignificantly increasing both the stretch, the sparsity, and the weight of the\nresulting spanner.\n  In this paper we devise an efficient algorithm for constructing sparse and\nlight spanners. Specifically, our algorithm constructs $((2k-1) \\cdot\n(1+\\eps))$-spanners with $O(k \\cdot n^{1 + 1/k})$ edges and weight $O(k \\cdot\nn^{1/k}) \\cdot \\omega(MST(G))$, where $\\eps > 0$ is an arbitrarily small\nconstant. The running time of our algorithm is $O(k \\cdot m + \\min\\{n \\cdot\n\\log n,m \\cdot \\alpha(n)\\})$. Moreover, by slightly increasing the running time\nwe can reduce the other parameters. These results address an open problem from\nthe ESA'04 paper by Roditty and Zwick \\cite{RZ04}."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.1831v4", 
    "other_authors": "Michael Elkin, Shay Solomon", 
    "title": "Optimal Euclidean spanners: really short, thin and lanky", 
    "arxiv-id": "1207.1831v4", 
    "author": "Shay Solomon", 
    "publish": "2012-07-07T22:39:46Z", 
    "summary": "In a seminal STOC'95 paper, titled \"Euclidean spanners: short, thin and\nlanky\", Arya et al. devised a construction of Euclidean $(1+\\eps)$-spanners\nthat achieves constant degree, diameter $O(\\log n)$, and weight $O(\\log^2 n)\n\\cdot \\omega(MST)$, and has running time $O(n \\cdot \\log n)$. This construction\napplies to $n$-point constant-dimensional Euclidean spaces. Moreover, Arya et\nal. conjectured that the weight bound can be improved by a logarithmic factor,\nwithout increasing the degree and the diameter of the spanner, and within the\nsame running time.\n  This conjecture of Arya et al. became a central open problem in the area of\nEuclidean spanners.\n  In this paper we resolve the long-standing conjecture of Arya et al. in the\naffirmative. Specifically, we present a construction of spanners with the same\nstretch, degree, diameter, and running time, as in Arya et al.'s result, but\nwith optimal weight $O(\\log n) \\cdot \\omega(MST)$.\n  Moreover, our result is more general in three ways. First, we demonstrate\nthat the conjecture holds true not only in constant-dimensional Euclidean\nspaces, but also in doubling metrics. Second, we provide a general tradeoff\nbetween the three involved parameters, which is tight in the entire range.\nThird, we devise a transformation that decreases the lightness of spanners in\ngeneral metrics, while keeping all their other parameters in check. Our main\nresult is obtained as a corollary of this transformation."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.2341v1", 
    "other_authors": "Casper Kejlberg-Rasmussen, Konstantinos Tsakalidis, Kostas Tsichlas", 
    "title": "I/O-Efficient Dynamic Planar Range Skyline Queries", 
    "arxiv-id": "1207.2341v1", 
    "author": "Kostas Tsichlas", 
    "publish": "2012-07-10T13:32:12Z", 
    "summary": "We present the first fully dynamic worst case I/O-efficient data structures\nthat support planar orthogonal \\textit{3-sided range skyline reporting queries}\nin $\\bigO (\\log_{2B^\\epsilon} n + \\frac{t}{B^{1-\\epsilon}})$ I/Os and updates\nin $\\bigO (\\log_{2B^\\epsilon} n)$ I/Os, using $\\bigO\n(\\frac{n}{B^{1-\\epsilon}})$ blocks of space, for $n$ input planar points, $t$\nreported points, and parameter $0 \\leq \\epsilon \\leq 1$. We obtain the result\nby extending Sundar's priority queues with attrition to support the operations\n\\textsc{DeleteMin} and \\textsc{CatenateAndAttrite} in $\\bigO (1)$ worst case\nI/Os, and in $\\bigO(1/B)$ amortized I/Os given that a constant number of blocks\nis already loaded in main memory. Finally, we show that any pointer-based\nstatic data structure that supports \\textit{dominated maxima reporting\nqueries}, namely the difficult special case of 4-sided skyline queries, in\n$\\bigO(\\log^{\\bigO(1)}n +t)$ worst case time must occupy $\\Omega(n \\frac{\\log\nn}{\\log \\log n})$ space, by adapting a similar lower bounding argument for\nplanar 4-sided range reporting queries."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.2632v2", 
    "other_authors": "Rahul Shah, Cheng Sheng, Sharma V. Thankachan, Jeffrey Scott Vitter", 
    "title": "On Optimal Top-K String Retrieval", 
    "arxiv-id": "1207.2632v2", 
    "author": "Jeffrey Scott Vitter", 
    "publish": "2012-07-11T13:30:06Z", 
    "summary": "Let ${\\cal{D}}$ = $\\{d_1, d_2, d_3, ..., d_D\\}$ be a given set of $D$\n(string) documents of total length $n$. The top-$k$ document retrieval problem\nis to index $\\cal{D}$ such that when a pattern $P$ of length $p$, and a\nparameter $k$ come as a query, the index returns the $k$ most relevant\ndocuments to the pattern $P$. Hon et. al. \\cite{HSV09} gave the first linear\nspace framework to solve this problem in $O(p + k\\log k)$ time. This was\nimproved by Navarro and Nekrich \\cite{NN12} to $O(p + k)$. These results are\npowerful enough to support arbitrary relevance functions like frequency,\nproximity, PageRank, etc. In many applications like desktop or email search,\nthe data resides on disk and hence disk-bound indexes are needed. Despite of\ncontinued progress on this problem in terms of theoretical, practical and\ncompression aspects, any non-trivial bounds in external memory model have so\nfar been elusive. Internal memory (or RAM) solution to this problem decomposes\nthe problem into $O(p)$ subproblems and thus incurs the additive factor of\n$O(p)$. In external memory, these approaches will lead to $O(p)$ I/Os instead\nof optimal $O(p/B)$ I/O term where $B$ is the block-size. We re-interpret the\nproblem independent of $p$, as interval stabbing with priority over tree-shaped\nstructure. This leads us to a linear space index in external memory supporting\ntop-$k$ queries (with unsorted outputs) in near optimal $O(p/B + \\log_B n +\n\\log^{(h)} n + k/B)$ I/Os for any constant $h${$\\log^{(1)}n =\\log n$ and\n$\\log^{(h)} n = \\log (\\log^{(h-1)} n)$}. Then we get $O(n\\log^*n)$ space index\nwith optimal $O(p/B+\\log_B n + k/B)$ I/Os."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.ipl.2013.11.006", 
    "link": "http://arxiv.org/pdf/1207.3564v1", 
    "other_authors": "Yitong Yin, Chihao Zhang", 
    "title": "Approximate Counting via Correlation Decay on Planar Graphs", 
    "arxiv-id": "1207.3564v1", 
    "author": "Chihao Zhang", 
    "publish": "2012-07-16T02:35:37Z", 
    "summary": "We show for a broad class of counting problems, correlation decay (strong\nspatial mixing) implies FPTAS on planar graphs. The framework for the counting\nproblems considered by us is the Holant problems with arbitrary constant-size\ndomain and symmetric constraint functions. We define a notion of regularity on\nthe constraint functions, which covers a wide range of natural and important\ncounting problems, including all multi-state spin systems, counting graph\nhomomorphisms, counting weighted matchings or perfect matchings, the subgraphs\nworld problem transformed from the ferromagnetic Ising model, and all counting\nCSPs and Holant problems with symmetric constraint functions of constant arity.\n  The core of our algorithm is a fixed-parameter tractable algorithm which\ncomputes the exact values of the Holant problems with regular constraint\nfunctions on graphs of bounded treewidth. By utilizing the locally tree-like\nproperty of apex-minor-free families of graphs, the parameterized exact\nalgorithm implies an FPTAS for the Holant problem on these graph families\nwhenever the Gibbs measure defined by the problem exhibits strong spatial\nmixing. We further extend the recursive coupling technique to Holant problems\nand establish strong spatial mixing for the ferromagnetic Potts model and the\nsubgraphs world problem. As consequences, we have new deterministic\napproximation algorithms on planar graphs and all apex-minor-free graphs for\nseveral counting problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.3807v1", 
    "other_authors": "Hao-Hsiang Hung", 
    "title": "Light Spanner and Monotone Tree", 
    "arxiv-id": "1207.3807v1", 
    "author": "Hao-Hsiang Hung", 
    "publish": "2012-07-16T20:01:55Z", 
    "summary": "In approximation algorithm design, light spanners has applications in\ngraph-metric problems such as metric TSP (the traveling salesman problem). We\nhave developed an efficient algorithm for light spanners in bounded pathwidth\ngraphs, based on an intermediate data structure called monotone tree. In this\npaper, we extended the results to include bounded catwidth graphs."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.3976v3", 
    "other_authors": "Abhash Anand, Surender Baswana, Manoj Gupta, Sandeep Sen", 
    "title": "Maintaining Approximate Maximum Weighted Matching in Fully Dynamic   Graphs", 
    "arxiv-id": "1207.3976v3", 
    "author": "Sandeep Sen", 
    "publish": "2012-07-17T12:51:55Z", 
    "summary": "We present a fully dynamic algorithm for maintaining approximate maximum\nweight matching in general weighted graphs. The algorithm maintains a matching\n${\\cal M}$ whose weight is at least $1/8 M^{*}$ where $M^{*}$ is the weight of\nthe maximum weight matching. The algorithm achieves an expected amortized\n$O(\\log n \\log \\mathcal C)$ time per edge insertion or deletion, where\n$\\mathcal C$ is the ratio of the weights of the highest weight edge to the\nsmallest weight edge in the given graph. Using a simple randomized scaling\ntechnique, we are able to obtain a matching whith expected approximation ratio\n4.9108."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.4079v2", 
    "other_authors": "Rajesh Chitnis, Marek Cygan, MohammadTaghi Hajiaghayi, Marcin Pilipczuk, Micha\u0142 Pilipczuk", 
    "title": "Designing FPT algorithms for cut problems using randomized contractions", 
    "arxiv-id": "1207.4079v2", 
    "author": "Micha\u0142 Pilipczuk", 
    "publish": "2012-07-17T18:24:13Z", 
    "summary": "We introduce a new technique for designing fixed-parameter algorithms for cut\nproblems, namely randomized contractions. We apply our framework to obtain the\nfirst FPT algorithm for the Unique Label Cover problem and new FPT algorithms\nwith exponential speed up for the Steiner Cut and Node Multiway Cut-Uncut\nproblems. More precisely, we show the following:\n  - We prove that the parameterized version of the Unique Label Cover problem,\nwhich is the base of the Unique Games Conjecture, can be solved in 2^{O(k^2\\log\n|\\Sigma|)}n^4\\log n deterministic time (even in the stronger, vertex-deletion\nvariant) where k is the number of unsatisfied edges and |\\Sigma| is the size of\nthe alphabet. As a consequence, we show that one can in polynomial time solve\ninstances of Unique Games where the number of edges allowed not to be satisfied\nis upper bounded by O(\\sqrt{\\log n}) to optimality, which improves over the\ntrivial O(1) upper bound.\n  - We prove that the Steiner Cut problem can be solved in 2^{O(k^2\\log\nk)}n^4\\log n deterministic time and \\tilde{O}(2^{O(k^2\\log k)}n^2) randomized\ntime where k is the size of the cutset. This result improves the double\nexponential running time of the recent work of Kawarabayashi and Thorup\n(FOCS'11).\n  - We show how to combine considering `cut' and `uncut' constraints at the\nsame time. More precisely, we define a robust problem Node Multiway Cut-Uncut\nthat can serve as an abstraction of introducing uncut constraints, and show\nthat it admits an algorithm running in 2^{O(k^2\\log k)}n^4\\log n deterministic\ntime where k is the size of the cutset. To the best of our knowledge, the only\nknown way of tackling uncut constraints was via the approach of Marx,\nO'Sullivan and Razgon (STACS'10), which yields algorithms with double\nexponential running time.\n  An interesting aspect of our technique is that, unlike important separators,\nit can handle real weights."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.4366v1", 
    "other_authors": "Zeev Nutov", 
    "title": "Approximating minimum-cost edge-covers of crossing biset-families", 
    "arxiv-id": "1207.4366v1", 
    "author": "Zeev Nutov", 
    "publish": "2012-07-18T13:08:39Z", 
    "summary": "An ordered pair $\\hat{S}=(S,S^+)$ of subsets of $V$ is called a {\\em biset}\nif $S \\subseteq S^+$; $(V-S^+,V-S)$ is the co-biset of $\\hat{S}$. Two bisets\n$\\hat{X},\\hat{Y}$ intersect if $X \\cap Y \\neq \\emptyset$ and cross if both $X\n\\cap Y \\neq \\emptyset$ and $X^+ \\cup Y^+ \\neq V$. The intersection and the\nunion of two bisets $\\hat{X},\\hat{Y}$ is defined by $\\hat{X} \\cap \\hat{Y} = (X\n\\cap Y, X^+ \\cap Y^+)$ and $\\hat{X} \\cup \\hat{Y} = (X \\cup Y,X^+ \\cup Y^+)$. A\nbiset-family ${\\cal F}$ is crossing (intersecting) if $\\hat{X} \\cap \\hat{Y},\n\\hat{X} \\cup \\hat{Y} \\in {\\cal F}$ for any $\\hat{X},\\hat{Y} \\in {\\cal F}$ that\ncross (intersect). A directed edge covers a biset $\\hat{S}$ if it goes from $S$\nto $V-S^+$. We consider the problem of covering a crossing biset-family ${\\cal\nF}$ by a minimum-cost set of directed edges. While for intersecting ${\\cal F}$,\na standard primal-dual algorithm computes an optimal solution, the\napproximability of the case of crossing ${\\cal F}$ is not yet understood, as it\nincludes several NP-hard problems, for which a poly-logarithmic approximation\nwas discovered only recently. Let us say that a biset-family ${\\cal F}$ is\n$k$-regular if $\\hat{X} \\cap \\hat{Y}, \\hat{X} \\cup \\hat{Y} \\in {\\cal F}$ for\nany $\\hat{X},\\hat{Y} \\in {\\cal F}$ with $|V-(X \\cup Y)| \\geq k+1$ that\nintersect. In this paper we obtain an $O(\\log |V|)$-approximation algorithm for\narbitrary crossing ${\\cal F}$; if in addition both ${\\cal F}$ and the family of\nco-bisets of ${\\cal F}$ are $k$-regular, our ratios are: $O(\\log\n\\frac{|V|}{|V|-k})$ if $|S^+ \\setminus S|=k$ for all $\\hat{S} \\in {\\cal F}$,\nand $O(\\frac{|V|}{|V|-k} \\log \\frac{|V|}{|V|-k})$ if $|S^+ \\setminus S| \\leq k$\nfor all $\\hat{S} \\in {\\cal F}$. Using these generic algorithms, we derive\napproximation algorithms for some network design problems."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.4372v2", 
    "other_authors": "Venkatesan Guruswami, Ali Kemal Sinop", 
    "title": "Faster SDP hierarchy solvers for local rounding algorithms", 
    "arxiv-id": "1207.4372v2", 
    "author": "Ali Kemal Sinop", 
    "publish": "2012-07-18T13:21:26Z", 
    "summary": "Convex relaxations based on different hierarchies of linear/semi-definite\nprograms have been used recently to devise approximation algorithms for various\noptimization problems. The approximation guarantee of these algorithms improves\nwith the number of {\\em rounds} $r$ in the hierarchy, though the complexity of\nsolving (or even writing down the solution for) the $r$'th level program grows\nas $n^{\\Omega(r)}$ where $n$ is the input size.\n  In this work, we observe that many of these algorithms are based on {\\em\nlocal} rounding procedures that only use a small part of the SDP solution (of\nsize $n^{O(1)} 2^{O(r)}$ instead of $n^{\\Omega(r)}$). We give an algorithm to\nfind the requisite portion in time polynomial in its size. The challenge in\nachieving this is that the required portion of the solution is not fixed a\npriori but depends on other parts of the solution, sometimes in a complicated\niterative manner.\n  Our solver leads to $n^{O(1)} 2^{O(r)}$ time algorithms to obtain the same\nguarantees in many cases as the earlier $n^{O(r)}$ time algorithms based on $r$\nrounds of the Lasserre hierarchy. In particular, guarantees based on $O(\\log\nn)$ rounds can be realized in polynomial time.\n  We develop and describe our algorithm in a fairly general abstract framework.\nThe main technical tool in our work, which might be of independent interest in\nconvex optimization, is an efficient ellipsoid algorithm based separation\noracle for convex programs that can output a {\\em certificate of infeasibility\nwith restricted support}. This is used in a recursive manner to find a sequence\nof consistent points in nested convex bodies that \"fools\" local rounding\nalgorithms."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.4382v3", 
    "other_authors": "Zhewei Wei, Ke Yi", 
    "title": "The Space Complexity of 2-Dimensional Approximate Range Counting", 
    "arxiv-id": "1207.4382v3", 
    "author": "Ke Yi", 
    "publish": "2012-07-18T14:25:56Z", 
    "summary": "We study the problem of $2$-dimensional orthogonal range counting with\nadditive error. Given a set $P$ of $n$ points drawn from an $n\\times n$ grid\nand an error parameter $\\eps$, the goal is to build a data structure, such that\nfor any orthogonal range $R$, it can return the number of points in $P\\cap R$\nwith additive error $\\eps n$. A well-known solution for this problem is the\n{\\em $\\eps$-approximation}, which is a subset $A\\subseteq P$ that can estimate\nthe number of points in $P\\cap R$ with the number of points in $A\\cap R$. It is\nknown that an $\\eps$-approximation of size $O(\\frac{1}{\\eps} \\log^{2.5}\n\\frac{1}{\\eps})$ exists for any $P$ with respect to orthogonal ranges, and the\nbest lower bound is $\\Omega(\\frac{1}{\\eps} \\log \\frac{1}{\\eps})$. The\n$\\eps$-approximation is a rather restricted data structure, as we are not\nallowed to store any information other than the coordinates of the points in\n$P$. In this paper, we explore what can be achieved without any restriction on\nthe data structure. We first describe a simple data structure that uses\n$O(\\frac{1}{\\eps}(\\log^2\\frac{1} {\\eps} + \\log n) )$ bits and answers queries\nwith error $\\eps n$. We then prove a lower bound that any data structure that\nanswers queries with error $\\eps n$ must use\n$\\Omega(\\frac{1}{\\eps}(\\log^2\\frac{1} {\\eps} + \\log n) )$ bits. Our lower bound\nis information-theoretic: We show that there is a collection of\n$2^{\\Omega(n\\log n)}$ point sets with large {\\em union combinatorial\ndiscrepancy}, and thus are hard to distinguish unless we use $\\Omega(n\\log n)$\nbits."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-32589-2_42", 
    "link": "http://arxiv.org/pdf/1207.4383v1", 
    "other_authors": "Zhewei Wei, Ke Yi", 
    "title": "Equivalence between Priority Queues and Sorting in External Memory", 
    "arxiv-id": "1207.4383v1", 
    "author": "Ke Yi", 
    "publish": "2012-07-18T14:32:57Z", 
    "summary": "A priority queue is a fundamental data structure that maintains a dynamic\nordered set of keys and supports the followig basic operations: insertion of a\nkey, deletion of a key, and finding the smallest key. The complexity of the\npriority queue is closely related to that of sorting: A priority queue can be\nused to implement a sorting algorithm trivially. Thorup\n\\cite{thorup2007equivalence} proved that the converse is also true in the RAM\nmodel. In particular, he designed a priority queue that uses the sorting\nalgorithm as a black box, such that the per-operation cost of the priority\nqueue is asymptotically the same as the per-key cost of sorting. In this paper,\nwe prove an analogous result in the external memory model, showing that\npriority queues are computationally equivalent to sorting in external memory,\nunder some mild assumptions. The reduction provides a possibility for proving\nlower bounds for external sorting via showing a lower bound for priority\nqueues."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.4607v1", 
    "other_authors": "Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", 
    "title": "Efficient LZ78 factorization of grammar compressed text", 
    "arxiv-id": "1207.4607v1", 
    "author": "Masayuki Takeda", 
    "publish": "2012-07-19T10:28:56Z", 
    "summary": "We present an efficient algorithm for computing the LZ78 factorization of a\ntext, where the text is represented as a straight line program (SLP), which is\na context free grammar in the Chomsky normal form that generates a single\nstring. Given an SLP of size $n$ representing a text $S$ of length $N$, our\nalgorithm computes the LZ78 factorization of $T$ in $O(n\\sqrt{N}+m\\log N)$ time\nand $O(n\\sqrt{N}+m)$ space, where $m$ is the number of resulting LZ78 factors.\nWe also show how to improve the algorithm so that the $n\\sqrt{N}$ term in the\ntime and space complexities becomes either $nL$, where $L$ is the length of the\nlongest LZ78 factor, or $(N - \\alpha)$ where $\\alpha \\geq 0$ is a quantity\nwhich depends on the amount of redundancy that the SLP captures with respect to\nsubstrings of $S$ of a certain length. Since $m = O(N/\\log_\\sigma N)$ where\n$\\sigma$ is the alphabet size, the latter is asymptotically at least as fast as\na linear time algorithm which runs on the uncompressed string when $\\sigma$ is\nconstant, and can be more efficient when the text is compressible, i.e. when\n$m$ and $n$ are small."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.4616v1", 
    "other_authors": "Patrick Prosser", 
    "title": "Exact Algorithms for Maximum Clique: a computational study", 
    "arxiv-id": "1207.4616v1", 
    "author": "Patrick Prosser", 
    "publish": "2012-07-19T11:26:59Z", 
    "summary": "We investigate a number of recently reported exact algorithms for the maximum\nclique problem (MCQ, MCR, MCS, BBMC). The program code used is presented and\ncritiqued showing how small changes in implementation can have a drastic effect\non performance. The computational study demonstrates how problem features and\nhardware platforms influence algorithm behaviour. The minimum width order\n(smallest-last) is investigated, and MCS is broken into its consituent parts\nand we discover that one of these parts degrades performance. It is shown that\nthe standard procedure used for rescaling published results is unsafe."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.5200v3", 
    "other_authors": "Gregory T. Minton, Eric Price", 
    "title": "Improved Concentration Bounds for Count-Sketch", 
    "arxiv-id": "1207.5200v3", 
    "author": "Eric Price", 
    "publish": "2012-07-22T06:23:09Z", 
    "summary": "We present a refined analysis of the classic Count-Sketch streaming heavy\nhitters algorithm [CCF02]. Count-Sketch uses O(k log n) linear measurements of\na vector x in R^n to give an estimate x' of x. The standard analysis shows that\nthis estimate x' satisfies ||x'-x||_infty^2 < ||x_tail||_2^2 / k, where x_tail\nis the vector containing all but the largest k coordinates of x. Our main\nresult is that most of the coordinates of x' have substantially less error than\nthis upper bound; namely, for any c < O(log n), we show that each coordinate i\nsatisfies (x'_i - x_i)^2 < (c/log n) ||x_tail||_2^2/k with probability\n1-2^{-Omega(c)}, as long as the hash functions are fully independent. This\nsubsumes the previous bound and is optimal for all c. Using these improved\npoint estimates, we prove a stronger concentration result for set estimates by\nfirst analyzing the covariance matrix and then using a\nmedian-of-median-of-medians argument to bootstrap the failure probability\nbounds. These results also give improved results for l_2 recovery of exactly\nk-sparse estimates x^* when x is drawn from a distribution with suitable decay,\nsuch as a power law or lognormal. We complement our results with simulations of\nCount-Sketch on a power law distribution. The empirical evidence indicates that\nour theoretical bounds give a precise characterization of the algorithm's\nperformance: the asymptotics are correct and the associated constants are\nsmall. Our proof shows that any symmetric random variable with finite variance\nand positive Fourier transform concentrates around 0 at least as well as a\nGaussian. This result, which may be of independent interest, gives good\nconcentration even when the noise does not converge to a Gaussian."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.5722v1", 
    "other_authors": "Joseph Cheriyan, Zachary Friggstad, Zhihan Gao", 
    "title": "Approximating Minimum-Cost Connected T-Joins", 
    "arxiv-id": "1207.5722v1", 
    "author": "Zhihan Gao", 
    "publish": "2012-07-24T15:20:22Z", 
    "summary": "We design and analyse approximation algorithms for the minimum-cost connected\nT-join problem: given an undirected graph G = (V;E) with nonnegative costs on\nthe edges, and a subset of nodes T, find (if it exists) a spanning connected\nsubgraph H of minimum cost such that every node in T has odd degree and every\nnode not in T has even degree; H may have multiple copies of any edge of G. Two\nwell-known special cases are the TSP (|T| = 0) and the s-t path TSP (|T| = 2).\nRecently, An, Kleinberg, and Shmoys [STOC 2012] improved on the long-standing\n5/3-approximation guarantee for the latter problem and presented an algorithm\nbased on LP rounding that achieves an approximation guarantee of (1+sqrt(5))/2\n< 1.6181.\n  We show that the methods of An et al. extend to the minimum-cost connected\nT-join problem. They presented a new proof for a 5/3-approximation guarantee\nfor the s-t path TSP; their proof extends easily to the minimum-cost connected\nT-join problem. Next, we improve on the approximation guarantee of 5/3 by\nextending their LP-rounding algorithm to get an approximation guarantee of 13/8\n= 1.625 for all |T| >= 4.\n  Finally, we focus on the prize-collecting version of the problem, and present\na primal-dual algorithm that is \"Lagrangian multiplier preserving\" and that\nachieves an approximation guarantee 3 - 2/(|T|-1) when |T| >= 4. Our\nprimal-dual algorithm is a generalization of the known primal-dual\n2-approximation for the prize-collecting s-t path TSP. Furthermore, we show\nthat our analysis is tight by presenting instances with |T| >= 4 such that the\ncost of the solution found by the algorithm is exactly 3 - 2/(|T|-1) times the\ncost of the constructed dual solution."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.5959v2", 
    "other_authors": "Jun Kawahara, Koji M. Kobayashi, Tomotaka Maeda", 
    "title": "Tight Analysis of Priority Queuing Policy for Egress Traffic", 
    "arxiv-id": "1207.5959v2", 
    "author": "Tomotaka Maeda", 
    "publish": "2012-07-25T11:43:26Z", 
    "summary": "Recently, the problems of evaluating performances of switches and routers\nhave been formulated as online problems, and a great amount of results have\nbeen presented. In this paper, we focus on managing outgoing packets (called\n{\\em egress traffic}) on switches that support Quality of Service (QoS), and\nanalyze the performance of one of the most fundamental scheduling policies {\\em\nPriority Queuing} ($PQ$) using competitive analysis. We formulate the problem\nof managing egress queues as follows: An output interface is equipped with $m$\nqueues, each of which has a buffer of size $B$. The size of a packet is unit,\nand each buffer can store up to $B$ packets simultaneously. Each packet is\nassociated with one of $m$ priority values $\\alpha_{j}$ ($1 \\leq j \\leq m$),\nwhere $\\alpha_{1} \\leq \\alpha_{2} \\leq \\cdots \\leq \\alpha_{m}$, $\\alpha_{1} =\n1$, and $\\alpha_{m} = \\alpha$ and the task of an online algorithm is to select\none of $m$ queues at each scheduling step. The purpose of this problem is to\nmaximize the sum of the values of the scheduled packets. For any $B$ and any\n$m$, we show that the competitive ratio of $PQ$ is exactly $2 - \\min_{x \\in [1,\nm-1] } \\{ \\frac{ \\alpha_{x+1} }{ \\sum_{j = 1}^{x+1} \\alpha_{j} } \\}$. That is,\nwe conduct a complete analysis of the performance of $PQ$ using worst case\nanalysis. Moreover, we show that no deterministic online algorithm can have a\ncompetitive ratio smaller than $1 + \\frac{ \\alpha^3 + \\alpha^2 + \\alpha }{\n\\alpha^4 + 4 \\alpha^3 + 3 \\alpha^2 + 4 \\alpha + 1 }$."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.6246v1", 
    "other_authors": "Robert Krauthgamer, Inbal Rika", 
    "title": "Mimicking Networks and Succinct Representations of Terminal Cuts", 
    "arxiv-id": "1207.6246v1", 
    "author": "Inbal Rika", 
    "publish": "2012-07-26T11:55:13Z", 
    "summary": "Given a large edge-weighted network $G$ with $k$ terminal vertices, we wish\nto compress it and store, using little memory, the value of the minimum cut (or\nequivalently, maximum flow) between every bipartition of terminals. One\nappealing methodology to implement a compression of $G$ is to construct a\n\\emph{mimicking network}: a small network $G'$ with the same $k$ terminals, in\nwhich the minimum cut value between every bipartition of terminals is the same\nas in $G$. This notion was introduced by Hagerup, Katajainen, Nishimura, and\nRagde [JCSS '98], who proved that such $G'$ of size at most $2^{2^k}$ always\nexists. Obviously, by having access to the smaller network $G'$, certain\ncomputations involving cuts can be carried out much more efficiently.\n  We provide several new bounds, which together narrow the previously known gap\nfrom doubly-exponential to only singly-exponential, both for planar and for\ngeneral graphs. Our first and main result is that every $k$-terminal planar\nnetwork admits a mimicking network $G'$ of size $O(k^2 2^{2k})$, which is\nmoreover a minor of $G$. On the other hand, some planar networks $G$ require\n$|E(G')| \\ge \\Omega(k^2)$. For general networks, we show that certain bipartite\ngraphs only admit mimicking networks of size $|V(G')| \\geq 2^{\\Omega(k)}$, and\nmoreover, every data structure that stores the minimum cut value between all\nbipartitions of the terminals must use $2^{\\Omega(k)}$ machine words."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.6365v4", 
    "other_authors": "Kenneth L. Clarkson, David P. Woodruff", 
    "title": "Low Rank Approximation and Regression in Input Sparsity Time", 
    "arxiv-id": "1207.6365v4", 
    "author": "David P. Woodruff", 
    "publish": "2012-07-26T18:50:00Z", 
    "summary": "We design a new distribution over $\\poly(r \\eps^{-1}) \\times n$ matrices $S$\nso that for any fixed $n \\times d$ matrix $A$ of rank $r$, with probability at\nleast 9/10, $\\norm{SAx}_2 = (1 \\pm \\eps)\\norm{Ax}_2$ simultaneously for all $x\n\\in \\mathbb{R}^d$. Such a matrix $S$ is called a \\emph{subspace embedding}.\nFurthermore, $SA$ can be computed in $\\nnz(A) + \\poly(d \\eps^{-1})$ time, where\n$\\nnz(A)$ is the number of non-zero entries of $A$. This improves over all\nprevious subspace embeddings, which required at least $\\Omega(nd \\log d)$ time\nto achieve this property. We call our matrices $S$ \\emph{sparse embedding\nmatrices}.\n  Using our sparse embedding matrices, we obtain the fastest known algorithms\nfor $(1+\\eps)$-approximation for overconstrained least-squares regression,\nlow-rank approximation, approximating all leverage scores, and\n$\\ell_p$-regression. The leading order term in the time complexity of our\nalgorithms is $O(\\nnz(A))$ or $O(\\nnz(A)\\log n)$.\n  We optimize the low-order $\\poly(d/\\eps)$ terms in our running times (or for\nrank-$k$ approximation, the $n*\\poly(k/eps)$ term), and show various tradeoffs.\nFor instance, we also use our methods to design new preconditioners that\nimprove the dependence on $\\eps$ in least squares regression to $\\log 1/\\eps$.\nFinally, we provide preliminary experimental results which suggest that our\nalgorithms are competitive in practice."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1207.6371v1", 
    "other_authors": "Arindam Khan, Prasad Raghavendra, Prasad Tetali, L\u00e1szl\u00f3 A. V\u00e9gh", 
    "title": "On Mimicking Networks Representing Minimum Terminal Cuts", 
    "arxiv-id": "1207.6371v1", 
    "author": "L\u00e1szl\u00f3 A. V\u00e9gh", 
    "publish": "2012-07-26T19:03:25Z", 
    "summary": "Given a capacitated undirected graph $G=(V,E)$ with a set of terminals $K\n\\subset V$, a mimicking network is a smaller graph $H=(V_H,E_H)$ that exactly\npreserves all the minimum cuts between the terminals. Specifically, the vertex\nset of the sparsifier $V_H$ contains the set of terminals $K$ and for every\nbipartition $U, K-U $ of the terminals $K$, the size of the minimum cut\nseparating $U$ from $K-U$ in $G$ is exactly equal to the size of the minimum\ncut separating $U$ from $K-U$ in $H$.\n  This notion of a mimicking network was introduced by Hagerup, Katajainen,\nNishimura and Ragde (1995) who also exhibited a mimicking network of size\n$2^{2^{k}}$ for every graph with $k$ terminals. The best known lower bound on\nthe size of a mimicking network is linear in the number of terminals. More\nprecisely, the best known lower bound is $k+1$ for graphs with $k$ terminals\n(Chaudhuri et al. 2000).\n  In this work, we improve both the upper and lower bounds reducing the\ndoubly-exponential gap between them to a single-exponential gap. Specifically,\nwe obtain the following upper and lower bounds on mimicking networks: 1) Given\na graph $G$, we exhibit a construction of mimicking network with at most\n$(|K|-1)$'th Dedekind number ($\\approx 2^{{(k-1)} \\choose {\\lfloor {{(k-1)}/2}\n\\rfloor}}$) of vertices (independent of size of $V$). Furthermore, we show that\nthe construction is optimal among all {\\it restricted mimicking networks} -- a\nnatural class of mimicking networks that are obtained by clustering vertices\ntogether. 2) There exists graphs with $k$ terminals that have no mimicking\nnetwork of size smaller than $2^{\\frac{k-1}{2}}$.\n  We also exhibit improved constructions of mimicking networks for trees and\ngraphs of bounded tree-width."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1307.0531v1", 
    "other_authors": "Ahmed Abousamra, David P. Bunde, Kirk Pruhs", 
    "title": "An Experimental Comparison of Speed Scaling Algorithms with Deadline   Feasibility Constraints", 
    "arxiv-id": "1307.0531v1", 
    "author": "Kirk Pruhs", 
    "publish": "2013-07-01T21:02:49Z", 
    "summary": "We consider the first, and most well studied, speed scaling problem in the\nalgorithmic literature: where the scheduling quality of service measure is a\ndeadline feasibility constraint, and where the power objective is to minimize\nthe total energy used. Four online algorithms for this problem have been\nproposed in the algorithmic literature. Based on the best upper bound that can\nbe proved on the competitive ratio, the ranking of the online algorithms from\nbest to worst is: $\\qOA$, $\\OA$, $\\AVR$, $\\BKP$. As a test case on the\neffectiveness of competitive analysis to predict the best online algorithm, we\nreport on an experimental \"horse race\" between these algorithms using instances\nbased on web server traces. Our main conclusion is that the ranking of our\nalgorithms based on their performance in our experiments is identical to the\norder predicted by competitive analysis. This ranking holds over a large range\nof possible power functions, and even if the power objective is temperature."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1307.1406v1", 
    "other_authors": "Marius Nicolae, Sanguthevar Rajasekaran", 
    "title": "On string matching with k mismatches", 
    "arxiv-id": "1307.1406v1", 
    "author": "Sanguthevar Rajasekaran", 
    "publish": "2013-07-04T16:56:09Z", 
    "summary": "In this paper we consider several variants of the pattern matching problem.\nIn particular, we investigate the following problems: 1) Pattern matching with\nk mismatches; 2) Approximate counting of mismatches; and 3) Pattern matching\nwith mismatches. The distance metric used is the Hamming distance. We present\nsome novel algorithms and techniques for solving these problems. Both\ndeterministic and randomized algorithms are offered. Variants of these problems\nwhere there could be wild cards in either the text or the pattern or both are\nconsidered. An experimental evaluation of these algorithms is also presented.\nThe source code is available at http://www.engr.uconn.edu/~man09004/kmis.zip."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/1307.1417v1", 
    "other_authors": "Sanguthevar Rajasekaran, Marius Nicolae", 
    "title": "An Elegant Algorithm for the Construction of Suffix Arrays", 
    "arxiv-id": "1307.1417v1", 
    "author": "Marius Nicolae", 
    "publish": "2013-07-04T17:10:08Z", 
    "summary": "The suffix array is a data structure that finds numerous applications in\nstring processing problems for both linguistic texts and biological data. It\nhas been introduced as a memory efficient alternative for suffix trees. The\nsuffix array consists of the sorted suffixes of a string. There are several\nlinear time suffix array construction algorithms (SACAs) known in the\nliterature. However, one of the fastest algorithms in practice has a worst case\nrun time of $O(n^2)$. The problem of designing practically and theoretically\nefficient techniques remains open. In this paper we present an elegant\nalgorithm for suffix array construction which takes linear time with high\nprobability; the probability is on the space of all possible inputs. Our\nalgorithm is one of the simplest of the known SACAs and it opens up a new\ndimension of suffix array construction that has not been explored until now.\nOur algorithm is easily parallelizable. We offer parallel implementations on\nvarious parallel models of computing. We prove a lemma on the $\\ell$-mers of a\nrandom string which might find independent applications. We also present\nanother algorithm that utilizes the above algorithm. This algorithm is called\nRadixSA and has a worst case run time of $O(n\\log{n})$. RadixSA introduces an\nidea that may find independent applications as a speedup technique for other\nSACAs. An empirical comparison of RadixSA with other algorithms on various\ndatasets reveals that our algorithm is one of the fastest algorithms to date.\nThe C++ source code is freely available at\nhttp://www.engr.uconn.edu/~man09004/radixSA.zip"
}]