[{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/9809009v1", 
    "title": "Developing numerical libraries in Java", 
    "arxiv-id": "cs/9809009v1", 
    "author": "G. W. Stewart", 
    "publish": "1998-09-02T19:27:07Z", 
    "summary": "The rapid and widespread adoption of Java has created a demand for reliable\nand reusable mathematical software components to support the growing number of\ncompute-intensive applications now under development, particularly in science\nand engineering. In this paper we address practical issues of the Java language\nand environment which have an effect on numerical library design and\ndevelopment. Benchmarks which illustrate the current levels of performance of\nkey numerical kernels on a variety of Java platforms are presented. Finally, a\nstrategy for the development of a fundamental numerical toolkit for Java is\nproposed and its current status is described."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/9809105v1", 
    "title": "Hyper-Systolic Matrix Multiplication", 
    "arxiv-id": "cs/9809105v1", 
    "author": "Klaus Schilling", 
    "publish": "1998-09-24T20:56:11Z", 
    "summary": "A novel parallel algorithm for matrix multiplication is presented. The\nhyper-systolic algorithm makes use of a one-dimensional processor abstraction.\nThe procedure can be implemented on all types of parallel systems. It can\nhandle matrix-vector multiplications as well as transposed matrix products."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0004004v1", 
    "title": "Mathematical Software: Past, Present, and Future", 
    "arxiv-id": "cs/0004004v1", 
    "author": "Ronald F. Boisvert", 
    "publish": "2000-04-13T13:43:48Z", 
    "summary": "This paper provides some reflections on the field of mathematical software on\nthe occasion of John Rice's 65th birthday. I describe some of the common themes\nof research in this field and recall some significant events in its evolution.\nFinally, I raise a number of issues that are of concern to future developments."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0101001v1", 
    "title": "Automatic Differentiation Tools in Optimization Software", 
    "arxiv-id": "cs/0101001v1", 
    "author": "Jorge J. Mor\u00e9", 
    "publish": "2001-01-03T17:33:59Z", 
    "summary": "We discuss the role of automatic differentiation tools in optimization\nsoftware. We emphasize issues that are important to large-scale optimization\nand that have proved useful in the installation of nonlinear solvers in the\nNEOS Server. Our discussion centers on the computation of the gradient and\nHessian matrix for partially separable functions and shows that the gradient\nand Hessian matrix can be computed with guaranteed bounds in time and memory\nrequirements"
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0101018v1", 
    "title": "GPCG: A Case Study in the Performance and Scalability of Optimization   Algorithms", 
    "arxiv-id": "cs/0101018v1", 
    "author": "Jorge J. Mor\u00e9", 
    "publish": "2001-01-19T21:14:52Z", 
    "summary": "GPCG is an algorithm within the Toolkit for Advanced Optimization (TAO) for\nsolving bound constrained, convex quadratic problems. Originally developed by\nMore' and Toraldo, this algorithm was designed for large-scale problems but had\nbeen implemented only for a single processor. The TAO implementation is\navailable for a wide range of high-performance architecture, and has been\ntested on up to 64 processors to solve problems with over 2.5 million\nvariables."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0102001v2", 
    "title": "Benchmarking Optimization Software with Performance Profiles", 
    "arxiv-id": "cs/0102001v2", 
    "author": "Jorge J. Mor\u00e9", 
    "publish": "2001-02-01T20:37:46Z", 
    "summary": "We propose performance profiles-distribution functions for a performance\nmetric-as a tool for benchmarking and comparing optimization software. We show\nthat performance profiles combine the best features of other tools for\nperformance evaluation."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0106051v1", 
    "title": "Users Guide for SnadiOpt: A Package Adding Automatic Differentiation to   Snopt", 
    "arxiv-id": "cs/0106051v1", 
    "author": "Julia Muetherig", 
    "publish": "2001-06-25T19:30:10Z", 
    "summary": "SnadiOpt is a package that supports the use of the automatic differentiation\npackage ADIFOR with the optimization package Snopt. Snopt is a general-purpose\nsystem for solving optimization problems with many variables and constraints.\nIt minimizes a linear or nonlinear function subject to bounds on the variables\nand sparse linear or nonlinear constraints. It is suitable for large-scale\nlinear and quadratic programming and for linearly constrained optimization, as\nwell as for general nonlinear programs. The method used by Snopt requires the\nfirst derivatives of the objective and constraint functions to be available.\nThe SnadiOpt package allows users to avoid the time-consuming and error-prone\nprocess of evaluating and coding these derivatives. Given Fortran code for\nevaluating only the values of the objective and constraints, SnadiOpt\nautomatically generates the code for evaluating the derivatives and builds the\nrelevant Snopt input files and sparse data structures."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0107025v2", 
    "title": "Computer validated proofs of a toolset for adaptable arithmetic", 
    "arxiv-id": "cs/0107025v2", 
    "author": "Laurent Thery", 
    "publish": "2001-07-19T13:18:31Z", 
    "summary": "Most existing implementations of multiple precision arithmetic demand that\nthe user sets the precision {\\em a priori}. Some libraries are said adaptable\nin the sense that they dynamically change the precision of each intermediate\noperation individually to deliver the target accuracy according to the actual\ninputs. We present in this text a new adaptable numeric core inspired both from\nfloating point expansions and from on-line arithmetic.\n  The numeric core is cut down to four tools. The tool that contains arithmetic\noperations is proved to be correct. The proofs have been formally checked by\nthe Coq assistant. Developing the proofs, we have formally proved many results\npublished in the literature and we have extended a few of them. This work may\nlet users (i) develop application specific adaptable libraries based on the\ntoolset and / or (ii) write new formal proofs based on the set of validated\nfacts."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0306127v1", 
    "title": "Development of a Java Package for Matrix Programming", 
    "arxiv-id": "cs/0306127v1", 
    "author": "Henry BK Teo", 
    "publish": "2003-06-24T12:37:26Z", 
    "summary": "We had assembled a Java package, known as MatrixPak, of four classes for the\npurpose of numerical matrix computation. The classes are matrix,\nmatrix_operations, StrToMatrix, and MatrixToStr; all of which are inherited\nfrom java.lang.Object class. Class matrix defines a matrix as a two-dimensional\narray of float types, and contains the following mathematical methods:\ntranspose, adjoint, determinant, inverse, minor and cofactor. Class\nmatrix_operations contains the following mathematical methods: matrix addition,\nmatrix subtraction, matrix multiplication, and matrix exponential. Class\nStrToMatrix contains methods necessary to parse a string representation (for\nexample, [[2 3 4]-[5 6 7]]) of a matrix into a matrix definition, whereas class\nMatrixToStr does the reverse."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0307009v1", 
    "title": "Finding the \"truncated\" polynomial that is closest to a function", 
    "arxiv-id": "cs/0307009v1", 
    "author": "Jean-Michel Muller", 
    "publish": "2003-07-04T13:15:09Z", 
    "summary": "When implementing regular enough functions (e.g., elementary or special\nfunctions) on a computing system, we frequently use polynomial approximations.\nIn most cases, the polynomial that best approximates (for a given distance and\nin a given interval) a function has coefficients that are not exactly\nrepresentable with a finite number of bits. And yet, the polynomial\napproximations that are actually implemented do have coefficients that are\nrepresented with a finite - and sometimes small - number of bits: this is due\nto the finiteness of the floating-point representations (for software\nimplementations), and to the need to have small, hence fast and/or inexpensive,\nmultipliers (for hardware implementations). We then have to consider polynomial\napproximations for which the degree-$i$ coefficient has at most $m_i$\nfractional bits (in other words, it is a rational number with denominator\n$2^{m_i}$). We provide a general method for finding the best polynomial\napproximation under this constraint. Then, we suggest refinements than can be\nused to accelerate our method."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0310057v1", 
    "title": "An Introduction to Using Software Tools for Automatic Differentiation", 
    "arxiv-id": "cs/0310057v1", 
    "author": "Andrea Walther", 
    "publish": "2003-10-28T19:41:44Z", 
    "summary": "We give a gentle introduction to using various software tools for automatic\ndifferentiation (AD). Ready-to-use examples are discussed, and links to further\ninformation are presented. Our target audience includes all those who are\nlooking for a straightforward way to get started using the available AD\ntechnology. The document is dynamic in the sense that its content will be\nupdated as the AD software evolves."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0406049v1", 
    "title": "A Fast, Vectorizable Algorithm for Producing Single-Precision   Sine-Cosine Pairs", 
    "arxiv-id": "cs/0406049v1", 
    "author": "Marcus H. Mendenhall", 
    "publish": "2004-06-25T20:37:44Z", 
    "summary": "This paper presents an algorithm for computing Sine-Cosine pairs to modest\naccuracy, but in a manner which contains no conditional tests or branching,\nmaking it highly amenable to vectorization. An exemplary implementation for\nPowerPC AltiVec processors is included, but the algorithm should be easily\nportable to other achitectures, such as Intel SSE."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0407052v1", 
    "title": "M@th Desktop and MD Tools - Mathematics and Mathematica Made Easy for   Students", 
    "arxiv-id": "cs/0407052v1", 
    "author": "Reinhard V. Simonovits", 
    "publish": "2004-07-20T13:46:09Z", 
    "summary": "We present two add-ons for Mathematica for teaching mathematics to\nundergraduate and high school students. These two applications, M@th Desktop\n(MD) and M@th Desktop Tools (MDTools), include several palettes and notebooks\ncovering almost every field. The underlying didactic concept is so-called\n\"blended learning\", in which these tools are meant to be used as a complement\nto the professor or teacher rather than as a replacement, which other\ne-learning applications do. They enable students to avoid the usual problem of\ncomputer-based learning, namely that too large an amount of time is wasted\nstruggling with computer and program errors instead of actually learning the\nmathematical concepts.\n  M@th Desktop Tools is palette-based and provides easily accessible and\nuser-friendly templates for the most important functions in the fields of\nAnalysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, is\na modern, interactive teaching and learning software package for mathematics\nclasses. It is comprised of modules for Differentiation, Integration, and\nStatistics, and each module presents its topic with a combination of\ninteractive notebooks and palettes.\n  Both packages can be obtained from Deltasoft's homepage at\nhttp://www.deltasoft.at/ ."
},{
    "category": "cs.MS", 
    "doi": "10.2168/LMCS-8(4:18)2012", 
    "link": "http://arxiv.org/pdf/cs/0408029v1", 
    "title": "Tsnnls: A solver for large sparse least squares problems with   non-negative variables", 
    "arxiv-id": "cs/0408029v1", 
    "author": "Michael Piatek", 
    "publish": "2004-08-13T21:25:42Z", 
    "summary": "The solution of large, sparse constrained least-squares problems is a staple\nin scientific and engineering applications. However, currently available codes\nfor such problems are proprietary or based on MATLAB. We announce a freely\navailable C implementation of the fast block pivoting algorithm of Portugal,\nJudice, and Vicente. Our version is several times faster than Matstoms' MATLAB\nimplementation of the same algorithm. Further, our code matches the accuracy of\nMATLAB's built-in lsqnonneg function."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0503014v1", 
    "title": "ADF95: Tool for automatic differentiation of a FORTRAN code designed for   large numbers of independent variables", 
    "arxiv-id": "cs/0503014v1", 
    "author": "Christian W. Straka", 
    "publish": "2005-03-04T19:20:04Z", 
    "summary": "ADF95 is a tool to automatically calculate numerical first derivatives for\nany mathematical expression as a function of user defined independent\nvariables. Accuracy of derivatives is achieved within machine precision. ADF95\nmay be applied to any FORTRAN 77/90/95 conforming code and requires minimal\nchanges by the user. It provides a new derived data type that holds the value\nand derivatives and applies forward differencing by overloading all FORTRAN\noperators and intrinsic functions. An efficient indexing technique leads to a\nreduced memory usage and a substantially increased performance gain over other\navailable tools with operator overloading. This gain is especially pronounced\nfor sparse systems with large number of independent variables. A wide class of\nnumerical simulations, e.g., those employing implicit solvers, can profit from\nADF95."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0602005v1", 
    "title": "A library of Taylor models for PVS automatic proof checker", 
    "arxiv-id": "cs/0602005v1", 
    "author": "Marc Daumas", 
    "publish": "2006-02-03T18:29:52Z", 
    "summary": "We present in this paper a library to compute with Taylor models, a technique\nextending interval arithmetic to reduce decorrelation and to solve differential\nequations. Numerical software usually produces only numerical results. Our\nlibrary can be used to produce both results and proofs. As seen during the\ndevelopment of Fermat's last theorem reported by Aczel 1996, providing a proof\nis not sufficient. Our library provides a proof that has been thoroughly\nscrutinized by a trustworthy and tireless assistant. PVS is an automatic proof\nassistant that has been fairly developed and used and that has no internal\nconnection with interval arithmetic or Taylor models. We built our library so\nthat PVS validates each result as it is produced. As producing and validating a\nproof, is and will certainly remain a bigger task than just producing a\nnumerical result our library will never be a replacement to imperative\nimplementations of Taylor models such as Cosy Infinity. Our library should\nmainly be used to validate small to medium size results that are involved in\nsafety or life critical applications."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0603001v1", 
    "title": "BioSig - An application of Octave", 
    "arxiv-id": "cs/0603001v1", 
    "author": "Alois Schl\u00f6gl", 
    "publish": "2006-03-01T15:45:57Z", 
    "summary": "BioSig is an open source software library for biomedical signal processing.\nMost users in the field are using Matlab; however, significant effort was\nundertaken to provide compatibility to Octave, too. This effort has been widely\nsuccessful, only some non-critical components relying on a graphical user\ninterface are missing. Now, installing BioSig on Octave is as easy as on\nMatlab. Moreover, a benchmark test based on BioSig has been developed and the\nbenchmark results of several platforms are presented."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0603052v2", 
    "title": "Evaluation of interval extension of the power function by graph   decomposition", 
    "arxiv-id": "cs/0603052v2", 
    "author": "Evgueni Petrov", 
    "publish": "2006-03-13T12:05:08Z", 
    "summary": "The subject of our talk is the correct evaluation of interval extension of\nthe function specified by the expression x^y without any constraints on the\nvalues of x and y. The core of our approach is a decomposition of the graph of\nx^y into a small number of parts which can be transformed into subsets of the\ngraph of x^y for non-negative bases x. Because of this fact, evaluation of\ninterval extension of x^y, without any constraints on x and y, is not much\nharder than evaluation of interval extension of x^y for non-negative bases x."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0604006v1", 
    "title": "Sparse Matrix Implementation in Octave", 
    "arxiv-id": "cs/0604006v1", 
    "author": "Andy Adler", 
    "publish": "2006-04-03T12:56:32Z", 
    "summary": "There are many classes of mathematical problems which give rise to matrices,\nwhere a large number of the elements are zero. In this case it makes sense to\nhave a special matrix type to handle this class of problems where only the\nnon-zero elements of the matrix are stored. Not only does this reduce the\namount of memory to store the matrix, but it also means that operations on this\ntype of matrix can take advantage of the a-priori knowledge of the positions of\nthe non-zero elements to accelerate their calculations. A matrix type that\nstores only the non-zero elements is generally called sparse.\n  Until recently Octave has lacked a full implementation of sparse matrices.\nThis article address the implementation of sparse matrices within Octave,\nincluding their storage, creation, fundamental algorithms used, their\nimplementations and the basic operations and functions implemented for sparse\nmatrices. Mathematical issues such as the return types of sparse operations,\nmatrix fill-in and reordering for sparse matrix factorization is discussed in\nthe context of a real example.\n  Benchmarking of Octave's implementation of sparse operations compared to\ntheir equivalent in Matlab are given and their implications discussed. Results\nare presented for multiplication and linear algebra operations for various\nmatrix orders and densities. Furthermore, the use of Octave's sparse matrix\nimplementation is demonstrated using a real example of a finite element model\n(FEM) problem. Finally, the method of using sparse matrices with Octave's\noct-files is discussed. The means of creating, using and returning sparse\nmatrices within oct-files is discussed as well as the differences between\nOctave's Sparse and Array classes."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0604039v1", 
    "title": "A Fixed-Point Type for Octave", 
    "arxiv-id": "cs/0604039v1", 
    "author": "Markus Muck", 
    "publish": "2006-04-10T08:22:01Z", 
    "summary": "This paper announces the availability of a fixed point toolbox for the Matlab\ncompatible software package Octave. This toolbox is released under the GNU\nPublic License, and can be used to model the losses in algorithms implemented\nin hardware. Furthermore, this paper presents as an example of the use of this\ntoolbox, the effects of a fixed point implementation on the precision of an\nOFDM modulator."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0604088v3", 
    "title": "How to Run Mathematica Batch-files in Background ?", 
    "arxiv-id": "cs/0604088v3", 
    "author": "Santanu K. Maiti", 
    "publish": "2006-04-23T09:39:02Z", 
    "summary": "Mathematica is a versatile equipment for doing numeric and symbolic\ncomputations and it has wide spread applications in all branches of science.\nMathematica has a complete consistency to design it at every stage that gives\nit multilevel capability and helps advanced usage evolve naturally. Mathematica\nfunctions work for any precision of number and it can be easily computed with\nsymbols, represented graphically to get the best answer. Mathematica is a\nrobust software development that can be used in any popular operating systems\nand it can be communicated with external programs by using proper mathlink\ncommands.\n  Sometimes it is quite desirable to run jobs in background of a computer which\ncan take considerable amount of time to finish, and this allows us to do work\non other tasks, while keeping the jobs running. Most of us are very familiar to\nrun jobs in background for the programs written in the languages like C, C++,\nF77, F90, F95, etc. But the way of running jobs, written in a mathematica\nnotebook, in background is quite different from the conventional method. In\nthis article, we explore how to create a mathematica batch-file from a\nmathematica notebook and run it in background. Here we concentrate our study\nonly for the Unix version, but one can run mathematica programs in background\nfor the Windows version as well by using proper mathematica batch-file."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0605081v1", 
    "title": "Caract\u00e9ristiques arithm\u00e9tiques des processeurs graphiques", 
    "arxiv-id": "cs/0605081v1", 
    "author": "David Defour", 
    "publish": "2006-05-18T15:49:04Z", 
    "summary": "Les unit\\'{e}s graphiques (Graphic Processing Units- GPU) sont d\\'{e}sormais\ndes processeurs puissants et flexibles. Les derni\\`{e}res g\\'{e}n\\'{e}rations\nde GPU contiennent des unit\\'{e}s programmables de traitement des sommets\n(vertex shader) et des pixels (pixel shader) supportant des op\\'{e}rations en\nvirgule flottante sur 8, 16 ou 32 bits. La repr\\'{e}sentation flottante sur 32\nbits correspond \\`{a} la simple pr\\'{e}cision de la norme IEEE sur\nl'arithm\\'{e}tique en virgule flottante (IEEE-754). Les GPU sont bien\nadapt\\'{e}s aux applications avec un fort parall\\'{e}lisme de donn\\'{e}es.\nCependant ils ne sont que peu utilis\\'{e}s en dehors des calculs graphiques\n(General Purpose computation on GPU -- GPGPU). Une des raisons de cet \\'{e}tat\nde faits est la pauvret\\'{e} des documentations techniques fournies par les\nfabricants (ATI et Nvidia), particuli\\`{e}rement en ce qui concerne\nl'implantation des diff\\'{e}rents op\\'{e}rateurs arithm\\'{e}tiques\nembarqu\\'{e}s dans les diff\\'{e}rentes unit\\'{e}s de traitement. Or ces\ninformations sont essentielles pour estimer et contr\\^{o}ler les erreurs\nd'arrondi ou pour mettre en oeuvre des techniques de r\\'{e}duction ou de\ncompensation afin de travailler en pr\\'{e}cision double, quadruple ou\narbitrairement \\'{e}tendue. Nous proposons dans cet article un ensemble de\nprogrammes qui permettent de d\\'{e}couvrir les caract\\'{e}ristiques principales\ndes GPU en ce qui concerne l'arithm\\'{e}tique \\`{a} virgule flottante. Nous\ndonnons les r\\'{e}sultats obtenus sur deux cartes graphiques r\\'{e}centes: la\nNvidia 7800GTX et l'ATI RX1800XL."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0606101v4", 
    "title": "Stochastic Formal Methods: An application to accuracy of numeric   software", 
    "arxiv-id": "cs/0606101v4", 
    "author": "David Lester", 
    "publish": "2006-06-23T08:26:56Z", 
    "summary": "This paper provides a bound on the number of numeric operations (fixed or\nfloating point) that can safely be performed before accuracy is lost. This work\nhas important implications for control systems with safety-critical software,\nas these systems are now running fast enough and long enough for their errors\nto impact on their functionality. Furthermore, worst-case analysis would\nblindly advise the replacement of existing systems that have been successfully\nrunning for years. We present here a set of formal theorems validated by the\nPVS proof assistant. These theorems will allow code analyzing tools to produce\nformal certificates of accurate behavior. For example, FAA regulations for\naircraft require that the probability of an error be below $10^{-9}$ for a 10\nhour flight."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0610110v4", 
    "title": "Stochastic Formal Methods for Hybrid Systems", 
    "arxiv-id": "cs/0610110v4", 
    "author": "Annick Truffert", 
    "publish": "2006-10-18T19:57:11Z", 
    "summary": "We provide a framework to bound the probability that accumulated errors were\nnever above a given threshold on hybrid systems. Such systems are used for\nexample to model an aircraft or a nuclear power plant on one side and its\nsoftware on the other side. This report contains simple formulas based on\nL\\'evy's and Markov's inequalities and it presents a formal theory of random\nvariables with a special focus on producing concrete results. We selected four\nvery common applications that fit in our framework and cover the common\npractices of hybrid systems that evolve for a long time. We compute the number\nof bits that remain continuously significant in the first two applications with\na probability of failure around one against a billion, where worst case\nanalysis considers that no significant bit remains. We are using PVS as such\nformal tools force explicit statement of all hypotheses and prevent incorrect\nuses of theorems."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0701186v2", 
    "title": "Certification of bounds on expressions involving rounded operators", 
    "arxiv-id": "cs/0701186v2", 
    "author": "Guillaume Melquiond", 
    "publish": "2007-01-29T15:36:47Z", 
    "summary": "Gappa uses interval arithmetic to certify bounds on mathematical expressions\nthat involve rounded as well as exact operators. Gappa generates a theorem with\nits proof for each bound treated. The proof can be checked with a higher order\nlogic automatic proof checker, either Coq or HOL Light, and we have developed a\nlarge companion library of verified facts for Coq dealing with the addition,\nmultiplication, division, and square root, in fixed- and floating-point\narithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints\nof intervals and performs forward error analysis on rounded operators when\nnecessary. When asked, Gappa reports the best bounds it is able to reach for a\ngiven expression in a given context. This feature is used to quickly obtain\ncoarse bounds. It can also be used to identify where the set of facts and\nautomatic techniques implemented in Gappa becomes insufficient. Gappa handles\nseamlessly additional properties expressed as interval properties or rewriting\nrules in order to establish more intricate bounds. Recent work showed that\nGappa is perfectly suited to the proof of correctness of small pieces of\nsoftware. Proof obligations can be written by designers, produced by\nthird-party tools or obtained by overloading arithmetic operators."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/cs/0703040v1", 
    "title": "Why the Standard Data Processing should be changed", 
    "arxiv-id": "cs/0703040v1", 
    "author": "Yefim Bakman", 
    "publish": "2007-03-08T19:49:35Z", 
    "summary": "The basic statistical methods of data representation did not change since\ntheir emergence. Their simplicity was dictated by the intricacies of\ncomputations in the before computers epoch. It turns out that such approach is\nnot uniquely possible in the presence of quick computers. The suggested here\nmethod improves significantly the reliability of data processing and their\ngraphical representation. In this paper we show problems of the standard data\nprocessing which can bring to incorrect results. A method solving these\nproblems is proposed. It is based on modification of data representation. The\nmethod was implemented in a computer program Consensus5. The program\nperformances are illustrated through varied examples."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/0704.3141v1", 
    "title": "Algorithm for Evaluation of the Interval Power Function of Unconstrained   Arguments", 
    "arxiv-id": "0704.3141v1", 
    "author": "Evgueni Petrov", 
    "publish": "2007-04-24T08:33:52Z", 
    "summary": "We describe an algorithm for evaluation of the interval extension of the\npower function of variables x and y given by the expression x^y. Our algorithm\nreduces the general case to the case of non-negative bases."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/0707.2347v5", 
    "title": "Memory efficient scheduling of Strassen-Winograd's matrix multiplication   algorithm", 
    "arxiv-id": "0707.2347v5", 
    "author": "Wei Zhou", 
    "publish": "2007-07-16T16:02:50Z", 
    "summary": "We propose several new schedules for Strassen-Winograd's matrix\nmultiplication algorithm, they reduce the extra memory allocation requirements\nby three different means: by introducing a few pre-additions, by overwriting\nthe input matrices, or by using a first recursive level of classical\nmultiplication. In particular, we show two fully in-place schedules: one having\nthe same number of operations, if the input matrices can be overwritten; the\nother one, slightly increasing the constant of the leading term of the\ncomplexity, if the input matrices are read-only. Many of these schedules have\nbeen found by an implementation of an exhaustive search algorithm based on a\npebble game."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/0707.4651v1", 
    "title": "Comments on the Reliability of Lawson and Hanson's Linear Distance   Programming Algorithm: Subroutine LDP", 
    "arxiv-id": "0707.4651v1", 
    "author": "Alan Rufty", 
    "publish": "2007-07-31T17:04:34Z", 
    "summary": "This brief paper: (1) Discusses strategies to generate random test cases that\ncan be used to extensively test any Linear Distance Program (LDP) software. (2)\nGives three numerical examples of input cases generated by this strategy that\ncause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard\nmatter of acceptable implementation procedures, that (unless it is done\ninternally in the software itself, but, in general, this seems to be much rarer\nthan one would expect) all users should test the returned output from any LDP\nmodule for self-consistency since it incurs only a small amount of added\ncomputational overhead and it is not hard to do."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/0803.2093v1", 
    "title": "GraphStream: A Tool for bridging the gap between Complex Systems and   Dynamic Graphs", 
    "arxiv-id": "0803.2093v1", 
    "author": "Damien Olivier", 
    "publish": "2008-03-14T07:09:13Z", 
    "summary": "The notion of complex systems is common to many domains, from Biology to\nEconomy, Computer Science, Physics, etc. Often, these systems are made of sets\nof entities moving in an evolving environment. One of their major\ncharacteristics is the emergence of some global properties stemmed from local\ninteractions between the entities themselves and between the entities and the\nenvironment. The structure of these systems as sets of interacting entities\nleads researchers to model them as graphs. However, their understanding\nrequires most often to consider the dynamics of their evolution. It is indeed\nnot relevant to study some properties out of any temporal consideration. Thus,\ndynamic graphs seem to be a very suitable model for investigating the emergence\nand the conservation of some properties. GraphStream is a Java-based library\nwhose main purpose is to help researchers and developers in their daily tasks\nof dynamic problem modeling and of classical graph management tasks: creation,\nprocessing, display, etc. It may also be used, and is indeed already used, for\nteaching purpose. GraphStream relies on an event-based engine allowing several\nevent sources. Events may be included in the core of the application, read from\na file or received from an event handler."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2005.01.011", 
    "link": "http://arxiv.org/pdf/0803.2386v1", 
    "title": "Conformal Computing: Algebraically connecting the hardware/software   boundary using a uniform approach to high-performance computation for   software and hardware applications", 
    "arxiv-id": "0803.2386v1", 
    "author": "James E. Raynolds", 
    "publish": "2008-03-17T02:38:49Z", 
    "summary": "We present a systematic, algebraically based, design methodology for\nefficient implementation of computer programs optimized over multiple levels of\nthe processor/memory and network hierarchy. Using a common formalism to\ndescribe the problem and the partitioning of data over processors and memory\nlevels allows one to mathematically prove the efficiency and correctness of a\ngiven algorithm as measured in terms of a set of metrics (such as\nprocessor/network speeds, etc.). The approach allows the average programmer to\nachieve high-level optimizations similar to those used by compiler writers\n(e.g. the notion of \"tiling\").\n  The approach presented in this monograph makes use of A Mathematics of Arrays\n(MoA, Mullin 1988) and an indexing calculus (i.e. the psi-calculus) to enable\nthe programmer to develop algorithms using high-level compiler-like\noptimizations through the ability to algebraically compose and reduce sequences\nof array operations. Extensive discussion and benchmark results are presented\nfor the Fast Fourier Transform and other important algorithms."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2008.11.005", 
    "link": "http://arxiv.org/pdf/0808.2794v1", 
    "title": "Accelerating Scientific Computations with Mixed Precision Algorithms", 
    "arxiv-id": "0808.2794v1", 
    "author": "Stanimire Tomov", 
    "publish": "2008-08-20T17:50:36Z", 
    "summary": "On modern architectures, the performance of 32-bit operations is often at\nleast twice as fast as the performance of 64-bit operations. By using a\ncombination of 32-bit and 64-bit floating point arithmetic, the performance of\nmany dense and sparse linear algebra algorithms can be significantly enhanced\nwhile maintaining the 64-bit accuracy of the resulting solution. The approach\npresented here can apply not only to conventional processors but also to other\ntechnologies such as Field Programmable Gate Arrays (FPGA), Graphical\nProcessing Units (GPU), and the STI Cell BE processor. Results on modern\nprocessor architectures and the STI Cell BE are presented."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644010", 
    "link": "http://arxiv.org/pdf/0811.1714v1", 
    "title": "Efficient Multiplication of Dense Matrices over GF(2)", 
    "arxiv-id": "0811.1714v1", 
    "author": "William Hart", 
    "publish": "2008-11-11T14:23:49Z", 
    "summary": "We describe an efficient implementation of a hierarchy of algorithms for\nmultiplication of dense matrices over the field with two elements (GF(2)). In\nparticular we present our implementation -- in the M4RI library -- of\nStrassen-Winograd matrix multiplication and the \"Method of the Four Russians\"\nmultiplication (M4RM) and compare it against other available implementations.\nGood performance is demonstrated on on AMD's Opteron and particulary good\nperformance on Intel's Core 2 Duo. The open-source M4RI library is available\nstand-alone as well as part of the Sage mathematics software.\n  In machine terms, addition in GF(2) is logical-XOR, and multiplication is\nlogical-AND, thus a machine word of 64-bits allows one to operate on 64\nelements of GF(2) in parallel: at most one CPU cycle for 64 parallel additions\nor multiplications. As such, element-wise operations over GF(2) are relatively\ncheap. In fact, in this paper, we conclude that the actual bottlenecks are\nmemory reads and writes and issues of data locality. We present our empirical\nfindings in relation to minimizing these and give an analysis thereof."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644010", 
    "link": "http://arxiv.org/pdf/0901.1413v1", 
    "title": "Bitslicing and the Method of Four Russians Over Larger Finite Fields", 
    "arxiv-id": "0901.1413v1", 
    "author": "Robert W. Bradshaw", 
    "publish": "2009-01-11T04:43:51Z", 
    "summary": "We present a method of computing with matrices over very small finite fields\nof size larger than 2. Specifically, we show how the Method of Four Russians\ncan be efficiently adapted to these larger fields, and introduce a row-wise\nmatrix compression scheme that both reduces memory requirements and allows one\nto vectorize element operations. We also present timings which confirm the\nefficiency of these methods and exceed the speed of the fastest implementations\nthe authors are aware of."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644010", 
    "link": "http://arxiv.org/pdf/0903.4053v1", 
    "title": "The generating of Fractal Images Using MathCAD Program", 
    "arxiv-id": "0903.4053v1", 
    "author": "Laura Stefan", 
    "publish": "2009-03-24T10:10:37Z", 
    "summary": "This paper presents the graphic representation in the z-plane of the first\nthree iterations of the algorithm that generates the Sierpinski Gasket. It\nanalyzes the influence of the f(z) map when we represent fractal images."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644010", 
    "link": "http://arxiv.org/pdf/0903.4307v1", 
    "title": "FISLAB - the Fuzzy Inference Tool-box for SCILAB", 
    "arxiv-id": "0903.4307v1", 
    "author": "Simona Apostol", 
    "publish": "2009-03-25T11:42:46Z", 
    "summary": "The present study represents \"The Fislab package of programs meant to develop\nthe fuzzy regulators in the Scilab environment\" in which we present some\ngeneral issues, usage requirements and the working mode of the Fislab\nenvironment. In the second part of the article some features of the Scilab\nfunctions from the Fislab package are described."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644010", 
    "link": "http://arxiv.org/pdf/0903.4313v1", 
    "title": "The development of a fuzzy regulator with an entry and an output in   Fislab", 
    "arxiv-id": "0903.4313v1", 
    "author": "Simona Apostol", 
    "publish": "2009-03-25T12:10:32Z", 
    "summary": "The present article is a sequel of the article \"Fislab the Fuzzy Inference\nTool-Box for Scilab\" and it represents the practical application of:\"The\ndevelopment of the Fuzzy regulator with an input and an output in Fislab\". The\narticle contains, besides this application, some functions to be used in the\nprogram, namely Scilab functions for the fuzzification of the firm information,\nfunctions for the operation of de-fuzzification and functions for the\nimplementation of."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/0904.4152v1", 
    "title": "HONEI: A collection of libraries for numerical computations targeting   multiple processor architectures", 
    "arxiv-id": "0904.4152v1", 
    "author": "Carsten Gutwenger", 
    "publish": "2009-04-27T13:00:36Z", 
    "summary": "We present HONEI, an open-source collection of libraries offering a hardware\noriented approach to numerical calculations. HONEI abstracts the hardware, and\napplications written on top of HONEI can be executed on a wide range of\ncomputer architectures such as CPUs, GPUs and the Cell processor. We\ndemonstrate the flexibility and performance of our approach with two test\napplications, a Finite Element multigrid solver for the Poisson problem and a\nrobust and fast simulation of shallow water waves. By linking against HONEI's\nlibraries, we achieve a twofold speedup over straight forward C++ code using\nHONEI's SSE backend, and additional 3-4 and 4-16 times faster execution on the\nCell and a GPU. A second important aspect of our approach is that the full\nperformance capabilities of the hardware under consideration can be exploited\nby adding optimised application-specific operations to the HONEI libraries.\nHONEI provides all necessary infrastructure for development and evaluation of\nsuch kernels, significantly simplifying their development."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/0905.4430v1", 
    "title": "Limits of Educational Soft \"GeoGebra\" in a Critical Constructive Review", 
    "arxiv-id": "0905.4430v1", 
    "author": "Valerian Antohe", 
    "publish": "2009-05-27T14:11:37Z", 
    "summary": "Mathematical educational soft explore, investigating in a dynamical way, some\nalgebraically, geometrically problems, the expected results being used to\ninvolve a lot of mathematical results. One such software soft is GeoGebra. The\nsoftware is free and multi-platform dynamic mathematics software for learning\nand teaching, awards in Europe and the USA. This paper describes some critical\nbut constructive investigation using the platform for graph functions and\ndynamic geometry."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/0905.4598v1", 
    "title": "Iterative Methods for Systems' Solving - a C# approach", 
    "arxiv-id": "0905.4598v1", 
    "author": "Claudiu Chirilov", 
    "publish": "2009-05-28T10:10:06Z", 
    "summary": "This work wishes to support various mathematical issues concerning the\niterative methods with the help of new programming languages. We consider a way\nto show how problems in math have an answer by using different academic\nresources and different thoughts. Here we treat methods like Gauss-Seidel's,\nCramer's and Gauss-Jordan's."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/0910.1845v1", 
    "title": "Parallel Computation of Finite Element Navier-Stokes codes using MUMPS   Solver", 
    "arxiv-id": "0910.1845v1", 
    "author": "Mandhapati P. Raju", 
    "publish": "2009-10-09T20:11:36Z", 
    "summary": "The study deals with the parallelization of 2D and 3D finite element based\nNavier-Stokes codes using direct solvers. Development of sparse direct solvers\nusing multifrontal solvers has significantly reduced the computational time of\ndirect solution methods. Although limited by its stringent memory requirements,\nmultifrontal solvers can be computationally efficient. First the performance of\nMUltifrontal Massively Parallel Solver (MUMPS) is evaluated for both 2D and 3D\ncodes in terms of memory requirements and CPU times. The scalability of both\nNewton and modified Newton algorithms is tested."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/0912.3398v1", 
    "title": "NetEvo: A computational framework for the evolution of dynamical complex   networks", 
    "arxiv-id": "0912.3398v1", 
    "author": "Claire S. Grierson", 
    "publish": "2009-12-17T13:49:59Z", 
    "summary": "NetEvo is a computational framework designed to help understand the evolution\nof dynamical complex networks. It provides flexible tools for the simulation of\ndynamical processes on networks and methods for the evolution of underlying\ntopological structures. The concept of a supervisor is used to bring together\nboth these aspects in a coherent way. It is the job of the supervisor to rewire\nthe network topology and alter model parameters such that a user specified\nperformance measure is minimised. This performance measure can make use of\ncurrent topological information and simulated dynamical output from the system.\nSuch an abstraction provides a suitable basis in which to study many\noutstanding questions related to complex system design and evolution."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/1005.4395v1", 
    "title": "An OpenMath Content Dictionary for Tensor Concepts", 
    "arxiv-id": "1005.4395v1", 
    "author": "Joseph B. Collins", 
    "publish": "2010-05-24T18:46:34Z", 
    "summary": "We introduce a new OpenMath content dictionary, named tensor1, containing\nsymbols for the expression of tensor formulas. These symbols support the\nexpression of non-Cartesian coordinates and invariant, multilinear expressions\nin the context of coordinate transformations. While current OpenMath symbols\nsupport the expression of linear algebra formulas using matrices and vectors,\nwe find that there is an underlying assumption of Cartesian, or standard,\ncoordinates that makes the expression of general tensor formulas difficult, if\nnot impossible. In introducing these new OpenMath symbols for the expression of\ntensor formulas, we attempt to maintain, as much as possible, consistency with\nprior OpenMath symbol definitions for linear algebra."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/1005.4762v1", 
    "title": "Adapting Mathematical Domain Reasoners", 
    "arxiv-id": "1005.4762v1", 
    "author": "Johan Jeuring", 
    "publish": "2010-05-26T09:19:51Z", 
    "summary": "Mathematical learning environments help students in mastering mathematical\nknowledge. Mature environments typically offer thousands of interactive\nexercises. Providing feedback to students solving interactive exercises\nrequires domain reasoners for doing the exercise-specific calculations. Since a\ndomain reasoner has to solve an exercise in the same way a student should solve\nit, the structure of domain reasoners should follow the layered structure of\nthe mathematical domains. Furthermore, learners, teachers, and environment\nbuilders have different requirements for adapting domain reasoners, such as\nproviding more details, disallowing or enforcing certain solutions, and\ncombining multiple mathematical domains in a new domain. In previous work we\nhave shown how domain reasoners for solving interactive exercises can be\nexpressed in terms of rewrite strategies, rewrite rules, and views. This paper\nshows how users can adapt and configure such domain reasoners to their own\nneeds. This is achieved by enabling users to explicitly communicate the\ncomponents that are used for solving an exercise."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2009.04.018", 
    "link": "http://arxiv.org/pdf/1005.4973v3", 
    "title": "Variants of Mersenne Twister Suitable for Graphic Processors", 
    "arxiv-id": "1005.4973v3", 
    "author": "Makoto Matsumoto", 
    "publish": "2010-05-27T01:06:54Z", 
    "summary": "This paper proposes a type of pseudorandom number generator, Mersenne Twister\nfor Graphic Processor (MTGP), for efficient generation on graphic processessing\nunits (GPUs). MTGP supports large state sizes such as 11213 bits, and uses the\nhigh parallelism of GPUs in computing many steps of the recursion in parallel.\nThe second proposal is a parameter-set generator for MTGP, named MTGP Dynamic\nCreator (MTGPDC). MT- GPDC creates up to 2^32 distinct parameter sets which\ngenerate sequences with high-dimensional uniformity. This facility is suitable\nfor a large grid of GPUs where each GPU requires separate random number\nstreams. MTGP is based on linear recursion over the two-element field, and has\nbetter high-dimensional equidistribution than the Mersenne Twister pseudorandom\nnumber generator."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.24.18", 
    "link": "http://arxiv.org/pdf/1006.0404v1", 
    "title": "Computational Complexity of Iterated Maps on the Interval (Extended   Abstract)", 
    "arxiv-id": "1006.0404v1", 
    "author": "Christoph Spandl", 
    "publish": "2010-06-02T14:30:55Z", 
    "summary": "The exact computation of orbits of discrete dynamical systems on the interval\nis considered. Therefore, a multiple-precision floating point approach based on\nerror analysis is chosen and a general algorithm is presented. The correctness\nof the algorithm is shown and the computational complexity is analyzed. As a\nmain result, the computational complexity measure considered here is related to\nthe Ljapunow exponent of the dynamical system under consideration."
},{
    "category": "cs.MS", 
    "doi": "10.5121/ijcsit.2010.2313", 
    "link": "http://arxiv.org/pdf/1006.1193v1", 
    "title": "Genbit Compress Tool(GBC): A Java-Based Tool to Compress DNA Sequences   and Compute Compression Ratio(bits/base) of Genomes", 
    "arxiv-id": "1006.1193v1", 
    "author": "V. K. Kumar", 
    "publish": "2010-06-07T07:37:49Z", 
    "summary": "We present a Compression Tool, \"GenBit Compress\", for genetic sequences based\non our new proposed \"GenBit Compress Algorithm\". Our Tool achieves the best\ncompression ratios for Entire Genome (DNA sequences) . Significantly better\ncompression results show that GenBit compress algorithm is the best among the\nremaining Genome compression algorithms for non-repetitive DNA sequences in\nGenomes. The standard Compression algorithms such as gzip or compress cannot\ncompress DNA sequences but only expand them in size. In this paper we consider\nthe problem of DNA compression. It is well known that one of the main features\nof DNA Sequences is that they contain substrings which are duplicated except\nfor a few random Mutations. For this reason most DNA compressors work by\nsearching and encoding approximate repeats. We depart from this strategy by\nsearching and encoding only exact repeats. our proposed algorithm achieves the\nbest compression ratio for DNA sequences for larger genome. As long as 8 lakh\ncharacters can be given as input While achieving the best compression ratios\nfor DNA sequences, our new GenBit Compress program significantly improves the\nrunning time of all previous DNA compressors. Assigning binary bits for\nfragments of DNA sequence is also a unique concept introduced in this program\nfor the first time in DNA compression."
},{
    "category": "cs.MS", 
    "doi": "10.5121/ijcsit.2010.2313", 
    "link": "http://arxiv.org/pdf/1006.1744v1", 
    "title": "Efficient Decomposition of Dense Matrices over GF(2)", 
    "arxiv-id": "1006.1744v1", 
    "author": "Cl\u00e9ment Pernet", 
    "publish": "2010-06-09T08:56:07Z", 
    "summary": "In this work we describe an efficient implementation of a hierarchy of\nalgorithms for the decomposition of dense matrices over the field with two\nelements (GF(2)). Matrix decomposition is an essential building block for\nsolving dense systems of linear and non-linear equations and thus much research\nhas been devoted to improve the asymptotic complexity of such algorithms. In\nthis work we discuss an implementation of both well-known and improved\nalgorithms in the M4RI library. The focus of our discussion is on a new variant\nof the M4RI algorithm - denoted MMPF in this work -- which allows for\nconsiderable performance gains in practice when compared to the previously\nfastest implementation. We provide performance figures on x86_64 CPUs to\ndemonstrate the viability of our approach."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2011.05.002", 
    "link": "http://arxiv.org/pdf/1009.3457v2", 
    "title": "How to obtain efficient GPU kernels: an illustration using FMM & FGT   algorithms", 
    "arxiv-id": "1009.3457v2", 
    "author": "Lorena A. Barba", 
    "publish": "2010-09-17T15:47:25Z", 
    "summary": "Computing on graphics processors is maybe one of the most important\ndevelopments in computational science to happen in decades. Not since the\narrival of the Beowulf cluster, which combined open source software with\ncommodity hardware to truly democratize high-performance computing, has the\ncommunity been so electrified. Like then, the opportunity comes with\nchallenges. The formulation of scientific algorithms to take advantage of the\nperformance offered by the new architecture requires rethinking core methods.\nHere, we have tackled fast summation algorithms (fast multipole method and fast\nGauss transform), and applied algorithmic redesign for attaining performance on\ngpus. The progression of performance improvements attained illustrates the\nexercise of formulating algorithms for the massively parallel architecture of\nthe gpu. The end result has been gpu kernels that run at over 500 Gigaflops on\none nvidia Tesla C1060 card, thereby reaching close to practical peak. We can\nconfidently say that gpu computing is not just a vogue, it is truly an\nirresistible trend in high-performance computing."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1102.1523v1", 
    "title": "The NumPy array: a structure for efficient numerical computation", 
    "arxiv-id": "1102.1523v1", 
    "author": "Ga\u00ebl Varoquaux", 
    "publish": "2011-02-08T07:25:56Z", 
    "summary": "In the Python world, NumPy arrays are the standard representation for\nnumerical data. Here, we show how these arrays enable efficient implementation\nof numerical computations in a high-level language. Overall, three techniques\nare applied to improve performance: vectorizing calculations, avoiding copying\ndata in memory, and minimizing operation counts. We first present the NumPy\narray structure, then show how to use it for efficient computation, and finally\nhow to share array data with other libraries."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1102.5711v1", 
    "title": "XMLlab : multimedia publication of simulations applets using XML and   Scilab", 
    "arxiv-id": "1102.5711v1", 
    "author": "Andr\u00e9 Pauss", 
    "publish": "2011-02-28T17:16:06Z", 
    "summary": "We present an XML-based simulation authoring environment. The proposed\ndescription language allows to describe mathematical objects such as systems of\nordinary differential equations, partial differential equations in two\ndimensions, or simple curves and surfaces. It also allows to describe the\nparameters on which these objects depend. This language is independent of the\ntarget software and allows to ensure the perennity of author's work, as well as\ncollaborative work and content reuse. The actual implementation of XMLlab\nallows to run the generated simulations within the open source mathematical\nsoftware Scilab, either locally when Scilab is installed on the client\nmachines, or on thin clients running a simple web browser, when XMLlab and\nScilab are installed on a distant server running a standard HTTP server."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1106.1347v1", 
    "title": "Methods of Matrix Multiplication: An Overview of Several Methods and   their Implementation", 
    "arxiv-id": "1106.1347v1", 
    "author": "Ivo Hedtke", 
    "publish": "2011-06-03T09:17:11Z", 
    "summary": "In this overview article we present several methods for multiplying matrices\nand the implementation of these methods in C. Also a little test program is\ngiven to compare their running time and the numerical stability.\n  The methods are: naive method, naive method working on arrays, naive method\nwith the \\textsc{Kahan} trick, three methods with loop unrolling, winograd\nmethod and the scaled variant, original \\textsc{Strassen} method and the\n\\textsc{Strassen}-\\textsc{Winograd} variant.\n  Please note, that this is the FIRST version. The algorithms are not well\ntested and the implementation is not optimized. If you like to join the\nproject, please contact me."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1111.4144v2", 
    "title": "Matrix Inversion Using Cholesky Decomposition", 
    "arxiv-id": "1111.4144v2", 
    "author": "Deepak Menon", 
    "publish": "2011-11-17T16:44:40Z", 
    "summary": "In this paper we present a method for matrix inversion based on Cholesky\ndecomposition with reduced number of operations by avoiding computation of\nintermediate results; further, we use fixed point simulations to compare the\nnumerical accuracy of the method."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1111.6900v1", 
    "title": "The M4RIE library for dense linear algebra over small fields with even   characteristic", 
    "arxiv-id": "1111.6900v1", 
    "author": "Martin R. Albrecht", 
    "publish": "2011-11-29T17:05:00Z", 
    "summary": "In this work, we present the M4RIE library which implements efficient\nalgorithms for linear algebra with dense matrices over GF(2^e) for 2 <= 2 <=\n10. As the name of the library indicates, it makes heavy use of the M4RI\nlibrary both directly (i.e., by calling it) and indirectly (i.e., by using its\nconcepts). We provide an open-source GPLv2+ C library for efficient linear\nalgebra over GF(2^e) for e small. In this library we implemented an idea due to\nBradshaw and Boothby which reduces matrix multiplication over GF(p^k) to a\nseries of matrix multiplications over GF(p). Furthermore, we propose a caching\ntechnique - Newton-John tables - to avoid finite field multiplications which is\ninspired by Kronrod's method (\"M4RM\") for matrix multiplication over GF(2).\nUsing these two techniques we provide asymptotically fast triangular solving\nwith matrices (TRSM) and PLE-based Gaussian elimination. As a result, we are\nable to significantly improve upon the state of the art in dense linear algebra\nover GF(2^e) with 2 <= e <= 10."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1201.1473v1", 
    "title": "A Representation of Binary Matrices", 
    "arxiv-id": "1201.1473v1", 
    "author": "Krasimir Yordzhev", 
    "publish": "2012-01-06T18:49:43Z", 
    "summary": "In this article we discuss the presentation of a random binary matrix using\nsequence of whole nonnegative numbers. We examine some advantages and\ndisadvantages of this presentation as an alternative of the standard\npresentation using two-dimensional array. It is shown that the presentation of\nbinary matrices using ordered n-tuples of natural numbers makes the algorithms\nfaster and saves a lot of memory. In this work we use object-oriented\nprogramming using the syntax and the semantic of C++ programming language."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1202.5964v1", 
    "title": "Technique detection software for Sparse Matrices", 
    "arxiv-id": "1202.5964v1", 
    "author": "Anila Usman", 
    "publish": "2012-02-27T15:02:13Z", 
    "summary": "Sparse storage formats are techniques for storing and processing the sparse\nmatrix data efficiently. The performance of these storage formats depend upon\nthe distribution of non-zeros, within the matrix in different dimensions. In\norder to have better results we need a technique that suits best the\norganization of data in a particular matrix. So the decision of selecting a\nbetter technique is the main step towards improving the system's results\notherwise the efficiency can be decreased. The purpose of this research is to\nhelp identify the best storage format in case of reduced storage size and high\nprocessing efficiency for a sparse matrix."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1203.6005v2", 
    "title": "The Kernel Quantum Probabilities (KQP) Library", 
    "arxiv-id": "1203.6005v2", 
    "author": "Benjamin Piwowarski", 
    "publish": "2012-03-27T15:57:59Z", 
    "summary": "In this document, we show how the different quantities necessary to compute\nkernel quantum probabilities can be computed. This document form the basis of\nthe implementation of the Kernel Quantum Probability (KQP) open source project"
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1205.2927v1", 
    "title": "A Heterogeneous Accelerated Matrix Multiplication: OpenCL + APU + GPU+   Fast Matrix Multiply", 
    "arxiv-id": "1205.2927v1", 
    "author": "Paolo D'Alberto", 
    "publish": "2012-05-14T01:37:41Z", 
    "summary": "As users and developers, we are witnessing the opening of a new computing\nscenario: the introduction of hybrid processors into a single die, such as an\naccelerated processing unit (APU) processor, and the plug-and-play of\nadditional graphics processing units (GPUs) onto a single motherboard. These\nAPU processors provide multiple symmetric cores with their memory hierarchies\nand an integrated GPU. Moreover, these processors are designed to work with\nexternal GPUs that can push the peak performance towards the TeraFLOPS\nboundary. We present a case study for the development of dense Matrix\nMultiplication (MM) codes for matrix sizes up to 19K\\times19K, thus using all\nof the above computational engines, and an achievable peak performance of 200\nGFLOPS for, literally, a made- at-home built. We present the results of our\nexperience, the quirks, the pitfalls, the achieved performance, and the\nachievable peak performance."
},{
    "category": "cs.MS", 
    "doi": "10.1109/MCSE.2011.37", 
    "link": "http://arxiv.org/pdf/1208.4869v1", 
    "title": "User Manual for the Complex Conjugate Gradient Methods Library CCGPAK   2.0", 
    "arxiv-id": "1208.4869v1", 
    "author": "Piotr J. Flatau", 
    "publish": "2012-08-23T21:52:32Z", 
    "summary": "This manual describes the library of conjugate gradients codes CCGPAK, which\nsolves system of complex linear system of equations. The library is written in\nFORTRAN90 and is highly portable. The codes are general and provide mechanism\nfor matrix times vector multiplication which is separated from the conjugate\ngradient iterations itself. It is simple to switch between single and double\nprecisions. All codes follow the same naming conventions."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.01.005", 
    "link": "http://arxiv.org/pdf/1209.1003v5", 
    "title": "A C++11 implementation of arbitrary-rank tensors for high-performance   computing", 
    "arxiv-id": "1209.1003v5", 
    "author": "Alejandro M. Arag\u00f3n", 
    "publish": "2012-09-05T14:58:51Z", 
    "summary": "This article discusses an efficient implementation of tensors of arbitrary\nrank by using some of the idioms introduced by the recently published C++ ISO\nStandard (C++11). With the aims at providing a basic building block for\nhigh-performance computing, a single Array class template is carefully crafted,\nfrom which vectors, matrices, and even higher-order tensors can be created. An\nexpression template facility is also built around the array class template to\nprovide convenient mathematical syntax. As a result, by using templates, an\nextra high-level layer is added to the C++ language when dealing with algebraic\nobjects and their operations, without compromising performance. The\nimplementation is tested running on both CPU and GPU."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.01.005", 
    "link": "http://arxiv.org/pdf/1211.6989v2", 
    "title": "A framework for the automation of generalised stability theory", 
    "arxiv-id": "1211.6989v2", 
    "author": "Simon W. Funke", 
    "publish": "2012-11-29T17:15:08Z", 
    "summary": "The traditional approach to investigating the stability of a physical system\nis to linearise the equations about a steady base solution, and to examine the\neigenvalues of the linearised operator. Over the past several decades, it has\nbeen recognised that this approach only determines the asymptotic stability of\nthe system, and neglects the possibility of transient perturbation growth\narising due to the nonnormality of the system. This observation motivated the\ndevelopment of a more powerful generalised stability theory (GST), which\nfocusses instead on the singular value decomposition of the linearised\npropagator of the system. While GST has had significant successes in\nunderstanding the stability of phenomena in geophysical fluid dynamics, its\nmore widespread applicability has been hampered by the fact that computing the\nSVD requires both the tangent linear operator and its adjoint: deriving the\ntangent linear and adjoint models is usually a considerable challenge, and\nmanually embedding them inside an eigensolver is laborious. In this paper, we\npresent a framework for the automation of generalised stability theory, which\novercomes these difficulties. Given a compact high-level symbolic\nrepresentation of a finite element discretisation implemented in the FEniCS\nsystem, efficient C++ code is automatically generated to assemble the forward,\ntangent linear and adjoint models; these models are then used to calculate the\noptimally growing perturbations to the forward model, and their growth rates.\nBy automating the stability computations, we hope to make these powerful tools\na more routine part of computational analysis. The efficiency and generality of\nthe framework is demonstrated with applications drawn from geophysical fluid\ndynamics, phase separation and quantum mechanics."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.01.005", 
    "link": "http://arxiv.org/pdf/1302.3894v1", 
    "title": "A framework for automated PDE-constrained optimisation", 
    "arxiv-id": "1302.3894v1", 
    "author": "P. E. Farrell", 
    "publish": "2013-02-15T21:30:51Z", 
    "summary": "A generic framework for the solution of PDE-constrained optimisation problems\nbased on the FEniCS system is presented. Its main features are an intuitive\nmathematical interface, a high degree of automation, and an efficient\nimplementation of the generated adjoint model. The framework is based upon the\nextension of a domain-specific language for variational problems to cleanly\nexpress complex optimisation problems in a compact, high-level syntax. For\nexample, optimisation problems constrained by the time-dependent Navier-Stokes\nequations can be written in tens of lines of code. Based on this high-level\nrepresentation, the framework derives the associated adjoint equations in the\nsame domain-specific language, and uses the FEniCS code generation technology\nto emit parallel optimised low-level C++ code for the solution of the forward\nand adjoint systems. The functional and gradient information so computed is\nthen passed to the optimisation algorithm to update the parameter values. This\napproach works both for steady-state as well as transient, and for linear as\nwell as nonlinear governing PDEs and a wide range of functionals and control\nparameters. We demonstrate the applicability and efficiency of this approach on\nclassical textbook optimisation problems and advanced examples."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1308.1472v1", 
    "title": "ForestClaw: Hybrid forest-of-octrees AMR for hyperbolic conservation   laws", 
    "arxiv-id": "1308.1472v1", 
    "author": "Andy R. Terrel", 
    "publish": "2013-08-07T04:15:42Z", 
    "summary": "We present a new hybrid paradigm for parallel adaptive mesh refinement (AMR)\nthat combines the scalability and lightweight architecture of tree-based AMR\nwith the computational efficiency of patch-based solvers for hyperbolic\nconservation laws. The key idea is to interpret each leaf of the AMR hierarchy\nas one uniform compute patch in $\\sR^d$ with $m^d$ degrees of freedom, where\n$m$ is customarily between 8 and 32. Thus, computation on each patch can be\noptimized for speed, while we inherit the flexibility of adaptive meshes. In\nour work we choose to integrate with the p4est AMR library since it allows us\nto compose the mesh from multiple mapped octrees and enables the cubed sphere\nand other nontrivial multiblock geometries. We describe aspects of the parallel\nimplementation and close with scalings for both MPI-only and OpenMP/MPI hybrid\nruns, where the largest MPI run executes on 16,384 CPU cores."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1309.5498v1", 
    "title": "High Precision Arithmetic for Scientific Applications", 
    "arxiv-id": "1309.5498v1", 
    "author": "Foster Morrison", 
    "publish": "2013-09-21T16:45:13Z", 
    "summary": "All but a few digital computers used for scientific computations have\nsupported floating-point and digital arithmetic of rather limited numerical\nprecision. The underlying assumptions were that the systems being studied were\nbasically deterministic and of limited complexity. The ideal scientific\nparadigm was the orbits of the major planets, which could be observed with high\nprecision, predicted for thousands of years into the future, and extrapolated\nfor thousands of years into the past. Much the same technology that has made\ncomputers possible has also provided instrumentation that has vastly expanded\nthe scope and precision of scientific analysis. Complex nonlinear systems\nexhibiting so-called chaotic dynamics are now fair game for scientists and\nengineers in every discipline. Today it seems that computers need to enhance\nthe precision of their numerical computations to support the needs of science.\nHowever, there is no need to wait for the necessary updates in both hardware\nand software; it is easy enough to monitor numerical precision with a few minor\nmodifications to existing software."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1402.5835v2", 
    "title": "Polcovar: Software for Computing the Mean and Variance of Subgraph   Counts in Random Graphs", 
    "arxiv-id": "1402.5835v2", 
    "author": "J\u00e9r\u00f4me Kunegis", 
    "publish": "2014-02-24T14:26:08Z", 
    "summary": "The mean and variance of the number of appearances of a given subgraph $H$ in\nan Erd\\H{o}s--R\\'enyi random graph over $n$ nodes are rational polynomials in\n$n$. We present a piece of software named Polcovar (from \"polynomial\" and\n\"covariance\") that computes the exact rational coefficients of these\npolynomials in function of $H$."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1404.3406v1", 
    "title": "Knowledge-Based Automatic Generation of Linear Algebra Algorithms and   Code", 
    "arxiv-id": "1404.3406v1", 
    "author": "Diego Fabregat-Traver", 
    "publish": "2014-04-13T18:13:32Z", 
    "summary": "This dissertation focuses on the design and the implementation of\ndomain-specific compilers for linear algebra matrix equations. The development\nof efficient libraries for such equations, which lie at the heart of most\nsoftware for scientific computing, is a complex process that requires expertise\nin a variety of areas, including the application domain, algorithms, numerical\nanalysis and high-performance computing. Moreover, the process involves the\ncollaboration of several people for a considerable amount of time. With our\ncompilers, we aim to relieve the developers from both designing algorithms and\nwriting code, and to generate routines that match or even surpass the\nperformance of those written by human experts."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1406.5369v1", 
    "title": "A Scala Prototype to Generate Multigrid Solver Implementations for   Different Problems and Target Multi-Core Platforms", 
    "arxiv-id": "1406.5369v1", 
    "author": "Ulrich Ruede", 
    "publish": "2014-06-20T12:46:27Z", 
    "summary": "Many problems in computational science and engineering involve partial\ndifferential equations and thus require the numerical solution of large, sparse\n(non)linear systems of equations. Multigrid is known to be one of the most\nefficient methods for this purpose. However, the concrete multigrid algorithm\nand its implementation highly depend on the underlying problem and hardware.\nTherefore, changes in the code or many different variants are necessary to\ncover all relevant cases. In this article we provide a prototype implementation\nin Scala for a framework that allows abstract descriptions of PDEs, their\ndiscretization, and their numerical solution via multigrid algorithms. From\nthese, one is able to generate data structures and implementations of multigrid\ncomponents required to solve elliptic PDEs on structured grids. Two different\ntest problems showcase our proposed automatic generation of multigrid solvers\nfor both CPU and GPU target platforms."
},{
    "category": "cs.MS", 
    "doi": "10.3233/978-1-61499-381-0-253", 
    "link": "http://arxiv.org/pdf/1408.1363v1", 
    "title": "Lighthouse: A User-Centered Web Service for Linear Algebra Software", 
    "arxiv-id": "1408.1363v1", 
    "author": "Elizabeth Jessup", 
    "publish": "2014-08-06T17:37:03Z", 
    "summary": "Various fields of science and engineering rely on linear algebra for large\nscale data analysis, modeling and simulation, machine learning, and other\napplied problems. Linear algebra computations often dominate the execution time\nof such applications. Meanwhile, experts in these domains typically lack the\ntraining or time required to develop efficient, high-performance\nimplementations of linear algebra algorithms. In the Lighthouse project, we\nenable developers with varied backgrounds to readily discover and effectively\napply the best available numerical software for their problems. We have\ndeveloped a search-based expert system that combines expert knowledge, machine\nlearningbased classification of existing numerical software collections, and\nautomated code generation and optimization. Lighthouse provides a novel\nsoftware engineering environment aimed at maximizing both developer\nproductivity and application performance for dense and sparse linear algebra\ncomputations."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2964377", 
    "link": "http://arxiv.org/pdf/1408.3082v2", 
    "title": "Algorithm xxx: RIDC Methods -- A Family of Parallel Time-Integrators", 
    "arxiv-id": "1408.3082v2", 
    "author": "Kyle Ladd", 
    "publish": "2014-08-13T18:42:01Z", 
    "summary": "Revisionist integral deferred correction (RIDC) methods are a family of\nparallel--in--time methods to solve systems of initial values problems. The\napproach is able to bootstrap lower order time integrators to provide high\norder approximations in approximately the same wall clock time, hence providing\na multiplicative increase in the number of compute cores utilized. Here we\nprovide a C++ framework which automatically produces a parallel--in--time\nsolution of a system of initial value problems given user supplied code for the\nright hand side of the system and a sequential code for a first-order time\nstep. The user supplied time step routine may be explicit or implicit and may\nmake use of any auxiliary libraries which take care of the solution of any\nnonlinear algebraic systems which may arise or the numerical linear algebra\nrequired. The code contains six examples of increasing complexity which also\nserve as templates to solve user defined problems."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2964377", 
    "link": "http://arxiv.org/pdf/1409.2008v2", 
    "title": "Computing the coefficients for the power series solution of the   Lane-Emden equation with the Python library SymPy", 
    "arxiv-id": "1409.2008v2", 
    "author": "Klaus Rohe", 
    "publish": "2014-09-06T12:19:37Z", 
    "summary": "It is shown how the Python library Sympy can be used to compute symbolically\nthe coefficients of the power series solution of the Lane-Emden equation (LEE).\nSympy is an open source Python library for symbolic mathematics. The power\nseries solutions are compared to the numerically computed solutions using\nmatplotlib. The results of a run time measurement of the implemented algorithm\nare discussed at the end."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.03.013", 
    "link": "http://arxiv.org/pdf/1409.8186v1", 
    "title": "$\u03bc$-diff: an open-source Matlab toolbox for computing multiple   scattering problems by disks", 
    "arxiv-id": "1409.8186v1", 
    "author": "Hasan Alzubaidi", 
    "publish": "2014-09-29T16:48:16Z", 
    "summary": "The aim of this paper is to describe a Matlab toolbox, called $\\mu$-diff, for\nmodeling and numerically solving two-dimensional complex multiple scattering by\na large collection of circular cylinders. The approximation methods in\n$\\mu$-diff are based on the Fourier series expansions of the four basic\nintegral operators arising in scattering theory. Based on these expressions, an\nefficient spectrally accurate finite-dimensional solution of multiple\nscattering problems can be simply obtained for complex media even when many\nscatterers are considered as well as large frequencies. The solution of the\nglobal linear system to solve can use either direct solvers or preconditioned\niterative Krylov subspace solvers for block Toeplitz matrices. Based on this\napproach, this paper explains how the code is built and organized. Some\ncomplete numerical examples of applications (direct and inverse scattering) are\nprovided to show that $\\mu$-diff is a flexible, efficient and robust toolbox\nfor solving some complex multiple scattering problems."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1644001.1644009", 
    "link": "http://arxiv.org/pdf/1104.0199v1", 
    "title": "Optimisations for quadrature representations of finite element tensors   through automated code generation", 
    "arxiv-id": "1104.0199v1", 
    "author": "Garth N. Wells", 
    "publish": "2011-04-01T15:29:05Z", 
    "summary": "We examine aspects of the computation of finite element matrices and vectors\nwhich are made possible by automated code generation. Given a variational form\nin a syntax which resembles standard mathematical notation, the low-level\ncomputer code for building finite element tensors, typically matrices, vectors\nand scalars, can be generated automatically via a form compiler. In particular,\nthe generation of code for computing finite element matrices using a quadrature\napproach is addressed. For quadrature representations, a number of optimisation\nstrategies which are made possible by automated code generation are presented.\nThe relative performance of two different automatically generated\nrepresentations of finite element matrices is examined, with a particular\nemphasis on complicated variational forms. It is shown that approaches which\nperform best for simple forms are not tractable for more complicated problems\nin terms of run time performance, the time required to generate the code or the\nsize of the generated code. The approach and optimisations elaborated here are\neffective for a range of variational forms."
},{
    "category": "cs.MS", 
    "doi": "10.1137/070710032", 
    "link": "http://arxiv.org/pdf/1104.0628v1", 
    "title": "Automated code generation for discontinuous Galerkin methods", 
    "arxiv-id": "1104.0628v1", 
    "author": "Garth N. Wells", 
    "publish": "2011-04-04T17:12:55Z", 
    "summary": "A compiler approach for generating low-level computer code from high-level\ninput for discontinuous Galerkin finite element forms is presented. The input\nlanguage mirrors conventional mathematical notation, and the compiler generates\nefficient code in a standard programming language. This facilitates the rapid\ngeneration of efficient code for general equations in varying spatial\ndimensions. Key concepts underlying the compiler approach and the automated\ngeneration of computer code are elaborated. The approach is demonstrated for a\nrange of common problems, including the Poisson, biharmonic,\nadvection--diffusion and Stokes equations."
},{
    "category": "cs.MS", 
    "doi": "10.1137/070710032", 
    "link": "http://arxiv.org/pdf/1104.1533v1", 
    "title": "Operand Folding Hardware Multipliers", 
    "arxiv-id": "1104.1533v1", 
    "author": "Karim Sabeg", 
    "publish": "2011-04-08T09:40:04Z", 
    "summary": "This paper describes a new accumulate-and-add multiplication algorithm. The\nmethod partitions one of the operands and re-combines the results of\ncomputations done with each of the partitions. The resulting design turns-out\nto be both compact and fast.\n  When the operands' bit-length $m$ is 1024, the new algorithm requires only\n$0.194m+56$ additions (on average), this is about half the number of additions\nrequired by the classical accumulate-and-add multiplication algorithm\n($\\frac{m}2$)."
},{
    "category": "cs.MS", 
    "doi": "10.1137/070710032", 
    "link": "http://arxiv.org/pdf/1109.1264v1", 
    "title": "A New Vectorization Technique for Expression Templates in C++", 
    "arxiv-id": "1109.1264v1", 
    "author": "A. Adelmann", 
    "publish": "2011-09-06T18:42:48Z", 
    "summary": "Vector operations play an important role in high performance computing and\nare typically provided by highly optimized libraries that implement the BLAS\n(Basic Linear Algebra Subprograms) interface. In C++ templates and operator\noverloading allow the implementation of these vector operations as expression\ntemplates which construct custom loops at compile time and providing a more\nabstract interface. Unfortunately existing expression template libraries lack\nthe performance of fast BLAS(Basic Linear Algebra Subprograms) implementations.\nThis paper presents a new approach - Statically Accelerated Loop Templates\n(SALT) - to close this performance gap by combining expression templates with\nan aggressive loop unrolling technique. Benchmarks were conducted using the\nIntel C++ compiler and GNU Compiler Collection to assess the performance of our\nlibrary relative to Intel's Math Kernel Library as well as the Eigen template\nlibrary. The results show that the approach is able to provide optimization\ncomparable to the fastest available BLAS implementations, while retaining the\nconvenience and flexibility of a template library."
},{
    "category": "cs.MS", 
    "doi": "10.1137/070710032", 
    "link": "http://arxiv.org/pdf/1204.3020v1", 
    "title": "TeXmacs-Reduce interface", 
    "arxiv-id": "1204.3020v1", 
    "author": "Andrey Grozin", 
    "publish": "2012-04-12T16:36:59Z", 
    "summary": "This tutorial (based on the talk at the TeXmacs workshop in Faro, Portugal,\nFebruary 26 - March 2, 2012) describes the new and improved Reduce plugin in\nGNU TeXmacs."
},{
    "category": "cs.MS", 
    "doi": "10.1137/120873558", 
    "link": "http://arxiv.org/pdf/1204.5577v2", 
    "title": "Automated derivation of the adjoint of high-level transient finite   element programs", 
    "arxiv-id": "1204.5577v2", 
    "author": "Marie E. Rognes", 
    "publish": "2012-04-25T07:25:31Z", 
    "summary": "In this paper we demonstrate a new technique for deriving discrete adjoint\nand tangent linear models of finite element models. The technique is\nsignificantly more efficient and automatic than standard algorithmic\ndifferentiation techniques. The approach relies on a high-level symbolic\nrepresentation of the forward problem. In contrast to developing a model\ndirectly in Fortran or C++, high-level systems allow the developer to express\nthe variational problems to be solved in near-mathematical notation. As such,\nthese systems have a key advantage: since the mathematical structure of the\nproblem is preserved, they are more amenable to automated analysis and\nmanipulation. The framework introduced here is implemented in a freely\navailable software package named dolfin-adjoint, based on the FEniCS Project.\nOur approach to automated adjoint derivation relies on run-time annotation of\nthe temporal structure of the model, and employs the FEniCS finite element form\ncompiler to automatically generate the low-level code for the derived models.\nThe approach requires only trivial changes to a large class of forward models,\nincluding complicated time-dependent nonlinear models. The adjoint model\nautomatically employs optimal checkpointing schemes to mitigate storage\nrequirements for nonlinear models, without any user management or intervention.\nFurthermore, both the tangent linear and adjoint models naturally work in\nparallel, without any need to differentiate through calls to MPI or to parse\nOpenMP directives. The generality, applicability and efficiency of the approach\nare demonstrated with examples from a wide range of scientific applications."
},{
    "category": "cs.MS", 
    "doi": "10.1137/120873558", 
    "link": "http://arxiv.org/pdf/1307.3088v1", 
    "title": "The Declaratron, semantic specification for scientific computation using   MathML", 
    "arxiv-id": "1307.3088v1", 
    "author": "Peter Murray-Rust", 
    "publish": "2013-07-11T12:35:11Z", 
    "summary": "We introduce the Declaratron, a system which takes a declarative approach to\nspecifying mathematically based scientific computation. This uses displayable\nmathematical notation (Content MathML) and is both executable and semantically\nwell defined. We combine domain specific representations of physical science\n(e.g. CML, Chemical Markup Language), MathML formulae and computational\nspecifications (DeXML) to create executable documents which include scientific\ndata and mathematical formulae. These documents preserve the provenance of the\ndata used, and build tight semantic links between components of mathematical\nformulae and domain objects---in effect grounding the mathematical semantics in\nthe scientific domain. The Declaratron takes these specifications and i)\ncarries out entity resolution and decoration to prepare for computation ii)\nuses a MathML execution engine to run calculations over the revised tree iii)\noutputs domain objects and the complete document to give both results and an\nencapsulated history of the computation. A short description of a case study is\ngiven to illustrate how the system can be used. Many scientific problems\nrequire frequent change of the mathematical functional form and the Declaratron\nprovides this without requiring changes to code. Additionally, it supports\nreproducible science, machine indexing and semantic search of computations,\nmakes implicit assumptions visible, and separates domain knowledge from\ncomputational techniques. We believe that the Declaratron could replace much\nconventional procedural code in science."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.camwa.2014.01.021", 
    "link": "http://arxiv.org/pdf/1310.1191v1", 
    "title": "Numerical integration on GPUs for higher order finite elements", 
    "arxiv-id": "1310.1191v1", 
    "author": "Pawe\u0142 Macio\u0142", 
    "publish": "2013-10-04T07:50:02Z", 
    "summary": "The paper considers the problem of implementation on graphics processors of\nnumerical integration routines for higher order finite element approximations.\nThe design of suitable GPU kernels is investigated in the context of general\npurpose integration procedures, as well as particular example applications. The\nmost important characteristic of the problem investigated is the large\nvariation of required processor and memory resources associated with different\ndegrees of approximating polynomials. The questions that we try to answer are\nwhether it is possible to design a single integration kernel for different GPUs\nand different orders of approximation and what performance can be expected in\nsuch a case."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.camwa.2013.08.026", 
    "link": "http://arxiv.org/pdf/1310.1194v1", 
    "title": "Vectorized OpenCL implementation of numerical integration for higher   order finite elements", 
    "arxiv-id": "1310.1194v1", 
    "author": "Krzysztof Bana\u015b", 
    "publish": "2013-10-04T08:08:04Z", 
    "summary": "In our work we analyze computational aspects of the problem of numerical\nintegration in finite element calculations and consider an OpenCL\nimplementation of related algorithms for processors with wide vector registers.\n  As a platform for testing the implementation we choose the PowerXCell\nprocessor, being an example of the Cell Broadband Engine (CellBE) architecture.\nAlthough the processor is considered old for today's standards (its design\ndates back to year 2001), we investigate its performance due to two features\nthat it shares with recent Xeon Phi family of coprocessors: wide vector units\nand relatively slow connection of computing cores with main global memory. The\nperformed analysis of parallelization options can also be used for designing\nnumerical integration algorithms for other processors with vector registers,\nsuch as contemporary x86 microprocessors."
},{
    "category": "cs.MS", 
    "doi": "10.1109/ACSSC.2013.6810471", 
    "link": "http://arxiv.org/pdf/1312.0455v1", 
    "title": "Radix Conversion for IEEE754-2008 Mixed Radix Floating-Point Arithmetic", 
    "arxiv-id": "1312.0455v1", 
    "author": "J. -M. Muller", 
    "publish": "2013-12-02T13:47:14Z", 
    "summary": "Conversion between binary and decimal floating-point representations is\nubiquitous. Floating-point radix conversion means converting both the exponent\nand the mantissa. We develop an atomic operation for FP radix conversion with\nsimple straight-line algorithm, suitable for hardware design. Exponent\nconversion is performed with a small multiplication and a lookup table. It\nyields the correct result without error. Mantissa conversion uses a few\nmultiplications and a small lookup table that is shared amongst all types of\nconversions. The accuracy changes by adjusting the computing precision."
},{
    "category": "cs.MS", 
    "doi": "10.1109/ACSSC.2013.6810471", 
    "link": "http://arxiv.org/pdf/1401.1942v3", 
    "title": "Test Problem Construction for Single-Objective Bilevel Optimization", 
    "arxiv-id": "1401.1942v3", 
    "author": "Kalyanmoy Deb", 
    "publish": "2014-01-09T10:22:26Z", 
    "summary": "In this paper, we propose a procedure for designing controlled test problems\nfor single-objective bilevel optimization. The construction procedure is\nflexible and allows its user to control the different complexities that are to\nbe included in the test problems independently of each other. In addition to\nproperties that control the difficulty in convergence, the procedure also\nallows the user to introduce difficulties caused by interaction of the two\nlevels. As a companion to the test problem construction framework, the paper\npresents a standard test suite of twelve problems, which includes eight\nunconstrained and four constrained problems. Most of the problems are scalable\nin terms of variables and constraints. To provide baseline results, we have\nsolved the proposed test problems using a nested bilevel evolutionary\nalgorithm. The results can be used for comparison, while evaluating the\nperformance of any other bilevel optimization algorithm. The codes related to\nthe paper may be accessed from the website \\url{http://bilevel.org}."
},{
    "category": "cs.MS", 
    "doi": "10.1109/ACSSC.2013.6810471", 
    "link": "http://arxiv.org/pdf/1401.7962v1", 
    "title": "Numerical application and Turbo C program using the Gauss-Jordan Method", 
    "arxiv-id": "1401.7962v1", 
    "author": "Cornelia Victoria", 
    "publish": "2014-01-29T10:44:45Z", 
    "summary": "The article presents the general notions and algorithm about the Gauss-Jordan\nmethod. An eloquent example is given and the Turbo C program illustrated this\nmethod. We conclude that we can obtain by this method the determinant, by\nsimple calculations and reducing the rounding errors"
},{
    "category": "cs.MS", 
    "doi": "10.1109/ACSSC.2013.6810471", 
    "link": "http://arxiv.org/pdf/1403.6870v2", 
    "title": "A modified ziggurat algorithm for generating exponentially- and   normally-distributed pseudorandom numbers", 
    "arxiv-id": "1403.6870v2", 
    "author": "Christopher D McFarland", 
    "publish": "2014-03-26T21:51:41Z", 
    "summary": "The Ziggurat Algorithm is a very fast rejection sampling method for\ngenerating PseudoRandom Numbers (PRNs) from common statistical distributions.\nThe algorithm divides a distribution into rectangular layers that stack on top\nof each other (resembling a Ziggurat), subsuming the desired distribution.\nRandom values within these rectangular layers are then sampled by rejection.\nThis implementation splits layers into two types: those constituting the\nmajority that fall completely under the distribution and can be sampled\nextremely fast without a rejection test, and a few additional layers that\nencapsulate the fringe of the distribution and require a rejection test. This\nmethod offers speedups of 65% for exponentially- and 82% for\nnormally-distributed PRNs when compared to the best available C implementations\nof these generators. Even greater speedups are obtained when the algorithm is\nextended to the Python and MATLAB/OCTAVE programming environments."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1407.2925v1", 
    "title": "A data porting tool for coupling models with different discretization   needs", 
    "arxiv-id": "1407.2925v1", 
    "author": "Virgil Iordache", 
    "publish": "2014-07-09T10:45:40Z", 
    "summary": "The presented work is part of a larger research program dealing with\ndeveloping tools for coupling biogeochemical models in contaminated landscapes.\nThe specific objective of this article is to provide the researchers a tool to\nbuild hexagonal raster using information from a rectangular raster data (e.g.\nGIS format), data porting. This tool involves a computational algorithm and an\nopen source software (written in C). The method of extending the reticulated\nfunctions defined on 2D networks is an essential key of this algorithm and can\nalso be used for other purposes than data porting. The algorithm allows one to\nbuild the hexagonal raster with a cell size independent from the geometry of\nthe rectangular raster. The extended function is a bi-cubic spline which can\nexactly reconstruct polynomials up to degree three in each variable. We\nvalidate the method by analyzing errors in some theoretical case studies\nfollowed by other studies with real terrain elevation data. We also introduce\nand briefly present an iterative water routing method and use it for validation\non a case with concrete terrain data."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1407.3383v1", 
    "title": "Modular SIMD arithmetic in Mathemagix", 
    "arxiv-id": "1407.3383v1", 
    "author": "Guillaume Quintin", 
    "publish": "2014-07-12T13:21:01Z", 
    "summary": "Modular integer arithmetic occurs in many algorithms for computer algebra,\ncryptography, and error correcting codes. Although recent microprocessors\ntypically offer a wide range of highly optimized arithmetic functions, modular\ninteger operations still require dedicated implementations. In this article, we\nsurvey existing algorithms for modular integer arithmetic, and present detailed\nvectorized counterparts. We also present several applications, such as fast\nmodular Fourier transforms and multiplication of integer polynomials and\nmatrices. The vectorized algorithms have been implemented in C++ inside the\nfree computer algebra and analysis system Mathemagix. The performance of our\nimplementation is illustrated by various benchmarks."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1410.1726v1", 
    "title": "KBLAS: An Optimized Library for Dense Matrix-Vector Multiplication on   GPU Accelerators", 
    "arxiv-id": "1410.1726v1", 
    "author": "Hatem Ltaief", 
    "publish": "2014-10-07T13:43:53Z", 
    "summary": "KBLAS is a new open source high performance library that provides optimized\nkernels for a subset of Level 2 BLAS functionalities on CUDA-enabled GPUs.\nSince performance of dense matrix-vector multiplication is hindered by the\noverhead of memory accesses, a double-buffering optimization technique is\nemployed to overlap data motion with computation. After identifying a proper\nset of tuning parameters, KBLAS is able to efficiently run on various GPU\narchitectures across different generations, avoiding the time-consuming step of\ncode rewriting, while still being compliant with the standard BLAS API. Another\nadvanced optimization technique allows to ensure coalesced memory access when\ndealing with submatrices, especially in the context of high level dense linear\nalgebra algorithms. All four precisions KBLAS kernels have been leveraged to\nmulti-GPUs environment, which requires the introduction of new APIs to ease\nusers' experiences on these challenging systems. The KBLAS performance\noutperforms existing state-of-the-art implementations on all matrix sizes,\nachieves asymptotically up to 50% and 60% speedup on single GPU and multi-GPUs\nsystems, respectively, and validates our performance model. A subset of KBLAS\nhigh performance kernels has been integrated into NVIDIA's standard BLAS\nimplementation (cuBLAS) for larger dissemination, starting version 6.0."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1411.1607v4", 
    "title": "Julia: A Fresh Approach to Numerical Computing", 
    "arxiv-id": "1411.1607v4", 
    "author": "Viral B. Shah", 
    "publish": "2014-11-06T13:39:40Z", 
    "summary": "Bridging cultures that have often been distant, Julia combines expertise from\nthe diverse fields of computer science and computational science to create a\nnew approach to numerical computing. Julia is designed to be easy and fast.\nJulia questions notions generally held as \"laws of nature\" by practitioners of\nnumerical computing:\n  1. High-level dynamic programs have to be slow.\n  2. One must prototype in one language and then rewrite in another language\nfor speed or deployment, and\n  3. There are parts of a system for the programmer, and other parts best left\nuntouched as they are built by the experts.\n  We introduce the Julia programming language and its design --- a dance\nbetween specialization and abstraction. Specialization allows for custom\ntreatment. Multiple dispatch, a technique from computer science, picks the\nright algorithm for the right circumstance. Abstraction, what good computation\nis really about, recognizes what remains the same after differences are\nstripped away. Abstractions in mathematics are captured as code through another\ntechnique from computer science, generic programming.\n  Julia shows that one can have machine performance without sacrificing human\nconvenience."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1412.5316v1", 
    "title": "Twofolds in C and C++", 
    "arxiv-id": "1412.5316v1", 
    "author": "Evgeny Latkin", 
    "publish": "2014-12-17T10:06:49Z", 
    "summary": "Here I propose C and C++ interfaces and experimental implementation for\ntwofolds arithmetic. I introduce twofolds in my previous article entitled\n\"Twofold fast arithmetic\" for tracking floating-point inaccuracy. Testing\nshows, plain C enables high-performance computing with twofolds. C++ interface\nenables coding as easily as ordinary floating-point numbers. My goal is\nconvincing you to try twofolds; I think assuring accuracy of math computations\nis worth its cost. Code and use examples available at my web site, references\ninside."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1502.01367v2", 
    "title": "Visualizing Marden's theorem with Scilab", 
    "arxiv-id": "1502.01367v2", 
    "author": "Klaus Rohe", 
    "publish": "2015-02-04T21:48:25Z", 
    "summary": "A theorem which is named after the American Mathematician Moris Marden states\na very surprising and interesting fact concerning the relationship between the\npoints of a triangle in the complex plane and the zeros of two complex\npolynomials related to this triangle: \"Suppose the zeroes z1, z2, and z3 of a\nthird-degree polynomial p(z) are non-collinear. There is a unique ellipse\ninscribed in the triangle with vertices z1, z2, z3 and tangent to the sides at\ntheir midpoints: the Steiner in-ellipse. The foci of that ellipse are the\nzeroes of the derivative p'(z).\" (Wikipedia contributors, \"Marden's theorem\",\nhttp://en.wikipedia.org/wiki/Marden%27s_theorem). This document describes how\nScilab, a popular and powerful open source alternative to MATLAB, can be used\nto visualize the above stated theorem for arbitrary complex numbers z1, z2, and\nz3 which are not collinear. It is further demonstrated how the equations of the\nSteiner ellipses of a triangle in the complex plane can be calculated and\nplotted by applying this theorem."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1502.05216v1", 
    "title": "Twofold exp and log", 
    "arxiv-id": "1502.05216v1", 
    "author": "Evgeny Latkin", 
    "publish": "2015-02-17T02:55:35Z", 
    "summary": "This article is about twofold arithmetic. Here I introduce algorithms and\nexperimental code for twofold variant of C/C++ standard functions exp() and\nlog(), and expm1() and log1p(). Twofold function $y_0+y_1 \\approx f(x_0+x_1)$\nis nearly 2x-precise so can assess accuracy of standard one. Performance allows\nassessing on-fly: twofold texp() over double is ~10x times faster than expq()\nby GNU quadmath."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1502.07405v1", 
    "title": "An efficient multi-core implementation of a novel HSS-structured   multifrontal solver using randomized sampling", 
    "arxiv-id": "1502.07405v1", 
    "author": "Artem Napov", 
    "publish": "2015-02-25T23:54:16Z", 
    "summary": "We present a sparse linear system solver that is based on a multifrontal\nvariant of Gaussian elimination, and exploits low-rank approximation of the\nresulting dense frontal matrices. We use hierarchically semiseparable (HSS)\nmatrices, which have low-rank off-diagonal blocks, to approximate the frontal\nmatrices. For HSS matrix construction, a randomized sampling algorithm is used\ntogether with interpolative decompositions. The combination of the randomized\ncompression with a fast ULV HSS factorization leads to a solver with lower\ncomputational complexity than the standard multifrontal method for many\napplications, resulting in speedups up to 7 fold for problems in our test\nsuite. The implementation targets many-core systems by using task parallelism\nwith dynamic runtime scheduling. Numerical experiments show performance\nimprovements over state-of-the-art sparse direct solvers. The implementation\nachieves high performance and good scalability on a range of modern shared\nmemory parallel systems, including the Intel Xeon Phi (MIC). The code is part\nof a software package called STRUMPACK -- STRUctured Matrices PACKage, which\nalso has a distributed memory component for dense rank-structured matrices."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1503.04955v1", 
    "title": "Fast Multiplication of Large Integers: Implementation and Analysis of   the DKSS Algorithm", 
    "arxiv-id": "1503.04955v1", 
    "author": "Christoph L\u00fcders", 
    "publish": "2015-03-17T09:03:34Z", 
    "summary": "The Sch\\\"onhage-Strassen algorithm (SSA) is the de-facto standard for\nmultiplication of large integers. For $N$-bit numbers it has a time bound of\n$O(N \\cdot \\log N \\cdot \\log \\log N)$. De, Kurur, Saha and Saptharishi (DKSS)\npresented an asymptotically faster algorithm with a better time bound of $N\n\\cdot \\log N \\cdot 2^{O(\\log^* N)}$. In this diploma thesis, results of an\nimplementation of DKSS multiplication are presented: run-time is about 30 times\nlarger than SSA, while memory requirements are about 3.75 times higher than\nSSA. A possible crossover point is estimated to be out of reach even if we\nutilized the whole universe for computer memory."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1503.05464v2", 
    "title": "A distributed-memory package for dense Hierarchically Semi-Separable   matrix computations using randomization", 
    "arxiv-id": "1503.05464v2", 
    "author": "Artem Napov", 
    "publish": "2015-03-18T16:01:25Z", 
    "summary": "We present a distributed-memory library for computations with dense\nstructured matrices. A matrix is considered structured if its off-diagonal\nblocks can be approximated by a rank-deficient matrix with low numerical rank.\nHere, we use Hierarchically Semi-Separable representations (HSS). Such matrices\nappear in many applications, e.g., finite element methods, boundary element\nmethods, etc. Exploiting this structure allows for fast solution of linear\nsystems and/or fast computation of matrix-vector products, which are the two\nmain building blocks of matrix computations. The compression algorithm that we\nuse, that computes the HSS form of an input dense matrix, relies on randomized\nsampling with a novel adaptive sampling mechanism. We discuss the\nparallelization of this algorithm and also present the parallelization of\nstructured matrix-vector product, structured factorization and solution\nroutines. The efficiency of the approach is demonstrated on large problems from\ndifferent academic and industrial applications, on up to 8,000 cores.\n  This work is part of a more global effort, the STRUMPACK (STRUctured Matrices\nPACKage) software package for computations with sparse and dense structured\nmatrices. Hence, although useful on their own right, the routines also\nrepresent a step in the direction of a distributed-memory sparse solver."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.envsoft.2014.09.012", 
    "link": "http://arxiv.org/pdf/1503.06544v2", 
    "title": "GAIL---Guaranteed Automatic Integration Library in MATLAB: Documentation   for Version 2.1", 
    "arxiv-id": "1503.06544v2", 
    "author": "Xuan Zhou", 
    "publish": "2015-03-23T07:45:09Z", 
    "summary": "Automatic and adaptive approximation, optimization, or integration of\nfunctions in a cone with guarantee of accuracy is a relatively new paradigm.\nOur purpose is to create an open-source MATLAB package, Guaranteed Automatic\nIntegration Library (GAIL), following the philosophy of reproducible research\nand sustainable practices of robust scientific software development. For our\nconviction that true scholarship in computational sciences are characterized by\nreliable reproducibility, we employ the best practices in mathematical research\nand software engineering known to us and available in MATLAB. This document\ndescribes the key features of functions in GAIL, which includes one-dimensional\nfunction approximation and minimization using linear splines, one-dimensional\nnumerical integration using trapezoidal rule, and last but not least, mean\nestimation and multidimensional integration by Monte Carlo methods or Quasi\nMonte Carlo methods."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1504.01023v1", 
    "title": "Finite element numerical integration for first order approximations on   multi-core architectures", 
    "arxiv-id": "1504.01023v1", 
    "author": "Jan Biela\u0144ski", 
    "publish": "2015-04-04T16:56:02Z", 
    "summary": "The paper presents investigations on the implementation and performance of\nthe finite element numerical integration algorithm for first order\napproximations and three processor architectures, popular in scientific\ncomputing, classical CPU, Intel Xeon Phi and NVIDIA Kepler GPU. A unifying\nprogramming model and portable OpenCL implementation is considered for all\narchitectures. Variations of the algorithm due to different problems solved and\ndifferent element types are investigated and several optimizations aimed at\nproper optimization and mapping of the algorithm to computer architectures are\ndemonstrated. Performance models of execution are developed for different\nprocessors and tested in practical experiments. The results show the varying\nlevels of performance for different architectures, but indicate that the\nalgorithm can be effectively ported to all of them. The general conclusion is\nthat the finite element numerical integration can achieve sufficient\nperformance on different multi- and many-core architectures and should not\nbecome a performance bottleneck for finite element simulation codes. Specific\nobservations lead to practical advises on how to optimize the kernels and what\nperformance can be expected for the tested architectures."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1504.06734v1", 
    "title": "Symmetric matrix inversion using modified Gaussian elimination", 
    "arxiv-id": "1504.06734v1", 
    "author": "Nicolai Savelov", 
    "publish": "2015-04-25T14:45:33Z", 
    "summary": "In this paper we present two different variants of method for symmetric\nmatrix inversion, based on modified Gaussian elimination. Both methods avoid\ncomputation of square roots and have a reduced machine time's spending.\nFurther, both of them can be used efficiently not only for positive (semi-)\ndefinite, but for any non-singular symmetric matrix inversion. We use\nsimulation to verify results, which represented in this paper."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1505.00838v1", 
    "title": "Sparse Automatic Differentiation for Large-Scale Computations Using   Abstract Elementary Algebra", 
    "arxiv-id": "1505.00838v1", 
    "author": "Stefan Klus", 
    "publish": "2015-05-04T23:05:54Z", 
    "summary": "Most numerical solvers and libraries nowadays are implemented to use\nmathematical models created with language-specific built-in data types (e.g.\nreal in Fortran or double in C) and their respective elementary algebra\nimplementations. However, built-in elementary algebra typically has limited\nfunctionality and often restricts flexibility of mathematical models and\nanalysis types that can be applied to those models. To overcome this\nlimitation, a number of domain-specific languages with more feature-rich\nbuilt-in data types have been proposed. In this paper, we argue that if\nnumerical libraries and solvers are designed to use abstract elementary algebra\nrather than language-specific built-in algebra, modern mainstream languages can\nbe as effective as any domain-specific language. We illustrate our ideas using\nthe example of sparse Jacobian matrix computation. We implement an automatic\ndifferentiation method that takes advantage of sparse system structures and is\nstraightforward to parallelize in MPI setting. Furthermore, we show that the\ncomputational cost scales linearly with the size of the system."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1505.01577v1", 
    "title": "Documentation Generator Focusing on Symbols for the HTML-ized Mizar   Library", 
    "arxiv-id": "1505.01577v1", 
    "author": "Yasunari Shidama", 
    "publish": "2015-05-07T04:09:24Z", 
    "summary": "The purpose of this project is to collect symbol information in the Mizar\nMathematical Library and manipulate it into practical and organized\ndocumentation. Inspired by the MathWiki project and API reference systems for\ncomputer programs, we developed a documentation generator focusing on symbols\nfor the HTML-ized Mizar library. The system has several helpful features,\nincluding a symbol list, incremental search, and a referrer list. It targets\nthose who use proof assistance systems, the volume of whose libraries has been\nrapidly increasing year by year."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1505.04633v1", 
    "title": "Flexible, Scalable Mesh and Data Management using PETSc DMPlex", 
    "arxiv-id": "1505.04633v1", 
    "author": "Gerard J. Gorman", 
    "publish": "2015-05-18T13:35:50Z", 
    "summary": "Designing a scientific software stack to meet the needs of the\nnext-generation of mesh-based simulation demands, not only scalable and\nefficient mesh and data management on a wide range of platforms, but also an\nabstraction layer that makes it useful for a wide range of application codes.\nCommon utility tasks, such as file I/O, mesh distribution, and work\npartitioning, should be delegated to external libraries in order to promote\ncode re-use, extensibility and software interoperability. In this paper we\ndemonstrate the use of PETSc's DMPlex data management API to perform mesh input\nand domain partitioning in Fluidity, a large scale CFD application. We\ndemonstrate that raising the level of abstraction adds new functionality to the\napplication code, such as support for additional mesh file formats and mesh re-\nordering, while improving simulation startup cost through more efficient mesh\ndistribution. Moreover, the separation of concerns accomplished through this\ninterface shifts critical performance and interoperability issues, such as\nscalable I/O and file format support, to a widely used and supported open\nsource community library, improving the sustainability, performance, and\nfunctionality of Fluidity."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1506.01598v2", 
    "title": "decimalInfinite: All Decimals In Bits, No Loss, Same Order, Simple", 
    "arxiv-id": "1506.01598v2", 
    "author": "Ghislain Fourny", 
    "publish": "2015-06-04T14:09:47Z", 
    "summary": "This paper introduces a binary encoding that supports arbitrarily large,\nsmall and precise decimals. It completely preserves information and order. It\ndoes not rely on any arbitrary use-case-based choice of calibration and is\nreadily implementable and usable, as is. Finally, it is also simple to explain\nand understand."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2016.03.038", 
    "link": "http://arxiv.org/pdf/1506.04496v3", 
    "title": "The Peano software - parallel, automaton-based, dynamically adaptive   grid traversals", 
    "arxiv-id": "1506.04496v3", 
    "author": "Tobias Weinzierl", 
    "publish": "2015-06-15T07:33:41Z", 
    "summary": "We introduce the third generation of Peano, a framework for dynamically\nadaptive Cartesian meshes derived from spacetrees. Peano ties the mesh\ntraversal to the mesh storage and supports only one particular element-wise\ntraversal order resulting from space-filling curves. Its traversal thus can\nexploit regular grid subregions and shared memory as well as distributed memory\nsystems with almost no modifications to a serial application code. Relying on a\nformalism of the software design at hands of two interacting automata---one\nautomaton for the multiscale grid traversal and one for the\napplication-specific algorithmic steps---we discuss Peano's callback-based\nprogramming paradigm, supported application types and the two data storage\nschemes realised, before we detail high-performance computing aspects of the\nsoftware. Benchmarks highlight the code's potential. We put special emphasis on\na classification of the realised programming and algorithmic concepts against\nalternative approaches candidating for spacetree meshes. This transforms our\nideas from a \"one way to implement things\" discussion into generic patterns\nwhich can be used in other adaptive mesh refinement software."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1026092", 
    "link": "http://arxiv.org/pdf/1506.07749v1", 
    "title": "Efficient mesh management in Firedrake using PETSc-DMPlex", 
    "arxiv-id": "1506.07749v1", 
    "author": "Gerard J. Gorman", 
    "publish": "2015-06-25T13:42:29Z", 
    "summary": "The use of composable abstractions allows the application of new and\nestablished algorithms to a wide range of problems while automatically\ninheriting the benefits of well-known performance optimisations. This work\nhighlights the composition of the PETSc DMPlex domain topology abstraction with\nthe Firedrake automated finite element system to create a PDE solving\nenvironment that combines expressiveness, flexibility and high performance. We\ndescribe how Firedrake utilises DMPlex to provide the indirection maps required\nfor finite element assembly, while supporting various mesh input formats and\nruntime domain decomposition. In particular, we describe how DMPlex and its\naccompanying data structures allow the generic creation of user-defined\ndiscretisations, while utilising data layout optimisations that improve cache\ncoherency and ensure overlapped communication during assembly computation."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1026092", 
    "link": "http://arxiv.org/pdf/1508.00688v1", 
    "title": "Accelerating R with high performance linear algebra libraries", 
    "arxiv-id": "1508.00688v1", 
    "author": "Raluca Mariana Dragoescu", 
    "publish": "2015-08-04T07:25:56Z", 
    "summary": "Linear algebra routines are basic building blocks for the statistical\nsoftware. In this paper we analyzed how can we can improve R performance for\nmatrix computations. We benchmarked few matrix operations using the standard\nlinear algebra libraries included in the R distribution and high performance\nlibraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best\nresults are obtained with the MKL library, the other two libraries having\nsimilar performances, but lower than MKL"
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1026092", 
    "link": "http://arxiv.org/pdf/1508.02470v1", 
    "title": "Support for Non-conformal Meshes in PETSc's DMPlex Interface", 
    "arxiv-id": "1508.02470v1", 
    "author": "Matthew G. Knepley", 
    "publish": "2015-08-11T02:06:01Z", 
    "summary": "PETSc's DMPlex interface for unstructured meshes has been extended to support\nnon-conformal meshes. The topological construct that DMPlex implements---the\nCW-complex---is by definition conformal, so representing non- conformal meshes\nin a way that hides complexity requires careful attention to the interface\nbetween DMPlex and numerical methods such as the finite element method. Our\napproach---which combines a tree structure for subset- superset relationships\nand a \"reference tree\" describing the types of non-conformal\ninterfaces---allows finite element code written for conformal meshes to extend\nautomatically: in particular, all \"hanging-node\" constraint calculations are\nhandled behind the scenes. We give example code demonstrating the use of this\nextension, and use it to convert forests of quadtrees and forests of octrees\nfrom the p4est library to DMPlex meshes."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1026092", 
    "link": "http://arxiv.org/pdf/1508.03954v3", 
    "title": "Complex additive geometric multilevel solvers for Helmholtz equations on   spacetrees", 
    "arxiv-id": "1508.03954v3", 
    "author": "Tobias Weinzierl", 
    "publish": "2015-08-17T08:51:04Z", 
    "summary": "We introduce a family of implementations of low order, additive, geometric\nmultilevel solvers for systems of Helmholtz equations. Both grid spacing and\narithmetics may comprise complex numbers and we thus can apply complex scaling\ntechniques to the indefinite Helmholtz operator. Our implementations are based\nupon the notion of a spacetree and work exclusively with a finite number of\nprecomputed local element matrices. They are globally matrix-free.\n  Combining various relaxation factors with two grid transfer operators allows\nus to switch from pure additive multigrid over a hierarchical basis method into\nBPX with several multiscale smoothing variants within one code base. Pipelining\nallows us to realise a full approximation storage (FAS) scheme within the\nadditive environment where, amortised, each grid vertex carrying degrees of\nfreedom is read/written only once per iteration. The codes thus realise a\nsingle-touch policy. Among the features facilitated by matrix-free FAS is\narbitrary dynamic mesh refinement (AMR) for all solver variants. AMR as enabler\nfor full multigrid (FMG) cycling---the grid unfolds throughout the\ncomputation---allows us to reduce the cost per unknown per order of accuracy.\n  The present paper primary contributes towards software realisation and design\nquestions. Our experiments show that the consolidation of single-touch FAS,\ndynamic AMR and vectorisation-friendly, complex scaled, matrix-free FMG cycles\ndelivers a mature implementation blueprint for solvers for a non-trivial class\nof problems such as Helmholtz equations. Besides this validation, we put\nparticular emphasis on a strict implementation formalism as well as some\nimplementation correctness proofs."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.08.011", 
    "link": "http://arxiv.org/pdf/1508.05374v1", 
    "title": "POLYANA - A tool for the calculation of molecular radial distribution   functions based on Molecular Dynamics trajectories", 
    "arxiv-id": "1508.05374v1", 
    "author": "Vasilios Raptis", 
    "publish": "2015-05-11T19:11:56Z", 
    "summary": "We present an application for the calculation of radial distribution\nfunctions for molecular centres of mass, based on trajectories generated by\nmolecular simulation methods (Molecular Dynamics, Monte Carlo). When designing\nthis application, the emphasis was placed on ease of use as well as ease of\nfurther development. In its current version, the program can read trajectories\ngenerated by the well-known DL_POLY package, but it can be easily extended to\ntreat other formats. It is also very easy to 'hack' the program so it can\ncompute intermolecular radial distribution functions for groups of interaction\nsites rather than whole molecules."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.08.011", 
    "link": "http://arxiv.org/pdf/1508.07231v1", 
    "title": "Clone and graft: Testing scientific applications as they are built", 
    "arxiv-id": "1508.07231v1", 
    "author": "Wolfgang Bangerth", 
    "publish": "2015-08-28T14:42:09Z", 
    "summary": "This article describes our experience developing and maintaining automated\ntests for scientific applications. The main idea evolves around building on\nalready existing tests by cloning and grafting. The idea is demonstrated on a\nminimal model problem written in Python."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1509.02796v1", 
    "title": "Rust-Bio - a fast and safe bioinformatics library", 
    "arxiv-id": "1509.02796v1", 
    "author": "Johannes K\u00f6ster", 
    "publish": "2015-09-09T14:53:02Z", 
    "summary": "We present Rust-Bio, the first general purpose bioinformatics library for the\ninnovative Rust programming language. Rust-Bio leverages the unique combination\nof speed, memory safety and high-level syntax offered by Rust to provide a fast\nand safe set of bioinformatics algorithms and data structures with a focus on\nsequence analysis."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1509.07164v1", 
    "title": "The Stan Math Library: Reverse-Mode Automatic Differentiation in C++", 
    "arxiv-id": "1509.07164v1", 
    "author": "Michael Betancourt", 
    "publish": "2015-09-23T21:34:46Z", 
    "summary": "As computational challenges in optimization and statistical inference grow\never harder, algorithms that utilize derivatives are becoming increasingly more\nimportant. The implementation of the derivatives that make these algorithms so\npowerful, however, is a substantial user burden and the practicality of these\nalgorithms depends critically on tools like automatic differentiation that\nremove the implementation burden entirely. The Stan Math Library is a C++,\nreverse-mode automatic differentiation library designed to be usable, extensive\nand extensible, efficient, scalable, stable, portable, and redistributable in\norder to facilitate the construction and utilization of such algorithms.\n  Usability is achieved through a simple direct interface and a cleanly\nabstracted functional interface. The extensive built-in library includes\nfunctions for matrix operations, linear algebra, differential equation solving,\nand most common probability functions. Extensibility derives from a\nstraightforward object-oriented framework for expressions, allowing users to\neasily create custom functions. Efficiency is achieved through a combination of\ncustom memory management, subexpression caching, traits-based metaprogramming,\nand expression templates. Partial derivatives for compound functions are\nevaluated lazily for improved scalability. Stability is achieved by taking care\nwith arithmetic precision in algebraic expressions and providing stable,\ncompound functions where possible. For portability, the library is\nstandards-compliant C++ (03) and has been tested for all major compilers for\nWindows, Mac OS X, and Linux."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1510.02789v1", 
    "title": "A novel code generation methodology for block diagram modeler and   simulators Scicos and VSS", 
    "arxiv-id": "1510.02789v1", 
    "author": "Ramine Nikoukhah", 
    "publish": "2015-10-08T09:47:51Z", 
    "summary": "Block operations during simulation in Scicos and VSS environments can\nnaturally be described as Nsp functions. But the direct use of Nsp functions\nfor simulation leads to poor performance since the Nsp language is interpreted,\nnot compiled. The methodology presented in this paper is used to develop a tool\nfor generating efficient compilable code, such as C and ADA, for Scicos and VSS\nmodels from these block Nsp functions. Operator overloading and partial\nevaluation are the key elements of this novel approach. This methodology may be\nused in other simulation environments such as Matlab/Simulink."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1510.07244v1", 
    "title": "Approximation of BEM matrices using GPGPUs", 
    "arxiv-id": "1510.07244v1", 
    "author": "Sven Christophersen", 
    "publish": "2015-10-25T13:27:11Z", 
    "summary": "The efficiency of boundary element methods depends crucially on the time\nrequired for setting up the stiffness matrix. The far-field part of the matrix\ncan be approximated by compression schemes like the fast multipole method or\n$\\mathcal{H}$-matrix techniques. The near-field part is typically approximated\nby special quadrature rules like the Sauter-Schwab technique that can handle\nthe singular integrals appearing in the diagonal and near-diagonal matrix\nelements.\n  Since computing one element of the matrix requires only a small amount of\ndata but a fairly large number of operations, we propose to use GPUs to handle\nvectorizable portions of the computation: near-field computations are ideally\nsuited for vectorization and can therefore be handled very well by GPUs. Modern\nfar-field compression schemes can be split into a small adaptive portion that\nexhibits divergent control flows and is handled by the CPU and a vectorizable\nportion that can again be sent to GPUs.\n  We propose a hybrid algorithm that splits the computation into tasks for CPUs\nand GPUs. Our method presented in this article is able to speedup the setup\ntime of boundary integral operators by a significant factor of 19-30 for both\nthe Laplace and the Helmholtz equation in 3D when using two consumer GPGPUs\ncompared to a quad-core CPU."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1511.03167v1", 
    "title": "BOAT: a cross-platform software for data analysis and numerical   computing with arbitrary-precision", 
    "arxiv-id": "1511.03167v1", 
    "author": "Davide Pagano", 
    "publish": "2015-11-10T16:22:27Z", 
    "summary": "BOAT is a free cross-platform software for statistical data analysis and\nnumerical computing. Thanks to its multiple-precision floating point engine, it\nallows arbitrary-precision calculations, whose digits of precision are only\nlimited by the amount of memory of the host machine. At the core of the\nsoftware is a simple and efficient expression language, whose use is\nfacilitated by the assisted typing, the auto-complete engine and the built-in\nhelp for the syntax. In this paper a quick overview of the software is given.\nDetailed information, together with its applications to some case studies, is\navailable at the BOAT web page."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1511.05986v2", 
    "title": "Computing with Harmonic Functions", 
    "arxiv-id": "1511.05986v2", 
    "author": "Sheldon Axler", 
    "publish": "2015-11-15T19:33:39Z", 
    "summary": "This document is the manual for a free Mathematica package for computing with\nharmonic functions. This package allows the user to make calculations that\nwould take a prohibitive amount of time if done without a computer. For\nexample, the Poisson integral of any polynomial can be computed exactly. This\nsoftware can find exact solutions to Dirichlet, Neumann, and biDirichlet\nproblems in R^n with polynomial data on balls, ellipsoids, and annular regions.\nIt can also find bases for spaces of spherical harmonics, compute projections\nonto the harmonic Bergman space, and perform other manipulations with harmonic\nfunctions."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1511.07727v2", 
    "title": "DiffSharp: Automatic Differentiation Library", 
    "arxiv-id": "1511.07727v2", 
    "author": "Jeffrey Mark Siskind", 
    "publish": "2015-11-24T14:28:13Z", 
    "summary": "In this paper we introduce DiffSharp, an automatic differentiation (AD)\nlibrary designed with machine learning in mind. AD is a family of techniques\nthat evaluate derivatives at machine precision with only a small constant\nfactor of overhead, by systematically applying the chain rule of calculus at\nthe elementary operator level. DiffSharp aims to make an extensive array of AD\ntechniques available, in convenient form, to the machine learning community.\nThese including arbitrary nesting of forward/reverse AD operations, AD with\nlinear algebra primitives, and a functional API that emphasizes the use of\nhigher-order functions and composition. The library exposes this functionality\nthrough an API that provides gradients, Hessians, Jacobians, directional\nderivatives, and matrix-free Hessian- and Jacobian-vector products. Bearing the\nperformance requirements of the latest machine learning techniques in mind, the\nunderlying computations are run through a high-performance BLAS/LAPACK backend,\nusing OpenBLAS by default. GPU support is currently being implemented."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1512.00066v1", 
    "title": "Sparse Tensor Algebra as a Parallel Programming Model", 
    "arxiv-id": "1512.00066v1", 
    "author": "Torsten Hoefler", 
    "publish": "2015-11-30T22:08:23Z", 
    "summary": "Dense and sparse tensors allow the representation of most bulk data\nstructures in computational science applications. We show that sparse tensor\nalgebra can also be used to express many of the transformations on these\ndatasets, especially those which are parallelizable. Tensor computations are a\nnatural generalization of matrix and graph computations. We extend the usual\nbasic operations of tensor summation and contraction to arbitrary functions,\nand further operations such as reductions and mapping. The expression of these\ntransformations in a high-level sparse linear algebra domain specific language\nallows our framework to understand their properties at runtime to select the\npreferred communication-avoiding algorithm. To demonstrate the efficacy of our\napproach, we show how key graph algorithms as well as common numerical kernels\ncan be succinctly expressed using our interface and provide performance results\nof a general library implementation."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1512.06136v1", 
    "title": "The interface for functions in the dune-functions module", 
    "arxiv-id": "1512.06136v1", 
    "author": "Oliver Sander", 
    "publish": "2015-12-18T21:09:10Z", 
    "summary": "The dune-functions dune module introduces a new programmer interface for\ndiscrete and non-discrete functions. Unlike the previous interfaces considered\nin the existing dune modules, it is based on overloading operator(), and\nreturning values by-value. This makes user code much more readable, and allows\nthe incorporation of newer C++ features such as lambda expressions. Run-time\npolymorphism is implemented not by inheritance, but by type erasure,\ngeneralizing the ideas of the std::function class from the C++11 standard\nlibrary. We describe the new interface, show its possibilities, and measure the\nperformance impact of type erasure and return-by-value."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1512.08790v1", 
    "title": "Dynamic Computation of Runge Kutta Fourth Order Algorithm for First and   Second Order Ordinary Differential Equation Using Java", 
    "arxiv-id": "1512.08790v1", 
    "author": "A. O. Adekoya", 
    "publish": "2015-12-26T18:18:54Z", 
    "summary": "Differential equations arise in mathematics, physics,medicine, pharmacology,\ncommunications, image processing and animation, etc. An Ordinary Differential\nEquation (ODE) is a differential equation if it involves derivatives with\nrespect to only one independent variable which can be studied from different\nperspectives, such as: analytical methods, graphical methods and numerical\nmethods. This research paper therefore revises the standard Runge - Kutta\nfourth order algorithm by using compiler techniques to dynamically evaluate the\ninputs and implement the algorithm for both first and second order derivatives\nof the ODE. We have been able to develop and implement the software that can be\nused to evaluate inputs and compute solutions (approximately and analytically)\nfor the ODE function at a more efficient rate than the traditional method."
},{
    "category": "cs.MS", 
    "doi": "10.1093/bioinformatics/btv573", 
    "link": "http://arxiv.org/pdf/1601.05871v1", 
    "title": "Task Parallel Incomplete Cholesky Factorization using 2D   Partitioned-Block Layout", 
    "arxiv-id": "1601.05871v1", 
    "author": "Stephen L. Olivier", 
    "publish": "2016-01-22T03:19:45Z", 
    "summary": "We introduce a task-parallel algorithm for sparse incomplete Cholesky\nfactorization that utilizes a 2D sparse partitioned-block layout of a matrix.\nOur factorization algorithm follows the idea of algorithms-by-blocks by using\nthe block layout. The algorithm-by-blocks approach induces a task graph for the\nfactorization. These tasks are inter-related to each other through their data\ndependences in the factorization algorithm. To process the tasks on various\nmanycore architectures in a portable manner, we also present a portable tasking\nAPI that incorporates different tasking backends and device-specific features\nusing an open-source framework for manycore platforms i.e., Kokkos. A\nperformance evaluation is presented on both Intel Sandybridge and Xeon Phi\nplatforms for matrices from the University of Florida sparse matrix collection\nto illustrate merits of the proposed task-based factorization. Experimental\nresults demonstrate that our task-parallel implementation delivers about 26.6x\nspeedup (geometric mean) over single-threaded incomplete Cholesky-by-blocks and\n19.2x speedup over serial Cholesky performance which does not carry tasking\noverhead using 56 threads on the Intel Xeon Phi processor for sparse matrices\narising from various application problems."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2967938.2967966", 
    "link": "http://arxiv.org/pdf/1601.07789v3", 
    "title": "Vectorization of Multibyte Floating Point Data Formats", 
    "arxiv-id": "1601.07789v3", 
    "author": "David Gregg", 
    "publish": "2016-01-26T13:24:40Z", 
    "summary": "We propose a scheme for reduced-precision representation of floating point\ndata on a continuum between IEEE-754 floating point types. Our scheme enables\nthe use of lower precision formats for a reduction in storage space\nrequirements and data transfer volume. We describe how our scheme can be\naccelerated using existing hardware vector units on a general-purpose processor\n(GPP). Exploiting native vector hardware allows us to support reduced precision\nfloating point with low overhead. We demonstrate that supporting reduced\nprecision in the compiler as opposed to using a library approach can yield a\nlow overhead solution for GPPs."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.10.026", 
    "link": "http://arxiv.org/pdf/1602.03643v1", 
    "title": "Oasis: a high-level/high-performance open source Navier-Stokes solver", 
    "arxiv-id": "1602.03643v1", 
    "author": "Kristian Valen-Sendstad", 
    "publish": "2016-02-11T08:56:43Z", 
    "summary": "Oasis is a high-level/high-performance finite element Navier-Stokes solver\nwritten from scratch in Python using building blocks from the FEniCS project\n(fenicsproject.org). The solver is unstructured and targets large-scale\napplications in complex geometries on massively parallel clusters. Oasis\nutilizes MPI and interfaces, through FEniCS, to the linear algebra backend\nPETSc. Oasis advocates a high-level, programmable user interface through the\ncreation of highly flexible Python modules for new problems. Through the\nhigh-level Python interface the user is placed in complete control of every\naspect of the solver. A version of the solver, that is using piecewise linear\nelements for both velocity and pressure, is shown reproduce very well the\nclassical, spectral, turbulent channel simulations of Moser, Kim and Mansour at\n$Re_{\\tau}=180$ [Phys. Fluids, vol 11(4), p. 964]. The computational speed is\nstrongly dominated by the iterative solvers provided by the linear algebra\nbackend, which is arguably the best performance any similar implicit solver\nusing PETSc may hope for. Higher order accuracy is also demonstrated and new\nsolvers may be easily added within the same framework."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.10.026", 
    "link": "http://arxiv.org/pdf/1603.04483v1", 
    "title": "Fast calculation of inverse square root with the use of magic constant   $-$ analytical approach", 
    "arxiv-id": "1603.04483v1", 
    "author": "Jan L. Cie\u015bli\u0144ski", 
    "publish": "2016-03-14T21:28:46Z", 
    "summary": "We present a mathematical analysis of transformations used in fast\ncalculation of inverse square root for single-precision floating-point numbers.\nOptimal values of the so called magic constants are derived in a systematic\nway, minimizing either absolute or relative errors at subsequent stages of the\ndiscussed algorithm."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.10.026", 
    "link": "http://arxiv.org/pdf/1603.06424v1", 
    "title": "Interoperability in the OpenDreamKit Project: The Math-in-the-Middle   Approach", 
    "arxiv-id": "1603.06424v1", 
    "author": "Nicolas M. Thi\u00e9ry", 
    "publish": "2016-03-21T13:18:44Z", 
    "summary": "OpenDreamKit --- \"Open Digital Research Environment Toolkit for the\nAdvancement of Mathematics\" --- is an H2020 EU Research Infrastructure project\nthat aims at supporting, over the period 2015--2019, the ecosystem of\nopen-source mathematical software systems. From that, OpenDreamKit will deliver\na flexible toolkit enabling research groups to set up Virtual Research\nEnvironments, customised to meet the varied needs of research projects in pure\nmathematics and applications.\n  An important step in the OpenDreamKit endeavor is to foster the\ninteroperability between a variety of systems, ranging from computer algebra\nsystems over mathematical databases to front-ends. This is the mission of the\nintegration work package (WP6). We report on experiments and future plans with\nthe \\emph{Math-in-the-Middle} approach. This information architecture consists\nin a central mathematical ontology that documents the domain and fixes a joint\nvocabulary, combined with specifications of the functionalities of the various\nsystems. Interaction between systems can then be enriched by pivoting off this\ninformation architecture."
},{
    "category": "cs.MS", 
    "doi": "10.5334/jors.110", 
    "link": "http://arxiv.org/pdf/1603.06914v4", 
    "title": "SimOutUtils - Utilities for analyzing time series simulation output", 
    "arxiv-id": "1603.06914v4", 
    "author": "Agostinho C. Rosa", 
    "publish": "2016-03-22T19:11:07Z", 
    "summary": "SimOutUtils is a suite of MATLAB/Octave functions for studying and analyzing\ntime series-like output from stochastic simulation models. More specifically,\nSimOutUtils allows modelers to study and visualize simulation output dynamics,\nperform distributional analysis of output statistical summaries, as well as\ncompare these summaries in order to assert the statistical equivalence of two\nor more model implementations. Additionally, the provided functions are able to\nproduce publication quality figures and tables showcasing results from the\nspecified simulation output studies."
},{
    "category": "cs.MS", 
    "doi": "10.5334/jors.110", 
    "link": "http://arxiv.org/pdf/1603.07916v2", 
    "title": "A Subdivision Solver for Systems of Large Dense Polynomials", 
    "arxiv-id": "1603.07916v2", 
    "author": "R\u00e9mi Imbach", 
    "publish": "2016-03-25T14:07:49Z", 
    "summary": "We describe here the package {\\tt subdivision\\\\_solver} for the mathematical\nsoftware {\\tt SageMath}. It provides a solver on real numbers for square\nsystems of large dense polynomials. By large polynomials we mean multivariate\npolynomials with large degrees, which coefficients have large bit-size. While\nstaying robust, symbolic approaches to solve systems of polynomials see their\nperformances dramatically affected by high degree and bit-size of input\npolynomials.Available numeric approaches suffer from the cost of the evaluation\nof large polynomials and their derivatives.Our solver is based on interval\nanalysis and bisections of an initial compact domain of $\\R^n$ where solutions\nare sought. Evaluations on intervals with Horner scheme is performed by the\npackage {\\tt fast\\\\_polynomial} for {\\tt SageMath}.The non-existence of a\nsolution within a box is certified by an evaluation scheme that uses a Taylor\nexpansion at order 2, and existence and uniqueness of a solution within a box\nis certified with krawczyk operator.The precision of the working arithmetic is\nadapted on the fly during the subdivision process and we present a new\nheuristic criterion to decide if the arithmetic precision has to be increased."
},{
    "category": "cs.MS", 
    "doi": "10.5334/jors.110", 
    "link": "http://arxiv.org/pdf/1604.02528v1", 
    "title": "A Left-Looking Selected Inversion Algorithm and Task Parallelism on   Shared Memory Systems", 
    "arxiv-id": "1604.02528v1", 
    "author": "Chao Yang", 
    "publish": "2016-04-09T06:15:15Z", 
    "summary": "Given a sparse matrix $A$, the selected inversion algorithm is an efficient\nmethod for computing certain selected elements of $A^{-1}$. These selected\nelements correspond to all or some nonzero elements of the LU factors of $A$.\nIn many ways, the type of matrix updates performed in the selected inversion\nalgorithm is similar to that performed in the LU factorization, although the\nsequence of operation is different. In the context of LU factorization, it is\nknown that the left-looking and right-looking algorithms exhibit different\nmemory access and data communication patterns, and hence different behavior on\nshared memory and distributed memory parallel machines. Corresponding to\nright-looking and left-looking LU factorization, selected inversion algorithm\ncan be organized as a left-looking and a right-looking algorithm. The parallel\nright-looking version of the algorithm has been developed in [1]. The sequence\nof operations performed in this version of the selected inversion algorithm is\nsimilar to those performed in a left-looking LU factorization algorithm. In\nthis paper, we describe the left-looking variant of the selected inversion\nalgorithm, and based on task parallel method, present an efficient\nimplementation of the algorithm for shared memory machines. We demonstrate that\nwith the task scheduling features provided by OpenMP 4.0, the left-looking\nselected inversion algorithm can scale well both on the Intel Haswell multicore\narchitecture and on the Intel Knights Corner (KNC) manycore architecture.\nCompared to the right-looking selected inversion algorithm, the left-looking\nformulation facilitates pipelining of work along different branches of the\nelimination tree, and can be a promising candidate for future development of\nmassively parallel selected inversion algorithms on heterogeneous architecture."
},{
    "category": "cs.MS", 
    "doi": "10.5334/jors.110", 
    "link": "http://arxiv.org/pdf/1604.05872v1", 
    "title": "An algorithm for the optimization of finite element integration loops", 
    "arxiv-id": "1604.05872v1", 
    "author": "Paul H. J. Kelly", 
    "publish": "2016-04-20T09:39:29Z", 
    "summary": "We present an algorithm for the optimization of a class of finite element\nintegration loop nests. This algorithm, which exploits fundamental mathematical\nproperties of finite element operators, is proven to achieve a locally optimal\noperation count. In specified circumstances the optimum achieved is global.\nExtensive numerical experiments demonstrate significant performance\nimprovements over the state of the art in finite element code generation in\nalmost all cases. This validates the effectiveness of the algorithm presented\nhere, and illustrates its limitations."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1604.05937v3", 
    "title": "A structure-exploiting numbering algorithm for finite elements on   extruded meshes, and its performance evaluation in Firedrake", 
    "arxiv-id": "1604.05937v3", 
    "author": "Paul H. J. Kelly", 
    "publish": "2016-04-20T13:05:04Z", 
    "summary": "We present a generic algorithm for numbering and then efficiently iterating\nover the data values attached to an extruded mesh. An extruded mesh is formed\nby replicating an existing mesh, assumed to be unstructured, to form layers of\nprismatic cells. Applications of extruded meshes include, but are not limited\nto, the representation of 3D high aspect ratio domains employed by geophysical\nfinite element simulations. These meshes are structured in the extruded\ndirection. The algorithm presented here exploits this structure to avoid the\nperformance penalty traditionally associated with unstructured meshes. We\nevaluate the implementation of this algorithm in the Firedrake finite element\nsystem on a range of low compute intensity operations which constitute worst\ncases for data layout performance exploration. The experiments show that having\nstructure along the extruded direction enables the cost of the indirect data\naccesses to be amortized after 10-20 layers as long as the underlying mesh is\nwell-ordered. We characterise the resulting spatial and temporal reuse in a\nrepresentative set of both continuous-Galerkin and discontinuous-Galerkin\ndiscretisations. On meshes with realistic numbers of layers the performance\nachieved is between 70% and 90% of a theoretical hardware-specific limit."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1604.06112v1", 
    "title": "Convex Hull Calculations: a Matlab Implementation and Correctness Proofs   for the lrs-Algorithm", 
    "arxiv-id": "1604.06112v1", 
    "author": "Bernardete Ribeiro", 
    "publish": "2016-04-20T20:07:48Z", 
    "summary": "This paper provides full \\Matlab-code and informal correctness proofs for the\nlexicographic reverse search algorithm for convex hull calculations. The\nimplementation was tested on a 1993 486-PC for various small and some larger,\npartially highly degenerate combinatorial polytopes, one of which (a certain\n13-dimensional 24 vertex polyhedron) occurs naturally in the study of a well\nknown problem posed by Professor Graciano de Oliveira: see end of section 1."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1604.07163v1", 
    "title": "Extreme-scale Multigrid Components within PETSc", 
    "arxiv-id": "1604.07163v1", 
    "author": "Barry F. Smith", 
    "publish": "2016-04-25T08:35:12Z", 
    "summary": "Elliptic partial differential equations (PDEs) frequently arise in continuum\ndescriptions of physical processes relevant to science and engineering.\nMultilevel preconditioners represent a family of scalable techniques for\nsolving discrete PDEs of this type and thus are the method of choice for\nhigh-resolution simulations. The scalability and time-to-solution of massively\nparallel multilevel preconditioners can be adversely effected by using a\ncoarse-level solver with sub-optimal algorithmic complexity. To maintain\nscalability, agglomeration techniques applied to the coarse level have been\nshown to be necessary.\n  In this work, we present a new software component introduced within the\nPortable Extensible Toolkit for Scientific computation (PETSc) which permits\nagglomeration. We provide an overview of the design and implementation of this\nfunctionality, together with several use cases highlighting the benefits of\nagglomeration. Lastly, we demonstrate via numerical experiments employing\ngeometric multigrid with structured meshes, the flexibility and performance\ngains possible using our MPI-rank agglomeration implementation."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1604.07242v1", 
    "title": "Implementation of $hp$-adaptive discontinuous finite element methods in   Dune-Fem", 
    "arxiv-id": "1604.07242v1", 
    "author": "Christoph Gersbacher", 
    "publish": "2016-04-25T13:22:16Z", 
    "summary": "In this paper we describe generic algorithms and data structures for the\nimplementation of $hp$-adaptive discontinuous finite element methods in the\nDune-Fem library. Special attention is given to the often tedious and\nerror-prone task of transferring user data during adaptation. Simultaneously,\nwe generalize the approach to the restriction and prolongation of data\ncurrently implemented in Dune-Fem to the case of $p$- and $hp$-adaptation. The\ndune-fem-hpdg module described in this paper provides an extensible reference\nimplementation of $hp$-adaptive discontinuous discrete function spaces. We give\ndetails on its implementation and the extended adaptive interface. As proof of\nconcept we present the practical realization of an $hp$-adaptive interior\npenalty method for elliptic problems."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1605.01078v1", 
    "title": "Implementing Strassen's Algorithm with BLIS", 
    "arxiv-id": "1605.01078v1", 
    "author": "Robert A. van de Geijn", 
    "publish": "2016-05-03T20:28:00Z", 
    "summary": "We dispel with \"street wisdom\" regarding the practical implementation of\nStrassen's algorithm for matrix-matrix multiplication (DGEMM). Conventional\nwisdom: it is only practical for very large matrices. Our implementation is\npractical for small matrices. Conventional wisdom: the matrices being\nmultiplied should be relatively square. Our implementation is practical for\nrank-k updates, where k is relatively small (a shape of importance for\nlibraries like LAPACK). Conventional wisdom: it inherently requires substantial\nworkspace. Our implementation requires no workspace beyond buffers already\nincorporated into conventional high-performance DGEMM implementations.\nConventional wisdom: a Strassen DGEMM interface must pass in workspace. Our\nimplementation requires no such workspace and can be plug-compatible with the\nstandard DGEMM interface. Conventional wisdom: it is hard to demonstrate\nspeedup on multi-core architectures. Our implementation demonstrates speedup\nover conventional DGEMM even on an Intel(R) Xeon Phi(TM) coprocessor utilizing\n240 threads. We show how a distributed memory matrix-matrix multiplication also\nbenefits from these advances."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1605.05057v1", 
    "title": "The polymake XML file format", 
    "arxiv-id": "1605.05057v1", 
    "author": "Michael Joswig", 
    "publish": "2016-05-17T08:40:14Z", 
    "summary": "We describe an XML file format for storing data from computations in algebra\nand geometry. We also present a formal specification based on a RELAX-NG\nschema."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1605.06381v1", 
    "title": "OPESCI-FD: Automatic Code Generation Package for Finite Difference   Models", 
    "arxiv-id": "1605.06381v1", 
    "author": "Tianjiao Sun", 
    "publish": "2016-05-20T14:44:55Z", 
    "summary": "In this project, we introduce OPESCI-FD, a Python package built on symbolic\nmathematics to automatically generate Finite Difference models from a\nhigh-level description of the model equations. We investigate applying this\nframework to generate the propagator program used in seismic imaging. We\nimplement the 3D velocity-stress FD scheme as an example and demonstrate the\nadvantages of usability, flexibility and accuracy of the framework. The design\nof OPESCI-FD aims to allow rapid development, analysis and optimisation of\nFinite Difference programs. OPESCI-FD is the foundation for continuing\ndevelopment by the OPESCI project team, building on the research presented in\nthis report. This report concludes by reviewing the further developments that\nare already under way, as well as the scope for extension to cater for other\nequations and numerical schemes."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1606.06311v1", 
    "title": "Benchmarking Python Tools for Automatic Differentiation", 
    "arxiv-id": "1606.06311v1", 
    "author": "Aung Thu", 
    "publish": "2016-06-20T20:14:12Z", 
    "summary": "In this paper we compare several Python tools for automatic differentiation.\nIn order to assess the difference in performance and precision, the problem of\nfinding the optimal geometrical structure of the cluster with identical atoms\nis used as follows. First, we compare performance of calculating gradients for\nthe objective function. We showed that the PyADOL-C and PyCppAD tools have much\nbetter performance for big clusters than the other ones. Second, we assess\nprecision of these two tools by calculating the difference between the obtained\nat the optimal configuration gradient norms. We conclude that PyCppAD has the\nbest performance among others, while having almost the same precision as the\nsecond- best performing tool - PyADOL-C."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1606.07399v2", 
    "title": "jInv -- a flexible Julia package for PDE parameter estimation", 
    "arxiv-id": "1606.07399v2", 
    "author": "Eldad Haber", 
    "publish": "2016-06-23T18:37:41Z", 
    "summary": "Estimating parameters of Partial Differential Equations (PDEs) from noisy and\nindirect measurements often requires solving ill-posed inverse problems. These\nso called parameter estimation or inverse medium problems arise in a variety of\napplications such as geophysical, medical imaging, and nondestructive testing.\nTheir solution is computationally intense since the underlying PDEs need to be\nsolved numerous times until the reconstruction of the parameters is\nsufficiently accurate. Typically, the computational demand grows significantly\nwhen more measurements are available, which poses severe challenges to\ninversion algorithms as measurement devices become more powerful.\n  In this paper we present jInv, a flexible framework and open source software\nthat provides parallel algorithms for solving parameter estimation problems\nwith many measurements. Being written in the expressive programming language\nJulia, jInv is portable, easy to understand and extend, cross-platform tested,\nand well-documented. It provides novel parallelization schemes that exploit the\ninherent structure of many parameter estimation problems and can be used to\nsolve multiphysics inversion problems as is demonstrated using numerical\nexperiments motivated by geophysical imaging."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.00145v2", 
    "title": "Design of a high-performance GEMM-like Tensor-Tensor Multiplication", 
    "arxiv-id": "1607.00145v2", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-07-01T08:13:50Z", 
    "summary": "We present \"GEMM-like Tensor-Tensor multiplication\" (GETT), a novel approach\nto tensor contractions that mirrors the design of a high-performance general\nmatrix-matrix multiplication (GEMM). The critical insight behind GETT is the\nidentification of three index sets, involved in the tensor contraction, which\nenable us to systematically reduce an arbitrary tensor contraction to loops\naround a highly tuned \"macro-kernel\". This macro-kernel operates on suitably\nprepared (\"packed\") sub-tensors that reside in a specified level of the cache\nhierarchy. In contrast to previous approaches to tensor contractions, GETT\nexhibits desirable features such as unit-stride memory accesses,\ncache-awareness, as well as full vectorization, without requiring auxiliary\nmemory. To compare our technique with other modern tensor contractions, we\nintegrate GETT alongside the so called Transpose-Transpose-GEMM-Transpose and\nLoops-over-GEMM approaches into an open source \"Tensor Contraction Code\nGenerator\" (TCCG). The performance results for a wide range of tensor\ncontractions suggest that GETT has the potential of becoming the method of\nchoice: While GETT exhibits excellent performance across the board, its\neffectiveness for bandwidth-bound tensor contractions is especially impressive,\noutperforming existing approaches by up to $12.4\\times$. More precisely, GETT\nachieves speedups of up to $1.41\\times$ over an equivalent-sized GEMM for\nbandwidth-bound tensor contractions while attaining up to $91.3\\%$ of peak\nfloating-point performance for compute-bound tensor contractions."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.00844v1", 
    "title": "Using the pyMIC Offload Module in PyFR", 
    "arxiv-id": "1607.00844v1", 
    "author": "Peter Vincent", 
    "publish": "2016-07-01T18:59:11Z", 
    "summary": "PyFR is an open-source high-order accurate computational fluid dynamics\nsolver for unstructured grids. It is designed to efficiently solve the\ncompressible Navier-Stokes equations on a range of hardware platforms,\nincluding GPUs and CPUs. In this paper we will describe how the Python Offload\nInfrastructure for the Intel Many Integrated Core Architecture (pyMIC) was used\nto enable PyFR to run with near native performance on the Intel Xeon Phi\ncoprocessor. We will introduce the architecture of both pyMIC and PyFR and\npresent a variety of examples showcasing the capabilities of pyMIC. Further, we\nwill also compare the contrast pyMIC to other approaches including native\nexecution and OpenCL. The process of adding support for pyMIC into PyFR will be\ndescribed in detail. Benchmark results show that for a standard cylinder flow\nproblem PyFR with pyMIC is able achieve 240 GFLOP/s of sustained double\nprecision floating point performance; for a 1.85 times improvement over PyFR\nwith C/OpenMP on a 12 core Intel Xeon E5-2697 v2 CPU."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.00850v1", 
    "title": "Massively parallel implementation in Python of a pseudo-spectral DNS   code for turbulent flows", 
    "arxiv-id": "1607.00850v1", 
    "author": "Mikael Mortensen", 
    "publish": "2016-07-01T19:05:11Z", 
    "summary": "Direct Numerical Simulations (DNS) of the Navier Stokes equations is a\nvaluable research tool in fluid dynamics, but there are very few publicly\navailable codes and, due to heavy number crunching, codes are usually written\nin low-level languages. In this work a \\textasciitilde{}100 line standard\nscientific Python DNS code is described that nearly matches the performance of\npure C for thousands of processors and billions of unknowns. With optimization\nof a few routines in Cython, it is found to match the performance of a more or\nless identical solver implemented from scratch in C++. Keys to the efficiency\nof the solver are the mesh decomposition and three dimensional FFT routines,\nimplemented directly in Python using MPI, wrapped through MPI for Python, and a\nserial FFT module (both numpy.fft or pyFFTW may be used). Two popular\ndecomposition strategies, slab and pencil, have been implemented and tested."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.02835v1", 
    "title": "Form Follows Function -- Do algorithms and applications challenge or   drag behind the hardware evolution?", 
    "arxiv-id": "1607.02835v1", 
    "author": "Tobias Weinzierl", 
    "publish": "2016-07-11T06:51:50Z", 
    "summary": "We summarise some of the key statements made at the workshop Form Follows\nFunction at ISC High Performance 2016. The summary highlights what type of\nco-design the presented projects experience; often in the absence of an\nexplicit co-design agenda. Their software development picks up hardware trends\nbut it also influences the hardware development. Observations illustrate that\nthis cycle not always is optimal for both sides as it is not proactively\nsteered. Key statements characterise ideas how it might be possible to\nintegrate both hardware and software creation closer to the best of both\nworlds---again even without classic co-design in mind where new pieces of\nhardware are created. The workshop finally identified three development idioms\nthat might help to improve software and system design with respect to emerging\nhardware."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.04091v2", 
    "title": "Generalized Sampling in Julia", 
    "arxiv-id": "1607.04091v2", 
    "author": "Morten Grud Rasmussen", 
    "publish": "2016-07-14T11:26:28Z", 
    "summary": "Generalized sampling is a numerically stable framework for obtaining\nreconstructions of signals in different bases and frames from their samples. In\nthis paper, we will introduce a carefully documented toolbox for performing\ngeneralized sampling in Julia. Julia is a new language for technical computing\nwith focus on performance, which is ideally suited to handle the large size\nproblems often encountered in generalized sampling. The toolbox provides\nspecialized solutions for the setup of Fourier bases and wavelets. The\nperformance of the toolbox is compared to existing implementations of\ngeneralized sampling in MATLAB."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.04245v1", 
    "title": "Finite Element Integration with Quadrature on the GPU", 
    "arxiv-id": "1607.04245v1", 
    "author": "Andy R. Terrel", 
    "publish": "2016-07-14T18:53:48Z", 
    "summary": "We present a novel, quadrature-based finite element integration method for\nlow-order elements on GPUs, using a pattern we call \\textit{thread\ntransposition} to avoid reductions while vectorizing aggressively. On the\nNVIDIA GTX580, which has a nominal single precision peak flop rate of 1.5 TF/s\nand a memory bandwidth of 192 GB/s, we achieve close to 300 GF/s for element\nintegration on first-order discretization of the Laplacian operator with\nvariable coefficients in two dimensions, and over 400 GF/s in three dimensions.\nFrom our performance model we find that this corresponds to 90\\% of our\nmeasured achievable bandwidth peak of 310 GF/s. Further experimental results\nalso match the predicted performance when used with double precision (120 GF/s\nin two dimensions, 150 GF/s in three dimensions). Results obtained for the\nlinear elasticity equations (220 GF/s and 70 GF/s in two dimensions, 180 GF/s\nand 60 GF/s in three dimensions) also demonstrate the applicability of our\nmethod to vector-valued partial differential equations."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1607.07892v1", 
    "title": "Forward-Mode Automatic Differentiation in Julia", 
    "arxiv-id": "1607.07892v1", 
    "author": "Theodore Papamarkou", 
    "publish": "2016-07-26T20:32:29Z", 
    "summary": "We present ForwardDiff, a Julia package for forward-mode automatic\ndifferentiation (AD) featuring performance competitive with low-level languages\nlike C++. Unlike recently developed AD tools in other popular high-level\nlanguages such as Python and MATLAB, ForwardDiff takes advantage of\njust-in-time (JIT) compilation to transparently recompile AD-unaware user code,\nenabling efficient support for higher-order differentiation and differentiation\nusing custom number types (including complex numbers). For gradient and\nJacobian calculations, ForwardDiff provides a variant of vector-forward mode\nthat avoids expensive heap allocation and makes better use of memory bandwidth\nthan traditional vector mode. In our numerical experiments, we demonstrate that\nfor nontrivially large dimensions, ForwardDiff's gradient computations can be\nfaster than a reverse-mode implementation from the Python-based autograd\npackage. We also illustrate how ForwardDiff is used effectively within JuMP, a\nmodeling language for optimization. According to our usage statistics, 41\nunique repositories on GitHub depend on ForwardDiff, with users from diverse\nfields such as astronomy, optimization, finite element analysis, and\nstatistics.\n  This document is an extended abstract that has been accepted for presentation\nat the AD2016 7th International Conference on Algorithmic Differentiation."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1608.00044v2", 
    "title": "An Asynchronous Task-based Fan-Both Sparse Cholesky Solver", 
    "arxiv-id": "1608.00044v2", 
    "author": "Katherine Yelick", 
    "publish": "2016-07-29T22:37:07Z", 
    "summary": "Systems of linear equations arise at the heart of many scientific and\nengineering applications. Many of these linear systems are sparse; i.e., most\nof the elements in the coefficient matrix are zero. Direct methods based on\nmatrix factorizations are sometimes needed to ensure accurate solutions. For\nexample, accurate solution of sparse linear systems is needed in shift-invert\nLanczos to compute interior eigenvalues. The performance and resource usage of\nsparse matrix factorizations are critical to time-to-solution and maximum\nproblem size solvable on a given platform. In many applications, the\ncoefficient matrices are symmetric, and exploiting symmetry will reduce both\nthe amount of work and storage cost required for factorization. When the\nfactorization is performed on large-scale distributed memory platforms,\ncommunication cost is critical to the performance of the algorithm. At the same\ntime, network topologies have become increasingly complex, so that modern\nplatforms exhibit a high level of performance variability. This makes\nscheduling of computations an intricate and performance-critical task. In this\npaper, we investigate the use of an asynchronous task paradigm, one-sided\ncommunication and dynamic scheduling in implementing sparse Cholesky\nfactorization (symPACK) on large-scale distributed memory platforms. Our solver\nsymPACK relies on efficient and flexible communication primitives provided by\nthe UPC++ library. Performance evaluation shows good scalability and that\nsymPACK outperforms state-of-the-art parallel distributed memory factorization\npackages, validating our approach on practical cases."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1609.00076v1", 
    "title": "BLISlab: A Sandbox for Optimizing GEMM", 
    "arxiv-id": "1609.00076v1", 
    "author": "Robert A. van de Geijn", 
    "publish": "2016-09-01T01:11:48Z", 
    "summary": "Matrix-matrix multiplication is a fundamental operation of great importance\nto scientific computing and, increasingly, machine learning. It is a simple\nenough concept to be introduced in a typical high school algebra course yet in\npractice important enough that its implementation on computers continues to be\nan active research topic. This note describes a set of exercises that use this\noperation to illustrate how high performance can be attained on modern CPUs\nwith hierarchical memories (multiple caches). It does so by building on the\ninsights that underly the BLAS-like Library Instantiation Software (BLIS)\nframework by exposing a simplified \"sandbox\" that mimics the implementation in\nBLIS. As such, it also becomes a vehicle for the \"crowd sourcing\" of the\noptimization of BLIS. We call this set of exercises BLISlab."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1609.00999v1", 
    "title": "Automatic Generation of Vectorized Montgomery Algorithm", 
    "arxiv-id": "1609.00999v1", 
    "author": "Lingchuan Meng", 
    "publish": "2016-09-04T23:44:20Z", 
    "summary": "Modular arithmetic is widely used in crytography and symbolic computation.\nThis paper presents a vectorized Montgomery algorithm for modular\nmultiplication, the key to fast modular arithmetic, that fully utilizes the\nSIMD instructions. We further show how the vectorized algorithm can be\nautomatically generated by the {\\SPIRAL} system, as part of the effort for\nautomatic generation of a modular polynomial multiplication library."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1609.04504v1", 
    "title": "cesium: Open-Source Platform for Time-Series Inference", 
    "arxiv-id": "1609.04504v1", 
    "author": "Fernando P\u00e9rez", 
    "publish": "2016-09-15T04:09:48Z", 
    "summary": "Inference on time series data is a common requirement in many scientific\ndisciplines and internet of things (IoT) applications, yet there are few\nresources available to domain scientists to easily, robustly, and repeatably\nbuild such complex inference workflows: traditional statistical models of time\nseries are often too rigid to explain complex time domain behavior, while\npopular machine learning packages require already-featurized dataset inputs.\nMoreover, the software engineering tasks required to instantiate the\ncomputational platform are daunting. cesium is an end-to-end time series\nanalysis framework, consisting of a Python library as well as a web front-end\ninterface, that allows researchers to featurize raw data and apply modern\nmachine learning techniques in a simple, reproducible, and extensible way.\nUsers can apply out-of-the-box feature engineering workflows as well as save\nand replay their own analyses. Any steps taken in the front end can also be\nexported to a Jupyter notebook, so users can iterate between possible models\nwithin the front end and then fine-tune their analysis using the additional\ncapabilities of the back-end library. The open-source packages make us of many\nuse modern Python toolkits, including xarray, dask, Celery, Flask, and\nscikit-learn."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1611.01120v1", 
    "title": "Generating Families of Practical Fast Matrix Multiplication Algorithms", 
    "arxiv-id": "1611.01120v1", 
    "author": "Robert A. van de Geijn", 
    "publish": "2016-11-03T18:27:00Z", 
    "summary": "Matrix multiplication (GEMM) is a core operation to numerous scientific\napplications. Traditional implementations of Strassen-like fast matrix\nmultiplication (FMM) algorithms often do not perform well except for very large\nmatrix sizes, due to the increased cost of memory movement, which is\nparticularly noticeable for non-square matrices. Such implementations also\nrequire considerable workspace and modifications to the standard BLAS\ninterface. We propose a code generator framework to automatically implement a\nlarge family of FMM algorithms suitable for multiplications of arbitrary matrix\nsizes and shapes. By representing FMM with a triple of matrices [U,V,W] that\ncapture the linear combinations of submatrices that are formed, we can use the\nKronecker product to define a multi-level representation of Strassen-like\nalgorithms. Incorporating the matrix additions that must be performed for\nStrassen-like algorithms into the inherent packing and micro-kernel operations\ninside GEMM avoids extra workspace and reduces the cost of memory movement.\nAdopting the same loop structures as high-performance GEMM implementations\nallows parallelization of all FMM algorithms with simple but efficient data\nparallelism without the overhead of task parallelism. We present a simple\nperformance model for general FMM algorithms and compare actual performance of\n20+ FMM algorithms to modeled predictions. Our implementations demonstrate a\nperformance benefit over conventional GEMM on single core and multi-core\nsystems. This study shows that Strassen-like fast matrix multiplication can be\nincorporated into libraries for practical use."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1611.01534v1", 
    "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor   Analysis", 
    "arxiv-id": "1611.01534v1", 
    "author": "Samuel Kaski", 
    "publish": "2016-11-03T10:09:13Z", 
    "summary": "The R package GFA provides a full pipeline for factor analysis of multiple\ndata sources that are represented as matrices with co-occurring samples. It\nallows learning dependencies between subsets of the data sources, decomposed\ninto latent factors. The package also implements sparse priors for the\nfactorization, providing interpretable biclusters of the multi-source data"
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1611.08035v1", 
    "title": "Automating the Last-Mile for High Performance Dense Linear Algebra", 
    "arxiv-id": "1611.08035v1", 
    "author": "Franz Franchetti", 
    "publish": "2016-11-24T00:01:07Z", 
    "summary": "High performance dense linear algebra (DLA) libraries often rely on a general\nmatrix multiply (Gemm) kernel that is implemented using assembly or with vector\nintrinsics. In particular, the real-valued Gemm kernels provide the\noverwhelming fraction of performance for the complex-valued Gemm kernels, along\nwith the entire level-3 BLAS and many of the real and complex LAPACK routines.\nThus,achieving high performance for the Gemm kernel translates into a high\nperformance linear algebra stack above this kernel. However, it is a monumental\ntask for a domain expert to manually implement the kernel for every\nlibrary-supported architecture. This leads to the belief that the craft of a\nGemm kernel is more dark art than science. It is this premise that drives the\npopularity of autotuning with code generation in the domain of DLA.\n  This paper, instead, focuses on an analytical approach to code generation of\nthe Gemm kernel for different architecture, in order to shed light on the\ndetails or voo-doo required for implementing a high performance Gemm kernel. We\ndistill the implementation of the kernel into an even smaller kernel, an\nouter-product, and analytically determine how available SIMD instructions can\nbe used to compute the outer-product efficiently. We codify this approach into\na system to automatically generate a high performance SIMD implementation of\nthe Gemm kernel. Experimental results demonstrate that our approach yields\ngenerated kernels with performance that is competitive with kernels implemented\nmanually or using empirical search."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1611.09567v1", 
    "title": "Moore: Interval Arithmetic in Modern C++", 
    "arxiv-id": "1611.09567v1", 
    "author": "Walter F. Mascarenhas", 
    "publish": "2016-11-29T11:12:06Z", 
    "summary": "We present the library Moore, which implements Interval Arithmetic in modern\nC++. This library is based on a new feature in the C++ language called\nconcepts, which reduces the problems caused by template meta programming, and\nleads to a new approach for implementing interval arithmetic libraries in C++."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1701.05913v1", 
    "title": "Scalable linear solvers for sparse linear systems from large-scale   numerical simulations", 
    "arxiv-id": "1701.05913v1", 
    "author": "Zhangxin Chen", 
    "publish": "2017-01-20T19:37:44Z", 
    "summary": "This paper presents our work on designing scalable linear solvers for\nlarge-scale reservoir simulations. The main objective is to support\nimplementation of parallel reservoir simulators on distributed-memory parallel\nsystems, where MPI (Message Passing Interface) is employed for communications\namong computation nodes. Distributed matrix and vector modules are designed,\nwhich are the base of our parallel linear systems. Commonly-used Krylov\nsubspace linear solvers are implemented, including the restarted GMRES method,\nthe LGMRES method, and the BiCGSTAB method. It also has an interface to a\nparallel algebraic multigrid solver, BoomerAMG from HYPRE. Parallel\ngeneral-purpose preconditioners and special preconditioners for reservoir\nsimulations are also developed. The numerical experiments show that our linear\nsolvers have excellent scalability using thousands of CPU cores."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1702.04715v1", 
    "title": "Simflowny 2: An upgraded platform for scientific modeling and simulation", 
    "arxiv-id": "1702.04715v1", 
    "author": "J. Mass\u00f3", 
    "publish": "2017-02-14T20:49:22Z", 
    "summary": "Simflowny is an open platform which automatically generates parallel code of\nscientific dynamical models for different simulation frameworks. Here we\npresent major upgrades on this software to support an extended set of families\nof models, in particular: i) a new generic family for partial differential\nequations, which can include spatial derivatives of any order, ii) a new family\nfor agent based models to study complex phenomena --either on a spatial domain\nor on a graph--. Additionally we introduce a flexible graphical user interface\n(GUI) to accommodate these and future families of equations. This paper\ndescribes the new GUI architecture and summarizes the formal representation and\nimplementation of these new families, providing several validation results."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1702.08425v1", 
    "title": "xSDK Foundations: Toward an Extreme-scale Scientific Software   Development Kit", 
    "arxiv-id": "1702.08425v1", 
    "author": "Ulrike Meier Yang", 
    "publish": "2017-02-27T18:37:36Z", 
    "summary": "Extreme-scale computational science increasingly demands multiscale and\nmultiphysics formulations. Combining software developed by independent groups\nis imperative: no single team has resources for all predictive science and\ndecision support capabilities. Scientific libraries provide high-quality,\nreusable software components for constructing applications with improved\nrobustness and portability. However, without coordination, many libraries\ncannot be easily composed. Namespace collisions, inconsistent arguments, lack\nof third-party software versioning, and additional difficulties make\ncomposition costly.\n  The Extreme-scale Scientific Software Development Kit (xSDK) defines\ncommunity policies to improve code quality and compatibility across\nindependently developed packages (hypre, PETSc, SuperLU, Trilinos, and\nAlquimia) and provides a foundation for addressing broader issues in software\ninteroperability, performance portability, and sustainability. The xSDK\nprovides turnkey installation of member software and seamless combination of\naggregate capabilities, and it marks first steps toward extreme-scale\nscientific software ecosystems from which future applications can be composed\nrapidly with assured quality and scalability."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1703.03116v1", 
    "title": "ForestClaw: A parallel algorithm for patch-based adaptive mesh   refinement on a forest of quadtrees", 
    "arxiv-id": "1703.03116v1", 
    "author": "Carsten Burstedde", 
    "publish": "2017-03-09T03:01:31Z", 
    "summary": "We describe a parallel, adaptive, multi-block algorithm for explicit\nintegration of time dependent partial differential equations on two-dimensional\nCartesian grids. The grid layout we consider consists of a nested hierarchy of\nfixed size, non-overlapping, logically Cartesian grids stored as leaves in a\nquadtree. Dynamic grid refinement and parallel partitioning of the grids is\ndone through the use of the highly scalable quadtree/octree library p4est.\nBecause our concept is multi-block, we are able to easily solve on a variety of\ngeometries including the cubed sphere. In this paper, we pay special attention\nto providing details of the parallel ghost-filling algorithm needed to ensure\nthat both corner and edge ghost regions around each grid hold valid values.\n  We have implemented this algorithm in the ForestClaw code using single-grid\nsolvers from ClawPack, a software package for solving hyperbolic PDEs using\nfinite volumes methods. We show weak and strong scalability results for scalar\nadvection problems on two-dimensional manifold domains on 1 to 64Ki MPI\nprocesses, demonstrating neglible regridding overhead."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/1703.05522v1", 
    "title": "Treating Smoothness and Balance during Data Exchange in Explicit   Simulator Coupling or Cosimulation", 
    "arxiv-id": "1703.05522v1", 
    "author": "Jaroslav Vond\u0159ejc", 
    "publish": "2017-03-16T09:16:32Z", 
    "summary": "Cosimulation methods allow combination of simulation tools of physical\nsystems running in parallel to act as a single simulation environment for a big\nsystem. As data is passed across subsystem boundaries instead of solving the\nsystem as one single equation system, it is not ensured that systemwide\nbalances are fulfilled. If the exchanged data is a flow of a conserved\nquantity, approximation errors can accumulate and make simulation results\ninaccurate. The problem of approximation errors is typically addressed with\nextrapolation of exchanged data. Nevertheless balance errors occur as\nextrapolation is approximation. This problem can be handled with balance\ncorrection methods which compensate these errors by adding corrections for the\nbalances to the signal in next coupling time step. This work aims at combining\nextrapolation of exchanged data and balance correction in a way that the\nexchanged signal not only remains smooth, meaning the existence of continuous\nderivatives, but even in a way reducing the derivatives, in order to avoid\nunphysical dynamics caused by the coupling. To this end, suitable switch and\nhat functions are constructed and applied to the problem."
},{
    "category": "cond-mat.stat-mech", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/cond-mat/0605570v1", 
    "title": "Generalized Box-Muller method for generating q-Gaussian random deviates", 
    "arxiv-id": "cond-mat/0605570v1", 
    "author": "Constantino Tsallis", 
    "publish": "2006-05-23T18:04:56Z", 
    "summary": "The q-Gaussian distribution is known to be an attractor of certain correlated\nsystems, and is the distribution which, under appropriate constraints,\nmaximizes the entropy Sq, basis of nonextensive statistical mechanics. This\ntheory is postulated as a natural extension of the standard (Boltzmann-Gibbs)\nstatistical mechanics, and may explain the ubiquitous appearance of\nheavy-tailed distributions in both natural and man-made systems. The q-Gaussian\ndistribution is also used as a numerical tool, for example as a visiting\ndistribution in Generalized Simulated Annealing. We develop and present a\nsimple, easy to implement numerical method for generating random deviates from\na q-Gaussian distribution based upon a generalization of the well known\nBox-Muller method. Our method is suitable for a larger range of q values, q<3,\nthan has previously appeared in the literature, and can generate deviates from\nq-Gaussian distributions of arbitrary width and center. MATLAB code showing a\nstraightforward implementation is also included."
},{
    "category": "cs.MS", 
    "doi": "10.5194/gmd-9-3803-2016", 
    "link": "http://arxiv.org/pdf/cs/9901008v1", 
    "title": "Fast Computational Algorithms for the Discrete Wavelet Transform and   Applications of Localized Orthonormal Bases in Signal Classification", 
    "arxiv-id": "cs/9901008v1", 
    "author": "Eirik Fossgaard", 
    "publish": "1999-01-16T16:54:01Z", 
    "summary": "We construct an algorithm for implementing the discrete wavelet transform by\nmeans of matrices in SO_2(R) for orthonormal compactly supported wavelets and\nmatrices in SL_m(R), m > = 2, for compactly supported biorthogonal wavelets. We\nshow that in 1 dimension the total operation count using this algorithm can be\nreduced to about 50% of the conventional convolution and downsampling by\n2-operation for both orthonormal and biorthogonal filters. In the special case\nof biorthogonal symmetric odd-odd filters, we show an implementation yielding a\ntotal operation count of about 38% of the conventional method. In 2 dimensions\nwe show an implementation of this algorithm yielding a reduction in the total\noperation count of about 70% when the filters are orthonormal, a reduction of\nabout 62% for general biorthogonal filters, and a reduction of about 70% if the\nfilters are symmetric odd-odd length filters. We further extend these results\nto 3 dimensions. We also show how the SO_2(R)-method for implementing the\ndiscrete wavelet transform may be exploited to compute short FIR filters, and\nwe construct edge mappings where we try to improve upon the degree of\npreservation of regularity in the conventional methods. We also consider a\ntwo-class waveform discrimination problem. A statistical space-frequency\nanalysis is performed on a training data set using the LDB-algorithm of N.Saito\nand R.Coifman. The success of the algorithm on this particular problem is\nevaluated on a disjoint test data set."
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/9912021v2", 
    "title": "Seeing the Forest in the Tree: Applying VRML to Mathematical Problems in   Number Theory", 
    "arxiv-id": "cs/9912021v2", 
    "author": "Neil J. Gunther", 
    "publish": "1999-12-31T18:36:38Z", 
    "summary": "We show how VRML (Virtual Reality Modeling Language) can provide potentially\npowerful insight into the 3x + 1 problem via the introduction of a unique\ngeometrical object, called the 'G-cell', akin to a fractal generator. We\npresent an example of a VRML world developed programmatically with the G-cell.\nThe role of VRML as a tool for furthering the understanding the 3x+1 problem is\npotentially significant for several reasons: a) VRML permits the observer to\nzoom into the geometric structure at all scales (up to limitations of the\ncomputing platform). b) VRML enables rotation to alter comparative visual\nperspective (similar to Tukey's data-spinning concept). c) VRML facilitates the\ndemonstration of interesting tree features between collaborators on the\ninternet who might otherwise have difficulty conveying their ideas\nunambiguously. d) VRML promises to reveal any dimensional dependencies among\n3x+1 sequences."
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0001018v1", 
    "title": "Adaptive simulated annealing (ASA): Lessons learned", 
    "arxiv-id": "cs/0001018v1", 
    "author": "Lester Ingber", 
    "publish": "2000-01-23T20:36:49Z", 
    "summary": "Adaptive simulated annealing (ASA) is a global optimization algorithm based\non an associated proof that the parameter space can be sampled much more\nefficiently than by using other previous simulated annealing algorithms. The\nauthor's ASA code has been publicly available for over two years. During this\ntime the author has volunteered to help people via e-mail, and the feedback\nobtained has been used to further develop the code. Some lessons learned, in\nparticular some which are relevant to other simulated annealing algorithms, are\ndescribed."
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0201007v2", 
    "title": "Algorithm for generating orthogonal matrices with rational elements", 
    "arxiv-id": "cs/0201007v2", 
    "author": "Ruslan Sharipov", 
    "publish": "2002-01-10T15:54:24Z", 
    "summary": "Special orthogonal matrices with rational elements form the group SO(n,Q),\nwhere Q is the field of rational numbers. A theorem describing the structure of\nan arbitrary matrix from this group is proved. This theorem yields an algorithm\nfor generating such matrices by means of random number routines."
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0302026v1", 
    "title": "Recursive function templates as a solution of linear algebra expressions   in C++", 
    "arxiv-id": "cs/0302026v1", 
    "author": "Volodymyr Myrnyy", 
    "publish": "2003-02-19T15:59:28Z", 
    "summary": "The article deals with a kind of recursive function templates in C++, where\nthe recursion is realized corresponding template parameters to achieve better\ncomputational performance. Some specialization of these template functions ends\nthe recursion and can be implemented using optimized hardware dependent or\nindependent routines. The method is applied in addition to the known expression\ntemplates technique to solve linear algebra expressions with the help of the\nBLAS library. The whole implementation produces a new library, which keeps\nobject-oriented benefits and has a higher computational speed represented in\nthe tests."
},{
    "category": "cs.CE", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0307061v1", 
    "title": "Boundary knot method for Laplace and biharmonic problems", 
    "arxiv-id": "cs/0307061v1", 
    "author": "W. Chen", 
    "publish": "2003-07-28T09:35:52Z", 
    "summary": "The boundary knot method (BKM) [1] is a meshless boundary-type radial basis\nfunction (RBF) collocation scheme, where the nonsingular general solution is\nused instead of fundamental solution to evaluate the homogeneous solution,\nwhile the dual reciprocity method (DRM) is employed to approximation of\nparticular solution. Despite the fact that there are not nonsingular RBF\ngeneral solutions available for Laplace and biharmonic problems, this study\nshows that the method can be successfully applied to these problems. The\nhigh-order general and fundamental solutions of Burger and Winkler equations\nare also first presented here."
},{
    "category": "cs.SC", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0310055v1", 
    "title": "Mace4 Reference Manual and Guide", 
    "arxiv-id": "cs/0310055v1", 
    "author": "William McCune", 
    "publish": "2003-10-28T16:56:44Z", 
    "summary": "Mace4 is a program that searches for finite models of first-order formulas.\nFor a given domain size, all instances of the formulas over the domain are\nconstructed. The result is a set of ground clauses with equality. Then, a\ndecision procedure based on ground equational rewriting is applied. If\nsatisfiability is detected, one or more models are printed. Mace4 is a useful\ncomplement to first-order theorem provers, with the prover searching for proofs\nand Mace4 looking for countermodels, and it is useful for work on finite\nalgebras. Mace4 performs better on equational problems than did our previous\nmodel-searching program Mace2."
},{
    "category": "cs.SC", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0310056v1", 
    "title": "OTTER 3.3 Reference Manual", 
    "arxiv-id": "cs/0310056v1", 
    "author": "William McCune", 
    "publish": "2003-10-28T19:17:38Z", 
    "summary": "OTTER is a resolution-style theorem-proving program for first-order logic\nwith equality. OTTER includes the inference rules binary resolution,\nhyperresolution, UR-resolution, and binary paramodulation. Some of its other\nabilities and features are conversion from first-order formulas to clauses,\nforward and back subsumption, factoring, weighting, answer literals, term\nordering, forward and back demodulation, evaluable functions and predicates,\nKnuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is\nfree, and is portable to many different kinds of computer."
},{
    "category": "cs.DC", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0401006v1", 
    "title": "Cluster computing performances using virtual processors and mathematical   software", 
    "arxiv-id": "cs/0401006v1", 
    "author": "Gianluca Argentini", 
    "publish": "2004-01-09T12:09:51Z", 
    "summary": "In this paper I describe some results on the use of virtual processors\ntechnology for parallelize some SPMD computational programs in a cluster\nenvironment. The tested technology is the INTEL Hyper Threading on real\nprocessors, and the programs are MATLAB 6.5 Release 13 scripts for floating\npoints computation. By the use of this technology, I tested that a cluster can\nrun with benefit a number of concurrent processes double the amount of physical\nprocessors. The conclusions of the work concern on the utility and limits of\nthe used approach. The main result is that using virtual processors is a good\ntechnique for improving parallel programs not only for memory-based\ncomputations, but in the case of massive disk-storage operations too."
},{
    "category": "cs.AI", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0406025v1", 
    "title": "Directional Consistency for Continuous Numerical Constraints", 
    "arxiv-id": "cs/0406025v1", 
    "author": "Laurent Granvilliers", 
    "publish": "2004-06-16T16:33:39Z", 
    "summary": "Bounds consistency is usually enforced on continuous constraints by first\ndecomposing them into binary and ternary primitives. This decomposition has\nlong been shown to drastically slow down the computation of solutions. To\ntackle this, Benhamou et al. have introduced an algorithm that avoids formally\ndecomposing constraints. Its better efficiency compared to the former method\nhas already been experimentally demonstrated. It is shown here that their\nalgorithm implements a strategy to enforce on a continuous constraint a\nconsistency akin to Directional Bounds Consistency as introduced by Dechter and\nPearl for discrete problems. The algorithm is analyzed in this framework, and\ncompared with algorithms that enforce bounds consistency. These theoretical\nresults are eventually contrasted with new experimental results on standard\nbenchmarks from the interval constraint community."
},{
    "category": "cs.NA", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0409033v4", 
    "title": "Mean and Variance Estimation by Kriging", 
    "arxiv-id": "cs/0409033v4", 
    "author": "Tomasz Suslo", 
    "publish": "2004-09-17T10:39:45Z", 
    "summary": "The aim of the paper is to derive the numerical least-squares estimator for\nmean and variance of random variable. In order to do so the following questions\nhave to be answered: (i) what is the statistical model for the estimation\nprocedure? (ii) what are the properties of the estimator, like optimality (in\nwhich class) or asymptotic properties? (iii) how does the estimator work in\npractice, how compared to competing estimators?"
},{
    "category": "cs.MS", 
    "doi": "10.1117/12.373461", 
    "link": "http://arxiv.org/pdf/cs/0505031v1", 
    "title": "Estudo e Implementacao de Algoritmos de Roteamento sobre Grafos em um   Sistema de Informacoes Geograficas", 
    "arxiv-id": "cs/0505031v1", 
    "author": "Horacio H. Yanasse", 
    "publish": "2005-05-11T18:50:32Z", 
    "summary": "This article presents an implementation of a graphical software with various\nalgorithms in Operations research, like minimum path, minimum tree, chinese\npostman problem and travelling salesman."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.nima.2005.11.171", 
    "link": "http://arxiv.org/pdf/cs/0509070v2", 
    "title": "A Maple Package for Computing Groebner Bases for Linear Recurrence   Relations", 
    "arxiv-id": "cs/0509070v2", 
    "author": "Daniel Robertz", 
    "publish": "2005-09-22T15:45:35Z", 
    "summary": "A Maple package for computing Groebner bases of linear difference ideals is\ndescribed. The underlying algorithm is based on Janet and Janet-like monomial\ndivisions associated with finite difference operators. The package can be used,\nfor example, for automatic generation of difference schemes for linear partial\ndifferential equations and for reduction of multiloop Feynman integrals. These\ntwo possible applications are illustrated by simple examples of the Laplace\nequation and a one-loop scalar integral of propagator type"
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.nima.2005.11.171", 
    "link": "http://arxiv.org/pdf/cs/0512056v1", 
    "title": "PURRS: Towards Computer Algebra Support for Fully Automatic Worst-Case   Complexity Analysis", 
    "arxiv-id": "cs/0512056v1", 
    "author": "Enea Zaffanella", 
    "publish": "2005-12-14T09:54:01Z", 
    "summary": "Fully automatic worst-case complexity analysis has a number of applications\nin computer-assisted program manipulation. A classical and powerful approach to\ncomplexity analysis consists in formally deriving, from the program syntax, a\nset of constraints expressing bounds on the resources required by the program,\nwhich are then solved, possibly applying safe approximations. In several\ninteresting cases, these constraints take the form of recurrence relations.\nWhile techniques for solving recurrences are known and implemented in several\ncomputer algebra systems, these do not completely fulfill the needs of fully\nautomatic complexity analysis: they only deal with a somewhat restricted class\nof recurrence relations, or sometimes require user intervention, or they are\nrestricted to the computation of exact solutions that are often so complex to\nbe unmanageable, and thus useless in practice. In this paper we briefly\ndescribe PURRS, a system and software library aimed at providing all the\ncomputer algebra services needed by applications performing or exploiting the\nresults of worst-case complexity analyses. The capabilities of the system are\nillustrated by means of examples derived from the analysis of programs written\nin a domain-specific functional programming language for real-time embedded\nsystems."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.nima.2005.11.171", 
    "link": "http://arxiv.org/pdf/cs/0512072v1", 
    "title": "Computations with one and two real algebraic numbers", 
    "arxiv-id": "cs/0512072v1", 
    "author": "Elias P. Tsigaridas", 
    "publish": "2005-12-18T16:58:35Z", 
    "summary": "We present algorithmic and complexity results concerning computations with\none and two real algebraic numbers, as well as real solving of univariate\npolynomials and bivariate polynomial systems with integer coefficients using\nSturm-Habicht sequences.\n  Our main results, in the univariate case, concern the problems of real root\nisolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate,\nthe problems of system real solving (Th.42), sign evaluation (Th. 37) and\nsimultaneous inequalities (Cor. 43)."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.nima.2005.11.171", 
    "link": "http://arxiv.org/pdf/cs/0603005v4", 
    "title": "A Basic Introduction on Math-Link in Mathematica", 
    "arxiv-id": "cs/0603005v4", 
    "author": "Santanu K. Maiti", 
    "publish": "2006-03-01T14:29:19Z", 
    "summary": "Starting from the basic ideas of mathematica, we give a detailed description\nabout the way of linking of external programs with mathematica through proper\nmathlink commands. This article may be quite helpful for the beginners to start\nwith and write programs in mathematica.\n  In the first part, we illustrate how to use a mathemtica notebook and write a\ncomplete program in the notebook. Following with this, we also mention\nelaborately about the utility of the local and global variables those are very\nessential for writing a program in mathematica. All the commands needed for\ndoing different mathematical operations can be found with some proper examples\nin the mathematica book written by Stephen Wolfram \\cite{wolfram}.\n  In the rest of this article, we concentrate our study on the most significant\nissue which is the process of linking of {\\em external programs} with\nmathematica, so-called the mathlink operation. By using proper mathlink\ncommands one can run very tedious jobs efficiently and the operations become\nextremely fast."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.nima.2005.11.171", 
    "link": "http://arxiv.org/pdf/cs/0604038v1", 
    "title": "UniCalc.LIN: a linear constraint solver for the UniCalc system", 
    "arxiv-id": "cs/0604038v1", 
    "author": "E. Botoeva", 
    "publish": "2006-04-10T06:30:02Z", 
    "summary": "In this short paper we present a linear constraint solver for the UniCalc\nsystem, an environment for reliable solution of mathematical modeling problems."
},{
    "category": "cs.NA", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0605058v1", 
    "title": "A Monadic, Functional Implementation of Real Numbers", 
    "arxiv-id": "cs/0605058v1", 
    "author": "Russell O'Connor", 
    "publish": "2006-05-14T17:05:14Z", 
    "summary": "Large scale real number computation is an essential ingredient in several\nmodern mathematical proofs. Because such lengthy computations cannot be\nverified by hand, some mathematicians want to use software proof assistants to\nverify the correctness of these proofs. This paper develops a new\nimplementation of the constructive real numbers and elementary functions for\nsuch proofs by using the monad properties of the completion operation on metric\nspaces. Bishop and Bridges's notion of regular sequences is generalized to,\nwhat I call, regular functions which form the completion of any metric space.\nUsing the monad operations, continuous functions on length spaces (a common\nsubclass of metric spaces) are created by lifting continuous functions on the\noriginal space. A prototype Haskell implementation has been created. I believe\nthat this approach yields a real number library that is reasonably efficient\nfor computation, and still simple enough to easily verify its correctness."
},{
    "category": "cs.MS", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0605090v4", 
    "title": "Mathematica: A System of Computer Programs", 
    "arxiv-id": "cs/0605090v4", 
    "author": "Santanu K. Maiti", 
    "publish": "2006-05-20T05:43:55Z", 
    "summary": "Starting from the basic level of mathematica here we illustrate how to use a\nmathematica notebook and write a program in the notebook. Next, we investigate\nelaborately the way of linking of external programs with mathematica, so-called\nthe mathlink operation. Using this technique we can run very tedious jobs quite\nefficiently, and the operations become extremely fast. Sometimes it is quite\ndesirable to run jobs in background of a computer which can take considerable\namount of time to finish, and this allows us to do work on other tasks, while\nkeeping the jobs running. The way of running jobs, written in a mathematica\nnotebook, in background is quite different from the conventional methods i.e.,\nthe techniques for the programs written in other languages like C, C++, F77,\nF90, F95, etc. To illustrate it, in the present article we study how to create\na mathematica batch-file from a mathematica notebook and run it in the\nbackground. Finally, we explore the most significant issue of this article.\nHere we describe the basic ideas for parallelizing a mathematica program by\nsharing its independent parts into all other remote computers available in the\nnetwork. Doing the parallelization, we can perform large computational\noperations within a very short period of time, and therefore, the efficiency of\nthe numerical works can be achieved. Parallel computation supports any version\nof mathematica and it also works significantly well even if different versions\nof mathematica are installed in different computers. All the operations studied\nin this article run under any supported operating system like Unix, Windows,\nMacintosh, etc. For the sake of our illustrations, here we concentrate all the\ndiscussions only for the Unix based operating system."
},{
    "category": "cs.MS", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0606023v3", 
    "title": "Parallel Evaluation of Mathematica Programs in Remote Computers   Available in Network", 
    "arxiv-id": "cs/0606023v3", 
    "author": "Santanu K. Maiti", 
    "publish": "2006-06-06T12:11:23Z", 
    "summary": "Mathematica is a powerful application package for doing mathematics and is\nused almost in all branches of science. It has widespread applications ranging\nfrom quantum computation, statistical analysis, number theory, zoology,\nastronomy, and many more. Mathematica gives a rich set of programming\nextensions to its end-user language, and it permits us to write programs in\nprocedural, functional, or logic (rule-based) style, or a mixture of all three.\nFor tasks requiring interfaces to the external environment, mathematica\nprovides mathlink, which allows us to communicate mathematica programs with\nexternal programs written in C, C++, F77, F90, F95, Java, or other languages.\nIt has also extensive capabilities for editing graphics, equations, text, etc.\n  In this article, we explore the basic mechanisms of parallelization of a\nmathematica program by sharing different parts of the program into all other\ncomputers available in the network. Doing the parallelization, we can perform\nlarge computational operations within a very short period of time, and\ntherefore, the efficiency of the numerical works can be achieved. Parallel\ncomputation supports any version of mathematica and it also works as well even\nif different versions of mathematica are installed in different computers. The\nwhole operation can run under any supported operating system like Unix,\nWindows, Macintosh, etc. Here we focus our study only for the Unix based\noperating system, but this method works as well for all other cases."
},{
    "category": "cs.MS", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0609129v2", 
    "title": "One approach to the digital visualization of hedgehogs in holomorphic   dynamics", 
    "arxiv-id": "cs/0609129v2", 
    "author": "Alessandro Rosa", 
    "publish": "2006-09-22T18:53:17Z", 
    "summary": "In the field of holomorphic dynamics in one complex variable, hedgehog is the\nlocal invariant set arising about a Cremer point and endowed with a very\ncomplicate shape as well as relating to very weak numerical conditions. We give\na solution to the open problem of its digital visualization, featuring either a\ntime saving approach and a far-reaching insight."
},{
    "category": "cs.NA", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0610122v1", 
    "title": "Faithful Polynomial Evaluation with Compensated Horner Algorithm", 
    "arxiv-id": "cs/0610122v1", 
    "author": "Nicolas Louvet", 
    "publish": "2006-10-20T11:22:52Z", 
    "summary": "This paper presents two sufficient conditions to ensure a faithful evaluation\nof polynomial in IEEE-754 floating point arithmetic. Faithfulness means that\nthe computed value is one of the two floating point neighbours of the exact\nresult; it can be satisfied using a more accurate algorithm than the classic\nHorner scheme. One condition here provided is an apriori bound of the\npolynomial condition number derived from the error analysis of the compensated\nHorner algorithm. The second condition is both dynamic and validated to check\nat the running time the faithfulness of a given evaluation. Numerical\nexperiments illustrate the behavior of these two conditions and that associated\nrunning time over-cost is really interesting."
},{
    "category": "cs.MS", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0611127v1", 
    "title": "Coupling Methodology within the Software Platform Alliances", 
    "arxiv-id": "cs/0611127v1", 
    "author": "Cl\u00e9ment Chavant", 
    "publish": "2006-11-24T16:43:06Z", 
    "summary": "CEA, ANDRA and EDF are jointly developing the software platform ALLIANCES\nwhich aim is to produce a tool for the simulation of nuclear waste storage and\ndisposal repository. This type of simulations deals with highly coupled\nthermo-hydro-mechanical and chemical (T-H-M-C) processes. A key objective of\nAlliances is to give the capability for coupling algorithms development between\nexisting codes. The aim of this paper is to present coupling methodology use in\nthe context of this software platform."
},{
    "category": "cs.DC", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0612036v1", 
    "title": "Revisiting Matrix Product on Master-Worker Platforms", 
    "arxiv-id": "cs/0612036v1", 
    "author": "Frederic Vivien", 
    "publish": "2006-12-06T14:34:29Z", 
    "summary": "This paper is aimed at designing efficient parallel matrix-product algorithms\nfor heterogeneous master-worker platforms. While matrix-product is\nwell-understood for homogeneous 2D-arrays of processors (e.g., Cannon algorithm\nand ScaLAPACK outer product algorithm), there are three key hypotheses that\nrender our work original and innovative:\n  - Centralized data. We assume that all matrix files originate from, and must\nbe returned to, the master.\n  - Heterogeneous star-shaped platforms. We target fully heterogeneous\nplatforms, where computational resources have different computing powers.\n  - Limited memory. Because we investigate the parallelization of large\nproblems, we cannot assume that full matrix panels can be stored in the worker\nmemories and re-used for subsequent updates (as in ScaLAPACK).\n  We have devised efficient algorithms for resource selection (deciding which\nworkers to enroll) and communication ordering (both for input and result\nmessages), and we report a set of numerical experiments on various platforms at\nEcole Normale Superieure de Lyon and the University of Tennessee. However, we\npoint out that in this first version of the report, experiments are limited to\nhomogeneous platforms."
},{
    "category": "cs.MS", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0612085v1", 
    "title": "The Parma Polyhedra Library: Toward a Complete Set of Numerical   Abstractions for the Analysis and Verification of Hardware and Software   Systems", 
    "arxiv-id": "cs/0612085v1", 
    "author": "Enea Zaffanella", 
    "publish": "2006-12-18T10:15:38Z", 
    "summary": "Since its inception as a student project in 2001, initially just for the\nhandling (as the name implies) of convex polyhedra, the Parma Polyhedra Library\nhas been continuously improved and extended by joining scrupulous research on\nthe theoretical foundations of (possibly non-convex) numerical abstractions to\na total adherence to the best available practices in software development. Even\nthough it is still not fully mature and functionally complete, the Parma\nPolyhedra Library already offers a combination of functionality, reliability,\nusability and performance that is not matched by similar, freely available\nlibraries. In this paper, we present the main features of the current version\nof the library, emphasizing those that distinguish it from other similar\nlibraries and those that are important for applications in the field of\nanalysis and verification of hardware and software systems."
},{
    "category": "cs.CE", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0612126v1", 
    "title": "The virtual reality framework for engineering objects", 
    "arxiv-id": "cs/0612126v1", 
    "author": "Nikolay P. Ivankov", 
    "publish": "2006-12-22T19:19:41Z", 
    "summary": "A framework for virtual reality of engineering objects has been developed.\nThis framework may simulate different equipment related to virtual reality.\nFramework supports 6D dynamics, ordinary differential equations, finite\nformulas, vector and matrix operations. The framework also supports embedding\nof external software."
},{
    "category": "cs.CG", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0701122v2", 
    "title": "Applications of Polyhedral Computations to the Analysis and Verification   of Hardware and Software Systems", 
    "arxiv-id": "cs/0701122v2", 
    "author": "Enea Zaffanella", 
    "publish": "2007-01-19T08:39:34Z", 
    "summary": "Convex polyhedra are the basis for several abstractions used in static\nanalysis and computer-aided verification of complex and sometimes mission\ncritical systems. For such applications, the identification of an appropriate\ncomplexity-precision trade-off is a particularly acute problem, so that the\navailability of a wide spectrum of alternative solutions is mandatory. We\nsurvey the range of applications of polyhedral computations in this area; give\nan overview of the different classes of polyhedra that may be adopted; outline\nthe main polyhedral operations required by automatic analyzers and verifiers;\nand look at some possible combinations of polyhedra with other numerical\nabstractions that have the potential to improve the precision of the analysis.\nAreas where further theoretical investigations can result in important\ncontributions are highlighted."
},{
    "category": "cs.SC", 
    "doi": "10.1017/S0960129506005871", 
    "link": "http://arxiv.org/pdf/cs/0702010v1", 
    "title": "A canonical form for some piecewise defined functions", 
    "arxiv-id": "cs/0702010v1", 
    "author": "Jacques Carette", 
    "publish": "2007-02-01T17:54:50Z", 
    "summary": "We define a canonical form for piecewise defined functions. We show that this\nhas a wider range of application as well as better complexity properties than\nprevious work."
},{
    "category": "cs.MS", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0705.2626v1", 
    "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in   hypre and PETSc", 
    "arxiv-id": "0705.2626v1", 
    "author": "E. E. Ovtchinnikov", 
    "publish": "2007-05-18T02:25:16Z", 
    "summary": "We describe our software package Block Locally Optimal Preconditioned\nEigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as\na stand-alone serial library, as an external package to PETSc (``Portable,\nExtensible Toolkit for Scientific Computation'', a general purpose suite of\ntools for the scalable solution of partial differential equations and related\nproblems developed by Argonne National Laboratory), and is also built into {\\it\nhypre} (``High Performance Preconditioners'', scalable linear solvers package\ndeveloped by Lawrence Livermore National Laboratory). The present BLOPEX\nrelease includes only one solver--the Locally Optimal Block Preconditioned\nConjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it\nhypre} provides users with advanced high-quality parallel preconditioners for\nlinear systems, in particular, with domain decomposition and multigrid\npreconditioners. With BLOPEX, the same preconditioners can now be efficiently\nused for symmetric eigenvalue problems. PETSc facilitates the integration of\nindependently developed application modules with strict attention to component\ninteroperability, and makes BLOPEX extremely easy to compile and use with\npreconditioners that are available via PETSc. We present the LOBPCG algorithm\nin BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability\nof BLOPEX by testing it on a number of distributed and shared memory parallel\nsystems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron\nworkstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition\nand {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem,\nthe standard 7-point finite-difference approximation of the 3-D Laplacian, with\nthe problem size in the range $10^5-10^8$."
},{
    "category": "cs.NA", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0705.4369v1", 
    "title": "Computing Integer Powers in Floating-Point Arithmetic", 
    "arxiv-id": "0705.4369v1", 
    "author": "Jean-Michel Muller", 
    "publish": "2007-05-30T11:34:39Z", 
    "summary": "We introduce two algorithms for accurately evaluating powers to a positive\ninteger in floating-point arithmetic, assuming a fused multiply-add (fma)\ninstruction is available. We show that our log-time algorithm always produce\nfaithfully-rounded results, discuss the possibility of getting correctly\nrounded results, and show that results correctly rounded in double precision\ncan be obtained if extended-precision is available with the possibility to\nround into double precision (with a single rounding)."
},{
    "category": "math.NA", 
    "doi": "10.1137/060661624", 
    "link": "http://arxiv.org/pdf/0707.1490v1", 
    "title": "Fast computing of velocity field for flows in industrial burners and   pumps", 
    "arxiv-id": "0707.1490v1", 
    "author": "Gianluca Argentini", 
    "publish": "2007-07-10T16:23:43Z", 
    "summary": "In this work we present a technique of fast numerical computation for\nsolutions of Navier-Stokes equations in the case of flows of industrial\ninterest. At first the partial differential equations are translated into a set\nof nonlinear ordinary differential equations using the geometrical shape of the\ndomain where the flow is developing, then these ODEs are numerically resolved\nusing a set of computations distributed among the available processors. We\npresent some results from simulations on a parallel hardware architecture using\nnative multithreads software and simulating a shared-memory or a\ndistributed-memory environment."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0707.1716v1", 
    "title": "Numerical Calculation With Arbitrary Precision", 
    "arxiv-id": "0707.1716v1", 
    "author": "L. G. S. Duarte", 
    "publish": "2007-07-11T22:24:48Z", 
    "summary": "The vast use of computers on scientific numerical computation makes the\nawareness of the limited precision that these machines are able to provide us\nan essential matter. A limited and insufficient precision allied to the\ntruncation and rounding errors may induce the user to incorrect interpretation\nof his/hers answer. In this work, we have developed a computational package to\nminimize this kind of error by offering arbitrary precision numbers and\ncalculation. This is very important in Physics where we can work with numbers\ntoo small and too big simultaneously."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0708.3721v1", 
    "title": "Verified Real Number Calculations: A Library for Interval Arithmetic", 
    "arxiv-id": "0708.3721v1", 
    "author": "C\u00e9sar Mu\u00f1oz", 
    "publish": "2007-08-28T07:14:29Z", 
    "summary": "Real number calculations on elementary functions are remarkably difficult to\nhandle in mechanical proofs. In this paper, we show how these calculations can\nbe performed within a theorem prover or proof assistant in a convenient and\nhighly automated as well as interactive way. First, we formally establish upper\nand lower bounds for elementary functions. Then, based on these bounds, we\ndevelop a rational interval arithmetic where real number calculations take\nplace in an algebraic setting. In order to reduce the dependency effect of\ninterval arithmetic, we integrate two techniques: interval splitting and taylor\nseries expansions. This pragmatic approach has been developed, and formally\nverified, in a theorem prover. The formal development also includes a set of\ncustomizable strategies to automate proofs involving explicit calculations over\nreal numbers. Our ultimate goal is to provide guaranteed proofs of numerical\nproperties with minimal human theorem-prover interaction."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0708.3722v1", 
    "title": "Formally Verified Argument Reduction with a Fused-Multiply-Add", 
    "arxiv-id": "0708.3722v1", 
    "author": "Ren Cang Li", 
    "publish": "2007-08-28T07:15:08Z", 
    "summary": "Cody & Waite argument reduction technique works perfectly for reasonably\nlarge arguments but as the input grows there are no bit left to approximate the\nconstant with enough accuracy. Under mild assumptions, we show that the result\ncomputed with a fused-multiply-add provides a fully accurate result for many\npossible values of the input with a constant almost accurate to the full\nworking precision. We also present an algorithm for a fully accurate second\nreduction step to reach double full accuracy (all the significand bits of two\nnumbers are significant) even in the worst cases of argument reduction. Our\nwork recalls the common algorithms and presents proofs of correctness. All the\nproofs are formally verified using the Coq automatic proof checker."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0709.1272v3", 
    "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore   Architectures", 
    "arxiv-id": "0709.1272v3", 
    "author": "Jack Dongarra", 
    "publish": "2007-09-09T16:32:46Z", 
    "summary": "As multicore systems continue to gain ground in the High Performance\nComputing world, linear algebra algorithms have to be reformulated or new\nalgorithms have to be developed in order to take advantage of the architectural\nfeatures on these new processors. Fine grain parallelism becomes a major\nrequirement and introduces the necessity of loose synchronization in the\nparallel execution of an operation. This paper presents an algorithm for the\nCholesky, LU and QR factorization where the operations can be represented as a\nsequence of small tasks that operate on square blocks of data. These tasks can\nbe dynamically scheduled for execution based on the dependencies among them and\non the availability of computational resources. This may result in an out of\norder execution of the tasks which will completely hide the presence of\nintrinsically sequential tasks in the factorization. Performance comparisons\nare presented with the LAPACK algorithms where parallelism can only be\nexploited at the level of the BLAS operations and vendor implementations."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0711.4444v2", 
    "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation   Model OPA with the Automatic Differentiation tool TAPENADE", 
    "arxiv-id": "0711.4444v2", 
    "author": "Benjamin Dauvergne", 
    "publish": "2007-11-28T08:04:18Z", 
    "summary": "The ocean general circulation model OPA is developed by the LODYC team at\nParis VI university. OPA has recently undergone a major rewriting, migrating to\nFORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the\nadjoint of OPA was written by hand at a high development cost. We use the\nAutomatic Differentiation tool TAPENADE to build mechanicaly the tangent and\nadjoint codes of OPA. We validate the differentiated codes by comparison with\ndivided differences, and also with an identical twin experiment. We apply\nstate-of-the-art methods to improve the performance of the adjoint code. In\nparticular we implement the Griewank and Walther's binomial checkpointing\nalgorithm which gives us an optimal trade-off between time and memory\nconsumption. We apply a specific strategy to differentiate the iterative linear\nsolver that comes from the implicit time stepping scheme"
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0801.0523v1", 
    "title": "Certifying floating-point implementations using Gappa", 
    "arxiv-id": "0801.0523v1", 
    "author": "Guillaume Melquiond", 
    "publish": "2008-01-03T13:34:03Z", 
    "summary": "High confidence in floating-point programs requires proving numerical\nproperties of final and intermediate values. One may need to guarantee that a\nvalue stays within some range, or that the error relative to some ideal value\nis well bounded. Such work may require several lines of proof for each line of\ncode, and will usually be broken by the smallest change to the code (e.g. for\nmaintenance or optimization purpose). Certifying these programs by hand is\ntherefore very tedious and error-prone. This article discusses the use of the\nGappa proof assistant in this context. Gappa has two main advantages over\nprevious approaches: Its input format is very close to the actual C code to\nvalidate, and it automates error evaluation and propagation using interval\narithmetic. Besides, it can be used to incrementally prove complex mathematical\nproperties pertaining to the C code. Yet it does not require any specific\nknowledge about automatic theorem proving, and thus is accessible to a wide\ncommunity. Moreover, Gappa may generate a formal proof of the results that can\nbe checked independently by a lower-level proof assistant like Coq, hence\nproviding an even higher confidence in the certification of the numerical code.\nThe article demonstrates the use of this tool on a real-size example, an\nelementary function with correctly rounded output."
},{
    "category": "cs.OH", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0802.2371v1", 
    "title": "Generic and Typical Ranks of Three-Way Arrays", 
    "arxiv-id": "0802.2371v1", 
    "author": "J. ten Berge", 
    "publish": "2008-02-17T09:48:07Z", 
    "summary": "The concept of tensor rank, introduced in the twenties, has been popularized\nat the beginning of the seventies. This has allowed to carry out Factor\nAnalysis on arrays with more than two indices. The generic rank may be seen as\nan upper bound to the number of factors that can be extracted from a given\ntensor. We explain in this short paper how to obtain numerically the generic\nrank of tensors of arbitrary dimensions, and compare it with the rare algebraic\nresults already known at order three. In particular, we examine the cases of\nsymmetric tensors, tensors with symmetric matrix slices, or tensors with free\nentries."
},{
    "category": "cs.NA", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0803.0439v1", 
    "title": "Optimizing polynomials for floating-point implementation", 
    "arxiv-id": "0803.0439v1", 
    "author": "Christoph Quirin Lauter", 
    "publish": "2008-03-04T13:49:44Z", 
    "summary": "The floating-point implementation of a function on an interval often reduces\nto polynomial approximation, the polynomial being typically provided by Remez\nalgorithm. However, the floating-point evaluation of a Remez polynomial\nsometimes leads to catastrophic cancellations. This happens when some of the\npolynomial coefficients are very small in magnitude with respects to others. In\nthis case, it is better to force these coefficients to zero, which also reduces\nthe operation count. This technique, classically used for odd or even\nfunctions, may be generalized to a much larger class of functions. An algorithm\nis presented that forces to zero the smaller coefficients of the initial\npolynomial thanks to a modified Remez algorithm targeting an incomplete\nmonomial basis. One advantage of this technique is that it is purely numerical,\nthe function being used as a numerical black box. This algorithm is implemented\nwithin a larger polynomial implementation tool that is demonstrated on a range\nof examples, resulting in polynomials with less coefficients than those\nobtained the usual way."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218301307009014", 
    "link": "http://arxiv.org/pdf/0803.0874v3", 
    "title": "A Method for Solving Cyclic Block Penta-diagonal Systems of Linear   Equations", 
    "arxiv-id": "0803.0874v3", 
    "author": "Milan Batista", 
    "publish": "2008-03-06T18:45:39Z", 
    "summary": "A method for solving cyclic block three-diagonal systems of equations is\ngeneralized for solving a block cyclic penta-diagonal system of equations.\nIntroducing a special form of two new variables the original system is split\ninto three block pentagonal systems, which can be solved by the known methods.\nAs such method belongs to class of direct methods without pivoting.\nImplementation of the algorithm is discussed in some details and the numerical\nexamples are present."
},{
    "category": "quant-ph", 
    "doi": "10.1016/j.cpc.2008.02.019", 
    "link": "http://arxiv.org/pdf/0803.3459v1", 
    "title": "The QWalk Simulator of Quantum Walks", 
    "arxiv-id": "0803.3459v1", 
    "author": "R. Portugal", 
    "publish": "2008-03-24T20:25:49Z", 
    "summary": "Several research groups are giving special attention to quantum walks\nrecently, because this research area have been used with success in the\ndevelopment of new efficient quantum algorithms. A general simulator of quantum\nwalks is very important for the development of this area, since it allows the\nresearchers to focus on the mathematical and physical aspects of the research\ninstead of deviating the efforts to the implementation of specific numerical\nsimulations. In this paper we present QWalk, a quantum walk simulator for one-\nand two-dimensional lattices. Finite two-dimensional lattices with generic\ntopologies can be used. Decoherence can be simulated by performing measurements\nor by breaking links of the lattice. We use examples to explain the usage of\nthe software and to show some recent results of the literature that are easily\nreproduced by the simulator."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jsc.2010.08.012", 
    "link": "http://arxiv.org/pdf/0804.1021v1", 
    "title": "Differentiation of Kaltofen's division-free determinant algorithm", 
    "arxiv-id": "0804.1021v1", 
    "author": "Gilles Villard", 
    "publish": "2008-04-07T12:37:43Z", 
    "summary": "Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix\ndeterminants. The algorithm is based on a baby steps/giant steps construction\nof Krylov subspaces, and computes the determinant as the constant term of a\ncharacteristic polynomial. For matrices over an abstract field and by the\nresults of Baur and Strassen 1983, the determinant algorithm, actually a\nstraight-line program, leads to an algorithm with the same complexity for\ncomputing the adjoint of a matrix [Kaltofen 1992]. However, the latter is\nobtained by the reverse mode of automatic differentiation and somehow is not\n``explicit''. We study this adjoint algorithm, show how it can be implemented\n(without resorting to an automatic transformation), and demonstrate its use on\npolynomial matrices."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jsc.2010.08.012", 
    "link": "http://arxiv.org/pdf/0806.3121v1", 
    "title": "Algorithmic Based Fault Tolerance Applied to High Performance Computing", 
    "arxiv-id": "0806.3121v1", 
    "author": "Julien Langou", 
    "publish": "2008-06-19T02:06:57Z", 
    "summary": "We present a new approach to fault tolerance for High Performance Computing\nsystem. Our approach is based on a careful adaptation of the Algorithmic Based\nFault Tolerance technique (Huang and Abraham, 1984) to the need of parallel\ndistributed computation. We obtain a strongly scalable mechanism for fault\ntolerance. We can also detect and correct errors (bit-flip) on the fly of a\ncomputation. To assess the viability of our approach, we have developed a fault\ntolerant matrix-matrix multiplication subroutine and we propose some models to\npredict its running time. Our parallel fault-tolerant matrix-matrix\nmultiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov)\nand returns a correct result while one process failure has happened. This\nrepresents 65% of the machine peak efficiency and less than 12% overhead with\nrespect to the fastest failure-free implementation. We predict (and have\nobserved) that, as we increase the processor count, the overhead of the fault\ntolerance drops significantly."
},{
    "category": "cs.LO", 
    "doi": "10.1016/j.jsc.2010.08.012", 
    "link": "http://arxiv.org/pdf/0808.0554v1", 
    "title": "Ranking and Unranking of Hereditarily Finite Functions and Permutations", 
    "arxiv-id": "0808.0554v1", 
    "author": "Paul Tarau", 
    "publish": "2008-08-05T05:20:51Z", 
    "summary": "Prolog's ability to return multiple answers on backtracking provides an\nelegant mechanism to derive reversible encodings of combinatorial objects as\nNatural Numbers i.e. {\\em ranking} and {\\em unranking} functions. Starting from\na generalization of Ackerman's encoding of Hereditarily Finite Sets with\nUrelements and a novel tupling/untupling operation, we derive encodings for\nFinite Functions and use them as building blocks for an executable theory of\n{\\em Hereditarily Finite Functions}. The more difficult problem of {\\em\nranking} and {\\em unranking} {\\em Hereditarily Finite Permutations} is then\ntackled using Lehmer codes and factoradics.\n  The paper is organized as a self-contained literate Prolog program available\nat \\url{http://logic.csci.unt.edu/tarau/research/2008/pHFF.zip}"
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jsc.2010.08.012", 
    "link": "http://arxiv.org/pdf/0812.2769v2", 
    "title": "Geometric scaling: a simple preconditioner for certain linear systems   with discontinuous coefficients", 
    "arxiv-id": "0812.2769v2", 
    "author": "Rachel Gordon", 
    "publish": "2008-12-15T11:35:17Z", 
    "summary": "Linear systems with large differences between coefficients (\"discontinuous\ncoefficients\") arise in many cases in which partial differential\nequations(PDEs) model physical phenomena involving heterogeneous media. The\nstandard approach to solving such problems is to use domain decomposition\ntechniques, with domain boundaries conforming to the boundaries between the\ndifferent media. This approach can be difficult to implement when the geometry\nof the domain boundaries is complicated or the grid is unstructured. This work\nexamines the simple preconditioning technique of scaling the equations by\ndividing each equation by the Lp-norm of its coefficients. This preconditioning\nis called geometric scaling (GS). It has long been known that diagonal scaling\ncan be useful in improving convergence, but there is no study on the general\nusefulness of this approach for discontinuous coefficients. GS was tested on\nseveral nonsymmetric linear systems with discontinuous coefficients derived\nfrom convection-diffusion elliptic PDEs with small to moderate convection\nterms. It is shown that GS improved the convergence properties of restarted\nGMRES and Bi-CGSTAB, with and without the ILUT preconditioner. GS was also\nshown to improve the distribution of the eigenvalues by reducing their\nconcentration around the origin very significantly."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jsc.2010.08.012", 
    "link": "http://arxiv.org/pdf/0901.1696v1", 
    "title": "Rectangular Full Packed Format for Cholesky's Algorithm: Factorization,   Solution and Inversion", 
    "arxiv-id": "0901.1696v1", 
    "author": "Julien Langou", 
    "publish": "2009-01-13T01:08:27Z", 
    "summary": "We describe a new data format for storing triangular, symmetric, and\nHermitian matrices called RFPF (Rectangular Full Packed Format). The standard\ntwo dimensional arrays of Fortran and C (also known as full format) that are\nused to represent triangular and symmetric matrices waste nearly half of the\nstorage space but provide high performance via the use of Level 3 BLAS.\nStandard packed format arrays fully utilize storage (array space) but provide\nlow performance as there is no Level 3 packed BLAS. We combine the good\nfeatures of packed and full storage using RFPF to obtain high performance via\nusing Level 3 BLAS as RFPF is a standard full format representation. Also, RFPF\nrequires exactly the same minimal storage as packed format. Each LAPACK full\nand/or packed triangular, symmetric, and Hermitian routine becomes a single new\nRFPF routine based on eight possible data layouts of RFPF. This new RFPF\nroutine usually consists of two calls to the corresponding LAPACK full format\nroutine and two calls to Level 3 BLAS routines. This means {\\it no} new\nsoftware is required. As examples, we present LAPACK routines for Cholesky\nfactorization, Cholesky solution and Cholesky inverse computation in RFPF to\nillustrate this new work and to describe its performance on several commonly\nused computer platforms. Performance of LAPACK full routines using RFPF versus\nLAPACK full routines using standard format for both serial and SMP parallel\nprocessing is about the same while using half the storage. Performance gains\nare roughly one to a factor of 43 for serial and one to a factor of 97 for SMP\nparallel times faster using vendor LAPACK full routines with RFPF than with\nusing vendor and/or reference packed routines."
},{
    "category": "cs.CE", 
    "doi": "10.1103/PhysRevB.79.115112", 
    "link": "http://arxiv.org/pdf/0901.2665v1", 
    "title": "A Density Matrix-based Algorithm for Solving Eigenvalue Problems", 
    "arxiv-id": "0901.2665v1", 
    "author": "Eric Polizzi", 
    "publish": "2009-01-17T23:36:23Z", 
    "summary": "A new numerical algorithm for solving the symmetric eigenvalue problem is\npresented. The technique deviates fundamentally from the traditional Krylov\nsubspace iteration based techniques (Arnoldi and Lanczos algorithms) or other\nDavidson-Jacobi techniques, and takes its inspiration from the contour\nintegration and density matrix representation in quantum mechanics. It will be\nshown that this new algorithm - named FEAST - exhibits high efficiency,\nrobustness, accuracy and scalability on parallel architectures. Examples from\nelectronic structure calculations of Carbon nanotubes (CNT) are presented, and\nnumerical performances and capabilities are discussed."
},{
    "category": "cs.DS", 
    "doi": "10.1103/PhysRevB.79.115112", 
    "link": "http://arxiv.org/pdf/0901.4323v1", 
    "title": "On the bit-complexity of sparse polynomial multiplication", 
    "arxiv-id": "0901.4323v1", 
    "author": "Gr\u00e9goire Lecerf", 
    "publish": "2009-01-26T21:48:35Z", 
    "summary": "In this paper, we present fast algorithms for the product of two multivariate\npolynomials in sparse representation. The bit complexity of our algorithms are\nstudied in detail for various types of coefficients, and we derive new\ncomplexity results for the power series multiplication in many variables. Our\nalgorithms are implemented and freely available within the Mathemagix software.\nWe show that their theoretical costs are well-reflected in practice."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cam.2009.01.016", 
    "link": "http://arxiv.org/pdf/0902.1040v2", 
    "title": "Fast solving of Weighted Pairing Least-Squares systems", 
    "arxiv-id": "0902.1040v2", 
    "author": "Pierre Courrieu", 
    "publish": "2009-02-06T09:51:09Z", 
    "summary": "This paper presents a generalization of the \"weighted least-squares\" (WLS),\nnamed \"weighted pairing least-squares\" (WPLS), which uses a rectangular weight\nmatrix and is suitable for data alignment problems. Two fast solving methods,\nsuitable for solving full rank systems as well as rank deficient systems, are\nstudied. Computational experiments clearly show that the best method, in terms\nof speed, accuracy, and numerical stability, is based on a special {1, 2,\n3}-inverse, whose computation reduces to a very simple generalization of the\nusual \"Cholesky factorization-backward substitution\" method for solving linear\nsystems."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cam.2009.01.016", 
    "link": "http://arxiv.org/pdf/0902.3088v1", 
    "title": "Automatic generation of non-uniform random variates for arbitrary   pointwise computable probability densities by tiling", 
    "arxiv-id": "0902.3088v1", 
    "author": "Guido Germano", 
    "publish": "2009-02-18T10:28:45Z", 
    "summary": "We present a rejection method based on recursive covering of the probability\ndensity function with equal tiles. The concept works for any probability\ndensity function that is pointwise computable or representable by tabular data.\nBy the implicit construction of piecewise constant majorizing and minorizing\nfunctions that are arbitrarily close to the density function the production of\nrandom variates is arbitrarily independent of the computation of the density\nfunction and extremely fast. The method works unattended for probability\ndensities with discontinuities (jumps and poles). The setup time is short,\nmarginally independent of the shape of the probability density and linear in\ntable size. Recently formulated requirements to a general and automatic\nnon-uniform random number generator are topped. We give benchmarks together\nwith a similar rejection method and with a transformation method."
},{
    "category": "cs.MS", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0902.3207v1", 
    "title": "Random numbers from the tails of probability distributions using the   transformation method", 
    "arxiv-id": "0902.3207v1", 
    "author": "Guido Germano", 
    "publish": "2009-02-18T17:53:36Z", 
    "summary": "The speed of many one-line transformation methods for the production of, for\nexample, Levy alpha-stable random numbers, which generalize Gaussian ones, and\nMittag-Leffler random numbers, which generalize exponential ones, is very high\nand satisfactory for most purposes. However, for the class of decreasing\nprobability densities fast rejection implementations like the Ziggurat by\nMarsaglia and Tsang promise a significant speed-up if it is possible to\ncomplement them with a method that samples the tails of the infinite support.\nThis requires the fast generation of random numbers greater or smaller than a\ncertain value. We present a method to achieve this, and also to generate random\nnumbers within any arbitrary interval. We demonstrate the method showing the\nproperties of the transform maps of the above mentioned distributions as\nexamples of stable and geometric stable random numbers used for the stochastic\nsolution of the space-time fractional diffusion equation."
},{
    "category": "cs.SC", 
    "doi": "10.2478/s13540-013-0021-z", 
    "link": "http://arxiv.org/pdf/0906.3065v1", 
    "title": "Real Solution Isolation with Multiplicity of Zero-Dimensional Triangular   Systems", 
    "arxiv-id": "0906.3065v1", 
    "author": "Bican Xia", 
    "publish": "2009-06-17T03:29:37Z", 
    "summary": "Existing algorithms for isolating real solutions of zero-dimensional\npolynomial systems do not compute the multiplicities of the solutions. In this\npaper, we define in a natural way the multiplicity of solutions of\nzero-dimensional triangular polynomial systems and prove that our definition is\nequivalent to the classical definition of local (intersection) multiplicity.\nThen we present an effective and complete algorithm for isolating real\nsolutions with multiplicities of zero-dimensional triangular polynomial systems\nusing our definition. The algorithm is based on interval arithmetic and\nsquare-free factorization of polynomials with real algebraic coefficients. The\ncomputational results on some examples from the literature are presented."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-642-04103-7_12", 
    "link": "http://arxiv.org/pdf/0906.4121v1", 
    "title": "On computing the Hermite form of a matrix of differential polynomials", 
    "arxiv-id": "0906.4121v1", 
    "author": "Myung Sub Kim", 
    "publish": "2009-06-22T20:29:09Z", 
    "summary": "Given an n x n matrix over the ring of differential polynomials\nF(t)[\\D;\\delta], we show how to compute the Hermite form H of A, and a\nunimodular matrix U such that UA=H. The algorithm requires a polynomial number\nof operations in terms of n, deg_D(A), and deg_t(A). When F is the field of\nrational numbers, it also requires time polynomial in the bit-length of the\ncoefficients."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-04103-7_12", 
    "link": "http://arxiv.org/pdf/0907.0792v1", 
    "title": "A generalized inner and outer product of arbitrary multi-dimensional   arrays using A Mathematics of Arrays (MoA)", 
    "arxiv-id": "0907.0792v1", 
    "author": "Lenore M. Mullin", 
    "publish": "2009-07-04T20:17:28Z", 
    "summary": "An algorithm has been devised to compute the inner and outer product between\ntwo arbitrary multi-dimensional arrays A and B in a single piece of code. It\nwas derived using A Mathematics of Arrays (MoA) and the $\\psi$-calculus.\nExtensive tests of the new algorithm are presented for running in sequential as\nwell as OpenMP multiple processor modes."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-04103-7_12", 
    "link": "http://arxiv.org/pdf/0907.0796v1", 
    "title": "Tensors and n-d Arrays:A Mathematics of Arrays (MoA), psi-Calculus and   the Composition of Tensor and Array Operations", 
    "arxiv-id": "0907.0796v1", 
    "author": "James E. Raynolds", 
    "publish": "2009-07-04T20:38:39Z", 
    "summary": "The Kronecker product is a key algorithm and is ubiquitous across the\nphysical, biological, and computation social sciences. Thus considerations of\noptimal implementation are important. The need to have high performance and\ncomputational reproducibility is paramount. Moreover, due to the need to\ncompose multiple Kronecker products, issues related to data structures, layout\nand indexing algebra require a new look at an old problem. This paper discusses\nthe outer product/tensor product and a special case of the tensor product: the\nKronecker product, along with optimal implementation when composed, and mapped\nto complex processor/memory hierarchies. We discuss how the use of ``A\nMathematics of Arrays\" (MoA), and the psi-Calculus, (a calculus of indexing\nwith shapes), provides optimal, verifiable, reproducible, scalable, and\nportable implementations of both hardware and software."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-04103-7_12", 
    "link": "http://arxiv.org/pdf/0908.3091v2", 
    "title": "Computational Understanding and Manipulation of Symmetries", 
    "arxiv-id": "0908.3091v2", 
    "author": "Chrystopher L. Nehaniv", 
    "publish": "2009-08-21T10:37:52Z", 
    "summary": "For natural and artificial systems with some symmetry structure,\ncomputational understanding and manipulation can be achieved without learning\nby exploiting the algebraic structure. Here we describe this algebraic\ncoordinatization method and apply it to permutation puzzles. Coordinatization\nyields a structural understanding, not just solutions for the puzzles."
},{
    "category": "cs.CE", 
    "doi": "10.3233/SPR-2009-0249", 
    "link": "http://arxiv.org/pdf/0908.4427v1", 
    "title": "Mesh Algorithms for PDE with Sieve I: Mesh Distribution", 
    "arxiv-id": "0908.4427v1", 
    "author": "Dmitry A. Karpeev", 
    "publish": "2009-08-30T21:53:01Z", 
    "summary": "We have developed a new programming framework, called Sieve, to support\nparallel numerical PDE algorithms operating over distributed meshes. We have\nalso developed a reference implementation of Sieve in C++ as a library of\ngeneric algorithms operating on distributed containers conforming to the Sieve\ninterface. Sieve makes instances of the incidence relation, or \\emph{arrows},\nthe conceptual first-class objects represented in the containers. Further,\ngeneric algorithms acting on this arrow container are systematically used to\nprovide natural geometric operations on the topology and also, through duality,\non the data. Finally, coverings and duality are used to encode not only\nindividual meshes, but all types of hierarchies underlying PDE data structures,\nincluding multigrid and mesh partitions.\n  In order to demonstrate the usefulness of the framework, we show how the mesh\npartition data can be represented and manipulated using the same fundamental\nmechanisms used to represent meshes. We present the complete description of an\nalgorithm to encode a mesh partition and then distribute a mesh, which is\nindependent of the mesh dimension, element shape, or embedding. Moreover, data\nassociated with the mesh can be similarly distributed with exactly the same\nalgorithm. The use of a high level of abstraction within the Sieve leads to\nseveral benefits in terms of code reuse, simplicity, and extensibility. We\ndiscuss these benefits and compare our approach to other existing mesh\nlibraries."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0909.3248v1", 
    "title": "Topology of 2D and 3D Rational Curves", 
    "arxiv-id": "0909.3248v1", 
    "author": "Gema Maria Diaz-Toca", 
    "publish": "2009-09-17T14:58:09Z", 
    "summary": "In this paper we present algorithms for computing the topology of planar and\nspace rational curves defined by a parametrization. The algorithms given here\nwork directly with the parametrization of the curve, and do not require to\ncompute or use the implicit equation of the curve (in the case of planar\ncurves) or of any projection (in the case of space curves). Moreover, these\nalgorithms have been implemented in Maple; the examples considered and the\ntimings obtained show good performance skills."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0909.4888v1", 
    "title": "Approximating Mathematical Semantic Web Services Using Approximation   Formulas and Numerical Methods", 
    "arxiv-id": "0909.4888v1", 
    "author": "Mugurel Ionut Andreica", 
    "publish": "2009-09-26T18:52:59Z", 
    "summary": "Mathematical semantic web services are very useful in practice, but only a\nsmall number of research results are reported in this area. In this paper we\npresent a method of obtaining an approximation of a mathematical semantic web\nservice, from its semantic description, using existing mathematical semantic\nweb services, approximation formulas, and numerical methods techniques. We also\ngive a method for automatic comparison of two complexity functions. In\naddition, we present a method for classifying the numerical methods\nmathematical semantic web services from a library."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0909.4955v2", 
    "title": "On the Different Shapes Arising in a Family of Rational Curves Depending   on a Parameter", 
    "arxiv-id": "0909.4955v2", 
    "author": "Juan Gerardo Alcazar", 
    "publish": "2009-09-27T18:47:17Z", 
    "summary": "Given a family of rational curves depending on a real parameter, defined by\nits parametric equations, we provide an algorithm to compute a finite partition\nof the parameter space (${\\Bbb R}$, in general) so that the shape of the family\nstays invariant along each element of the partition. So, from this partition\nthe topology types in the family can be determined. The algorithm is based on a\ngeometric interpretation of previous work (\\cite{JGRS}) for the implicit case.\nHowever, in our case the algorithm works directly with the parametrization of\nthe family, and the implicit equation does not need to be computed. Timings\ncomparing the algorithm in the implicit and the parametric cases are given;\nthese timings show that the parametric algorithm developed here provides in\ngeneral better results than the known algorithm for the implicit case."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0909.4956v1", 
    "title": "Local Shape of Generalized Offsets to Algebraic Curves", 
    "arxiv-id": "0909.4956v1", 
    "author": "Juan Gerardo Alcazar", 
    "publish": "2009-09-27T19:38:00Z", 
    "summary": "In this paper we study the local behavior of an algebraic curve under a\ngeometric construction which is a variation of the usual offsetting\nconstruction, namely the {\\it generalized} offsetting process (\\cite {SS99}).\nMore precisely, here we discuss when and how this geometric construction may\ncause local changes in the shape of an algebraic curve, and we compare our\nresults with those obtained for the case of classical offsets (\\cite{JGS07}).\nFor these purposes, we use well-known notions of Differential Geometry, and\nalso the notion of {\\it local shape} introduced in \\cite{JGS07}."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0910.1923v1", 
    "title": "A Branch and Cut Algorithm for the Halfspace Depth Problem", 
    "arxiv-id": "0910.1923v1", 
    "author": "Dan Chen", 
    "publish": "2009-10-10T14:42:28Z", 
    "summary": "The concept of \\emph{data depth} in non-parametric multivariate descriptive\nstatistics is the generalization of the univariate rank method to multivariate\ndata. \\emph{Halfspace depth} is a measure of data depth. Given a set $S$ of\npoints and a point $p$, the halfspace depth (or rank) of $p$ is defined as the\nminimum number of points of $S$ contained in any closed halfspace with $p$ on\nits boundary. Computing halfspace depth is NP-hard, and it is equivalent to the\nMaximum Feasible Subsystem problem. In this paper a mixed integer program is\nformulated with the big-$M$ method for the halfspace depth problem. We suggest\na branch and cut algorithm for these integer programs. In this algorithm,\nChinneck's heuristic algorithm is used to find an upper bound and a related\ntechnique based on sensitivity analysis is used for branching. Irreducible\nInfeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a\nbinary search algorithm which may be more numerically stable. The algorithms\nare implemented with the BCP framework from the \\textbf{COIN-OR} project."
},{
    "category": "cs.CR", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0910.5370v1", 
    "title": "Isogenies of Elliptic Curves: A Computational Approach", 
    "arxiv-id": "0910.5370v1", 
    "author": "Daniel Shumow", 
    "publish": "2009-10-28T01:48:42Z", 
    "summary": "Isogenies, the mappings of elliptic curves, have become a useful tool in\ncryptology. These mathematical objects have been proposed for use in computing\npairings, constructing hash functions and random number generators, and\nanalyzing the reducibility of the elliptic curve discrete logarithm problem.\nWith such diverse uses, understanding these objects is important for anyone\ninterested in the field of elliptic curve cryptography. This paper, targeted at\nan audience with a knowledge of the basic theory of elliptic curves, provides\nan introduction to the necessary theoretical background for understanding what\nisogenies are and their basic properties. This theoretical background is used\nto explain some of the basic computational tasks associated with isogenies.\nHerein, algorithms for computing isogenies are collected and presented with\nproofs of correctness and complexity analyses. As opposed to the complex\nanalytic approach provided in most texts on the subject, the proofs in this\npaper are primarily algebraic in nature. This provides alternate explanations\nthat some with a more concrete or computational bias may find more clear."
},{
    "category": "math.AG", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/0911.1783v2", 
    "title": "Numerical Algebraic Geometry for Macaulay2", 
    "arxiv-id": "0911.1783v2", 
    "author": "Anton Leykin", 
    "publish": "2009-11-09T21:17:43Z", 
    "summary": "Numerical Algebraic Geometry uses numerical data to describe algebraic\nvarieties. It is based on the methods of numerical polynomial homotopy\ncontinuation, an alternative to the classical symbolic approaches of\ncomputational algebraic geometry. We present a package, the driving idea behind\nwhich is to interlink the existing symbolic methods of Macaulay2 and the\npowerful engine of numerical approximate computations. The core procedures of\nthe package exhibit performance competitive with the other homotopy\ncontinuation software."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1002.2594v1", 
    "title": "Fast Arithmetics in Artin-Schreier Towers over Finite Fields", 
    "arxiv-id": "1002.2594v1", 
    "author": "\u00c9ric Schost", 
    "publish": "2010-02-12T16:44:18Z", 
    "summary": "An Artin-Schreier tower over the finite field F_p is a tower of field\nextensions generated by polynomials of the form X^p - X - a. Following Cantor\nand Couveignes, we give algorithms with quasi-linear time complexity for\narithmetic operations in such towers. As an application, we present an\nimplementation of Couveignes' algorithm for computing isogenies between\nelliptic curves using the p-torsion."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1002.2654v1", 
    "title": "Assessment Of The Wind Farm Impact On The Radar", 
    "arxiv-id": "1002.2654v1", 
    "author": "Evgeny D. Norman", 
    "publish": "2010-02-13T01:19:53Z", 
    "summary": "This study shows the means to evaluate the wind farm impact on the radar. It\nproposes the set of tools, which can be used to realise this objective. The big\npart of report covers the study of complex pattern propagation factor as the\ncritical issue of the Advanced Propagation Model (APM). Finally, the reader can\nfind here the implementation of this algorithm - the real scenario in Inverness\nairport (the United Kingdom), where the ATC radar STAR 2000, developed by\nThales Air Systems, operates in the presence of several wind farms. Basically,\nthe project is based on terms of the department \"Strategy Technology &\nInnovation\", where it has been done. Also you can find here how the radar\nindustry can act with the problem engendered by wind farms. The current\nstrategies in this area are presented, such as a wind turbine production,\nimprovements of air traffic handling procedures and the collaboration between\ndevelopers of radars and wind turbines. The possible strategy for Thales as a\nmain pioneer was given as well."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1002.3180v1", 
    "title": "Factorization of Non-Commutative Polynomials", 
    "arxiv-id": "1002.3180v1", 
    "author": "Fabrizio Caruso", 
    "publish": "2010-02-16T22:17:56Z", 
    "summary": "We describe an algorithm for the factorization of non-commutative polynomials\nover a field. The first sketch of this algorithm appeared in an unpublished\nmanuscript (literally hand written notes) by James H. Davenport more than 20\nyears ago. This version of the algorithm contains some improvements with\nrespect to the original sketch. An improved version of the algorithm has been\nfully implemented in the Axiom computer algebra system."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1002.4057v1", 
    "title": "Towards an Efficient Tile Matrix Inversion of Symmetric Positive   Definite Matrices on Multicore Architectures", 
    "arxiv-id": "1002.4057v1", 
    "author": "Lee Rosenberg", 
    "publish": "2010-02-22T06:11:41Z", 
    "summary": "The algorithms in the current sequential numerical linear algebra libraries\n(e.g. LAPACK) do not parallelize well on multicore architectures. A new family\nof algorithms, the tile algorithms, has recently been introduced. Previous\nresearch has shown that it is possible to write efficient and scalable tile\nalgorithms for performing a Cholesky factorization, a (pseudo) LU\nfactorization, and a QR factorization. In this extended abstract, we attack the\nproblem of the computation of the inverse of a symmetric positive definite\nmatrix. We observe that, using a dynamic task scheduler, it is relatively\npainless to translate existing LAPACK code to obtain a ready-to-be-executed\ntile algorithm. However we demonstrate that non trivial compiler techniques\n(array renaming, loop reversal and pipelining) need then to be applied to\nfurther increase the parallelism of our application. We present preliminary\nexperimental results."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1002.4725v1", 
    "title": "Transferring a symbolic polynomial expression from \\emph{Mathematica} to   \\emph{Matlab}", 
    "arxiv-id": "1002.4725v1", 
    "author": "A. Bret", 
    "publish": "2010-02-25T15:42:39Z", 
    "summary": "A \\emph{Mathematica} Notebook is presented which allows for the transfer or\nany kind of polynomial expression to \\emph{Matlab}. The output is formatted in\nsuch a way that \\emph{Matlab} routines such as \"Root\" can be readily\nimplemented. Once the Notebook has been executed, only one copy-paste operation\nin necessary."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1003.3689v2", 
    "title": "A Highly Efficient Parallel Algorithm for Computing the Fiedler Vector", 
    "arxiv-id": "1003.3689v2", 
    "author": "Murat Manguoglu", 
    "publish": "2010-03-18T22:56:57Z", 
    "summary": "This paper has been withdrawn by the author."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1005.0080v1", 
    "title": "Electronic Geometry Textbook: A Geometric Textbook Knowledge Management   System", 
    "arxiv-id": "1005.0080v1", 
    "author": "Xiaoyu Chen", 
    "publish": "2010-05-01T15:06:32Z", 
    "summary": "Electronic Geometry Textbook is a knowledge management system that manages\ngeometric textbook knowledge to enable users to construct and share dynamic\ngeometry textbooks interactively and efficiently. Based on a knowledge base\norganizing and storing the knowledge represented in specific languages, the\nsystem implements interfaces for maintaining the data representing that\nknowledge as well as relations among those data, for automatically generating\nreadable documents for viewing or printing, and for automatically discovering\nthe relations among knowledge data. An interface has been developed for users\nto create geometry textbooks with automatic checking, in real time, of the\nconsistency of the structure of each resulting textbook. By integrating an\nexternal geometric theorem prover and an external dynamic geometry software\npackage, the system offers the facilities for automatically proving theorems\nand generating dynamic figures in the created textbooks. This paper provides a\ncomprehensive account of the current version of Electronic Geometry Textbook."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1005.2314v1", 
    "title": "Some comments on C. S. Wallace's random number generators", 
    "arxiv-id": "1005.2314v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-05-13T13:22:04Z", 
    "summary": "We outline some of Chris Wallace's contributions to pseudo-random number\ngeneration. In particular, we consider his idea for generating normally\ndistributed variates without relying on a source of uniform random numbers, and\ncompare it with more conventional methods for generating normal random numbers.\nImplementations of Wallace's idea can be very fast (approximately as fast as\ngood uniform generators). We discuss the statistical quality of the output, and\nmention how certain pitfalls can be avoided."
},{
    "category": "math.NT", 
    "doi": "10.1016/j.cagd.2010.07.001", 
    "link": "http://arxiv.org/pdf/1005.3153v2", 
    "title": "NZMATH 1.0", 
    "arxiv-id": "1005.3153v2", 
    "author": "Shigenori Uchiyama", 
    "publish": "2010-05-18T10:42:40Z", 
    "summary": "This is an announcement of the first official release ver.1.0 of a Python\nsystem NZMATH for number theory. We overview all functions in NZMATH 1.0, show\nmain properties after former report on NZMATH 0.5.0, and describe new features\nfor stable development. The most important point of the release is that we can\nnow treat number fields. The second big change is that new type of polynomial\nprograms are provided. Elliptic curve primality proving and its related\nprograms are also available, where we partly use a library outside NZMATH as an\nadvantage of writing the system only by Python. On method of development, a new\nfeature is that NZMATH is registered on SourceForge as an open source project\nto keep continuous development of the project. This is a unique attempt among\nexisting systems for number theory."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.24.15", 
    "link": "http://arxiv.org/pdf/1006.0401v1", 
    "title": "Making big steps in trajectories", 
    "arxiv-id": "1006.0401v1", 
    "author": "Margarita Korovina", 
    "publish": "2010-06-02T14:30:40Z", 
    "summary": "We consider the solution of initial value problems within the context of\nhybrid systems and emphasise the use of high precision approximations (in\nsoftware for exact real arithmetic). We propose a novel algorithm for the\ncomputation of trajectories up to the area where discontinuous jumps appear,\napplicable for holomorphic flow functions. Examples with a prototypical\nimplementation illustrate that the algorithm might provide results with higher\nprecision than well-known ODE solvers at a similar computation time."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.ijheatmasstransfer.2011.12.029", 
    "link": "http://arxiv.org/pdf/1006.0758v2", 
    "title": "LSMR: An iterative algorithm for sparse least-squares problems", 
    "arxiv-id": "1006.0758v2", 
    "author": "Michael Saunders", 
    "publish": "2010-06-04T00:16:09Z", 
    "summary": "An iterative method LSMR is presented for solving linear systems $Ax=b$ and\nleast-squares problem $\\min \\norm{Ax-b}_2$, with $A$ being sparse or a fast\nlinear operator. LSMR is based on the Golub-Kahan bidiagonalization process. It\nis analytically equivalent to the MINRES method applied to the normal equation\n$A\\T Ax = A\\T b$, so that the quantities $\\norm{A\\T r_k}$ are monotonically\ndecreasing (where $r_k = b - Ax_k$ is the residual for the current iterate\n$x_k$). In practice we observe that $\\norm{r_k}$ also decreases monotonically.\nCompared to LSQR, for which only $\\norm{r_k}$ is monotonic, it is safer to\nterminate LSMR early. Improvements for the new iterative method in the presence\nof extra available memory are also explored."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.ijheatmasstransfer.2011.12.029", 
    "link": "http://arxiv.org/pdf/1006.4057v1", 
    "title": "Towards OpenMath Content Dictionaries as Linked Data", 
    "arxiv-id": "1006.4057v1", 
    "author": "Christoph Lange", 
    "publish": "2010-06-21T13:00:22Z", 
    "summary": "\"The term 'Linked Data' refers to a set of best practices for publishing and\nconnecting structured data on the web\". Linked Data make the Semantic Web work\npractically, which means that information can be retrieved without complicated\nlookup mechanisms, that a lightweight semantics enables scalable reasoning, and\nthat the decentral nature of the Web is respected. OpenMath Content\nDictionaries (CDs) have the same characteristics - in principle, but not yet in\npractice. The Linking Open Data movement has made a considerable practical\nimpact: Governments, broadcasting stations, scientific publishers, and many\nmore actors are already contributing to the \"Web of Data\". Queries can be\nanswered in a distributed way, and services aggregating data from different\nsources are replacing hard-coded mashups. However, these services are currently\nentirely lacking mathematical functionality. I will discuss real-world\nscenarios, where today's RDF-based Linked Data do not quite get their job done,\nbut where an integration of OpenMath would help - were it not for certain\nconceptual and practical restrictions. I will point out conceptual shortcomings\nin the OpenMath 2 specification and common bad practices in publishing CDs and\nthen propose concrete steps to overcome them and to contribute OpenMath CDs to\nthe Web of Data."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cam.2011.07.017", 
    "link": "http://arxiv.org/pdf/1008.1700v2", 
    "title": "A domain decomposing parallel sparse linear system solver", 
    "arxiv-id": "1008.1700v2", 
    "author": "Murat Manguoglu", 
    "publish": "2010-08-10T12:14:09Z", 
    "summary": "The solution of large sparse linear systems is often the most time-consuming\npart of many science and engineering applications. Computational fluid\ndynamics, circuit simulation, power network analysis, and material science are\njust a few examples of the application areas in which large sparse linear\nsystems need to be solved effectively. In this paper we introduce a new\nparallel hybrid sparse linear system solver for distributed memory\narchitectures that contains both direct and iterative components. We show that\nby using our solver one can alleviate the drawbacks of direct and iterative\nsolvers, achieving better scalability than with direct solvers and more\nrobustness than with classical preconditioned iterative solvers. Comparisons to\nwell-known direct and iterative solvers on a parallel architecture are\nprovided."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1008.4874v3", 
    "title": "Applying dissipative dynamical systems to pseudorandom number   generation: Equidistribution property and statistical independence of bits at   distances up to logarithm of mesh size", 
    "arxiv-id": "1008.4874v3", 
    "author": "L. Yu. Barash", 
    "publish": "2010-08-28T15:26:26Z", 
    "summary": "The behavior of a family of dissipative dynamical systems representing\ntransformations of two-dimensional torus is studied on a discrete lattice and\ncompared with that of conservative hyperbolic automorphisms of the torus.\nApplying dissipative dynamical systems to generation of pseudorandom numbers is\nshown to be advantageous and equidistribution of probabilities for the\nsequences of bits can be achieved. A new algorithm for generating uniform\npseudorandom numbers is proposed. The theory of the generator, which includes\nproofs of periodic properties and of statistical independence of bits at\ndistances up to logarithm of mesh size, is presented. Extensive statistical\ntesting using available test packages demonstrates excellent results, while the\nspeed of the generator is comparable to other modern generators."
},{
    "category": "cs.CE", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1010.1456v1", 
    "title": "A Hybrid Parallelization of AIM for Multi-Core Clusters: Implementation   Details and Benchmark Results on Ranger", 
    "arxiv-id": "1010.1456v1", 
    "author": "Ali E. Y\u0131lmaz", 
    "publish": "2010-10-07T15:35:24Z", 
    "summary": "This paper presents implementation details and empirical results for a hybrid\nmessage passing and shared memory paralleliziation of the adaptive integral\nmethod (AIM). AIM is implemented on a (near) petaflop supercomputing cluster of\nquad-core processors and its accuracy, complexity, and scalability are\ninvestigated by solving benchmark scattering problems. The timing and speedup\nresults on up to 1024 processors show that the hybrid MPI/OpenMP\nparallelization of AIM exhibits better strong scalability (fixed problem size\nspeedup) than pure MPI parallelization of it when multiple cores are used on\neach processor."
},{
    "category": "hep-lat", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1011.3318v1", 
    "title": "Domain Decomposition method on GPU cluster", 
    "arxiv-id": "1011.3318v1", 
    "author": "Ken-Ichi Ishikawa", 
    "publish": "2010-11-15T09:12:23Z", 
    "summary": "Pallalel GPGPU computing for lattice QCD simulations has a bottleneck on the\nGPU to GPU data communication due to the lack of the direct data exchanging\nfacility. In this work we investigate the performance of quark solver using the\nrestricted additive Schwarz (RAS) preconditioner on a low cost GPU cluster. We\nexpect that the RAS preconditioner with appropriate domaindecomposition and\ntask distribution reduces the communication bottleneck. The GPU cluster we\nconstructed is composed of four PC boxes, two GPU cards are attached to each\nbox, and we have eight GPU cards in total. The compute nodes are connected with\nrather slow but low cost Gigabit-Ethernet. We include the RAS preconditioner in\nthe single-precision part of the mixedprecision nested-BiCGStab algorithm and\nthe single-precision task is distributed to the multiple GPUs. The benchmarking\nis done with the O(a)-improved Wilson quark on a randomly generated gauge\nconfiguration with the size of $32^4$. We observe a factor two improvment on\nthe solver performance with the RAS precoditioner compared to that without the\npreconditioner and find that the improvment mainly comes from the reduction of\nthe communication bottleneck as we expected."
},{
    "category": "cs.SC", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1101.3218v3", 
    "title": "A Symbolic Transformation Language and its Application to a Multiscale   Method", 
    "arxiv-id": "1101.3218v3", 
    "author": "Michel Lenczner", 
    "publish": "2011-01-17T14:06:26Z", 
    "summary": "The context of this work is the design of a software, called MEMSALab,\ndedicated to the automatic derivation of multiscale models of arrays of micro-\nand nanosystems. In this domain a model is a partial differential equation.\nMultiscale methods approximate it by another partial differential equation\nwhich can be numerically simulated in a reasonable time. The challenge consists\nin taking into account a wide range of geometries combining thin and periodic\nstructures with the possibility of multiple nested scales.\n  In this paper we present a transformation language that will make the\ndevelopment of MEMSALab more feasible. It is proposed as a Maple package for\nrule-based programming, rewriting strategies and their combination with\nstandard Maple code. We illustrate the practical interest of this language by\nusing it to encode two examples of multiscale derivations, namely the two-scale\nlimit of the derivative operator and the two-scale model of the stationary heat\nequation."
},{
    "category": "cs.MS", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1101.5151v1", 
    "title": "Simulation of Self-Assembly in the Abstract Tile Assembly Model with ISU   TAS", 
    "arxiv-id": "1101.5151v1", 
    "author": "Matthew J. Patitz", 
    "publish": "2011-01-27T03:16:16Z", 
    "summary": "Since its introduction by Erik Winfree in 1998, the abstract Tile Assembly\nModel (aTAM) has inspired a wealth of research. As an abstract model for tile\nbased self-assembly, it has proven to be remarkably powerful and expressive in\nterms of the structures which can self-assemble within it. As research has\nprogressed in the aTAM, the self-assembling structures being studied have\nbecome progressively more complex. This increasing complexity, along with a\nneed for standardization of definitions and tools among researchers, motivated\nthe development of the Iowa State University Tile Assembly Simulator (ISU TAS).\nISU TAS is a graphical simulator and tile set editor for designing and building\n2-D and 3-D aTAM tile assembly systems and simulating their self-assembly. This\npaper reviews the features and functionality of ISU TAS and describes how it\ncan be used to further research into the complexities of the aTAM. Software and\nsource code are available at http://www.cs.iastate.edu/~lnsa."
},{
    "category": "cs.MS", 
    "doi": "10.1209/0295-5075/95/10003", 
    "link": "http://arxiv.org/pdf/1102.3774v1", 
    "title": "Quantum Anticipation Explorer", 
    "arxiv-id": "1102.3774v1", 
    "author": "Hans-Rudolf Thomann", 
    "publish": "2011-02-18T08:00:01Z", 
    "summary": "Quantum anticipation explorer is a computer program allowing the numerical\nexploration of quantum anticipation which has been analyzed in arXiv:0810.183v1\nand arXiv:1003.1090v1 for H-Atom, equidistant, random and custom spectra. This\ntool determines the anticipation strength at those times orthogonal evolution\nis possible. This paper is the user's guide explaining its capabilities,\ninstallation and usage, and documenting the mathematics and algorithms\nimplemented in the software. A zip file containing the setup and documentation\ncan be downloaded from\nhttp://www.thomannconsulting.ch/public/aboutus/aboutus-en.htm free of cost."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2427023.2427027", 
    "link": "http://arxiv.org/pdf/1103.0066v1", 
    "title": "Finite Element Integration on GPUs", 
    "arxiv-id": "1103.0066v1", 
    "author": "Andy R. Terrel", 
    "publish": "2011-03-01T02:05:42Z", 
    "summary": "We present a novel finite element integration method for low order elements\non GPUs. We achieve more than 100GF for element integration on first order\ndiscretizations of both the Laplacian and Elasticity operators."
},{
    "category": "cs.DL", 
    "doi": "10.1145/2427023.2427027", 
    "link": "http://arxiv.org/pdf/1103.1482v1", 
    "title": "The Planetary System: Executable Science, Technology, Engineering and   Math Papers", 
    "arxiv-id": "1103.1482v1", 
    "author": "Vyacheslav Zholudev", 
    "publish": "2011-03-08T10:12:40Z", 
    "summary": "Executable scientific papers contain not just layouted text for reading. They\ncontain, or link to, machine-comprehensible representations of the scientific\nfindings or experiments they describe. Client-side players can thus enable\nreaders to \"check, manipulate and explore the result space\". We have realized\nexecutable papers in the STEM domain with the Planetary system. Semantic\nannotations associate the papers with a content commons holding the background\nontology, the annotations are exposed as Linked Data, and a frontend player\napplication hooks modular interactive services into the semantic annotations."
},{
    "category": "cs.NA", 
    "doi": "10.1145/2427023.2427027", 
    "link": "http://arxiv.org/pdf/1103.2405v1", 
    "title": "Fast Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph   Mining", 
    "arxiv-id": "1103.2405v1", 
    "author": "Ponnuswamy Sadayappan", 
    "publish": "2011-03-12T01:04:56Z", 
    "summary": "Scaling up the sparse matrix-vector multiplication kernel on modern Graphics\nProcessing Units (GPU) has been at the heart of numerous studies in both\nacademia and industry. In this article we present a novel non-parametric,\nself-tunable, approach to data representation for computing this kernel,\nparticularly targeting sparse matrices representing power-law graphs. Using\nreal data, we show how our representation scheme, coupled with a novel tiling\nalgorithm, can yield significant benefits over the current state of the art GPU\nefforts on a number of core data mining algorithms such as PageRank, HITS and\nRandom Walk with Restart."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2427023.2427027", 
    "link": "http://arxiv.org/pdf/1103.2952v1", 
    "title": "Data sets of very large linear feasibility problems solved by projection   methods", 
    "arxiv-id": "1103.2952v1", 
    "author": "Wei Chen", 
    "publish": "2011-03-14T15:20:46Z", 
    "summary": "We give a link to a page on the Web on which we deposited a set of eight huge\nLinear Programming (LP) problems for Intensity-Modulated Proton Therapy (IMPT)\ntreatment planning. These huge LP problems were employed in our recent research\nand we were asked to make them public."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2427023.2427027", 
    "link": "http://arxiv.org/pdf/1103.3020v1", 
    "title": "A study of the existing linear algebra libraries that you can use from   C++ (Une \u00e9tude des biblioth\u00e8ques d'alg\u00e8bre lin\u00e9aire utilisables en   C++)", 
    "arxiv-id": "1103.3020v1", 
    "author": "Claire Mouton", 
    "publish": "2011-03-15T20:27:16Z", 
    "summary": "A study of the existing linear algebra libraries that you can use from C++"
},{
    "category": "cs.MS", 
    "doi": "10.1145/1731022.1731030", 
    "link": "http://arxiv.org/pdf/1103.6248v1", 
    "title": "DOLFIN: Automated Finite Element Computing", 
    "arxiv-id": "1103.6248v1", 
    "author": "Garth N. Wells", 
    "publish": "2011-03-31T17:29:27Z", 
    "summary": "We describe here a library aimed at automating the solution of partial\ndifferential equations using the finite element method. By employing novel\ntechniques for automated code generation, the library combines a high level of\nexpressiveness with efficient computation. Finite element variational forms may\nbe expressed in near mathematical notation, from which low-level code is\nautomatically generated, compiled and seamlessly integrated with efficient\nimplementations of computational meshes and high-performance linear algebra.\nEasy-to-use object-oriented interfaces to the library are provided in the form\nof a C++ library and a Python module. This paper discusses the mathematical\nabstractions and methods used in the design of the library and its\nimplementation. A number of examples are presented to demonstrate the use of\nthe library in application code."
},{
    "category": "cs.AI", 
    "doi": "10.1145/1731022.1731030", 
    "link": "http://arxiv.org/pdf/1105.6314v1", 
    "title": "Activity-Based Search for Black-Box Contraint-Programming Solvers", 
    "arxiv-id": "1105.6314v1", 
    "author": "P. Van Hentenryck", 
    "publish": "2011-05-31T15:25:33Z", 
    "summary": "Robust search procedures are a central component in the design of black-box\nconstraint-programming solvers. This paper proposes activity-based search, the\nidea of using the activity of variables during propagation to guide the search.\nActivity-based search was compared experimentally to impact-based search and\nthe WDEG heuristics. Experimental results on a variety of benchmarks show that\nactivity-based search is more robust than other heuristics and may produce\nsignificant improvements in performance."
},{
    "category": "cs.NA", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.2176v2", 
    "title": "A Tuned and Scalable Fast Multipole Method as a Preeminent Algorithm for   Exascale Systems", 
    "arxiv-id": "1106.2176v2", 
    "author": "Lorena Barba", 
    "publish": "2011-06-10T21:02:40Z", 
    "summary": "Among the algorithms that are likely to play a major role in future exascale\ncomputing, the fast multipole method (FMM) appears as a rising star. Our\nprevious recent work showed scaling of an FMM on GPU clusters, with problem\nsizes in the order of billions of unknowns. That work led to an extremely\nparallel FMM, scaling to thousands of GPUs or tens of thousands of CPUs. This\npaper reports on a a campaign of performance tuning and scalability studies\nusing multi-core CPUs, on the Kraken supercomputer. All kernels in the FMM were\nparallelized using OpenMP, and a test using 10^7 particles randomly distributed\nin a cube showed 78% efficiency on 8 threads. Tuning of the\nparticle-to-particle kernel using SIMD instructions resulted in 4x speed-up of\nthe overall algorithm on single-core tests with 10^3 - 10^7 particles. Parallel\nscalability was studied in both strong and weak scaling. The strong scaling\ntest used 10^8 particles and resulted in 93% parallel efficiency on 2048\nprocesses for the non-SIMD code and 54% for the SIMD-optimized code (which was\nstill 2x faster). The weak scaling test used 10^6 particles per process, and\nresulted in 72% efficiency on 32,768 processes, with the largest calculation\ntaking about 40 seconds to evaluate more than 32 billion unknowns. This work\nbuilds up evidence for our view that FMM is poised to play a leading role in\nexascale computing, and we end the paper with a discussion of the features that\nmake it a particularly favorable algorithm for the emerging heterogeneous and\nmassively parallel architectural landscape."
},{
    "category": "cs.DS", 
    "doi": "10.1177/1094342011429952", 
    "link": "http://arxiv.org/pdf/1106.2263v1", 
    "title": "A Library for Implementing the Multiple Hypothesis Tracking Algorithm", 
    "arxiv-id": "1106.2263v1", 
    "author": "Jos\u00e9 Gaspar", 
    "publish": "2011-06-11T22:32:32Z", 
    "summary": "The Multiple Hypothesis Tracking (MHT) algorithm is known to produce good\nresults in difficult multi-target tracking situations. However, its\nimplementation is not trivial, and is associated with a significant programming\neffort, code size and long implementation time. We propose a library which\naddresses these problems by providing a domain independent implementation of\nthe most complex MHT operations. We also address the problem of applying\nclustering in domain independent manner."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-25379-9_14", 
    "link": "http://arxiv.org/pdf/1106.4448v2", 
    "title": "Tactics for Reasoning modulo AC in Coq", 
    "arxiv-id": "1106.4448v2", 
    "author": "Damien Pous", 
    "publish": "2011-06-22T13:58:58Z", 
    "summary": "We present a set of tools for rewriting modulo associativity and\ncommutativity (AC) in Coq, solving a long-standing practical problem. We use\ntwo building blocks: first, an extensible reflexive decision procedure for\nequality modulo AC; second, an OCaml plug-in for pattern matching modulo AC. We\nhandle associative only operations, neutral elements, uninterpreted function\nsymbols, and user-defined equivalence relations. By relying on type-classes for\nthe reification phase, we can infer these properties automatically, so that\nend-users do not need to specify which operation is A or AC, or which constant\nis a neutral element."
},{
    "category": "cs.DL", 
    "doi": "10.1007/978-3-642-22673-1_10", 
    "link": "http://arxiv.org/pdf/1107.3212v1", 
    "title": "Licensing the Mizar Mathematical Library", 
    "arxiv-id": "1107.3212v1", 
    "author": "Lionel Mamane", 
    "publish": "2011-07-16T09:06:49Z", 
    "summary": "The Mizar Mathematical Library (MML) is a large corpus of formalised\nmathematical knowledge. It has been constructed over the course of many years\nby a large number of authors and maintainers. Yet the legal status of these\nefforts of the Mizar community has never been clarified. In 2010, after many\nyears of loose deliberations, the community decided to investigate the issue of\nlicensing the content of the MML, thereby clarifying and crystallizing the\nstatus of the texts, the text's authors, and the library's long-term\nmaintainers. The community has settled on a copyright and license policy that\nsuits the peculiar features of Mizar and its community. In this paper we\ndiscuss the copyright and license solutions. We offer our experience in the\nhopes that the communities of other libraries of formalised mathematical\nknowledge might take up the legal and scientific problems that we addressed for\nMizar."
},{
    "category": "math.OC", 
    "doi": "10.1007/978-3-642-22673-1_10", 
    "link": "http://arxiv.org/pdf/1108.1548v3", 
    "title": "Some Software Packages for Partial SVD Computation", 
    "arxiv-id": "1108.1548v3", 
    "author": "Zhouchen Lin", 
    "publish": "2011-08-07T15:09:34Z", 
    "summary": "This technical report introduces some software packages for partial SVD\ncomputation, including optimized PROPACK, modified PROPACK for computing\nsingular values above a threshold and the corresponding singular vectors, and\nblock Lanczos with warm start (BLWS). The current version is preliminary. The\ndetails will be enriched soon."
},{
    "category": "cs.NA", 
    "doi": "10.1109/MCSE.2012.1", 
    "link": "http://arxiv.org/pdf/1108.5815v2", 
    "title": "Hierarchical N-body simulations with auto-tuning for heterogeneous   systems", 
    "arxiv-id": "1108.5815v2", 
    "author": "Lorena A. Barba", 
    "publish": "2011-08-30T03:27:14Z", 
    "summary": "With the current hybridization of treecodes and FMMs, combined with\nauto-tuning capabilities on heterogeneous architectures, the flexibility of\nfast N-body methods has been greatly enhanced. These features are a requirement\nto developing a black-box software library for fast N-body algorithms on\nheterogeneous systems, which is our immediate goal."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1063/1.3637933", 
    "link": "http://arxiv.org/pdf/1110.3233v1", 
    "title": "Metaprogramming Applied to Numerical Problems", 
    "arxiv-id": "1110.3233v1", 
    "author": "Karsten Ahnert", 
    "publish": "2011-10-14T15:04:14Z", 
    "summary": "From the discovery that the template system of C++ forms a Turing complete\nlanguage in 1994, a programming technique called Template Metaprogramming has\nemerged that allows for the creation of faster, more generic and better code.\nHere, we apply Template Metaprogramming to implement a generic Runge-Kutta\nscheme that can be used to numerically solve ordinary differential equations.\nWe show that using Template Metaprogramming results in a significantly improved\nperformance compared to a classical implementation."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.70", 
    "link": "http://arxiv.org/pdf/1110.4473v1", 
    "title": "Proceedings 10th International Workshop on the ACL2 Theorem Prover and   its Applications", 
    "arxiv-id": "1110.4473v1", 
    "author": "Julien Schmaltz", 
    "publish": "2011-10-20T08:48:32Z", 
    "summary": "This volume contains the proceedings of ACL2 2011, the International Workshop\non the ACL2 Theorem Prover and its Applications. The workshop was held in\nAustin, Texas, USA, on November 3-4 2011. ACL2 2011 is the tenth in a series of\nworkshops on the ACL2 Theorem Prover and its Applications. The workshop was\nco-located with the eleventh Conference on Formal Methods in Computer Aided\nDesign (FMCAD'11). The ACL2 Workshop series provide a major technical forum for\nresearchers to present and discuss improvements and extensions to the theorem\nprover, comparisons of ACL2 with other systems, and applications of ACL2 in\nformal verification or formalized mathematics. Workshops have been held at\napproxiamately 18 month intervals since 1999. ACL2 is the most recent\nincarnation of the Boyer-Moore family of theorem provers, for which, Robert\nBoyer, Matt Kaufmann and J Strother Moore received the 2005 ACM Software System\nAward. It is state-of-the-art automated reasoning system that has been\nsuccessfully used in academia, government and industry for specification and\nverification of computing systems. More details can be found in the proceedings\nand on the workshop web page (www.cs.ru.nl/~julien/acl2-11/)."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1110.5765v1", 
    "title": "Throughput-Distortion Computation Of Generic Matrix Multiplication:   Toward A Computation Channel For Digital Signal Processing Systems", 
    "arxiv-id": "1110.5765v1", 
    "author": "Yiannis Andreopoulos", 
    "publish": "2011-10-26T11:17:21Z", 
    "summary": "The generic matrix multiply (GEMM) function is the core element of\nhigh-performance linear algebra libraries used in many\ncomputationally-demanding digital signal processing (DSP) systems. We propose\nan acceleration technique for GEMM based on dynamically adjusting the\nimprecision (distortion) of computation. Our technique employs adaptive scalar\ncompanding and rounding to input matrix blocks followed by two forms of packing\nin floating-point that allow for concurrent calculation of multiple results.\nSince the adaptive companding process controls the increase of concurrency (via\npacking), the increase in processing throughput (and the corresponding increase\nin distortion) depends on the input data statistics. To demonstrate this, we\nderive the optimal throughput-distortion control framework for GEMM for the\nbroad class of zero-mean, independent identically distributed, input sources.\nOur approach converts matrix multiplication in programmable processors into a\ncomputation channel: when increasing the processing throughput, the output\nnoise (error) increases due to (i) coarser quantization and (ii) computational\nerrors caused by exceeding the machine-precision limitations. We show that,\nunder certain distortion in the GEMM computation, the proposed framework can\nsignificantly surpass 100% of the peak performance of a given processor. The\npractical benefits of our proposal are shown in a face recognition system and a\nmulti-layer perceptron system trained for metadata learning from a large music\nfeature database."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1111.3124v1", 
    "title": "A multiprecision matrix calculation library and its extension library   for a matrix-product-state simulation of quantum computing", 
    "arxiv-id": "1111.3124v1", 
    "author": "Akira SaiToh", 
    "publish": "2011-11-14T07:50:31Z", 
    "summary": "A C++ library, named ZKCM, has been developed for the purpose of\nmultiprecision matrix calculations, which is based on the GNU MP and MPFR\nlibraries. It is especially convenient for writing programs involving\ntensor-product operations, tracing-out operations, and singular-value\ndecompositions. Its extension library, ZKCM_QC, for simulating quantum\ncomputing has been developed using the time-dependent matrix-product-state\nsimulation method. This report gives a brief introduction to the libraries with\nsample programs."
},{
    "category": "cs.PL", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1111.3606v3", 
    "title": "tym: Typed Matlab", 
    "arxiv-id": "1111.3606v3", 
    "author": "Hamid A. Toussi", 
    "publish": "2011-11-15T18:32:29Z", 
    "summary": "Although, many scientists and engineers use Octave or MATLAB as their\npreferred programming language, dynamic nature of these languages can lead to\nslower running-time of programs written in these languages compared to programs\nwritten in languages which are not as dynamic, like C, C++ and Fortran. In this\nwork we developed a translator for a new programming language (tym) which tries\nto address performance issues, common in scientific programs, by adding new\nconstructs to a subset of Octave/MATLAB language. Our translator compiles\nprograms written in tym, to efficient C++ code."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1111.6549v1", 
    "title": "Efficient Dense Gaussian Elimination over the Finite Field with Two   Elements", 
    "arxiv-id": "1111.6549v1", 
    "author": "Cl\u00e9ment Pernet", 
    "publish": "2011-11-28T18:55:12Z", 
    "summary": "In this work we describe an efficient implementation of a hierarchy of\nalgorithms for Gaussian elimination upon dense matrices over the field with two\nelements. We discuss both well-known and new algorithms as well as our\nimplementations in the M4RI library, which has been adopted into Sage. The\nfocus of our discussion is a block iterative algorithm for PLE decomposition\nwhich is inspired by the M4RI algorithm. The implementation presented in this\nwork provides considerable performance gains in practice when compared to the\npreviously fastest implementation. We provide performance figures on x86_64\nCPUs to demonstrate the alacrity of our approach."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1112.5717v2", 
    "title": "Rank-profile revealing Gaussian elimination and the CUP matrix   decomposition", 
    "arxiv-id": "1112.5717v2", 
    "author": "Arne Storjohann", 
    "publish": "2011-12-24T11:30:09Z", 
    "summary": "Transforming a matrix over a field to echelon form, or decomposing the matrix\nas a product of structured matrices that reveal the rank profile, is a\nfundamental building block of computational exact linear algebra. This paper\nsurveys the well known variations of such decompositions and transformations\nthat have been proposed in the literature. We present an algorithm to compute\nthe CUP decomposition of a matrix, adapted from the LSP algorithm of Ibarra,\nMoran and Hui (1982), and show reductions from the other most common Gaussian\nelimination based matrix transformations and decompositions to the CUP\ndecomposition. We discuss the advantages of the CUP algorithm over other\nexisting algorithms by studying time and space complexities: the asymptotic\ntime complexity is rank sensitive, and comparing the constants of the leading\nterms, the algorithms for computing matrix invariants based on the CUP\ndecomposition are always at least as good except in one case. We also show that\nthe CUP algorithm, as well as the computation of other invariants such as\ntransformation to reduced column echelon form using the CUP algorithm, all work\nin place, allowing for example to compute the inverse of a matrix on the same\nstorage as the input matrix."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1201.0490v3", 
    "title": "Scikit-learn: Machine Learning in Python", 
    "arxiv-id": "1201.0490v3", 
    "author": "\u00c9douard Duchesnay", 
    "publish": "2012-01-02T16:42:40Z", 
    "summary": "Scikit-learn is a Python module integrating a wide range of state-of-the-art\nmachine learning algorithms for medium-scale supervised and unsupervised\nproblems. This package focuses on bringing machine learning to non-specialists\nusing a general-purpose high-level language. Emphasis is put on ease of use,\nperformance, documentation, and API consistency. It has minimal dependencies\nand is distributed under the simplified BSD license, encouraging its use in\nboth academic and commercial settings. Source code, binaries, and documentation\ncan be downloaded from http://scikit-learn.sourceforge.net."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1201.0499v1", 
    "title": "Evaluating polynomials in several variables and their derivatives on a   GPU computing processor", 
    "arxiv-id": "1201.0499v1", 
    "author": "Genady Yoffe", 
    "publish": "2012-01-02T17:10:49Z", 
    "summary": "In order to obtain more accurate solutions of polynomial systems with\nnumerical continuation methods we use multiprecision arithmetic. Our goal is to\noffset the overhead of double double arithmetic accelerating the path trackers\nand in particular Newton's method with a general purpose graphics processing\nunit. In this paper we describe algorithms for the massively parallel\nevaluation and differentiation of sparse polynomials in several variables. We\nreport on our implementation of the algorithmic differentiation of products of\nvariables on the NVIDIA Tesla C2050 Computing Processor using the NVIDIA CUDA\ncompiler tools."
},{
    "category": "cs.DM", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1202.1055v1", 
    "title": "The Optimal Uncertainty Algorithm in the Mystic Framework", 
    "arxiv-id": "1202.1055v1", 
    "author": "M. Ortiz", 
    "publish": "2012-02-06T06:45:53Z", 
    "summary": "We have recently proposed a rigorous framework for Uncertainty Quantification\n(UQ) in which UQ objectives and assumption/information set are brought into the\nforefront, providing a framework for the communication and comparison of UQ\nresults. In particular, this framework does not implicitly impose inappropriate\nassumptions nor does it repudiate relevant information. This framework, which\nwe call Optimal Uncertainty Quantification (OUQ), is based on the observation\nthat given a set of assumptions and information, there exist bounds on\nuncertainties obtained as values of optimization problems and that these bounds\nare optimal. It provides a uniform environment for the optimal solution of the\nproblems of validation, certification, experimental design, reduced order\nmodeling, prediction, extrapolation, all under aleatoric and epistemic\nuncertainties. OUQ optimization problems are extremely large, and even though\nunder general conditions they have finite-dimensional reductions, they must\noften be solved numerically. This general algorithmic framework for OUQ has\nbeen implemented in the mystic optimization framework. We describe this\nimplementation, and demonstrate its use in the context of the Caltech surrogate\nmodel for hypervelocity impact."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1202.1490v1", 
    "title": "Singular Values using Cholesky Decomposition", 
    "arxiv-id": "1202.1490v1", 
    "author": "Kenan Kocagoez", 
    "publish": "2012-02-07T18:37:07Z", 
    "summary": "In this paper two ways to compute singular values are presented which use\nCholesky decomposition as their basic operation."
},{
    "category": "math.CO", 
    "doi": "10.1109/TSP.2011.2176337", 
    "link": "http://arxiv.org/pdf/1202.4061v2", 
    "title": "Implementation of a Unimodularity Test", 
    "arxiv-id": "1202.4061v2", 
    "author": "Klaus Truemper", 
    "publish": "2012-02-18T06:49:39Z", 
    "summary": "This paper describes implementation and computational results of a polynomial\ntest of total unimodularity. The test is a simplified version of a prior\nmethod. The program also decides two related unimodularity properties. The\nsoftware is available free of charge in source code form under the Boost\nSoftware License."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.3", 
    "link": "http://arxiv.org/pdf/1202.4830v1", 
    "title": "Automatic Deduction in Dynamic Geometry using Sage", 
    "arxiv-id": "1202.4830v1", 
    "author": "Miguel A. Ab\u00e1nades", 
    "publish": "2012-02-22T06:41:37Z", 
    "summary": "We present a symbolic tool that provides robust algebraic methods to handle\nautomatic deduction tasks for a dynamic geometry construction. The main\nprototype has been developed as two different worksheets for the open source\ncomputer algebra system Sage, corresponding to two different ways of coding a\ngeometric construction. In one worksheet, diagrams constructed with the open\nsource dynamic geometry system GeoGebra are accepted. In this worksheet,\nGroebner bases are used to either compute the equation of a geometric locus in\nthe case of a locus construction or to determine the truth of a general\ngeometric statement included in the GeoGebra construction as a boolean\nvariable. In the second worksheet, locus constructions coded using the common\nfile format for dynamic geometry developed by the Intergeo project are accepted\nfor computation. The prototype and several examples are provided for testing.\nMoreover, a third Sage worksheet is presented in which a novel algorithm to\neliminate extraneous parts in symbolically computed loci has been implemented.\nThe algorithm, based on a recent work on the Groebner cover of parametric\nsystems, identifies degenerate components and extraneous adherence points in\nloci, both natural byproducts of general polynomial algebraic methods. Detailed\nexamples are discussed."
},{
    "category": "cs.CG", 
    "doi": "10.4204/EPTCS.79.7", 
    "link": "http://arxiv.org/pdf/1202.4833v1", 
    "title": "Integrating DGSs and GATPs in an Adaptative and Collaborative   Blended-Learning Web-Environment", 
    "arxiv-id": "1202.4833v1", 
    "author": "Pedro Quaresma", 
    "publish": "2012-02-22T06:42:02Z", 
    "summary": "The area of geometry with its very strong and appealing visual contents and\nits also strong and appealing connection between the visual content and its\nformal specification, is an area where computational tools can enhance, in a\nsignificant way, the learning environments.\n  The dynamic geometry software systems (DGSs) can be used to explore the\nvisual contents of geometry. This already mature tools allows an easy\nconstruction of geometric figures build from free objects and elementary\nconstructions. The geometric automated theorem provers (GATPs) allows formal\ndeductive reasoning about geometric constructions, extending the reasoning via\nconcrete instances in a given model to formal deductive reasoning in a\ngeometric theory.\n  An adaptative and collaborative blended-learning environment where the DGS\nand GATP features could be fully explored would be, in our opinion a very rich\nand challenging learning environment for teachers and students.\n  In this text we will describe the Web Geometry Laboratory a Web environment\nincorporating a DGS and a repository of geometric problems, that can be used in\na synchronous and asynchronous fashion and with some adaptative and\ncollaborative features.\n  As future work we want to enhance the adaptative and collaborative aspects of\nthe environment and also to incorporate a GATP, constructing a dynamic and\nindividualised learning environment for geometry."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1202.4837v1", 
    "title": "The GF Mathematics Library", 
    "arxiv-id": "1202.4837v1", 
    "author": "Sebastian Xamb\u00f3", 
    "publish": "2012-02-22T06:43:11Z", 
    "summary": "This paper is devoted to present the Mathematics Grammar Library, a system\nfor multilingual mathematical text processing. We explain the context in which\nit originated, its current design and functionality and the current development\ngoals. We also present two prototype services and comment on possible future\napplications in the area of artificial mathematics assistants."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.1023v1", 
    "title": "Can the Eureqa symbolic regression program, computer algebra and   numerical analysis help each other?", 
    "arxiv-id": "1203.1023v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2012-03-05T19:49:02Z", 
    "summary": "The Eureqa symbolic regression program has recently received extensive press\npraise. A representative quote is\n  \"There are very clever 'thinking machines' in existence today, such as\nWatson, the IBM computer that conquered Jeopardy! last year. But next to\nEureqa, Watson is merely a glorified search engine.\"\n  The program was designed to work with noisy experimental data. However, if\nthe data is generated from an expression for which there exists more concise\nequivalent expressions, sometimes some of the Eureqa results are one or more of\nthose more concise equivalents. If not, perhaps one or more of the returned\nEureqa results might be a sufficiently accurate approximation that is more\nconcise than the given expression. Moreover, when there is no known closed form\nexpression, the data points can be generated by numerical methods, enabling\nEureqa to find expressions that concisely fit those data points with sufficient\naccuracy. In contrast to typical regression software, the user does not have to\nexplicitly or implicitly provide a specific expression or class of expressions\ncontainiing unknown constants for the software to determine.\n  Is Eureqa useful enough in these regards to provide an additional tool for\nexperimental mathematics, computer algebra users and numerical analysis? Yes if\nused carefully. Can computer algebra and numerical methods help Eureqa?\nDefinitely."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.1295v1", 
    "title": "Subtotal ordering -- a pedagogically advantageous algorithm for   computing total degree reverse lexicographic order", 
    "arxiv-id": "1203.1295v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2012-03-06T19:51:48Z", 
    "summary": "Total degree reverse lexicographic order is currently generally regarded as\nmost often fastest for computing Groebner bases. This article describes an\nalternate less mysterious algorithm for computing this order using exponent\nsubtotals and describes why it should be very nearly the same speed the\ntraditional algorithm, all other things being equal. However, experimental\nevidence suggests that subtotal order is actually slightly faster for the\nMathematica Groebner basis implementation more often than not. This is probably\nbecause the weight vectors associated with the natural subtotal weight matrix\nand with the usual total degree reverse lexicographic weight matrix are\ndifferent, and Mathematica also uses those the corresponding weight vectors to\nhelp select successive S polynomials and divisor polynomials: Those selection\nheuristics appear to work slightly better more often with subtotal weight\nvectors.\n  However, the most important advantage of exponent subtotals is pedagogical.\nIt is easier to understand than the total degree reverse lexicographic\nalgorithm, and it is more evident why the resulting order is often the fastest\nknown order for computing Groebner bases.\n  Keywords: Term order, Total degree reverse lexicographic, tdeg, grevlex,\nGroebner basis"
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.1350v1", 
    "title": "Simplifying products of fractional powers of powers", 
    "arxiv-id": "1203.1350v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2012-03-06T23:02:46Z", 
    "summary": "Most computer algebra systems incorrectly simplify (z - z)/(sqrt(w^2)/w^3 -\n1/(w*sqrt(w^2))) to 0 rather than to 0/0. The reasons for this are:\n  1. The default simplification doesn't succeed in simplifying the denominator\nto 0.\n  2. There is a rule that 0 is the result of 0 divided by anything that doesn't\nsimplify to either 0 or 0/0.\n  Try it on your computer algebra systems!\n  This article describes how to simplify products of the form w^a*(w^b1)^g1 ...\n(w^bn)^gn correctly and well, where w is any real or complex expression and the\nexponents are rational numbers.\n  It might seem that correct good simplification of such a restrictive\nexpression class must already be published and/or built into at least one\nwidely used computer-algebra system, but apparently this issue has been\noverlooked. Default and relevant optional simplification was tested with 86\nexamples for n=1 on Derive, Maple, Mathematica, Maxima and TI-CAS. Totaled over\nall five systems, 11% of the results were not equivalent to the input\neverywhere, 50% of the results did not simplify to 0 a result that was\nequivalent to 0, and at least 16% of the results exhibited one or more of four\nadditional flaw types. There was substantial room for improvement in all five\nsystems, including the two for which I was a co-author.\n  The good news is: These flaws are easy to fix."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.1357v1", 
    "title": "Series misdemeanors", 
    "arxiv-id": "1203.1357v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2012-03-07T00:34:35Z", 
    "summary": "Puiseux series are power series in which the exponents can be fractional\nand/or negative rational numbers. Several computer algebra systems have one or\nmore built-in or loadable functions for computing truncated Puiseux series --\nperhaps generalized to allow coefficients containing functions of the series\nvariable that are dominated by any power of that variable, such as logarithms\nand nested logarithms of the series variable. Some computer-algebra systems\nalso offer functions that can compute more-general truncated recursive\nhierarchical series. However, for all of these kinds of truncated series there\nare important implementation details that haven't been addressed before in the\npublished literature and in current implementations.\n  For implementers this article contains ideas for designing more convenient,\ncorrect, and efficient implementations or improving existing ones. For users,\nthis article is a warning about some of these limitations. Many of the ideas in\nthis article have been implemented in the computer-algebra within the TI-Nspire\ncalculator, Windows and Macintosh products."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.4009v1", 
    "title": "Scilab and SIP for Image Processing", 
    "arxiv-id": "1203.4009v1", 
    "author": "Luciano da Fontoura Costa", 
    "publish": "2012-03-18T23:51:19Z", 
    "summary": "This paper is an overview of Image Processing and Analysis using Scilab, a\nfree prototyping environment for numerical calculations similar to Matlab. We\ndemonstrate the capabilities of SIP -- the Scilab Image Processing Toolbox --\nwhich extends Scilab with many functions to read and write images in over 100\nmajor file formats, including PNG, JPEG, BMP, and TIFF. It also provides\nroutines for image filtering, edge detection, blurring, segmentation, shape\nanalysis, and image recognition. Basic directions to install Scilab and SIP are\ngiven, and also a mini-tutorial on Scilab. Three practical examples of image\nanalysis are presented, in increasing degrees of complexity, showing how\nadvanced image analysis techniques seems uncomplicated in this environment."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1203.5846v1", 
    "title": "Series Crimes", 
    "arxiv-id": "1203.5846v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2012-03-27T00:31:28Z", 
    "summary": "Puiseux series are power series in which the exponents can be fractional\nand/or negative rational numbers. Several computer algebra systems have one or\nmore built-in or loadable functions for computing truncated Puiseux series.\nSome are generalized to allow coefficients containing functions of the series\nvariable that are dominated by any power of that variable, such as logarithms\nand nested logarithms of the series variable. Some computer algebra systems\nalso have built-in or loadable functions that compute infinite Puiseux series.\nUnfortunately, there are some little-known pitfalls in computing Puiseux\nseries. The most serious of these is expansions within branch cuts or at branch\npoints that are incorrect for some directions in the complex plane. For example\nwith each series implementation accessible to you:\n  Compare the value of (z^2 + z^3)^(3/2) with that of its truncated series\nexpansion about z = 0, approximated at z = -0.01. Does the series converge to a\nvalue that is the negative of the correct value?\n  Compare the value of ln(z^2 + z^3) with its truncated series expansion about\nz = 0, approximated at z = -0.01 + 0.1i. Does the series converge to a value\nthat is incorrect by 2pi i?\n  Compare arctanh(-2 + ln(z)z) with its truncated series expansion about z = 0,\napproximated at z = -0.01. Does the series converge to a value that is\nincorrect by about pi i?\n  At the time of this writing, most implementations that accommodate such\nseries exhibit such errors. This article describes how to avoid these errors\nboth for manual derivation of series and when implementing series packages."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1205.0790v2", 
    "title": "Automating embedded analysis capabilities and managing software   complexity in multiphysics simulation part I: template-based generic   programming", 
    "arxiv-id": "1205.0790v2", 
    "author": "Andrew G. Salinger", 
    "publish": "2012-05-03T18:24:54Z", 
    "summary": "An approach for incorporating embedded simulation and analysis capabilities\nin complex simulation codes through template-based generic programming is\npresented. This approach relies on templating and operator overloading within\nthe C++ language to transform a given calculation into one that can compute a\nvariety of additional quantities that are necessary for many state-of-the-art\nsimulation and analysis algorithms. An approach for incorporating these ideas\ninto complex simulation codes through general graph-based assembly is also\npresented. These ideas have been implemented within a set of packages in the\nTrilinos framework and are demonstrated on a simple problem from chemical\nengineering."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1205.2107v2", 
    "title": "High-Performance Solvers for Dense Hermitian Eigenproblems", 
    "arxiv-id": "1205.2107v2", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-05-09T21:20:55Z", 
    "summary": "We introduce a new collection of solvers - subsequently called EleMRRR - for\nlarge-scale dense Hermitian eigenproblems. EleMRRR solves various types of\nproblems: generalized, standard, and tridiagonal eigenproblems. Among these,\nthe last is of particular importance as it is a solver on its own right, as\nwell as the computational kernel for the first two; we present a fast and\nscalable tridiagonal solver based on the Algorithm of Multiple Relatively\nRobust Representations - referred to as PMRRR. Like the other EleMRRR solvers,\nPMRRR is part of the freely available Elemental library, and is designed to\nfully support both message-passing (MPI) and multithreading parallelism (SMP).\nAs a result, the solvers can equally be used in pure MPI or in hybrid MPI-SMP\nfashion. We conducted a thorough performance study of EleMRRR and ScaLAPACK's\nsolvers on two supercomputers. Such a study, performed with up to 8,192 cores,\nprovides precise guidelines to assemble the fastest solver within the ScaLAPACK\nframework; it also indicates that EleMRRR outperforms even the fastest solvers\nbuilt from ScaLAPACK's components."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1205.3506v1", 
    "title": "Efficient Expression Templates for Operator Overloading-based Automatic   Differentiation", 
    "arxiv-id": "1205.3506v1", 
    "author": "Roger Pawlowski", 
    "publish": "2012-05-15T20:42:23Z", 
    "summary": "Expression templates are a well-known set of techniques for improving the\nefficiency of operator overloading-based forward mode automatic differentiation\nschemes in the C++ programming language by translating the differentiation from\nindividual operators to whole expressions. However standard expression template\napproaches result in a large amount of duplicate computation, particularly for\nlarge expression trees, degrading their performance. In this paper we describe\nseveral techniques for improving the efficiency of expression templates and\ntheir implementation in the automatic differentiation package Sacado. We\ndemonstrate their improved efficiency through test functions as well as their\napplication to differentiation of a large-scale fluid dynamics simulation code."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1205.3952v1", 
    "title": "Automating embedded analysis capabilities and managing software   complexity in multiphysics simulation part II: application to partial   differential equations", 
    "arxiv-id": "1205.3952v1", 
    "author": "Matthew L. Staten", 
    "publish": "2012-05-17T15:18:00Z", 
    "summary": "A template-based generic programming approach was presented in a previous\npaper that separates the development effort of programming a physical model\nfrom that of computing additional quantities, such as derivatives, needed for\nembedded analysis algorithms. In this paper, we describe the implementation\ndetails for using the template-based generic programming approach for\nsimulation and analysis of partial differential equations (PDEs). We detail\nseveral of the hurdles that we have encountered, and some of the software\ninfrastructure developed to overcome them. We end with a demonstration where we\npresent shape optimization and uncertainty quantification results for a 3D PDE\napplication."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1205.4212v1", 
    "title": "Sample programs in C++ for matrix computations in max plus algebra", 
    "arxiv-id": "1205.4212v1", 
    "author": "Gheorghe Ivan", 
    "publish": "2012-05-17T07:32:57Z", 
    "summary": "The main purpose of this paper is to propose five programs in C++ for matrix\ncomputations and solving recurrent equations systems with entries in max plus\nalgebra."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1206.0141v2", 
    "title": "Parallelizing Mizar", 
    "arxiv-id": "1206.0141v2", 
    "author": "Josef Urban", 
    "publish": "2012-06-01T10:36:53Z", 
    "summary": "This paper surveys and describes the implementation of parallelization of the\nMizar proof checking and of related Mizar utilities. The implementation makes\nuse of Mizar's compiler-like division into several relatively independent\npasses, with typically quite different processing speeds. The information\nproduced in earlier (typically much faster) passes can be used to parallelize\nthe later (typically much slower) passes. The parallelization now works by\nsplitting the formalization into a suitable number of pieces that are processed\nin parallel, assembling from them together the required results. The\nimplementation is evaluated on examples from the Mizar library, and future\nextensions are discussed."
},{
    "category": "physics.comp-ph", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1206.3215v2", 
    "title": "Performance of FORTRAN and C GPU Extensions for a Benchmark Suite of   Fourier Pseudospectral Algorithms", 
    "arxiv-id": "1206.3215v2", 
    "author": "P. Rigge", 
    "publish": "2012-06-14T19:07:42Z", 
    "summary": "A comparison of PGI OpenACC, FORTRAN CUDA, and Nvidia CUDA pseudospectral\nmethods on a single GPU and GCC FORTRAN on single and multiple CPU cores is\nreported. The GPU implementations use CuFFT and the CPU implementations use\nFFTW. Porting pre-existing FORTRAN codes to utilize a GPUs is efficient and\neasy to implement with OpenACC and CUDA FORTRAN. Example programs are provided."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.79.6", 
    "link": "http://arxiv.org/pdf/1208.6465v1", 
    "title": "Parallel Random Search Algorithm of Constrained Pseudo-Boolean   Optimization for Some Distinctive Large-Scale Problems", 
    "arxiv-id": "1208.6465v1", 
    "author": "Lev Kazakovtsev", 
    "publish": "2012-08-31T11:32:31Z", 
    "summary": "In this paper, we consider an approach to the parallelizing of the algorithms\nrealizing the modified probability changigng method with adaptation and partial\nrollback procedure for constrained pseudo-Boolean optimization problems.\nExisting optimization algorithms are adapted for the shared memory and clusters\n(PVM library). The parallel efficiency is estimated for the lagre-scale\nnon-linear pseudo-Boolean optimization problems with linear constraints.\nInitially designed for unconstrained optimization, the probability changing\nmethod (MIVER) allows us finding the approximate solution of different linear\nand non-linear pseudo-Boolean optimization problems with constraints. Although,\nin case of large-scale problems, the computational demands are also very high\nand the precision of the result depends on the time spent. In case of the\nconstrained optimization problem, even the search of any permissibly solution\ncan take very large computational resources. The rapid development of the\nparallel processor systems which are often implemented even in the computer\nsystems designed for home use allows to reduce significantly the time spent to\nfind the acceptable solution with a speed-up close to ideal."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-32313-3_10", 
    "link": "http://arxiv.org/pdf/1209.4233v1", 
    "title": "Writing Reusable Digital Geometry Algorithms in a Generic Image   Processing Framework", 
    "arxiv-id": "1209.4233v1", 
    "author": "Laurent Najman", 
    "publish": "2012-09-18T15:17:10Z", 
    "summary": "Digital Geometry software should reflect the generality of the underlying\nmathe- matics: mapping the latter to the former requires genericity. By\ndesigning generic solutions, one can effectively reuse digital geometry data\nstructures and algorithms. We propose an image processing framework focused on\nthe Generic Programming paradigm in which an algorithm on the paper can be\nturned into a single code, written once and usable with various input types.\nThis approach enables users to design and implement new methods at a lower\ncost, try cross-domain experiments and help generalize results"
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-642-32313-3_10", 
    "link": "http://arxiv.org/pdf/1209.5429v4", 
    "title": "copulaedas: An R Package for Estimation of Distribution Algorithms Based   on Copulas", 
    "arxiv-id": "1209.5429v4", 
    "author": "Marta Soto", 
    "publish": "2012-09-24T21:24:17Z", 
    "summary": "The use of copula-based models in EDAs (estimation of distribution\nalgorithms) is currently an active area of research. In this context, the\ncopulaedas package for R provides a platform where EDAs based on copulas can be\nimplemented and studied. The package offers complete implementations of various\nEDAs based on copulas and vines, a group of well-known optimization problems,\nand utility functions to study the performance of the algorithms. Newly\ndeveloped EDAs can be easily integrated into the package by extending an S4\nclass with generic functions for their main components. This paper presents\ncopulaedas by providing an overview of EDAs based on copulas, a description of\nthe implementation of the package, and an illustration of its use through\nexamples. The examples include running the EDAs defined in the package,\nimplementing new algorithms, and performing an empirical study to compare the\nbehavior of different algorithms on benchmark functions and a real-world\nproblem."
},{
    "category": "cs.SC", 
    "doi": "10.1109/TC.2013.94", 
    "link": "http://arxiv.org/pdf/1209.6626v3", 
    "title": "On Newton-Raphson iteration for multiplicative inverses modulo prime   powers", 
    "arxiv-id": "1209.6626v3", 
    "author": "Jean-Guillaume Dumas", 
    "publish": "2012-09-28T19:52:06Z", 
    "summary": "We study algorithms for the fast computation of modular inverses.\nNewton-Raphson iteration over $p$-adic numbers gives a recurrence relation\ncomputing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve\nthe recurrence to obtain an explicit formula for the inverse. Then we study\ndifferent implementation variants of this iteration and show that our explicit\nformula is interesting for small exponent values but slower or large exponent,\nsay of more than 700 bits. Overall we thus propose a hybrid combination of our\nexplicit formula and the best asymptotic variants. This hybrid combination\nyields then a constant factor improvement, also for large exponents."
},{
    "category": "cs.MS", 
    "doi": "10.1371/journal.pbio.1001745", 
    "link": "http://arxiv.org/pdf/1210.0530v4", 
    "title": "Best Practices for Scientific Computing", 
    "arxiv-id": "1210.0530v4", 
    "author": "Paul Wilson", 
    "publish": "2012-10-01T01:04:04Z", 
    "summary": "Scientists spend an increasing amount of time building and using software.\nHowever, most scientists are never taught how to do this efficiently. As a\nresult, many are unaware of tools and practices that would allow them to write\nmore reliable and maintainable code with less effort. We describe a set of best\npractices for scientific software development that have solid foundations in\nresearch and experience, and that improve scientists' productivity and the\nreliability of their software."
},{
    "category": "cs.MS", 
    "doi": "10.1371/journal.pbio.1001745", 
    "link": "http://arxiv.org/pdf/1210.2536v1", 
    "title": "SMAT: An Input Adaptive Sparse Matrix-Vector Multiplication Auto-Tuner", 
    "arxiv-id": "1210.2536v1", 
    "author": "Mingyu Chen", 
    "publish": "2012-10-09T09:19:43Z", 
    "summary": "Sparse matrix vector multiplication (SpMV) is an important kernel in\nscientific and engineering applications. The previous optimizations are sparse\nmatrix format specific and expose the choice of the best format to application\nprogrammers. In this work we develop an auto-tuning framework to bridge gap\nbetween the specific optimized kernels and their general-purpose use. We\npropose an SpMV auto-tuner (SMAT) that provides an unified interface based on\ncompressed sparse row (CSR) to programmers by implicitly choosing the best\nformat and the fastest implementation of any input sparse matrix in runtime.\nSMAT leverage a data mining model, which is formulated based on a set of\nperformance parameters extracted from 2373 matrices in UF sparse matrix\ncollection, to fast search the best combination. The experiments show that SMAT\nachieves the maximum performance of 75 GFLOP/s in single-precision and 33\nGFLOP/s in double-precision on Intel, and 41 GFLOP/s in single-precision and 34\nGFLOP/s in double-precision on AMD. Compared with the sparse functions in MKL\nlibrary, SMAT runs faster by more than 3 times."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-642-23568-9_22", 
    "link": "http://arxiv.org/pdf/1210.2951v1", 
    "title": "Regular and Singular Boundary Problems in Maple", 
    "arxiv-id": "1210.2951v1", 
    "author": "Markus Rosenkranz", 
    "publish": "2012-10-10T15:13:55Z", 
    "summary": "We describe a new Maple package for treating boundary problems for linear\nordinary differential equations, allowing two-/multipoint as well as Stieltjes\nboundary conditions. For expressing differential operators, boundary\nconditions, and Green's operators, we employ the algebra of\nintegro-differential operators. The operations implemented for regular boundary\nproblems include computing Green's operators as well as composing and factoring\nboundary problems. Our symbolic approach to singular boundary problems is new;\nit provides algorithms for computing compatibility conditions and generalized\nGreen's operators."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-23568-9_22", 
    "link": "http://arxiv.org/pdf/1210.4539v2", 
    "title": "A Robust Complex Division in Scilab", 
    "arxiv-id": "1210.4539v2", 
    "author": "Robert L. Smith", 
    "publish": "2012-10-16T19:52:16Z", 
    "summary": "The most widely used algorithm for floating point complex division, known as\nSmith's method, may fail more often than expected. This document presents two\nimproved complex division algorithms. We present a proof of the robustness of\nthe first improved algorithm. Numerical simulations show that this algorithm\nperforms well in practice and is significantly more robust than other known\nimplementations. By combining additionnal scaling methods with this first\nalgorithm, we were able to create a second algorithm, which rarely fails."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-642-23568-9_22", 
    "link": "http://arxiv.org/pdf/1210.4662v1", 
    "title": "A New Recursive Algorithm For Inverting A General Comrade Matrix", 
    "arxiv-id": "1210.4662v1", 
    "author": "A. A. Karawia", 
    "publish": "2012-10-17T08:09:08Z", 
    "summary": "In this paper, the author present a reliable symbolic computational algorithm\nfor inverting a general comrade matrix by using parallel computing along with\nrecursion. The computational cost of our algorithm is O(n^2). The algorithm is\nimplementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and\nMATHEMATICA. Three examples are presented for the sake of illustration."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-23568-9_22", 
    "link": "http://arxiv.org/pdf/1211.0582v1", 
    "title": "High-Order Discontinuous Galerkin Methods by GPU Metaprogramming", 
    "arxiv-id": "1211.0582v1", 
    "author": "Jan S. Hesthaven", 
    "publish": "2012-11-02T23:55:22Z", 
    "summary": "Discontinuous Galerkin (DG) methods for the numerical solution of partial\ndifferential equations have enjoyed considerable success because they are both\nflexible and robust: They allow arbitrary unstructured geometries and easy\ncontrol of accuracy without compromising simulation stability. In a recent\npublication, we have shown that DG methods also adapt readily to execution on\nmodern, massively parallel graphics processors (GPUs). A number of qualities of\nthe method contribute to this suitability, reaching from locality of reference,\nthrough regularity of access patterns, to high arithmetic intensity. In this\narticle, we illuminate a few of the more practical aspects of bringing DG onto\na GPU, including the use of a Python-based metaprogramming infrastructure that\nwas created specifically to support DG, but has found many uses across all\ndisciplines of computational science."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1109/MCSE.2013.51", 
    "link": "http://arxiv.org/pdf/1211.2075v1", 
    "title": "A multi-scale code for flexible hybrid simulations", 
    "arxiv-id": "1211.2075v1", 
    "author": "O. Lopez-Acevedo", 
    "publish": "2012-11-09T08:55:53Z", 
    "summary": "Multi-scale computer simulations combine the computationally efficient\nclassical algorithms with more expensive but also more accurate ab-initio\nquantum mechanical algorithms. This work describes one implementation of\nmulti-scale computations using the Atomistic Simulation Environment (ASE). This\nimplementation can mix classical codes like LAMMPS and the Density Functional\nTheory-based GPAW. Any combination of codes linked via the ASE interface\nhowever can be mixed. We also introduce a framework to easily add classical\nforce fields calculators for ASE using LAMMPS, which also allows harnessing the\nfull performance of classical-only molecular dynamics. Our work makes it\npossible to combine different simulation codes, quantum mechanical or\nclassical, with great ease and minimal coding effort."
},{
    "category": "cs.SC", 
    "doi": "10.1109/MCSE.2013.51", 
    "link": "http://arxiv.org/pdf/1302.5674v3", 
    "title": "Factorization of Z-homogeneous polynomials in the First (q)-Weyl Algebra", 
    "arxiv-id": "1302.5674v3", 
    "author": "Viktor Levandovskyy", 
    "publish": "2013-01-20T18:37:47Z", 
    "summary": "We present algorithms to factorize weighted homogeneous elements in the first\npolynomial Weyl algebra and $q$-Weyl algebra, which are both viewed as a\n$\\mathbb{Z}$-graded rings. We show, that factorization of homogeneous\npolynomials can be almost completely reduced to commutative univariate\nfactorization over the same base field with some additional uncomplicated\ncombinatorial steps. This allows to deduce the complexity of our algorithms in\ndetail. Furthermore, we will show for homogeneous polynomials that\nirreducibility in the polynomial first Weyl algebra also implies irreducibility\nin the rational one, which is of interest for practical reasons. We report on\nour implementation in the computer algebra system \\textsc{Singular}. It\noutperforms for homogeneous polynomials currently available implementations\ndealing with factorization in the first Weyl algebra both in speed and elegancy\nof the results."
},{
    "category": "cs.PF", 
    "doi": "10.1109/MCSE.2013.51", 
    "link": "http://arxiv.org/pdf/1303.1651v2", 
    "title": "Model-guided Performance Analysis of the Sparse Matrix-Matrix   Multiplication", 
    "arxiv-id": "1303.1651v2", 
    "author": "Ulrich Ruede", 
    "publish": "2013-03-07T11:40:27Z", 
    "summary": "Achieving high efficiency with numerical kernels for sparse matrices is of\nutmost importance, since they are part of many simulation codes and tend to use\nmost of the available compute time and resources. In addition, especially in\nlarge scale simulation frameworks the readability and ease of use of\nmathematical expressions are essential components for the continuous\nmaintenance, modification, and extension of software. In this context, the\nsparse matrix-matrix multiplication is of special interest. In this paper we\nthoroughly analyze the single-core performance of sparse matrix-matrix\nmultiplication kernels in the Blaze Smart Expression Template (SET) framework.\nWe develop simple models for estimating the achievable maximum performance, and\nuse them to assess the efficiency of our implementations. Additionally, we\ncompare these kernels with several commonly used SET-based C++ libraries,\nwhich, just as Blaze, aim at combining the requirements of high performance\nwith an elegant user interface. For the different sparse matrix structures\nconsidered here, we show that our implementations are competitive or faster\nthan those of the other SET libraries for most problem sizes on a current Intel\nmulticore processor."
},{
    "category": "cs.CE", 
    "doi": "10.1109/MCSE.2013.51", 
    "link": "http://arxiv.org/pdf/1308.1464v1", 
    "title": "ManyClaw: Slicing and dicing Riemann solvers for next generation highly   parallel architectures", 
    "arxiv-id": "1308.1464v1", 
    "author": "Kyle T. Mandli", 
    "publish": "2013-08-07T02:24:20Z", 
    "summary": "Next generation computer architectures will include order of magnitude more\nintra-node parallelism; however, many application programmers have a difficult\ntime keeping their codes current with the state-of-the-art machines. In this\ncontext, we analyze Hyperbolic PDE solvers, which are used in the solution of\nmany important applications in science and engineering. We present ManyClaw, a\nproject intended to explore the exploitation of intra-node parallelism in\nhyperbolic PDE solvers via the Clawpack software package for solving hyperbolic\nPDEs. Our goal is to separate the low level parallelism and the physical\nequations thus providing users the capability to leverage intra-node\nparallelism without explicitly writing code to take advantage of newer\narchitectures."
},{
    "category": "cs.SC", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1308.6523v1", 
    "title": "Branch Cuts in Maple 17", 
    "arxiv-id": "1308.6523v1", 
    "author": "D. Wilson", 
    "publish": "2013-08-29T17:06:18Z", 
    "summary": "Accurate and comprehensible knowledge about the position of branch cuts is\nessential for correctly working with multi-valued functions, such as the square\nroot and logarithm. We discuss the new tools in Maple 17 for calculating and\nvisualising the branch cuts of such functions, and others built up from them.\nThe cuts are described in an intuitive and accurate form, offering substantial\nimprovement on the descriptions previously available."
},{
    "category": "cs.LG", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1309.0238v1", 
    "title": "API design for machine learning software: experiences from the   scikit-learn project", 
    "arxiv-id": "1309.0238v1", 
    "author": "Ga\u00ebl Varoquaux", 
    "publish": "2013-09-01T16:22:48Z", 
    "summary": "Scikit-learn is an increasingly popular machine learning li- brary. Written\nin Python, it is designed to be simple and efficient, accessible to\nnon-experts, and reusable in various contexts. In this paper, we present and\ndiscuss our design choices for the application programming interface (API) of\nthe project. In particular, we describe the simple and elegant interface shared\nby all learning and processing units in the library and then discuss its\nadvantages in terms of composition and reusability. The paper also comments on\nimplementation details specific to the Python ecosystem and analyzes obstacles\nfaced by users and developers of the library."
},{
    "category": "cs.CE", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1309.1199v1", 
    "title": "Experiences with Automated Build and Test for Geodynamics Simulation   Codes", 
    "arxiv-id": "1309.1199v1", 
    "author": "Louise H. Kellogg", 
    "publish": "2013-09-04T22:22:52Z", 
    "summary": "The Computational Infrastructure for Geodynamics (CIG) is an NSF funded\nproject that develops, supports, and disseminates community-accessible software\nfor the geodynamics research community. CIG software supports a variety of\ncomputational geodynamic research from mantle and core dynamics, to crustal and\nearthquake dynamics, to magma migration and seismology. To support this type of\nproject a backend computational infrastructure is necessary.\n  Part of this backend infrastructure is an automated build and testing system\nto ensure codes and changes to them are compatible with multiple platforms and\nthat the changes do not significantly affect the scientific results. In this\npaper we describe the build and test infrastructure for CIG based on the BaTLab\nsystem, how it is organized, and how it assists in operations. We demonstrate\nthe use of this type of testing for a suite of geophysics codes, show why codes\nmay compile on one platform but not on another, and demonstrate how minor\nchanges may alter the computed results in unexpected ways that can influence\nthe scientific interpretation. Finally, we examine result comparison between\nplatforms and show how the compiler or operating system may affect results."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1309.1204v2", 
    "title": "Achieving High Performance with Unified Residual Evaluation", 
    "arxiv-id": "1309.1204v2", 
    "author": "Barry F. Smith", 
    "publish": "2013-09-04T23:03:33Z", 
    "summary": "We examine residual evaluation, perhaps the most basic operation in numerical\nsimulation. By raising the level of abstraction in this operation, we can\neliminate specialized code, enable optimization, and greatly increase the\nextensibility of existing code."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1309.1783v2", 
    "title": "DUNE as an Example of Sustainable Open Source Scientific Software   Development", 
    "arxiv-id": "1309.1783v2", 
    "author": "Makus Blatt", 
    "publish": "2013-09-06T21:55:23Z", 
    "summary": "In this paper we describe how DUNE, an open source scientific software\nframework, is developed. Having a sustainable software framework for the\nsolution of partial differential equations is the main driver of DUNE's\ndevelopment. We take a look how DUNE strives to stay sustainable software."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1309.5377v1", 
    "title": "Advanced Techniques for Scientific Programming and Collaborative   Development of Open Source Software Packages at the International Centre for   Theoretical Physics (ICTP)", 
    "arxiv-id": "1309.5377v1", 
    "author": "Shawn T. Brown", 
    "publish": "2013-09-06T15:07:47Z", 
    "summary": "A large number of computational scientific research projects make use of open\nsource software packages. However, the development process of such tools\nfrequently differs from conventional software development; partly because of\nthe nature of research, where the problems being addressed are not always fully\nunderstood; partly because the majority of the development is often carried out\nby scientists with limited experience and exposure to best practices of\nsoftware engineering. Often the software development suffers from the pressure\nto publish scientific results and that credit for software development is\nlimited in comparison. Fundamental components of software engineering like\nmodular and reusable design, validation, documentation, and software\nintegration as well as effective maintenance and user support tend to be\ndisregarded due to lack of resources and qualified specialists. Thus innovative\ndevelopments are often hindered by steep learning curves required to master\ndevelopment for legacy software packages full of ad hoc solutions. The growing\ncomplexity of research, however, requires suitable and maintainable\ncomputational tools, resulting in a widening gap between the potential users\n(often growing in number) and contributors to the development of such a\npackage. In this paper we share our experiences aiming to improve the situation\nby training particularly young scientists, through disseminating our own\nexperiences at contributing to open source software packages and practicing key\ncomponents of software engineering adapted for scientists and scientific\nsoftware development. Specifically we summarize the outcome of the Workshop in\nAdvanced Techniques for Scientific Programming and Collaborative Development of\nOpen Source Software Packages run at the Abdus Salam International Centre for\nTheoretical Physics in March 2013, and discuss our conclusions for future\nefforts."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2644288.2644293", 
    "link": "http://arxiv.org/pdf/1311.0681v1", 
    "title": "Computation of the Marcum Q-function", 
    "arxiv-id": "1311.0681v1", 
    "author": "N. M. Temme", 
    "publish": "2013-11-04T12:50:09Z", 
    "summary": "Methods and an algorithm for computing the generalized Marcum $Q-$function\n($Q_{\\mu}(x,y)$) and the complementary function ($P_{\\mu}(x,y)$) are described.\nThese functions appear in problems of different technical and scientific areas\nsuch as, for example, radar detection and communications, statistics and\nprobability theory, where they are called the non-central chi-square or the non\ncentral gamma cumulative distribution functions.\n  The algorithm for computing the Marcum functions combines different methods\nof evaluation in different regions: series expansions, integral\nrepresentations, asymptotic expansions, and use of three-term homogeneous\nrecurrence relations. A relative accuracy close to $10^{-12}$ can be obtained\nin the parameter region $(x,y,\\mu) \\in [0,\\,A]\\times [0,\\,A]\\times [1,\\,A]$,\n$A=200$, while for larger parameters the accuracy decreases (close to\n$10^{-11}$ for $A=1000$ and close to $5\\times 10^{-11}$ for $A=10000$)."
},{
    "category": "cs.DC", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1311.1753v1", 
    "title": "GooFit: A library for massively parallelising maximum-likelihood fits", 
    "arxiv-id": "1311.1753v1", 
    "author": "K. Tomko", 
    "publish": "2013-11-07T17:18:42Z", 
    "summary": "Fitting complicated models to large datasets is a bottleneck of many\nanalyses. We present GooFit, a library and tool for constructing\narbitrarily-complex probability density functions (PDFs) to be evaluated on\nnVidia GPUs or on multicore CPUs using OpenMP. The massive parallelisation of\ndividing up event calculations between hundreds of processors can achieve\nspeedups of factors 200-300 in real-world problems."
},{
    "category": "cs.SC", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1402.0622v1", 
    "title": "Divide-And-Conquer Computation of Cylindrical Algebraic Decomposition", 
    "arxiv-id": "1402.0622v1", 
    "author": "Adam Strzebonski", 
    "publish": "2014-02-04T05:52:47Z", 
    "summary": "We present a divide-and-conquer version of the Cylindrical Algebraic\nDecomposition (CAD) algorithm. The algorithm represents the input as a Boolean\ncombination of subformulas, computes cylindrical algebraic decompositions of\nsolution sets of the subformulas, and combines the results. We propose a\ngraph-based heuristic to find a suitable partitioning of the input and present\nempirical comparison with direct CAD computation."
},{
    "category": "cs.MS", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1402.3809v2", 
    "title": "Toward Resilient Algorithms and Applications", 
    "arxiv-id": "1402.3809v2", 
    "author": "Michael A. Heroux", 
    "publish": "2014-02-16T15:43:05Z", 
    "summary": "Over the past decade, the high performance computing community has become\nincreasingly concerned that preserving the reliable, digital machine model will\nbecome too costly or infeasible. In this paper we discuss four approaches for\ndeveloping new algorithms that are resilient to hard and soft failures."
},{
    "category": "cs.MS", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1404.4410v2", 
    "title": "A heuristic prover for real inequalities", 
    "arxiv-id": "1404.4410v2", 
    "author": "Cody Roux", 
    "publish": "2014-04-17T01:31:39Z", 
    "summary": "We describe a general method for verifying inequalities between real-valued\nexpressions, especially the kinds of straightforward inferences that arise in\ninteractive theorem proving. In contrast to approaches that aim to be complete\nwith respect to a particular language or class of formulas, our method\nestablishes claims that require heterogeneous forms of reasoning, relying on a\nNelson-Oppen-style architecture in which special-purpose modules collaborate\nand share information. The framework is thus modular and extensible. A\nprototype implementation shows that the method works well on a variety of\nexamples, and complements techniques that are used by contemporary interactive\nprovers."
},{
    "category": "cs.MS", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1404.6383v2", 
    "title": "Bloscpack: a compressed lightweight serialization format for numerical   data", 
    "arxiv-id": "1404.6383v2", 
    "author": "Valentin Haenel", 
    "publish": "2014-04-25T10:53:23Z", 
    "summary": "This paper introduces the Bloscpack file format and the accompanying Python\nreference implementation. Bloscpack is a lightweight, compressed binary\nfile-format based on the Blosc codec and is designed for lightweight, fast\nserialization of numerical data. This article presents the features of the\nfile-format and some some API aspects of the reference implementation, in\nparticular the ability to handle Numpy ndarrays. Furthermore, in order to\ndemonstrate its utility, the format is compared both feature- and\nperformance-wise to a few alternative lightweight serialization solutions for\nNumpy ndarrays. The performance comparisons take the form of some comprehensive\nbenchmarks over a range of different artificial datasets with varying size and\ncomplexity, the results of which are presented as the last section of this\narticle."
},{
    "category": "cs.MS", 
    "doi": "10.1088/1742-6596/513/5/052003", 
    "link": "http://arxiv.org/pdf/1404.6388v2", 
    "title": "Performance of Python runtimes on a non-numeric scientific code", 
    "arxiv-id": "1404.6388v2", 
    "author": "Riccardo Murri", 
    "publish": "2014-04-25T10:55:48Z", 
    "summary": "The Python library FatGHol FatGHoL used in Murri2012 to reckon the rational\nhomology of the moduli space of Riemann surfaces is an example of a non-numeric\nscientific code: most of the processing it does is generating graphs\n(represented by complex Python objects) and computing their isomorphisms (a\ntriple of Python lists; again a nested data structure). These operations are\nrepeated many times over: for example, the spaces and are triangulated by\n4'583'322 and 747'664 graphs, respectively. This is an opportunity for every\nPython runtime to prove its strength in optimization. The purpose of this\nexperiment was to assess the maturity of alternative Python runtimes, in terms\nof: compatibility with the language as implemented in CPython 2.7, and\nperformance speedup. This paper compares the results and experiences from\nrunning FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython\n0.19, Numba 0.11, Nuitka 0.4.4 and Falcon."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-08434-3_24", 
    "link": "http://arxiv.org/pdf/1406.0292v1", 
    "title": "Interactive Simplifier Tracing and Debugging in Isabelle", 
    "arxiv-id": "1406.0292v1", 
    "author": "Lars Hupel", 
    "publish": "2014-06-02T08:44:33Z", 
    "summary": "The Isabelle proof assistant comes equipped with a very powerful tactic for\nterm simplification. While tremendously useful, the results of simplifying a\nterm do not always match the user's expectation: sometimes, the resulting term\nis not in the form the user expected, or the simplifier fails to apply a rule.\nWe describe a new, interactive tracing facility which offers insight into the\nhierarchical structure of the simplification with user-defined filtering,\nmemoization and search. The new simplifier trace is integrated into the\nIsabelle/jEdit Prover IDE."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.152", 
    "link": "http://arxiv.org/pdf/1406.1238v1", 
    "title": "Proceedings Twelfth International Workshop on the ACL2 Theorem Prover   and its Applications", 
    "arxiv-id": "1406.1238v1", 
    "author": "Julien Schmaltz", 
    "publish": "2014-06-04T23:49:33Z", 
    "summary": "This volume contains the proceedings of the Twelfth International Workshop on\nthe ACL2 Theorem Prover and Its Applications, ACL2'14, a two-day workshop held\nin Vienna, Austria, on July 12-13, 2014. ACL2 workshops occur at approximately\n18-month intervals and provide a major technical forum for researchers to\npresent and discuss improvements and extensions to the theorem prover,\ncomparisons of ACL2 with other systems, and applications of ACL2 in formal\nverification. These proceedings include 13 peer reviewed technical papers.\n  ACL2 is a state-of-the-art automated reasoning system that has been\nsuccessfully applied in academia, government, and industry for specification\nand verification of computing systems and in teaching computer science courses.\nIn 2005, Boyer, Kaufmann, and Moore were awarded the 2005 ACM Software System\nAward for their work in ACL2 and the other theorem provers in the Boyer-Moore\nfamily."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.152.9", 
    "link": "http://arxiv.org/pdf/1406.1561v1", 
    "title": "Formal Verification of Medina's Sequence of Polynomials for   Approximating Arctangent", 
    "arxiv-id": "1406.1561v1", 
    "author": "John Cowles", 
    "publish": "2014-06-06T01:48:16Z", 
    "summary": "The verification of many algorithms for calculating transcendental functions\nis based on polynomial approximations to these functions, often Taylor series\napproximations. However, computing and verifying approximations to the\narctangent function are very challenging problems, in large part because the\nTaylor series converges very slowly to arctangent-a 57th-degree polynomial is\nneeded to get three decimal places for arctan(0.95). Medina proposed a series\nof polynomials that approximate arctangent with far faster convergence-a\n7th-degree polynomial is all that is needed to get three decimal places for\narctan(0.95). We present in this paper a proof in ACL2(r) of the correctness\nand convergence rate of this sequence of polynomials. The proof is particularly\nbeautiful, in that it uses many results from real analysis. Some of these\nnecessary results were proven in prior work, but some were proven as part of\nthis effort."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.152.9", 
    "link": "http://arxiv.org/pdf/1406.1796v1", 
    "title": "A Generic Numbering System based on Catalan Families of Combinatorial   Objects", 
    "arxiv-id": "1406.1796v1", 
    "author": "Paul Tarau", 
    "publish": "2014-06-06T19:22:44Z", 
    "summary": "We study novel arithmetic algorithms on a canonical number representation\nbased on the Catalan family of combinatorial objects.\n  Our algorithms work on a generic representation that we illustrate on\ninstances like ordered binary and multiway trees, balanced parentheses\nlanguages as well as the usual bitstring-based natural numbers seen through the\nsame generic interface as members of the Catalan family.\n  For numbers corresponding to Catalan objects of low representation\ncomplexity, our algorithms provide super-exponential gains while their average\nand worst case complexity is within constant factors of their traditional\ncounterparts."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.152.9", 
    "link": "http://arxiv.org/pdf/1406.2079v4", 
    "title": "Program Verification of Numerical Computation - Part 2", 
    "arxiv-id": "1406.2079v4", 
    "author": "Garry Pantelis", 
    "publish": "2014-06-09T05:36:44Z", 
    "summary": "These notes present some extensions of a formal method introduced in an\nearlier paper. The formal method is designed as a tool for program verification\nof numerical computation and forms the basis of the software package VPC.\nIncluded in the extensions that are presented here are disjunctions and methods\nfor detecting non-computable programs. A more comprehensive list of the\nconstruction rules as higher order constructs is also presented."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.152.2", 
    "link": "http://arxiv.org/pdf/1406.2266v1", 
    "title": "Industrial-Strength Documentation for ACL2", 
    "arxiv-id": "1406.2266v1", 
    "author": "Matt Kaufmann", 
    "publish": "2014-06-06T01:47:12Z", 
    "summary": "The ACL2 theorem prover is a complex system. Its libraries are vast.\nIndustrial verification efforts may extend this base with hundreds of thousands\nof lines of additional modeling tools, specifications, and proof scripts. High\nquality documentation is vital for teams that are working together on projects\nof this scale. We have developed XDOC, a flexible, scalable documentation tool\nfor ACL2 that can incorporate the documentation for ACL2 itself, the Community\nBooks, and an organization's internal formal verification projects, and which\nhas many features that help to keep the resulting manuals up to date. Using\nthis tool, we have produced a comprehensive, publicly available ACL2+Books\nManual that brings better documentation to all ACL2 users. We have also\ndeveloped an extended manual for use within Centaur Technology that extends the\npublic manual to cover Centaur's internal books. We expect that other\norganizations using ACL2 will wish to develop similarly extended manuals."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.152.2", 
    "link": "http://arxiv.org/pdf/1406.5550v2", 
    "title": "ViDaExpert: user-friendly tool for nonlinear visualization and analysis   of multidimensional vectorial data", 
    "arxiv-id": "1406.5550v2", 
    "author": "Andrei Zinovyev", 
    "publish": "2014-06-20T22:31:25Z", 
    "summary": "ViDaExpert is a tool for visualization and analysis of multidimensional\nvectorial data. ViDaExpert is able to work with data tables of \"object-feature\"\ntype that might contain numerical feature values as well as textual labels for\nrows (objects) and columns (features). ViDaExpert implements several\nstatistical methods such as standard and weighted Principal Component Analysis\n(PCA) and the method of elastic maps (non-linear version of PCA), Linear\nDiscriminant Analysis (LDA), multilinear regression, K-Means clustering, a\nvariant of decision tree construction algorithm. Equipped with several\nuser-friendly dialogs for configuring data point representations (size, shape,\ncolor) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an\ninteractive 3D-scene representing a table of data in multidimensional space and\nperform its quick and insightfull statistical analysis, from basic to advanced\nmethods."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.152.2", 
    "link": "http://arxiv.org/pdf/1408.0074v1", 
    "title": "Software for Computing the Spheroidal Wave Functions Using Arbitrary   Precision Arithmetic", 
    "arxiv-id": "1408.0074v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-08-01T04:29:30Z", 
    "summary": "The spheroidal wave functions, which are the solutions to the Helmholtz\nequation in spheroidal coordinates, are notoriously difficult to compute.\nBecause of this, practically no programming language comes equipped with the\nmeans to compute them. This makes problems that require their use hard to\ntackle. We have developed computational software for calculating these special\nfunctions. Our software is called spheroidal and includes several novel\nfeatures, such as: using arbitrary precision arithmetic; adaptively choosing\nthe number of expansion coefficients to compute and use; and using the\nWronskian to choose from several different methods for computing the spheroidal\nradial functions to improve their accuracy. There are two types of spheroidal\nwave functions: the prolate kind when prolate spheroidal coordinates are used;\nand the oblate kind when oblate spheroidal coordinate are used. In this paper,\nwe describe both, methods for computing them, and our software. We have made\nour software freely available on our webpage."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140979381", 
    "link": "http://arxiv.org/pdf/1408.5535v3", 
    "title": "A Preconditioned Hybrid SVD Method for Computing Accurately Singular   Triplets of Large Matrices", 
    "arxiv-id": "1408.5535v3", 
    "author": "Andreas Stathopoulos", 
    "publish": "2014-08-23T23:19:39Z", 
    "summary": "The computation of a few singular triplets of large, sparse matrices is a\nchallenging task, especially when the smallest magnitude singular values are\nneeded in high accuracy. Most recent efforts try to address this problem\nthrough variations of the Lanczos bidiagonalization method, but they are still\nchallenged even for medium matrix sizes due to the difficulty of the problem.\nWe propose a novel SVD approach that can take advantage of preconditioning and\nof any well designed eigensolver to compute both largest and smallest singular\ntriplets. Accuracy and efficiency is achieved through a hybrid, two-stage\nmeta-method, PHSVDS. In the first stage, PHSVDS solves the normal equations up\nto the best achievable accuracy. If further accuracy is required, the method\nswitches automatically to an eigenvalue problem with the augmented matrix. Thus\nit combines the advantages of the two stages, faster convergence and accuracy,\nrespectively. For the augmented matrix, solving the interior eigenvalue is\nfacilitated by a proper use of the good initial guesses from the first stage\nand an efficient implementation of the refined projection method. We also\ndiscuss how to precondition PHSVDS and to cope with some issues that arise.\nNumerical experiments illustrate the efficiency and robustness of the method."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1409.4618v2", 
    "title": "Fast MATLAB assembly of FEM matrices in 2D and 3D: Edge elements", 
    "arxiv-id": "1409.4618v2", 
    "author": "Jan Valdman", 
    "publish": "2014-09-16T13:08:47Z", 
    "summary": "We propose an effective and flexible way to assemble finite element stiffness\nand mass matrices in MATLAB. We apply this for problems discretized by edge\nfinite elements. Typical edge finite elements are Raviart-Thomas elements used\nin discretizations of H(div) spaces and Nedelec elements in discretizations of\nH(curl) spaces. We explain vectorization ideas and comment on a freely\navailable MATLAB code which is fast and scalable with respect to time."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1409.7316v1", 
    "title": "An Analysis of Publication Venues for Automatic Differentiation Research", 
    "arxiv-id": "1409.7316v1", 
    "author": "Barak A. Pearlmutter", 
    "publish": "2014-09-25T16:20:16Z", 
    "summary": "We present the results of our analysis of publication venues for papers on\nautomatic differentiation (AD), covering academic journals and conference\nproceedings. Our data are collected from the AD publications database\nmaintained by the autodiff.org community website. The database is purpose-built\nfor the AD field and is expanding via submissions by AD researchers. Therefore,\nit provides a relatively noise-free list of publications relating to the field.\nHowever, it does include noise in the form of variant spellings of journal and\nconference names. We handle this by manually correcting and merging these\nvariants under the official names of corresponding venues. We also share the\nraw data we get after these corrections."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1409.8608v1", 
    "title": "On the Performance Prediction of BLAS-based Tensor Contractions", 
    "arxiv-id": "1409.8608v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2014-09-30T15:54:13Z", 
    "summary": "Tensor operations are surging as the computational building blocks for a\nvariety of scientific simulations and the development of high-performance\nkernels for such operations is known to be a challenging task. While for\noperations on one- and two-dimensional tensors there exist standardized\ninterfaces and highly-optimized libraries (BLAS), for higher dimensional\ntensors neither standards nor highly-tuned implementations exist yet. In this\npaper, we consider contractions between two tensors of arbitrary dimensionality\nand take on the challenge of generating high-performance implementations by\nresorting to sequences of BLAS kernels. The approach consists in breaking the\ncontraction down into operations that only involve matrices or vectors. Since\nin general there are many alternative ways of decomposing a contraction, we are\nable to methodically derive a large family of algorithms. The main contribution\nof this paper is a systematic methodology to accurately identify the fastest\nalgorithms in the bunch, without executing them. The goal is instead\naccomplished with the help of a set of cache-aware micro-benchmarks for the\nunderlying BLAS kernels. The predictions we construct from such benchmarks\nallow us to reliably single out the best-performing algorithms in a tiny\nfraction of the time taken by the direct execution of the algorithms."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1501.00324v1", 
    "title": "A New Sparse Matrix Vector Multiplication GPU Algorithm Designed for   Finite Element Problems", 
    "arxiv-id": "1501.00324v1", 
    "author": "Eric Darve", 
    "publish": "2015-01-01T21:57:19Z", 
    "summary": "Recently, graphics processors (GPUs) have been increasingly leveraged in a\nvariety of scientific computing applications. However, architectural\ndifferences between CPUs and GPUs necessitate the development of algorithms\nthat take advantage of GPU hardware. As sparse matrix vector multiplication\n(SPMV) operations are commonly used in finite element analysis, a new SPMV\nalgorithm and several variations are developed for unstructured finite element\nmeshes on GPUs. The effective bandwidth of current GPU algorithms and the newly\nproposed algorithms are measured and analyzed for 15 sparse matrices of varying\nsizes and varying sparsity structures. The effects of optimization and\ndifferences between the new GPU algorithm and its variants are then\nsubsequently studied. Lastly, both new and current SPMV GPU algorithms are\nutilized in the GPU CG Solver in GPU finite element simulations of the heart.\nThese results are then compared against parallel PETSc finite element\nimplementation results. The effective bandwidth tests indicate that the new\nalgorithms compare very favorably with current algorithms for a wide variety of\nsparse matrices and can yield very notable benefits. GPU finite element\nsimulation results demonstrate the benefit of using GPUs for finite element\nanalysis, and also show that the proposed algorithms can yield speedup factors\nup to 12-fold for real finite element applications."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1501.05709v1", 
    "title": "Associative Arrays: Unified Mathematics for Spreadsheets, Databases,   Matrices, and Graphs", 
    "arxiv-id": "1501.05709v1", 
    "author": "Hayden Jansen", 
    "publish": "2015-01-23T04:16:04Z", 
    "summary": "Data processing systems impose multiple views on data as it is processed by\nthe system. These views include spreadsheets, databases, matrices, and graphs.\nThe common theme amongst these views is the need to store and operate on data\nas whole sets instead of as individual data elements. This work describes a\ncommon mathematical representation of these data sets (associative arrays) that\napplies across a wide range of applications and technologies. Associative\narrays unify and simplify these different approaches for representing and\nmanipulating data into common two-dimensional view of data. Specifically,\nassociative arrays (1) reduce the effort required to pass data between steps in\na data processing system, (2) allow steps to be interchanged with full\nconfidence that the results will be unchanged, and (3) make it possible to\nrecognize when steps can be simplified or eliminated. Most database system\nnaturally support associative arrays via their tabular interfaces. The D4M\nimplementation of associative arrays uses this feature to provide a common\ninterface across SQL, NoSQL, and NewSQL databases."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1501.07350v2", 
    "title": "Performance Tuning of a Parallel 3-D FFT Package OpenFFT", 
    "arxiv-id": "1501.07350v2", 
    "author": "Taisuke Ozaki", 
    "publish": "2015-01-29T06:12:52Z", 
    "summary": "The fast Fourier transform (FFT) is a primitive kernel in numerous fields of\nscience and engineering. OpenFFT is an open-source parallel package for 3-D\nFFTs, built on a communication-optimal domain decomposition method for\nachieving minimal volume of communication. In this paper, we analyze and tune\nthe performance of OpenFFT, paying a particular attention to tuning of\ncommunication that dominates the run time of large-scale calculations. We first\nanalyze its performance on different machines for an understanding of the\nbehaviors of the package and machines. Based on the performance analysis, we\ndevelop six communication methods for performing communication with the aim of\ncovering varied calculation scales on a variety of computational platforms.\nOpenFFT is then augmented with an auto-tuning of communication to select the\nbest method in run time depending on their performance. Numerical results\ndemonstrate that the optimized OpenFFT is able to deliver relatively good\nperformance in comparison with other state-of-the-art packages at different\ncomputational scales on a number of parallel machines."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.amc.2015.03.105", 
    "link": "http://arxiv.org/pdf/1104.0689v1", 
    "title": "Algorithms for Computing Triangular Decompositions of Polynomial Systems", 
    "arxiv-id": "1104.0689v1", 
    "author": "Marc Moreno Maza", 
    "publish": "2011-04-04T20:52:03Z", 
    "summary": "We propose new algorithms for computing triangular decompositions of\npolynomial systems incrementally. With respect to previous works, our\nimprovements are based on a {\\em weakened} notion of a polynomial GCD modulo a\nregular chain, which permits to greatly simplify and optimize the\nsub-algorithms. Extracting common work from similar expensive computations is\nalso a key feature of our algorithms. In our experimental results the\nimplementation of our new algorithms, realized with the {\\RegularChains}\nlibrary in {\\Maple}, outperforms solvers with similar specifications by several\norders of magnitude on sufficiently difficult problems."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jcp.2013.01.030", 
    "link": "http://arxiv.org/pdf/1204.3853v2", 
    "title": "Block-Structured Adaptive Mesh Refinement Algorithms for Vlasov   Simulation", 
    "arxiv-id": "1204.3853v2", 
    "author": "J. W. Banks", 
    "publish": "2012-04-17T17:25:52Z", 
    "summary": "Direct discretization of continuum kinetic equations, like the Vlasov\nequation, are under-utilized because the distribution function generally exists\nin a high-dimensional (>3D) space and computational cost increases\ngeometrically with dimension. We propose to use high-order finite-volume\ntechniques with block-structured adaptive mesh refinement (AMR) to reduce the\ncomputational cost. The primary complication comes from a solution state\ncomprised of variables of different dimensions. We develop the algorithms\nrequired to extend standard single-dimension block structured AMR to the\nmulti-dimension case. Specifically, algorithms for reduction and injection\noperations that transfer data between mesh hierarchies of different dimensions\nare explained in detail. In addition, modifications to the basic AMR algorithm\nthat enable the use of high-order spatial and temporal discretizations are\ndiscussed. Preliminary results for a standard 1D+1V Vlasov-Poisson test problem\nare presented. Results indicate that there is potential for significant savings\nfor some classes of Vlasov problems."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.jcp.2013.01.030", 
    "link": "http://arxiv.org/pdf/1204.5086v1", 
    "title": "Reimplementing the Mathematical Subject Classification (MSC) as a Linked   Open Dataset", 
    "arxiv-id": "1204.5086v1", 
    "author": "Ioannis Antoniou", 
    "publish": "2012-04-23T15:29:30Z", 
    "summary": "The Mathematics Subject Classification (MSC) is a widely used scheme for\nclassifying documents in mathematics by subject. Its traditional, idiosyncratic\nconceptualization and representation makes the scheme hard to maintain and\nrequires custom implementations of search, query and annotation support. This\nlimits uptake e.g. in semantic web technologies in general and the creation and\nexploration of connections between mathematics and related domains (e.g.\nscience) in particular.\n  This paper presents the new official implementation of the MSC2010 as a\nLinked Open Dataset, building on SKOS (Simple Knowledge Organization System).\nWe provide a brief overview of the dataset's structure, its available\nimplementations, and first applications."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jcp.2013.01.030", 
    "link": "http://arxiv.org/pdf/1204.5094v2", 
    "title": "Point-and-write --- Documenting Formal Mathematics by Reference", 
    "arxiv-id": "1204.5094v2", 
    "author": "Josef Urban", 
    "publish": "2012-04-23T16:05:46Z", 
    "summary": "This paper describes the design and implementation of mechanisms for\nlight-weight inclusion of formal mathematics in informal mathematical writings,\nparticularly in a Web-based setting. This is conceptually done in three stages:\n(i) by choosing a suitable representation layer (based on RDF) for encoding the\ninformation about available resources of formal mathematics, (ii) by exporting\nthis information from formal libraries, and (iii) by providing syntax and\nimplementation for including formal mathematics in informal writings.\n  We describe the use case of an author referring to formal text from an\ninformal narrative, and discuss design choices entailed by this use case.\nFurthermore, we describe an implementation of the use case within the Agora\nprototype: a Wiki for collaborating on formalized mathematics."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jcp.2013.01.030", 
    "link": "http://arxiv.org/pdf/1207.1916v1", 
    "title": "How good are MatLab, Octave and Scilab for Computational Modelling?", 
    "arxiv-id": "1207.1916v1", 
    "author": "Alejandro C. Frery", 
    "publish": "2012-07-08T21:52:03Z", 
    "summary": "In this article we test the accuracy of three platforms used in computational\nmodelling: MatLab, Octave and Scilab, running on i386 architecture and three\noperating systems (Windows, Ubuntu and Mac OS). We submitted them to numerical\ntests using standard data sets and using the functions provided by each\nplatform. A Monte Carlo study was conducted in some of the datasets in order to\nverify the stability of the results with respect to small departures from the\noriginal input. We propose a set of operations which include the computation of\nmatrix determinants and eigenvalues, whose results are known. We also used data\nprovided by NIST (National Institute of Standards and Technology), a protocol\nwhich includes the computation of basic univariate statistics (mean, standard\ndeviation and first-lag correlation), linear regression and extremes of\nprobability distributions. The assessment was made comparing the results\ncomputed by the platforms with certified values, that is, known results,\ncomputing the number of correct significant digits."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-31374-5_33", 
    "link": "http://arxiv.org/pdf/1207.2291v1", 
    "title": "On Formal Specification of Maple Programs", 
    "arxiv-id": "1207.2291v1", 
    "author": "Wolfgang Schreiner", 
    "publish": "2012-07-10T10:21:34Z", 
    "summary": "This paper is an example-based demonstration of our initial results on the\nformal specification of programs written in the computer algebra language\nMiniMaple (a substantial subset of Maple with slight extensions). The main goal\nof this work is to define a verification framework for MiniMaple. Formal\nspecification of MiniMaple programs is rather complex task as it supports\nnon-standard types of objects, e.g. symbols and unevaluated expressions, and\nadditional functions and predicates, e.g. runtime type tests etc. We have used\nthe specification language to specify various computer algebra concepts\nrespective objects of the Maple package DifferenceDifferential developed at our\ninstitute."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-642-31374-5_33", 
    "link": "http://arxiv.org/pdf/1301.0763v1", 
    "title": "Improved QFT algorithm for power-of-two FFT", 
    "arxiv-id": "1301.0763v1", 
    "author": "Lorenzo Pasquini", 
    "publish": "2013-01-04T16:25:56Z", 
    "summary": "This paper shows that it is possible to improve the computational cost, the\nmemory requirements and the accuracy of Quick Fourier Transform (QFT) algorithm\nfor power-of-two FFT (Fast Fourier Transform) just introducing a slight\nmodification in this algorithm. The new algorithm requires the same number of\nadditions and multiplications of split-radix 3add/3mul, one of the most\nappreciated FFT algorithms appeared in the literature, but employing only half\nof the trigonometric constants. These results can elevate the QFT approach to\nthe level of most used FFT procedures. A new quite general way to describe FFT\nalgorithms, based on signal types and on a particular notation, is also\nproposed and used, highligting its advantages."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-31374-5_33", 
    "link": "http://arxiv.org/pdf/1301.1704v1", 
    "title": "Parallel Algorithms for Constructing Data Structures for Fast Multipole   Methods", 
    "arxiv-id": "1301.1704v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2013-01-08T21:57:20Z", 
    "summary": "We present efficient algorithms to build data structures and the lists needed\nfor fast multipole methods. The algorithms are capable of being efficiently\nimplemented on both serial, data parallel GPU and on distributed architectures.\nWith these algorithms it is possible to map the FMM efficiently on to the GPU\nor distributed heterogeneous CPU-GPU systems. Further, in dynamic problems, as\nthe distribution of the particles change, the reduced cost of building the data\nstructures improves performance. Using these algorithms, we demonstrate example\nhigh fidelity simulations with large problem sizes by using FMM on both single\nand multiple heterogeneous computing facilities equipped with multi-core CPU\nand many-core GPUs."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-31374-5_33", 
    "link": "http://arxiv.org/pdf/1301.4438v1", 
    "title": "Simultaneous computation of the row and column rank profiles", 
    "arxiv-id": "1301.4438v1", 
    "author": "Ziad Sultan", 
    "publish": "2013-01-18T17:23:01Z", 
    "summary": "Gaussian elimination with full pivoting generates a PLUQ matrix\ndecomposition. Depending on the strategy used in the search for pivots, the\npermutation matrices can reveal some information about the row or the column\nrank profiles of the matrix. We propose a new pivoting strategy that makes it\npossible to recover at the same time both row and column rank profiles of the\ninput matrix and of any of its leading sub-matrices. We propose a\nrank-sensitive and quad-recursive algorithm that computes the latter PLUQ\ntriangular decomposition of an m \\times n matrix of rank r in O(mnr^{\\omega-2})\nfield operations, with \\omega the exponent of matrix multiplication. Compared\nto the LEU decomposition by Malashonock, sharing a similar recursive structure,\nits time complexity is rank sensitive and has a lower leading constant. Over a\nword size finite field, this algorithm also improveLs the practical efficiency\nof previously known implementations."
},{
    "category": "math.NA", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1301.7744v3", 
    "title": "Exploiting Symmetry in Tensors for High Performance: Multiplication with   Symmetric Tensors", 
    "arxiv-id": "1301.7744v3", 
    "author": "Tamara G. Kolda", 
    "publish": "2013-01-31T20:39:44Z", 
    "summary": "Symmetric tensor operations arise in a wide variety of computations. However,\nthe benefits of exploiting symmetry in order to reduce storage and computation\nis in conflict with a desire to simplify memory access patterns. In this paper,\nwe propose a blocked data structure (Blocked Compact Symmetric Storage) wherein\nwe consider the tensor by blocks and store only the unique blocks of a\nsymmetric tensor. We propose an algorithm-by-blocks, already shown of benefit\nfor matrix computations, that exploits this storage format by utilizing a\nseries of temporary tensors to avoid redundant computation. Further, partial\nsymmetry within temporaries is exploited to further avoid redundant storage and\nredundant computation. A detailed analysis shows that, relative to storing and\ncomputing with tensors without taking advantage of symmetry and partial\nsymmetry, storage requirements are reduced by a factor of $ O\\left( m! \\right)$\nand computational requirements by a factor of $O\\left( (m+1)!/2^m \\right)$,\nwhere $ m $ is the order of the tensor. However, as the analysis shows, care\nmust be taken in choosing the correct block size to ensure these storage and\ncomputational benefits are achieved (particularly for low-order tensors). An\nimplementation demonstrates that storage is greatly reduced and the complexity\nintroduced by storing and computing with tensors by blocks is manageable.\nPreliminary results demonstrate that computational time is also reduced. The\npaper concludes with a discussion of how insights in this paper point to\nopportunities for generalizing recent advances in the domain of linear algebra\nlibraries to the field of multi-linear computation."
},{
    "category": "cs.NA", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1304.1864v3", 
    "title": "Improved Accuracy and Parallelism for MRRR-based Eigensolvers -- A Mixed   Precision Approach", 
    "arxiv-id": "1304.1864v3", 
    "author": "Paolo Bientinesi", 
    "publish": "2013-04-06T08:14:25Z", 
    "summary": "The real symmetric tridiagonal eigenproblem is of outstanding importance in\nnumerical computations; it arises frequently as part of eigensolvers for\nstandard and generalized dense Hermitian eigenproblems that are based on a\nreduction to tridiagonal form. For its solution, the algorithm of Multiple\nRelatively Robust Representations (MRRR) is among the fastest methods. Although\nfast, the solvers based on MRRR do not deliver the same accuracy as competing\nmethods like Divide & Conquer or the QR algorithm. In this paper, we\ndemonstrate that the use of mixed precisions leads to improved accuracy of\nMRRR-based eigensolvers with limited or no performance penalty. As a result, we\nobtain eigensolvers that are not only equally or more accurate than the best\navailable methods, but also -in most circumstances- faster and more scalable\nthan the competition."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1304.5546v1", 
    "title": "Solving Wave Equations on Unstructured Geometries", 
    "arxiv-id": "1304.5546v1", 
    "author": "Jan S. Hesthaven", 
    "publish": "2013-04-19T21:07:10Z", 
    "summary": "Waves are all around us--be it in the form of sound, electromagnetic\nradiation, water waves, or earthquakes. Their study is an important basic tool\nacross engineering and science disciplines. Every wave solver serving the\ncomputational study of waves meets a trade-off of two figures of merit--its\ncomputational speed and its accuracy. Discontinuous Galerkin (DG) methods fall\non the high-accuracy end of this spectrum. Fortuitously, their computational\nstructure is so ideally suited to GPUs that they also achieve very high\ncomputational speeds. In other words, the use of DG methods on GPUs\nsignificantly lowers the cost of obtaining accurate solutions. This article\naims to give the reader an easy on-ramp to the use of this technology, based on\na sample implementation which demonstrates a highly accurate, GPU-capable,\nreal-time visualizing finite element solver in about 1500 lines of code."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1304.6782v2", 
    "title": "Minimal Residual Methods for Complex Symmetric, Skew Symmetric, and Skew   Hermitian Systems", 
    "arxiv-id": "1304.6782v2", 
    "author": "Choi", 
    "publish": "2013-04-25T01:21:48Z", 
    "summary": "While there is no lack of efficient Krylov subspace solvers for Hermitian\nsystems, there are few for complex symmetric, skew symmetric, or skew Hermitian\nsystems, which are increasingly important in modern applications including\nquantum dynamics, electromagnetics, and power systems. For a large consistent\ncomplex symmetric system, one may apply a non-Hermitian Krylov subspace method\ndisregarding the symmetry of $A$, or a Hermitian Krylov solver on the\nequivalent normal equation or an augmented system twice the original dimension.\nThese have the disadvantages of increasing either memory, conditioning, or\ncomputational costs. An exception is a special version of QMR by Freund (1992),\nbut that may be affected by non-benign breakdowns unless look-ahead is\nimplemented; furthermore, it is designed for only consistent and nonsingular\nproblems. For skew symmetric systems, Greif and Varah (2009) adapted CG for\nnonsingular skew symmetric linear systems that are necessarily and\nrestrictively of even order.\n  We extend the symmetric and Hermitian algorithms MINRES and MINRES-QLP by\nChoi, Paige and Saunders (2011) to complex symmetric, skew symmetric, and skew\nHermitian systems. In particular, MINRES-QLP uses a rank-revealing QLP\ndecomposition of the tridiagonal matrix from a three-term recurrent\ncomplex-symmetric Lanczos process. Whether the systems are real or complex,\nsingular or invertible, compatible or inconsistent, MINRES-QLP computes the\nunique minimum-length, i.e., pseudoinverse, solutions. It is a significant\nextension of MINRES by Paige and Saunders (1975) with enhanced stability and\ncapability."
},{
    "category": "math.NA", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1304.7049v1", 
    "title": "Subspace-preserving sparsification of matrices with minimal perturbation   to the near null-space. Part I: Basics", 
    "arxiv-id": "1304.7049v1", 
    "author": "Chetan Jhurani", 
    "publish": "2013-04-26T01:43:09Z", 
    "summary": "This is the first of two papers to describe a matrix sparsification algorithm\nthat takes a general real or complex matrix as input and produces a sparse\noutput matrix of the same size. The non-zero entries in the output are chosen\nto minimize changes to the singular values and singular vectors corresponding\nto the near null-space of the input. The output matrix is constrained to\npreserve left and right null-spaces exactly. The sparsity pattern of the output\nmatrix is automatically determined or can be given as input.\n  If the input matrix belongs to a common matrix subspace, we prove that the\ncomputed sparse matrix belongs to the same subspace. This works without\nimposing explicit constraints pertaining to the subspace. This property holds\nfor the subspaces of Hermitian, complex-symmetric, Hamiltonian, circulant,\ncentrosymmetric, and persymmetric matrices, and for each of the skew\ncounterparts.\n  Applications of our method include computation of reusable sparse\npreconditioning matrices for reliable and efficient solution of high-order\nfinite element systems. The second paper in this series describes our\nopen-source implementation, and presents further technical details."
},{
    "category": "math.NA", 
    "doi": "10.1137/130907215", 
    "link": "http://arxiv.org/pdf/1304.7050v1", 
    "title": "Subspace-preserving sparsification of matrices with minimal perturbation   to the near null-space. Part II: Approximation and Implementation", 
    "arxiv-id": "1304.7050v1", 
    "author": "Chetan Jhurani", 
    "publish": "2013-04-26T01:44:09Z", 
    "summary": "This is the second of two papers to describe a matrix sparsification\nalgorithm that takes a general real or complex matrix as input and produces a\nsparse output matrix of the same size. The first paper presented the original\nalgorithm, its features, and theoretical results.\n  Since the output of this sparsification algorithm is a matrix rather than a\nvector, it can be costly in memory and run-time if an implementation does not\nexploit the structural properties of the algorithm and the matrix. Here we show\nhow to modify the original algorithm to increase its efficiency. This is\npossible by computing an approximation to the exact result. We introduce extra\nconstraints that are automatically determined based on the input matrix. This\naddition reduces the number of unknown degrees of freedom but still preserves\nmany matrix subspaces. We also describe our open-source library that implements\nthis sparsification algorithm and has interfaces in C++, C, and MATLAB."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.114", 
    "link": "http://arxiv.org/pdf/1304.7123v1", 
    "title": "Proceedings International Workshop on the ACL2 Theorem Prover and its   Applications", 
    "arxiv-id": "1304.7123v1", 
    "author": "Jared Davis", 
    "publish": "2013-04-26T10:59:21Z", 
    "summary": "This volume contains the proceedings of the Eleventh International Workshop\non the ACL2 Theorem Prover and its Applications, held on May 30 and 31, 2013,\nin Laramie, Wyoming, USA.\n  ACL2 is an industrial-strength automated reasoning system, the latest in the\nBoyer-Moore family of theorem provers. The ACL2 workshop is the major technical\nforum for users of the ACL2 theorem proving system to present research on the\nprover and its applications.\n  This year's workshop received 15 submissions covering a wide range of\napplications, libraries, prover enhancements, interfaces, and experience\nreports. 11 papers were selected by the program committee for presentation at\nthe workshop."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-39320-4_9", 
    "link": "http://arxiv.org/pdf/1304.7223v3", 
    "title": "Understanding Branch Cuts of Expressions", 
    "arxiv-id": "1304.7223v3", 
    "author": "David Wilson", 
    "publish": "2013-04-26T16:46:48Z", 
    "summary": "We assume some standard choices for the branch cuts of a group of functions\nand consider the problem of then calculating the branch cuts of expressions\ninvolving those functions. Typical examples include the addition formulae for\ninverse trigonometric functions. Understanding these cuts is essential for\nworking with the single-valued counterparts, the common approach to encoding\nmulti-valued functions in computer algebra systems. While the defining choices\nare usually simple (typically portions of either the real or imaginary axes)\nthe cuts induced by the expression may be surprisingly complicated. We have\nmade explicit and implemented techniques for calculating the cuts in the\ncomputer algebra programme Maple. We discuss the issues raised, classifying the\ndifferent cuts produced. The techniques have been gathered in the BranchCuts\npackage, along with tools for visualising the cuts. The package is included in\nMaple 17 as part of the FunctionAdvisor tool."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.114.8", 
    "link": "http://arxiv.org/pdf/1304.7861v1", 
    "title": "Verified AIG Algorithms in ACL2", 
    "arxiv-id": "1304.7861v1", 
    "author": "Sol Swords", 
    "publish": "2013-04-30T04:14:44Z", 
    "summary": "And-Inverter Graphs (AIGs) are a popular way to represent Boolean functions\n(like circuits). AIG simplification algorithms can dramatically reduce an AIG,\nand play an important role in modern hardware verification tools like\nequivalence checkers. In practice, these tricky algorithms are implemented with\noptimized C or C++ routines with no guarantee of correctness. Meanwhile, many\ninteractive theorem provers can now employ SAT or SMT solvers to automatically\nsolve finite goals, but no theorem prover makes use of these advanced,\nAIG-based approaches.\n  We have developed two ways to represent AIGs within the ACL2 theorem prover.\nOne representation, Hons-AIGs, is especially convenient to use and reason\nabout. The other, Aignet, is the opposite; it is styled after modern AIG\npackages and allows for efficient algorithms. We have implemented functions for\nconverting between these representations, random vector simulation, conversion\nto CNF, etc., and developed reasoning strategies for verifying these\nalgorithms.\n  Aside from these contributions towards verifying AIG algorithms, this work\nhas an immediate, practical benefit for ACL2 users who are using GL to\nbit-blast finite ACL2 theorems: they can now optionally trust an off-the-shelf\nSAT solver to carry out the proof, instead of using the built-in BDD package.\nLooking to the future, it is a first step toward implementing verified AIG\nsimplification algorithms that might further improve GL performance."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.114.8", 
    "link": "http://arxiv.org/pdf/1305.3215v1", 
    "title": "A computer algebra user interface manifesto", 
    "arxiv-id": "1305.3215v1", 
    "author": "David R. Stoutemyer", 
    "publish": "2013-05-14T17:21:29Z", 
    "summary": "Many computer algebra systems have more than 1000 built-in functions, making\nexpertise difficult. Using mock dialog boxes, this article describes a proposed\ninteractive general-purpose wizard for organizing optional transformations and\nallowing easy fine grain control over the form of the result even by amateurs.\nThis wizard integrates ideas including:\n  * flexible subexpression selection;\n  * complete control over the ordering of variables and commutative operands,\nwith well-chosen defaults;\n  * interleaving the choice of successively less main variables with applicable\nfunction choices to provide detailed control without incurring a combinatorial\nnumber of applicable alternatives at any one level;\n  * quick applicability tests to reduce the listing of inapplicable\ntransformations;\n  * using an organizing principle to order the alternatives in a helpful\nmanner;\n  * labeling quickly-computed alternatives in dialog boxes with a preview of\ntheir results,\n  * using ellipsis elisions if necessary or helpful;\n  * allowing the user to retreat from a sequence of choices to explore other\nbranches of the tree of alternatives or to return quickly to branches already\nvisited;\n  * allowing the user to accumulate more than one of the alternative forms;\n  * integrating direct manipulation into the wizard; and\n  * supporting not only the usual input-result pair mode, but also the useful\nalternative derivational and in situ replacement modes in a unified window."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.114.8", 
    "link": "http://arxiv.org/pdf/1305.4452v3", 
    "title": "PetIGA: A Framework for High-Performance Isogeometric Analysis", 
    "arxiv-id": "1305.4452v3", 
    "author": "V. M. Calo", 
    "publish": "2013-05-20T07:26:33Z", 
    "summary": "We present PetIGA, a code framework to approximate the solution of partial\ndifferential equations using isogeometric analysis. PetIGA can be used to\nassemble matrices and vectors which come from a Galerkin weak form, discretized\nwith Non-Uniform Rational B-spline basis functions. We base our framework on\nPETSc, a high-performance library for the scalable solution of partial\ndifferential equations, which simplifies the development of large-scale\nscientific codes, provides a rich environment for prototyping, and separates\nparallelism from algorithm choice. We describe the implementation of PetIGA,\nand exemplify its use by solving a model nonlinear problem. To illustrate the\nrobustness and flexibility of PetIGA, we solve some challenging nonlinear\npartial differential equations that include problems in both solid and fluid\nmechanics. We show strong scaling results on up to 4096 cores, which confirm\nthe suitability of PetIGA for large scale simulations."
},{
    "category": "stat.CO", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1305.4886v1", 
    "title": "Parallelizing Gaussian Process Calculations in R", 
    "arxiv-id": "1305.4886v1", 
    "author": "Rollin C. Thomas", 
    "publish": "2013-05-21T17:08:54Z", 
    "summary": "We consider parallel computation for Gaussian process calculations to\novercome computational and memory constraints on the size of datasets that can\nbe analyzed. Using a hybrid parallelization approach that uses both threading\n(shared memory) and message-passing (distributed memory), we implement the core\nlinear algebra operations used in spatial statistics and Gaussian process\nregression in an R package called bigGP that relies on C and MPI. The approach\ndivides the matrix into blocks such that the computational load is balanced\nacross processes while communication between processes is limited. The package\nprovides an API enabling R programmers to implement Gaussian process-based\nmethods by using the distributed linear algebra operations without any C or MPI\ncoding. We illustrate the approach and software by analyzing an astrophysics\ndataset with n=67,275 observations."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1305.5710v1", 
    "title": "Formal Mathematics on Display: A Wiki for Flyspeck", 
    "arxiv-id": "1305.5710v1", 
    "author": "Herman Geuvers", 
    "publish": "2013-05-24T12:26:45Z", 
    "summary": "The Agora system is a prototype \"Wiki for Formal Mathematics\", with an aim to\nsupport developing and documenting large formalizations of mathematics in a\nproof assistant. The functions implemented in Agora include in-browser editing,\nstrong AI/ATP proof advice, verification, and HTML rendering. The HTML\nrendering contains hyperlinks and provides on-demand explanation of the proof\nstate for each proof step. In the present paper we show the prototype Flyspeck\nWiki as an instance of Agora for HOL Light formalizations. The wiki can be used\nfor formalizations of mathematics and for writing informal wiki pages about\nmathematics. Such informal pages may contain islands of formal text, which is\nused here for providing an initial cross-linking between Hales's informal\nFlyspeck book, and the formal Flyspeck development.\n  The Agora platform intends to address distributed wiki-style collaboration on\nlarge formalization projects, in particular both the aspect of immediate\nediting, verification and rendering of formal code, and the aspect of gradual\nand mutual refactoring and correspondence of the initial informal text and its\nformalization. Here, we highlight these features within the Flyspeck Wiki."
},{
    "category": "cs.DL", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1306.1036v1", 
    "title": "swMATH - a new information service for mathematical software", 
    "arxiv-id": "1306.1036v1", 
    "author": "Wolfram Sperber", 
    "publish": "2013-06-05T09:53:29Z", 
    "summary": "An information service for mathematical software is presented. Publications\nand software are two closely connected facets of mathematical knowledge. This\nrelation can be used to identify mathematical software and find relevant\ninformation about it. The approach and the state of the art of the information\nservice are described here."
},{
    "category": "cs.LO", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1306.3198v1", 
    "title": "A Universal Machine for Biform Theory Graphs", 
    "arxiv-id": "1306.3198v1", 
    "author": "Florian Rabe", 
    "publish": "2013-06-13T19:06:45Z", 
    "summary": "Broadly speaking, there are two kinds of semantics-aware assistant systems\nfor mathematics: proof assistants express the semantic in logic and emphasize\ndeduction, and computer algebra systems express the semantics in programming\nlanguages and emphasize computation. Combining the complementary strengths of\nboth approaches while mending their complementary weaknesses has been an\nimportant goal of the mechanized mathematics community for some time. We pick\nup on the idea of biform theories and interpret it in the MMTt/OMDoc framework\nwhich introduced the foundations-as-theories approach, and can thus represent\nboth logics and programming languages as theories. This yields a formal,\nmodular framework of biform theory graphs which mixes specifications and\nimplementations sharing the module system and typing information. We present\nautomated knowledge management work flows that interface to existing\nspecification/programming tools and enable an OpenMath Machine, that\noperationalizes biform theories, evaluating expressions by exhaustively\napplying the implementations of the respective operators. We evaluate the new\nbiform framework by adding implementations to the OpenMath standard content\ndictionaries."
},{
    "category": "math.RA", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1306.5526v1", 
    "title": "Programs in C++ for matrix computations in min plus algebra", 
    "arxiv-id": "1306.5526v1", 
    "author": "Gheorghe Ivan", 
    "publish": "2013-06-24T07:33:16Z", 
    "summary": "The main purpose of this paper is to propose six programs in C++ for matrix\ncomputations and solving recurrent equations systems with entries in min plus\nalgebra."
},{
    "category": "stat.CO", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1306.5583v1", 
    "title": "vSMC: Parallel Sequential Monte Carlo in C++", 
    "arxiv-id": "1306.5583v1", 
    "author": "Yan Zhou", 
    "publish": "2013-06-24T11:44:30Z", 
    "summary": "Sequential Monte Carlo is a family of algorithms for sampling from a sequence\nof distributions. Some of these algorithms, such as particle filters, are\nwidely used in the physics and signal processing researches. More recent\ndevelopments have established their application in more general inference\nproblems such as Bayesian modeling.\n  These algorithms have attracted considerable attentions in recent years as\nthey admit natural and scalable parallelizations. However, these algorithms are\nperceived to be difficult to implement. In addition, parallel programming is\noften unfamiliar to many researchers though conceptually appealing, especially\nfor sequential Monte Carlo related fields.\n  A C++ template library is presented for the purpose of implementing general\nsequential Monte Carlo algorithms on parallel hardware. Two examples are\npresented: a simple particle filter and a classic Bayesian modeling problem."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1307.1343v1", 
    "title": "Building Bricks with Bricks, with Mathematica", 
    "arxiv-id": "1307.1343v1", 
    "author": "Daniele Filaretti", 
    "publish": "2013-07-04T14:21:49Z", 
    "summary": "In this work we solve a special case of the problem of building an\nn-dimensional parallelepiped using a given set of n-dimensional\nparallelepipeds. Consider the identity x^3 = x(x-1)(x-2)+3x(x-1+x). For\nsufficiently large x, we associate with x^3 a cube with edges of size x, with\nx(x-1)(x-2) a parallelepiped with edges x, x-1, x-2, with 3x(x-1+x) three\nparallelepipeds of edges x, x-1, 1, and with x a parallelepiped of edges x, 1,\n1. The problem we takle is the actual construction of the cube using the given\nparallelepipeds. In [DDNP90] it was shown how to solve this specific problem\nand all similar instances in which a (monic) polynomial is expressed as a\nlinear combination of a persistent basis. That is to say a sequence of\npolynomials q_0 = 1, and q_k(x) = q_{k-1}(x)(x-r_k) for k > 0. Here, after\n[Fil10], we deal with a multivariate version of the problem with respect to a\nbasis of polynomials of the same degree (binomial basis). We show that it is\npossible to build the parallelepiped associated with a multivariate polynomial\nP(x_1, ..., x_n)=(x_1- s_1)...(x_n-s_n) with integer roots, using the\nparallelepipeds described by the elements of the basis. We provide an algorithm\nin Mathematica to solve the problem for each n. Moreover, for n = 2, 3, 4 (in\nthe latter case, only when a projection is possible) we use Mathematica to\ndisplay a step by step construction of the parallelepiped P(x1,...,x_n)."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1307.1348v1", 
    "title": "Making simple proofs simpler", 
    "arxiv-id": "1307.1348v1", 
    "author": "Corrado Monti", 
    "publish": "2013-07-04T14:29:01Z", 
    "summary": "An open partition \\pi{} [Cod09a, Cod09b] of a tree T is a partition of the\nvertices of T with the property that, for each block B of \\pi, the upset of B\nis a union of blocks of \\pi. This paper deals with the number, NP(n), of open\npartitions of the tree, V_n, made of two chains with n points each, that share\nthe root."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1307.1352v1", 
    "title": "A Mathematica package to cope with partially ordered sets", 
    "arxiv-id": "1307.1352v1", 
    "author": "Pietro Codara", 
    "publish": "2013-07-04T14:33:31Z", 
    "summary": "Mathematica offers, by way of the package Combinatorics, many useful\nfunctions to work on graphs and ordered structures, but none of these functions\nwas specific enough to meet the needs of our research group. Moreover, the\nexisting functions are not always helpful when one has to work on new concepts.\n  In this paper we present a package of features developed in Mathematica which\nwe consider particularly useful for the study of certain categories of\npartially ordered sets. Among the features offered, the package includes: (1)\nsome basic features to treat partially ordered sets; (2) the ability to\nenumerate, create, and display monotone and regular partitions of partially\nordered sets; (3) the capability of constructing the lattices of partitions of\na poset, and of doing some useful computations on these structures; (4) the\npossibility of computing products and coproducts in the category of partially\nordered sets and monotone maps; (5) the possibility of computing products and\ncoproducts in the category of forests (disjoint union of trees) and open maps\n(cf. [DM06] for the product between forests)."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v063.i10", 
    "link": "http://arxiv.org/pdf/1307.2100v1", 
    "title": "Towards an Efficient Use of the BLAS Library for Multilinear Tensor   Contractions", 
    "arxiv-id": "1307.2100v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2013-07-08T14:12:46Z", 
    "summary": "Mathematical operators whose transformation rules constitute the building\nblocks of a multi-linear algebra are widely used in physics and engineering\napplications where they are very often represented as tensors. In the last\ncentury, thanks to the advances in tensor calculus, it was possible to uncover\nnew research fields and make remarkable progress in the existing ones, from\nelectromagnetism to the dynamics of fluids and from the mechanics of rigid\nbodies to quantum mechanics of many atoms. By now, the formal mathematical and\ngeometrical properties of tensors are well defined and understood; conversely,\nin the context of scientific and high-performance computing, many tensor-\nrelated problems are still open. In this paper, we address the problem of\nefficiently computing contractions among two tensors of arbitrary dimension by\nusing kernels from the highly optimized BLAS library. In particular, we\nestablish precise conditions to determine if and when GEMM, the kernel for\nmatrix products, can be used. Such conditions take into consideration both the\nnature of the operation and the storage scheme of the tensors, and induce a\nclassification of the contractions into three groups. For each group, we\nprovide a recipe to guide the users towards the most effective use of BLAS."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2013.04.007", 
    "link": "http://arxiv.org/pdf/1307.5866v1", 
    "title": "RNGSSELIB: Program library for random number generation. More   generators, parallel streams of random numbers and Fortran compatibility", 
    "arxiv-id": "1307.5866v1", 
    "author": "L. N. Shchur", 
    "publish": "2013-07-22T20:15:34Z", 
    "summary": "In this update, we present the new version of the random number generator\n(RNG) library RNGSSELIB, which, in particular, contains fast SSE realizations\nof a number of modern and most reliable generators \\cite{RNGSSELIB1}. The new\nfeatures are: i) Fortran compatibility and examples of using the library in\nFortran; ii) new modern and reliable generators; iii) the abilities to jump\nahead inside RNG sequence and to initialize up to $10^{19}$ independent random\nnumber streams with block splitting method."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2014.01.007", 
    "link": "http://arxiv.org/pdf/1307.5869v2", 
    "title": "PRAND: GPU accelerated parallel random number generation library: Using   most reliable algorithms and applying parallelism of modern GPUs and CPUs", 
    "arxiv-id": "1307.5869v2", 
    "author": "L. N. Shchur", 
    "publish": "2013-07-22T20:21:20Z", 
    "summary": "The library PRAND for pseudorandom number generation for modern CPUs and GPUs\nis presented. It contains both single-threaded and multi-threaded realizations\nof a number of modern and most reliable generators recently proposed and\nstudied in [1,2,3,4,5] and the efficient SIMD realizations proposed in [6]. One\nof the useful features for using PRAND in parallel simulations is the ability\nto initialize up to $10^{19}$ independent streams. Using massive parallelism of\nmodern GPUs and SIMD parallelism of modern CPUs substantially improves\nperformance of the generators."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1307.6209v2", 
    "title": "A unified sparse matrix data format for efficient general sparse   matrix-vector multiply on modern processors with wide SIMD units", 
    "arxiv-id": "1307.6209v2", 
    "author": "Alan R. Bishop", 
    "publish": "2013-07-23T12:50:28Z", 
    "summary": "Sparse matrix-vector multiplication (spMVM) is the most time-consuming kernel\nin many numerical algorithms and has been studied extensively on all modern\nprocessor and accelerator architectures. However, the optimal sparse matrix\ndata storage format is highly hardware-specific, which could become an obstacle\nwhen using heterogeneous systems. Also, it is as yet unclear how the wide\nsingle instruction multiple data (SIMD) units in current multi- and many-core\nprocessors should be used most efficiently if there is no structure in the\nsparsity pattern of the matrix. We suggest SELL-C-sigma, a variant of Sliced\nELLPACK, as a SIMD-friendly data format which combines long-standing ideas from\nGeneral Purpose Graphics Processing Units (GPGPUs) and vector computer\nprogramming. We discuss the advantages of SELL-C-sigma compared to established\nformats like Compressed Row Storage (CRS) and ELLPACK and show its suitability\non a variety of hardware platforms (Intel Sandy Bridge, Intel Xeon Phi and\nNvidia Tesla K20) for a wide range of test matrices from different application\nareas. Using appropriate performance models we develop deep insight into the\ndata transfer properties of the SELL-C-sigma spMVM kernel. SELL-C-sigma comes\nwith two tuning parameters whose performance impact across the range of test\nmatrices is studied and for which reasonable choices are proposed. This leads\nto a hardware-independent (\"catch-all\") sparse matrix format, which achieves\nvery high efficiency for all test matrices across all hardware platforms."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1307.7042v1", 
    "title": "Python for education: permutations", 
    "arxiv-id": "1307.7042v1", 
    "author": "Andrzej Kapanowski", 
    "publish": "2013-07-26T14:18:21Z", 
    "summary": "Python implementation of permutations is presented. Three classes are\nintroduced: Perm for permutations, Group for permutation groups, and PermError\nto report any errors for both classes. The class Perm is based on Python\ndictionaries and utilize cycle notation. The methods of calculation for the\nperm order, parity, ranking and unranking are given. A random permutation\ngeneration is also shown. The class Group is very simple and it is also based\non dictionaries. It is mainly the presentation of the permutation groups\ninterface with methods for the group order, subgroups (normalizer, centralizer,\ncenter, stabilizer), orbits, and several tests. The corresponding Python code\nis contained in the modules perms and groups."
},{
    "category": "math.AG", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1310.3297v1", 
    "title": "Bertini for Macaulay2", 
    "arxiv-id": "1310.3297v1", 
    "author": "Jose Israel Rodriguez", 
    "publish": "2013-10-11T21:17:59Z", 
    "summary": "Numerical algebraic geometry is the field of computational mathematics\nconcerning the numerical solution of polynomial systems of equations. Bertini,\na popular software package for computational applications of this field,\nincludes implementations of a variety of algorithms based on polynomial\nhomotopy continuation. The Macaulay2 package Bertini.m2 provides an interface\nto Bertini, making it possible to access the core run modes of Bertini in\nMacaulay2. With these run modes, users can find approximate solutions to\nzero-dimensional systems and positive-dimensional systems, test numerically\nwhether a point lies on a variety, sample numerically from a variety, and\nperform parameter homotopy runs."
},{
    "category": "cs.CG", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1312.2873v2", 
    "title": "Efficient Random-Walk Methods for Approximating Polytope Volume", 
    "arxiv-id": "1312.2873v2", 
    "author": "Vissarion Fisikopoulos", 
    "publish": "2013-12-10T16:58:04Z", 
    "summary": "We experimentally study the fundamental problem of computing the volume of a\nconvex polytope given as an intersection of linear inequalities. We implement\nand evaluate practical randomized algorithms for accurately approximating the\npolytope's volume in high dimensions (e.g. one hundred). To carry out this\nefficiently we experimentally correlate the effect of parameters, such as\nrandom walk length and number of sample points, on accuracy and runtime.\nMoreover, we exploit the problem's geometry by implementing an iterative\nrounding procedure, computing partial generations of random points and\ndesigning fast polytope boundary oracles. Our publicly available code is\nsignificantly faster than exact computation and more accurate than existing\napproximation methods. We provide volume approximations for the Birkhoff\npolytopes B_11,...,B_15, whereas exact methods have only computed that of B_10."
},{
    "category": "cs.SC", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1312.3270v2", 
    "title": "Misfortunes of a mathematicians' trio using Computer Algebra Systems:   Can we trust?", 
    "arxiv-id": "1312.3270v2", 
    "author": "Juan L. Varona", 
    "publish": "2013-12-11T18:25:46Z", 
    "summary": "Computer algebra systems are a great help for mathematical research but\nsometimes unexpected errors in the software can also badly affect it. As an\nexample, we show how we have detected an error of Mathematica computing\ndeterminants of matrices of integer numbers: not only it computes the\ndeterminants wrongly, but also it produces different results if one evaluates\nthe same determinant twice."
},{
    "category": "cs.DC", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1401.0763v1", 
    "title": "A Study of Successive Over-relaxation Method Parallelization Over Modern   HPC Languages", 
    "arxiv-id": "1401.0763v1", 
    "author": "Sparsh Mittal", 
    "publish": "2014-01-04T01:31:48Z", 
    "summary": "Successive over-relaxation (SOR) is a computationally intensive, yet\nextremely important iterative solver for solving linear systems. Due to recent\ntrends of exponential growth in amount of data generated and increasing problem\nsizes, serial platforms have proved to be insufficient in providing the\nrequired computational power. In this paper, we present parallel\nimplementations of red-black SOR method using three modern programming\nlanguages namely Chapel, D and Go. We employ SOR method for solving 2D\nsteady-state heat conduction problem. We discuss the optimizations incorporated\nand the features of these languages which are crucial for improving the program\nperformance. Experiments have been performed using 2, 4, and 8 threads and\nperformance results are compared with serial execution. The analysis of results\nprovides important insights into working of SOR method."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1401.1290v1", 
    "title": "Program Verification of Numerical Computation", 
    "arxiv-id": "1401.1290v1", 
    "author": "Garry Pantelis", 
    "publish": "2014-01-07T07:07:49Z", 
    "summary": "These notes outline a formal method for program verification of numerical\ncomputation. It forms the basis of the software package VPC in its initial\nphase of development. Much of the style of presentation is in the form of notes\nthat outline the definitions and rules upon which VPC is based. The initial\nmotivation of this project was to address some practical issues of computation,\nespecially of numerically intensive programs that are commonplace in computer\nmodels. The project evolved into a wider area for program construction as\nproofs leading to a model of inference in a more general sense. Some basic\nresults of machine arithmetic are derived as a demonstration of VPC."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1401.2248v4", 
    "title": "Boolean Functions, Quantum Gates, Hamilton Operators, Spin Systems and   Computer Algebra", 
    "arxiv-id": "1401.2248v4", 
    "author": "Willi-Hans Steeb", 
    "publish": "2014-01-10T08:12:46Z", 
    "summary": "We describe the construction of quantum gates (unitary operators) from\nboolean functions and give a number of applications. Both non-reversible and\nreversible boolean functions are considered. The construction of the Hamilton\noperator for a quantum gate is also described with the Hamilton operator\nexpressed as spin system. Computer algebra implementations are provided."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1403.1140v1", 
    "title": "Matrix Methods for Solving Algebraic Systems", 
    "arxiv-id": "1403.1140v1", 
    "author": "Ioannis Z. Emiris", 
    "publish": "2014-03-05T14:14:12Z", 
    "summary": "We present our public-domain software for the following tasks in sparse (or\ntoric) elimination theory, given a well-constrained polynomial system. First, C\ncode for computing the mixed volume of the system. Second, Maple code for\ndefining an overconstrained system and constructing a Sylvester-type matrix of\nits sparse resultant. Third, C code for a Sylvester-type matrix of the sparse\nresultant and a superset of all common roots of the initial well-constrained\nsystem by computing the eigen-decomposition of a square matrix obtained from\nthe resultant matrix. We conclude with experiments in computing molecular\nconformations."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130930352", 
    "link": "http://arxiv.org/pdf/1403.2630v1", 
    "title": "A SageTeX Hypermatrix Algebra Package", 
    "arxiv-id": "1403.2630v1", 
    "author": "Yuval Filmus", 
    "publish": "2014-03-11T16:23:23Z", 
    "summary": "We describe here a rudimentary sage implementation of the Bhattacharya-Mesner\nhypermatrix algebra package."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00180-014-0492-3", 
    "link": "http://arxiv.org/pdf/1403.7645v1", 
    "title": "Using RngStreams for Parallel Random Number Generation in C++ and R", 
    "arxiv-id": "1403.7645v1", 
    "author": "Dennis Young", 
    "publish": "2014-03-29T16:30:21Z", 
    "summary": "The RngStreams software package provides one viable solution to the problem\nof creating independent random number streams for simulations in parallel\nprocessing environments. Techniques are presented for effectively using\nRngStreams with C++ programs that are parallelized via OpenMP or MPI. Ways to\naccess the backbone generator from RngStreams in R through the parallel and\nrstream packages are also described. The ideas in the paper are illustrated\nwith both a simple running example and a Monte Carlo integration application."
},{
    "category": "hep-lat", 
    "doi": "10.1007/s00180-014-0492-3", 
    "link": "http://arxiv.org/pdf/1405.0700v1", 
    "title": "PLQCD library for Lattice QCD on multi-core machines", 
    "arxiv-id": "1405.0700v1", 
    "author": "N. Papadopoulou", 
    "publish": "2014-05-04T14:12:53Z", 
    "summary": "PLQCD is a stand-alone software library developed under PRACE for lattice\nQCD. It provides an implementation of the Dirac operator for Wilson type\nfermions and few efficient linear solvers. The library is optimized for\nmulti-core machines using a hybrid parallelization with OpenMP+MPI. The main\nobjectives of the library is to provide a scalable implementation of the Dirac\noperator for efficient computation of the quark propagator. In this\ncontribution, a description of the PLQCD library is given together with some\nbenchmark results."
},{
    "category": "cs.LO", 
    "doi": "10.1007/s00180-014-0492-3", 
    "link": "http://arxiv.org/pdf/1405.3426v1", 
    "title": "Hipster: Integrating Theory Exploration in a Proof Assistant", 
    "arxiv-id": "1405.3426v1", 
    "author": "Koen Claessen", 
    "publish": "2014-05-14T09:43:09Z", 
    "summary": "This paper describes Hipster, a system integrating theory exploration with\nthe proof assistant Isabelle/HOL. Theory exploration is a technique for\nautomatically discovering new interesting lemmas in a given theory development.\nHipster can be used in two main modes. The first is exploratory mode, used for\nautomatically generating basic lemmas about a given set of datatypes and\nfunctions in a new theory development. The second is proof mode, used in a\nparticular proof attempt, trying to discover the missing lemmas which would\nallow the current goal to be proved. Hipster's proof mode complements and\nboosts existing proof automation techniques that rely on automatically\nselecting existing lemmas, by inventing new lemmas that need induction to be\nproved. We show example uses of both modes."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/s00180-014-0492-3", 
    "link": "http://arxiv.org/pdf/1405.3630v1", 
    "title": "DDscat.C++ User and programmer guide", 
    "arxiv-id": "1405.3630v1", 
    "author": "Vasyl Choliy", 
    "publish": "2013-12-20T10:29:47Z", 
    "summary": "DDscat.C++ 7.3.0 is a freely available open-source C++ software package\napplying the \"discrete dipole approximation\" (DDA) to calculate scattering and\nabsorption of electromagnetic waves by targets with arbitrary geometries and a\ncomplex refractive index. DDscat.C++ is a clone of well known DDscat Fortran-90\nsoftware. We refer to DDscat as to the parent code in this document. Versions\n7.3.0 of both codes have the identical functionality but the quite different\nimplementation. Started as a teaching project, the DDscat.C++ code differs from\nthe parent code DDscat in programming techniques and features, essential for\nC++ but quite seldom in Fortran.\n  As DDscat.C++ in its current version is just a clone, usage of DDscat.C++ for\nelectromagnetic calculations is the same as of DDscat. Please, refer to \"User\nGuide for the Discrete Dipole Approximation Code DDSCAT 7.3\" to start using the\ncode(s).\n  This document consists of two parts. In the first part we present Quick start\nguide for users who want to begin to use the code. Only differencies between\nDDscat.C++ and DDscat are explained. The second part of the document explains\nprogramming tips for the persons who want to change the code, to add the\nfunctionality or help the author with code refactoring and debugging."
},{
    "category": "cs.MS", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1405.4644v1", 
    "title": "Changing Computing Paradigms Towards Power Efficiency", 
    "arxiv-id": "1405.4644v1", 
    "author": "Alessandro Curioni", 
    "publish": "2014-05-19T09:03:58Z", 
    "summary": "Power awareness is fast becoming immensely important in computing, ranging\nfrom the traditional High Performance Computing applications, to the new\ngeneration of data centric workloads.\n  In this work we describe our efforts towards a power efficient computing\nparadigm that combines low precision and high precision arithmetic. We showcase\nour ideas for the widely used kernel of solving systems of linear equations\nthat finds numerous applications in scientific and engineering disciplines as\nwell as in large scale data analytics, statistics and machine learning.\n  Towards this goal we developed tools for the seamless power profiling of\napplications at a fine grain level. In addition, we verify here previous work\non post FLOPS/Watt metrics and show that these can shed much more light in the\npower/energy profile of important applications."
},{
    "category": "physics.acc-ph", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1405.4921v1", 
    "title": "Zgoubi: A startup guide for the complete beginner", 
    "arxiv-id": "1405.4921v1", 
    "author": "Kai Hock", 
    "publish": "2014-05-19T23:53:58Z", 
    "summary": "Zgoubi is a code which can be used to model accelerators and beam lines,\ncomprised of magnetic and electrostatic elements. It has been extensively\ndeveloped since the mid-1980s to include circular accelerators and related beam\nphysics. It has been made freely available by its author on a code development\nsite, including a Users' Guide, a data treatment/graphic interfacing tool, and\nmany examples. This startup guide give directions to install the required\nelements onto a Windows or Unix system to enable running of the Zgoubi code\nwith examples of code written to model the EMMA accelerator based at the\nCockcroft Institute."
},{
    "category": "cs.MS", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1405.5956v1", 
    "title": "Realms: A Structure for Consolidating Knowledge about Mathematical   Theories", 
    "arxiv-id": "1405.5956v1", 
    "author": "Michael Kohlhase", 
    "publish": "2014-05-23T03:05:42Z", 
    "summary": "Since there are different ways of axiomatizing and developing a mathematical\ntheory, knowledge about a such a theory may reside in many places and in many\nforms within a library of formalized mathematics. We introduce the notion of a\nrealm as a structure for consolidating knowledge about a mathematical theory. A\nrealm contains several axiomatizations of a theory that are separately\ndeveloped. Views interconnect these developments and establish that the\naxiomatizations are equivalent in the sense of being mutually interpretable. A\nrealm also contains an external interface that is convenient for users of the\nlibrary who want to apply the concepts and facts of the theory without delving\ninto the details of how the concepts and facts were developed. We illustrate\nthe utility of realms through a series of examples. We also give an outline of\nthe mechanisms that are needed to create and maintain realms."
},{
    "category": "cs.MS", 
    "doi": "10.1098/rsta.2013.0278", 
    "link": "http://arxiv.org/pdf/1407.0039v1", 
    "title": "Integer formula encoding SageTeX package", 
    "arxiv-id": "1407.0039v1", 
    "author": "Edinah K. Gnang", 
    "publish": "2014-06-27T00:13:14Z", 
    "summary": "The paper describes a SageTeX implementation of an integer encoding\nprocedures."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.cpc.2016.04.015", 
    "link": "http://arxiv.org/pdf/1407.3189v1", 
    "title": "A modern resistive magnetohydrodynamics solver using C++ and the Boost   library", 
    "arxiv-id": "1407.3189v1", 
    "author": "Lukas Einkemmer", 
    "publish": "2014-07-11T15:05:33Z", 
    "summary": "In this paper we describe the implementation of our C++ resistive\nmagnetohydrodynamics solver. The framework developed facilitates the separation\nof the code implementing the specific numerical method and the physical model,\non the one hand, from the handling of boundary conditions and the management of\nthe computational domain, on the other hand. In particular, this will allow us\nto use finite difference stencils which are only defined in the interior of the\ndomain (the boundary conditions are handled automatically). We will discuss\nthis and other design considerations and their impact on performance in some\ndetail. In addition, we provide a documentation of the code developed and\ndemonstrate that a performance comparable to Fortran can be achieved, while\nstill maintaining a maximum of code readability and extensibility."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1407.6245v1", 
    "title": "scikit-image: Image processing in Python", 
    "arxiv-id": "1407.6245v1", 
    "author": "the scikit-image contributors", 
    "publish": "2014-07-23T14:55:21Z", 
    "summary": "scikit-image is an image processing library that implements algorithms and\nutilities for use in research, education and industry applications. It is\nreleased under the liberal \"Modified BSD\" open source license, provides a\nwell-documented API in the Python programming language, and is developed by an\nactive, international team of collaborators. In this paper we highlight the\nadvantages of open source to achieve the goals of the scikit-image library, and\nwe showcase several real-world image processing applications that use\nscikit-image."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1407.6954v3", 
    "title": "The DUNE-ALUGrid Module", 
    "arxiv-id": "1407.6954v3", 
    "author": "Martin Nolte", 
    "publish": "2014-07-25T16:12:26Z", 
    "summary": "In this paper we present the new DUNE-ALUGrid module. This module contains a\nmajor overhaul of the sources from the ALUgrid library and the binding to the\nDUNE software framework. The main changes include user defined load balancing,\nparallel grid construction, and an redesign of the 2d grid which can now also\nbe used for parallel computations. In addition many improvements have been\nintroduced into the code to increase the parallel efficiency and to decrease\nthe memory footprint.\n  The original ALUGrid library is widely used within the DUNE community due to\nits good parallel performance for problems requiring local adaptivity and\ndynamic load balancing. Therefore, this new model will benefit a number of DUNE\nusers. In addition we have added features to increase the range of problems for\nwhich the grid manager can be used, for example, introducing a 3d tetrahedral\ngrid using a parallel newest vertex bisection algorithm for conforming grid\nrefinement. In this paper we will discuss the new features, extensions to the\nDUNE interface, and explain for various examples how the code is used in\nparallel environments."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.0564v1", 
    "title": "Automatic Generation of Loop-Invariants for Matrix Operations", 
    "arxiv-id": "1410.0564v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2014-10-02T14:27:17Z", 
    "summary": "In recent years it has been shown that for many linear algebra operations it\nis possible to create families of algorithms following a very systematic\nprocedure. We do not refer to the fine tuning of a known algorithm, but to a\nmethodology for the actual generation of both algorithms and routines to solve\na given target matrix equation. Although systematic, the methodology relies on\ncomplex algebraic manipulations and non-obvious pattern matching, making the\nprocedure challenging to be performed by hand, our goal is the development of a\nfully automated system that from the sole description of a target equation\ncreates multiple algorithms and routines. We present CL1ck, a symbolic system\nwritten in Mathematica, that starts with an equation, decomposes it into\nmultiple equations, and returns a set of loop-invariants for the algorithms --\nyet to be generated -- that will solve the equation. In a successive step each\nloop-invariant is then mapped to its corresponding algorithm and routine. For a\nlarge class of equations, the methodology generates known algorithms as well as\nmany previously unknown ones. Most interestingly, the methodology unifies\nalgorithms traditionally developed in isolation. As an example, the five well\nknown algorithms for the LU factorization are for the first time unified under\na common root."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.0567v1", 
    "title": "Knowledge-Based Automatic Generation of Partitioned Matrix Expressions", 
    "arxiv-id": "1410.0567v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2014-10-02T14:33:43Z", 
    "summary": "In a series of papers it has been shown that for many linear algebra\noperations it is possible to generate families of algorithms by following a\nsystematic procedure. Although powerful, such a methodology involves complex\nalgebraic manipulation, symbolic computations and pattern matching, making the\ngeneration a process challenging to be performed by hand. We aim for a fully\nautomated system that from the sole description of a target operation creates\nmultiple algorithms without any human intervention. Our approach consists of\nthree main stages. The first stage yields the core object for the entire\nprocess, the Partitioned Matrix Expression (PME), which establishes how the\ntarget problem may be decomposed in terms of simpler sub-problems. In the\nsecond stage the PME is inspected to identify predicates, the Loop-Invariants,\nto be used to set up the skeleton of a family of proofs of correctness. In the\nthird and last stage the actual algorithms are constructed so that each of them\nsatisfies its corresponding proof of correctness. In this paper we focus on the\nfirst stage of the process, the automatic generation of Partitioned Matrix\nExpressions. In particular, we discuss the steps leading to a PME and the\nknowledge necessary for a symbolic system to perform such steps. We also\nintroduce Cl1ck, a prototype system written in Mathematica that generates PMEs\nautomatically."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.1764v1", 
    "title": "Chemora: A PDE Solving Framework for Modern HPC Architectures", 
    "arxiv-id": "1410.1764v1", 
    "author": "Frank L\u00f6ffler", 
    "publish": "2014-10-03T20:53:26Z", 
    "summary": "Modern HPC architectures consist of heterogeneous multi-core, many-node\nsystems with deep memory hierarchies. Modern applications employ ever more\nadvanced discretisation methods to study multi-physics problems. Developing\nsuch applications that explore cutting-edge physics on cutting-edge HPC systems\nhas become a complex task that requires significant HPC knowledge and\nexperience. Unfortunately, this combined knowledge is currently out of reach\nfor all but a few groups of application developers.\n  Chemora is a framework for solving systems of Partial Differential Equations\n(PDEs) that targets modern HPC architectures. Chemora is based on Cactus, which\nsees prominent usage in the computational relativistic astrophysics community.\nIn Chemora, PDEs are expressed either in a high-level \\LaTeX-like language or\nin Mathematica. Discretisation stencils are defined separately from equations,\nand can include Finite Differences, Discontinuous Galerkin Finite Elements\n(DGFE), Adaptive Mesh Refinement (AMR), and multi-block systems.\n  We use Chemora in the Einstein Toolkit to implement the Einstein Equations on\nCPUs and on accelerators, and study astrophysical systems such as black hole\nbinaries, neutron stars, and core-collapse supernovae."
},{
    "category": "cs.CV", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.5263v2", 
    "title": "Building pattern recognition applications with the SPARE library", 
    "arxiv-id": "1410.5263v2", 
    "author": "Fabio Massimo Frattale Mascioli", 
    "publish": "2014-10-20T13:18:33Z", 
    "summary": "This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.7176v2", 
    "title": "Efficient implementation of elementary functions in the medium-precision   range", 
    "arxiv-id": "1410.7176v2", 
    "author": "Fredrik Johansson", 
    "publish": "2014-10-27T10:35:42Z", 
    "summary": "We describe a new implementation of the elementary transcendental functions\nexp, sin, cos, log and atan for variable precision up to approximately 4096\nbits. Compared to the MPFR library, we achieve a maximum speedup ranging from a\nfactor 3 for cos to 30 for atan. Our implementation uses table-based argument\nreduction together with rectangular splitting to evaluate Taylor series. We\ncollect denominators to reduce the number of divisions in the Taylor series,\nand avoid overhead by doing all multiprecision arithmetic using the mpn layer\nof the GMP library. Our implementation provides rigorous error bounds."
},{
    "category": "astro-ph.IM", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1410.8507v1", 
    "title": "External Use of TOPCAT's Plotting Library", 
    "arxiv-id": "1410.8507v1", 
    "author": "M. B. Taylor", 
    "publish": "2014-10-30T19:29:08Z", 
    "summary": "The table analysis application TOPCAT uses a custom Java plotting library for\nhighly configurable high-performance interactive or exported visualisations in\ntwo and three dimensions. We present here a variety of ways for end users or\napplication developers to make use of this library outside of the TOPCAT\napplication: via the command-line suite STILTS or its Jython variant JyStilts,\nvia a traditional Java API, or by programmatically assigning values to a set of\nparameters in java code or using some form of inter-process communication. The\nlibrary has been built with large datasets in mind; interactive plots scale\nwell up to several million points, and static output to standard graphics\nformats is possible for unlimited sized input data."
},{
    "category": "cs.MM", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1411.2860v1", 
    "title": "Precision-Energy-Throughput Scaling Of Generic Matrix Multiplication and   Convolution Kernels Via Linear Projections", 
    "arxiv-id": "1411.2860v1", 
    "author": "Yiannis Andreopoulos", 
    "publish": "2014-11-11T15:59:35Z", 
    "summary": "Generic matrix multiplication (GEMM) and one-dimensional\nconvolution/cross-correlation (CONV) kernels often constitute the bulk of the\ncompute- and memory-intensive processing within image/audio recognition and\nmatching systems. We propose a novel method to scale the energy and processing\nthroughput of GEMM and CONV kernels for such error-tolerant multimedia\napplications by adjusting the precision of computation. Our technique employs\nlinear projections to the input matrix or signal data during the top-level GEMM\nand CONV blocking and reordering. The GEMM and CONV kernel processing then uses\nthe projected inputs and the results are accumulated to form the final outputs.\nThroughput and energy scaling takes place by changing the number of projections\ncomputed by each kernel, which in turn produces approximate results, i.e.\nchanges the precision of the performed computation. Results derived from a\nvoltage- and frequency-scaled ARM Cortex A15 processor running face recognition\nand music matching algorithms demonstrate that the proposed approach allows for\n280%~440% increase of processing throughput and 75%~80% decrease of energy\nconsumption against optimized GEMM and CONV kernels without any impact in the\nobtained recognition or matching accuracy. Even higher gains can be obtained if\none is willing to tolerate some reduction in the accuracy of the recognition\nand matching applications."
},{
    "category": "stat.CO", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.3510v1", 
    "title": "An implementation of a randomized algorithm for principal component   analysis", 
    "arxiv-id": "1412.3510v1", 
    "author": "Mark Tygert", 
    "publish": "2014-12-11T00:52:41Z", 
    "summary": "Recent years have witnessed intense development of randomized methods for\nlow-rank approximation. These methods target principal component analysis (PCA)\nand the calculation of truncated singular value decompositions (SVD). The\npresent paper presents an essentially black-box, fool-proof implementation for\nMathworks' MATLAB, a popular software platform for numerical computation. As\nillustrated via several tests, the randomized algorithms for low-rank\napproximation outperform or at least match the classical techniques (such as\nLanczos iterations) in basically all respects: accuracy, computational\nefficiency (both speed and memory usage), ease-of-use, parallelizability, and\nreliability. However, the classical procedures remain the methods of choice for\nestimating spectral norms, and are far superior for calculating the least\nsingular values and corresponding singular vectors (or singular subspaces)."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.4690v2", 
    "title": "GPTIPS 2: an open-source software platform for symbolic data mining", 
    "arxiv-id": "1412.4690v2", 
    "author": "Dominic P. Searson", 
    "publish": "2014-12-15T17:36:45Z", 
    "summary": "GPTIPS is a free, open source MATLAB based software platform for symbolic\ndata mining (SDM). It uses a multigene variant of the biologically inspired\nmachine learning method of genetic programming (MGGP) as the engine that drives\nthe automatic model discovery process. Symbolic data mining is the process of\nextracting hidden, meaningful relationships from data in the form of symbolic\nequations. In contrast to other data-mining methods, the structural\ntransparency of the generated predictive equations can give new insights into\nthe physical systems or processes that generated the data. Furthermore, this\ntransparency makes the models very easy to deploy outside of MATLAB. The\nrationale behind GPTIPS is to reduce the technical barriers to using,\nunderstanding, visualising and deploying GP based symbolic models of data,\nwhilst at the same time remaining highly customisable and delivering robust\nnumerical performance for power users. In this chapter, notable new features of\nthe latest version of the software are discussed with these aims in mind.\nAdditionally, a simplified variant of the MGGP high level gene crossover\nmechanism is proposed. It is demonstrated that the new functionality of GPTIPS\n2 (a) facilitates the discovery of compact symbolic relationships from data\nusing multiple approaches, e.g. using novel gene-centric visualisation analysis\nto mitigate horizontal bloat and reduce complexity in multigene symbolic\nregression models (b) provides numerous methods for visualising the properties\nof symbolic models (c) emphasises the generation of graphically navigable\nlibraries of models that are optimal in terms of the Pareto trade off surface\nof model performance and complexity and (d) expedites real world applications\nby the simple, rapid and robust deployment of symbolic models outside the\nsoftware environment they were developed in."
},{
    "category": "stat.CO", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.4825v1", 
    "title": "Efficient SIMD RNG for Varying-Parameter Streams: C++ Class BatchRNG", 
    "arxiv-id": "1412.4825v1", 
    "author": "Mansour T. A. Sharabiani", 
    "publish": "2014-12-15T22:27:57Z", 
    "summary": "Single-Instruction, Multiple-Data (SIMD) random number generators (RNGs) take\nadvantage of vector units to offer significant performance gain over\nnon-vectorized libraries, but they often rely on batch production of deviates\nfrom distributions with fixed parameters. In many statistical applications such\nas Gibbs sampling, parameters of sampled distributions change from one\niteration to the next, requiring that random deviates be generated\none-at-a-time. This situation can render vectorized RNGs inefficient, and even\ninferior to their scalar counterparts. The C++ class BatchRNG uses buffers of\nbase distributions such uniform, Gaussian and exponential to take advantage of\nvector units while allowing for sequences of deviates to be generated with\nvarying parameters. These small buffers are consumed and replenished as needed\nduring a program execution. Performance tests using Intel Vector Statistical\nLibrary (VSL) on various probability distributions illustrates the\neffectiveness of the proposed batching strategy."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.5720v1", 
    "title": "FlexDM: Enabling robust and reliable parallel data mining using WEKA", 
    "arxiv-id": "1412.5720v1", 
    "author": "Alexandre Mendes", 
    "publish": "2014-12-18T05:07:44Z", 
    "summary": "Performing massive data mining experiments with multiple datasets and methods\nis a common task faced by most bioinformatics and computational biology\nlaboratories. WEKA is a machine learning package designed to facilitate this\ntask by providing tools that allow researchers to select from several\nclassification methods and specific test strategies. Despite its popularity,\nthe current WEKA environment for batch experiments, namely Experimenter, has\nfour limitations that impact its usability: the selection of value ranges for\nmethods options lacks flexibility and is not intuitive; there is no support for\nparallelisation when running large-scale data mining tasks; the XML schema is\ndifficult to read, necessitating the use of the Experimenter's graphical user\ninterface for generation and modification; and robustness is limited by the\nfact that results are not saved until the last test has concluded.\n  FlexDM implements an interface to WEKA to run batch processing tasks in a\nsimple and intuitive way. In a short and easy-to-understand XML file, one can\ndefine hundreds of tests to be performed on several datasets. FlexDM also\nallows those tests to be executed asynchronously in parallel to take advantage\nof multi-core processors, significantly increasing usability and productivity.\nResults are saved incrementally for better robustness and reliability.\n  FlexDM is implemented in Java and runs on Windows, Linux and OSX. As we\nencourage other researchers to explore and adopt our software, FlexDM is made\navailable as a pre-configured bootable reference environment. All code,\nsupporting documentation and usage examples are also available for download at\nhttp://sourceforge.net/projects/flexdm."
},{
    "category": "cs.MS", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.6395v1", 
    "title": "SClib, a hack for straightforward embedded C functions in Python", 
    "arxiv-id": "1412.6395v1", 
    "author": "Hector E. Martinez", 
    "publish": "2014-12-19T15:51:21Z", 
    "summary": "We present SClib, a simple hack that allows easy and straightforward\nevaluation of C functions within Python code, boosting flexibility for better\ntrade-off between computation power and feature availability, such as\nvisualization and existing computation routines in SciPy. We also present two\ncases were SClib has been used. In the first set of applications we use SClib\nto write a port to Python of a Schr\\\"odinger equation solver that has been\nextensively used the literature, the resulting script presents a speed-up of\nabout 150x with respect to the original one. A review of the situations where\nthe speeded-up script has been used is presented. We also describe the solution\nto the related problem of solving a set of coupled Schr\\\"odinger-like equations\nwhere SClib is used to implement the speed-critical parts of the code. We argue\nthat when using SClib within IPython we can use NumPy and Matplotlib for the\nmanipulation and visualization of the solutions in an interactive environment\nwith no performance compromise. The second case is an engineering application.\nWe use SClib to evaluate the control and system derivatives in a feedback\ncontrol loop for electrical motors. With this and the integration routines\navailable in SciPy, we can run simulations of the control loop a la Simulink.\nThe use of C code not only boosts the speed of the simulations, but also\nenables to test the exact same code that we use in the test rig to get\nexperimental results. Again, integration with IPython gives us the flexibility\nto analyze and visualize the data."
},{
    "category": "cs.CE", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1412.7030v1", 
    "title": "Proceedings of the 7th European Conference on Python in Science   (EuroSciPy 2014)", 
    "arxiv-id": "1412.7030v1", 
    "author": "Nelle Varoquaux", 
    "publish": "2014-12-22T15:47:51Z", 
    "summary": "These are the proceedings of the 7th European Conference on Python in\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014)."
},{
    "category": "math.NA", 
    "doi": "10.7717/peerj.453", 
    "link": "http://arxiv.org/pdf/1502.05366v3", 
    "title": "RSVDPACK: An implementation of randomized algorithms for computing the   singular value, interpolative, and CUR decompositions of matrices on   multi-core and GPU architectures", 
    "arxiv-id": "1502.05366v3", 
    "author": "Per-Gunnar Martinsson", 
    "publish": "2015-02-18T20:13:26Z", 
    "summary": "RSVDPACK is a library of functions for computing low rank approximations of\nmatrices. The library includes functions for computing standard (partial)\nfactorizations such as the Singular Value Decomposition (SVD), and also so\ncalled \"structure preserving\" factorizations such as the Interpolative\nDecomposition (ID) and the CUR decomposition. The ID and CUR factorizations\npick subsets of the rows/columns of a matrix to use as bases for its row/column\nspace. Such factorizations preserve properties of the matrix such as sparsity\nor non-negativity, are helpful in data interpretation, and require in certain\ncontexts less memory than a partial SVD. The package implements highly\nefficient computational algorithms based on randomized sampling, as described\nand analyzed in [N. Halko, P.G. Martinsson, J. Tropp, \"Finding structure with\nrandomness: Probabilistic algorithms for constructing approximate matrix\ndecompositions,\" SIAM Review, 53(2), 2011], and subsequent papers. This\nmanuscript presents some modifications to the basic algorithms that improve\nperformance and ease of use. The library is written in C and supports both\nmulti-core CPU and GPU architectures."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s10444-015-9442-z", 
    "link": "http://arxiv.org/pdf/1502.07191v4", 
    "title": "Construction and implementation of asymptotic expansions for   Jacobi--type orthogonal polynomials", 
    "arxiv-id": "1502.07191v4", 
    "author": "Peter Opsomer", 
    "publish": "2015-02-25T15:07:30Z", 
    "summary": "We are interested in the asymptotic behavior of orthogonal polynomials of the\ngeneralized Jacobi type as their degree $n$ goes to $\\infty$. These are defined\non the interval $[-1,1]$ with weight function\n$w(x)=(1-x)^{\\alpha}(1+x)^{\\beta}h(x)$, $\\alpha,\\beta>-1$ and $h(x)$ a real,\nanalytic and strictly positive function on $[-1,1]$. This information is\navailable in the work of Kuijlaars, McLaughlin, Van Assche and Vanlessen, where\nthe authors use the Riemann--Hilbert formulation and the Deift--Zhou non-linear\nsteepest descent method. We show that computing higher-order terms can be\nsimplified, leading to their efficient construction. The resulting asymptotic\nexpansions in every region of the complex plane are implemented both\nsymbolically and numerically, and the code is made publicly available. The main\nadvantage of these expansions is that they lead to increasing accuracy for\nincreasing degree of the polynomials, at a computational cost that is actually\nindependent of the degree. In contrast, the typical use of the recurrence\nrelation for orthogonal polynomials in computations leads to a cost that is at\nleast linear in the degree. Furthermore, the expansions may be used to compute\nGaussian quadrature rules in $\\mathcal{O}(n)$ operations, rather than\n$\\mathcal{O}(n^2)$ based on the recurrence relation."
},{
    "category": "q-bio.SC", 
    "doi": "10.1007/s10444-015-9442-z", 
    "link": "http://arxiv.org/pdf/1503.01095v1", 
    "title": "libRoadRunner: A High Performance SBML Simulation and Analysis Library", 
    "arxiv-id": "1503.01095v1", 
    "author": "Herbert M. Sauro", 
    "publish": "2015-03-03T20:40:39Z", 
    "summary": "This paper presents libRoadRunner, an extensible, high-performance,\ncross-platform, open-source software library for the simulation and analysis of\nmodels \\ expressed using Systems Biology Markup Language (SBML). SBML is the\nmost widely used standard for representing dynamic networks, especially\nbiochemical networks. libRoadRunner supports solution of both large models and\nmultiple replicas of a single model on desktop, mobile and cluster computers.\nlibRoadRunner is a self-contained library, able to run both as a component\ninside other tools via its C++ and C bindings andnteractively through its\nPython interface. The Python Application Programming Interface (API) is similar\nto the APIs of Matlab and SciPy, making it fast and easy to learn, even for new\nusers. libRoadRunner uses a custom Just-In-Time (JIT) compiler built on the\nwidely-used LLVM JIT compiler framework to compile SBML-specified models\ndirectly into very fast native machine code for a variety of processors, making\nit appropriate for solving very large models or multiple replicas of smaller\nmodels. libRoadRunner is flexible, supporting the bulk of the SBML\nspecification (except for delay and nonlinear algebraic equations) and several\nof its extensions. It offers multiple deterministic and stochastic integrators,\nas well as tools for steady-state, stability analyses and flux balance\nanalysis. We regularly update libRoadRunner binary distributions for Mac OS X,\nLinux and Windows and license them under Apache License Version 2.0.\nhttp://www.libroadrunner.org provides online documentation, full build\ninstructions, binaries and a git source repository."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s10444-015-9442-z", 
    "link": "http://arxiv.org/pdf/1503.08376v1", 
    "title": "Assessing Excel VBA Suitability for Monte Carlo Simulation", 
    "arxiv-id": "1503.08376v1", 
    "author": "Alexei Botchkarev", 
    "publish": "2015-03-29T01:51:49Z", 
    "summary": "Monte Carlo (MC) simulation includes a wide range of stochastic techniques\nused to quantitatively evaluate the behavior of complex systems or processes.\nMicrosoft Excel spreadsheets with Visual Basic for Applications (VBA) software\nis, arguably, the most commonly employed general purpose tool for MC\nsimulation. Despite the popularity of the Excel in many industries and\neducational institutions, it has been repeatedly criticized for its flaws and\noften described as questionable, if not completely unsuitable, for statistical\nproblems. The purpose of this study is to assess suitability of the Excel\n(specifically its 2010 and 2013 versions) with VBA programming as a tool for MC\nsimulation. The results of the study indicate that Microsoft Excel (versions\n2010 and 2013) is a strong Monte Carlo simulation application offering a solid\nframework of core simulation components including spreadsheets for data input\nand output, VBA development environment and summary statistics functions. This\nframework should be complemented with an external high-quality pseudo-random\nnumber generator added as a VBA module. A large and diverse category of Excel\nincidental simulation components that includes statistical distributions,\nlinear and non-linear regression and other statistical, engineering and\nbusiness functions require execution of due diligence to determine their\nsuitability for a specific MC project."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10444-015-9442-z", 
    "link": "http://arxiv.org/pdf/1504.01329v1", 
    "title": "Achieving algorithmic resilience for temporal integration through   spectral deferred corrections", 
    "arxiv-id": "1504.01329v1", 
    "author": "J. B. Bell", 
    "publish": "2015-04-06T17:26:32Z", 
    "summary": "Spectral deferred corrections (SDC) is an iterative approach for constructing\nhigher- order accurate numerical approximations of ordinary differential\nequations. SDC starts with an initial approximation of the solution defined at\na set of Gaussian or spectral collocation nodes over a time interval and uses\nan iterative application of lower-order time discretizations applied to a\ncorrection equation to improve the solution at these nodes. Each deferred\ncorrection sweep increases the formal order of accuracy of the method up to the\nlimit inherent in the accuracy defined by the collocation points. In this\npaper, we demonstrate that SDC is well suited to recovering from soft\n(transient) hardware faults in the data. A strategy where extra correction\niterations are used to recover from soft errors and provide algorithmic\nresilience is proposed. Specifically, in this approach the iteration is\ncontinued until the residual (a measure of the error in the approximation) is\nsmall relative to the residual on the first correction iteration and changes\nslowly between successive iterations. We demonstrate the effectiveness of this\nstrategy for both canonical test problems and a comprehen- sive situation\ninvolving a mature scientific application code that solves the reacting\nNavier-Stokes equations for combustion research."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.jcp.2015.11.026", 
    "link": "http://arxiv.org/pdf/1504.01380v2", 
    "title": "The swept rule for breaking the latency barrier in time advancing PDEs", 
    "arxiv-id": "1504.01380v2", 
    "author": "Qiqi Wang", 
    "publish": "2015-04-06T16:00:32Z", 
    "summary": "This article investigates the swept rule of space-time domain decomposition,\nan idea to break the latency barrier via communicating less often when\nexplicitly solving time-dependent PDEs. The swept rule decomposes space and\ntime among computing nodes in ways that exploit the domains of influence and\nthe domain of dependency, making it possible to communicate once per many\ntimesteps without redundant computation. The article presents simple\ntheoretical analysis to the performance of the swept rule which then was shown\nto be accurate by conducting numerical experiments."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jcp.2015.11.026", 
    "link": "http://arxiv.org/pdf/1504.04714v1", 
    "title": "Enhancing the scalability and load balancing of the parallel selected   inversion algorithm via tree-based asynchronous communication", 
    "arxiv-id": "1504.04714v1", 
    "author": "Chao Yang", 
    "publish": "2015-04-18T12:46:33Z", 
    "summary": "We develop a method for improving the parallel scalability of the recently\ndeveloped parallel selected inversion algorithm [Jacquelin, Lin and Yang 2014],\nnamed PSelInv, on massively parallel distributed memory machines. In the\nPSelInv method, we compute selected elements of the inverse of a sparse matrix\nA that can be decomposed as A = LU, where L is lower triangular and U is upper\ntriangular. Updating these selected elements of A-1 requires restricted\ncollective communications among a subset of processors within each column or\nrow communication group created by a block cyclic distribution of L and U. We\ndescribe how this type of restricted collective communication can be\nimplemented by using asynchronous point-to-point MPI communication functions\ncombined with a binary tree based data propagation scheme. Because multiple\nrestricted collective communications may take place at the same time in the\nparallel selected inversion algorithm, we need to use a heuristic to prevent\nprocessors participating in multiple collective communications from receiving\ntoo many messages. This heuristic allows us to reduce communication load\nimbalance and improve the overall scalability of the selected inversion\nalgorithm. For instance, when 6,400 processors are used, we observe over 5x\nspeedup for test matrices. It also mitigates the performance variability\nintroduced by an inhomogeneous network topology."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.jcp.2015.11.026", 
    "link": "http://arxiv.org/pdf/1504.07890v1", 
    "title": "Large-scale linear regression: Development of high-performance routines", 
    "arxiv-id": "1504.07890v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2015-04-29T15:24:33Z", 
    "summary": "In statistics, series of ordinary least squares problems (OLS) are used to\nstudy the linear correlation among sets of variables of interest; in many\nstudies, the number of such variables is at least in the millions, and the\ncorresponding datasets occupy terabytes of disk space. As the availability of\nlarge-scale datasets increases regularly, so does the challenge in dealing with\nthem. Indeed, traditional solvers---which rely on the use of black-box\"\nroutines optimized for one single OLS---are highly inefficient and fail to\nprovide a viable solution for big-data analyses. As a case study, in this paper\nwe consider a linear regression consisting of two-dimensional grids of related\nOLS problems that arise in the context of genome-wide association analyses, and\ngive a careful walkthrough for the development of {\\sc ols-grid}, a\nhigh-performance routine for shared-memory architectures; analogous steps are\nrelevant for tailoring OLS solvers to other applications. In particular, we\nfirst illustrate the design of efficient algorithms that exploit the structure\nof the OLS problems and eliminate redundant computations; then, we show how to\neffectively deal with datasets that do not fit in main memory; finally, we\ndiscuss how to cast the computation in terms of efficient kernels and how to\nachieve scalability. Importantly, each design decision along the way is\njustified by simple performance models. {\\sc ols-grid} enables the solution of\n$10^{11}$ correlated OLS problems operating on terabytes of data in a matter of\nhours."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-27436-2_8", 
    "link": "http://arxiv.org/pdf/1505.01962v2", 
    "title": "Applying Sorting Networks to Synthesize Optimized Sorting Libraries", 
    "arxiv-id": "1505.01962v2", 
    "author": "Peter Schneider-Kamp", 
    "publish": "2015-05-08T09:13:25Z", 
    "summary": "This paper shows an application of the theory of sorting networks to\nfacilitate the synthesis of optimized general purpose sorting libraries.\nStandard sorting libraries are often based on combinations of the classic\nQuicksort algorithm with insertion sort applied as the base case for small\nfixed numbers of inputs. Unrolling the code for the base case by ignoring loop\nconditions eliminates branching and results in code which is equivalent to a\nsorting network. This enables the application of further program\ntransformations based on sorting network optimizations, and eventually the\nsynthesis of code from sorting networks. We show that if considering the number\nof comparisons and swaps then theory predicts no real advantage of this\napproach. However, significant speed-ups are obtained when taking advantage of\ninstruction level parallelism and non-branching conditional assignment\ninstructions, both of which are common in modern CPU architectures. We provide\nempirical evidence that using code synthesized from efficient sorting networks\nas the base case for Quicksort libraries results in significant real-world\nspeed-ups."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.03357v3", 
    "title": "A parallel edge orientation algorithm for quadrilateral meshes", 
    "arxiv-id": "1505.03357v3", 
    "author": "David A. Ham", 
    "publish": "2015-05-13T12:34:54Z", 
    "summary": "One approach to achieving correct finite element assembly is to ensure that\nthe local orientation of facets relative to each cell in the mesh is consistent\nwith the global orientation of that facet. Rognes et al. have shown how to\nachieve this for any mesh composed of simplex elements, and deal.II contains a\nserial algorithm to construct a consistent orientation of any quadrilateral\nmesh of an orientable manifold.\n  The core contribution of this paper is the extension of this algorithm for\ndistributed memory parallel computers, which facilitates its seamless\napplication as part of a parallel simulation system.\n  Furthermore, our analysis establishes a link between the well-known\nUnion-Find algorithm and the construction of a consistent orientation of a\nquadrilateral mesh. As a result, existing work on the parallelisation of the\nUnion-Find algorithm can be easily adapted to construct further parallel\nalgorithms for mesh orientations."
},{
    "category": "cs.LO", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.05028v4", 
    "title": "Automatic and Transparent Transfer of Theorems along Isomorphisms in the   Coq Proof Assistant", 
    "arxiv-id": "1505.05028v4", 
    "author": "Hugo Herbelin", 
    "publish": "2015-05-19T14:50:14Z", 
    "summary": "In mathematics, it is common practice to have several constructions for the\nsame objects. Mathematicians will identify them modulo isomorphism and will not\nworry later on which construction they use, as theorems proved for one\nconstruction will be valid for all.\n  When working with proof assistants, it is also common to see several\ndata-types representing the same objects. This work aims at making the use of\nseveral isomorphic constructions as simple and as transparent as it can be done\ninformally in mathematics. This requires inferring automatically the missing\nproof-steps.\n  We are designing an algorithm which finds and fills these missing proof-steps\nand we are implementing it as a plugin for Coq."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.06848v1", 
    "title": "Remark on \"Algorithm 916: Computing the Faddeyeva and Voigt functions\":   Efficiency Improvements and Fortran Translation", 
    "arxiv-id": "1505.06848v1", 
    "author": "Mofreh R. Zaghloul", 
    "publish": "2015-05-26T08:25:45Z", 
    "summary": "This remark describes efficiency improvements to Algorithm 916 [Zaghloul and\nAli 2011]. It is shown that the execution time required by the algorithm, when\nrun at its highest accuracy, may be improved by more than a factor of two. A\nbetter accuracy vs efficiency trade off scheme is also implemented; this\nrequires the user to supply the number of significant figures desired in the\ncomputed values as an extra input argument to the function. Using this\ntrade-off, it is shown that the efficiency of the algorithm may be further\nimproved significantly while maintaining reasonably accurate and safe results\nthat are free of the pitfalls and complete loss of accuracy seen in other\ncompetitive techniques. The current version of the code is provided in Matlab\nand Scilab in addition to a Fortran translation prepared to meet the needs of\nreal-world problems where very large numbers of function evaluations would\nrequire the use of a compiled language. To fulfill this last requirement, a\nrecently proposed reformed version of Humlicek's w4 routine, shown to maintain\nthe claimed accuracy of the algorithm over a wide and fine grid is implemented\nin the present Fortran translation for the case of 4 significant figures. This\nlatter modification assures the reliability of the code to be employed in the\nsolution of practical problems requiring numerous evaluation of the function\nfor applications tolerating low accuracy computations (<10-4)."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.07570v6", 
    "title": "A Practical Guide to Randomized Matrix Computations with MATLAB   Implementations", 
    "arxiv-id": "1505.07570v6", 
    "author": "Shusen Wang", 
    "publish": "2015-05-28T07:33:21Z", 
    "summary": "Matrix operations such as matrix inversion, eigenvalue decomposition,\nsingular value decomposition are ubiquitous in real-world applications.\nUnfortunately, many of these matrix operations so time and memory expensive\nthat they are prohibitive when the scale of data is large. In real-world\napplications, since the data themselves are noisy, machine-precision matrix\noperations are not necessary at all, and one can sacrifice a reasonable amount\nof accuracy for computational efficiency.\n  In recent years, a bunch of randomized algorithms have been devised to make\nmatrix computations more scalable. Mahoney (2011) and Woodruff (2014) have\nwritten excellent but very technical reviews of the randomized algorithms.\nDifferently, the focus of this manuscript is on intuition, algorithm\nderivation, and implementation. This manuscript should be accessible to people\nwith knowledge in elementary matrix algebra but unfamiliar with randomized\nmatrix computations. The algorithms introduced in this manuscript are all\nsummarized in a user-friendly way, and they can be implemented in lines of\nMATLAB code. The readers can easily follow the implementations even if they do\nnot understand the maths and algorithms."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.08019v1", 
    "title": "Research on the fast Fourier transform of image based on GPU", 
    "arxiv-id": "1505.08019v1", 
    "author": "Qingyun Wang", 
    "publish": "2015-05-29T12:33:52Z", 
    "summary": "Study of general purpose computation by GPU (Graphics Processing Unit) can\nimprove the image processing capability of micro-computer system. This paper\nstudies the parallelism of the different stages of decimation in time radix 2\nFFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT\non GPU. The experiment result demonstrates the validity and advantage over\ngeneral CPU, especially in the condition of large input size. The approach can\nalso be generalized to other transforms alike."
},{
    "category": "cs.DC", 
    "doi": "10.1137/15M1021325", 
    "link": "http://arxiv.org/pdf/1505.08023v1", 
    "title": "The Research and Optimization of Parallel Finite Element Algorithm based   on MiniFE", 
    "arxiv-id": "1505.08023v1", 
    "author": "Daning Cheng", 
    "publish": "2015-05-29T12:44:04Z", 
    "summary": "Finite element method (FEM) is one of the most important numerical methods in\nmodern engineering design and analysis. Since traditional serial FEM is\ndifficult to solve large FE problems efficiently and accurately,\nhigh-performance parallel FEM has become one of the essential way to solve\npractical engineering problems. Based on MiniFE program, which is released by\nNational Energy Research Scientific Computing Center(NERSC), this work analyzes\nconcrete steps, key computing pattern and parallel mechanism of parallel FEM.\nAccording to experimental results, this work analyzes the proportion of\ncalculation amount of each module and concludes the main performance bottleneck\nof the program. Based on that, we optimize the MiniFE program on a server\nplatform. The optimization focuses on the bottleneck of the program - SpMV\nkernel, and uses an efficient storage format named BCRS. Moreover, an improving\nplan of hybrid MPI+OpenMP programming is provided. Experimental results show\nthat the optimized program performs better in both SpMV kernel and\nsynchronization. It can increase the performance of the program, on average, by\n8.31%. Keywords : finite element, parallel, MiniFE, SpMV, performance\noptimization"
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.03726v2", 
    "title": "Lacunaryx: Computing bounded-degree factors of lacunary polynomials", 
    "arxiv-id": "1506.03726v2", 
    "author": "Bruno Grenet", 
    "publish": "2015-06-11T16:08:54Z", 
    "summary": "In this paper, we report on an implementation in the free software Mathemagix\nof lacunary factorization algorithms, distributed as a library called\nLacunaryx. These algorithms take as input a polynomial in sparse\nrepresentation, that is as a list of nonzero monomials, and an integer $d$, and\ncompute its irreducible degree-$\\le d$ factors. The complexity of these\nalgorithms is polynomial in the sparse size of the input polynomial and $d$."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.04776v1", 
    "title": "Encog: Library of Interchangeable Machine Learning Models for Java and   C#", 
    "arxiv-id": "1506.04776v1", 
    "author": "Jeff Heaton", 
    "publish": "2015-06-15T21:20:06Z", 
    "summary": "This paper introduces the Encog library for Java and C#, a scalable,\nadaptable, multiplatform machine learning framework that was 1st released in\n2008. Encog allows a variety of machine learning models to be applied to\ndatasets using regression, classification, and clustering. Various supported\nmachine learning models can be used interchangeably with minimal recoding.\nEncog uses efficient multithreaded code to reduce training time by exploiting\nmodern multicore processors. The current version of Encog can be downloaded\nfrom http://www.encog.org."
},{
    "category": "math.NA", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.04802v2", 
    "title": "A fast exact simulation method for a class of Markov jump processes", 
    "arxiv-id": "1506.04802v2", 
    "author": "Lili Hu", 
    "publish": "2015-06-16T00:10:16Z", 
    "summary": "A new method of the stochastic simulation algorithm (SSA), named the\nHashing-Leaping method (HLM), for exact simulations of a class of Markov jump\nprocesses, is presented in this paper. The HLM has a conditional constant\ncomputational cost per event, which is independent of the number of exponential\nclocks in the Markov process. The main idea of the HLM is to repeatedly\nimplement a hash-table-like bucket sort algorithm for all times of occurrence\ncovered by a time step with length $\\tau$. This paper serves as an introduction\nto this new SSA method. We introduce the method, demonstrate its\nimplementation, analyze its properties, and compare its performance with three\nother commonly used SSA methods in four examples. Our performance tests and CPU\noperation statistics show certain advantage of the HLM for large scale\nproblems."
},{
    "category": "cs.LO", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.05605v1", 
    "title": "Asynchronous processing of Coq documents: from the kernel up to the user   interface", 
    "arxiv-id": "1506.05605v1", 
    "author": "Enrico Tassi", 
    "publish": "2015-06-18T09:47:41Z", 
    "summary": "The work described in this paper improves the reactivity of the Coq system by\ncompletely redesigning the way it processes a formal document. By subdividing\nsuch work into independent tasks the system can give precedence to the ones of\nimmediate interest for the user and postpones the others. On the user side, a\nmodern interface based on the PIDE middleware aggregates and present in a\nconsistent way the output of the prover. Finally postponed tasks are processed\nexploiting modern, parallel, hardware to offer better scalability."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.06102v1", 
    "title": "GRINS: A Multiphysics Framework Based on the libMesh Finite Element   Library", 
    "arxiv-id": "1506.06102v1", 
    "author": "Roy H. Stogner", 
    "publish": "2015-06-19T18:04:01Z", 
    "summary": "The progression of scientific computing resources has enabled the numerical\napproximation of mathematical models describing complex physical phenomena. A\nsignificant portion of researcher time is typically dedicated to the\ndevelopment of software to compute the numerical solutions. This work describes\na flexible C++ software framework, built on the libMesh finite element library,\ndesigned to alleviate developer burden and provide easy access to modern\ncomputational algorithms, including quantity-of-interest-driven parallel\nadaptive mesh refinement on unstructured grids and adjoint-based sensitivities.\nOther software environments are highlighted and the current work motivated; in\nparticular, the present work is an attempt to balance software infrastructure\nand user flexibility. The applicable class of problems and design of the\nsoftware components is discussed in detail. Several examples demonstrate the\neffectiveness of the design, including applications that incorporate\nuncertainty. Current and planned developments are discussed."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.06185v1", 
    "title": "Resilience for Multigrid Software at the Extreme Scale", 
    "arxiv-id": "1506.06185v1", 
    "author": "Barbara Wohlmuth", 
    "publish": "2015-06-20T00:03:16Z", 
    "summary": "Fault tolerant algorithms for the numerical approximation of elliptic partial\ndifferential equations on modern supercomputers play a more and more important\nrole in the future design of exa-scale enabled iterative solvers. Here, we\ncombine domain partitioning with highly scalable geometric multigrid schemes to\nobtain fast and fault-robust solvers in three dimensions. The recovery strategy\nis based on a hierarchical hybrid concept where the values on lower dimensional\nprimitives such as faces are stored redundantly and thus can be recovered\neasily in case of a failure. The lost volume unknowns in the faulty region are\nre-computed approximately with multigrid cycles by solving a local Dirichlet\nproblem on the faulty subdomain. Different strategies are compared and\nevaluated with respect to performance, computational cost, and speed up.\nEspecially effective are strategies in which the local recovery in the faulty\nregion is executed in parallel with global solves and when the local recovery\nis additionally accelerated. This results in an asynchronous multigrid\niteration that can fully compensate faults. Excellent parallel performance on a\ncurrent peta-scale system is demonstrated."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.06704v1", 
    "title": "Software realization of the complex spectra analysis algorithm in R", 
    "arxiv-id": "1506.06704v1", 
    "author": "Vladimir Bakhrushin", 
    "publish": "2015-04-25T07:16:35Z", 
    "summary": "Software realization of the complex spectra decomposition on unknown number\nof similarcomponents is proposed.The algorithm is based on non-linear\nminimizing the sum of squared residuals of the spectrum model. For the adequacy\nchecking the complex of criteria is used.It tests the model residuals\ncorrespondence with the normal distribution, equality to zero of their mean\nvalue and autocorrelation. Also the closeness of residuals and experimental\ndata variances is checked."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.07094v3", 
    "title": "pyMOR - Generic Algorithms and Interfaces for Model Order Reduction", 
    "arxiv-id": "1506.07094v3", 
    "author": "Felix Schindler", 
    "publish": "2015-06-23T17:08:59Z", 
    "summary": "Reduced basis methods are projection-based model order reduction techniques\nfor reducing the computational complexity of solving parametrized partial\ndifferential equation problems. In this work we discuss the design of pyMOR, a\nfreely available software library of model order reduction algorithms, in\nparticular reduced basis methods, implemented with the Python programming\nlanguage. As its main design feature, all reduction algorithms in pyMOR are\nimplemented generically via operations on well-defined vector array, operator\nand discretization interface classes. This allows for an easy integration with\nexisting open-source high-performance partial differential equation solvers\nwithout adding any model reduction specific code to these solvers. Besides an\nin-depth discussion of pyMOR's design philosophy and architecture, we present\nseveral benchmark results and numerical examples showing the feasibility of our\napproach."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.07980v1", 
    "title": "A Java Implementation of the SGA, UMDA, ECGA, and HBOA", 
    "arxiv-id": "1506.07980v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2015-06-26T07:44:38Z", 
    "summary": "The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,\nthe Extended Compact Genetic Algorithm, and the Hierarchical Bayesian\nOptimization Algorithm are all well known Evolutionary Algorithms.\n  In this report we present a Java implementation of these four algorithms with\ndetailed instructions on how to use each of them to solve a given set of\noptimization problems. Additionally, it is explained how to implement and\nintegrate new problems within the provided set. The source and binary files of\nthe Java implementations are available for free download at\nhttps://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.08694v1", 
    "title": "A Java Implementation of Parameter-less Evolutionary Algorithms", 
    "arxiv-id": "1506.08694v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2015-06-26T07:53:57Z", 
    "summary": "The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in\n1999 as an alternative to the usual trial-and-error method of finding, for each\ngiven problem, an acceptable set-up of the parameter values of the genetic\nalgorithm. Since then, the same strategy has been successfully applied to\ncreate parameter-less versions of other population-based search algorithms such\nas the Extended Compact Genetic Algorithm and the Hierarchical Bayesian\nOptimization Algorithm. This report describes a Java implementation,\nParameter-less Evolutionary Algorithm (P-EAJava), that integrates several\nparameter-less evolutionary algorithms into a single platform. Along with a\nbrief description of P-EAJava, we also provide detailed instructions on how to\nuse it, how to implement new problems, and how to generate new parameter-less\nversions of evolutionary algorithms.\n  At present time, P-EAJava already includes parameter-less versions of the\nSimple Genetic Algorithm, the Extended Compact Genetic Algorithm, the\nUnivariate Marginal Distribution Algorithm, and the Hierarchical Bayesian\nOptimization Algorithm. The source and binary files of the Java implementation\nof P-EAJava are available for free download at\nhttps://github.com/JoseCPereira/2015ParameterlessEvolutionaryAlgorithmsJava."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2893803.2893807", 
    "link": "http://arxiv.org/pdf/1506.08867v1", 
    "title": "Java Implementation of a Parameter-less Evolutionary Portfolio", 
    "arxiv-id": "1506.08867v1", 
    "author": "Fernando G. Lobo", 
    "publish": "2015-06-26T07:58:32Z", 
    "summary": "The Java implementation of a portfolio of parameter-less evolutionary\nalgorithms is presented. The Parameter-less Evolutionary Portfolio implements a\nheuristic that performs adaptive selection of parameter-less evolutionary\nalgorithms in accordance with performance criteria that are measured during\nrunning time. At present time, the portfolio includes three parameter-less\nevolutionary algorithms: Parameter-less Univariate Marginal Distribution\nAlgorithm, Parameter-less Extended Compact Genetic Algorithm, and\nParameter-less Hierarchical Bayesian Optimization Algorithm. Initial\nexperiments showed that the parameter-less portfolio can solve various classes\nof problems without the need for any prior parameter setting technique and with\nan increase in computational effort that can be considered acceptable."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1016/j.cpc.2015.06.001", 
    "link": "http://arxiv.org/pdf/1507.02506v1", 
    "title": "GenASiS Basics: Object-oriented utilitarian functionality for   large-scale physics simulations", 
    "arxiv-id": "1507.02506v1", 
    "author": "Reuben D. Budiardja", 
    "publish": "2015-07-09T13:38:55Z", 
    "summary": "Aside from numerical algorithms and problem setup, large-scale physics\nsimulations on distributed-memory supercomputers require more basic utilitarian\nfunctionality, such as physical units and constants; display to the screen or\nstandard output device; message passing; I/O to disk; and runtime parameter\nmanagement and usage statistics. Here we describe and make available Fortran\n2003 classes furnishing extensible object-oriented implementations of this sort\nof rudimentary functionality, along with individual `unit test' programs and\nlarger example problems demonstrating their use. These classes compose the\nBasics division of our developing astrophysics simulation code GenASiS (General\nAstrophysical Simulation System), but their fundamental nature makes them\nuseful for physics simulations in many fields."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10766-016-0464-z", 
    "link": "http://arxiv.org/pdf/1507.08101v3", 
    "title": "GHOST: Building blocks for high performance sparse linear algebra on   heterogeneous systems", 
    "arxiv-id": "1507.08101v3", 
    "author": "Gerhard Wellein", 
    "publish": "2015-07-29T11:08:57Z", 
    "summary": "While many of the architectural details of future exascale-class high\nperformance computer systems are still a matter of intense research, there\nappears to be a general consensus that they will be strongly heterogeneous,\nfeaturing \"standard\" as well as \"accelerated\" resources. Today, such resources\nare available as multicore processors, graphics processing units (GPUs), and\nother accelerators such as the Intel Xeon Phi. Any software infrastructure that\nclaims usefulness for such environments must be able to meet their inherent\nchallenges: massive multi-level parallelism, topology, asynchronicity, and\nabstraction. The \"General, Hybrid, and Optimized Sparse Toolkit\" (GHOST) is a\ncollection of building blocks that targets algorithms dealing with sparse\nmatrix representations on current and future large-scale systems. It implements\nthe \"MPI+X\" paradigm, has a pure C interface, and provides hybrid-parallel\nnumerical kernels, intelligent resource management, and truly heterogeneous\nparallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We\ndescribe the details of its design with respect to the challenges posed by\nmodern heterogeneous supercomputers and recent algorithmic developments.\nImplementation details which are indispensable for achieving high efficiency\nare pointed out and their necessity is justified by performance measurements or\npredictions based on performance models. The library code and several\napplications are available as open source. We also provide instructions on how\nto make use of GHOST in existing software packages, together with a case study\nwhich demonstrates the applicability and performance of GHOST as a component\nwithin a larger software stack."
},{
    "category": "math.OC", 
    "doi": "10.1007/s10766-016-0464-z", 
    "link": "http://arxiv.org/pdf/1508.01982v3", 
    "title": "JuMP: A Modeling Language for Mathematical Optimization", 
    "arxiv-id": "1508.01982v3", 
    "author": "Miles Lubin", 
    "publish": "2015-08-09T03:55:19Z", 
    "summary": "JuMP is an open-source modeling language that allows users to express a wide\nrange of optimization problems (linear, mixed-integer, quadratic,\nconic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic\nsyntax. JuMP takes advantage of advanced features of the Julia programming\nlanguage to offer unique functionality while achieving performance on par with\ncommercial modeling tools for standard tasks. In this work we will provide\nbenchmarks, present the novel aspects of the implementation, and discuss how\nJuMP can be extended to new problem classes and composed with state-of-the-art\ntools for visualization and interactivity."
},{
    "category": "cs.DM", 
    "doi": "10.1371/journal.pone.0147935", 
    "link": "http://arxiv.org/pdf/1508.04740v2", 
    "title": "Marathon: An open source software library for the analysis of   Markov-Chain Monte Carlo algorithms", 
    "arxiv-id": "1508.04740v2", 
    "author": "Annabell Berger", 
    "publish": "2015-08-19T18:55:18Z", 
    "summary": "In this paper, we consider the Markov-Chain Monte Carlo (MCMC) approach for\nrandom sampling of combinatorial objects. The running time of such an algorithm\ndepends on the total mixing time of the underlying Markov chain and is unknown\nin general. For some Markov chains, upper bounds on this total mixing time\nexist but are too large to be applicable in practice. We try to answer the\nquestion, whether the total mixing time is close to its upper bounds, or if\nthere is a significant gap between them. In doing so, we present the software\nlibrary marathon which is designed to support the analysis of MCMC based\nsampling algorithms. The main application of this library is to compute\nproperties of so-called state graphs which represent the structure of Markov\nchains. We use marathon to investigate the quality of several bounding methods\non four well-known Markov chains for sampling perfect matchings and bipartite\ngraph realizations. In a set of experiments, we compute the total mixing time\nand several of its bounds for a large number of input instances. We find that\nthe upper bound gained by the famous canonical path method is several\nmagnitudes larger than the total mixing time and deteriorates with growing\ninput size. In contrast, the spectral bound is found to be a precise\napproximation of the total mixing time."
},{
    "category": "cs.MS", 
    "doi": "10.1371/journal.pone.0147935", 
    "link": "http://arxiv.org/pdf/1508.05470v2", 
    "title": "Non-Metric Space Library Manual", 
    "arxiv-id": "1508.05470v2", 
    "author": "Leonid Boytsov", 
    "publish": "2015-08-22T04:43:36Z", 
    "summary": "This document describes a library for similarity searching. Even though the\nlibrary contains a variety of metric-space access methods, our main focus is on\nsearch methods for non-metric spaces. Because there are fewer exact solutions\nfor non-metric spaces, many of our methods give only approximate answers. Thus,\nthe methods are evaluated in terms of efficiency-effectiveness trade-offs\nrather than merely in terms of their efficiency. Our goal is, therefore, to\nprovide not only state-of-the-art approximate search methods for both\nnon-metric and metric spaces, but also the tools to measure search quality. We\nconcentrate on technical details, i.e., how to compile the code, run the\nbenchmarks, evaluate results, and use our code in other applications.\nAdditionally, we explain how to extend the code by adding new search methods\nand spaces."
},{
    "category": "cs.MS", 
    "doi": "10.1371/journal.pone.0147935", 
    "link": "http://arxiv.org/pdf/1509.01347v3", 
    "title": "Verificarlo: checking floating point accuracy through Monte Carlo   Arithmetic", 
    "arxiv-id": "1509.01347v3", 
    "author": "Eric Petit", 
    "publish": "2015-09-04T06:20:18Z", 
    "summary": "Numerical accuracy of floating point computation is a well studied topic\nwhich has not made its way to the end-user in scientific computing. Yet, it has\nbecome a critical issue with the recent requirements for code modernization to\nharness new highly parallel hardware and perform higher resolution computation.\nTo democratize numerical accuracy analysis, it is important to propose tools\nand methodologies to study large use cases in a reliable and automatic way. In\nthis paper, we propose verificarlo, an extension to the LLVM compiler to\nautomatically use Monte Carlo Arithmetic in a transparent way for the end-user.\nIt supports all the major languages including C, C++, and Fortran. Unlike\nsource-to-source approaches, our implementation captures the influence of\ncompiler optimizations on the numerical accuracy. We illustrate how Monte Carlo\nArithmetic using the verificarlo tool outperforms the existing approaches on\nvarious use cases and is a step toward automatic numerical analysis."
},{
    "category": "cs.DC", 
    "doi": "10.1137/15M1040049", 
    "link": "http://arxiv.org/pdf/1509.04627v1", 
    "title": "A tetrahedral space-filling curve for non-conforming adaptive meshes", 
    "arxiv-id": "1509.04627v1", 
    "author": "Johannes Holke", 
    "publish": "2015-09-15T16:16:54Z", 
    "summary": "We introduce a space-filling curve for triangular and tetrahedral\nred-refinement that can be computed using bitwise interleaving operations\nsimilar to the well-known Z-order or Morton curve for cubical meshes. To store\nthe information necessary for random access, we suggest 10 bytes per triangle\nand 14 bytes per tetrahedron. We present algorithms that compute the parent,\nchildren, and face-neighbors of a mesh element in constant time, as well as the\nnext and previous element in the space-filling curve and whether a given\nelement is on the boundary of the root simplex or not. Furthermore, we prove\nthat the maximum number of face-connected components in any segment of this\ncurve is bounded by twice the refinement level plus one (minus one in 2d) and\nthat the number of corner-connected components is bounded by two. We conclude\nwith a scalability demonstration that creates and adapts selected meshes on a\nlarge distributed-memory system."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1509.07659v2", 
    "title": "A dedicated greedy pursuit algorithm for sparse spectral representation   of music sound", 
    "arxiv-id": "1509.07659v2", 
    "author": "Gagan Aggarwal", 
    "publish": "2015-09-25T10:03:17Z", 
    "summary": "A dedicated algorithm for sparse spectral representation of music sound is\npresented. The goal is to enable the representation of a piece of music signal,\nas a linear superposition of as few spectral components as possible. A\nrepresentation of this nature is said to be sparse. In the present context\nsparsity is accomplished by greedy selection of the spectral components, from\nan overcomplete set called a dictionary. The proposed algorithm is tailored to\nbe applied with trigonometric dictionaries. Its distinctive feature being that\nit avoids the need for the actual construction of the whole dictionary, by\nimplementing the required operations via the Fast Fourier Transform. The\nachieved sparsity is theoretically equivalent to that rendered by the\nOrthogonal Matching Pursuit method. The contribution of the proposed dedicated\nimplementation is to extend the applicability of the standard Orthogonal\nMatching Pursuit algorithm, by reducing its storage and computational demands.\nThe suitability of the approach for producing sparse spectral models is\nillustrated by comparison with the traditional method, in the line of the Short\nTime Fourier Transform, involving only the corresponding orthonormal\ntrigonometric basis."
},{
    "category": "cs.OH", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.01122v1", 
    "title": "On The Evolution Of User Support Topics in Computational Science and   Engineering Software", 
    "arxiv-id": "1510.01122v1", 
    "author": "B. Smith", 
    "publish": "2015-10-05T12:19:46Z", 
    "summary": "We investigate ten years of user support emails in the large-scale solver\nlibrary PETSc in order to identify changes in user requests. For this purpose\nwe assign each email thread to one or several categories describing the type of\nsupport request. We find that despite several changes in hardware architecture\nas well programming models, the relative share of emails for the individual\ncategories does not show a notable change over time. This is particularly\nremarkable as the total communication volume has increased four-fold in the\nconsidered time frame, indicating a considerable growth of the user base. Our\ndata also demonstrates that user support cannot be substituted with what is\noften referred to as 'better documentation' and that the involvement of core\ndevelopers in user support is essential."
},{
    "category": "cs.MS", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.02545v3", 
    "title": "Comparative computational results for some vertex and facet enumeration   codes", 
    "arxiv-id": "1510.02545v3", 
    "author": "Charles Jordan", 
    "publish": "2015-10-09T02:07:10Z", 
    "summary": "We report some computational results comparing parallel and sequential codes\nfor vertex/facet enumeration problems for convex polyhedra. The problems chosen\nspan the range from simple to highly degenerate polytopes. We tested one code\n(lrs) based on pivoting and four codes (cddr+, ppl, normaliz, PORTA) based on\nthe double description method. normaliz employs parallelization as do the codes\nplrs and mplrs which are based on lrs. We tested these codes using various\nhardware configurations with up to 1200 cores. Major speedups were obtained by\nparallelization, particularly by the code mplrs which uses MPI and can operate\non clusters of machines."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.04068v1", 
    "title": "Sapporo2: A versatile direct $N$-body library", 
    "arxiv-id": "1510.04068v1", 
    "author": "Simon Portegies Zwart", 
    "publish": "2015-10-14T12:56:13Z", 
    "summary": "Astrophysical direct $N$-body methods have been one of the first production\nalgorithms to be implemented using NVIDIA's CUDA architecture. Now, almost\nseven years later, the GPU is the most used accelerator device in astronomy for\nsimulating stellar systems. In this paper we present the implementation of the\nSapporo2 $N$-body library, which allows researchers to use the GPU for $N$-body\nsimulations with little to no effort. The first version, released five years\nago, is actively used, but lacks advanced features and versatility in numerical\nprecision and support for higher order integrators. In this updated version we\nhave rebuilt the code from scratch and added support for OpenCL,\nmulti-precision and higher order integrators. We show how to tune these codes\nfor different GPU architectures and present how to continue utilizing the GPU\noptimal even when only a small number of particles ($N < 100$) is integrated.\nThis careful tuning allows Sapporo2 to be faster than Sapporo1 even with the\nadded options and double precision data loads. The code runs on a range of\nNVIDIA and AMD GPUs in single and double precision accuracy. With the addition\nof OpenCL support the library is also able to run on CPUs and other\naccelerators that support OpenCL."
},{
    "category": "cond-mat.str-el", 
    "doi": "10.1016/j.cpc.2016.07.018", 
    "link": "http://arxiv.org/pdf/1511.00863v2", 
    "title": "Exact diagonalization of quantum lattice models on coprocessors", 
    "arxiv-id": "1511.00863v2", 
    "author": "Ari Harju", 
    "publish": "2015-11-03T11:26:28Z", 
    "summary": "We implement the Lanczos algorithm on an Intel Xeon Phi coprocessor and\ncompare its performance to a multi-core Intel Xeon CPU and an NVIDIA graphics\nprocessor. The Xeon and the Xeon Phi are parallelized with OpenMP and the\ngraphics processor is programmed with CUDA. The performance is evaluated by\nmeasuring the execution time of a single step in the Lanczos algorithm. We\nstudy two quantum lattice models with different particle numbers, and conclude\nthat for small systems, the multi-core CPU is the fastest platform, while for\nlarge systems, the graphics processor is the clear winner, reaching speedups of\nup to 7.6 compared to the CPU. The Xeon Phi outperforms the CPU with\nsufficiently large particle number, reaching a speedup of 2.5."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.07.018", 
    "link": "http://arxiv.org/pdf/1511.02171v1", 
    "title": "Multi-Threaded Dense Linear Algebra Libraries for Low-Power Asymmetric   Multicore Processors", 
    "arxiv-id": "1511.02171v1", 
    "author": "Enrique S. Quintana-Ort\u00ed", 
    "publish": "2015-11-06T17:36:20Z", 
    "summary": "Dense linear algebra libraries, such as BLAS and LAPACK, provide a relevant\ncollection of numerical tools for many scientific and engineering applications.\nWhile there exist high performance implementations of the BLAS (and LAPACK)\nfunctionality for many current multi-threaded architectures,the adaption of\nthese libraries for asymmetric multicore processors (AMPs)is still pending. In\nthis paper we address this challenge by developing an asymmetry-aware\nimplementation of the BLAS, based on the BLIS framework, and tailored for AMPs\nequipped with two types of cores: fast/power hungry versus slow/energy\nefficient. For this purpose, we integrate coarse-grain and fine-grain\nparallelization strategies into the library routines which, respectively,\ndynamically distribute the workload between the two core types and statically\nrepartition this work among the cores of the same type.\n  Our results on an ARM big.LITTLE processor embedded in the Exynos 5422 SoC,\nusing the asymmetry-aware version of the BLAS and a plain migration of the\nlegacy version of LAPACK, experimentally assess the benefits, limitations, and\npotential of this approach."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.07.018", 
    "link": "http://arxiv.org/pdf/1511.03415v1", 
    "title": "The Dune FoamGrid implementation for surface and network grids", 
    "arxiv-id": "1511.03415v1", 
    "author": "Bernd Flemisch", 
    "publish": "2015-11-11T08:23:46Z", 
    "summary": "We present FoamGrid, a new implementation of the DUNE grid interface.\nFoamGrid implements one- and two-dimensional grids in a physical space of\narbitrary dimension, which allows for grids for curved domains. Even more, the\ngrids are not expected to have a manifold structure, i.e., more than two\nelements can share a common facet. This makes FoamGrid the grid data structure\nof choice for simulating structures such as foams, discrete fracture networks,\nor network flow problems. FoamGrid implements adaptive non-conforming\nrefinement with element parametrizations. As an additional feature it allows\nremoval and addition of elements in an existing grid, which makes FoamGrid\nsuitable for network growth problems. We show how to use FoamGrid, with\nparticular attention to the extensions of the grid interface needed to handle\nnon-manifold topology and grid growth. Three numerical examples demonstrate the\npossibilities offered by FoamGrid."
},{
    "category": "hep-ph", 
    "doi": "10.1016/j.cpc.2016.03.013", 
    "link": "http://arxiv.org/pdf/1511.03614v1", 
    "title": "FIESTA 4: optimized Feynman integral calculations with GPU support", 
    "arxiv-id": "1511.03614v1", 
    "author": "Alexander V. Smirnov", 
    "publish": "2015-10-23T16:07:56Z", 
    "summary": "This paper presents a new major release of the program FIESTA (Feynman\nIntegral Evaluation by a Sector decomposiTion Approach). The new release is\nmainly aimed at optimal performance at large scales when one is increasing the\nnumber of sampling points in order to reduce the uncertainty estimates. The\nrelease now supports graphical processor units (GPU) for the numerical\nintegration, methods to optimize cluster-usage, as well as other speed, memory,\nand stability improvements."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.03.013", 
    "link": "http://arxiv.org/pdf/1511.03703v1", 
    "title": "Embedded Ensemble Propagation for Improving Performance, Portability and   Scalability of Uncertainty Quantification on Emerging Computational   Architectures", 
    "arxiv-id": "1511.03703v1", 
    "author": "S. Rajamanickam", 
    "publish": "2015-11-11T21:55:35Z", 
    "summary": "Quantifying simulation uncertainties is a critical component of rigorous\npredictive simulation. A key component of this is forward propagation of\nuncertainties in simulation input data to output quantities of interest.\nTypical approaches involve repeated sampling of the simulation over the\nuncertain input data, and can require numerous samples when accurately\npropagating uncertainties from large numbers of sources. Often simulation\nprocesses from sample to sample are similar and much of the data generated from\neach sample evaluation could be reused. We explore a new method for\nimplementing sampling methods that simultaneously propagates groups of samples\ntogether in an embedded fashion, which we call embedded ensemble propagation.\nWe show how this approach takes advantage of properties of modern computer\narchitectures to improve performance by enabling reuse between samples,\nreducing memory bandwidth requirements, improving memory access patterns,\nimproving opportunities for fine-grained parallelization, and reducing\ncommunication costs. We describe a software technique for implementing embedded\nensemble propagation based on the use of C++ templates and describe its\nintegration with various scientific computing libraries within Trilinos. We\ndemonstrate improved performance, portability and scalability for the approach\napplied to the simulation of partial differential equations on a variety of\nCPU, GPU, and accelerator architectures, including up to 131,072 cores on a\nCray XK7 (Titan)."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.03.013", 
    "link": "http://arxiv.org/pdf/1511.03742v2", 
    "title": "GEMMbench: a framework for reproducible and collaborative benchmarking   of matrix multiplication", 
    "arxiv-id": "1511.03742v2", 
    "author": "Anton Lokhmotov", 
    "publish": "2015-11-12T01:02:58Z", 
    "summary": "The generic matrix-matrix multiplication (GEMM) is arguably the most popular\ncomputational kernel of the 20th century. Yet, surprisingly, no common\nmethodology for evaluating GEMM performance has been established over the many\ndecades of using GEMM for comparing architectures, compilers and ninja-class\nprogrammers.\n  We introduce GEMMbench, a framework and methodology for evaluating\nperformance of GEMM implementations. GEMMbench is implemented on top of\nCollective Knowledge (CK), a lightweight framework for reproducible and\ncollaborative R&D in computer systems. Using CK allows the R&D community to\ncrowdsource hand-written and compiler-generated GEMM implementations and to\nstudy their performance across multiple platforms, data sizes and data types.\n  Our initial implementation supports hand-written OpenCL kernels operating on\nmatrices consisting of single- and double-precision floating-point values, and\nproducing single or multiple output elements per work-item (via thread\ncoarsening and vectorization)."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2016.03.013", 
    "link": "http://arxiv.org/pdf/1511.07174v1", 
    "title": "Developing a High Performance Software Library with MPI and CUDA for   Matrix Computations", 
    "arxiv-id": "1511.07174v1", 
    "author": "Tudorel Andrei", 
    "publish": "2015-11-23T11:06:12Z", 
    "summary": "Nowadays, the paradigm of parallel computing is changing. CUDA is now a\npopular programming model for general purpose computations on GPUs and a great\nnumber of applications were ported to CUDA obtaining speedups of orders of\nmagnitude comparing to optimized CPU implementations. Hybrid approaches that\ncombine the message passing model with the shared memory model for parallel\ncomputing are a solution for very large applications. We considered a\nheterogeneous cluster that combines the CPU and GPU computations using MPI and\nCUDA for developing a high performance linear algebra library. Our library\ndeals with large linear systems solvers because they are a common problem in\nthe fields of science and engineering. Direct methods for computing the\nsolution of such systems can be very expensive due to high memory requirements\nand computational cost. An efficient alternative are iterative methods which\ncomputes only an approximation of the solution. In this paper we present an\nimplementation of a library that uses a hybrid model of computation using MPI\nand CUDA implementing both direct and iterative linear systems solvers. Our\nlibrary implements LU and Cholesky factorization based solvers and some of the\nnon-stationary iterative methods using the MPI/CUDA combination. We compared\nthe performance of our MPI/CUDA implementation with classic programs written to\nbe run on a single CPU."
},{
    "category": "cs.DC", 
    "doi": "10.1080/17445760.2015.1118478", 
    "link": "http://arxiv.org/pdf/1511.07261v1", 
    "title": "A Python Extension for the Massively Parallel Multiphysics Simulation   Framework waLBerla", 
    "arxiv-id": "1511.07261v1", 
    "author": "Ulrich R\u00fcde", 
    "publish": "2015-11-23T15:06:47Z", 
    "summary": "We present a Python extension to the massively parallel HPC simulation\ntoolkit waLBerla. waLBerla is a framework for stencil based algorithms\noperating on block-structured grids, with the main application field being\nfluid simulations in complex geometries using the lattice Boltzmann method.\nCareful performance engineering results in excellent node performance and good\nscalability to over 400,000 cores. To increase the usability and flexibility of\nthe framework, a Python interface was developed. Python extensions are used at\nall stages of the simulation pipeline: They simplify and automate scenario\nsetup, evaluation, and plotting. We show how our Python interface outperforms\nthe existing text-file-based configuration mechanism, providing features like\nautomatic nondimensionalization of physical quantities and handling of complex\nparameter dependencies. Furthermore, Python is used to process and evaluate\nresults while the simulation is running, leading to smaller output files and\nthe possibility to adjust parameters dependent on the current simulation state.\nC++ data structures are exported such that a seamless interfacing to other\nnumerical Python libraries is possible. The expressive power of Python and the\nperformance of C++ make development of efficient code with low time effort\npossible."
},{
    "category": "cs.MS", 
    "doi": "10.1080/17445760.2015.1118478", 
    "link": "http://arxiv.org/pdf/1601.02683v1", 
    "title": "Software for enumerative and analytic combinatorics", 
    "arxiv-id": "1601.02683v1", 
    "author": "Andrew MacFie", 
    "publish": "2016-01-11T23:03:10Z", 
    "summary": "We survey some general-purpose symbolic software packages that implement\nalgorithms from enumerative and analytic combinatorics. Software for the\nfollowing areas is covered: basic combinatorial objects, symbolic\ncombinatorics, P\\'olya theory, combinatorial species, and asymptotics. We\ndescribe the capabilities that the packages offer as well as some of the\nalgorithms used, and provide links to original documentation. Most of the\npackages are freely downloadable from the web."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.11.001", 
    "link": "http://arxiv.org/pdf/1601.03623v2", 
    "title": "Evaluation of the Partitioned Global Address Space (PGAS) model for an   inviscid Euler solver", 
    "arxiv-id": "1601.03623v2", 
    "author": "Alexander Ostermann", 
    "publish": "2016-01-14T15:30:47Z", 
    "summary": "In this paper we evaluate the performance of Unified Parallel C (which\nimplements the partitioned global address space programming model) using a\nnumerical method that is widely used in fluid dynamics. In order to evaluate\nthe incremental approach to parallelization (which is possible with UPC) and\nits performance characteristics, we implement different levels of optimization\nof the UPC code and compare it with an MPI parallelization on four different\nclusters of the Austrian HPC infrastructure (LEO3, LEO3E, VSC2, VSC3) and on an\nIntel Xeon Phi. We find that UPC is significantly easier to develop in compared\nto MPI and that the performance achieved is comparable to MPI in most\nsituations. The obtained results show worse performance (on VSC2), competitive\nperformance (on LEO3, LEO3E and VSC3), and superior performance (on the Intel\nXeon Phi)."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.11.001", 
    "link": "http://arxiv.org/pdf/1602.01421v3", 
    "title": "An SSD-based eigensolver for spectral analysis on billion-node graphs", 
    "arxiv-id": "1602.01421v3", 
    "author": "Alexander S. Szalay", 
    "publish": "2016-02-03T19:23:44Z", 
    "summary": "Many eigensolvers such as ARPACK and Anasazi have been developed to compute\neigenvalues of a large sparse matrix. These eigensolvers are limited by the\ncapacity of RAM. They run in memory of a single machine for smaller eigenvalue\nproblems and require the distributed memory for larger problems.\n  In contrast, we develop an SSD-based eigensolver framework called FlashEigen,\nwhich extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph\nwith hundreds of millions or even billions of vertices in a single machine.\nFlashEigen performs sparse matrix multiplication in a semi-external memory\nfashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in\nmemory. We store the entire vector subspace on SSDs and reduce I/O to improve\nperformance through caching the most recent dense matrix. Our result shows that\nFlashEigen is able to achieve 40%-60% performance of its in-memory\nimplementation and has performance comparable to the Anasazi eigensolvers on a\nmachine with 48 CPU cores. Furthermore, it is capable of scaling to a graph\nwith 3.4 billion vertices and 129 billion edges. It takes about four hours to\ncompute eight eigenvalues of the billion-node graph using 120 GB memory."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.02.005", 
    "link": "http://arxiv.org/pdf/1602.03638v1", 
    "title": "High performance Python for direct numerical simulations of turbulent   flows", 
    "arxiv-id": "1602.03638v1", 
    "author": "Hans Petter Langtangen", 
    "publish": "2016-02-11T08:12:37Z", 
    "summary": "Direct Numerical Simulations (DNS) of the Navier Stokes equations is an\ninvaluable research tool in fluid dynamics. Still, there are few publicly\navailable research codes and, due to the heavy number crunching implied,\navailable codes are usually written in low-level languages such as C/C++ or\nFortran. In this paper we describe a pure scientific Python pseudo-spectral DNS\ncode that nearly matches the performance of C++ for thousands of processors and\nbillions of unknowns. We also describe a version optimized through Cython, that\nis found to match the speed of C++. The solvers are written from scratch in\nPython, both the mesh, the MPI domain decomposition, and the temporal\nintegrators. The solvers have been verified and benchmarked on the Shaheen\nsupercomputer at the KAUST supercomputing laboratory, and we are able to show\nvery good scaling up to several thousand cores.\n  A very important part of the implementation is the mesh decomposition (we\nimplement both slab and pencil decompositions) and 3D parallel Fast Fourier\nTransforms (FFT). The mesh decomposition and FFT routines have been implemented\nin Python using serial FFT routines (either NumPy, pyFFTW or any other serial\nFFT module), NumPy array manipulations and with MPI communications handled by\nMPI for Python (mpi4py). We show how we are able to execute a 3D parallel FFT\nin Python for a slab mesh decomposition using 4 lines of compact Python code,\nfor which the parallel performance on Shaheen is found to be slightly better\nthan similar routines provided through the FFTW library. For a pencil mesh\ndecomposition 7 lines of code is required to execute a transform."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.02.005", 
    "link": "http://arxiv.org/pdf/1602.06763v1", 
    "title": "Recursive Algorithms for Dense Linear Algebra: The ReLAPACK Collection", 
    "arxiv-id": "1602.06763v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-02-22T13:21:05Z", 
    "summary": "To exploit both memory locality and the full performance potential of highly\ntuned kernels, dense linear algebra libraries such as LAPACK commonly implement\noperations as blocked algorithms. However, to achieve next-to-optimal\nperformance with such algorithms, significant tuning is required. On the other\nhand, recursive algorithms are virtually tuning free, and yet attain similar\nperformance. In this paper, we first analyze and compare blocked and recursive\nalgorithms in terms of performance, and then introduce ReLAPACK, an open-source\nlibrary of recursive algorithms to seamlessly replace most of LAPACK's blocked\nalgorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,\nand even improves upon the performance of optimizes libraries."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.cpc.2016.02.005", 
    "link": "http://arxiv.org/pdf/1602.07527v1", 
    "title": "Differentiation of the Cholesky decomposition", 
    "arxiv-id": "1602.07527v1", 
    "author": "Iain Murray", 
    "publish": "2016-02-24T14:35:31Z", 
    "summary": "We review strategies for differentiating matrix-based computations, and\nderive symbolic and algorithmic update rules for differentiating expressions\ncontaining the Cholesky decomposition. We recommend new `blocked' algorithms,\nbased on differentiating the Cholesky algorithm DPOTRF in the LAPACK library,\nwhich uses `Level 3' matrix-matrix operations from BLAS, and so is\ncache-friendly and easy to parallelize. For large matrices, the resulting\nalgorithms are the fastest way to compute Cholesky derivatives, and are an\norder of magnitude faster than the algorithms in common usage. In some\ncomputing environments, symbolically-derived updates are faster for small\nmatrices than those based on differentiating Cholesky algorithms. The symbolic\nand algorithmic approaches can be combined to get the best of both worlds."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1602.08477v1", 
    "title": "Alpaka - An Abstraction Library for Parallel Kernel Acceleration", 
    "arxiv-id": "1602.08477v1", 
    "author": "Michael Bussmann", 
    "publish": "2016-02-26T20:49:37Z", 
    "summary": "Porting applications to new hardware or programming models is a tedious and\nerror prone process. Every help that eases these burdens is saving developer\ntime that can then be invested into the advancement of the application itself\ninstead of preserving the status-quo on a new platform.\n  The Alpaka library defines and implements an abstract hierarchical redundant\nparallelism model. The model exploits parallelism and memory hierarchies on a\nnode at all levels available in current hardware. By doing so, it allows to\nachieve platform and performance portability across various types of\naccelerators by ignoring specific unsupported levels and utilizing only the\nones supported on a specific accelerator. All hardware types (multi- and\nmany-core CPUs, GPUs and other accelerators) are supported for and can be\nprogrammed in the same way. The Alpaka C++ template interface allows for\nstraightforward extension of the library to support other accelerators and\nspecialization of its internals for optimization.\n  Running Alpaka applications on a new (and supported) platform requires the\nchange of only one source code line instead of a lot of \\#ifdefs."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1602.08991v1", 
    "title": "Extending DUNE: The dune-xt modules", 
    "arxiv-id": "1602.08991v1", 
    "author": "Felix Schindler", 
    "publish": "2016-02-25T22:42:50Z", 
    "summary": "We present our effort to extend and complement the core modules of the\nDistributed and Unified Numerics Environment DUNE (http://dune-project.org) by\na well tested and structured collection of utilities and concepts. We describe\nkey elements of our four modules dune-xt-common, dune-xt-grid, dune-xt-la and\ndune-xt-functions, which aim at further enabling the programming of generic\nalgorithms within DUNE as well as adding an extra layer of usability and\nconvenience."
},{
    "category": "cs.NA", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1603.01793v1", 
    "title": "Coupling of finite element method with boundary algebraic equations", 
    "arxiv-id": "1603.01793v1", 
    "author": "A. V. Shanin", 
    "publish": "2016-03-06T06:21:30Z", 
    "summary": "Recently, a combined approach of CFIE--BAE has been proposed by authors for\nsolving external scattering problems in acoustics. CFIE stands for\ncombined-field integral equations, and BAE is the method of boundary\nalgebraical equation. The combined method is, essentially, a discrete analogue\nof the boundary element method (BEM), having none of its disadvantages. Namely,\ndue to the discrete nature of BAE one should not compute quadratures of\noversingular integrals. Moreover, due to CFIE formulation, the method does not\npossess spurious resonances.\n  However, the CFIE--BAE method has an important drawback. Since the modelling\nis performed in a regular discrete space, the shape of the obstacle should be\nassembled of elementary \"bricks\", so smooth scatterers (like spheres,\ncylinders, etc) are approximated with a poor accuracy. This loss of accuracy\nbecomes the bottleneck of the method. Here this disadvantage is overcome. The\nCFIE--BAE method developed for regular meshing of the outer space is coupled in\na standard way with a relatively small irregular mesh enabling one to describe\nthe shape of the obstacle accurately enough."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1603.06907v5", 
    "title": "micompr: An R Package for Multivariate Independent Comparison of   Observations", 
    "arxiv-id": "1603.06907v5", 
    "author": "Agostinho C. Rosa", 
    "publish": "2016-03-22T18:57:41Z", 
    "summary": "The R package micompr implements a procedure for assessing if two or more\nmultivariate samples are drawn from the same distribution. The procedure uses\nprincipal component analysis to convert multivariate observations into a set of\nlinearly uncorrelated statistical measures, which are then compared using a\nnumber of statistical methods. This technique is independent of the\ndistributional properties of samples and automatically selects features that\nbest explain their differences. The procedure is appropriate for comparing\nsamples of time series, images, spectrometric measures or similar\nhigh-dimension multivariate observations."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1603.07342v1", 
    "title": "BEANS - a software package for distributed Big Data analysis", 
    "arxiv-id": "1603.07342v1", 
    "author": "Arkadiusz Hypki", 
    "publish": "2016-03-23T20:14:34Z", 
    "summary": "BEANS software is a web based, easy to install and maintain, new tool to\nstore and analyse data in a distributed way for a massive amount of data. It\nprovides a clear interface for querying, filtering, aggregating, and plotting\ndata from an arbitrary number of datasets. Its main purpose is to simplify the\nprocess of storing, examining and finding new relations in the so-called Big\nData.\n  Creation of BEANS software is an answer to the growing needs of the\nastronomical community to have a versatile tool to store, analyse and compare\nthe complex astrophysical numerical simulations with observations (e.g.\nsimulations of the Galaxy or star clusters with the Gaia archive). However,\nthis software was built in a general form and it is ready to use in any other\nresearch field or open source software."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1604.03570v1", 
    "title": "BoxLib with Tiling: An AMR Software Framework", 
    "arxiv-id": "1604.03570v1", 
    "author": "Didem Unat", 
    "publish": "2016-04-12T20:13:04Z", 
    "summary": "In this paper we introduce a block-structured adaptive mesh refinement (AMR)\nsoftware framework that incorporates tiling, a well-known loop transformation.\nBecause the multiscale, multiphysics codes built in BoxLib are designed to\nsolve complex systems at high resolution, performance on current and next\ngeneration architectures is essential. With the expectation of many more cores\nper node on next generation architectures, the ability to effectively utilize\nthreads within a node is essential, and the current model for parallelization\nwill not be sufficient. We describe a new version of BoxLib in which the tiling\nconstructs are embedded so that BoxLib-based applications can easily realize\nexpected performance gains without extra effort on the part of the application\ndeveloper. We also discuss a path forward to enable future versions of BoxLib\nto take advantage of NUMA-aware optimizations using the TiDA portable library."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1605.00998v1", 
    "title": "Blackbox: A procedure for parallel optimization of expensive black-box   functions", 
    "arxiv-id": "1605.00998v1", 
    "author": "Yannis Korkolis", 
    "publish": "2016-05-03T17:49:27Z", 
    "summary": "This note provides a description of a procedure that is designed to\nefficiently optimize expensive black-box functions. It uses the response\nsurface methodology by incorporating radial basis functions as the response\nmodel. A simple method based on a Latin hypercube is used for initial sampling.\nA modified version of CORS algorithm with space rescaling is used for the\nsubsequent sampling. The procedure is able to scale on multicore processors by\nperforming multiple function evaluations in parallel. The source code of the\nprocedure is written in Python."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1605.02532v2", 
    "title": "HLinear: Exact Dense Linear Algebra in Haskell", 
    "arxiv-id": "1605.02532v2", 
    "author": "Martin Westerholt-Raum", 
    "publish": "2016-05-09T11:24:39Z", 
    "summary": "We present an implementation in the functional programming language Haskell\nof the PLE decomposition of matrices over division rings. We discover in our\nbenchmarks that in a relevant number of cases it is significantly faster than\nthe C-based implementation provided in FLINT. Describing the guiding principles\nof our work, we introduce the reader to basic ideas from high performance\nfunctional programming."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1606.00541v1", 
    "title": "Parallel Triangular Solvers on GPU", 
    "arxiv-id": "1606.00541v1", 
    "author": "Bo Yang", 
    "publish": "2016-06-02T05:54:09Z", 
    "summary": "In this paper, we investigate GPU based parallel triangular solvers\nsystematically. The parallel triangular solvers are fundamental to incomplete\nLU factorization family preconditioners and algebraic multigrid solvers. We\ndevelop a new matrix format suitable for GPU devices. Parallel lower triangular\nsolvers and upper triangular solvers are developed for this new data structure.\nWith these solvers, ILU preconditioners and domain decomposition\npreconditioners are developed. Numerical results show that we can speed\ntriangular solvers around seven times faster."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1606.00545v1", 
    "title": "Development of Krylov and AMG linear solvers for large-scale sparse   matrices on GPUs", 
    "arxiv-id": "1606.00545v1", 
    "author": "Zhangxin Chen", 
    "publish": "2016-06-02T06:01:05Z", 
    "summary": "This research introduce our work on developing Krylov subspace and AMG\nsolvers on NVIDIA GPUs. As SpMV is a crucial part for these iterative methods,\nSpMV algorithms for single GPU and multiple GPUs are implemented. A HEC matrix\nformat and a communication mechanism are established. And also, a set of\nspecific algorithms for solving preconditioned systems in parallel environments\nare designed, including ILU(k), RAS and parallel triangular solvers. Based on\nthese work, several Krylov solvers and AMG solvers are developed. According to\nnumerical experiments, favorable acceleration performance is acquired from our\nKrylov solver and AMG solver under various parameter conditions."
},{
    "category": "cs.CE", 
    "doi": "10.1109/IPDPSW.2016.50", 
    "link": "http://arxiv.org/pdf/1606.04987v1", 
    "title": "Automatic finite element implementation of hyperelastic material with a   double numerical differentiation algorithm", 
    "arxiv-id": "1606.04987v1", 
    "author": "Gregory J. Gerling", 
    "publish": "2016-06-13T17:54:41Z", 
    "summary": "In order to accelerate implementation of hyperelastic materials for finite\nelement analysis, we developed an automatic numerical algorithm that only\nrequires the strain energy function. This saves the effort on analytical\nderivation and coding of stress and tangent modulus, which is time-consuming\nand prone to human errors. Using the one-sided Newton difference quotients, the\nproposed algorithm first perturbs deformation gradients and calculate the\ndifference on strain energy to approximate stress. Then, we perturb again to\nget difference in stress to approximate tangent modulus. Accuracy of the\napproximations were evaluated across the perturbation parameter space, where we\nfind the optimal amount of perturbation being $10^{-6}$ to obtain stress and\n$10^{-4}$ to obtain tangent modulus. Single element verification in ABAQUS with\nNeo-Hookean material resulted in a small stress error of only $7\\times10^{-5}$\non average across uniaxial compression and tension, biaxial tension and simple\nshear situations. A full 3D model with Holzapfel anisotropic material for\nartery inflation generated a small relative error of $4\\times10^{-6}$ for\ninflated radius at $25 kPa$ pressure. Results of the verification tests suggest\nthat the proposed numerical method has good accuracy and convergence\nperformance, therefore a good material implementation algorithm in small scale\nmodels and a useful debugging tool for large scale models."
},{
    "category": "cs.MS", 
    "doi": "10.1186/s40537-016-0052-5", 
    "link": "http://arxiv.org/pdf/1606.05385v2", 
    "title": "D2O - a distributed data object for parallel high-performance computing   in Python", 
    "arxiv-id": "1606.05385v2", 
    "author": "T. En\u00dflin", 
    "publish": "2016-06-16T23:19:58Z", 
    "summary": "We introduce D2O, a Python module for cluster-distributed multi-dimensional\nnumerical arrays. It acts as a layer of abstraction between the algorithm code\nand the data-distribution logic. The main goal is to achieve usability without\nlosing numerical performance and scalability. D2O's global interface is similar\nto the one of a numpy.ndarray, whereas the cluster node's local data is\ndirectly accessible for use in customized high-performance modules. D2O is\nwritten in pure Python which makes it portable and easy to use and modify.\nExpensive operations are carried out by dedicated external libraries like numpy\nand mpi4py. The performance of D2O is on a par with numpy for serial\napplications and scales well when moving to an MPI cluster. D2O is open-source\nsoftware available under the GNU General Public License v3 (GPL-3) at\nhttps://gitlab.mpcdf.mpg.de/ift/D2O"
},{
    "category": "cs.NA", 
    "doi": "10.1186/s40537-016-0052-5", 
    "link": "http://arxiv.org/pdf/1607.00648v4", 
    "title": "Quasi-matrix-free hybrid multigrid on dynamically adaptive Cartesian   grids", 
    "arxiv-id": "1607.00648v4", 
    "author": "Tobias Weinzierl", 
    "publish": "2016-07-03T14:54:45Z", 
    "summary": "We introduce a family of spacetree-based multigrid realizations using the\ntree's multiscale nature to derive coarse grids. They align with matrix-free\ngeometric multigrid solvers as they never assemble the system matrices which is\ncumbersome for dynamically adaptive grids and full multigrid. The most\nsophisticated realizations use BoxMG which defines operator-dependent\nprolongation and restriction in combination with Galerkin/Petrov-Galerkin\ncoarse-grid operators. This yields robust solvers for nontrivial elliptic\nproblems. We propose to embed the algebraic, problem- and grid-dependent\nmultigrid operators as stencils into the grid and to evaluate all matrix-vector\nproducts in-situ throughout the grid traversals. While such a realization idiom\nis not literally matrix-free---the grid carries the matrix---we propose to\nswitch to a hierarchical representation of all operators. Only differences of\nalgebraic operators to their geometric counterparts are held. These\nhierarchical differences can be stored and exchanged with small memory\nfootprint. Our realizations support arbitrary dynamically adaptive grids while\nthey vertically integrate the multilevel operations through spacetree\nlinearization. This yields good memory access characteristics, while standard\ncolouring of mesh entities with domain decomposition allow us to use parallel\nmanycore clusters. Our techniques yield reasonably mature and robust\nimplementation blueprints for non-trivial multigrid solver parts running on\nparallel machines."
},{
    "category": "cs.MS", 
    "doi": "10.3934/Math.2016.3.261", 
    "link": "http://arxiv.org/pdf/1607.01191v1", 
    "title": "Best Practices for Replicability, Reproducibility and Reusability of   Computer-Based Experiments Exemplified by Model Reduction Software", 
    "arxiv-id": "1607.01191v1", 
    "author": "Jens Saak", 
    "publish": "2016-07-05T11:02:45Z", 
    "summary": "Over the recent years the importance of numerical experiments has gradually\nbeen more recognized. Nonetheless, sufficient documentation of how\ncomputational results have been obtained is often not available. Especially in\nthe scientific computing and applied mathematics domain this is crucial, since\nnumerical experiments are usually employed to verify the proposed hypothesis in\na publication. This work aims to propose standards and best practices for the\nsetup and publication of numerical experiments. Naturally, this amounts to a\nguideline for development, maintenance, and publication of numerical research\nsoftware. Such a primer will enable the replicability and reproducibility of\ncomputer-based experiments and published results and also promote the\nreusability of the associated software."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2935323.2935328", 
    "link": "http://arxiv.org/pdf/1607.01249v1", 
    "title": "TTC: A Tensor Transposition Compiler for Multiple Architectures", 
    "arxiv-id": "1607.01249v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-07-05T13:53:57Z", 
    "summary": "We consider the problem of transposing tensors of arbitrary dimension and\ndescribe TTC, an open source domain-specific parallel compiler. TTC generates\noptimized parallel C++/CUDA C code that achieves a significant fraction of the\nsystem's peak memory bandwidth. TTC exhibits high performance across multiple\narchitectures, including modern AVX-based systems (e.g.,~Intel Haswell, AMD\nSteamroller), Intel's Knights Corner as well as different CUDA-based GPUs such\nas NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a\nmeaningful baseline implementation generated by external C++ compilers; the\nresults suggest that a domain-specific compiler can outperform its general\npurpose counterpart significantly: For instance, comparing with Intel's latest\nC++ compiler on the Haswell and Knights Corner architecture, TTC yields\nspeedups of up to $8\\times$ and $32\\times$, respectively. We also showcase\nTTC's support for multiple leading dimensions, making it a suitable candidate\nfor the generation of performance-critical packing functions that are at the\ncore of the ubiquitous BLAS 3 routines."
},{
    "category": "math.NA", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1607.04254v1", 
    "title": "Composing Scalable Nonlinear Algebraic Solvers", 
    "arxiv-id": "1607.04254v1", 
    "author": "Xuemin Tu", 
    "publish": "2016-07-14T19:21:43Z", 
    "summary": "Most efficient linear solvers use composable algorithmic components, with the\nmost common model being the combination of a Krylov accelerator and one or more\npreconditioners. A similar set of concepts may be used for nonlinear algebraic\nsystems, where nonlinear composition of different nonlinear solvers may\nsignificantly improve the time to solution. We describe the basic concepts of\nnonlinear composition and preconditioning and present a number of solvers\napplicable to nonlinear partial differential equations. We have developed a\nsoftware framework in order to easily explore the possible combinations of\nsolvers. We show that the performance gains from using composed solvers can be\nsubstantial compared with gains from standard Newton-Krylov methods."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1607.04767v1", 
    "title": "Optimized Automatic Code Generation for Geometric Algebra Based   Algorithms with Ray Tracing Application", 
    "arxiv-id": "1607.04767v1", 
    "author": "Ahmad Hosney Awad Eid", 
    "publish": "2016-07-16T16:54:39Z", 
    "summary": "Automatic code generation for low-dimensional geometric algorithms is capable\nof producing efficient low-level software code through a high-level geometric\ndomain specific language. Geometric Algebra (GA) is one of the most suitable\nalgebraic systems for being the base for such code generator. This work\npresents an attempt at realizing such idea in practice. A novel GA-based\ngeometric code generator, called GMac, is proposed. Comparisons to similar\nGA-based code generators are provided. The possibility of fully benefiting from\nthe symbolic power of GA while obtaining good performance and maintainability\nof software implementations is illustrated through a ray tracing application."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.00099v1", 
    "title": "TRIOT: Faster iteration over multidimensional arrays in C++11", 
    "arxiv-id": "1608.00099v1", 
    "author": "Oliver Serang", 
    "publish": "2016-07-30T10:40:29Z", 
    "summary": "Tensor indexing is a fundamental component of numeric algorithms and is used\nin many programming languages and across many fields. This manuscript proposes\na new template-recursive design pattern for implementing faster vectorizing\nover tensors of different shapes in C++11. The proposed method,\n\"template-recursive iteration over tensors\", is comparable to using nested for\nloops in C and to vectorized code in Fortran, and it outperforms numpy, C-style\ntuple iteration, C-style integer reindexing, and boost::multi_array, and unlike\nsome of those methods, it can be used when the dimension of the tensor is\nunknown at compile time."
},{
    "category": "cs.CG", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.00206v1", 
    "title": "An exact, cache-localized algorithm for the sub-quadratic convolution of   hypercubes", 
    "arxiv-id": "1608.00206v1", 
    "author": "Oliver Serang", 
    "publish": "2016-07-31T10:22:40Z", 
    "summary": "Fast multidimensional convolution can be performed naively in quadratic time\nand can often be performed more efficiently via the Fourier transform; however,\nwhen the dimensionality is large, these algorithms become more challenging. A\nmethod is proposed for performing exact hypercube convolution in sub-quadratic\ntime. The method outperforms FFTPACK, called via numpy, and FFTW, called via\npyfftw) for hypercube convolution. Embeddings in hypercubes can be paired with\nsub-quadratic hypercube convolution method to construct sub-quadratic\nalgorithms for variants of vector convolution."
},{
    "category": "stat.ME", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.00476v2", 
    "title": "R package imputeTestbench to compare imputations methods for univariate   time series", 
    "arxiv-id": "1608.00476v2", 
    "author": "Gualberto Asencio-Cort\u00e9s", 
    "publish": "2016-08-01T15:54:26Z", 
    "summary": "This paper describes the R package imputeTestbench that provides a testbench\nfor comparing imputation methods for missing data in univariate time series.\nThe imputeTestbench package can be used to simulate the amount and type of\nmissing data in a complete dataset and compare filled data using different\nimputation methods. The user has the option to simulate missing data by\nremoving observations completely at random or in blocks of different sizes.\nSeveral default imputation methods are included with the package, including\nhistorical means, linear interpolation, and last observation carried forward.\nThe testbench is not limited to the default functions and users can add or\nremove additional methods using a simple two-step process. The testbench\ncompares the actual missing and imputed data for each method with different\nerror metrics, including RMSE, MAE, and MAPE. Alternative error metrics can\nalso be supplied by the user. The simplicity of use and significant reduction\nin time to compare imputation methods for missing data in univariate time\nseries is a significant advantage of the package. This paper provides an\noverview of the core functions, including a demonstration with examples."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.04152v1", 
    "title": "Computation of the incomplete gamma function for negative values of the   argument", 
    "arxiv-id": "1608.04152v1", 
    "author": "N. M. Temme", 
    "publish": "2016-08-14T23:17:03Z", 
    "summary": "An algorithm for computing the incomplete gamma function $\\gamma^*(a,z)$ for\nreal values of the parameter $a$ and negative real values of the argument $z$\nis presented. The algorithm combines the use of series expansions,\nPoincar\\'e-type expansions, uniform asymptotic expansions and recurrence\nrelations, depending on the parameter region. A relative accuracy $\\sim\n10^{-13}$ in the parameter region $(a,z) \\in [-500,\\,500] \\times [-500,\\,0)$\ncan be obtained when computing the function $\\gamma^*(a,z)$ with the Fortran 90\nmodule IncgamNEG implementing the algorithm."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.04815v3", 
    "title": "A Functional Package for Automatic Solution of Ordinary Differential   Equations with Spectral Methods", 
    "arxiv-id": "1608.04815v3", 
    "author": "Youran Zhang", 
    "publish": "2016-08-17T00:06:48Z", 
    "summary": "We present a Python module named PyCheb, to solve the ordinary differential\nequations by using spectral collocation method. PyCheb incorporates\ndiscretization using Chebyshev points, barycentric interpolation and iterate\nmethods. With this Python module, users can initialize the ODEsolver class by\npassing attributes, including the both sides of a given differential equation,\nboundary conditions, and the number of Chebyshev points, which can also be\ngenerated automatically by the ideal precision, to the constructor of ODEsolver\nclass. Then, the instance of the ODEsolver class can be used to automatically\ndetermine the resolution of the differential equation as well as generate the\ngraph of the high-precision approximate solution. (If you have any questions,\nplease send me an email and I will reply ASAP.\ne-mail:shaohui_liu@qq.com/2013141482143@stu.scu.edu.cn)"
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1608.08658v2", 
    "title": "Devito: automated fast finite difference computation", 
    "arxiv-id": "1608.08658v2", 
    "author": "Gerard Gorman", 
    "publish": "2016-08-30T21:05:21Z", 
    "summary": "Domain specific languages have successfully been used in a variety of fields\nto cleanly express scientific problems as well as to simplify implementation\nand performance opti- mization on different computer architectures. Although a\nlarge number of stencil languages are available, finite differ- ence domain\nspecific languages have proved challenging to design because most practical use\ncases require additional features that fall outside the finite difference\nabstraction. Inspired by the complexity of real-world seismic imaging problems,\nwe introduce Devito, a domain specific language in which high level equations\nare expressed using symbolic expressions from the SymPy package. Complex\nequations are automatically manipulated, optimized, and translated into highly\noptimized C code that aims to perform compa- rably or better than hand-tuned\ncode. All this is transpar- ent to users, who only see concise symbolic\nmathematical expressions."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1609.03361v1", 
    "title": "Devito: Towards a generic Finite Difference DSL using Symbolic Python", 
    "arxiv-id": "1609.03361v1", 
    "author": "Gerard Gorman", 
    "publish": "2016-09-12T12:15:36Z", 
    "summary": "Domain specific languages (DSL) have been used in a variety of fields to\nexpress complex scientific problems in a concise manner and provide automated\nperformance optimization for a range of computational architectures. As such\nDSLs provide a powerful mechanism to speed up scientific Python computation\nthat goes beyond traditional vectorization and pre-compilation approaches,\nwhile allowing domain scientists to build applications within the comforts of\nthe Python software ecosystem. In this paper we present Devito, a new finite\ndifference DSL that provides optimized stencil computation from high-level\nproblem specifications based on symbolic Python expressions. We demonstrate\nDevito's symbolic API and performance advantages over traditional Python\nacceleration methods before highlighting its use in the scientific context of\nseismic inversion problems."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1609.04809v1", 
    "title": "An object oriented parallel finite element scheme for computations of   PDEs: Design and implementation", 
    "arxiv-id": "1609.04809v1", 
    "author": "Ulrich Wilbrandt", 
    "publish": "2016-09-15T05:05:29Z", 
    "summary": "Parallel finite element algorithms based on object-oriented concepts are\npresented. Moreover, the design and implementation of a data structure proposed\nare utilized in realizing a parallel geometric multigrid method. The\nParFEMapper and the ParFECommunicator are the key components of the data\nstructure in the proposed parallel scheme. These classes are constructed based\non the type of finite elements (continuous or nonconforming or discontinuous)\nused. The proposed solver is compared with the open source direct solvers,\nMUMPS and PasTiX. Further, the performance of the parallel multigrid solver is\nanalyzed up to 1080 processors. The solver shows a very good speedup up to 960\nprocessors and the problem size has to be increased in order to maintain the\ngood speedup when the number of processors are increased further. As a result,\nthe parallel solver is able to handle large scale problems on massively\nparallel supercomputers. The proposed parallel finite element algorithms and\nmultigrid solver are implemented in our in-house package ParMooN."
},{
    "category": "math.CO", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1609.07301v1", 
    "title": "A Computer Algebra Package for Polynomial Sequence Recognition", 
    "arxiv-id": "1609.07301v1", 
    "author": "Maxie D. Schmidt", 
    "publish": "2016-09-23T10:31:39Z", 
    "summary": "The software package developed in the MS thesis research implements functions\nfor the intelligent guessing of polynomial sequence formulas based on\nuser-defined expected sequence factors of the input coefficients. We present a\nspecialized hybrid approach to finding exact representations for polynomial\nsequences that is motivated by the need for an automated procedures to discover\nthe precise forms of these sums based on user guidance, or intuition, as to\nspecial sequence factors present in the formulas. In particular, the package\ncombines the user input on the expected special sequence factors in the\npolynomial coefficient formulas with calls to the existing functions as\nsubroutines that then process formulas for the remaining sequence terms already\nrecognized by these packages.\n  The factorization-based approach to polynomial sequence recognition is unique\nto this package and allows the search functions to find expressions for\npolynomial sums involving Stirling numbers and other special triangular\nsequences that are not readily handled by other software packages. In contrast\nto many other sequence recognition and summation software, the package not\nprovide an explicit proof, or certificate, for the correctness of these\nsequence formulas -- only computationally guided educated guesses at a complete\nidentity generating the sequence over all $n$. The thesis contains a number of\nconcrete, working examples of the package that are intended to both demonstrate\nits usage and to document its current sequence recognition capabilities."
},{
    "category": "math.AG", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1609.08722v2", 
    "title": "Solving polynomial systems via homotopy continuation and monodromy", 
    "arxiv-id": "1609.08722v2", 
    "author": "Jeff Sommars", 
    "publish": "2016-09-28T01:48:48Z", 
    "summary": "We develop an algorithm to find all solutions of a generic system in a family\nof polynomial systems with parametric coefficients using numerical homotopy\ncontinuation and the action of the monodromy group. We argue that the expected\nnumber of homotopy paths that this algorithm needs to follow is roughly linear\nin the number of solutions. We demonstrate that our software implementation is\ncompetitive with the existing state-of-the-art methods implemented in other\nsoftware packages."
},{
    "category": "cs.AR", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1610.06385v5", 
    "title": "Accelerating BLAS on Custom Architecture through Algorithm-Architecture   Co-design", 
    "arxiv-id": "1610.06385v5", 
    "author": "Ranjani Narayan", 
    "publish": "2016-10-20T12:46:07Z", 
    "summary": "Basic Linear Algebra Subprograms (BLAS) play key role in high performance and\nscientific computing applications. Experimentally, yesteryear multicore and\nGeneral Purpose Graphics Processing Units (GPGPUs) are capable of achieving up\nto 15 to 57% of the theoretical peak performance at 65W to 240W respectively\nfor compute bound operations like Double/Single Precision General Matrix\nMultiplication (XGEMM). For bandwidth bound operations like Single/Double\nprecision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to\n7% of the theoretical peak performance in multicores and GPGPUs respectively.\nAchieving performance in BLAS requires moving away from conventional wisdom and\nevolving towards customized accelerator tailored for BLAS through\nalgorithm-architecture co-design. In this paper, we present acceleration of\nLevel-1 (vector operations), Level-2 (matrix-vector operations), and Level-3\n(matrix-matrix operations) BLAS through algorithm architecture co-design on a\nCoarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a\nplatform for our experiments since REDEFINE can be adapted to support domain of\ninterest through tailor-made Custom Function Units (CFUs). For efficient\nsequential realization of BLAS, we present design of a Processing Element (PE)\nand perform micro-architectural enhancements in the PE to achieve up-to 74% of\nthe theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double\nprecision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and\nshow the scalability of our solution. Finally, we show performance improvement\nof 3-140x in PE over commercially available Intel micro-architectures,\nClearSpeed CSX700, FPGA, and Nvidia GPGPUs."
},{
    "category": "physics.flu-dyn", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1610.07944v1", 
    "title": "IB2d: a Python and MATLAB implementation of the immersed boundary method", 
    "arxiv-id": "1610.07944v1", 
    "author": "Laura A. Miller", 
    "publish": "2016-10-24T16:15:40Z", 
    "summary": "The development of fluid-structure interaction (FSI) software involves\ntrade-offs between ease of use, generality, performance, and cost. Typically\nthere are large learning curves when using low-level software to model the\ninteraction of an elastic structure immersed in a uniform density fluid. Many\nexisting codes are not publicly available, and the commercial software that\nexists usually requires expensive licenses and may not be as robust or allow\nthe necessary flexibility that in house codes can provide. We present an open\nsource immersed boundary software package, IB2d, with full implementations in\nboth MATLAB and Python, that is capable of running a vast range of biomechanics\nmodels and is accessible to scientists who have experience in high-level\nprogramming environments. IB2d contains multiple options for constructing\nmaterial properties of the fiber structure, as well as the advection-diffusion\nof a chemical gradient, muscle mechanics models, and artificial forcing to\ndrive boundaries with a preferred motion."
},{
    "category": "cs.PL", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1611.03416v1", 
    "title": "Efficient Implementation of a Higher-Order Language with Built-In AD", 
    "arxiv-id": "1611.03416v1", 
    "author": "Barak A. Pearlmutter", 
    "publish": "2016-11-10T17:40:53Z", 
    "summary": "We show that Automatic Differentiation (AD) operators can be provided in a\ndynamic language without sacrificing numeric performance. To achieve this,\ngeneral forward and reverse AD functions are added to a simple high-level\ndynamic language, and support for them is included in an aggressive optimizing\ncompiler. Novel technical mechanisms are discussed, which have the ability to\nmigrate the AD transformations from run-time to compile-time. The resulting\nsystem, although only a research prototype, exhibits startlingly good\nperformance. In fact, despite the potential inefficiencies entailed by support\nof a functional-programming language and a first-class AD operator, performance\nis competitive with the fastest available preprocessor-based Fortran AD\nsystems. On benchmarks involving nested use of the AD operators, it can even\ndramatically exceed their performance."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1611.03423v1", 
    "title": "DiffSharp: An AD Library for .NET Languages", 
    "arxiv-id": "1611.03423v1", 
    "author": "Jeffrey Mark Siskind", 
    "publish": "2016-11-10T17:50:06Z", 
    "summary": "DiffSharp is an algorithmic differentiation or automatic differentiation (AD)\nlibrary for the .NET ecosystem, which is targeted by the C# and F# languages,\namong others. The library has been designed with machine learning applications\nin mind, allowing very succinct implementations of models and optimization\nroutines. DiffSharp is implemented in F# and exposes forward and reverse AD\noperators as general nestable higher-order functions, usable by any .NET\nlanguage. It provides high-performance linear algebra primitives---scalars,\nvectors, and matrices, with a generalization to tensors underway---that are\nfully supported by all the AD operators, and which use a BLAS/LAPACK backend\nvia the highly optimized OpenBLAS library. DiffSharp currently uses operator\noverloading, but we are developing a transformation-based version of the\nlibrary using F#'s \"code quotation\" metaprogramming facility. Work on a\nCUDA-based GPU backend is also underway."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1612.00530v1", 
    "title": "Implementation and evaluation of data-compression algorithms for   irregular-grid iterative methods on the PEZY-SC processor", 
    "arxiv-id": "1612.00530v1", 
    "author": "Jun Makino", 
    "publish": "2016-12-02T01:09:23Z", 
    "summary": "Iterative methods on irregular grids have been used widely in all areas of\ncomptational science and engineering for solving partial differential equations\nwith complex geometry. They provide the flexibility to express complex shapes\nwith relatively low computational cost. However, the direction of the evolution\nof high-performance processors in the last two decades have caused serious\ndegradation of the computational efficiency of iterative methods on irregular\ngrids, because of relatively low memory bandwidth. Data compression can in\nprinciple reduce the necessary memory memory bandwidth of iterative methods and\nthus improve the efficiency. We have implemented several data compression\nalgorithms on the PEZY-SC processor, using the matrix generated for the HPCG\nbenchmark as an example. For the SpMV (Sparse Matrix-Vector multiplication)\npart of the HPCG benchmark, the best implementation without data compression\nachieved 11.6Gflops/chip, close to the theoretical limit due to the memory\nbandwidth. Our implementation with data compression has achieved 32.4Gflops.\nThis is of course rather extreme case, since the grid used in HPCG is\ngeometrically regular and thus its compression efficiency is very high.\nHowever, in real applications, it is in many cases possible to make a large\npart of the grid to have regular geometry, in particular when the resolution is\nhigh. Note that we do not need to change the structure of the program, except\nfor the addition of the data compression/decompression subroutines. Thus, we\nbelieve the data compression will be very useful way to improve the performance\nof many applications which rely on the use of irregular grids."
},{
    "category": "cs.PF", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1612.04470v1", 
    "title": "Efficient Realization of Householder Transform through   Algorithm-Architecture Co-design for Acceleration of QR Factorization", 
    "arxiv-id": "1612.04470v1", 
    "author": "Ranjani Narayan", 
    "publish": "2016-12-14T03:22:44Z", 
    "summary": "We present efficient realization of Householder Transform (HT) based QR\nfactorization through algorithm-architecture co-design where we achieve\nperformance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art\nmulticore, General Purpose Graphics Processing Units (GPGPUs), Field\nProgrammable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and\nexperimental analysis of classical HT is performed for opportunities to exhibit\nhigher degree of parallelism where parallelism is quantified as a number of\nparallel operations per level in the Directed Acyclic Graph (DAG) of the\ntransform. Based on theoretical analysis of classical HT, an opportunity\nre-arrange computations in the classical HT is identified that results in\nModified HT (MHT) where it is shown that MHT exhibits 1.33x times higher\nparallelism than classical HT. Experiments in off-the-shelf multicore and\nGeneral Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that\nMHT is capable of achieving slightly better or equal performance compared to\nclassical HT based QR factorization realizations in the optimized software\npackages for Dense Linear Algebra (DLA). We implement MHT on a customized\nplatform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better\nperformance than native implementation of classical HT on the same accelerator.\nFor custom realization of HT and MHT based QR factorization, we also identify\nmacro operations in the DAGs of HT and MHT that are realized on a\nReconfigurable Data-path (RDP). We also observe that due to re-arrangement in\nthe computations in MHT, custom realization of MHT is capable of achieving 12%\nbetter performance improvement over multicore and GPGPUs than the performance\nimprovement reported by General Matrix Multiplication (GEMM) over highly tuned\nDLA software packages for multicore and GPGPUs which is counter-intuitive."
},{
    "category": "cs.SC", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1612.05778v1", 
    "title": "Parallel Integer Polynomial Multiplication", 
    "arxiv-id": "1612.05778v1", 
    "author": "Yuzhen Xie", 
    "publish": "2016-12-17T14:54:52Z", 
    "summary": "We propose a new algorithm for multiplying dense polynomials with integer\ncoefficients in a parallel fashion, targeting multi-core processor\narchitectures. Complexity estimates and experimental comparisons demonstrate\nthe advantages of this new approach."
},{
    "category": "cs.MS", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1612.07848v1", 
    "title": "BSEPACK User's Guide", 
    "arxiv-id": "1612.07848v1", 
    "author": "Chao Yang", 
    "publish": "2016-12-23T01:16:54Z", 
    "summary": "This is the user manual for the software package BSEPACK (Bethe--Salpeter\nEigenvalue Solver Package)."
},{
    "category": "cs.DC", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1612.08060v1", 
    "title": "TAPSpMV: Topology-Aware Parallel Sparse Matrix Vector Multiplication", 
    "arxiv-id": "1612.08060v1", 
    "author": "Luke N. Olson", 
    "publish": "2016-12-23T18:40:46Z", 
    "summary": "This paper introduces a method to reduce communication that is injected into\nthe network during a sparse matrix-vector multiply by reorganizing messages on\neach node. This results in a reduction of the inter-node communication,\nreplaced by less-costly intra-node communication, which reduces both the number\nand size of messages that are injected into the network."
},{
    "category": "cs.NA", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1701.00722v1", 
    "title": "The Unum Number Format: Mathematical Foundations, Implementation and   Comparison to IEEE 754 Floating-Point Numbers", 
    "arxiv-id": "1701.00722v1", 
    "author": "Laslo Hunhold", 
    "publish": "2017-01-02T23:21:43Z", 
    "summary": "This thesis examines a modern concept for machine numbers based on interval\narithmetic called 'Unums' and compares it to IEEE 754 floating-point\narithmetic, evaluating possible uses of this format where floating-point\nnumbers are inadequate. In the course of this examination, this thesis builds\ntheoretical foundations for IEEE 754 floating-point numbers, interval\narithmetic based on the projectively extended real numbers and Unums."
},{
    "category": "cs.PF", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.00629v1", 
    "title": "gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms", 
    "arxiv-id": "1702.00629v1", 
    "author": "Matthias Werner", 
    "publish": "2017-02-02T11:41:32Z", 
    "summary": "Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields\nranging from computer science to natural sciences and engineering. With the\nrising data production bandwidths of modern FFT applications, judging best\nwhich algorithmic tool to apply, can be vital to any scientific endeavor. As\ntailored FFT implementations exist for an ever increasing variety of high\nperformance computer hardware, choosing the best performing FFT implementation\nhas strong implications for future hardware purchase decisions, for resources\nFFTs consume and for possibly decisive financial and time savings ahead of the\ncompetition. This paper therefor presents gearshifft, which is an open-source\nand vendor agnostic benchmark suite to process a wide variety of problem sizes\nand types with state-of-the-art FFT implementations (fftw, clfft and cufft).\ngearshifft provides a reproducible, unbiased and fair comparison on a wide\nvariety of hardware to explore which FFT variant is best for a given problem\nsize."
},{
    "category": "cs.AI", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.01332v1", 
    "title": "Manyopt: An Extensible Tool for Mixed, Non-Linear Optimization Through   SMT Solving", 
    "arxiv-id": "1702.01332v1", 
    "author": "Michael Huth", 
    "publish": "2017-02-04T19:49:06Z", 
    "summary": "Optimization of Mixed-Integer Non-Linear Programming (MINLP) supports\nimportant decisions in applications such as Chemical Process Engineering. But\ncurrent solvers have limited ability for deductive reasoning or the use of\ndomain-specific theories, and the management of integrality constraints does\nnot yet exploit automated reasoning tools such as SMT solvers. This seems to\nlimit both scalability and reach of such tools in practice. We therefore\npresent a tool, ManyOpt, for MINLP optimization that enables experimentation\nwith reduction techniques which transform a MINLP problem to feasibility\nchecking realized by an SMT solver. ManyOpt is similar to the SAT solver\nManySAT in that it runs a specified number of such reduction techniques in\nparallel to get the strongest result on a given MINLP problem. The tool is\nimplemented in layers, which we may see as features and where reduction\ntechniques are feature vectors. Some of these features are inspired by known\nMINLP techniques whereas others are novel and specific to SMT. Our experimental\nresults on standard benchmarks demonstrate the benefits of this approach. The\ntool supports a variety of SMT solvers and is easily extensible with new\nfeatures, courtesy of its layered structure. For example, logical formulas for\ndeductive reasoning are easily added to constrain further the optimization of a\nMINLP problem of interest."
},{
    "category": "cs.LG", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.01460v3", 
    "title": "A scikit-based Python environment for performing multi-label   classification", 
    "arxiv-id": "1702.01460v3", 
    "author": "Tomasz Kajdanowicz", 
    "publish": "2017-02-05T22:28:20Z", 
    "summary": "Scikit-multilearn is a Python library for performing multi-label\nclassification. The library is compatible with the scikit/scipy ecosystem and\nuses sparse matrices for all internal operations. It provides native Python\nimplementations of popular multi-label classification methods alongside novel\nframework for label space partitioning and division. It includes graph-based\ncommunity detection methods that utilize the powerful igraph library for\nextracting label dependency information. In addition its code is well test\ncovered and follows PEP8. Source code and documentation can be downloaded from\nhttp://scikit.ml and also via pip. The library follows scikit's BSD licencing\nscheme."
},{
    "category": "cs.PL", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.06343v2", 
    "title": "Scalar Functions and Tensor Functions: A Method to Import Tensor Index   Notation Including Einstein Summation Notation", 
    "arxiv-id": "1702.06343v2", 
    "author": "Satoshi Egi", 
    "publish": "2017-02-21T12:05:59Z", 
    "summary": "In this paper, we import tensor index notation including Einstein summation\nnotation into programming by introducing two kinds of functions, tensor\nfunctions and scalar functions. Tensor functions are functions that contract\nthe tensors given as an argument, and scalar functions are the others. As with\nordinary functions, when a tensor function obtains a tensor as an argument, the\ntensor function treats the tensor as it is as a tensor. On the other hand, when\na scalar function obtains a tensor as an argument, the scalar function is\napplied to each component of the tensor. This paper shows that, by introducing\nthese two kinds of functions, index notation can be imported into whole\nprogramming, that means we can use index notation for arbitrary functions,\nwithout requiring annoying description to enable each function to handle\ntensors. This method can be applied to arbitrary programming languages."
},{
    "category": "stat.CO", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.06407v2", 
    "title": "General Semiparametric Shared Frailty Model Estimation and Simulation   with frailtySurv", 
    "arxiv-id": "1702.06407v2", 
    "author": "Li Hsu", 
    "publish": "2017-02-21T14:40:29Z", 
    "summary": "The R package frailtySurv for simulating and fitting semi-parametric shared\nfrailty models is introduced. frailtySurv implements semi-parametric consistent\nestimators for a variety of frailty distributions, including gamma, log-normal,\ninverse Gaussian and power variance function, and provides consistent\nestimators of the standard errors of the parameters' estimators. The\nparameters' estimators are asymptotically normally distributed, and therefore\nstatistical inference based on the results of this package, such as hypothesis\ntesting and confidence intervals, can be performed using the normal\ndistribution. Extensive simulations demonstrate the flexibility and correct\nimplementation of the estimator. Two case studies performed with\npublicly-available datasets demonstrate applicability of the package. In the\nDiabetic Retinopathy Study, the onset of blindness is clustered by patient, and\nin a large hard drive failure dataset, failure times are thought to be\nclustered by the hard drive manufacturer and model."
},{
    "category": "cs.CE", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1702.08880v2", 
    "title": "Landau Collision Integral Solver with Adaptive Mesh Refinement on   Emerging Architectures", 
    "arxiv-id": "1702.08880v2", 
    "author": "R. Mills", 
    "publish": "2017-02-27T15:06:53Z", 
    "summary": "The Landau collision integral is an accurate model for the small-angle\ndominated Coulomb collisions in fusion plasmas. We investigate a high order\naccurate, fully conservative, finite element discretization of the nonlinear\nmulti-species Landau integral with adaptive mesh refinement using the PETSc\nlibrary (www.mcs.anl.gov/petsc). We develop algorithms and techniques to\nefficiently utilize emerging architectures with an approach that minimizes\nmemory usage and movement and is suitable for vector processing. The Landau\ncollision integral is vectorized with Intel AVX-512 intrinsics and the solver\nsustains as much as 22% of the theoretical peak flop rate of the Second\nGeneration Intel Xeon Phi, Knights Landing, processor."
},{
    "category": "math.NA", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1703.00985v1", 
    "title": "Small Superposition Dimension and Active Set Construction for   Multivariate Integration Under Modest Error Demand", 
    "arxiv-id": "1703.00985v1", 
    "author": "Greg W. Wasilkowski", 
    "publish": "2017-03-02T23:27:33Z", 
    "summary": "Constructing active sets is a key part of the Multivariate Decomposition\nMethod. An algorithm for constructing optimal or quasi-optimal active sets is\nproposed in the paper. By numerical experiments, it is shown that the new\nmethod can provide sets that are significantly smaller than the sets\nconstructed by the already existing method. The experiments also show that the\nsuperposition dimension could surprisingly be very small, at most 3, when the\nerror demand is not smaller than $10^{-3}$ and the weights decay sufficiently\nfast."
},{
    "category": "cs.NA", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/1703.01325v1", 
    "title": "Decoupled Block-Wise ILU(k) Preconditioner on GPU", 
    "arxiv-id": "1703.01325v1", 
    "author": "Zhangxin Chen", 
    "publish": "2017-03-03T20:08:02Z", 
    "summary": "This research investigates the implementation mechanism of block-wise ILU(k)\npreconditioner on GPU. The block-wise ILU(k) algorithm requires both the level\nk and the block size to be designed as variables. A decoupled ILU(k) algorithm\nconsists of a symbolic phase and a factorization phase. In the symbolic phase,\na ILU(k) nonzero pattern is established from the point-wise structure extracted\nfrom a block-wise matrix. In the factorization phase, the block-wise matrix\nwith a variable block size is factorized into a block lower triangular matrix\nand a block upper triangular matrix. And a further diagonal factorization is\nrequired to perform on the block upper triangular matrix for adapting a\nparallel triangular solver on GPU.We also present the numerical experiments to\nstudy the preconditioner actions on different k levels and block sizes."
},{
    "category": "cs.SE", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/cs/9903002v1", 
    "title": "An Algebraic Programming Style for Numerical Software and its   Optimization", 
    "arxiv-id": "cs/9903002v1", 
    "author": "J. Heering", 
    "publish": "1999-03-01T11:03:47Z", 
    "summary": "The abstract mathematical theory of partial differential equations (PDEs) is\nformulated in terms of manifolds, scalar fields, tensors, and the like, but\nthese algebraic structures are hardly recognizable in actual PDE solvers. The\ngeneral aim of the Sophus programming style is to bridge the gap between theory\nand practice in the domain of PDE solvers. Its main ingredients are a library\nof abstract datatypes corresponding to the algebraic structures used in the\nmathematical theory and an algebraic expression style similar to the expression\nstyle used in the mathematical theory. Because of its emphasis on abstract\ndatatypes, Sophus is most naturally combined with object-oriented languages or\nother languages supporting abstract datatypes. The resulting source code\npatterns are beyond the scope of current compiler optimizations, but are\nsufficiently specific for a dedicated source-to-source optimizer. The limited,\ndomain-specific, character of Sophus is the key to success here. This kind of\noptimization has been tested on computationally intensive Sophus style code\nwith promising results. The general approach may be useful for other styles and\nin other application domains as well."
},{
    "category": "cs.SC", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/cs/0105033v1", 
    "title": "Lectures on Reduce and Maple at UAM I - Mexico", 
    "arxiv-id": "cs/0105033v1", 
    "author": "Marc Toussaint", 
    "publish": "2001-05-25T15:08:13Z", 
    "summary": "These lectures give a brief introduction to the Computer Algebra systems\nReduce and Maple. The aim is to provide a systematic survey of most important\ncommands and concepts. In particular, this includes a discussion of\nsimplification schemes and the handling of simplification and substitution\nrules (e.g., a Lie Algebra is implemented in Reduce by means of simplification\nrules).\n  Another emphasis is on the different implementations of tensor calculi and\nthe exterior calculus by Reduce and Maple and their application in Gravitation\ntheory and Differential Geometry.\n  I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa,\nDepartamento de Fisica, Mexico, in November 1999."
},{
    "category": "cs.SC", 
    "doi": "10.1137/130936725", 
    "link": "http://arxiv.org/pdf/cs/0107036v2", 
    "title": "TeXmacs interfaces to Maxima, MuPAD and REDUCE", 
    "arxiv-id": "cs/0107036v2", 
    "author": "A. G. Grozin", 
    "publish": "2001-07-29T11:00:59Z", 
    "summary": "GNU TeXmacs is a free wysiwyg word processor providing an excellent\ntypesetting quality of texts and formulae. It can also be used as an interface\nto Computer Algebra Systems (CASs). In the present work, interfaces to three\ngeneral-purpose CASs have been implemented."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0303004v1", 
    "title": "Reliability Conditions in Quadrature Algorithms", 
    "arxiv-id": "cs/0303004v1", 
    "author": "N. M. Plakida", 
    "publish": "2003-03-06T09:19:42Z", 
    "summary": "The detection of insufficiently resolved or ill-conditioned integrand\nstructures is critical for the reliability assessment of the quadrature rule\noutputs. We discuss a method of analysis of the profile of the integrand at the\nquadrature knots which allows inferences approaching the theoretical 100% rate\nof success, under error estimate sharpening. The proposed procedure is of the\nhighest interest for the solution of parametric integrals arising in complex\nphysical models."
},{
    "category": "cs.CC", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0305008v1", 
    "title": "A Representation of Changes of Images and its Application for   Developmental Biolology", 
    "arxiv-id": "cs/0305008v1", 
    "author": "MyungHo Kim", 
    "publish": "2003-05-13T17:48:35Z", 
    "summary": "In this paper, we consider a series of events observed at spaced time\nintervals and present a method of representation of the series. To explain an\nidea, by dealing with a set of gene expression data, which could be obtained\nfrom developmental biology, the procedures are sketched with comments in some\ndetails. We mean representation by choosing a proper function, which fits well\nwith observed data of a series, and turning its characteristics into numbers,\nwhich extract the intrinsic properties of fluctuating data. With help of a\nmachine learning techniques, this method will give a classification of\ndevelopmental biological data as well as any varying data during a certain\nperiod and the classification can be applied for diagnosis of a disease."
},{
    "category": "cs.MS", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0401008v1", 
    "title": "Algorithm xxx: Modified Bessel functions of imaginary order and positive   argument", 
    "arxiv-id": "cs/0401008v1", 
    "author": "Nico M. Temme", 
    "publish": "2004-01-13T14:29:26Z", 
    "summary": "Fortran 77 programs for the computation of modified Bessel functions of\npurely imaginary order are presented. The codes compute the functions\n$K_{ia}(x)$, $L_{ia}(x)$ and their derivatives for real $a$ and positive $x$;\nthese functions are independent solutions of the differential equation $x^2 w''\n+x w' +(a^2 -x^2)w=0$. The code also computes exponentially scaled functions.\nThe range of computation is $(x,a)\\in (0,1500]\\times [-1500,1500]$ when scaled\nfunctions are considered and it is larger than $(0,500]\\times [-400,400]$ for\nstandard IEEE double precision arithmetic. The relative accuracy is better than\n$10^{-13}$ in the range $(0,200]\\times [-200,200]$ and close to $10^{-12}$ in\n$(0,1500]\\times [-1500,1500]$."
},{
    "category": "cs.MS", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0410044v5", 
    "title": "An Example of Clifford Algebras Calculations with GiNaC", 
    "arxiv-id": "cs/0410044v5", 
    "author": "Vladimir V. Kisil", 
    "publish": "2004-10-18T17:39:51Z", 
    "summary": "This example of Clifford algebras calculations uses GiNaC\n(http://www.ginac.de/) library, which includes a support for generic Clifford\nalgebra starting from version~1.3.0. Both symbolic and numeric calculation are\npossible and can be blended with other functions of GiNaC. This calculations\nwas made for the paper math.CV/0410399.\n  Described features of GiNaC are already available at PyGiNaC\n(http://sourceforge.net/projects/pyginac/) and due to course should propagate\ninto other software like GNU Octave (http://www.octave.org/), gTybalt\n(http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as\ntheir back-end."
},{
    "category": "cs.SC", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0504039v1", 
    "title": "TeXmacs-maxima interface", 
    "arxiv-id": "cs/0504039v1", 
    "author": "A. G. Grozin", 
    "publish": "2005-04-11T16:39:16Z", 
    "summary": "This tutorial presents features of the new and improved TeXmacs-maxima\ninterface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or\nlater)."
},{
    "category": "cs.HC", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0510034v1", 
    "title": "COMODI: On the Graphical User Interface", 
    "arxiv-id": "cs/0510034v1", 
    "author": "Bazil P\u00e2rv", 
    "publish": "2005-10-14T01:03:55Z", 
    "summary": "We propose a series of features for the graphical user interface (GUI) of the\nCOmputational MOdule Integrator (COMODI) \\cite{Synasc05a}\\cite{COMODI}. In view\nof the special requirements that a COMODI type of framework for scientific\ncomputing imposes and inspiring from existing solutions that provide advanced\ngraphical visual programming environments, we identify those elements and\nassociated behaviors that will have to find their way into the first release of\nCOMODI."
},{
    "category": "cs.NA", 
    "doi": "10.1016/S0010-4655(03)00282-0", 
    "link": "http://arxiv.org/pdf/cs/0510051v1", 
    "title": "Numerical resolution of some BVP using Bernstein polynomials", 
    "arxiv-id": "cs/0510051v1", 
    "author": "Gianluca Argentini", 
    "publish": "2005-10-18T09:55:42Z", 
    "summary": "In this work we present a method, based on the use of Bernstein polynomials,\nfor the numerical resolution of some boundary values problems. The computations\nhave not need of particular approximations of derivatives, such as finite\ndifferences, or particular techniques, such as finite elements. Also, the\nmethod doesn't require the use of matrices, as in resolution of linear\nalgebraic systems, nor the use of like-Newton algorithms, as in resolution of\nnon linear sets of equations. An initial equation is resolved only once, then\nthe method is based on iterated evaluations of appropriate polynomials."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0512073v12", 
    "title": "Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC", 
    "arxiv-id": "cs/0512073v12", 
    "author": "Vladimir V. Kisil", 
    "publish": "2005-12-17T15:09:11Z", 
    "summary": "This paper presents an implementation of the\nSchwerdtfeger-Fillmore-Springer-Cnops construction (SFSCc) along with\nillustrations of its usage. SFSCc linearises the linear-fraction action of the\nMoebius group in R^n. This has clear advantages in several theoretical and\napplied fields including engineering. Our implementation is based on the\nClifford algebra capacities of the GiNaC computer algebra system\n(http://www.ginac.de/), which were described in cs.MS/0410044.\n  The core of this realisation of SFSCc is done for an arbitrary dimension of\nR^n with a metric given by an arbitrary bilinear form. We also present a\nsubclass for two dimensional cycles (i.e. circles, parabolas and hyperbolas),\nwhich add some 2D specific routines including a visualisation to PostScript\nfiles through the MetaPost (http://www.tug.org/metapost.html) or Asymptote\n(http://asymptote.sourceforge.net/) packages.\n  This software is the backbone of many results published in math.CV/0512416\nand we use its applications their for demonstration. The library can be ported\n(with various level of required changes) to other CAS with Clifford algebras\ncapabilities similar to GiNaC.\n  There is an ISO image of a Live Debian DVD attached to this paper as an\nauxiliary file, a copy is stored on Google Drive as well."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0604066v1", 
    "title": "Univariate polynomial real root isolation: Continued Fractions revisited", 
    "arxiv-id": "cs/0604066v1", 
    "author": "Ioannis Z. Emiris", 
    "publish": "2006-04-17T10:52:35Z", 
    "summary": "We present algorithmic, complexity and implementation results concerning real\nroot isolation of integer univariate polynomials using the continued fraction\nexpansion of real algebraic numbers. One motivation is to explain the method's\ngood performance in practice. We improve the previously known bound by a factor\nof $d \\tau$, where $d$ is the polynomial degree and $\\tau$ bounds the\ncoefficient bitsize, thus matching the current record complexity for real root\nisolation by exact methods. Namely, the complexity bound is $\\sOB(d^4 \\tau^2)$\nusing the standard bound on the expected bitsize of the integers in the\ncontinued fraction expansion. We show how to compute the multiplicities within\nthe same complexity and extend the algorithm to non square-free polynomials.\nFinally, we present an efficient open-source \\texttt{C++} implementation in the\nalgebraic library \\synaps, and illustrate its efficiency as compared to other\navailable software. We use polynomials with coefficient bitsize up to 8000 and\ndegree up to 1000."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0607103v1", 
    "title": "Ideas by Statistical Mechanics (ISM)", 
    "arxiv-id": "cs/0607103v1", 
    "author": "Lester Ingber", 
    "publish": "2006-07-23T16:12:47Z", 
    "summary": "Ideas by Statistical Mechanics (ISM) is a generic program to model evolution\nand propagation of ideas/patterns throughout populations subjected to\nendogenous and exogenous interactions. The program is based on the author's\nwork in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the\nauthor's Adaptive Simulated Annealing (ASA) code for optimizations of training\nsets, as well as for importance-sampling to apply the author's copula financial\nrisk-management codes, Trading in Risk Dimensions (TRD), for assessments of\nrisk and uncertainty. This product can be used for decision support for\nprojects ranging from diplomatic, information, military, and economic (DIME)\nfactors of propagation/evolution of ideas, to commercial sales, trading\nindicators across sectors of financial markets, advertising and political\ncampaigns, etc. A statistical mechanical model of neocortical interactions,\ndeveloped by the author and tested successfully in describing short-term memory\nand EEG indicators, is the proposed model. Parameters with a given subset of\nmacrocolumns will be fit using ASA to patterns representing ideas. Parameters\nof external and inter-regional interactions will be determined that promote or\ninhibit the spread of these ideas. Tools of financial risk management,\ndeveloped by the author to process correlated multivariate systems with\ndiffering non-Gaussian distributions using modern copula analysis,\nimportance-sampled using ASA, will enable bona fide correlations and\nuncertainties of success and failure to be calculated. Marginal distributions\nwill be evolved to determine their expected duration and stability using\nalgorithms developed by the author, i.e., PATHTREE and PATHINT codes."
},{
    "category": "cs.GR", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0608003v2", 
    "title": "On a solution to display non-filled-in quaternionic Julia sets", 
    "arxiv-id": "cs/0608003v2", 
    "author": "Alessandro Rosa", 
    "publish": "2006-08-01T19:25:17Z", 
    "summary": "During early 1980s, the so-called `escape time' method, developed to display\nthe Julia sets for complex dynamical systems, was exported to quaternions in\norder to draw analogous pictures in this wider numerical field. Despite of the\nfine results in the complex plane, where all topological configurations of\nJulia sets have been successfully displayed, the `escape time' method fails to\nrender properly the non-filled-in variety of quaternionic Julia sets. So their\ndigital visualisation remained an open problem for several years. Both the\nsolution for extending this old method to non-filled-in quaternionic Julia sets\nand its implementation into a program are explained here."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0609082v1", 
    "title": "Classifying extrema using intervals", 
    "arxiv-id": "cs/0609082v1", 
    "author": "Marek W. Gutowski", 
    "publish": "2006-09-14T18:32:46Z", 
    "summary": "We present a straightforward and verified method of deciding whether the\nn-dimensional point x (n>=1), such that \\nabla f(x)=0, is the local minimizer,\nmaximizer or just a saddle point of a real-valued function f.\n  The method scales linearly with dimensionality of the problem and never\nproduces false results."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0610120v2", 
    "title": "Classdesc and Graphcode: support for scientific programming in C++", 
    "arxiv-id": "cs/0610120v2", 
    "author": "Duraid Madina", 
    "publish": "2006-10-20T05:11:21Z", 
    "summary": "Object-oriented programming languages such as Java and Objective C have\nbecome popular for implementing agent-based and other object-based simulations\nsince objects in those languages can {\\em reflect} (i.e. make runtime queries\nof an object's structure). This allows, for example, a fairly trivial {\\em\nserialisation} routine (conversion of an object into a binary representation\nthat can be stored or passed over a network) to be written. However C++ does\nnot offer this ability, as type information is thrown away at compile time. Yet\nC++ is often a preferred development environment, whether for performance\nreasons or for its expressive features such as operator overloading.\n  In scientific coding, changes to a model's codes takes place constantly, as\nthe model is refined, and different phenomena are studied. Yet traditionally,\nfacilities such as checkpointing, routines for initialising model parameters\nand analysis of model output depend on the underlying model remaining static,\notherwise each time a model is modified, a whole slew of supporting routines\nneeds to be changed to reflect the new data structures. Reflection offers the\nadvantage of the simulation framework adapting to the underlying model without\nprogrammer intervention, reducing the effort of modifying the model.\n  In this paper, we present the {\\em Classdesc} system which brings many of the\nbenefits of object reflection to C++, {\\em ClassdescMP} which dramatically\nsimplifies coding of MPI based parallel programs and {\\em\n  Graphcode} a general purpose data parallel programming environment."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00006-006-0017-4", 
    "link": "http://arxiv.org/pdf/cs/0703025v1", 
    "title": "LIBOPT - An environment for testing solvers on heterogeneous collections   of problems - Version 1.0", 
    "arxiv-id": "cs/0703025v1", 
    "author": "Xavier Jonsson", 
    "publish": "2007-03-06T14:05:28Z", 
    "summary": "The Libopt environment is both a methodology and a set of tools that can be\nused for testing, comparing, and profiling solvers on problems belonging to\nvarious collections. These collections can be heterogeneous in the sense that\ntheir problems can have common features that differ from one collection to the\nother. Libopt brings a unified view on this composite world by offering, for\nexample, the possibility to run any solver on any problem compatible with it,\nusing the same Unix/Linux command. The environment also provides tools for\ncomparing the results obtained by solvers on a specified set of problems. Most\nof the scripts going with the Libopt environment have been written in Perl."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.sigpro.2008.01.004", 
    "link": "http://arxiv.org/pdf/cs/0703150v2", 
    "title": "Type-II/III DCT/DST algorithms with reduced number of arithmetic   operations", 
    "arxiv-id": "cs/0703150v2", 
    "author": "Steven G. Johnson", 
    "publish": "2007-03-30T00:53:48Z", 
    "summary": "We present algorithms for the discrete cosine transform (DCT) and discrete\nsine transform (DST), of types II and III, that achieve a lower count of real\nmultiplications and additions than previously published algorithms, without\nsacrificing numerical accuracy. Asymptotically, the operation count is reduced\nfrom ~ 2N log_2 N to ~ (17/9) N log_2 N for a power-of-two transform size N.\nFurthermore, we show that a further N multiplications may be saved by a certain\nrescaling of the inputs or outputs, generalizing a well-known technique for N=8\nby Arai et al. These results are derived by considering the DCT to be a special\ncase of a DFT of length 4N, with certain symmetries, and then pruning redundant\noperations from a recent improved fast Fourier transform algorithm (based on a\nrecursive rescaling of the conjugate-pair split radix algorithm). The improved\nalgorithms for DCT-III, DST-II, and DST-III follow immediately from the\nimproved count for the DCT-II."
},{
    "category": "hep-lat", 
    "doi": "10.1016/S0010-4655(01)00297-1", 
    "link": "http://arxiv.org/pdf/hep-lat/0004007v4", 
    "title": "Matrix Distributed Processing: A set of C++ Tools for implementing   generic lattice computations on parallel systems", 
    "arxiv-id": "hep-lat/0004007v4", 
    "author": "Massimo Di Pierro", 
    "publish": "2000-04-11T22:59:10Z", 
    "summary": "We present a set of programming tools (classes and functions written in C++\nand based on Message Passing Interface) for fast development of generic\nparallel (and non-parallel) lattice simulations. They are collectively called\nMDP 1.2.\n  These programming tools include classes and algorithms for matrices, random\nnumber generators, distributed lattices (with arbitrary topology), fields and\nparallel iterations. No previous knowledge of MPI is required in order to use\nthem.\n  Some applications in electromagnetism, electronics, condensed matter and\nlattice QCD are presented."
},{
    "category": "hep-th", 
    "doi": "10.1016/S0010-4655(01)00297-1", 
    "link": "http://arxiv.org/pdf/hep-th/0208218v2", 
    "title": "Introducing LambdaTensor1.0 - A package for explicit symbolic and   numeric Lie algebra and Lie group calculations", 
    "arxiv-id": "hep-th/0208218v2", 
    "author": "Thomas Fischbacher", 
    "publish": "2002-08-29T16:37:45Z", 
    "summary": "Due to the occurrence of large exceptional Lie groups in supergravity,\ncalculations involving explicit Lie algebra and Lie group element manipulations\neasily become very complicated and hence also error-prone if done by hand.\nResearch on the extremal structure of maximal gauged supergravity theories in\nvarious dimensions sparked the development of a library for efficient abstract\nmultilinear algebra calculations involving sparse and non-sparse higher-rank\ntensors, which is presented here."
},{
    "category": "math.GR", 
    "doi": "10.1016/S0010-4655(01)00297-1", 
    "link": "http://arxiv.org/pdf/math/9905155v2", 
    "title": "An Implementation of the Bestvina-Handel Algorithm for Surface   Homeomorphisms", 
    "arxiv-id": "math/9905155v2", 
    "author": "Peter Brinkmann", 
    "publish": "1999-05-25T20:13:09Z", 
    "summary": "Bestvina and Handel have found an effective algorithm that determines whether\na given homeomorphism of an orientable, possibly punctured surface is\npseudo-Anosov. We present a software package in Java that realizes this\nalgorithm for surfaces with one puncture. Moreover, the package allows the user\nto define homeomorphisms in terms of Dehn twists, and in the pseudo-Anosov case\nit generates images of train tracks in the sense of Bestvina-Handel."
},{
    "category": "math.NA", 
    "doi": "10.1016/S0010-4655(01)00297-1", 
    "link": "http://arxiv.org/pdf/math/0603606v1", 
    "title": "Lanczos $\u03c4$-method optimal algorithm in APS for approximating the   mathematical functions", 
    "arxiv-id": "math/0603606v1", 
    "author": "P. N. Denisenko", 
    "publish": "2006-03-26T12:30:11Z", 
    "summary": "A new procedure is constructed by means of APS in APLAN language. The\nprocedure solves the initial-value problem for linear differential equations of\norder $k$ with polynomial coefficients and regular singularity in the\ninitialization point in the interval $[a, b]$ and computes the algebraic\npolynomial $y_n$ of given order $n$. A new algorithm of Lanczos $\\tau$-method\nis built for this procedure, the solution existence $y_n$ of the initial-value\nproblem proved on this algorithm and also is proved the optimality by precision\nof order $k$ derivative of the initial-value problem solution."
},{
    "category": "math.CA", 
    "doi": "10.1007/s00025-015-0485-8", 
    "link": "http://arxiv.org/pdf/math/0701020v12", 
    "title": "Some notes on a method for proving inequalities by computer", 
    "arxiv-id": "math/0701020v12", 
    "author": "Branko J. Malesevic", 
    "publish": "2006-12-31T07:16:29Z", 
    "summary": "In this article we consider mathematical fundamentals of one method for\nproving inequalities by computer, based on the Remez algorithm. Using the\nwell-known results of undecidability of the existence of zeros of real\nelementary functions, we demonstrate that the considered method generally in\npractice becomes one heuristic for the verification of inequalities. We give\nsome improvements of the inequalities considered in the theorems for which the\nexisting proofs have been based on the numerical verifications of Remez\nalgorithm."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00025-015-0485-8", 
    "link": "http://arxiv.org/pdf/0705.1033v2", 
    "title": "Optimal Cache-Oblivious Mesh Layouts", 
    "arxiv-id": "0705.1033v2", 
    "author": "Kebin Wang", 
    "publish": "2007-05-08T05:59:55Z", 
    "summary": "A mesh is a graph that divides physical space into regularly-shaped regions.\nMeshes computations form the basis of many applications, e.g. finite-element\nmethods, image rendering, and collision detection. In one important mesh\nprimitive, called a mesh update, each mesh vertex stores a value and repeatedly\nupdates this value based on the values stored in all neighboring vertices. The\nperformance of a mesh update depends on the layout of the mesh in memory.\n  This paper shows how to find a memory layout that guarantees that the mesh\nupdate has asymptotically optimal memory performance for any set of memory\nparameters. Such a memory layout is called cache-oblivious. Formally, for a\n$d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where\n$M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers.\nThe paper also shows how the mesh-update performance degrades for smaller\ncaches, where $M=o(B^d)$.\n  The paper then gives two algorithms for finding cache-oblivious mesh layouts.\nThe first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation\nand with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory\ntransfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory\ntransfers with high probability in the cache-oblivious and disk-access machine\n(DAM) models. The layout is obtained by finding a fully balanced decomposition\ntree of $G$ and then performing an in-order traversal of the leaves of the\ntree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor\nin all three memory models, both in expectation and with high probability. The\nlayout obtained by finding a relax-balanced decomposition tree of $G$ and then\nperforming an in-order traversal of the leaves of the tree."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s00025-015-0485-8", 
    "link": "http://arxiv.org/pdf/0707.0701v2", 
    "title": "Clustering and Feature Selection using Sparse Principal Component   Analysis", 
    "arxiv-id": "0707.0701v2", 
    "author": "Alexandre d'Aspremont", 
    "publish": "2007-07-04T21:53:11Z", 
    "summary": "In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology."
},{
    "category": "math-ph", 
    "doi": "10.1007/s00025-015-0485-8", 
    "link": "http://arxiv.org/pdf/0707.4659v1", 
    "title": "Difference Equations in Massive Higher Order Calculations", 
    "arxiv-id": "0707.4659v1", 
    "author": "C. Schneider", 
    "publish": "2007-07-31T16:54:33Z", 
    "summary": "The calculation of massive 2--loop operator matrix elements, required for the\nhigher order Wilson coefficients for heavy flavor production in deeply\ninelastic scattering, leads to new types of multiple infinite sums over\nharmonic sums and related functions, which depend on the Mellin parameter $N$.\nWe report on the solution of these sums through higher order difference\nequations using the summation package {\\tt Sigma}."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-540-71067-7_21", 
    "link": "http://arxiv.org/pdf/0805.2438v1", 
    "title": "Certified Exact Transcendental Real Number Computation in Coq", 
    "arxiv-id": "0805.2438v1", 
    "author": "Russell O'Connor", 
    "publish": "2008-05-16T18:02:24Z", 
    "summary": "Reasoning about real number expressions in a proof assistant is challenging.\nSeveral problems in theorem proving can be solved by using exact real number\ncomputation. I have implemented a library for reasoning and computing with\ncomplete metric spaces in the Coq proof assistant and used this library to\nbuild a constructive real number implementation including elementary real\nnumber functions and proofs of correctness. Using this library, I have created\na tactic that automatically proves strict inequalities over closed elementary\nreal number expressions by computation."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-540-71067-7_21", 
    "link": "http://arxiv.org/pdf/0807.2382v1", 
    "title": "Revisiting the upper bounding process in a safe Branch and Bound   algorithm", 
    "arxiv-id": "0807.2382v1", 
    "author": "Michel Rueher", 
    "publish": "2008-07-15T14:18:23Z", 
    "summary": "Finding feasible points for which the proof succeeds is a critical issue in\nsafe Branch and Bound algorithms which handle continuous problems. In this\npaper, we introduce a new strategy to compute very accurate approximations of\nfeasible points. This strategy takes advantage of the Newton method for\nunder-constrained systems of equations and inequalities. More precisely, it\nexploits the optimal solution of a linear relaxation of the problem to compute\nefficiently a promising upper bound. First experiments on the Coconuts\nbenchmarks demonstrate that this approach is very effective."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-540-71067-7_21", 
    "link": "http://arxiv.org/pdf/0808.0754v1", 
    "title": "A Functional Hitchhiker's Guide to Hereditarily Finite Sets, Ackermann   Encodings and Pairing Functions", 
    "arxiv-id": "0808.0754v1", 
    "author": "Paul Tarau", 
    "publish": "2008-08-06T01:05:09Z", 
    "summary": "The paper is organized as a self-contained literate Haskell program that\nimplements elements of an executable finite set theory with focus on\ncombinatorial generation and arithmetic encodings. The code, tested under GHC\n6.6.1, is available at http://logic.csci.unt.edu/tarau/research/2008/fSET.zip .\n  We introduce ranking and unranking functions generalizing Ackermann's\nencoding to the universe of Hereditarily Finite Sets with Urelements. Then we\nbuild a lazy enumerator for Hereditarily Finite Sets with Urelements that\nmatches the unranking function provided by the inverse of Ackermann's encoding\nand we describe functors between them resulting in arithmetic encodings for\npowersets, hypergraphs, ordinals and choice functions. After implementing a\ndigraph representation of Hereditarily Finite Sets we define {\\em decoration\nfunctions} that can recover well-founded sets from encodings of their\nassociated acyclic digraphs. We conclude with an encoding of arbitrary digraphs\nand discuss a concept of duality induced by the set membership relation.\n  Keywords: hereditarily finite sets, ranking and unranking functions,\nexecutable set theory, arithmetic encodings, Haskell data representations,\nfunctional programming and computational mathematics"
},{
    "category": "cs.SC", 
    "doi": "10.1007/s00037-010-0294-0", 
    "link": "http://arxiv.org/pdf/0810.5685v6", 
    "title": "Interpolation of Shifted-Lacunary Polynomials", 
    "arxiv-id": "0810.5685v6", 
    "author": "Daniel S. Roche", 
    "publish": "2008-10-31T13:35:08Z", 
    "summary": "Given a \"black box\" function to evaluate an unknown rational polynomial f in\nQ[x] at points modulo a prime p, we exhibit algorithms to compute the\nrepresentation of the polynomial in the sparsest shifted power basis. That is,\nwe determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 <\ne2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) =\nc1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely\nminimal over any shifted power basis. The novelty of our algorithm is that the\ncomplexity is polynomial in the (sparse) representation size, and in particular\nis logarithmic in deg(f). Our method combines previous celebrated results on\nsparse interpolation and computing sparsest shifts, and provides a way to\nhandle polynomials with extremely high degree which are, in some sense, sparse\nin information."
},{
    "category": "q-bio.QM", 
    "doi": "10.1007/s00037-010-0294-0", 
    "link": "http://arxiv.org/pdf/0811.1081v1", 
    "title": "Parallel GPU Implementation of Iterative PCA Algorithms", 
    "arxiv-id": "0811.1081v1", 
    "author": "M. Andrecut", 
    "publish": "2008-11-07T04:34:01Z", 
    "summary": "Principal component analysis (PCA) is a key statistical technique for\nmultivariate data analysis. For large data sets the common approach to PCA\ncomputation is based on the standard NIPALS-PCA algorithm, which unfortunately\nsuffers from loss of orthogonality, and therefore its applicability is usually\nlimited to the estimation of the first few components. Here we present an\nalgorithm based on Gram-Schmidt orthogonalization (called GS-PCA), which\neliminates this shortcoming of NIPALS-PCA. Also, we discuss the GPU (Graphics\nProcessing Unit) parallel implementation of both NIPALS-PCA and GS-PCA\nalgorithms. The numerical results show that the GPU parallel optimized\nversions, based on CUBLAS (NVIDIA) are substantially faster (up to 12 times)\nthan the CPU optimized versions based on CBLAS (GNU Scientific Library)."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00037-010-0294-0", 
    "link": "http://arxiv.org/pdf/0901.4417v2", 
    "title": "A novel type of branch and bound for maximum independent set", 
    "arxiv-id": "0901.4417v2", 
    "author": "Marcel Wild", 
    "publish": "2009-01-28T09:09:36Z", 
    "summary": "Several algorithms are presented. The standard algorithm generates all N\nanticliques of a graph with v vertices in time O(N^v2). It can e.g. be adapted\nto calculate the independence polynomial of G, to generate all maximum\ncardinality anticliques, or just one maximum anticlique. The latter was\nprogrammed using the Mathematica 6.0 code. For a random (45, 92)-graph G a\nmaximum anticlique of size 21 was found in 1.344 sec, whereas the \"hardwired\"\nMathematica command MaximumIndependentSet[G] clocked in at 155838 sec, which is\nfive orders of magnitude slower."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s00037-010-0294-0", 
    "link": "http://arxiv.org/pdf/0902.3208v1", 
    "title": "A Fast Multigrid Algorithm for Energy Minimization Under Planar Density   Constraints", 
    "arxiv-id": "0902.3208v1", 
    "author": "Achi Brandt", 
    "publish": "2009-02-18T17:59:47Z", 
    "summary": "The two-dimensional layout optimization problem reinforced by the efficient\nspace utilization demand has a wide spectrum of practical applications.\nFormulating the problem as a nonlinear minimization problem under planar\nequality and/or inequality density constraints, we present a linear time\nmultigrid algorithm for solving correction to this problem. The method is\ndemonstrated on various graph drawing (visualization) instances."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s00037-010-0294-0", 
    "link": "http://arxiv.org/pdf/0905.0586v1", 
    "title": "WinBioinfTools: Bioinformatics Tools for Windows High Performance   Computing Server 2008", 
    "arxiv-id": "0905.0586v1", 
    "author": "Hisham Mohamed", 
    "publish": "2009-05-05T12:04:12Z", 
    "summary": "Open source bioinformatics tools running under MS Windows are rare to find,\nand those running under Windows HPC cluster are almost non-existing. This is\ndespite the fact that the Windows is the most popular operating system used\namong life scientists. Therefore, we introduce in this initiative\nWinBioinfTools, a toolkit containing a number of bioinformatics tools running\nunder Windows High Performance Computing Server 2008. It is an open source code\npackage, where users and developers can share and add to. We currently start\nwith three programs from the area of sequence analysis: 1) CoCoNUT for pairwise\ngenome comparison, 2) parallel BLAST for biological database search, and 3)\nparallel global pairwise sequence alignment. In this report, we focus on\ntechnical aspects concerning how some components of these tools were ported\nfrom Linux/Unix environment to run under Windows. We also show the advantages\nof using the Windows HPC Cluster 2008. We demonstrate by experiments the\nperformance gain achieved when using a computer cluster against a single\nmachine. Furthermore, we show the results of comparing the performance of\nWinBioinfTools on the Windows and Linux Cluster."
},{
    "category": "math.RA", 
    "doi": "10.1080/10586458.2012.738538", 
    "link": "http://arxiv.org/pdf/0906.1272v3", 
    "title": "The alternative operad is not Koszul", 
    "arxiv-id": "0906.1272v3", 
    "author": "Pasha Zusmanovich", 
    "publish": "2009-06-06T12:44:23Z", 
    "summary": "Using computer calculations, we prove the statement in the title."
},{
    "category": "cs.IT", 
    "doi": "10.1080/10586458.2012.738538", 
    "link": "http://arxiv.org/pdf/0906.5339v2", 
    "title": "Asymmetric Quantum Cyclic Codes", 
    "arxiv-id": "0906.5339v2", 
    "author": "Salah A. Aly", 
    "publish": "2009-06-29T19:37:12Z", 
    "summary": "It is recently conjectured in quantum information processing that phase-shift\nerrors occur with high probability than qubit-flip errors, hence the former is\nmore disturbing to quantum information than the later one. This leads us to\nconstruct asymmetric quantum error controlling codes to protect quantum\ninformation over asymmetric channels, $\\Pr Z \\geq \\Pr X$. In this paper we\npresent two generic methods to derive asymmetric quantum cyclic codes using the\ngenerator polynomials and defining sets of classical cyclic codes.\nConsequently, the methods allow us to construct several families of asymmetric\nquantum BCH, RS, and RM codes. Finally, the methods are used to construct\nfamilies of asymmetric subsystem codes."
},{
    "category": "cs.NA", 
    "doi": "10.1109/JSTSP.2009.2039176", 
    "link": "http://arxiv.org/pdf/0909.0777v1", 
    "title": "Optimally Tuned Iterative Reconstruction Algorithms for Compressed   Sensing", 
    "arxiv-id": "0909.0777v1", 
    "author": "David L. Donoho", 
    "publish": "2009-09-03T22:26:32Z", 
    "summary": "We conducted an extensive computational experiment, lasting multiple\nCPU-years, to optimally select parameters for two important classes of\nalgorithms for finding sparse solutions of underdetermined systems of linear\nequations. We make the optimally tuned implementations available at {\\tt\nsparselab.stanford.edu}; they run `out of the box' with no user tuning: it is\nnot necessary to select thresholds or know the likely degree of sparsity. Our\nclass of algorithms includes iterative hard and soft thresholding with or\nwithout relaxation, as well as CoSaMP, subspace pursuit and some natural\nextensions. As a result, our optimally tuned algorithms dominate such\nproposals. Our notion of optimality is defined in terms of phase transitions,\ni.e. we maximize the number of nonzeros at which the algorithm can successfully\noperate. We show that the phase transition is a well-defined quantity with our\nsuite of random underdetermined linear systems. Our tuning gives the highest\ntransition possible within each class of algorithms."
},{
    "category": "cs.SC", 
    "doi": "10.1109/JSTSP.2009.2039176", 
    "link": "http://arxiv.org/pdf/0909.4950v2", 
    "title": "Implementing Gr\u00f6bner bases for operads", 
    "arxiv-id": "0909.4950v2", 
    "author": "Mikael Vejdemo-Johansson", 
    "publish": "2009-09-27T16:54:34Z", 
    "summary": "We present an implementation of the algorithm for computing Groebner bases\nfor operads due to the first author and A. Khoroshkin. We discuss the actual\nalgorithms, the choices made for the implementation platform and the data\nrepresentation, and strengths and weaknesses of our approach."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/0909.5413v1", 
    "title": "PetRBF--A parallel O(N) algorithm for radial basis function   interpolation", 
    "arxiv-id": "0909.5413v1", 
    "author": "Matthew G. Knepley", 
    "publish": "2009-09-29T19:27:51Z", 
    "summary": "We have developed a parallel algorithm for radial basis function (RBF)\ninterpolation that exhibits O(N) complexity,requires O(N) storage, and scales\nexcellently up to a thousand processes. The algorithm uses a GMRES iterative\nsolver with a restricted additive Schwarz method (RASM) as a preconditioner and\na fast matrix-vector algorithm. Previous fast RBF methods, --,achieving at most\nO(NlogN) complexity,--, were developed using multiquadric and polyharmonic\nbasis functions. In contrast, the present method uses Gaussians with a small\nvariance (a common choice in particle methods for fluid simulation, our main\ntarget application). The fast decay of the Gaussian basis function allows rapid\nconvergence of the iterative solver even when the subdomains in the RASM are\nvery small. The present method was implemented in parallel using the PETSc\nlibrary (developer version). Numerical experiments demonstrate its capability\nin problems of RBF interpolation with more than 50 million data points, timing\nat 106 seconds (19 iterations for an error tolerance of 10^-15 on 1024\nprocessors of a Blue Gene/L (700 MHz PowerPC processors). The parallel code is\nfreely available in the open-source model."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/0910.0663v7", 
    "title": "Transmission line inspires a new distributed algorithm to solve linear   system of circuit", 
    "arxiv-id": "0910.0663v7", 
    "author": "Huazhong Yang", 
    "publish": "2009-10-05T03:08:43Z", 
    "summary": "Transmission line, or wire, is always troublesome to integrated circuits\ndesigners, but it could be helpful to parallel computing researchers. This\npaper proposes the Virtual Transmission Method (VTM), which is a new\ndistributed and stationary iterative algorithm to solve the linear system\nextracted from circuit. It tears the circuit by virtual transmission lines to\nachieve distributed computing. For the symmetric positive definite (SPD) linear\nsystem, VTM is proved to be convergent. For the unsymmetrical linear system,\nnumerical experiments show that VTM is possible to achieve better convergence\nproperty than the traditional stationary algorithms. VTM could be accelerated\nby some preconditioning techniques, and the convergence speed of VTM is fast\nwhen its preconditioner is properly chosen."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/0911.2889v1", 
    "title": "Global communications in multiprocessor simulations of flames", 
    "arxiv-id": "0911.2889v1", 
    "author": "V. Karlin", 
    "publish": "2009-11-15T17:10:31Z", 
    "summary": "In this paper we investigate performance of global communications in a\nparticular parallel code. The code simulates dynamics of expansion of premixed\nspherical flames using an asymptotic model of Sivashinsky type and a spectral\nnumerical algorithm. As a result, the code heavily relies on global all-to-all\ninterprocessor communications implementing transposition of the distributed\ndata array in which numerical solution to the problem is stored. This global\ndata interdependence makes interprocessor connectivity of the HPC system as\nimportant as the floating-point power of the processors of which the system is\nbuilt. Our experiments show that efficient numerical simulation of this\nparticular model, with global data interdependence, on modern HPC systems is\npossible. Prospects of performance of more sophisticated models of flame\ndynamics are analysed as well."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/1001.1435v6", 
    "title": "JBotSim, a Tool for Fast Prototyping of Distributed Algorithms in   Dynamic Networks", 
    "arxiv-id": "1001.1435v6", 
    "author": "Arnaud Casteigts", 
    "publish": "2010-01-09T18:26:00Z", 
    "summary": "JBotSim is a java library that offers basic primitives for prototyping,\nrunning, and visualizing distributed algorithms in dynamic networks. With\nJBotSim, one can implement an idea in minutes and interact with it ({\\it e.g.},\nadd, move, or delete nodes) while it is running. JBotSim is well suited to\nprepare live demonstrations of your algorithms to colleagues or students; it\ncan also be used to evaluate performance at the algorithmic level (number of\nmessages, number of rounds, etc.). Unlike most tools, JBotSim is not an\nintegrated environment. It is a lightweight library to be used in your program.\nIn this paper, we present an overview of its distinctive features and\narchitecture."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/1001.1654v2", 
    "title": "Algorithmic Differentiation of Linear Algebra Functions with Application   in Optimum Experimental Design (Extended Version)", 
    "arxiv-id": "1001.1654v2", 
    "author": "L. Lehmann", 
    "publish": "2010-01-11T13:37:38Z", 
    "summary": "We derive algorithms for higher order derivative computation of the\nrectangular $QR$ and eigenvalue decomposition of symmetric matrices with\ndistinct eigenvalues in the forward and reverse mode of algorithmic\ndifferentiation (AD) using univariate Taylor propagation of matrices (UTPM).\nLinear algebra functions are regarded as elementary functions and not as\nalgorithms. The presented algorithms are implemented in the BSD licensed AD\ntool \\texttt{ALGOPY}. Numerical tests show that the UTPM algorithms derived in\nthis paper produce results close to machine precision accuracy. The theory\ndeveloped in this paper is applied to compute the gradient of an objective\nfunction motivated from optimum experimental design: $\\nabla_x\n\\Phi(C(J(F(x,y))))$, where $\\Phi = \\{\\lambda_1 : \\lambda_1 C\\}$, $C = (J^T\nJ)^{-1}$, $J = \\frac{\\dd F}{\\dd y}$ and $F = F(x,y)$."
},{
    "category": "cs.DS", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/1001.5272v1", 
    "title": "An in-place truncated Fourier transform and applications to polynomial   multiplication", 
    "arxiv-id": "1001.5272v1", 
    "author": "Daniel S. Roche", 
    "publish": "2010-01-28T21:10:41Z", 
    "summary": "The truncated Fourier transform (TFT) was introduced by van der Hoeven in\n2004 as a means of smoothing the \"jumps\" in running time of the ordinary FFT\nalgorithm that occur at power-of-two input sizes. However, the TFT still\nintroduces these jumps in memory usage. We describe in-place variants of the\nforward and inverse TFT algorithms, achieving time complexity O(n log n) with\nonly O(1) auxiliary space. As an application, we extend the second author's\nresults on space-restricted FFT-based polynomial multiplication to polynomials\nof arbitrary degree."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cma.2010.02.008", 
    "link": "http://arxiv.org/pdf/1002.0012v1", 
    "title": "The Power of Vocabulary: The Case of Cyclotomic Polynomials", 
    "arxiv-id": "1002.0012v1", 
    "author": "James H. Davenport", 
    "publish": "2010-01-29T21:23:05Z", 
    "summary": "We observe that the vocabulary used to construct the \"answer\" to problems in\ncomputer algebra can have a dramatic effect on the computational complexity of\nsolving that problem. We recall a formalization of this observation and explain\nthe classic example of sparse polynomial arithmetic. For this case, we show\nthat it is possible to extend the vocabulary so as reap the benefits of\nconciseness whilst avoiding the obvious pitfall of repeating the problem\nstatement as the \"solution\".\n  It is possible to extend the vocabulary either by irreducible cyclotomics or\nby $x^n-1$: we look at the options and suggest that the pragmatist might opt\nfor both."
},{
    "category": "cs.CE", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1002.4661v1", 
    "title": "Complementary approaches to understanding the plant circadian clock", 
    "arxiv-id": "1002.4661v1", 
    "author": "Carl Troein", 
    "publish": "2010-02-24T23:47:53Z", 
    "summary": "Circadian clocks are oscillatory genetic networks that help organisms adapt\nto the 24-hour day/night cycle. The clock of the green alga Ostreococcus tauri\nis the simplest plant clock discovered so far. Its many advantages as an\nexperimental system facilitate the testing of computational predictions.\n  We present a model of the Ostreococcus clock in the stochastic process\nalgebra Bio-PEPA and exploit its mapping to different analysis techniques, such\nas ordinary differential equations, stochastic simulation algorithms and\nmodel-checking. The small number of molecules reported for this system tests\nthe limits of the continuous approximation underlying differential equations.\nWe investigate the difference between continuous-deterministic and\ndiscrete-stochastic approaches. Stochastic simulation and model-checking allow\nus to formulate new hypotheses on the system behaviour, such as the presence of\nself-sustained oscillations in single cells under constant light conditions.\n  We investigate how to model the timing of dawn and dusk in the context of\nmodel-checking, which we use to compute how the probability distributions of\nkey biochemical species change over time. These show that the relative\nvariation in expression level is smallest at the time of peak expression,\nmaking peak time an optimal experimental phase marker. Building on these\nanalyses, we use approaches from evolutionary systems biology to investigate\nhow changes in the rate of mRNA degradation impacts the phase of a key protein\nlikely to affect fitness. We explore how robust this circadian clock is towards\nsuch potential mutational changes in its underlying biochemistry. Our work\nshows that multiple approaches lead to a more complete understanding of the\nclock."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1002.4784v2", 
    "title": "Triangular Decomposition of Semi-algebraic Systems", 
    "arxiv-id": "1002.4784v2", 
    "author": "Rong Xiao", 
    "publish": "2010-02-25T13:39:09Z", 
    "summary": "Regular chains and triangular decompositions are fundamental and\nwell-developed tools for describing the complex solutions of polynomial\nsystems. This paper proposes adaptations of these tools focusing on solutions\nof the real analogue: semi-algebraic systems. We show that any such system can\nbe decomposed into finitely many {\\em regular semi-algebraic systems}. We\npropose two specifications of such a decomposition and present corresponding\nalgorithms. Under some assumptions, one type of decomposition can be computed\nin singly exponential time w.r.t.\\ the number of variables. We implement our\nalgorithms and the experimental results illustrate their effectiveness."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1003.1628v1", 
    "title": "Having Fun with Lambert W(x) Function", 
    "arxiv-id": "1003.1628v1", 
    "author": "Darko Veberic", 
    "publish": "2010-03-08T13:47:25Z", 
    "summary": "This short note presents the Lambert W(x) function and its possible\napplication in the framework of physics related to the Pierre Auger\nObservatory. The actual numerical implementation in C++ consists of Halley's\nand Fritsch's iteration with branch-point expansion, asymptotic series and\nrational fits as initial approximations."
},{
    "category": "cs.NA", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1003.4628v2", 
    "title": "Efficient Construction, Update and Downdate Of The Coefficients Of   Interpolants Based On Polynomials Satisfying A Three-Term Recurrence Relation", 
    "arxiv-id": "1003.4628v2", 
    "author": "Pedro Gonnet", 
    "publish": "2010-03-24T12:46:52Z", 
    "summary": "In this paper, we consider methods to compute the coefficients of\ninterpolants relative to a basis of polynomials satisfying a three-term\nrecurrence relation. Two new algorithms are presented: the first constructs the\ncoefficients of the interpolation incrementally and can be used to update the\ncoefficients whenever a nodes is added to or removed from the interpolation.\nThe second algorithm, which constructs the interpolation coefficients by\ndecomposing the Vandermonde-like matrix iteratively, can not be used to update\nor downdate an interpolation, yet is more numerically stable than the first\nalgorithm and is more efficient when the coefficients of multiple\ninterpolations are to be computed over the same set of nodes."
},{
    "category": "cs.NA", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1003.4629v2", 
    "title": "A Review of Error Estimation in Adaptive Quadrature", 
    "arxiv-id": "1003.4629v2", 
    "author": "Pedro Gonnet", 
    "publish": "2010-03-24T12:47:13Z", 
    "summary": "The most critical component of any adaptive numerical quadrature routine is\nthe estimation of the integration error. Since the publication of the first\nalgorithms in the 1960s, many error estimation schemes have been presented,\nevaluated and discussed. This paper presents a review of existing error\nestimation techniques and discusses their differences and their common\nfeatures. Some common shortcomings of these algorithms are discussed and a new\ngeneral error estimation technique is presented."
},{
    "category": "cs.DL", 
    "doi": "10.4204/EPTCS.19.1", 
    "link": "http://arxiv.org/pdf/1003.5192v1", 
    "title": "wiki.openmath.org - how it works, how you can participate", 
    "arxiv-id": "1003.5192v1", 
    "author": "Christoph Lange", 
    "publish": "2010-03-26T17:32:10Z", 
    "summary": "At http://wiki.openmath.org, the OpenMath 2 and 3 Content Dictionaries are\naccessible via a semantic wiki interface, powered by the SWiM system. We\nshortly introduce the inner workings of the system, then describe how to use\nit, and conclude with first experiences gained from OpenMath society members\nworking with the system and an outlook to further development plans."
},{
    "category": "cs.DL", 
    "doi": "10.1007/978-3-540-68234-9_68", 
    "link": "http://arxiv.org/pdf/1003.5196v1", 
    "title": "SWiM -- A Semantic Wiki for Mathematical Knowledge Management", 
    "arxiv-id": "1003.5196v1", 
    "author": "Christoph Lange", 
    "publish": "2010-03-26T18:17:01Z", 
    "summary": "SWiM is a semantic wiki for collaboratively building, editing and browsing\nmathematical knowledge represented in the domain-specific structural semantic\nmarkup language OMDoc. It motivates users to contribute to collections of\nmathematical knowledge by instantly sharing the benefits of knowledge-powered\nservices with them. SWiM is currently being used for authoring content\ndictionaries, i. e. collections of uniquely identified mathematical symbols,\nand prepared for managing a large-scale proof formalisation effort."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-540-68234-9_68", 
    "link": "http://arxiv.org/pdf/1003.6036v3", 
    "title": "Computational Complexity of Iterated Maps on the Interval", 
    "arxiv-id": "1003.6036v3", 
    "author": "Christoph Spandl", 
    "publish": "2010-03-31T12:26:10Z", 
    "summary": "The correct computation of orbits of discrete dynamical systems on the\ninterval is considered. Therefore, an arbitrary-precision floating-point\napproach based on automatic error analysis is chosen and a general algorithm is\npresented. The correctness of the algorithm is shown and the computational\ncomplexity is analyzed. There are two main results. First, the computational\ncomplexity measure considered here is related to the Lyapunov exponent of the\ndynamical system under consideration. Second, the presented algorithm is\noptimal with regard to that complexity measure."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-540-68234-9_68", 
    "link": "http://arxiv.org/pdf/1004.3173v2", 
    "title": "MP users guide", 
    "arxiv-id": "1004.3173v2", 
    "author": "Richard P. Brent", 
    "publish": "2010-04-19T12:26:15Z", 
    "summary": "MP is a package of ANSI Standard Fortran (ANS X3.9-1966) subroutines for\nperforming multiple-precision floating-point arithmetic and evaluating\nelementary and special functions. The subroutines are machine independent and\nthe precision is arbitrary, subject to storage limitations. The User's Guide\ndescribes the routines and their calling sequences, example and test programs,\nuse of the Augment precompiler, and gives installation instructions for the\npackage."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1004.3719v1", 
    "title": "Exact Sparse Matrix-Vector Multiplication on GPU's and Multicore   Architectures", 
    "arxiv-id": "1004.3719v1", 
    "author": "Pascal Giorgi", 
    "publish": "2010-04-21T14:52:36Z", 
    "summary": "We propose different implementations of the sparse matrix--dense vector\nmultiplication (\\spmv{}) for finite fields and rings $\\Zb/m\\Zb$. We take\nadvantage of graphic card processors (GPU) and multi-core architectures. Our\naim is to improve the speed of \\spmv{} in the \\linbox library, and henceforth\nthe speed of its black box algorithms. Besides, we use this and a new\nparallelization of the sigma-basis algorithm in a parallel block Wiedemann rank\nimplementation over finite fields."
},{
    "category": "cs.LO", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1004.5034v1", 
    "title": "Formal Proof of SCHUR Conjugate Function", 
    "arxiv-id": "1004.5034v1", 
    "author": "Fr\u00e9d\u00e9ric Toumazet", 
    "publish": "2010-04-28T14:12:43Z", 
    "summary": "The main goal of our work is to formally prove the correctness of the key\ncommands of the SCHUR software, an interactive program for calculating with\ncharacters of Lie groups and symmetric functions. The core of the computations\nrelies on enumeration and manipulation of combinatorial structures. As a first\n\"proof of concept\", we present a formal proof of the conjugate function,\nwritten in C. This function computes the conjugate of an integer partition. To\nformally prove this program, we use the Frama-C software. It allows us to\nannotate C functions and to generate proof obligations, which are proved using\nseveral automated theorem provers. In this paper, we also draw on methodology,\ndiscussing on how to formally prove this kind of program."
},{
    "category": "cs.DL", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.0146v1", 
    "title": "The Formulator MathML Editor Project: User-Friendly Authoring of Content   Markup Documents", 
    "arxiv-id": "1005.0146v1", 
    "author": "Valentyn Yanchuk", 
    "publish": "2010-05-02T16:12:32Z", 
    "summary": "Implementation of an editing process for Content MathML formulas in common\nvisual style is a real challenge for a software developer who does not really\nwant the user to have to understand the structure of Content MathML in order to\nedit an expression, since it is expected that users are often not that\ntechnically minded. In this paper, we demonstrate how this aim is achieved in\nthe context of the Formulator project and discuss features of this MathML\neditor, which provides a user with a WYSIWYG editing style while authoring\nMathML documents with Content or mixed markup. We also present the approach\ntaken to enhance availability of the MathML editor to end-users, demonstrating\nan online version of the editor that runs inside a Web browser."
},{
    "category": "math.NA", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.1252v1", 
    "title": "Universal algorithms, mathematics of semirings and parallel computations", 
    "arxiv-id": "1005.1252v1", 
    "author": "A. N. Sobolevski", 
    "publish": "2010-05-07T17:31:20Z", 
    "summary": "This is a survey paper on applications of mathematics of semirings to\nnumerical analysis and computing. Concepts of universal algorithm and generic\nprogram are discussed. Relations between these concepts and mathematics of\nsemirings are examined. A very brief introduction to mathematics of semirings\n(including idempotent and tropical mathematics) is presented. Concrete\napplications to optimization problems, idempotent linear algebra and interval\nanalysis are indicated. It is known that some nonlinear problems (and\nespecially optimization problems) become linear over appropriate semirings with\nidempotent addition (the so-called idempotent superposition principle). This\nlinearity over semirings is convenient for parallel computations."
},{
    "category": "cs.CE", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.2819v1", 
    "title": "SABRE: A Tool for Stochastic Analysis of Biochemical Reaction Networks", 
    "arxiv-id": "1005.2819v1", 
    "author": "Verena Wolf", 
    "publish": "2010-05-17T06:58:07Z", 
    "summary": "The importance of stochasticity within biological systems has been shown\nrepeatedly during the last years and has raised the need for efficient\nstochastic tools. We present SABRE, a tool for stochastic analysis of\nbiochemical reaction networks. SABRE implements fast adaptive uniformization\n(FAU), a direct numerical approximation algorithm for computing transient\nsolutions of biochemical reaction networks. Biochemical reactions networks\nrepresent biological systems studied at a molecular level and these reactions\ncan be modeled as transitions of a Markov chain. SABRE accepts as input the\nformalism of guarded commands, which it interprets either as continuous-time or\nas discrete-time Markov chains. Besides operating in a stochastic mode, SABRE\nmay also perform a deterministic analysis by directly computing a mean-field\napproximation of the system under study. We illustrate the different\nfunctionalities of SABRE by means of biological case studies."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.3992v2", 
    "title": "Groebner bases in Java with applications in computer graphics", 
    "arxiv-id": "1005.3992v2", 
    "author": "Milan Z. Campara", 
    "publish": "2010-05-19T11:26:36Z", 
    "summary": "In this paper we present a Java implementation of the algorithm that computes\nBuchbereger's and reduced Groebner's basis step by step. The Java application\nenables graphical representation of the intersection of two surfaces in\n3-dimensional space and determines conditions of existence and planarity of the\nintersection."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.4117v1", 
    "title": "Random Numbers in Scientific Computing: An Introduction", 
    "arxiv-id": "1005.4117v1", 
    "author": "Helmut G. Katzgraber", 
    "publish": "2010-05-22T10:10:33Z", 
    "summary": "Random numbers play a crucial role in science and industry. Many numerical\nmethods require the use of random numbers, in particular the Monte Carlo\nmethod. Therefore it is of paramount importance to have efficient random number\ngenerators. The differences, advantages and disadvantages of true and pseudo\nrandom number generators are discussed with an emphasis on the intrinsic\ndetails of modern and fast pseudo random number generators. Furthermore,\nstandard tests to verify the quality of the random numbers produced by a given\ngenerator are outlined. Finally, standard scientific libraries with built-in\ngenerators are presented, as well as different approaches to generate\nnonuniform random numbers. Potential problems that one might encounter when\nusing large parallel machines are discussed."
},{
    "category": "cs.MS", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1005.4661v2", 
    "title": "Nonsingular Efficient Modeling of Rotations in 3-space using three   components", 
    "arxiv-id": "1005.4661v2", 
    "author": "Norman J. Goldstein", 
    "publish": "2010-05-22T22:21:10Z", 
    "summary": "This article introduces yet another representation of rotations in 3-space.\nThe rotations form a 3-dimensional projective space, which fact has not been\nexploited in Computer Science. We use the four affine patches of this\nprojective space to parametrize the rotations. This affine patch representation\nis more compact than quaternions (which require 4 components for calculations),\nencompasses the entire rotation group without singularities (unlike the Euler\nangles and rotation vector approaches), and requires only ratios of linear or\nquadratic polynomials for basic computations (unlike the Euler angles and\nrotation vector approaches which require transcendental functions).\n  As an example, we derive the differential equation for the integration of\nangular velocity using this affine patch representation of rotations. We remark\nthat the complexity of this equation is the same as the corresponding\nquaternion equation, but has advantages over the quaternion approach e.g.\nrenormalization to unit length is not required, and state space has no dead\ndirections."
},{
    "category": "cs.DC", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1006.2183v1", 
    "title": "Highly Parallel Sparse Matrix-Matrix Multiplication", 
    "arxiv-id": "1006.2183v1", 
    "author": "John R. Gilbert", 
    "publish": "2010-06-11T02:10:58Z", 
    "summary": "Generalized sparse matrix-matrix multiplication is a key primitive for many\nhigh performance graph algorithms as well as some linear solvers such as\nmultigrid. We present the first parallel algorithms that achieve increasing\nspeedups for an unbounded number of processors. Our algorithms are based on\ntwo-dimensional block distribution of sparse matrices where serial sections use\na novel hypersparse kernel for scalability. We give a state-of-the-art MPI\nimplementation of one of our algorithms. Our experiments show scaling up to\nthousands of processors on a variety of test scenarios."
},{
    "category": "cs.DS", 
    "doi": "10.1145/1837210.1837224", 
    "link": "http://arxiv.org/pdf/1008.2909v1", 
    "title": "Runtime-Flexible Multi-dimensional Arrays and Views for C++98 and C++0x", 
    "arxiv-id": "1008.2909v1", 
    "author": "Fred A. Hamprecht", 
    "publish": "2010-08-17T14:50:56Z", 
    "summary": "Multi-dimensional arrays are among the most fundamental and most useful data\nstructures of all. In C++, excellent template libraries exist for arrays whose\ndimension is fixed at runtime. Arrays whose dimension can change at runtime\nhave been implemented in C. However, a generic object-oriented C++\nimplementation of runtime-flexible arrays has so far been missing. In this\narticle, we discuss our new implementation called Marray, a package of class\ntemplates that fills this gap. Marray is based on views as an underlying\nconcept. This concept brings some of the flexibility known from script\nlanguages such as R and MATLAB to C++. Marray is free both for commercial and\nnon-commercial use and is publicly available from www.andres.sc/marray"
},{
    "category": "cs.MM", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1009.1170v1", 
    "title": "M-Learning: A New Paradigm of Learning Mathematics in Malaysia", 
    "arxiv-id": "1009.1170v1", 
    "author": "Shakirah Mohd Taib", 
    "publish": "2010-09-06T22:33:38Z", 
    "summary": "M-Learning is a new learning paradigm of the new social structure with mobile\nand wireless technologies.Smart school is one of the four flagship applications\nfor Multimedia Super Corridor (MSC) under Malaysian government initiative to\nimprove education standard in the country. With the advances of mobile devices\ntechnologies, mobile learning could help the government in realizing the\ninitiative. This paper discusses the prospect of implementing mobile learning\nfor primary school students. It indicates significant and challenges and\nanalysis of user perceptions on potential mobile applications through a survey\ndone in primary school context. The authors propose the m-Learning for\nmathematics by allowing the extension of technology in the traditional\nclassroom in term of learning and teaching."
},{
    "category": "cs.SE", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1009.1317v1", 
    "title": "LinBox founding scope allocation, parallel building blocks, and separate   compilation", 
    "arxiv-id": "1009.1317v1", 
    "author": "B. David Saunders", 
    "publish": "2010-07-28T12:20:06Z", 
    "summary": "To maximize efficiency in time and space, allocations and deallocations, in\nthe exact linear algebra library \\linbox, must always occur in the founding\nscope. This provides a simple lightweight allocation model. We present this\nmodel and its usage for the rebinding of matrices between different coefficient\ndomains. We also present automatic tools to speed-up the compilation of\ntemplate libraries and a software abstraction layer for the introduction of\ntransparent parallelism at the algorithmic level."
},{
    "category": "hep-ph", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1009.4647v2", 
    "title": "Parameterized Adaptive Multidimensional Integration Routines (PAMIR):   Localization by Repeated 2^p Subdivision", 
    "arxiv-id": "1009.4647v2", 
    "author": "Stephen L. Adler", 
    "publish": "2010-09-23T15:54:20Z", 
    "summary": "This book draft gives the theory of a new method for p dimensional adaptive\nintegration by repeated 2^p subdivision of simplexes and hypercubes. A new\nmethod of constructing high order integration routines for these geometries\npermits adjustable samplings of the integration region controlled by user\nsupplied parameters. An outline of the programs and use instructions are also\nincluded in the draft. The fortran programs are not included, but will be\npublished with this draft as a book."
},{
    "category": "cs.MS", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1010.1386v1", 
    "title": "An Elimination Method for Solving Bivariate Polynomial Systems:   Eliminating the Usual Drawbacks", 
    "arxiv-id": "1010.1386v1", 
    "author": "Michael Sagraloff", 
    "publish": "2010-10-07T10:18:32Z", 
    "summary": "We present an exact and complete algorithm to isolate the real solutions of a\nzero-dimensional bivariate polynomial system. The proposed algorithm\nconstitutes an elimination method which improves upon existing approaches in a\nnumber of points. First, the amount of purely symbolic operations is\nsignificantly reduced, that is, only resultant computation and square-free\nfactorization is still needed. Second, our algorithm neither assumes generic\nposition of the input system nor demands for any change of the coordinate\nsystem. The latter is due to a novel inclusion predicate to certify that a\ncertain region is isolating for a solution. Our implementation exploits\ngraphics hardware to expedite the resultant computation. Furthermore, we\nintegrate a number of filtering techniques to improve the overall performance.\nEfficiency of the proposed method is proven by a comparison of our\nimplementation with two state-of-the-art implementations, that is, LPG and\nMaple's isolate. For a series of challenging benchmark instances, experiments\nshow that our implementation outperforms both contestants."
},{
    "category": "math.NA", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1011.1091v2", 
    "title": "alphaCertified: certifying solutions to polynomial systems", 
    "arxiv-id": "1011.1091v2", 
    "author": "Frank Sottile", 
    "publish": "2010-11-04T09:57:00Z", 
    "summary": "Smale's alpha-theory uses estimates related to the convergence of Newton's\nmethod to give criteria implying that Newton iterations will converge\nquadratically to solutions to a square polynomial system. The program\nalphaCertified implements algorithms based on alpha-theory to certify solutions\nto polynomial systems using both exact rational arithmetic and arbitrary\nprecision floating point arithmetic. It also implements an algorithm to certify\nwhether a given point corresponds to a real solution to a real polynomial\nsystem, as well as algorithms to heuristically validate solutions to\noverdetermined systems. Examples are presented to demonstrate the algorithms."
},{
    "category": "math.NA", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1011.3077v1", 
    "title": "Minimizing Communication for Eigenproblems and the Singular Value   Decomposition", 
    "arxiv-id": "1011.3077v1", 
    "author": "Ioana Dumitriu", 
    "publish": "2010-11-13T00:18:02Z", 
    "summary": "Algorithms have two costs: arithmetic and communication. The latter\nrepresents the cost of moving data, either between levels of a memory\nhierarchy, or between processors over a network. Communication often dominates\narithmetic and represents a rapidly increasing proportion of the total cost, so\nwe seek algorithms that minimize communication. In \\cite{BDHS10} lower bounds\nwere presented on the amount of communication required for essentially all\n$O(n^3)$-like algorithms for linear algebra, including eigenvalue problems and\nthe SVD. Conventional algorithms, including those currently implemented in\n(Sca)LAPACK, perform asymptotically more communication than these lower bounds\nrequire. In this paper we present parallel and sequential eigenvalue algorithms\n(for pencils, nonsymmetric matrices, and symmetric matrices) and SVD algorithms\nthat do attain these lower bounds, and analyze their convergence and\ncommunication costs."
},{
    "category": "cs.DS", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1011.3534v1", 
    "title": "Fast Multiplication of Matrices with Decay", 
    "arxiv-id": "1011.3534v1", 
    "author": "Nicolas Bock", 
    "publish": "2010-11-15T21:59:11Z", 
    "summary": "A fast algorithm for the approximate multiplication of matrices with decay is\nintroduced; the Sparse Approximate Matrix Multiply (SpAMM) reduces complexity\nin the product space, a different approach from current methods that economize\nwithin the matrix space through truncation or rank reduction. Matrix truncation\n(element dropping) is compared to SpAMM for quantum chemical matrices with\napproximate exponential and algebraic decay. For matched errors in the\nelectronic total energy, SpAMM is found to require fewer to far fewer floating\npoint operations relative to dropping. The challenges and opportunities\nafforded by this new approach are discussed, including the potential for high\nperformance implementations."
},{
    "category": "cs.SC", 
    "doi": "10.5121/ijcsit.2010.2407", 
    "link": "http://arxiv.org/pdf/1101.3682v3", 
    "title": "Diversification improves interpolation", 
    "arxiv-id": "1101.3682v3", 
    "author": "Daniel S. Roche", 
    "publish": "2011-01-19T13:14:32Z", 
    "summary": "We consider the problem of interpolating an unknown multivariate polynomial\nwith coefficients taken from a finite field or as numerical approximations of\ncomplex numbers. Building on the recent work of Garg and Schost, we improve on\nthe best-known algorithm for interpolation over large finite fields by\npresenting a Las Vegas randomized algorithm that uses fewer black box\nevaluations. Using related techniques, we also address numerical interpolation\nof sparse polynomials with complex coefficients, and provide the first provably\nstable algorithm (in the sense of relative error) for this problem, at the cost\nof modestly more evaluations. A key new technique is a randomization which\nmakes all coefficients of the unknown polynomial distinguishable, producing\nwhat we call a diverse polynomial. Another departure from most previous\napproaches is that our algorithms do not rely on root finding as a subroutine.\nWe show how these improvements affect the practical performance with trial\nimplementations."
},{
    "category": "cond-mat.mes-hall", 
    "doi": "10.1145/2331130.2331138", 
    "link": "http://arxiv.org/pdf/1102.3440v2", 
    "title": "Efficient numerical computation of the Pfaffian for dense and banded   skew-symmetric matrices", 
    "arxiv-id": "1102.3440v2", 
    "author": "M. Wimmer", 
    "publish": "2011-02-16T21:48:45Z", 
    "summary": "Computing the Pfaffian of a skew-symmetric matrix is a problem that arises in\nvarious fields of physics. Both computing the Pfaffian and a related problem,\ncomputing the canonical form of a skew-symmetric matrix under unitary\ncongruence, can be solved easily once the skew-symmetric matrix has been\nreduced to skew-symmetric tridiagonal form. We develop efficient numerical\nmethods for computing this tridiagonal form based on Gauss transformations,\nusing a skew-symmetric, blocked form of the Parlett-Reid algorithm, or based on\nunitary transformations, using block Householder transformations and Givens\nrotations, that are applicable to dense and banded matrices, respectively. We\nalso give a complete and fully optimized implementation of these algorithms in\nFortran, and also provide Python, Matlab and Mathematica implementations for\nconvenience. Finally, we apply these methods to compute the topological charge\nof a class D nanowire, and show numerically the equivalence of definitions\nbased on the Hamiltonian and the scattering matrix."
},{
    "category": "quant-ph", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1102.4598v2", 
    "title": "Generating and using truly random quantum states in Mathematica", 
    "arxiv-id": "1102.4598v2", 
    "author": "J. A. Miszczak", 
    "publish": "2011-02-22T19:57:11Z", 
    "summary": "The problem of generating random quantum states is of a great interest from\nthe quantum information theory point of view. In this paper we present a\npackage for Mathematica computing system harnessing a specific piece of\nhardware, namely Quantis quantum random number generator (QRNG), for\ninvestigating statistical properties of quantum states. The described package\nimplements a number of functions for generating random states, which use\nQuantis QRNG as a source of randomness. It also provides procedures which can\nbe used in simulations not related directly to quantum information processing."
},{
    "category": "nlin.CD", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1103.3160v2", 
    "title": "sin[n Delta t sin (n Delta t1)] as a source of unpredictable dynamics", 
    "arxiv-id": "1103.3160v2", 
    "author": "Stefano Morosetti", 
    "publish": "2011-03-16T12:55:13Z", 
    "summary": "We investigate the ability of the function sin[n Delta t sin (n Delta t1)],\nwhere n is an integer and growing number, to produce unpredictable sequences of\nnumbers. Classical mathematical tools for distinguishing periodic from chaotic\nor random behaviour, such as sensitivity to the initial conditions, Fourier\nanalysis, and autocorrelation are used. Moreover, the function acos{sin[n Delta\nt sin (n Delta t1)]}/pigreek is introduced to have an uniform density of\nnumbers in the interval [0,1], so it can be submitted to a battery of widely\nused tests for random number generators. All these tools show that a proper\nchoice of Delta t and Delta t1, can produce a sequence of numbers behaving as\nunpredictable dynamics."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1105.4136v3", 
    "title": "A novel parallel algorithm for Gaussian Elimination of sparse   unsymmetric matrices", 
    "arxiv-id": "1105.4136v3", 
    "author": "Riccardo Murri", 
    "publish": "2011-05-20T17:29:57Z", 
    "summary": "We describe a new algorithm for Gaussian Elimination suitable for general\n(unsymmetric and possibly singular) sparse matrices, of any entry type, which\nhas a natural parallel and distributed-memory formulation but degrades\ngracefully to sequential execution.\n  We present a sample MPI implementation of a program computing the rank of a\nsparse integer matrix using the proposed algorithm. Some preliminary\nperformance measurements are presented and discussed, and the performance of\nthe algorithm is compared to corresponding state-of-the-art algorithms for\nfloating-point and integer matrices."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1105.4324v1", 
    "title": "A search for an optimal start system for numerical homotopy continuation", 
    "arxiv-id": "1105.4324v1", 
    "author": "Anton Leykin", 
    "publish": "2011-05-22T09:57:05Z", 
    "summary": "We use our recent implementation of a certified homotopy tracking algorithm\nto search for start systems that minimize the average complexity of finding all\nroots of a regular system of polynomial equations. While finding optimal start\nsystems is a hard problem, our experiments show that it is possible to find\nstart systems that deliver better average complexity than the ones that are\ncommonly used in the existing homotopy continuation software."
},{
    "category": "math.AG", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1105.4881v2", 
    "title": "PHCpack in Macaulay2", 
    "arxiv-id": "1105.4881v2", 
    "author": "Jan Verschelde", 
    "publish": "2011-05-24T20:15:08Z", 
    "summary": "The Macaulay2 package PHCpack.m2 provides an interface to PHCpack, a\ngeneral-purpose polynomial system solver that uses homotopy continuation. The\nmain method is a numerical blackbox solver which is implemented for all Laurent\nsystems. The package also provides a fast mixed volume computation, the ability\nto filter solutions, homotopy path tracking, and a numerical irreducible\ndecomposition method. As the size of many problems in applied algebraic\ngeometry often surpasses the capabilities of symbolic software, this package\nwill be of interest to those working on problems involving large polynomial\nsystems."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1106.1319v1", 
    "title": "ShearLab: A Rational Design of a Digital Parabolic Scaling Algorithm", 
    "arxiv-id": "1106.1319v1", 
    "author": "Xiaosheng Zhuang", 
    "publish": "2011-06-07T11:36:55Z", 
    "summary": "Multivariate problems are typically governed by anisotropic features such as\nedges in images. A common bracket of most of the various directional\nrepresentation systems which have been proposed to deliver sparse\napproximations of such features is the utilization of parabolic scaling. One\nprominent example is the shearlet system. Our objective in this paper is\nthree-fold: We firstly develop a digital shearlet theory which is rationally\ndesigned in the sense that it is the digitization of the existing shearlet\ntheory for continuous data. This implicates that shearlet theory provides a\nunified treatment of both the continuum and digital realm. Secondly, we analyze\nthe utilization of pseudo-polar grids and the pseudo-polar Fourier transform\nfor digital implementations of parabolic scaling algorithms. We derive an\nisometric pseudo-polar Fourier transform by careful weighting of the\npseudo-polar grid, allowing exploitation of its adjoint for the inverse\ntransform. This leads to a digital implementation of the shearlet transform; an\naccompanying Matlab toolbox called ShearLab is provided. And, thirdly, we\nintroduce various quantitative measures for digital parabolic scaling\nalgorithms in general, allowing one to tune parameters and objectively improve\nthe implementation as well as compare different directional transform\nimplementations. The usefulness of such measures is exemplarily demonstrated\nfor the digital shearlet transform."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1106.1862v1", 
    "title": "The MathScheme Library: Some Preliminary Experiments", 
    "arxiv-id": "1106.1862v1", 
    "author": "Quang M. Tran", 
    "publish": "2011-06-09T17:16:38Z", 
    "summary": "We present some of the experiments we have performed to best test our design\nfor a library for MathScheme, the mechanized mathematics software system we are\nbuilding. We wish for our library design to use and reflect, as much as\npossible, the mathematical structure present in the objects which populate the\nlibrary."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1106.5694v2", 
    "title": "GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under   Real-time Constraints", 
    "arxiv-id": "1106.5694v2", 
    "author": "Sameh El-Ansary", 
    "publish": "2011-06-28T14:53:58Z", 
    "summary": "In this paper we modify a fast heuristic solver for the Linear Sum Assignment\nProblem (LSAP) for use on Graphical Processing Units (GPUs). The motivating\nscenario is an industrial application for P2P live streaming that is moderated\nby a central node which is periodically solving LSAP instances for assigning\npeers to one another. The central node needs to handle LSAP instances involving\nthousands of peers in as near to real-time as possible. Our findings are\ngeneric enough to be applied in other contexts. Our main result is a parallel\nversion of a heuristic algorithm called Deep Greedy Switching (DGS) on GPUs\nusing the CUDA programming language. DGS sacrifices absolute optimality in\nfavor of low computation time and was designed as an alternative to classical\nLSAP solvers such as the Hungarian and auctioning methods. The contribution of\nthe paper is threefold: First, we present the process of trial and error we\nwent through, in the hope that our experience will be beneficial to adopters of\nGPU programming for similar problems. Second, we show the modifications needed\nto parallelize the DGS algorithm. Third, we show the performance gains of our\napproach compared to both a sequential CPU-based implementation of DGS and a\nparallel GPU-based implementation of the auctioning algorithm."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1107.0385v1", 
    "title": "An algorithm for autonomously plotting solution sets in the presence of   turning points", 
    "arxiv-id": "1107.0385v1", 
    "author": "Jonathan Pollack", 
    "publish": "2011-07-02T15:07:48Z", 
    "summary": "Plotting solution sets for particular equations may be complicated by the\nexistence of turning points. Here we describe an algorithm which not only\novercomes such problematic points, but does so in the most general of settings.\nApplications of the algorithm are highlighted through two examples: the first\nprovides verification, while the second demonstrates a non-trivial application.\nThe latter is followed by a thorough run-time analysis. While both examples\ndeal with bivariate equations, it is discussed how the algorithm may be\ngeneralized for space curves in $\\R^{3}$."
},{
    "category": "cs.RO", 
    "doi": "10.1016/j.cpc.2011.08.002", 
    "link": "http://arxiv.org/pdf/1107.1119v1", 
    "title": "Integrating Generic Sensor Fusion Algorithms with Sound State   Representations through Encapsulation of Manifolds", 
    "arxiv-id": "1107.1119v1", 
    "author": "Lutz Schr\u00f6der", 
    "publish": "2011-07-06T13:04:47Z", 
    "summary": "Common estimation algorithms, such as least squares estimation or the Kalman\nfilter, operate on a state in a state space S that is represented as a\nreal-valued vector. However, for many quantities, most notably orientations in\n3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a\nvector space locally but has a more complex global topological structure. For\nintegrating these quantities, several ad-hoc approaches have been proposed.\n  Here, we present a principled solution to this problem where the structure of\nthe manifold S is encapsulated by two operators, state displacement [+]:S x R^n\n--> S and its inverse [-]: S x S --> R^n. These operators provide a local\nvector-space view \\delta; --> x [+] \\delta; around a given state x. Generic\nestimation algorithms can then work on the manifold S mainly by replacing +/-\nwith [+]/[-] where appropriate. We analyze these operators axiomatically, and\ndemonstrate their use in least-squares estimation and the Unscented Kalman\nFilter. Moreover, we exploit the idea of encapsulation from a software\nengineering perspective in the Manifold Toolkit, where the [+]/[-] operators\nmediate between a \"flat-vector\" view for the generic algorithm and a\n\"named-members\" view for the problem specific functions."
},{
    "category": "cs.CE", 
    "doi": "10.1007/s10686-011-9241-6", 
    "link": "http://arxiv.org/pdf/1108.0355v1", 
    "title": "Using Java for distributed computing in the Gaia satellite data   processing", 
    "arxiv-id": "1108.0355v1", 
    "author": "Jose Hernandez", 
    "publish": "2011-08-01T16:24:33Z", 
    "summary": "In recent years Java has matured to a stable easy-to-use language with the\nflexibility of an interpreter (for reflection etc.) but the performance and\ntype checking of a compiled language. When we started using Java for\nastronomical applications around 1999 they were the first of their kind in\nastronomy. Now a great deal of astronomy software is written in Java as are\nmany business applications.\n  We discuss the current environment and trends concerning the language and\npresent an actual example of scientific use of Java for high-performance\ndistributed computing: ESA's mission Gaia. The Gaia scanning satellite will\nperform a galactic census of about 1000 million objects in our galaxy. The Gaia\ncommunity has chosen to write its processing software in Java. We explore the\nmanifold reasons for choosing Java for this large science collaboration.\n  Gaia processing is numerically complex but highly distributable, some parts\nbeing embarrassingly parallel. We describe the Gaia processing architecture and\nits realisation in Java. We delve into the astrometric solution which is the\nmost advanced and most complex part of the processing. The Gaia simulator is\nalso written in Java and is the most mature code in the system. This has been\nsuccessfully running since about 2005 on the supercomputer \"Marenostrum\" in\nBarcelona. We relate experiences of using Java on a large shared machine.\n  Finally we discuss Java, including some of its problems, for scientific\ncomputing."
},{
    "category": "cs.SC", 
    "doi": "10.1007/s10686-011-9241-6", 
    "link": "http://arxiv.org/pdf/1110.2263v1", 
    "title": "Asymptotic Methods of ODEs: Exploring Singularities of the Second Kind", 
    "arxiv-id": "1110.2263v1", 
    "author": "Christopher J. Winfield", 
    "publish": "2011-10-11T04:03:22Z", 
    "summary": "We develop symbolic methods of asymptotic approximations for solutions of\nlinear ordinary differential equations and use to them stabilize numerical\ncalculations. Our method follows classical analysis for first-order systems and\nhigher-order scalar equations where growth behavior is expressed in terms of\nelementary functions. We then recast our equations in mollified form - thereby\nobtaining stability."
},{
    "category": "cs.MS", 
    "doi": "10.1063/1.3637934", 
    "link": "http://arxiv.org/pdf/1110.3397v1", 
    "title": "Odeint - Solving ordinary differential equations in C++", 
    "arxiv-id": "1110.3397v1", 
    "author": "Mario Mulansky", 
    "publish": "2011-10-15T09:41:24Z", 
    "summary": "Many physical, biological or chemical systems are modeled by ordinary\ndifferential equations (ODEs) and finding their solution is an every-day-task\nfor many scientists. Here, we introduce a new C++ library dedicated to find\nnumerical solutions of initial value problems of ODEs: odeint (www.odeint.com).\nodeint is implemented in a highly generic way and provides extensive\ninteroperability at top performance. For example, due to it's modular design it\ncan be easily parallized with OpenMP and even runs on CUDA GPUs. Despite that,\nit provides a convenient interface that allows for a simple and easy usage."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.70.4", 
    "link": "http://arxiv.org/pdf/1110.4673v1", 
    "title": "How Can I Do That with ACL2? Recent Enhancements to ACL2", 
    "arxiv-id": "1110.4673v1", 
    "author": "J Strother Moore", 
    "publish": "2011-10-21T00:45:38Z", 
    "summary": "The last several years have seen major enhancements to ACL2 functionality,\nlargely driven by requests from its user community, including utilities now in\ncommon use such as 'make-event', 'mbe', and trust tags. In this paper we\nprovide user-level summaries of some ACL2 enhancements introduced after the\nrelease of Version 3.5 (in May, 2009, at about the time of the 2009 ACL2\nworkshop) up through the release of Version 4.3 in July, 2011, roughly a couple\nof years later. Many of these features are not particularly well known yet, but\nmost ACL2 users could take advantage of at least some of them. Some of the\nchanges could affect existing proof efforts, such as a change that treats pairs\nof functions such as 'member' and 'member-equal' as the same function."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.70.6", 
    "link": "http://arxiv.org/pdf/1110.4675v1", 
    "title": "Formal Verification of an Iterative Low-Power x86 Floating-Point   Multiplier with Redundant Feedback", 
    "arxiv-id": "1110.4675v1", 
    "author": "Peter-Michael Seidel", 
    "publish": "2011-10-21T00:45:53Z", 
    "summary": "We present the formal verification of a low-power x86 floating-point\nmultiplier. The multiplier operates iteratively and feeds back intermediate\nresults in redundant representation. It supports x87 and SSE instructions in\nvarious precisions and can block the issuing of new instructions. The design\nhas been optimized for low-power operation and has not been constrained by the\nformal verification effort. Additional improvements for the implementation were\nidentified through formal verification. The formal verification of the design\nalso incorporates the implementation of clock-gating and control logic. The\ncore of the verification effort was based on ACL2 theorem proving.\nAdditionally, model checking has been used to verify some properties of the\nfloating-point scheduler that are relevant for the correct operation of the\nunit."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2012.05.005", 
    "link": "http://arxiv.org/pdf/1110.5441v2", 
    "title": "LINPRO: linear inverse problem library for data contaminated by   statistical noise", 
    "arxiv-id": "1110.5441v2", 
    "author": "Gabriel Wlazlowski", 
    "publish": "2011-10-25T08:44:51Z", 
    "summary": "The library LINPRO which provides solution to the linear inverse problem for\ndata contaminated by a statistical noise is presented. The library makes use of\ntwo methods: Maximum Entropy Method and Singular Value Decomposition. As an\nexample it has been applied to perform an analytic continuation of the\nimaginary time propagator obtained within the Quantum Monte Carlo method."
},{
    "category": "cs.PF", 
    "doi": "10.1016/j.amc.2012.05.020", 
    "link": "http://arxiv.org/pdf/1111.6374v2", 
    "title": "Solving Dense Generalized Eigenproblems on Multi-threaded Architectures", 
    "arxiv-id": "1111.6374v2", 
    "author": "Enrique S. Quintana-Ort\u00ed", 
    "publish": "2011-11-28T08:52:04Z", 
    "summary": "We compare two approaches to compute a portion of the spectrum of dense\nsymmetric definite generalized eigenproblems: one is based on the reduction to\ntridiagonal form, and the other on the Krylov-subspace iteration. Two\nlarge-scale applications, arising in molecular dynamics and material science,\nare employed to investigate the contributions of the application, architecture,\nand parallelism of the method to the performance of the solvers. The\nexperimental results on a state-of-the-art 8-core platform, equipped with a\ngraphics processing unit (GPU), reveal that in real applications, iterative\nKrylov-subspace methods can be a competitive approach also for the solution of\ndense problems."
},{
    "category": "math.NA", 
    "doi": "10.1137/110856976", 
    "link": "http://arxiv.org/pdf/1111.6583v2", 
    "title": "PyClaw: Accessible, Extensible, Scalable Tools for Wave Propagation   Problems", 
    "arxiv-id": "1111.6583v2", 
    "author": "Matthew Emmett", 
    "publish": "2011-11-27T10:53:39Z", 
    "summary": "Development of scientific software involves tradeoffs between ease of use,\ngenerality, and performance. We describe the design of a general hyperbolic PDE\nsolver that can be operated with the convenience of MATLAB yet achieves\nefficiency near that of hand-coded Fortran and scales to the largest\nsupercomputers. This is achieved by using Python for most of the code while\nemploying automatically-wrapped Fortran kernels for computationally intensive\nroutines, and using Python bindings to interface with a parallel computing\nlibrary and other numerical packages. The software described here is PyClaw, a\nPython-based structured grid solver for general systems of hyperbolic PDEs\n\\cite{pyclaw}. PyClaw provides a powerful and intuitive interface to the\nalgorithms of the existing Fortran codes Clawpack and SharpClaw, simplifying\ncode development and use while providing massive parallelism and scalable\nsolvers via the PETSc library. The package is further augmented by use of\nPyWENO for generation of efficient high-order weighted essentially\nnon-oscillatory reconstruction code. The simplicity, capability, and\nperformance of this approach are demonstrated through application to example\nproblems in shallow water flow, compressible flow and elasticity."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPSW.2012.211", 
    "link": "http://arxiv.org/pdf/1112.5588v2", 
    "title": "Sparse matrix-vector multiplication on GPGPU clusters: A new storage   format and a scalable implementation", 
    "arxiv-id": "1112.5588v2", 
    "author": "Alan R. Bishop", 
    "publish": "2011-12-23T14:03:56Z", 
    "summary": "Sparse matrix-vector multiplication (spMVM) is the dominant operation in many\nsparse solvers. We investigate performance properties of spMVM with matrices of\nvarious sparsity patterns on the nVidia \"Fermi\" class of GPGPUs. A new \"padded\njagged diagonals storage\" (pJDS) format is proposed which may substantially\nreduce the memory overhead intrinsic to the widespread ELLPACK-R scheme. In our\ntest scenarios the pJDS format cuts the overall spMVM memory footprint on the\nGPGPU by up to 70%, and achieves 95% to 130% of the ELLPACK-R performance.\nUsing a suitable performance model we identify performance bottlenecks on the\nnode level that invalidate some types of matrix structures for efficient\nmulti-GPGPU parallelization. For appropriate sparsity patterns we extend\nprevious work on distributed-memory parallel spMVM to demonstrate a scalable\nhybrid MPI-GPGPU code, achieving efficient overlap of communication and\ncomputation."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2012.211", 
    "link": "http://arxiv.org/pdf/1201.0540v1", 
    "title": "ProofPeer - A Cloud-based Interactive Theorem Proving System", 
    "arxiv-id": "1201.0540v1", 
    "author": "Steven Obua", 
    "publish": "2012-01-02T21:48:33Z", 
    "summary": "ProofPeer strives to be a system for cloud-based interactive theorem proving.\nAfter illustrating why such a system is needed, the paper presents some of the\ndesign challenges that ProofPeer needs to meet to succeed. Contexts are\npresented as a solution to the problem of sharing proof state among the users\nof ProofPeer. Chronicles are introduced as a way to organize and version\ncontexts."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPSW.2012.211", 
    "link": "http://arxiv.org/pdf/1201.3179v1", 
    "title": "Matrix representation of a solution of a combinatorial problem of the   group theory", 
    "arxiv-id": "1201.3179v1", 
    "author": "Lilyana Totina", 
    "publish": "2012-01-16T09:12:34Z", 
    "summary": "An equivalence relation in the symmetric group, where is a positive integer\nhas been considered. An algorithm for calculation of the number of the\nequivalence classes by this relation for arbitrary integer has been described."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cagd.2012.09.001", 
    "link": "http://arxiv.org/pdf/1201.4050v2", 
    "title": "On the Shape of Curves that are Rational in Polar Coordinates", 
    "arxiv-id": "1201.4050v2", 
    "author": "G. M. D\u00edaz-Toca", 
    "publish": "2012-01-19T13:01:17Z", 
    "summary": "In this paper we provide a computational approach to the shape of curves\nwhich are rational in polar coordinates, i.e. which are defined by means of a\nparametrization (r(t),\\theta(t)) where both r(t),\\theta(t) are rational\nfunctions. Our study includes theoretical aspects on the shape of these curves,\nand algorithmic results which eventually lead to an algorithm for plotting the\n\"interesting parts\" of the curve, i.e. the parts showing the main geometrical\nfeatures of it. On the theoretical side, we prove that these curves, with the\nexceptions of lines and circles, cannot be algebraic (in cartesian\ncoordinates), we characterize the existence of infinitely many\nself-intersections, and we connect this with certain phenomena which are not\npossible in the algebraic world, namely the existence of limit circles, limit\npoints, or spiral branches. On the practical side, we provide an algorithm\nwhich has been implemented in the computer algebra system Maple to visualize\nthis kind of curves. Our implementation makes use (and improves some aspects\nof) the command polarplot currently available in Maple for plotting curves in\npolar form."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cagd.2012.09.001", 
    "link": "http://arxiv.org/pdf/1202.1056v1", 
    "title": "Building a Framework for Predictive Science", 
    "arxiv-id": "1202.1056v1", 
    "author": "Michael A. G. Aivazis", 
    "publish": "2012-02-06T06:53:25Z", 
    "summary": "Key questions that scientists and engineers typically want to address can be\nformulated in terms of predictive science. Questions such as: \"How well does my\ncomputational model represent reality?\", \"What are the most important\nparameters in the problem?\", and \"What is the best next experiment to perform?\"\nare fundamental in solving scientific problems. Mystic is a framework for\nmassively-parallel optimization and rigorous sensitivity analysis that enables\nthese motivating questions to be addressed quantitatively as global\noptimization problems. Often realistic physics, engineering, and materials\nmodels may have hundreds of input parameters, hundreds of constraints, and may\nrequire execution times of seconds or longer. In more extreme cases, realistic\nmodels may be multi-scale, and require the use of high-performance computing\nclusters for their evaluation. Predictive calculations, formulated as a global\noptimization over a potential surface in design parameter space, may require an\nalready prohibitively large simulation to be performed hundreds, if not\nthousands, of times. The need to prepare, schedule, and monitor thousands of\nmodel evaluations, and dynamically explore and analyze results, is a\nchallenging problem that requires a software infrastructure capable of\ndistributing and managing computations on large-scale heterogeneous resources.\nIn this paper, we present the design behind an optimization framework, and also\na framework for heterogeneous computing, that when utilized together, can make\ncomputationally intractable sensitivity and optimization problems much more\ntractable."
},{
    "category": "math.AG", 
    "doi": "10.1016/j.cagd.2012.09.001", 
    "link": "http://arxiv.org/pdf/1202.1820v2", 
    "title": "Fatgraph Algorithms and the Homology of the Kontsevich Complex", 
    "arxiv-id": "1202.1820v2", 
    "author": "Riccardo Murri", 
    "publish": "2012-02-08T20:58:11Z", 
    "summary": "Fatgraphs are multigraphs enriched with a cyclic order of the edges incident\nto a vertex. This paper presents algorithms to: (1) generate the set of all\nfatgraphs having a given genus and number of boundary cycles; (2) compute\nautomorphisms of any given fatgraph; (3) compute the homology of the fatgraph\ncomplex. The algorithms are suitable for effective computer implementation.\n  In particular, this allows us to compute the rational homology of the moduli\nspace of Riemann surfaces with marked points. We thus compute the Betti numbers\nof $M_{g,n}$ with $(2g + n) \\leq 6$, corroborating known results."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cagd.2012.09.001", 
    "link": "http://arxiv.org/pdf/1202.2736v1", 
    "title": "Function call overhead benchmarks with MATLAB, Octave, Python, Cython   and C", 
    "arxiv-id": "1202.2736v1", 
    "author": "Andr\u00e9 Gaul", 
    "publish": "2012-02-13T14:14:00Z", 
    "summary": "We consider the overhead of function calls in the programming languages\nMATLAB/Octave, Python, Cython and C. In many applications a function has to be\ncalled very often inside a loop. One such application in numerical analysis is\nthe finite element method where integrals have to be computed on each element\nin a loop. The called functions can often be evaluated efficiently but the\nfunction call itself may be time-consuming. We present a benchmark whose goal\nis to identify and quantify optimization potentials with respect to time\nconsumption caused by function calls in the mentioned programming languages."
},{
    "category": "cs.SY", 
    "doi": "10.4204/EPTCS.79", 
    "link": "http://arxiv.org/pdf/1202.4535v1", 
    "title": "Proceedings First Workshop on CTP Components for Educational Software", 
    "arxiv-id": "1202.4535v1", 
    "author": "Ralph-Johan Back", 
    "publish": "2012-02-21T05:56:10Z", 
    "summary": "The THedu'11 workshop received thirteen submissions, twelve of which were\naccepted and presented during the workshop. For the post-conference proceedings\nnine submission where received and accepted. The submissions are within the\nscope of the following points, which have been announced in the call of papers:\nCTP-based software tools for education; CTP technology combined with novel\ninterfaces, drag and drop, etc.; technologies to access ITP knowledge relevant\nfor a certain step of problem solving; usability considerations on representing\nITP knowledge; combination of deduction and computation; formal problem\nspecifications; effectiveness of ATP in checking user input; formats for\ndeductive content in proof documents, geometric constructions, etc; formal\ndomain models for e-learning in mathematics and applications."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.79.1", 
    "link": "http://arxiv.org/pdf/1202.4828v1", 
    "title": "Towards an Intelligent Tutor for Mathematical Proofs", 
    "arxiv-id": "1202.4828v1", 
    "author": "Marvin Schiller", 
    "publish": "2012-02-22T06:41:20Z", 
    "summary": "Computer-supported learning is an increasingly important form of study since\nit allows for independent learning and individualized instruction. In this\npaper, we discuss a novel approach to developing an intelligent tutoring system\nfor teaching textbook-style mathematical proofs. We characterize the\nparticularities of the domain and discuss common ITS design models. Our\napproach is motivated by phenomena found in a corpus of tutorial dialogs that\nwere collected in a Wizard-of-Oz experiment. We show how an intelligent tutor\nfor textbook-style mathematical proofs can be built on top of an adapted\nassertion-level proof assistant by reusing representations and proof search\nstrategies originally developed for automated and interactive theorem proving.\nThe resulting prototype was successfully evaluated on a corpus of tutorial\ndialogs and yields good results."
},{
    "category": "cs.SC", 
    "doi": "10.4204/EPTCS.79.4", 
    "link": "http://arxiv.org/pdf/1202.4831v1", 
    "title": "Formalization and Implementation of Algebraic Methods in Geometry", 
    "arxiv-id": "1202.4831v1", 
    "author": "Predrag Jani\u010di\u0107", 
    "publish": "2012-02-22T06:41:45Z", 
    "summary": "We describe our ongoing project of formalization of algebraic methods for\ngeometry theorem proving (Wu's method and the Groebner bases method), their\nimplementation and integration in educational tools. The project includes\nformal verification of the algebraic methods within Isabelle/HOL proof\nassistant and development of a new, open-source Java implementation of the\nalgebraic methods. The project should fill-in some gaps still existing in this\narea (e.g., the lack of formal links between algebraic methods and synthetic\ngeometry and the lack of self-contained implementations of algebraic methods\nsuitable for integration with dynamic geometry tools) and should enable new\napplications of theorem proving in education."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.79.8", 
    "link": "http://arxiv.org/pdf/1202.4834v1", 
    "title": "Computer-Assisted Program Reasoning Based on a Relational Semantics of   Programs", 
    "arxiv-id": "1202.4834v1", 
    "author": "Wolfgang Schreiner", 
    "publish": "2012-02-22T06:42:11Z", 
    "summary": "We present an approach to program reasoning which inserts between a program\nand its verification conditions an additional layer, the denotation of the\nprogram expressed in a declarative form. The program is first translated into\nits denotation from which subsequently the verification conditions are\ngenerated. However, even before (and independently of) any verification\nattempt, one may investigate the denotation itself to get insight into the\n\"semantic essence\" of the program, in particular to see whether the denotation\nindeed gives reason to believe that the program has the expected behavior.\nErrors in the program and in the meta-information may thus be detected and\nfixed prior to actually performing the formal verification. More concretely,\nfollowing the relational approach to program semantics, we model the effect of\na program as a binary relation on program states. A formal calculus is devised\nto derive from a program a logic formula that describes this relation and is\nsubject for inspection and manipulation. We have implemented this idea in a\ncomprehensive form in the RISC ProgramExplorer, a new program reasoning\nenvironment for educational purposes which encompasses the previously developed\nRISC ProofNavigator as an interactive proving assistant."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.79.9", 
    "link": "http://arxiv.org/pdf/1202.4835v1", 
    "title": "Isabelle/PIDE as Platform for Educational Tools", 
    "arxiv-id": "1202.4835v1", 
    "author": "Burkhart Wolff", 
    "publish": "2012-02-22T06:42:17Z", 
    "summary": "The Isabelle/PIDE platform addresses the question whether proof assistants of\nthe LCF family are suitable as technological basis for educational tools. The\ntraditionally strong logical foundations of systems like HOL, Coq, or Isabelle\nhave so far been counter-balanced by somewhat inaccessible interaction via the\nTTY (or minor variations like the well-known Proof General / Emacs interface).\nThus the fundamental question of math education tools with fully-formal\nbackground theories has often been answered negatively due to accidental\nweaknesses of existing proof engines.\n  The idea of \"PIDE\" (which means \"Prover IDE\") is to integrate existing\nprovers like Isabelle into a larger environment, that facilitates access by\nend-users and other tools. We use Scala to expose the proof engine in ML to the\nJVM world, where many user-interfaces, editor frameworks, and educational tools\nalready exist. This shall ultimately lead to combined mathematical assistants,\nwhere the logical engine is in the background, without obstructing the view on\napplications of formal methods, formalized mathematics, and math education in\nparticular."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1002/ggge.20071", 
    "link": "http://arxiv.org/pdf/1202.6522v5", 
    "title": "Efficient Spherical Harmonic Transforms aimed at pseudo-spectral   numerical simulations", 
    "arxiv-id": "1202.6522v5", 
    "author": "Nathana\u00ebl Schaeffer", 
    "publish": "2012-02-29T12:05:12Z", 
    "summary": "In this paper, we report on very efficient algorithms for the spherical\nharmonic transform (SHT). Explicitly vectorized variations of the algorithm\nbased on the Gauss-Legendre quadrature are discussed and implemented in the\nSHTns library which includes scalar and vector transforms. The main\nbreakthrough is to achieve very efficient on-the-fly computations of the\nLegendre associated functions, even for very high resolutions, by taking\nadvantage of the specific properties of the SHT and the advanced capabilities\nof current and future computers. This allows us to simultaneously and\nsignificantly reduce memory usage and computation time of the SHT. We measure\nthe performance and accuracy of our algorithms. Even though the complexity of\nthe algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum\nharmonic degree of the transform), they perform much better than any third\nparty implementation, including lower complexity algorithms, even for\ntruncations as high as N=1023. SHTns is available at\nhttps://bitbucket.org/nschaeff/shtns as open source software."
},{
    "category": "cs.MS", 
    "doi": "10.1002/ggge.20071", 
    "link": "http://arxiv.org/pdf/1202.6548v2", 
    "title": "mlpy: Machine Learning Python", 
    "arxiv-id": "1202.6548v2", 
    "author": "Cesare Furlanello", 
    "publish": "2012-02-29T13:49:10Z", 
    "summary": "mlpy is a Python Open Source Machine Learning library built on top of\nNumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of\nstate-of-the-art machine learning methods for supervised and unsupervised\nproblems and it is aimed at finding a reasonable compromise among modularity,\nmaintainability, reproducibility, usability and efficiency. mlpy is\nmultiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at\nthe website http://mlpy.fbk.eu."
},{
    "category": "astro-ph.EP", 
    "doi": "10.1002/ggge.20071", 
    "link": "http://arxiv.org/pdf/1203.1034v1", 
    "title": "General Complex Polynomial Root Solver and Its Further Optimization for   Binary Microlenses", 
    "arxiv-id": "1203.1034v1", 
    "author": "A. Gould", 
    "publish": "2012-03-05T21:00:01Z", 
    "summary": "We present a new algorithm to solve polynomial equations, and publish its\ncode, which is 1.6-3 times faster than the ZROOTS subroutine that is\ncommercially available from Numerical Recipes, depending on application. The\nlargest improvement, when compared to naive solvers, comes from a fail-safe\nprocedure that permits us to skip the majority of the calculations in the great\nmajority of cases, without risking catastrophic failure in the few cases that\nthese are actually required. Second, we identify a discriminant that enables a\nrational choice between Laguerre's Method and Newton's Method (or a new\nintermediate method) on a case-by-case basis. We briefly review the history of\nroot solving and demonstrate that \"Newton's Method\" was discovered neither by\nNewton (1671) nor by Raphson (1690), but only by Simpson (1740). Some of the\narguments leading to this conclusion were first given by the British historian\nof science Nick Kollerstrom in 1992, but these do not appear to have penetrated\nthe astronomical community. Finally, we argue that Numerical Recipes should\nvoluntarily surrender its copyright protection for non-profit applications,\ndespite the fact that, in this particular case, such protection was the major\nstimulant for developing our improved algorithm."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2012.12.010", 
    "link": "http://arxiv.org/pdf/1203.1263v2", 
    "title": "NLSEmagic: Nonlinear Schr\u00f6dinger Equation Multidimensional   Matlab-based GPU-accelerated Integrators using Compact High-order Schemes", 
    "arxiv-id": "1203.1263v2", 
    "author": "R. M. Caplan", 
    "publish": "2012-03-06T17:45:07Z", 
    "summary": "We present a simple to use, yet powerful code package called NLSEmagic to\nnumerically integrate the nonlinear Schr\u007f\\\"odinger equation in one, two, and\nthree dimensions. NLSEmagic is a high-order finite-difference code package\nwhich utilizes graphic processing unit (GPU) parallel architectures. The codes\nrunning on the GPU are many times faster than their serial counterparts, and\nare much cheaper to run than on standard parallel clusters. The codes are\ndeveloped with usability and portability in mind, and therefore are written to\ninterface with MATLAB utilizing custom GPU-enabled C codes with the\nMEX-compiler interface. The packages are freely distributed, including user\nmanuals and set-up files."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cpc.2012.12.010", 
    "link": "http://arxiv.org/pdf/1203.1448v2", 
    "title": "AD in Fortran, Part 1: Design", 
    "arxiv-id": "1203.1448v2", 
    "author": "Jeffrey Mark Siskind", 
    "publish": "2012-03-07T12:04:05Z", 
    "summary": "We propose extensions to Fortran which integrate forward and reverse\nAutomatic Differentiation (AD) directly into the programming model.\nIrrespective of implementation technology, embedding AD constructs directly\ninto the language extends the reach and convenience of AD while allowing\nabstraction of concepts of interest to scientific-computing practice, such as\nroot finding, optimization, and finding equilibria of continuous games.\nMultiple different subprograms for these tasks can share common interfaces,\nregardless of whether and how they use AD internally. A programmer can maximize\na function F by calling a library maximizer, XSTAR=ARGMAX(F,X0), which\ninternally constructs derivatives of F by AD, without having to learn how to\nuse any particular AD tool. We illustrate the utility of these extensions by\nexample: programs become much more concise and closer to traditional\nmathematical notation. A companion paper describes how these extensions can be\nimplemented by a program that generates input to existing Fortran-based AD\ntools."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-30023-3_25", 
    "link": "http://arxiv.org/pdf/1203.1450v2", 
    "title": "AD in Fortran, Part 2: Implementation via Prepreprocessor", 
    "arxiv-id": "1203.1450v2", 
    "author": "Jeffrey Mark Siskind", 
    "publish": "2012-03-07T12:16:30Z", 
    "summary": "We describe an implementation of the Farfel Fortran AD extensions. These\nextensions integrate forward and reverse AD directly into the programming\nmodel, with attendant benefits to flexibility, modularity, and ease of use. The\nimplementation we describe is a \"prepreprocessor\" that generates input to\nexisting Fortran-based AD tools. In essence, blocks of code which are targeted\nfor AD by Farfel constructs are put into subprograms which capture their\nlexical variable context, and these are closure-converted into top-level\nsubprograms and specialized to eliminate EXTERNAL arguments, rendering them\namenable to existing AD preprocessors, which are then invoked, possibly\nrepeatedly if the AD is nested."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-30023-3_25", 
    "link": "http://arxiv.org/pdf/1203.1692v5", 
    "title": "An Optimized Sparse Approximate Matrix Multiply for Matrices with Decay", 
    "arxiv-id": "1203.1692v5", 
    "author": "Matt Challacombe", 
    "publish": "2012-03-08T05:33:01Z", 
    "summary": "We present an optimized single-precision implementation of the Sparse\nApproximate Matrix Multiply (\\SpAMM{}) [M. Challacombe and N. Bock, arXiv {\\bf\n1011.3534} (2010)], a fast algorithm for matrix-matrix multiplication for\nmatrices with decay that achieves an $\\mathcal{O} (n \\log n)$ computational\ncomplexity with respect to matrix dimension $n$. We find that the max norm of\nthe error achieved with a \\SpAMM{} tolerance below $2 \\times 10^{-8}$ is lower\nthan that of the single-precision {\\tt SGEMM} for dense quantum chemical\nmatrices, while outperforming {\\tt SGEMM} with a cross-over already for small\nmatrices ($n \\sim 1000$). Relative to naive implementations of \\SpAMM{} using\nIntel's Math Kernel Library ({\\tt MKL}) or AMD's Core Math Library ({\\tt\nACML}), our optimized version is found to be significantly faster. Detailed\nperformance comparisons are made for quantum chemical matrices with differently\nstructured sub-blocks. Finally, we discuss the potential of improved hardware\nprefetch to yield 2--3x speedups."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-30023-3_25", 
    "link": "http://arxiv.org/pdf/1203.3059v1", 
    "title": "Set Reduction In Nonlinear Equations", 
    "arxiv-id": "1203.3059v1", 
    "author": "Ali Ecder", 
    "publish": "2012-03-14T12:00:08Z", 
    "summary": "In this paper, an idea to solve nonlinear equations is presented. During the\nsolution of any problem with Newton's Method, it might happen that some of the\nunknowns satisfy the convergence criteria where the others fail. The\nconvergence happens only when all variables reach to the convergence limit. A\nmethod to reduce the dimension of the overall system by excluding some of the\nunknowns that satisfy an intermediate tolerance is introduced. In this\napproach, a smaller system is solved in less amount of time and already\nestablished local solutions are preserved and kept as constants while the other\nvariables that belong to the \"set\" will be relaxed. To realize the idea, an\nalgorithm is given that utilizes applications of pointers to reduce and\nevaluate the sets. Matrix-free Newton-Krylov Techniques are used on a test\nproblem and it is shown that proposed idea improves the overall convergence."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-30023-3_25", 
    "link": "http://arxiv.org/pdf/1203.4031v3", 
    "title": "FEAST Eigenvalue Solver v3.0 User Guide", 
    "arxiv-id": "1203.4031v3", 
    "author": "James Kestyn", 
    "publish": "2012-03-19T03:47:55Z", 
    "summary": "The FEAST eigensolver package is a free high-performance numerical library\nfor solving the Hermitian and non-Hermitian eigenvalue problems, and obtaining\nall the eigenvalues and (right/left) eigenvectors within a given search\ninterval or arbitrary contour in the complex plane. Its originality lies with a\nnew transformative numerical approach to the traditional eigenvalue algorithm\ndesign - the FEAST algorithm. The FEAST eigensolver combines simplicity and\nefficiency and it offers many important capabilities for achieving high\nperformance, robustness, accuracy, and scalability on parallel architectures.\nFEAST is both a comprehensive library package, and an easy to use software. It\nincludes flexible reverse communication interfaces and ready to use predefined\ninterfaces for dense, banded and sparse systems. The current version v3.0 of\nthe FEAST package can address both Hermitian and non-Hermitian eigenvalue\nproblems (real symmetric, real non-symmetric, complex Hermitian, complex\nsymmetric, or complex general systems) on both shared-memory and distributed\nmemory architectures (i.e contains both FEAST-SMP and FEAST-MPI packages). This\nUser's guide provides instructions for installation setup, a detailed\ndescription of the FEAST interfaces and a large number of examples."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-30023-3_25", 
    "link": "http://arxiv.org/pdf/1205.1098v1", 
    "title": "Reliable Generation of High-Performance Matrix Algebra", 
    "arxiv-id": "1205.1098v1", 
    "author": "Jeremy G. Siek", 
    "publish": "2012-05-05T04:30:14Z", 
    "summary": "Scientific programmers often turn to vendor-tuned Basic Linear Algebra\nSubprograms (BLAS) to obtain portable high performance. However, many numerical\nalgorithms require several BLAS calls in sequence, and those successive calls\nresult in suboptimal performance. The entire sequence needs to be optimized in\nconcert. Instead of vendor-tuned BLAS, a programmer could start with source\ncode in Fortran or C (e.g., based on the Netlib BLAS) and use a\nstate-of-the-art optimizing compiler. However, our experiments show that\noptimizing compilers often attain only one-quarter the performance of\nhand-optimized code. In this paper we present a domain-specific compiler for\nmatrix algebra, the Build to Order BLAS (BTO), that reliably achieves high\nperformance using a scalable search algorithm for choosing the best combination\nof loop fusion, array contraction, and multithreading for data parallelism. The\nBTO compiler generates code that is between 16% slower and 39% faster than\nhand-optimized code."
},{
    "category": "cs.NA", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1205.2129v2", 
    "title": "Isogeometric analysis: an overview and computer implementation aspects", 
    "arxiv-id": "1205.2129v2", 
    "author": "Timon Rabczuk", 
    "publish": "2012-05-10T00:48:08Z", 
    "summary": "Isogeometric analysis (IGA) represents a recently developed technology in\ncomputational mechanics that offers the possibility of integrating methods for\nanalysis and Computer Aided Design (CAD) into a single, unified process. The\nimplications to practical engineering design scenarios are profound, since the\ntime taken from design to analysis is greatly reduced, leading to dramatic\ngains in efficiency. The tight coupling of CAD and analysis within IGA requires\nknowledge from both fields and it is one of the goals of the present paper to\noutline much of the commonly used notation. In this manuscript, through a clear\nand simple Matlab implementation, we present an introduction to IGA applied to\nthe Finite Element (FE) method and related computer implementation aspects.\nFurthermore, implemen- tation of the extended IGA which incorporates enrichment\nfunctions through the partition of unity method (PUM) is also presented, where\nseveral examples for both two-dimensional and three-dimensional fracture are\nillustrated. The open source Matlab code which accompanies the present paper\ncan be applied to one, two and three-dimensional problems for linear\nelasticity, linear elastic fracture mechanics, structural mechanics\n(beams/plates/shells including large displacements and rotations) and Poisson\nproblems with or without enrichment. The Bezier extraction concept that allows\nFE analysis to be performed efficiently on T-spline geometries is also\nincorporated. The article includes a summary of recent trends and developments\nwithin the field of IGA."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1205.5975v1", 
    "title": "A Domain-Specific Compiler for Linear Algebra Operations", 
    "arxiv-id": "1205.5975v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-05-27T15:22:23Z", 
    "summary": "We present a prototypical linear algebra compiler that automatically exploits\ndomain-specific knowledge to generate high-performance algorithms. The input to\nthe compiler is a target equation together with knowledge of both the structure\nof the problem and the properties of the operands. The output is a variety of\nhigh-performance algorithms, and the corresponding source code, to solve the\ntarget equation. Our approach consists in the decomposition of the input\nequation into a sequence of library-supported kernels. Since in general such a\ndecomposition is not unique, our compiler returns not one but a number of\nalgorithms. The potential of the compiler is shown by means of its application\nto a challenging equation arising within the genome-wide association study. As\na result, the compiler produces multiple \"best\" algorithms that outperform the\nbest existing libraries."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1206.0111v1", 
    "title": "OpenGM: A C++ Library for Discrete Graphical Models", 
    "arxiv-id": "1206.0111v1", 
    "author": "Joerg H. Kappes", 
    "publish": "2012-06-01T07:36:54Z", 
    "summary": "OpenGM is a C++ template library for defining discrete graphical models and\nperforming inference on these models, using a wide range of state-of-the-art\nalgorithms. No restrictions are imposed on the factor graph to allow for\nhigher-order factors and arbitrary neighborhood structures. Large models with\nrepetitive structure are handled efficiently because (i) functions that occur\nrepeatedly need to be stored only once, and (ii) distinct functions can be\nimplemented differently, using different encodings alongside each other in the\nsame model. Several parametric functions (e.g. metrics), sparse and dense value\ntables are provided and so is an interface for custom C++ code. Algorithms are\nseparated by design from the representation of graphical models and are easily\nexchangeable. OpenGM, its algorithms, HDF5 file format and command line tools\nare modular and extendible."
},{
    "category": "stat.ML", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1206.5754v6", 
    "title": "Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox", 
    "arxiv-id": "1206.5754v6", 
    "author": "Aki Vehtari", 
    "publish": "2012-06-25T18:19:45Z", 
    "summary": "Gaussian processes (GP) are powerful tools for probabilistic modeling\npurposes. They can be used to define prior distributions over latent functions\nin hierarchical Bayesian models. The prior over functions is defined implicitly\nby the mean and covariance function, which determine the smoothness and\nvariability of the function. The inference can then be conducted directly in\nthe function space by evaluating or approximating the posterior process.\nDespite their attractive theoretical properties GPs provide practical\nchallenges in their implementation. GPstuff is a versatile collection of\ncomputational tools for GP models compatible with Linux and Windows MATLAB and\nOctave. It includes, among others, various inference methods, sparse\napproximations and tools for model assessment. In this work, we review these\ntools and demonstrate the use of GPstuff in several models."
},{
    "category": "cs.NE", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1206.6466v1", 
    "title": "Utilizing Static Analysis and Code Generation to Accelerate Neural   Networks", 
    "arxiv-id": "1206.6466v1", 
    "author": "Kunle Olukotun", 
    "publish": "2012-06-27T19:59:59Z", 
    "summary": "As datasets continue to grow, neural network (NN) applications are becoming\nincreasingly limited by both the amount of available computational power and\nthe ease of developing high-performance applications. Researchers often must\nhave expert systems knowledge to make their algorithms run efficiently.\nAlthough available computing power increases rapidly each year, algorithm\nefficiency is not able to keep pace due to the use of general purpose\ncompilers, which are not able to fully optimize specialized application\ndomains. Within the domain of NNs, we have the added knowledge that network\narchitecture remains constant during training, meaning the architecture's data\nstructure can be statically optimized by a compiler. In this paper, we present\nSONNC, a compiler for NNs that utilizes static analysis to generate optimized\nparallel code. We show that SONNC's use of static optimizations make it able to\noutperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X.\nAdditionally, we show that use of SONNC significantly reduces code complexity\nwhen using structurally sparse networks."
},{
    "category": "math.GT", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1208.2504v2", 
    "title": "Computational topology with Regina: Algorithms, heuristics and   implementations", 
    "arxiv-id": "1208.2504v2", 
    "author": "Benjamin A. Burton", 
    "publish": "2012-08-13T06:22:37Z", 
    "summary": "Regina is a software package for studying 3-manifold triangulations and\nnormal surfaces. It includes a graphical user interface and Python bindings,\nand also supports angle structures, census enumeration, combinatorial\nrecognition of triangulations, and high-level functions such as 3-sphere\nrecognition, unknot recognition and connected sum decomposition.\n  This paper brings 3-manifold topologists up-to-date with Regina as it appears\ntoday, and documents for the first time in the literature some of the key\nalgorithms, heuristics and implementations that are central to Regina's\nperformance. These include the all-important simplification heuristics, key\nchoices of data structures and algorithms to alleviate bottlenecks in normal\nsurface enumeration, modern implementations of 3-sphere recognition and\nconnected sum decomposition, and more. We also give some historical background\nfor the project, including the key role played by Rubinstein in its genesis 15\nyears ago, and discuss current directions for future development."
},{
    "category": "physics.chem-ph", 
    "doi": "10.1016/j.matcom.2015.05.008", 
    "link": "http://arxiv.org/pdf/1208.3866v2", 
    "title": "Analytical Nonlocal Electrostatics Using Eigenfunction Expansions of   Boundary-Integral Operators", 
    "arxiv-id": "1208.3866v2", 
    "author": "Peter R. Brune", 
    "publish": "2012-08-19T16:56:04Z", 
    "summary": "In this paper, we present an analytical solution to nonlocal continuum\nelectrostatics for an arbitrary charge distribution in a spherical solute. Our\napproach relies on two key steps: (1) re-formulating the PDE problem using\nboundary-integral equations, and (2) diagonalizing the boundary-integral\noperators using the fact their eigenfunctions are the surface spherical\nharmonics. To introduce this uncommon approach for analytical calculations in\nseparable geometries, we rederive Kirkwood's classic results for a protein\nsurrounded concentrically by a pure-water ion-exclusion layer and then a dilute\nelectrolyte (modeled with the linearized Poisson--Boltzmann equation). Our main\nresult, however, is an analytical method for calculating the reaction potential\nin a protein embedded in a nonlocal-dielectric solvent, the Lorentz model\nstudied by Dogonadze and Kornyshev. The analytical method enables biophysicists\nto study the new nonlocal theory in a simple, computationally fast way; an\nopen-source MATLAB implementation is included as supplemental information."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2012.08.012", 
    "link": "http://arxiv.org/pdf/1208.3970v1", 
    "title": "Employing online quantum random number generators for generating truly   random quantum states in Mathematica", 
    "arxiv-id": "1208.3970v1", 
    "author": "J. A. Miszczak", 
    "publish": "2012-08-20T10:56:41Z", 
    "summary": "We present a new version of TRQS package for Mathematica computing system.\nThe package allows harnessing quantum random number generators (QRNG) for\ninvestigating the statistical properties of quantum states. It implements a\nnumber of functions for generating random states. The new version of the\npackage adds the ability to use the on-line quantum random number generator\nservice and implements new functions for retrieving lists of random numbers.\nThanks to the introduced improvements, the new version provides faster access\nto high-quality sources of random numbers and can be used in simulations\nrequiring large amount of random data."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0129183112500957", 
    "link": "http://arxiv.org/pdf/1208.4721v1", 
    "title": "Hamilton Operators, Discrete Symmetries, Brute Force and SymbolicC++", 
    "arxiv-id": "1208.4721v1", 
    "author": "Yorick Hardy", 
    "publish": "2012-08-23T11:09:57Z", 
    "summary": "To find the discrete symmetries of a Hamilton operator $\\hat H$ is of central\nimportance in quantum theory. Here we describe and implement a brute force\nmethod to determine the discrete symmetries given by permutation matrices for\nHamilton operators acting in a finite-dimensional Hilbert space. Spin and Fermi\nsystems are considered as examples. A computer algebra implementation in\nSymbolicC++ is provided."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2012.07.008", 
    "link": "http://arxiv.org/pdf/1209.0735v1", 
    "title": "Lambert W Function for Applications in Physics", 
    "arxiv-id": "1209.0735v1", 
    "author": "Darko Veberic", 
    "publish": "2012-08-31T21:07:48Z", 
    "summary": "The Lambert W(x) function and its possible applications in physics are\npresented. The actual numerical implementation in C++ consists of Halley's and\nFritsch's iterations with initial approximations based on branch-point\nexpansion, asymptotic series, rational fits, and continued-logarithm recursion."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2012.07.008", 
    "link": "http://arxiv.org/pdf/1209.0960v2", 
    "title": "A Massively Parallel Algebraic Multigrid Preconditioner based on   Aggregation for Elliptic Problems with Heterogeneous Coefficients", 
    "arxiv-id": "1209.0960v2", 
    "author": "Peter Bastian", 
    "publish": "2012-09-05T13:07:36Z", 
    "summary": "This paper describes a massively parallel algebraic multigrid method based on\nnon-smoothed aggregation. It is especially suited for solving heterogeneous\nelliptic problems as it uses a greedy heuristic algorithm for the aggregation\nthat detects changes in the coefficients and prevents aggregation across them.\nUsing decoupled aggregation on each process with data agglomeration onto fewer\nprocesses on the coarse level, it weakly scales well in terms of both total\ntime to solution and time per iteration to nearly 300,000 cores. Because of\nsimple piecewise constant interpolation between the levels, its memory\nconsumption is low and allows solving problems with more than 100,000,000,000\ndegrees of freedom."
},{
    "category": "cs.PL", 
    "doi": "10.1016/j.cpc.2012.07.008", 
    "link": "http://arxiv.org/pdf/1209.1711v1", 
    "title": "Programming Languages for Scientific Computing", 
    "arxiv-id": "1209.1711v1", 
    "author": "Matthew G. Knepley", 
    "publish": "2012-09-08T12:31:50Z", 
    "summary": "Scientific computation is a discipline that combines numerical analysis,\nphysical understanding, algorithm development, and structured programming.\nSeveral yottacycles per year on the world's largest computers are spent\nsimulating problems as diverse as weather prediction, the properties of\nmaterial composites, the behavior of biomolecules in solution, and the quantum\nnature of chemical compounds. This article is intended to review specfic\nlanguages features and their use in computational science. We will review the\nstrengths and weaknesses of different programming styles, with examples taken\nfrom widely used scientific codes."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2012.07.008", 
    "link": "http://arxiv.org/pdf/1209.2364v2", 
    "title": "Performance Modeling for Dense Linear Algebra", 
    "arxiv-id": "1209.2364v2", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-09-11T16:37:20Z", 
    "summary": "It is well known that the behavior of dense linear algebra algorithms is\ngreatly influenced by factors like target architecture, underlying libraries\nand even problem size; because of this, the accurate prediction of their\nperformance is a real challenge. In this article, we are not interested in\ncreating accurate models for a given algorithm, but in correctly ranking a set\nof equivalent algorithms according to their performance. Aware of the\nhierarchical structure of dense linear algebra routines, we approach the\nproblem by developing a framework for the automatic generation of statistical\nperformance models for BLAS and LAPACK libraries. This allows us to obtain\npredictions through evaluating and combining such models. We demonstrate that\nour approach is successful in both single- and multi-core environments, not\nonly in the ranking of algorithms but also in tuning their parameters."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2012.07.008", 
    "link": "http://arxiv.org/pdf/1210.0800v2", 
    "title": "Orthogononalization on a general purpose graphics processing unit with   double double and quad double arithmetic", 
    "arxiv-id": "1210.0800v2", 
    "author": "Genady Yoffe", 
    "publish": "2012-10-02T15:08:27Z", 
    "summary": "Our problem is to accurately solve linear systems on a general purpose\ngraphics processing unit with double double and quad double arithmetic. The\nlinear systems originate from the application of Newton's method on polynomial\nsystems. Newton's method is applied as a corrector in a path following method,\nso the linear systems are solved in sequence and not simultaneously. One\nsolution path may require the solution of thousands of linear systems. In\nprevious work we reported good speedups with our implementation to evaluate and\ndifferentiate polynomial systems on the NVIDIA Tesla C2050. Although the cost\nof evaluation and differentiation often dominates the cost of linear system\nsolving in Newton's method, because of the limited bandwidth of the\ncommunication between CPU and GPU, we cannot afford to send the linear system\nto the CPU for solving during path tracking.\n  Because of large degrees, the Jacobian matrix may contain extreme values,\nrequiring extended precision, leading to a significant overhead. This overhead\nof multiprecision arithmetic is our main motivation to develop a massively\nparallel algorithm. To allow overdetermined linear systems we solve linear\nsystems in the least squares sense, computing the QR decomposition of the\nmatrix by the modified Gram-Schmidt algorithm. We describe our implementation\nof the modified Gram-Schmidt orthogonalization method for the NVIDIA Tesla\nC2050, using double double and quad double arithmetic. Our experimental results\nshow that the achieved speedups are sufficiently high to compensate for the\noverhead of one extra level of precision."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1210.2950v1", 
    "title": "Symbolic Analysis for Boundary Problems: From Rewriting to Parametrized   Gr\u00f6bner Bases", 
    "arxiv-id": "1210.2950v1", 
    "author": "Bruno Buchberger", 
    "publish": "2012-10-10T15:11:49Z", 
    "summary": "We review our algebraic framework for linear boundary problems (concentrating\non ordinary differential equations). Its starting point is an appropriate\nalgebraization of the domain of functions, which we have named\nintegro-differential algebras. The algebraic treatment of boundary problems\nbrings up two new algebraic structures whose symbolic representation and\ncomputational realization is based on canonical forms in certain commutative\nand noncommutative polynomial domains. The first of these, the ring of\nintegro-differential operators, is used for both stating and solving linear\nboundary problems. The other structure, called integro-differential\npolynomials, is the key tool for describing extensions of integro-differential\nalgebras. We use the canonical simplifier for integro-differential polynomials\nfor generating an automated proof establishing a canonical simplifier for\nintegro-differential operators. Our approach is fully implemented in the\nTheorema system; some code fragments and sample computations are included."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1210.6293v1", 
    "title": "MLPACK: A Scalable C++ Machine Learning Library", 
    "arxiv-id": "1210.6293v1", 
    "author": "Alexander G. Gray", 
    "publish": "2012-10-23T17:15:03Z", 
    "summary": "MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning\nlibrary released in late 2011 offering both a simple, consistent API accessible\nto novice users and high performance and flexibility to expert users by\nleveraging modern features of C++. MLPACK provides cutting-edge algorithms\nwhose benchmarks exhibit far better performance than other leading machine\nlearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available\nat http://www.mlpack.org."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1210.7292v2", 
    "title": "Optimized M2L Kernels for the Chebyshev Interpolation based Fast   Multipole Method", 
    "arxiv-id": "1210.7292v2", 
    "author": "Eric Darve", 
    "publish": "2012-10-27T05:46:13Z", 
    "summary": "A fast multipole method (FMM) for asymptotically smooth kernel functions\n(1/r, 1/r^4, Gauss and Stokes kernels, radial basis functions, etc.) based on a\nChebyshev interpolation scheme has been introduced in [Fong et al., 2009]. The\nmethod has been extended to oscillatory kernels (e.g., Helmholtz kernel) in\n[Messner et al., 2012]. Beside its generality this FMM turns out to be\nfavorable due to its easy implementation and its high performance based on\nintensive use of highly optimized BLAS libraries. However, one of its\nbottlenecks is the precomputation of the multiple-to-local (M2L) operator, and\nits higher number of floating point operations (flops) compared to other FMM\nformulations. Here, we present several optimizations for that operator, which\nis known to be the costliest FMM operator. The most efficient ones do not only\nreduce the precomputation time by a factor up to 340 but they also speed up the\nmatrix-vector product. We conclude with comparisons and numerical validations\nof all presented optimizations."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1210.7325v1", 
    "title": "Solving Sequences of Generalized Least-Squares Problems on   Multi-threaded Architectures", 
    "arxiv-id": "1210.7325v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-10-27T14:26:32Z", 
    "summary": "Generalized linear mixed-effects models in the context of genome-wide\nassociation studies (GWAS) represent a formidable computational challenge: the\nsolution of millions of correlated generalized least-squares problems, and the\nprocessing of terabytes of data. We present high performance in-core and\nout-of-core shared-memory algorithms for GWAS: By taking advantage of\ndomain-specific knowledge, exploiting multi-core parallelism, and handling data\nefficiently, our algorithms attain unequalled performance. When compared to\nGenABEL, one of the most widely used libraries for GWAS, on a 12-core processor\nwe obtain 50-fold speedups. As a consequence, our routines enable genome\nstudies of unprecedented size."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1210.8418v1", 
    "title": "Demonstrating the Usefulness of CAELinux for Computer Aided Engineering   using an Example of the Three Dimensional Reconstruction of a Pig Liver", 
    "arxiv-id": "1210.8418v1", 
    "author": "Kirana Kumara P.", 
    "publish": "2012-10-26T03:14:03Z", 
    "summary": "CAELinux is a Linux distribution which is bundled with free software packages\nrelated to Computer Aided Engineering (CAE). The free software packages include\nsoftware that can build a three dimensional solid model, programs that can mesh\na geometry, software for carrying out Finite Element Analysis (FEA), programs\nthat can carry out image processing etc. Present work has two goals: 1) To give\na brief description of CAELinux 2) To demonstrate that CAELinux could be useful\nfor Computer Aided Engineering, using an example of the three dimensional\nreconstruction of a pig liver from a stack of CT-scan images. One can note that\ninstead of using CAELinux, using commercial software for reconstructing the\nliver would cost a lot of money. One can also note that CAELinux is a free and\nopen source operating system and all software packages that are included in the\noperating system are also free. Hence one can conclude that CAELinux could be a\nvery useful tool in application areas like surgical simulation which require\nthree dimensional reconstructions of biological organs. Also, one can see that\nCAELinux could be a very useful tool for Computer Aided Engineering, in\ngeneral."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.3056v2", 
    "title": "GPU-accelerated generation of correctly-rounded elementary functions", 
    "arxiv-id": "1211.3056v2", 
    "author": "Stef Graillat", 
    "publish": "2012-11-13T17:28:03Z", 
    "summary": "The IEEE 754-2008 standard recommends the correct rounding of some elementary\nfunctions. This requires to solve the Table Maker's Dilemma which implies a\nhuge amount of CPU computation time. We consider in this paper accelerating\nsuch computations, namely Lefe'vre algorithm on Graphics Processing Units\n(GPUs) which are massively parallel architectures with a partial SIMD execution\n(Single Instruction Multiple Data). We first propose an analysis of the\nLef\\`evre hard-to-round argument search using the concept of continued\nfractions. We then propose a new parallel search algorithm much more efficient\non GPU thanks to its more regular control flow. We also present an efficient\nhybrid CPU-GPU deployment of the generation of the polynomial approximations\nrequired in Lef\\`evre algorithm. In the end, we manage to obtain overall\nspeedups up to 53.4x on one GPU over a sequential CPU execution, and up to 7.1x\nover a multi-core CPU, which enable a much faster solving of the Table Maker's\nDilemma for the double precision format."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.3567v1", 
    "title": "A Bernstein Polynomial Collocation Method for the Solution of Elliptic   Boundary Value Problems", 
    "arxiv-id": "1211.3567v1", 
    "author": "Bosko Rasuo", 
    "publish": "2012-11-15T10:38:46Z", 
    "summary": "In this article, a formulation of a point-collocation method in which the\nunknown function is approximated using global expansion in tensor product\nBernstein polynomial basis is presented. Bernstein polynomials used in this\nstudy are defined over general interval [a,b]. Method incorporates several\nideas that enable higher numerical efficiency compared to Bernstein polynomial\nmethods that have been previously presented. The approach is illustrated by a\nsolution of Poisson, Helmholtz and Biharmonic equations with Dirichlet and\nNeumann type boundary conditions. Comparisons with analytical solutions are\ngiven to demonstrate the accuracy and convergence properties of the current\nprocedure. The method is implemented in an open-source code, and a library for\nmanipulation of Bernstein polynomials bernstein-poly, developed by the authors."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.4047v2", 
    "title": "Unified Form Language: A domain-specific language for weak formulations   of partial differential equations", 
    "arxiv-id": "1211.4047v2", 
    "author": "Garth N. Wells", 
    "publish": "2012-11-16T21:56:02Z", 
    "summary": "We present the Unified Form Language (UFL), which is a domain-specific\nlanguage for representing weak formulations of partial differential equations\nwith a view to numerical approximation. Features of UFL include support for\nvariational forms and functionals, automatic differentiation of forms and\nexpressions, arbitrary function space hierarchies for multi-field problems,\ngeneral differential operators and flexible tensor algebra. With these\nfeatures, UFL has been used to effortlessly express finite element methods for\ncomplex systems of partial differential equations in near-mathematical\nnotation, resulting in compact, intuitive and readable programs. We present in\nthis work the language and its construction. An implementation of UFL is freely\navailable as an open-source software library. The library generates abstract\nsyntax tree representations of variational problems, which are used by other\nsoftware libraries to generate concrete low-level implementations. Some\napplication examples are presented and libraries that support UFL are\nhighlighted."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.4892v1", 
    "title": "Confusion of Tagged Perturbations in Forward Automatic Differentiation   of Higher-Order Functions", 
    "arxiv-id": "1211.4892v1", 
    "author": "Jeffrey Mark Siskind", 
    "publish": "2012-11-20T22:08:31Z", 
    "summary": "Forward Automatic Differentiation (AD) is a technique for augmenting programs\nto both perform their original calculation and also compute its directional\nderivative. The essence of Forward AD is to attach a derivative value to each\nnumber, and propagate these through the computation. When derivatives are\nnested, the distinct derivative calculations, and their associated attached\nvalues, must be distinguished. In dynamic languages this is typically\naccomplished by creating a unique tag for each application of the derivative\noperator, tagging the attached values, and overloading the arithmetic\noperators. We exhibit a subtle bug, present in fielded implementations, in\nwhich perturbations are confused despite the tagging machinery."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.5904v1", 
    "title": "Application-tailored Linear Algebra Algorithms: A search-based Approach", 
    "arxiv-id": "1211.5904v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-11-26T10:15:29Z", 
    "summary": "In this paper, we tackle the problem of automatically generating algorithms\nfor linear algebra operations by taking advantage of problem-specific\nknowledge. In most situations, users possess much more information about the\nproblem at hand than what current libraries and computing environments accept;\nevidence shows that if properly exploited, such information leads to\nuncommon/unexpected speedups. We introduce a knowledge-aware linear algebra\ncompiler that allows users to input matrix equations together with properties\nabout the operands and the problem itself; for instance, they can specify that\nthe equation is part of a sequence, and how successive instances are related to\none another. The compiler exploits all this information to guide the generation\nof algorithms, to limit the size of the search space, and to avoid redundant\ncomputations. We applied the compiler to equations arising as part of\nsensitivity and genome studies; the algorithms produced exhibit, respectively,\n100- and 1000-fold speedups."
},{
    "category": "math.CO", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1211.7110v1", 
    "title": "Algorithms for discovering and proving theorems about permutation   patterns", 
    "arxiv-id": "1211.7110v1", 
    "author": "Henning Ulfarsson", 
    "publish": "2012-11-29T22:38:57Z", 
    "summary": "We present an algorithm, called BiSC, that describes the patterns avoided by\na given set of permutations. It automatically conjectures the statements of\nknown theorems such as the descriptions of stack-sortable (Knuth 1975) and\nWest-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya\n1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun\npermutations (Branden and Claesson 2011). The algorithm has also been used to\ndiscover new theorems and conjectures related to Young tableaux,\nWilf-equivalences and sorting devices. We further give algorithms to prove a\ncomplete description of preimages of pattern classes under certain sorting\ndevices. These generalize an algorithm of Claesson and Ulfarsson (2012) and\nallow us to prove a linear time algorithm for finding occurrences of the\npattern 4312."
},{
    "category": "cs.IT", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1212.0520v1", 
    "title": "A modular framework for randomness extraction based on Trevisan's   construction", 
    "arxiv-id": "1212.0520v1", 
    "author": "Volkher B. Scholz", 
    "publish": "2012-12-03T20:20:02Z", 
    "summary": "Informally, an extractor delivers perfect randomness from a source that may\nbe far away from the uniform distribution, yet contains some randomness. This\ntask is a crucial ingredient of any attempt to produce perfectly random\nnumbers---required, for instance, by cryptographic protocols, numerical\nsimulations, or randomised computations. Trevisan's extractor raised\nconsiderable theoretical interest not only because of its data parsimony\ncompared to other constructions, but particularly because it is secure against\nquantum adversaries, making it applicable to quantum key distribution.\n  We discuss a modular, extensible and high-performance implementation of the\nconstruction based on various building blocks that can be flexibly combined to\nsatisfy the requirements of a wide range of scenarios. Besides quantitatively\nanalysing the properties of many combinations in practical settings, we improve\nprevious theoretical proofs, and give explicit results for non-asymptotic\ncases. The self-contained description does not assume familiarity with\nextractors."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1212.2350v1", 
    "title": "Automated verification of termination certificates", 
    "arxiv-id": "1212.2350v1", 
    "author": "Kim Quyen Ly", 
    "publish": "2012-12-11T09:24:46Z", 
    "summary": "In order to increase user confidence, many automated theorem provers provide\ncertificates that can be independently verified. In this paper, we report on\nour progress in developing a standalone tool for checking the correctness of\ncertificates for the termination of term rewrite systems, and formally proving\nits correctness in the proof assistant Coq. To this end, we use the extraction\nmechanism of Coq and the library on rewriting theory and termination called\nCoLoR."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1212.6326v2", 
    "title": "Programming CUDA and OpenCL: A Case Study Using Modern C++ Libraries", 
    "arxiv-id": "1212.6326v2", 
    "author": "Peter Gottschling", 
    "publish": "2012-12-27T08:56:00Z", 
    "summary": "We present a comparison of several modern C++ libraries providing high-level\ninterfaces for programming multi- and many-core architectures on top of CUDA or\nOpenCL. The comparison focuses on the solution of ordinary differential\nequations and is based on odeint, a framework for the solution of systems of\nordinary differential equations. Odeint is designed in a very flexible way and\nmay be easily adapted for effective use of libraries such as Thrust, MTL4,\nVexCL, or ViennaCL, using CUDA or OpenCL technologies. We found that CUDA and\nOpenCL work equally well for problems of large sizes, while OpenCL has higher\noverhead for smaller problems. Furthermore, we show that modern high-level\nlibraries allow to effectively use the computational resources of many-core\nGPUs or multi-core CPUs without much knowledge of the underlying technologies."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1302.0432v4", 
    "title": "FEAST as a Subspace Iteration Eigensolver Accelerated by Approximate   Spectral Projection", 
    "arxiv-id": "1302.0432v4", 
    "author": "Eric Polizzi", 
    "publish": "2013-02-02T22:30:20Z", 
    "summary": "The calculation of a segment of eigenvalues and their corresponding\neigenvectors of a Hermitian matrix or matrix pencil has many applications. A\nnew density-matrix-based algorithm has been proposed recently and a software\npackage FEAST has been developed. The density-matrix approach allows FEAST's\nimplementation to exploit a key strength of modern computer architectures,\nnamely, multiple levels of parallelism. Consequently, the software package has\nbeen well received, especially in the electronic structure community.\nNevertheless, theoretical analysis of FEAST has lagged. For instance, the FEAST\nalgorithm has not been proven to converge. This paper offers a detailed\nnumerical analysis of FEAST. In particular, we show that the FEAST algorithm\ncan be understood as an accelerated subspace iteration algorithm in conjunction\nwith the Rayleigh-Ritz procedure. The novelty of FEAST lies in its accelerator\nwhich is a rational matrix function that approximates the spectral projector\nonto the eigenspace in question. Analysis of the numerical nature of this\napproximate spectral projector and the resulting subspaces generated in the\nFEAST algorithm establishes the algorithm's convergence. This paper shows that\nFEAST is resilient against rounding errors and establishes properties that can\nbe leveraged to enhance the algorithm's robustness. Finally, we propose an\nextension of FEAST to handle non-Hermitian problems and suggest some future\nresearch directions."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1302.1737v1", 
    "title": "Kleene Algebra with Tests and Coq Tools for While Programs", 
    "arxiv-id": "1302.1737v1", 
    "author": "Damien Pous", 
    "publish": "2013-02-07T13:15:45Z", 
    "summary": "We present a Coq library about Kleene algebra with tests, including a proof\nof their completeness over the appropriate notion of languages, a decision\nprocedure for their equational theory, and tools for exploiting hypotheses of a\nparticular shape in such a theory. Kleene algebra with tests make it possible\nto represent if-then-else statements and while loops in most imperative\nprogramming languages. They were actually introduced by Kozen as an alternative\nto propositional Hoare logic. We show how to exploit the corresponding Coq\ntools in the context of program verification by proving equivalences of while\nprograms, correctness of some standard compiler optimisations, Hoare rules for\npartial correctness, and a particularly challenging equivalence of flowchart\nschemes."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1302.2738v2", 
    "title": "RandFile package for Mathematica for accessing file-based sources of   randomness", 
    "arxiv-id": "1302.2738v2", 
    "author": "M. Wahl", 
    "publish": "2013-02-12T09:38:46Z", 
    "summary": "We present a package for Mathematica computer algebra system which allows the\nexploitation of local files as sources of random data. We provide the\ndescription of the package and illustrate its usage by showing some examples.\nWe also compare the provided functionality with alternative sources of\nrandomness, namely a built-in pseudo-random generator and the package for\naccessing hardware true random number generators."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1302.4332v1", 
    "title": "Streaming Data from HDD to GPUs for Sustained Peak Performance", 
    "arxiv-id": "1302.4332v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2013-02-18T16:03:08Z", 
    "summary": "In the context of the genome-wide association studies (GWAS), one has to\nsolve long sequences of generalized least-squares problems; such a task has two\nlimiting factors: execution time --often in the range of days or weeks-- and\ndata management --data sets in the order of Terabytes. We present an algorithm\nthat obviates both issues. By pipelining the computation, and thanks to a\nsophisticated transfer strategy, we stream data from hard disk to main memory\nto GPUs and achieve sustained peak performance; with respect to a\nhighly-optimized CPU implementation, our algorithm shows a speedup of 2.6x.\nMoreover, the approach lends itself to multiple GPUs and attains almost perfect\nscalability. When using 4 GPUs, we observe speedups of 9x over the\naforementioned implementation, and 488x over a widespread biology library."
},{
    "category": "cs.ET", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1302.5133v1", 
    "title": "Q#, a quantum computation package for the .NET platform", 
    "arxiv-id": "1302.5133v1", 
    "author": "M. A. El-Dosuky", 
    "publish": "2013-02-20T21:37:43Z", 
    "summary": "Quantum computing is a promising approach of computation that is based on\nequations from Quantum Mechanics. A simulator for quantum algorithms must be\ncapable of performing heavy mathematical matrix transforms. The design of the\nsimulator itself takes one of three forms: Quantum Turing Machine, Network\nModel or circuit model of connected gates or, Quantum Programming Language,\nyet, some simulators are hybrid. We studied previous simulators and then we\nadopt features from three simulators of different implementation languages,\ndifferent paradigms, and for different platforms. They are Quantum Computing\nLanguage (QCL), QUASI, and Quantum Optics Toolbox for Matlab 5. Our simulator\nfor quantum algorithms takes the form of a package or a programming library for\nQuantum computing, with a case study showing the ability of using it in the\ncircuit model. The .NET is a promising platform for computing. VB.NET is an\neasy, high productive programming language with the full power and\nfunctionality provided by the .NET framework. It is highly readable, writeable,\nand flexible language, compared to another language such as C#.NET in many\naspects. We adopted VB.NET although its shortage in built-in mathematical\ncomplex and matrix operations, compared to Matlab. For implementation, we first\nbuilt a mathematical core of matrix operations. Then, we built a quantum core\nwhich contains: basic qubits and register operations, basic 1D, 2D, and 3D\nquantum gates, and multi-view visualization of the quantum state, then a window\nfor demos to show you how to use and get the most of the package."
},{
    "category": "cs.LG", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.0934v1", 
    "title": "GURLS: a Least Squares Library for Supervised Learning", 
    "arxiv-id": "1303.0934v1", 
    "author": "Lorenzo Rosasco", 
    "publish": "2013-03-05T05:55:59Z", 
    "summary": "We present GURLS, a least squares, modular, easy-to-extend software library\nfor efficient supervised learning. GURLS is targeted to machine learning\npractitioners, as well as non-specialists. It offers a number state-of-the-art\ntraining strategies for medium and large-scale learning, and routines for\nefficient model selection. The library is particularly well suited for\nmulti-output problems (multi-category/multi-label). GURLS is currently\navailable in two independent implementations: Matlab and C++. It takes\nadvantage of the favorable properties of regularized least squares algorithm to\nexploit advanced tools in linear algebra. Routines to handle computations with\nvery large matrices by means of memory-mapped storage and distributed task\nexecution are available. The package is distributed under the BSD licence and\nis available for download at https://github.com/CBCL/GURLS."
},{
    "category": "cs.SE", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.2140v2", 
    "title": "Possible Directions for Improving Dependency Versioning in R", 
    "arxiv-id": "1303.2140v2", 
    "author": "Jeroen Ooms", 
    "publish": "2013-03-08T22:32:22Z", 
    "summary": "One of the most powerful features of R is its infrastructure for contributed\ncode. The built-in package manager and complementary repositories provide a\ngreat system for development and exchange of code, and have played an important\nrole in the growth of the platform towards the de-facto standard in statistical\ncomputing that it is today. However, the number of packages on CRAN and other\nrepositories has increased beyond what might have been foreseen, and is\nrevealing some limitations of the current design. One such problem is the\ngeneral lack of dependency versioning in the infrastructure. This paper\nexplores this problem in greater detail, and suggests approaches taken by other\nopen source communities that might work for R as well. Three use cases are\ndefined that exemplify the issue, and illustrate how improving this aspect of\npackage management could increase reliability while supporting further growth\nof the R community."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.3182v1", 
    "title": "Tiled Algorithms for Matrix Computations on Multicore Architectures", 
    "arxiv-id": "1303.3182v1", 
    "author": "Henricus Bouwmeester", 
    "publish": "2013-03-13T15:04:05Z", 
    "summary": "The current computer architecture has moved towards the multi/many-core\nstructure. However, the algorithms in the current sequential dense numerical\nlinear algebra libraries (e.g. LAPACK) do not parallelize well on\nmulti/many-core architectures. A new family of algorithms, the tile algorithms,\nhas recently been introduced to circumvent this problem. Previous research has\nshown that it is possible to write efficient and scalable tile algorithms for\nperforming a Cholesky factorization, a (pseudo) LU factorization, and a QR\nfactorization. The goal of this thesis is to study tiled algorithms in a\nmulti/many-core setting and to provide new algorithms which exploit the current\narchitecture to improve performance relative to current state-of-the-art\nlibraries while maintaining the stability and robustness of these libraries."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.3761v2", 
    "title": "Update report: LEO-II version 1.5", 
    "arxiv-id": "1303.3761v2", 
    "author": "Nik Sultana", 
    "publish": "2013-03-15T13:03:40Z", 
    "summary": "Recent improvements of the LEO-II theorem prover are presented. These\nimprovements include a revised ATP interface, new translations into first-order\nlogic, rule support for the axiom of choice, detection of defined equality, and\nmore flexible strategy scheduling."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.4193v3", 
    "title": "A Qualitative Comparison of the Suitability of Four Theorem Provers for   Basic Auction Theory", 
    "arxiv-id": "1303.4193v3", 
    "author": "Wolfgang Windsteiger", 
    "publish": "2013-03-18T09:20:41Z", 
    "summary": "Novel auction schemes are constantly being designed. Their design has\nsignificant consequences for the allocation of goods and the revenues\ngenerated. But how to tell whether a new design has the desired properties,\nsuch as efficiency, i.e. allocating goods to those bidders who value them most?\nWe say: by formal, machine-checked proofs. We investigated the suitability of\nthe Isabelle, Theorema, Mizar, and Hets/CASL/TPTP theorem provers for\nreproducing a key result of auction theory: Vickrey's 1961 theorem on the\nproperties of second-price auctions. Based on our formalisation experience,\ntaking an auction designer's perspective, we give recommendations on what\nsystem to use for formalising auctions, and outline further steps towards a\ncomplete auction theory toolbox."
},{
    "category": "cs.CR", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.4808v3", 
    "title": "The RAppArmor Package: Enforcing Security Policies in R Using Dynamic   Sandboxing on Linux", 
    "arxiv-id": "1303.4808v3", 
    "author": "Jeroen Ooms", 
    "publish": "2013-03-20T01:57:36Z", 
    "summary": "The increasing availability of cloud computing and scientific super computers\nbrings great potential for making R accessible through public or shared\nresources. This allows us to efficiently run code requiring lots of cycles and\nmemory, or embed R functionality into, e.g., systems and web services. However\nsome important security concerns need to be addressed before this can be put in\nproduction. The prime use case in the design of R has always been a single\nstatistician running R on the local machine through the interactive console.\nTherefore the execution environment of R is entirely unrestricted, which could\nresult in malicious behavior or excessive use of hardware resources in a shared\nenvironment. Properly securing an R process turns out to be a complex problem.\nWe describe various approaches and illustrate potential issues using some of\nour personal experiences in hosting public web services. Finally we introduce\nthe RAppArmor package: a Linux based reference implementation for dynamic\nsandboxing in R on the level of the operating system."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-7091-0794-2_13", 
    "link": "http://arxiv.org/pdf/1303.4928v2", 
    "title": "Parameter identification in large kinetic networks with BioPARKIN", 
    "arxiv-id": "1303.4928v2", 
    "author": "Peter Deuflhard", 
    "publish": "2013-03-20T13:05:33Z", 
    "summary": "Modelling, parameter identification, and simulation play an important role in\nsystems biology. Usually, the goal is to determine parameter values that\nminimise the difference between experimental measurement values and model\npredictions in a least-squares sense. Large-scale biological networks, however,\noften suffer from missing data for parameter identification. Thus, the\nleast-squares problems are rank-deficient and solutions are not unique. Many\ncommon optimisation methods ignore this detail because they do not take into\naccount the structure of the underlying inverse problem. These algorithms\nsimply return a \"solution\" without additional information on identifiability or\nuniqueness. This can yield misleading results, especially if parameters are\nco-regulated and data are noisy."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2013.03.022", 
    "link": "http://arxiv.org/pdf/1303.6034v2", 
    "title": "ZKCM: a C++ library for multiprecision matrix computation with   applications in quantum information", 
    "arxiv-id": "1303.6034v2", 
    "author": "Akira SaiToh", 
    "publish": "2013-03-25T06:18:35Z", 
    "summary": "ZKCM is a C++ library developed for the purpose of multiprecision matrix\ncomputation, on the basis of the GNU MP and MPFR libraries. It provides an\neasy-to-use syntax and convenient functions for matrix manipulations including\nthose often used in numerical simulations in quantum physics. Its extension\nlibrary, ZKCM_QC, is developed for simulating quantum computing using the\ntime-dependent matrix-product-state simulation method. This paper gives an\nintroduction about the libraries with practical sample programs."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1145/2710016", 
    "link": "http://arxiv.org/pdf/1303.6257v2", 
    "title": "Sampling exactly from the normal distribution", 
    "arxiv-id": "1303.6257v2", 
    "author": "Charles F. F. Karney", 
    "publish": "2013-03-25T19:19:47Z", 
    "summary": "An algorithm for sampling exactly from the normal distribution is given. The\nalgorithm reads some number of uniformly distributed random digits in a given\nbase and generates an initial portion of the representation of a normal deviate\nin the same base. Thereafter, uniform random digits are copied directly into\nthe representation of the normal deviate. Thus, in contrast to existing\nmethods, it is possible to generate normal deviates exactly rounded to any\nprecision with a mean cost that scales linearly in the precision. The method\nperforms no extended precision arithmetic, calls no transcendental functions,\nand, indeed, uses no floating point arithmetic whatsoever; it uses only simple\ninteger operations. It can easily be adapted to sample exactly from the\ndiscrete normal distribution whose parameters are rational numbers."
},{
    "category": "cs.SC", 
    "doi": "10.1145/2710016", 
    "link": "http://arxiv.org/pdf/1303.7425v1", 
    "title": "Highly Scalable Multiplication for Distributed Sparse Multivariate   Polynomials on Many-core Systems", 
    "arxiv-id": "1303.7425v1", 
    "author": "Jacques Laskar", 
    "publish": "2013-03-29T15:47:45Z", 
    "summary": "We present a highly scalable algorithm for multiplying sparse multivariate\npolynomials represented in a distributed format. This algo- rithm targets not\nonly the shared memory multicore computers, but also computers clusters or\nspecialized hardware attached to a host computer, such as graphics processing\nunits or many-core coprocessors. The scal- ability on the large number of cores\nis ensured by the lacks of synchro- nizations, locks and false-sharing during\nthe main parallel step."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cpc.2014.02.006", 
    "link": "http://arxiv.org/pdf/1308.3493v1", 
    "title": "xTras: a field-theory inspired xAct package for Mathematica", 
    "arxiv-id": "1308.3493v1", 
    "author": "Teake Nutma", 
    "publish": "2013-08-15T20:00:02Z", 
    "summary": "We present the tensor computer algebra package xTras, which provides\nfunctions and methods frequently needed when doing (classical) field theory.\nAmongst others, it can compute contractions, make Ans\\\"atze, and solve\ntensorial equations. It is built upon the tensor computer algebra system xAct,\na collection of packages for Mathematica."
},{
    "category": "stat.ML", 
    "doi": "10.1016/j.cpc.2014.02.006", 
    "link": "http://arxiv.org/pdf/1308.4214v1", 
    "title": "Pylearn2: a machine learning research library", 
    "arxiv-id": "1308.4214v1", 
    "author": "Yoshua Bengio", 
    "publish": "2013-08-20T02:50:43Z", 
    "summary": "Pylearn2 is a machine learning research library. This does not just mean that\nit is a collection of machine learning algorithms that share a common API; it\nmeans that it has been designed for flexibility and extensibility in order to\nfacilitate research projects that involve new or unusual use cases. In this\npaper we give a brief history of the library, an overview of its basic\nphilosophy, a summary of the library's architecture, and a description of how\nthe Pylearn2 community functions socially."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2014.02.006", 
    "link": "http://arxiv.org/pdf/1308.5200v1", 
    "title": "Manopt, a Matlab toolbox for optimization on manifolds", 
    "arxiv-id": "1308.5200v1", 
    "author": "Rodolphe Sepulchre", 
    "publish": "2013-08-23T18:35:59Z", 
    "summary": "Optimization on manifolds is a rapidly developing branch of nonlinear\noptimization. Its focus is on problems where the smooth geometry of the search\nspace can be leveraged to design efficient numerical algorithms. In particular,\noptimization on manifolds is well-suited to deal with rank and orthogonality\nconstraints. Such structured constraints appear pervasively in machine learning\napplications, including low-rank matrix completion, sensor network\nlocalization, camera network registration, independent component analysis,\nmetric learning, dimensionality reduction and so on. The Manopt toolbox,\navailable at www.manopt.org, is a user-friendly, documented piece of software\ndedicated to simplify experimenting with state of the art Riemannian\noptimization algorithms. We aim particularly at reaching practitioners outside\nour field."
},{
    "category": "physics.geo-ph", 
    "doi": "10.1002/jgrb.50217", 
    "link": "http://arxiv.org/pdf/1308.5846v1", 
    "title": "A Domain Decomposition Approach to Implementing Fault Slip in   Finite-Element Models of Quasi-static and Dynamic Crustal Deformation", 
    "arxiv-id": "1308.5846v1", 
    "author": "Charles A. Williams", 
    "publish": "2013-08-27T12:46:37Z", 
    "summary": "We employ a domain decomposition approach with Lagrange multipliers to\nimplement fault slip in a finite-element code, PyLith, for use in both\nquasi-static and dynamic crustal deformation applications. This integrated\napproach to solving both quasi-static and dynamic simulations leverages common\nfinite-element data structures and implementations of various boundary\nconditions, discretization schemes, and bulk and fault rheologies. We have\ndeveloped a custom preconditioner for the Lagrange multiplier portion of the\nsystem of equations that provides excellent scalability with problem size\ncompared to conventional additive Schwarz methods. We demonstrate application\nof this approach using benchmarks for both quasi-static viscoelastic\ndeformation and dynamic spontaneous rupture propagation that verify the\nnumerical implementation in PyLith."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2699464", 
    "link": "http://arxiv.org/pdf/1308.6029v3", 
    "title": "Algorithm 950: Ncpol2sdpa---Sparse Semidefinite Programming Relaxations   for Polynomial Optimization Problems of Noncommuting Variables", 
    "arxiv-id": "1308.6029v3", 
    "author": "Peter Wittek", 
    "publish": "2013-08-28T02:14:20Z", 
    "summary": "A hierarchy of semidefinite programming (SDP) relaxations approximates the\nglobal optimum of polynomial optimization problems of noncommuting variables.\nGenerating the relaxation, however, is a computationally demanding task, and\nonly problems of commuting variables have efficient generators. We develop an\nimplementation for problems of noncommuting problems that creates the\nrelaxation to be solved by SDPA -- a high-performance solver that runs in a\ndistributed environment. We further exploit the inherent sparsity of\noptimization problems in quantum physics to reduce the complexity of the\nresulting relaxations. Constrained problems with a relaxation of order two may\ncontain up to a hundred variables. The implementation is available in Python.\nThe tool helps solve problems such as finding the ground state energy or\ntesting quantum correlations."
},{
    "category": "cs.RO", 
    "doi": "10.1145/2699464", 
    "link": "http://arxiv.org/pdf/1309.0671v1", 
    "title": "BayesOpt: A Library for Bayesian optimization with Robotics Applications", 
    "arxiv-id": "1309.0671v1", 
    "author": "Ruben Martinez-Cantin", 
    "publish": "2013-09-03T13:38:05Z", 
    "summary": "The purpose of this paper is twofold. On one side, we present a general\nframework for Bayesian optimization and we compare it with some related fields\nin active learning and Bayesian numerical analysis. On the other hand, Bayesian\noptimization and related problems (bandits, sequential experimental design) are\nhighly dependent on the surrogate model that is selected. However, there is no\nclear standard in the literature. Thus, we present a fast and flexible toolbox\nthat allows to test and combine different models and criteria with little\neffort. It includes most of the state-of-the-art contributions, algorithms and\nmodels. Its speed also removes part of the stigma that Bayesian optimization\nmethods are only good for \"expensive functions\". The software is free and it\ncan be used in many operating systems and computer languages."
},{
    "category": "cs.CE", 
    "doi": "10.5334/jors.aw", 
    "link": "http://arxiv.org/pdf/1309.1780v1", 
    "title": "Software Abstractions and Methodologies for HPC Simulation Codes on   Future Architectures", 
    "arxiv-id": "1309.1780v1", 
    "author": "R. Thakur", 
    "publish": "2013-09-06T21:41:20Z", 
    "summary": "Large, complex, multi-scale, multi-physics simulation codes, running on high\nperformance com-puting (HPC) platforms, have become essential to advancing\nscience and engineering. These codes simulate multi-scale, multi-physics\nphenomena with unprecedented fidelity on petascale platforms, and are used by\nlarge communities. Continued ability of these codes to run on future platforms\nis as crucial to their communities as continued improvements in instruments and\nfacilities are to experimental scientists. However, the ability of code\ndevelopers to do these things faces a serious challenge with the paradigm shift\nunderway in platform architecture. The complexity and uncertainty of the future\nplatforms makes it essential to approach this challenge cooperatively as a\ncommunity. We need to develop common abstractions, frameworks, programming\nmodels and software development methodologies that can be applied across a\nbroad range of complex simulation codes, and common software infrastructure to\nsupport them. In this position paper we express and discuss our belief that\nsuch an infrastructure is critical to the deployment of existing and new large,\nmulti-scale, multi-physics codes on future HPC platforms."
},{
    "category": "cs.CE", 
    "doi": "10.5334/jors.am", 
    "link": "http://arxiv.org/pdf/1309.1781v1", 
    "title": "Experiences from Software Engineering of Large Scale AMR Multiphysics   Code Frameworks", 
    "arxiv-id": "1309.1781v1", 
    "author": "B. Van Straalen", 
    "publish": "2013-09-06T21:48:39Z", 
    "summary": "Among the present generation of multiphysics HPC simulation codes there are\nmany that are built upon general infrastructural frameworks. This is especially\ntrue of the codes that make use of structured adaptive mesh refinement (SAMR)\nbecause of unique demands placed on the housekeeping aspects of the code. They\nhave varying degrees of abstractions between the infrastructure such as mesh\nmanagement and IO and the numerics of the physics solvers. In this experience\nreport we summarize the experiences and lessons learned from two of such major\nsoftware efforts, FLASH and Chombo."
},{
    "category": "cs.CE", 
    "doi": "10.5334/jors.am", 
    "link": "http://arxiv.org/pdf/1309.1812v2", 
    "title": "Cactus: Issues for Sustainable Simulation Software", 
    "arxiv-id": "1309.1812v2", 
    "author": "Erik Schnetter", 
    "publish": "2013-09-07T03:18:51Z", 
    "summary": "The Cactus Framework is an open-source, modular, portable programming\nenvironment for the collaborative development and deployment of scientific\napplications using high-performance computing. Its roots reach back to 1996 at\nthe National Center for Supercomputer Applications and the Albert Einstein\nInstitute in Germany, where its development jumpstarted. Since then, the Cactus\nframework has witnessed major changes in hardware infrastructure as well as its\nown community. This paper describes its endurance through these past changes\nand, drawing upon lessons from its past, also discusses future"
},{
    "category": "cs.MS", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1309.5479v1", 
    "title": "Higher-order Reverse Automatic Differentiation with emphasis on the   third-order", 
    "arxiv-id": "1309.5479v1", 
    "author": "Artur L. Gower", 
    "publish": "2013-09-21T14:00:40Z", 
    "summary": "It is commonly assumed that calculating third order information is too\nexpensive for most applications. But we show that the directional derivative of\nthe Hessian ($D^3f(x)\\cdot d$) can be calculated at a cost proportional to that\nof a state-of-the-art method for calculating the Hessian matrix. We do this by\nfirst presenting a simple procedure for designing high order reverse methods\nand applying it to deduce several methods including a reverse method that\ncalculates $D^3f(x)\\cdot d$. We have implemented this method taking into\naccount symmetry and sparsity, and successfully calculated this derivative for\nfunctions with a million variables. These results indicate that the use of\nthird order information in a general nonlinear solver, such as Halley-Chebyshev\nmethods, could be a practical alternative to Newton's method."
},{
    "category": "hep-lat", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1311.2719v1", 
    "title": "Lattice Simulations using OpenACC compilers", 
    "arxiv-id": "1311.2719v1", 
    "author": "Pushan Majumdar", 
    "publish": "2013-11-12T09:12:54Z", 
    "summary": "OpenACC compilers allow one to use Graphics Processing Units without having\nto write explicit CUDA codes. Programs can be modified incrementally using\nOpenMP like directives which causes the compiler to generate CUDA kernels to be\nrun on the GPUs. In this article we look at the performance gain in lattice\nsimulations with dynamical fermions using OpenACC compilers."
},{
    "category": "math.OC", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1311.5735v1", 
    "title": "MEIGO: an open-source software suite based on metaheuristics for global   optimization in systems biology and bioinformatics", 
    "arxiv-id": "1311.5735v1", 
    "author": "Julio Saez-Rodriguez", 
    "publish": "2013-11-22T12:39:29Z", 
    "summary": "Optimization is key to solve many problems in computational biology. Global\noptimization methods provide a robust methodology, and metaheuristics in\nparticular have proven to be the most efficient methods for many applications.\nDespite their utility, there is limited availability of metaheuristic tools. We\npresent MEIGO, an R and Matlab optimization toolbox (also available in Python\nvia a wrapper of the R version), that implements metaheuristics capable of\nsolving diverse problems arising in systems biology and bioinformatics:\nenhanced scatter search method (eSS) for continuous nonlinear programming\n(cNLP) and mixed-integer programming (MINLP) problems, and variable\nneighborhood search (VNS) for Integer Programming (IP) problems. Both methods\ncan be run on a single-thread or in parallel using a cooperative strategy. The\ncode is supplied under GPLv3 and is available at\n\\url{http://www.iim.csic.es/~gingproc/meigo.html}. Documentation and examples\nare included. The R package has been submitted to Bioconductor. We evaluate\nMEIGO against optimization benchmarks, and illustrate its applicability to a\nseries of case studies in bioinformatics and systems biology, outperforming\nother state-of-the-art methods. MEIGO provides a free, open-source platform for\noptimization, that can be applied to multiple domains of systems biology and\nbioinformatics. It includes efficient state of the art metaheuristics, and its\nopen and modular structure allows the addition of further methods."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.1285v1", 
    "title": "Constructing Performance Models for Dense Linear Algebra Algorithms on   Cray XE Systems", 
    "arxiv-id": "1402.1285v1", 
    "author": "Mar\u00eda J. Mart\u00edn", 
    "publish": "2014-02-06T09:16:38Z", 
    "summary": "Hiding or minimizing the communication cost is key in order to obtain good\nperformance on large-scale systems. While communication overlapping attempts to\nhide communications cost, 2.5D communication avoiding algorithms improve\nperformance scalability by reducing the volume of data transfers at the cost of\nextra memory usage. Both approaches can be used together or separately and the\nbest choice depends on the machine, the algorithm and the problem size. Thus,\nthe development of performance models is crucial to determine the best option\nfor each scenario. In this paper, we present a methodology for constructing\nperformance models for parallel numerical routines on Cray XE systems. Our\nmodels use portable benchmarks that measure computational cost and network\ncharacteristics, as well as performance degradation caused by simultaneous\naccesses to the network. We validate our methodology by constructing the\nperformance models for the 2D and 2.5D approaches, with and without\noverlapping, of two matrix multiplication algorithms (Cannon's and SUMMA),\ntriangular solve (TRSM) and Cholesky. We compare the estimations provided by\nthese models with the experimental results using up to 24,576 cores of a Cray\nXE6 system and predict the performance of the algorithms on larger systems.\nResults prove that the estimations significantly improve when taking into\naccount network contention."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.2626v2", 
    "title": "GPU acceleration of Newton's method for large systems of polynomial   equations in double double and quad double arithmetic", 
    "arxiv-id": "1402.2626v2", 
    "author": "Xiangcheng Yu", 
    "publish": "2014-02-11T20:18:31Z", 
    "summary": "In order to compensate for the higher cost of double double and quad double\narithmetic when solving large polynomial systems, we investigate the\napplication of NVIDIA Tesla K20C general purpose graphics processing unit. The\nfocus on this paper is on Newton's method, which requires the evaluation of the\npolynomials, their derivatives, and the solution of a linear system to compute\nthe update to the current approximation for the solution. The reverse mode of\nalgorithmic differentiation for a product of variables is rewritten in a binary\ntree fashion so all threads in a block can collaborate in the computation. For\ndouble arithmetic, the evaluation and differentiation problem is memory bound,\nwhereas for complex quad double arithmetic the problem is compute bound. With\nacceleration we can double the dimension and get results that are twice as\naccurate in about the same time."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.5086v1", 
    "title": "Symmetric QR Algorithm with Permutations", 
    "arxiv-id": "1402.5086v1", 
    "author": "Aravindh Krishnamoorthy", 
    "publish": "2014-02-20T17:34:49Z", 
    "summary": "In this paper, we present the QR Algorithm with Permutations that shows an\nimproved convergence rate compared to the classical QR algorithm. We determine\na bound for performance based on best instantaneous convergence, and develop\nlow complexity methods for computing the permutation matrices at every\niteration. We use simulations to verify the improvement, and to compare the\nperformance of proposed algorithms to the classical QR algorithm."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.5897v1", 
    "title": "A Study on the Influence of Caching: Sequences of Dense Linear Algebra   Kernels", 
    "arxiv-id": "1402.5897v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2014-02-21T12:23:19Z", 
    "summary": "It is universally known that caching is critical to attain high- performance\nimplementations: In many situations, data locality (in space and time) plays a\nbigger role than optimizing the (number of) arithmetic floating point\noperations. In this paper, we show evidence that at least for linear algebra\nalgorithms, caching is also a crucial factor for accurate performance modeling\nand performance prediction."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.6076v1", 
    "title": "Machine Learning at Scale", 
    "arxiv-id": "1402.6076v1", 
    "author": "Jeremy M. Stanley", 
    "publish": "2014-02-25T07:50:50Z", 
    "summary": "It takes skill to build a meaningful predictive model even with the abundance\nof implementations of modern machine learning algorithms and readily available\ncomputing resources. Building a model becomes challenging if hundreds of\nterabytes of data need to be processed to produce the training data set. In a\ndigital advertising technology setting, we are faced with the need to build\nthousands of such models that predict user behavior and power advertising\ncampaigns in a 24/7 chaotic real-time production environment. As data\nscientists, we also have to convince other internal departments critical to\nimplementation success, our management, and our customers that our machine\nlearning system works. In this paper, we present the details of the design and\nimplementation of an automated, robust machine learning platform that impacts\nbillions of advertising impressions monthly. This platform enables us to\ncontinuously optimize thousands of campaigns over hundreds of millions of\nusers, on multiple continents, against varying performance objectives."
},{
    "category": "cs.DS", 
    "doi": "10.1007/s10107-014-0827-4", 
    "link": "http://arxiv.org/pdf/1402.6246v5", 
    "title": "An experimental exploration of Marsaglia's xorshift generators,   scrambled", 
    "arxiv-id": "1402.6246v5", 
    "author": "Sebastiano Vigna", 
    "publish": "2014-01-22T19:18:20Z", 
    "summary": "Marsaglia proposed recently xorshift generators as a class of very fast,\ngood-quality pseudorandom number generators. Subsequent analysis by Panneton\nand L'Ecuyer has lowered the expectations raised by Marsaglia's paper, showing\nseveral weaknesses of such generators, verified experimentally using the\nTestU01 suite. Nonetheless, many of the weaknesses of xorshift generators fade\naway if their result is scrambled by a non-linear operation (as originally\nsuggested by Marsaglia). In this paper we explore the space of possible\ngenerators obtained by multiplying the result of a xorshift generator by a\nsuitable constant. We sample generators at 100 equispaced points of their state\nspace and obtain detailed statistics that lead us to choices of parameters that\nimprove on the current ones. We then explore for the first time the space of\nhigh-dimensional xorshift generators, following another suggestion in\nMarsaglia's paper, finding choices of parameters providing periods of length\n$2^{1024} - 1$ and $2^{4096} - 1$. The resulting generators are of extremely\nhigh quality, faster than current similar alternatives, and generate\nlong-period sequences passing strong statistical tests using only eight logical\noperations, one addition and one multiplication by a constant."
},{
    "category": "cs.SC", 
    "doi": "10.1134/S0361768813030031", 
    "link": "http://arxiv.org/pdf/1402.6635v1", 
    "title": "Tensor computations in computer algebra systems", 
    "arxiv-id": "1402.6635v1", 
    "author": "L. A. Sevastyanov", 
    "publish": "2014-02-22T15:47:58Z", 
    "summary": "This paper considers three types of tensor computations. On their basis, we\nattempt to formulate criteria that must be satisfied by a computer algebra\nsystem dealing with tensors. We briefly overview the current state of tensor\ncomputations in different computer algebra systems. The tensor computations are\nillustrated with appropriate examples implemented in specific systems: Cadabra\nand Maxima."
},{
    "category": "cs.DS", 
    "doi": "10.1134/S0361768813030031", 
    "link": "http://arxiv.org/pdf/1404.0390v3", 
    "title": "Further scramblings of Marsaglia's xorshift generators", 
    "arxiv-id": "1404.0390v3", 
    "author": "Sebastiano Vigna", 
    "publish": "2014-04-01T20:14:51Z", 
    "summary": "xorshift* generators are a variant of Marsaglia's xorshift generators that\neliminate linear artifacts typical of generators based on $\\mathbf Z/2\\mathbf\nZ$-linear operations using multiplication by a suitable constant. Shortly after\nhigh-dimensional xorshift* generators were introduced, Saito and Matsumoto\nsuggested a different way to eliminate linear artifacts based on addition in\n$\\mathbf Z/2^{32}\\mathbf Z$, leading to the XSadd generator. Starting from the\nobservation that the lower bits of XSadd are very weak, as its reverse fails\nsystematically several statistical tests, we explore xorshift+, a variant of\nXSadd using 64-bit operations, which leads, in small dimension, to extremely\nfast high-quality generators."
},{
    "category": "math.NA", 
    "doi": "10.1134/S0361768813030031", 
    "link": "http://arxiv.org/pdf/1404.2891v1", 
    "title": "A New Highly Parallel Non-Hermitian Eigensolver", 
    "arxiv-id": "1404.2891v1", 
    "author": "Eric Polizzi", 
    "publish": "2014-04-10T17:59:44Z", 
    "summary": "Calculating portions of eigenvalues and eigenvectors of matrices or matrix\npencils has many applications. An approach to this calculation for Hermitian\nproblems based on a density matrix has been proposed in 2009 and a software\npackage called FEAST has been developed. The density-matrix approach allows\nFEAST's implementation to exploit a key strength of modern computer\narchitectures, namely, multiple levels of parallelism. Consequently, the\nsoftware package has been well received and subsequently commercialized. A\ndetailed theoretical analysis of Hermitian FEAST has also been established very\nrecently. This paper generalizes the FEAST algorithm and theory, for the first\ntime, to tackle non-Hermitian problems. Fundamentally, the new algorithm is\nbasic subspace iteration or Bauer bi-iteration, except applied with a novel\naccelerator based on Cauchy integrals. The resulting algorithm retains the\nmulti-level parallelism of Hermitian FEAST, making it a valuable new tool for\nlarge-scale computational science and engineering problems on leading-edge\ncomputing platforms."
},{
    "category": "cs.MS", 
    "doi": "10.1134/S0361768813030031", 
    "link": "http://arxiv.org/pdf/1404.4161v2", 
    "title": "An Optimized and Scalable Eigensolver for Sequences of Eigenvalue   Problems", 
    "arxiv-id": "1404.4161v2", 
    "author": "Edoardo Di Napoli", 
    "publish": "2014-04-16T08:09:56Z", 
    "summary": "In many scientific applications the solution of non-linear differential\nequations are obtained through the set-up and solution of a number of\nsuccessive eigenproblems. These eigenproblems can be regarded as a sequence\nwhenever the solution of one problem fosters the initialization of the next. In\naddition, in some eigenproblem sequences there is a connection between the\nsolutions of adjacent eigenproblems. Whenever it is possible to unravel the\nexistence of such a connection, the eigenproblem sequence is said to be\ncorrelated. When facing with a sequence of correlated eigenproblems the current\nstrategy amounts to solving each eigenproblem in isolation. We propose a\nalternative approach which exploits such correlation through the use of an\neigensolver based on subspace iteration and accelerated with Chebyshev\npolynomials (ChFSI). The resulting eigensolver is optimized by minimizing the\nnumber of matrix-vector multiplications and parallelized using the Elemental\nlibrary framework. Numerical results show that ChFSI achieves excellent\nscalability and is competitive with current dense linear algebra parallel\neigensolvers."
},{
    "category": "cs.AI", 
    "doi": "10.1134/S0361768813030031", 
    "link": "http://arxiv.org/pdf/1404.4893v1", 
    "title": "CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit", 
    "arxiv-id": "1404.4893v1", 
    "author": "Fabio Stella", 
    "publish": "2014-04-18T21:48:34Z", 
    "summary": "Continuous time Bayesian network classifiers are designed for temporal\nclassification of multivariate streaming data when time duration of events\nmatters and the class does not change over time. This paper introduces the\nCTBNCToolkit: an open source Java toolkit which provides a stand-alone\napplication for temporal classification and a library for continuous time\nBayesian network classifiers. CTBNCToolkit implements the inference algorithm,\nthe parameter learning algorithm, and the structural learning algorithm for\ncontinuous time Bayesian network classifiers. The structural learning algorithm\nis based on scoring functions: the marginal log-likelihood score and the\nconditional log-likelihood score are provided. CTBNCToolkit provides also an\nimplementation of the expectation maximization algorithm for clustering\npurpose. The paper introduces continuous time Bayesian network classifiers. How\nto use the CTBNToolkit from the command line is described in a specific\nsection. Tutorial examples are included to facilitate users to understand how\nthe toolkit must be used. A section dedicate to the Java library is proposed to\nhelp further code extensions."
},{
    "category": "cs.DC", 
    "doi": "10.1137/140970963", 
    "link": "http://arxiv.org/pdf/1406.0089v3", 
    "title": "Recursive Algorithms for Distributed Forests of Octrees", 
    "arxiv-id": "1406.0089v3", 
    "author": "Omar Ghattas", 
    "publish": "2014-05-31T16:02:54Z", 
    "summary": "The forest-of-octrees approach to parallel adaptive mesh refinement and\ncoarsening (AMR) has recently been demonstrated in the context of a number of\nlarge-scale PDE-based applications. Although linear octrees, which store only\nleaf octants, have an underlying tree structure by definition, it is not often\nexploited in previously published mesh-related algorithms. This is because the\nbranches are not explicitly stored, and because the topological relationships\nin meshes, such as the adjacency between cells, introduce dependencies that do\nnot respect the octree hierarchy. In this work we combine hierarchical and\ntopological relationships between octree branches to design efficient recursive\nalgorithms.\n  We present three important algorithms with recursive implementations. The\nfirst is a parallel search for leaves matching any of a set of multiple search\ncriteria. The second is a ghost layer construction algorithm that handles\narbitrarily refined octrees that are not covered by previous algorithms, which\nrequire a 2:1 condition between neighboring leaves. The third is a universal\nmesh topology iterator. This iterator visits every cell in a domain partition,\nas well as every interface (face, edge and corner) between these cells. The\niterator calculates the local topological information for every interface that\nit visits, taking into account the nonconforming interfaces that increase the\ncomplexity of describing the local topology. To demonstrate the utility of the\ntopology iterator, we use it to compute the numbering and encoding of\nhigher-order $C^0$ nodal basis functions.\n  We analyze the complexity of the new recursive algorithms theoretically, and\nassess their performance, both in terms of single-processor efficiency and in\nterms of parallel scalability, demonstrating good weak and strong scaling up to\n458k cores of the JUQUEEN supercomputer."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.parco.2016.04.001", 
    "link": "http://arxiv.org/pdf/1406.1066v3", 
    "title": "Fast Matlab compatible sparse assembly on multicore computers", 
    "arxiv-id": "1406.1066v3", 
    "author": "Dimitar Lukarski", 
    "publish": "2014-06-04T15:01:23Z", 
    "summary": "We develop and implement in this paper a fast sparse assembly algorithm, the\nfundamental operation which creates a compressed matrix from raw index data.\nSince it is often a quite demanding and sometimes critical operation, it is of\ninterest to design a highly efficient implementation. We show how to do this,\nand moreover, we show how our implementation can be parallelized to utilize the\npower of modern multicore computers. Our freely available code, fully Matlab\ncompatible, achieves about a factor of 5 times in speedup on a typical 6-core\nmachine and 10 times on a dual-socket 16 core machine compared to the built-in\nserial implementation."
},{
    "category": "cs.AI", 
    "doi": "10.4204/EPTCS.152.1", 
    "link": "http://arxiv.org/pdf/1406.1556v1", 
    "title": "Enhancements to ACL2 in Versions 6.2, 6.3, and 6.4", 
    "arxiv-id": "1406.1556v1", 
    "author": "J Strother Moore", 
    "publish": "2014-06-06T01:46:54Z", 
    "summary": "We report on improvements to ACL2 made since the 2013 ACL2 Workshop."
},{
    "category": "stat.CO", 
    "doi": "10.4204/EPTCS.152.1", 
    "link": "http://arxiv.org/pdf/1406.4806v1", 
    "title": "The OpenCPU System: Towards a Universal Interface for Scientific   Computing through Separation of Concerns", 
    "arxiv-id": "1406.4806v1", 
    "author": "Jeroen Ooms", 
    "publish": "2014-06-04T00:03:52Z", 
    "summary": "Applications integrating analysis components require a programmable interface\nwhich defines statistical operations independently of any programming language.\nBy separating concerns of scientific computing from application and\nimplementation details we can derive an interoperable API for data analysis.\nBut what exactly are the concerns of scientific computing? To answer this\nquestion, the paper starts with an exploration of the purpose, problems,\ncharacteristics, struggles, culture, and community of this unique branch of\ncomputing. By mapping out the domain logic, we try to unveil the fundamental\nprinciples and concepts behind statistical software. Along the way we highlight\nimportant problems and bottlenecks that need to be addressed by the system in\norder to facilitate reliable and scalable analysis units. Finally, the OpenCPU\nsoftware is introduced as an example implementation that builds on HTTP and R\nto expose a simple, abstracted interface for scientific computing."
},{
    "category": "stat.ML", 
    "doi": "10.4204/EPTCS.152.1", 
    "link": "http://arxiv.org/pdf/1406.5565v1", 
    "title": "An Open Source Pattern Recognition Toolbox for MATLAB", 
    "arxiv-id": "1406.5565v1", 
    "author": "Sam Keene", 
    "publish": "2014-06-21T01:50:54Z", 
    "summary": "Pattern recognition and machine learning are becoming integral parts of\nalgorithms in a wide range of applications. Different algorithms and approaches\nfor machine learning include different tradeoffs between performance and\ncomputation, so during algorithm development it is often necessary to explore a\nvariety of different approaches to a given task. A toolbox with a unified\nframework across multiple pattern recognition techniques enables algorithm\ndevelopers the ability to rapidly evaluate different choices prior to\ndeployment. MATLAB is a widely used environment for algorithm development and\nprototyping, and although several MATLAB toolboxes for pattern recognition are\ncurrently available these are either incomplete, expensive, or restrictively\nlicensed. In this work we describe a MATLAB toolbox for pattern recognition and\nmachine learning known as the PRT (Pattern Recognition Toolbox), licensed under\nthe permissive MIT license. The PRT includes many popular techniques for data\npreprocessing, supervised learning, clustering, regression and feature\nselection, as well as a methodology for combining these components using a\nsimple, uniform syntax. The resulting algorithms can be evaluated using\ncross-validation and a variety of scoring metrics to ensure robust performance\nwhen the algorithm is deployed. This paper presents an overview of the PRT as\nwell as an example of usage on Fisher's Iris dataset."
},{
    "category": "stat.CO", 
    "doi": "10.4204/EPTCS.152.1", 
    "link": "http://arxiv.org/pdf/1406.7648v2", 
    "title": "Bayesian Network Constraint-Based Structure Learning Algorithms:   Parallel and Optimised Implementations in the bnlearn R Package", 
    "arxiv-id": "1406.7648v2", 
    "author": "Marco Scutari", 
    "publish": "2014-06-30T09:56:20Z", 
    "summary": "It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2013.6670338", 
    "link": "http://arxiv.org/pdf/1408.0393v1", 
    "title": "Standards for Graph Algorithm Primitives", 
    "arxiv-id": "1408.0393v1", 
    "author": "Andrew Yoo", 
    "publish": "2014-08-02T16:17:40Z", 
    "summary": "It is our view that the state of the art in constructing a large collection\nof graph algorithms in terms of linear algebraic operations is mature enough to\nsupport the emergence of a standard set of primitive building blocks. This\npaper is a position paper defining the problem and announcing our intention to\nlaunch an open effort to define this standard."
},{
    "category": "cs.MS", 
    "doi": "10.1121/1.4901318", 
    "link": "http://arxiv.org/pdf/1408.0854v1", 
    "title": "Semi-Analytical Computation of Acoustic Scattering by Spheroids and   Disks", 
    "arxiv-id": "1408.0854v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-08-05T03:18:28Z", 
    "summary": "Analytical solutions to acoustic scattering problems involving nonspherical\nshapes, such as spheroids and disks, have long been known and have many\napplications. However, these solutions require special functions that are not\neasily computable. For this reason, their asymptotic forms are typically used\nsince they are more readily available. We explore these solutions and provide\ncomputational software for calculating their nonasymptotic forms, which are\naccurate over a wide range of frequencies and distances. This software, which\nruns in MATLAB, computes the solutions to acoustic scattering problems\ninvolving spheroids and disks by semi-analytical means, and is freely available\nfrom our webpage."
},{
    "category": "cs.MS", 
    "doi": "10.1121/1.4901318", 
    "link": "http://arxiv.org/pdf/1408.1900v1", 
    "title": "A Report of a Significant Error On a Frequently Used Pseudo Random   Number Generator", 
    "arxiv-id": "1408.1900v1", 
    "author": "M. Cemal Yalabik", 
    "publish": "2014-08-07T07:16:07Z", 
    "summary": "Emergence of stochastic simulations as an extensively used computational tool\nfor scientific purposes intensified the need for more accurate ways of\ngenerating sufficiently long sequences of uncorrelated random numbers. Even\nthough several different methods have been proposed for this end, deterministic\nalgorithms known as pseudo-random number generators (PRNGs) emerged to be the\nmost widely used tool as a replicable, portable and easy to use method to\ngenerate such random number sequences. Here, we introduce a simple Poisson\nprocess whose simulation gives systematic errors when the very commonly used\nrandom number generator of the GNU C Library (Glibc) is utilised. The PRNG of\nGlibc is an additive lagged Fibonacci generator, the family of such PRNGs are\naccepted as relatively safe among other PRNGs. The systematic errors indicate\ncomplex correlation relations among random numbers which requires a further\nexplanation."
},{
    "category": "cs.DC", 
    "doi": "10.1137/1.9781611973754.11", 
    "link": "http://arxiv.org/pdf/1408.2858v2", 
    "title": "Experimental Evaluation of Multi-Round Matrix Multiplication on   MapReduce", 
    "arxiv-id": "1408.2858v2", 
    "author": "Francesco Silvestri", 
    "publish": "2014-08-12T21:23:11Z", 
    "summary": "A common approach in the design of MapReduce algorithms is to minimize the\nnumber of rounds. Indeed, there are many examples in the literature of\nmonolithic MapReduce algorithms, which are algorithms requiring just one or two\nrounds. However, we claim that the design of monolithic algorithms may not be\nthe best approach in cloud systems. Indeed, multi-round algorithms may exploit\nsome features of cloud platforms by suitably setting the round number according\nto the execution context. In this paper we carry out an experimental study of\nmulti-round MapReduce algorithms aiming at investigating the performance of the\nmulti-round approach. We use matrix multiplication as a case study. We first\npropose a scalable Hadoop library, named M$_3$, for matrix multiplication in\nthe dense and sparse cases which allows to tradeoff round number with the\namount of data shuffled in each round and the amount of memory required by\nreduce functions. Then, we present an extensive study of this library on an\nin-house cluster and on Amazon Web Services aiming at showing its performance\nand at comparing monolithic and multi-round approaches. The experiments show\nthat, even without a low level optimization, it is possible to design\nmulti-round algorithms with a small running time overhead."
},{
    "category": "cs.CV", 
    "doi": "10.1137/1.9781611973754.11", 
    "link": "http://arxiv.org/pdf/1408.3264v7", 
    "title": "A brief survey on deep belief networks and introducing a new object   oriented toolbox (DeeBNet)", 
    "arxiv-id": "1408.3264v7", 
    "author": "Mohammad Mehdi Homayounpour", 
    "publish": "2014-08-14T12:37:57Z", 
    "summary": "Nowadays, this is very popular to use the deep architectures in machine\nlearning. Deep Belief Networks (DBNs) are deep architectures that use stack of\nRestricted Boltzmann Machines (RBM) to create a powerful generative model using\ntraining data. DBNs have many ability like feature extraction and\nclassification that are used in many applications like image processing, speech\nprocessing and etc. This paper introduces a new object oriented MATLAB toolbox\nwith most of abilities needed for the implementation of DBNs. In the new\nversion, the toolbox can be used in Octave. According to the results of the\nexperiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups\n(text) datasets, it was shown that the toolbox can learn automatically a good\nrepresentation of the input from unlabeled data with better discrimination\nbetween different classes. Also on all datasets, the obtained classification\nerrors are comparable to those of state of the art classifiers. In addition,\nthe toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and our\nnew FEPCD method), different sparsity methods (quadratic, rate distortion and\nour new normal method), different RBM types (generative and discriminative),\nusing GPU, etc. The toolbox is a user-friendly open source software and is\nfreely available on the website\nhttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html ."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1063/1.4903450", 
    "link": "http://arxiv.org/pdf/1408.4701v2", 
    "title": "Orbital-Free Density Functional Theory Implementation with the Projector   Augmented-Wave Method", 
    "arxiv-id": "1408.4701v2", 
    "author": "O. Lopez-Acevedo", 
    "publish": "2014-08-20T15:46:07Z", 
    "summary": "We present a computational scheme for orbital-free density functional theory\n(OFDFT) that simultaneously provides access to all-electron values and\npreserves the OFDFT linear scaling as a function of the system size. Using the\nprojector augmented-wave method (PAW) in combination with real-space methods we\novercome some obstacles faced by other available implementation schemes.\nSpecifically, the advantages of using the PAW method are two fold. First, PAW\nreproduces all-electron values offering freedom in adjusting the convergence\nparameters and the atomic setups allow tuning the numerical accuracy per\nelement. Second, PAW can provide a solution to some of the convergence problems\nexhibited in other OFDFT implementations based on Kohn-Sham codes. Using PAW\nand real-space methods, our orbital-free results agree with the reference\nall-electron values with a mean absolute error of 10~meV and the number of\niterations required by the self-consistent cycle is comparable to the KS\nmethod. The comparison of all-electron and pseudopotential bulk modulus and\nlattice constant reveal an enormous difference, demonstrating that in order to\nassess the performance of OFDFT functionals it is necessary to use\nimplementations that obtain all-electron values. The proposed combination of\nmethods is the most promising route currently available. We finally show that a\nparametrized kinetic energy functional can give lattice constants and bulk\nmoduli comparable in accuracy to those obtained by the KS PBE method,\nexemplified with the case of diamond."
},{
    "category": "hep-lat", 
    "doi": "10.1109/IPDPS.2014.112", 
    "link": "http://arxiv.org/pdf/1408.5925v1", 
    "title": "A Framework for Lattice QCD Calculations on GPUs", 
    "arxiv-id": "1408.5925v1", 
    "author": "B. Jo\u00f3", 
    "publish": "2014-08-25T20:50:08Z", 
    "summary": "Computing platforms equipped with accelerators like GPUs have proven to\nprovide great computational power. However, exploiting such platforms for\nexisting scientific applications is not a trivial task. Current GPU programming\nframeworks such as CUDA C/C++ require low-level programming from the developer\nin order to achieve high performance code. As a result porting of applications\nto GPUs is typically limited to time-dominant algorithms and routines, leaving\nthe remainder not accelerated which can open a serious Amdahl's law issue. The\nlattice QCD application Chroma allows to explore a different porting strategy.\nThe layered structure of the software architecture logically separates the\ndata-parallel from the application layer. The QCD Data-Parallel software layer\nprovides data types and expressions with stencil-like operations suitable for\nlattice field theory and Chroma implements algorithms in terms of this\nhigh-level interface. Thus by porting the low-level layer one can effectively\nmove the whole application in one swing to a different platform. The\nQDP-JIT/PTX library, the reimplementation of the low-level layer, provides a\nframework for lattice QCD calculations for the CUDA architecture. The complete\nsoftware interface is supported and thus applications can be run unaltered on\nGPU-based parallel computers. This reimplementation was possible due to the\navailability of a JIT compiler (part of the NVIDIA Linux kernel driver) which\ntranslates an assembly-like language (PTX) to GPU code. The expression template\ntechnique is used to build PTX code generators and a software cache manages the\nGPU memory. This reimplementation allows us to deploy an efficient\nimplementation of the full gauge-generation program with dynamical fermions on\nlarge-scale GPU-based machines such as Titan and Blue Waters which accelerates\nthe algorithm by more than an order of magnitude."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1109/IPDPS.2014.112", 
    "link": "http://arxiv.org/pdf/1408.6373v1", 
    "title": "Concurrent Cuba", 
    "arxiv-id": "1408.6373v1", 
    "author": "T. Hahn", 
    "publish": "2014-08-27T10:13:45Z", 
    "summary": "The parallel version of the multidimensional numerical integration package\nCuba is presented and achievable speed-ups discussed."
},{
    "category": "cs.MS", 
    "doi": "10.1109/IPDPS.2014.112", 
    "link": "http://arxiv.org/pdf/1409.0669v1", 
    "title": "Performance Portability Study of Linear Algebra Kernels in OpenCL", 
    "arxiv-id": "1409.0669v1", 
    "author": "Ansgar J\u00fcngel", 
    "publish": "2014-09-02T11:21:13Z", 
    "summary": "The performance portability of OpenCL kernel implementations for common\nmemory bandwidth limited linear algebra operations across different hardware\ngenerations of the same vendor as well as across vendors is studied. Certain\ncombinations of kernel implementations and work sizes are found to exhibit good\nperformance across compute kernels, hardware generations, and, to a lesser\ndegree, vendors. As a consequence, it is demonstrated that the optimization of\na single kernel is often sufficient to obtain good performance for a large\nclass of more complicated operations."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1109/IPDPS.2014.112", 
    "link": "http://arxiv.org/pdf/1409.1354v3", 
    "title": "CosmoMC Installation and Running Guidelines", 
    "arxiv-id": "1409.1354v3", 
    "author": "Dong Zhao", 
    "publish": "2014-09-04T08:21:16Z", 
    "summary": "CosmoMC is a Fortran 95 Markov-Chain Monte-Carlo (MCMC) engine to explore the\ncosmological parameter space, plus a Python suite for plotting and presenting\nresults (see http://cosmologist.info/cosmomc/). This document describes the\ninstallation of the CosmoMC on a Linux system (Ubuntu 14.04.1 LTS 64-bit\nversion). It is written for those who want to use it in their scientific\nresearch but without much training on Linux and the program. Besides a\nstep-by-step installation guide, we also give a brief introduction of how to\nrun the program on both a desktop and a cluster. We share our way to generate\nthe plots that are commonly used in the references of cosmology. For more\ninformation, one can refer to the CosmoCoffee forum\n(http://cosmocoffee.info/viewforum.php?f=11) or contact the authors of this\ndocument. Questions and comments would be much appreciated."
},{
    "category": "cs.DC", 
    "doi": "10.1109/IPDPS.2014.112", 
    "link": "http://arxiv.org/pdf/1409.2908v1", 
    "title": "A Framework for Practical Parallel Fast Matrix Multiplication", 
    "arxiv-id": "1409.2908v1", 
    "author": "Grey Ballard", 
    "publish": "2014-09-09T22:28:36Z", 
    "summary": "Matrix multiplication is a fundamental computation in many scientific\ndisciplines. In this paper, we show that novel fast matrix multiplication\nalgorithms can significantly outperform vendor implementations of the classical\nalgorithm and Strassen's fast algorithm on modest problem sizes and shapes.\nFurthermore, we show that the best choice of fast algorithm depends not only on\nthe size of the matrices but also the shape. We develop a code generation tool\nto automatically implement multiple sequential and shared-memory parallel\nvariants of each fast algorithm, including our novel parallelization scheme.\nThis allows us to rapidly benchmark over 20 fast algorithms on several problem\nsizes. Furthermore, we discuss a number of practical implementation issues for\nthese algorithms on shared-memory machines that can direct further research on\nmaking fast algorithms practical."
},{
    "category": "stat.CO", 
    "doi": "10.1214/13-STS462", 
    "link": "http://arxiv.org/pdf/1409.3144v1", 
    "title": "Enhancing R with Advanced Compilation Tools and Methods", 
    "arxiv-id": "1409.3144v1", 
    "author": "Duncan Temple Lang", 
    "publish": "2014-09-09T10:37:20Z", 
    "summary": "I describe an approach to compiling common idioms in R code directly to\nnative machine code and illustrate it with several examples. Not only can this\nyield significant performance gains, but it allows us to use new approaches to\ncomputing in R. Importantly, the compilation requires no changes to R itself,\nbut is done entirely via R packages. This allows others to experiment with\ndifferent compilation strategies and even to define new domain-specific\nlanguages within R. We use the Low-Level Virtual Machine (LLVM) compiler\ntoolkit to create the native code and perform sophisticated optimizations on\nthe code. By adopting this widely used software within R, we leverage its\nability to generate code for different platforms such as CPUs and GPUs, and\nwill continue to benefit from its ongoing development. This approach\npotentially allows us to develop high-level R code that is also fast, that can\nbe compiled to work with different data representations and sources, and that\ncould even be run outside of R. The approach aims to both provide a compiler\nfor a limited subset of the R language and also to enable R programmers to\nwrite other compilers. This is another approach to help us write high-level\ndescriptions of what we want to compute, not how."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.parco.2015.05.004", 
    "link": "http://arxiv.org/pdf/1409.5757v1", 
    "title": "Intel Cilk Plus for Complex Parallel Algorithms: \"Enormous Fast Fourier   Transform\" (EFFT) Library", 
    "arxiv-id": "1409.5757v1", 
    "author": "Andrey Vladimirov", 
    "publish": "2014-09-19T18:48:58Z", 
    "summary": "In this paper we demonstrate the methodology for parallelizing the\ncomputation of large one-dimensional discrete fast Fourier transforms (DFFTs)\non multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey\nmethod have to control cache utilization, memory bandwidth and vector hardware\nusage, and at the same time scale across multiple threads or compute nodes. Our\nmethod builds on single-threaded Intel Math Kernel Library (MKL) implementation\nof DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We\ndemonstrate the ability of Intel Cilk Plus to handle parallel recursion with\nnested loop-centric parallelism without tuning the code to the number of cores\nor cache metrics. The result of our work is a library called EFFT that performs\n1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel\nDFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code\nof EFFT is available for free download under the GPLv3 license. This work\nprovides a new efficient DFFT implementation, and at the same time demonstrates\nan educational example of how computer science problems with complex parallel\npatterns can be optimized for high performance using the Intel Cilk Plus\nframework."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.jsc.2016.03.009", 
    "link": "http://arxiv.org/pdf/1501.00179v3", 
    "title": "A persistence landscapes toolbox for topological statistics", 
    "arxiv-id": "1501.00179v3", 
    "author": "Pawel Dlotko", 
    "publish": "2014-12-31T17:34:59Z", 
    "summary": "Topological data analysis provides a multiscale description of the geometry\nand topology of quantitative data. The persistence landscape is a topological\nsummary that can be easily combined with tools from statistics and machine\nlearning. We give efficient algorithms for calculating persistence landscapes,\ntheir averages, and distances between such averages. We discuss an\nimplementation of these algorithms and some related procedures. These are\nintended to facilitate the combination of statistics and machine learning with\ntopological data analysis. We present an experiment showing that the\nlow-dimensional persistence landscapes of points sampled from spheres (and\nboxes) of varying dimensions differ."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.01.004", 
    "link": "http://arxiv.org/pdf/1501.01578v1", 
    "title": "GammaCHI: a package for the inversion and computation of the gamma and   chi-square cumulative distribution functions (central and noncentral)", 
    "arxiv-id": "1501.01578v1", 
    "author": "N. M. Temme", 
    "publish": "2015-01-07T18:06:17Z", 
    "summary": "A Fortran 90 module (GammaCHI) for computing and inverting the gamma and\nchi-square cumulative distribution functions (central and noncentral) is\npresented. The main novelty of this package are the reliable and accurate\ninversion routines for the noncentral cumulative distribution functions.\nAdditionally, the package also provides routines for computing the gamma\nfunction, the error function and other functions related to the gamma function.\nThe module includes the routines cdfgamC, invcdfgamC, cdfgamNC, invcdfgamNC,\nerrorfunction, inverfc, gamma, loggam, gamstar and quotgamm for the computation\nof the central gamma distribution function (and its complementary function),\nthe inversion of the central gamma distribution function, the computation of\nthe noncentral gamma distribution function (and its complementary function),\nthe inversion of the noncentral gamma distribution function, the computation of\nthe error function and its complementary function, the inversion of the\ncomplementary error function, the computation of: the gamma function, the\nlogarithm of the gamma function, the regulated gamma function and the ratio of\ntwo gamma functions, respectively."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2998441", 
    "link": "http://arxiv.org/pdf/1501.01809v3", 
    "title": "Firedrake: automating the finite element method by composing   abstractions", 
    "arxiv-id": "1501.01809v3", 
    "author": "Paul H. J. Kelly", 
    "publish": "2015-01-08T11:57:05Z", 
    "summary": "Firedrake is a new tool for automating the numerical solution of partial\ndifferential equations. Firedrake adopts the domain-specific language for the\nfinite element method of the FEniCS project, but with a pure Python\nruntime-only implementation centred on the composition of several existing and\nnew abstractions for particular aspects of scientific computing. The result is\na more complete separation of concerns which eases the incorporation of\nseparate contributions from computer scientists, numerical analysts and\napplication specialists. These contributions may add functionality, or improve\nperformance.\n  Firedrake benefits from automatically applying new optimisations. This\nincludes factorising mixed function spaces, transforming and vectorising inner\nloops, and intrinsically supporting block matrix operations. Importantly,\nFiredrake presents a simple public API for escaping the UFL abstraction. This\nallows users to implement common operations that fall outside pure variational\nformulations, such as flux-limiters."
},{
    "category": "math.OC", 
    "doi": "10.1145/2998441", 
    "link": "http://arxiv.org/pdf/1501.03341v1", 
    "title": "Solving Polynomial Systems by Penetrating Gradient Algorithm Applying   Deepest Descent Strategy", 
    "arxiv-id": "1501.03341v1", 
    "author": "Ivo Beros", 
    "publish": "2015-01-14T13:34:50Z", 
    "summary": "An algorithm and associated strategy for solving polynomial systems within\nthe optimization framework is presented. The algorithm and strategy are named,\nrespectively, the penetrating gradient algorithm and the deepest descent\nstrategy. The most prominent feature of penetrating gradient algorithm, after\nwhich it was named, is its ability to see and penetrate through the obstacles\nin error space along the line of search direction and to jump to the global\nminimizer in a single step. The ability to find the deepest point in an\narbitrary direction, no matter how distant the point is and regardless of the\nrelief of error space between the current and the best point, motivates\nmovements in directions in which cost function can be maximally reduced, rather\nthan in directions that seem to be the best locally (like, for instance, the\nsteepest descent, i.e., negative gradient direction). Therefore, the strategy\nis named the deepest descent, in contrast but alluding to the steepest descent.\nPenetrating gradient algorithm is derived and its properties are proven\nmathematically, while features of the deepest descent strategy are shown by\ncomparative simulations. Extensive benchmark tests confirm that the proposed\nalgorithm and strategy jointly form an effective solver of polynomial systems.\nIn addition, further theoretical considerations in Section 5 about solving\nlinear systems by the proposed method reveal a surprising and interesting\nrelation of proposed and Gauss-Seidel method."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2998441", 
    "link": "http://arxiv.org/pdf/1501.04979v3", 
    "title": "FASTA: A Generalized Implementation of Forward-Backward Splitting", 
    "arxiv-id": "1501.04979v3", 
    "author": "Richard Baraniuk", 
    "publish": "2015-01-16T01:22:55Z", 
    "summary": "This is a user manual for the software package FASTA."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.26.16", 
    "link": "http://arxiv.org/pdf/1007.3794v1", 
    "title": "Open Graphs and Computational Reasoning", 
    "arxiv-id": "1007.3794v1", 
    "author": "Aleks Kissinger", 
    "publish": "2010-07-22T03:31:39Z", 
    "summary": "We present a form of algebraic reasoning for computational objects which are\nexpressed as graphs. Edges describe the flow of data between primitive\noperations which are represented by vertices. These graphs have an interface\nmade of half-edges (edges which are drawn with an unconnected end) and enjoy\nrich compositional principles by connecting graphs along these half-edges. In\nparticular, this allows equations and rewrite rules to be specified between\ngraphs. Particular computational models can then be encoded as an axiomatic set\nof such rules. Further rules can be derived graphically and rewriting can be\nused to simulate the dynamics of a computational system, e.g. evaluating a\nprogram on an input. Examples of models which can be formalised in this way\ninclude traditional electronic circuits as well as recent categorical accounts\nof quantum information."
},{
    "category": "hep-th", 
    "doi": "10.1155/2011/152749", 
    "link": "http://arxiv.org/pdf/1104.1187v1", 
    "title": "Computational Tools for Cohomology of Toric Varieties", 
    "arxiv-id": "1104.1187v1", 
    "author": "Thorsten Rahn", 
    "publish": "2011-04-06T20:29:22Z", 
    "summary": "In this review, novel non-standard techniques for the computation of\ncohomology classes on toric varieties are summarized. After an introduction of\nthe basic definitions and properties of toric geometry, we discuss a specific\ncomputational algorithm for the determination of the dimension of line-bundle\nvalued cohomology groups on toric varieties. Applications to the computation of\nchiral massless matter spectra in string compactifications are discussed and,\nusing the software package cohomCalg, its utility is highlighted on a new\ntarget space dual pair of (0,2) heterotic string models."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.amc.2006.11.114", 
    "link": "http://arxiv.org/pdf/1104.1696v1", 
    "title": "Symbolic computation of weighted Moore-Penrose inverse using   partitioning method", 
    "arxiv-id": "1104.1696v1", 
    "author": "M. D", 
    "publish": "2011-04-09T11:54:31Z", 
    "summary": "We propose a method and algorithm for computing the weighted Moore-Penrose\ninverse of one-variable rational matrices. Continuing this idea, we develop an\nalgorithm for computing the weighted Moore-Penrose inverse of one-variable\npolynomial matrix. These methods and algorithms are generalizations of the\nmethod for computing the weighted Moore-Penrose inverse for constant matrices,\noriginated in Wang and Chen [G.R. Wang, Y.L. Chen, A recursive algorithm for\ncomputing the weighted Moore-Penrose inverse AMN, J. Comput. Math. 4 (1986)\n74-85], and the partitioning method for computing the Moore-Penrose inverse of\nrational and polynomial matrices introduced in Stanimirovic and Tasic [P.S.\nStanimirovic, M.B. Tasic, Partitioning method for rational and polynomial\nmatrices, Appl. Math. Comput. 155 (2004) 137-163]. Algorithms are implemented\nin the symbolic computational package MATHEMATICA."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.amc.2006.11.114", 
    "link": "http://arxiv.org/pdf/1104.4518v2", 
    "title": "Parallel Breadth-First Search on Distributed Memory Systems", 
    "arxiv-id": "1104.4518v2", 
    "author": "Kamesh Madduri", 
    "publish": "2011-04-22T23:42:40Z", 
    "summary": "Data-intensive, graph-based computations are pervasive in several scientific\napplications, and are known to to be quite challenging to implement on\ndistributed memory systems. In this work, we explore the design space of\nparallel algorithms for Breadth-First Search (BFS), a key subroutine in several\ngraph algorithms. We present two highly-tuned parallel approaches for BFS on\nlarge parallel systems: a level-synchronous strategy that relies on a simple\nvertex-based partitioning of the graph, and a two-dimensional sparse\nmatrix-partitioning-based approach that mitigates parallel communication\noverhead. For both approaches, we also present hybrid versions with intra-node\nmultithreading. Our novel hybrid two-dimensional algorithm reduces\ncommunication times by up to a factor of 3.5, relative to a common vertex based\napproach. Our experimental study identifies execution regimes in which these\napproaches will be competitive, and we demonstrate extremely high performance\non leading distributed-memory parallel systems. For instance, for a 40,000-core\nparallel execution on Hopper, an AMD Magny-Cours based system, we achieve a BFS\nperformance rate of 17.8 billion edge visits per second on an undirected graph\nof 4.3 billion vertices and 68.7 billion edges with skewed degree distribution."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.66.10", 
    "link": "http://arxiv.org/pdf/1109.0783v1", 
    "title": "Specific \"scientific\" data structures, and their processing", 
    "arxiv-id": "1109.0783v1", 
    "author": "Jerzy Karczmarczuk", 
    "publish": "2011-09-05T01:57:07Z", 
    "summary": "Programming physicists use, as all programmers, arrays, lists, tuples,\nrecords, etc., and this requires some change in their thought patterns while\nconverting their formulae into some code, since the \"data structures\" operated\nupon, while elaborating some theory and its consequences, are rather: power\nseries and Pad\\'e approximants, differential forms and other instances of\ndifferential algebras, functionals (for the variational calculus), trajectories\n(solutions of differential equations), Young diagrams and Feynman graphs, etc.\nSuch data is often used in a [semi-]numerical setting, not necessarily\n\"symbolic\", appropriate for the computer algebra packages. Modules adapted to\nsuch data may be \"just libraries\", but often they become specific, embedded\nsub-languages, typically mapped into object-oriented frameworks, with\noverloaded mathematical operations. Here we present a functional approach to\nthis philosophy. We show how the usage of Haskell datatypes and - fundamental\nfor our tutorial - the application of lazy evaluation makes it possible to\noperate upon such data (in particular: the \"infinite\" sequences) in a natural\nand comfortable manner."
},{
    "category": "cs.DC", 
    "doi": "10.1137/110848244", 
    "link": "http://arxiv.org/pdf/1109.3739v2", 
    "title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing:   Implementation and Experiments", 
    "arxiv-id": "1109.3739v2", 
    "author": "John Gilbert", 
    "publish": "2011-09-16T23:25:28Z", 
    "summary": "Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key\nprimitive for many high performance graph algorithms as well as for some linear\nsolvers, such as algebraic multigrid. Here we show that SpGEMM also yields\nefficient algorithms for general sparse-matrix indexing in distributed memory,\nprovided that the underlying SpGEMM implementation is sufficiently flexible and\nscalable. We demonstrate that our parallel SpGEMM methods, which use\ntwo-dimensional block data distributions with serial hypersparse kernels, are\nindeed highly flexible, scalable, and memory-efficient in the general case.\nThis algorithm is the first to yield increasing speedup on an unbounded number\nof processors; our experiments show scaling up to thousands of processors in a\nvariety of test scenarios."
},{
    "category": "cs.DS", 
    "doi": "10.1137/110848244", 
    "link": "http://arxiv.org/pdf/1109.5981v2", 
    "title": "LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined   Systems", 
    "arxiv-id": "1109.5981v2", 
    "author": "Michael W. Mahoney", 
    "publish": "2011-09-27T18:06:44Z", 
    "summary": "We describe a parallel iterative least squares solver named \\texttt{LSRN}\nthat is based on random normal projection. \\texttt{LSRN} computes the\nmin-length solution to $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$, where $A \\in\n\\mathbb{R}^{m \\times n}$ with $m \\gg n$ or $m \\ll n$, and where $A$ may be\nrank-deficient. Tikhonov regularization may also be included. Since $A$ is only\ninvolved in matrix-matrix and matrix-vector multiplications, it can be a dense\nor sparse matrix or a linear operator, and \\texttt{LSRN} automatically speeds\nup when $A$ is sparse or a fast linear operator. The preconditioning phase\nconsists of a random normal projection, which is embarrassingly parallel, and a\nsingular value decomposition of size $\\lceil \\gamma \\min(m,n) \\rceil \\times\n\\min(m,n)$, where $\\gamma$ is moderately larger than 1, e.g., $\\gamma = 2$. We\nprove that the preconditioned system is well-conditioned, with a strong\nconcentration result on the extreme singular values, and hence that the number\nof iterations is fully predictable when we apply LSQR or the Chebyshev\nsemi-iterative method. As we demonstrate, the Chebyshev method is particularly\nefficient for solving large problems on clusters with high communication cost.\nNumerical results demonstrate that on a shared-memory machine, \\texttt{LSRN}\noutperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash\n(SuiteSparseQR) on sparse problems. Further experiments demonstrate that\n\\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-31374-5_14", 
    "link": "http://arxiv.org/pdf/1204.0053v2", 
    "title": "Theory Presentation Combinators", 
    "arxiv-id": "1204.0053v2", 
    "author": "Russell O'Connor", 
    "publish": "2012-03-31T00:56:44Z", 
    "summary": "We motivate and give semantics to theory presentation combinators as the\nfoundational building blocks for a scalable library of theories. The key\nobservation is that the category of contexts and fibered categories are the\nideal theoretical tools for this purpose."
},{
    "category": "cs.CE", 
    "doi": "10.1088/1749-4699/5/1/014006", 
    "link": "http://arxiv.org/pdf/1204.0267v2", 
    "title": "Computational science and re-discovery: open-source implementations of   ellipsoidal harmonics for problems in potential theory", 
    "arxiv-id": "1204.0267v2", 
    "author": "Matthew G. Knepley", 
    "publish": "2012-04-01T21:23:23Z", 
    "summary": "We present two open-source (BSD) implementations of ellipsoidal harmonic\nexpansions for solving problems of potential theory using separation of\nvariables. Ellipsoidal harmonics are used surprisingly infrequently,\nconsidering their substantial value for problems ranging in scale from\nmolecules to the entire solar system. In this article, we suggest two possible\nreasons for the paucity relative to spherical harmonics. The first is\nessentially historical---ellipsoidal harmonics developed during the late 19th\ncentury and early 20th, when it was found that only the lowest-order harmonics\nare expressible in closed form. Each higher-order term requires the solution of\nan eigenvalue problem, and tedious manual computation seems to have discouraged\napplications and theoretical studies. The second explanation is practical: even\nwith modern computers and accurate eigenvalue algorithms, expansions in\nellipsoidal harmonics are significantly more challenging to compute than those\nin Cartesian or spherical coordinates. The present implementations reduce the\n\"barrier to entry\" by providing an easy and free way for the community to begin\nusing ellipsoidal harmonics in actual research. We demonstrate our\nimplementation using the specific and physiologically crucial problem of how\ncharged proteins interact with their environment, and ask: what other\nanalytical tools await re-discovery in an era of inexpensive computation?"
},{
    "category": "cs.DC", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1204.3052v1", 
    "title": "Heterogeneous Highly Parallel Implementation of Matrix Exponentiation   Using GPU", 
    "arxiv-id": "1204.3052v1", 
    "author": "Prakash S Raghavendra", 
    "publish": "2012-04-13T17:13:50Z", 
    "summary": "The vision of super computer at every desk can be realized by powerful and\nhighly parallel CPUs or GPUs or APUs. Graphics processors once specialized for\nthe graphics applications only, are now used for the highly computational\nintensive general purpose applications. Very expensive GFLOPs and TFLOP\nperformance has become very cheap with the GPGPUs. Current work focuses mainly\non the highly parallel implementation of Matrix Exponentiation. Matrix\nExponentiation is widely used in many areas of scientific community ranging\nfrom highly critical flight, CAD simulations to financial, statistical\napplications. Proposed solution for Matrix Exponentiation uses OpenCL for\nexploiting the hyper parallelism offered by the many core GPGPUs. It employs\nmany general GPU optimizations and architectural specific optimizations. This\nexperimentation covers the optimizations targeted specific to the Scientific\nGraphics cards (Tesla-C2050). Heterogeneous Highly Parallel Matrix\nExponentiation method has been tested for matrices of different sizes and with\ndifferent powers. The devised Kernel has shown 1000X speedup and 44 fold\nspeedup with the naive GPU Kernel."
},{
    "category": "math.CO", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1207.0398v1", 
    "title": "Multivariate Polynomials in Sage", 
    "arxiv-id": "1207.0398v1", 
    "author": "Viviane Pons", 
    "publish": "2012-07-02T14:19:23Z", 
    "summary": "We have developed a patch implementing multivariate polynomials seen as a\nmulti-base algebra. The patch is to be released into the software Sage and can\nalready be found within the Sage-Combinat distribution. One can use our patch\nto define a polynomial in a set of indexed variables and expand it into a\nlinear basis of the multivariate polynomials. So far, we have the Schubert\npolynomials, the Key polynomials of types A, B, C, or D, the Grothendieck\npolynomials and the non-symmetric Macdonald polynomials. One can also use a\ndouble set of variables and work with specific double-linear bases like the\ndouble Schubert polynomials or double Grothendieck polynomials. Our\nimplementation is based on a definition of the basis using divided difference\noperators and one can also define new bases using these operators."
},{
    "category": "cs.ET", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1207.1161v1", 
    "title": "Modular Arithmetic Expressions and Primality Testing via DNA   Self-Assembly", 
    "arxiv-id": "1207.1161v1", 
    "author": "Jaley Dholakiya", 
    "publish": "2012-07-05T04:55:03Z", 
    "summary": "Self-assembly is a fundamental process by which supramolecular species form\nspontaneously from their components. This process is ubiquitous throughout the\nlife chemistry and is central to biological information processing. Algorithms\nfor solving many mathematical and computational problems via tile self assembly\nhave been proposed by many researchers in the last decade. In particular tile\nset for doing basic arithmetic of two inputs have been given. In this work we\ngive tile set for doing basic arithmetic (addition, subtraction,\nmultiplication) of n inputs and subsequently computing its modulo. We also\npresent a tile set for primality testing. Finally we present a software\n'xtilemod' for doing modular arithmetic. This simplifies the task of creating\nthe input files to xgrow simulator for doing basic (addition, subtraction,\nmultiplication and division) as well as modular arithmetic of n inputs. Similar\nsoftware for creating tile set for primality testing is also given."
},{
    "category": "cs.MS", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1207.1380v1", 
    "title": "Bayes Blocks: An Implementation of the Variational Bayesian Building   Blocks Framework", 
    "arxiv-id": "1207.1380v1", 
    "author": "Juha Karhunen", 
    "publish": "2012-07-04T16:10:18Z", 
    "summary": "A software library for constructing and learning probabilistic models is\npresented. The library offers a set of building blocks from which a large\nvariety of static and dynamic models can be built. These include hierarchical\nmodels for variances of other variables and many nonlinear models. The\nunderlying variational Bayesian machinery, providing for fast and robust\nestimation but being mathematically rather involved, is almost completely\nhidden from the user thus making it very easy to use the library. The building\nblocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables\nand computational nodes which can be combined rather freely."
},{
    "category": "cs.LG", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1207.1413v1", 
    "title": "Discovery of non-gaussian linear causal models using ICA", 
    "arxiv-id": "1207.1413v1", 
    "author": "Patrik O. Hoyer", 
    "publish": "2012-07-04T16:23:35Z", 
    "summary": "In recent years, several methods have been proposed for the discovery of\ncausal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).\nSuch methods make various assumptions on the data generating process to\nfacilitate its identification from purely observational data. Continuing this\nline of research, we show how to discover the complete causal structure of\ncontinuous-valued data, under the assumptions that (a) the data generating\nprocess is linear, (b) there are no unobserved confounders, and (c) disturbance\nvariables have non-gaussian distributions of non-zero variances. The solution\nrelies on the use of the statistical method known as independent component\nanalysis (ICA), and does not require any pre-specified time-ordering of the\nvariables. We provide a complete Matlab package for performing this LiNGAM\nanalysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the\neffectiveness of the method using artificially generated data."
},{
    "category": "cs.MS", 
    "doi": "10.5121/ijdps.2012.3209", 
    "link": "http://arxiv.org/pdf/1207.1746v1", 
    "title": "A Generic Library for Stencil Computations", 
    "arxiv-id": "1207.1746v1", 
    "author": "Ugo Varetto", 
    "publish": "2012-07-06T23:30:06Z", 
    "summary": "In this era of diverse and heterogeneous computer architectures, the\nprogrammability issues, such as productivity and portable efficiency, are\ncrucial to software development and algorithm design. One way to approach the\nproblem is to step away from traditional sequential programming languages and\nmove toward domain specific programming environments to balance between\nexpressivity and efficiency. In order to demonstrate this principle, we\ndeveloped a domain specific C++ generic library for stencil computations, like\nPDE solvers. The library features high level constructs to specify computation\nand allows the development of parallel stencil computations with very limited\neffort. The high abstraction constructs (like do_all and do_reduce) make the\nprogram shorter and cleaner with increased contextual information for better\nperformance exploitation. The results show good performance from Windows\nmulticores, to HPC clusters and machines with accelerators, like GPUs."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-31374-5_16", 
    "link": "http://arxiv.org/pdf/1207.2300v1", 
    "title": "Towards the Formal Specification and Verification of Maple Programs", 
    "arxiv-id": "1207.2300v1", 
    "author": "Wolfgang Schreiner", 
    "publish": "2012-07-10T10:35:17Z", 
    "summary": "In this paper, we present our ongoing work and initial results on the formal\nspecification and verification of MiniMaple (a substantial subset of Maple with\nslight extensions) programs. The main goal of our work is to find behavioral\nerrors in such programs w.r.t. their specifications by static analysis. This\ntask is more complex for widely used computer algebra languages like Maple as\nthese are fundamentally different from classical languages: they support\nnon-standard types of objects such as symbols, unevaluated expressions and\npolynomials and require abstract computer algebraic concepts and objects such\nas rings and orderings etc. As a starting point we have defined and formalized\na syntax, semantics, type system and specification language for MiniMaple."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-642-31374-5_15", 
    "link": "http://arxiv.org/pdf/1207.3315v1", 
    "title": "Verifying an algorithm computing Discrete Vector Fields for digital   imaging", 
    "arxiv-id": "1207.3315v1", 
    "author": "Julio Rubio", 
    "publish": "2012-07-13T17:42:52Z", 
    "summary": "In this paper, we present a formalization of an algorithm to construct\nadmissible discrete vector fields in the Coq theorem prover taking advantage of\nthe SSReflect library. Discrete vector fields are a tool which has been\nwelcomed in the homological analysis of digital images since it provides a\nprocedure to reduce the amount of information but preserving the homological\nproperties. In particular, thanks to discrete vector fields, we are able to\ncompute, inside Coq, homological properties of biomedical images which\notherwise are out of the reach of this system."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-642-31374-5_15", 
    "link": "http://arxiv.org/pdf/1207.3441v1", 
    "title": "Isabelle/jEdit --- a Prover IDE within the PIDE framework", 
    "arxiv-id": "1207.3441v1", 
    "author": "Makarius Wenzel", 
    "publish": "2012-07-14T16:47:03Z", 
    "summary": "PIDE is a general framework for document-oriented prover interaction and\nintegration, based on a bilingual architecture that combines ML and Scala. The\noverall aim is to connect LCF-style provers like Isabelle (or Coq or HOL) with\nsophisticated front-end technology on the JVM platform, overcoming command-line\ninteraction at last.\n  The present system description specifically covers Isabelle/jEdit as part of\nthe official release of Isabelle2011-1 (October 2011). It is a concrete Prover\nIDE implementation based on Isabelle/PIDE library modules (implemented in\nScala) on the one hand, and the well-known text editor framework of jEdit\n(implemented in Java) on the other hand.\n  The interaction model of our Prover IDE follows the idea of continuous proof\nchecking: the theory source text is annotated by semantic information by the\nprover as it becomes available incrementally. This works via an asynchronous\nprotocol that neither blocks the editor nor stops the prover from exploiting\nparallelism on multi-core hardware. The jEdit GUI provides standard metaphors\nfor augmented text editing (highlighting, squiggles, tooltips, hyperlinks etc.)\nthat we have instrumented to render the formal content from the prover context.\nFurther refinement of the jEdit display engine via suitable plugins and fonts\napproximates mathematical rendering in the text buffer, including symbols from\nthe TeX repertoire, and sub-/superscripts.\n  Isabelle/jEdit is presented here both as a usable interface for current\nIsabelle, and as a reference application to inspire further projects based on\nPIDE."
},{
    "category": "cs.SE", 
    "doi": "10.4204/EPTCS.105.8", 
    "link": "http://arxiv.org/pdf/1301.0044v1", 
    "title": "Formal Model-Driven Engineering: Generating Data and Behavioural   Components", 
    "arxiv-id": "1301.0044v1", 
    "author": "Jim Davies", 
    "publish": "2013-01-01T01:54:57Z", 
    "summary": "Model-driven engineering is the automatic production of software artefacts\nfrom abstract models of structure and functionality. By targeting a specific\nclass of system, it is possible to automate aspects of the development process,\nusing model transformations and code generators that encode domain knowledge\nand implementation strategies. Using this approach, questions of correctness\nfor a complex, software system may be answered through analysis of abstract\nmodels of lower complexity, under the assumption that the transformations and\ngenerators employed are themselves correct. This paper shows how formal\ntechniques can be used to establish the correctness of model transformations\nused in the generation of software components from precise object models. The\nsource language is based upon existing, formal techniques; the target language\nis the widely-used SQL notation for database programming. Correctness is\nestablished by giving comparable, relational semantics to both languages, and\nchecking that the transformations are semantics-preserving."
},{
    "category": "cs.PL", 
    "doi": "10.4204/EPTCS.105.8", 
    "link": "http://arxiv.org/pdf/1301.0114v1", 
    "title": "Tree-based Arithmetic and Compressed Representations of Giant Numbers", 
    "arxiv-id": "1301.0114v1", 
    "author": "Paul Tarau", 
    "publish": "2013-01-01T18:29:39Z", 
    "summary": "Can we do arithmetic in a completely different way, with a radically\ndifferent data structure? Could this approach provide practical benefits, like\noperations on giant numbers while having an average performance similar to\ntraditional bitstring representations?\n  While answering these questions positively, our tree based representation\ndescribed in this paper comes with a few extra benefits: it compresses giant\nnumbers such that, for instance, the largest known prime number as well as its\nrelated perfect number are represented as trees of small sizes. The same also\napplies to Fermat numbers and important computations like exponentiation of two\nbecome constant time operations.\n  At the same time, succinct representations of sparse sets, multisets and\nsequences become possible through bijections to our tree-represented natural\nnumbers."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.105.8", 
    "link": "http://arxiv.org/pdf/1301.0128v1", 
    "title": "Binary Tree Arithmetic with Generalized Constructors", 
    "arxiv-id": "1301.0128v1", 
    "author": "Paul Tarau", 
    "publish": "2013-01-01T19:58:26Z", 
    "summary": "We describe arithmetic computations in terms of operations on some well known\nfree algebras (S1S, S2S and ordered rooted binary trees) while emphasizing the\ncommon structure present in all them when seen as isomorphic with the set of\nnatural numbers.\n  Constructors and deconstructors seen through an initial algebra semantics are\ngeneralized to recursively defined functions obeying similar laws.\n  Implementation using Scala's apply and unapply are discussed together with an\napplication to a realistic arbitrary size arithmetic package written in Scala,\nbased on the free algebra of rooted ordered binary trees, which also supports\nrational number operations through an extension to signed rationals of the\nCalkin-Wilf bijection."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.105.8", 
    "link": "http://arxiv.org/pdf/1301.0129v1", 
    "title": "On Two Infinite Families of Pairing Bijections", 
    "arxiv-id": "1301.0129v1", 
    "author": "Paul Tarau", 
    "publish": "2013-01-01T20:10:51Z", 
    "summary": "We describe two general mechanisms for producing pairing bijections\n(bijective functions defined from N x N to N).\n  The first mechanism, using n-adic valuations results in parameterized\nalgorithms generating a countable family of distinct pairing bijections.\n  The second mechanism, using characteristic functions of subsets of N provides\n2^N distinct pairing bijections.\n  Mechanisms to combine such pairing functions and their application to\ngenerate families of permutations of N are also described.\n  The paper uses a small subset of the functional language Haskell to provide\ntype checked executable specifications of all the functions defined in a\nliterate programming style. The self-contained Haskell code extracted from the\npaper is available at http://logic.cse.unt.edu/tarau/research/2012/infpair.hs ."
},{
    "category": "physics.comp-ph", 
    "doi": "10.3233/SPR-140379", 
    "link": "http://arxiv.org/pdf/1301.1334v2", 
    "title": "Object-oriented implementations of the MPDATA advection equation solver   in C++, Python and Fortran", 
    "arxiv-id": "1301.1334v2", 
    "author": "Maciej Fija\u0142kowski", 
    "publish": "2013-01-07T20:59:13Z", 
    "summary": "Three object-oriented implementations of a prototype solver of the advection\nequation are introduced. The presented programs are based on Blitz++ (C++),\nNumPy (Python), and Fortran's built-in array containers. The solvers include an\nimplementation of the Multidimensional Positive-Definite Advective Transport\nAlgorithm (MPDATA). The introduced codes exemplify how the application of\nobject-oriented programming (OOP) techniques allows to reproduce the\nmathematical notation used in the literature within the program code. A\ndiscussion on the tradeoffs of the programming language choice is presented.\nThe main angles of comparison are code brevity and syntax clarity (and hence\nmaintainability and auditability) as well as performance. In the case of\nPython, a significant performance gain is observed when switching from the\nstandard interpreter (CPython) to the PyPy implementation of Python. Entire\nsource code of all three implementations is embedded in the text and is\nlicensed under the terms of the GNU GPL license."
},{
    "category": "math.NA", 
    "doi": "10.1007/s11075-014-9907-z", 
    "link": "http://arxiv.org/pdf/1301.2102v3", 
    "title": "A block MINRES algorithm based on the banded Lanczos method", 
    "arxiv-id": "1301.2102v3", 
    "author": "Kirk M. Soodhalter", 
    "publish": "2013-01-10T12:22:23Z", 
    "summary": "We develop a block minimum residual (MINRES) algorithm for symmetric\nindefinite matrices. This version is built upon the band Lanczos method that\ngenerates one basis vector of the block Krylov subspace per iteration rather\nthan a whole block as in the block Lanczos process. However, we modify the\nmethod such that the most expensive operations are still performed in a block\nfashion. The benefit of using the band Lanczos method is that one can detect\nbreakdowns from scalar values arising in the computation, allowing for a\nhandling of breakdown which is straightforward to implement.\n  We derive a progressive formulation of the MINRES method based on the band\nLanczos process and give some implementation details. Specifically, a simple\nreordering of the steps allows us to perform many of the operations at the\nblock level in order to take advantage of communication efficiencies offered by\nthe block Lanczos process. This is an important concern in the context of\nnext-generation super computing applications.\n  We also present a technique allowing us to maintain the block size by\nreplacing dependent Lanczos vectors with pregenerated random vectors whose\northogonality against all Lanczos vectors is maintained. Numerical results\nillustrate the performance on some sample problems. We present experiments that\nshow how the relationship between right-hand sides can effect the performance\nof the method."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.apnum.2014.02.006", 
    "link": "http://arxiv.org/pdf/1301.2650v3", 
    "title": "Krylov Subspace Recycling for Sequences of Shifted Linear Systems", 
    "arxiv-id": "1301.2650v3", 
    "author": "Fei Xue", 
    "publish": "2013-01-12T05:36:09Z", 
    "summary": "We study the use of Krylov subspace recycling for the solution of a sequence\nof slowly-changing families of linear systems, where each family consists of\nshifted linear systems that differ in the coefficient matrix only by multiples\nof the identity. Our aim is to explore the simultaneous solution of each family\nof shifted systems within the framework of subspace recycling, using one\naugmented subspace to extract candidate solutions for all the shifted systems.\nThe ideal method would use the same augmented subspace for all systems and have\nfixed storage requirements, independent of the number of shifted systems per\nfamily. We show that a method satisfying both requirements cannot exist in this\nframework.\n  As an alternative, we introduce two schemes. One constructs a separate\ndeflation space for each shifted system but solves each family of shifted\nsystems simultaneously. The other builds only one recycled subspace and\nconstructs approximate corrections to the solutions of the shifted systems at\neach cycle of the iterative linear solver while only minimizing the base system\nresidual. At convergence of the base system solution, we apply the method\nrecursively to the remaining unconverged systems. We present numerical examples\ninvolving systems arising in lattice quantum chromodynamics."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2527267", 
    "link": "http://arxiv.org/pdf/1301.2707v2", 
    "title": "ALGORITHM 937: MINRES-QLP for Singular Symmetric and Hermitian Linear   Equations and Least-Squares Problems", 
    "arxiv-id": "1301.2707v2", 
    "author": "Michael A. Saunders", 
    "publish": "2013-01-12T19:00:15Z", 
    "summary": "We describe algorithm MINRES-QLP and its FORTRAN 90 implementation for\nsolving symmetric or Hermitian linear systems or least-squares problems. If the\nsystem is singular, MINRES-QLP computes the unique minimum-length solution\n(also known as the pseudoinverse solution), which generally eludes MINRES. In\nall cases, it overcomes a potential instability in the original MINRES\nalgorithm. A positive-definite preconditioner may be supplied. Our FORTRAN 90\nimplementation illustrates a design pattern that allows users to make problem\ndata known to the solver but hidden and secure from other program units. In\nparticular, we circumvent the need for reverse communication. While we focus\nhere on a FORTRAN 90 implementation, we also provide and maintain MATLAB\nversions of MINRES and MINRES-QLP."
},{
    "category": "cs.AI", 
    "doi": "10.1145/2527267", 
    "link": "http://arxiv.org/pdf/1301.3863v1", 
    "title": "YGGDRASIL - A Statistical Package for Learning Split Models", 
    "arxiv-id": "1301.3863v1", 
    "author": "Soren Hojsgaard", 
    "publish": "2013-01-16T15:50:42Z", 
    "summary": "There are two main objectives of this paper. The first is to present a\nstatistical framework for models with context specific independence structures,\ni.e., conditional independences holding only for sepcific values of the\nconditioning variables. This framework is constituted by the class of split\nmodels. Split models are extension of graphical models for contigency tables\nand allow for a more sophisticiated modelling than graphical models. The\ntreatment of split models include estimation, representation and a Markov\nproperty for reading off those independencies holding in a specific context.\nThe second objective is to present a software package named YGGDRASIL which is\ndesigned for statistical inference in split models, i.e., for learning such\nmodels on the basis of data."
},{
    "category": "math.OC", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1301.6879v5", 
    "title": "A Unified Software Framework for Empirical Gramians", 
    "arxiv-id": "1301.6879v5", 
    "author": "Mario Ohlberger", 
    "publish": "2013-01-29T10:05:37Z", 
    "summary": "A common approach in model reduction is balanced truncation, which is based\non gramian matrices classifiying certain attributes of states or parameters of\na given dynamic system. Initially restricted to linear systems, the empirical\ngramians not only extended this concept to nonlinear systems, but also provide\na uniform computational method. This work introduces a unified software\nframework supplying routines for six types of empirical gramians. The gramian\ntypes will be discussed and applied in a model reduction framework for\nmultiple-input-multiple-output (MIMO) systems."
},{
    "category": "math.OC", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1301.7283v1", 
    "title": "On the effects of scaling on the performance of Ipopt", 
    "arxiv-id": "1301.7283v1", 
    "author": "J. A. Scott", 
    "publish": "2013-01-30T16:52:05Z", 
    "summary": "The open-source nonlinear solver Ipopt (https://projects.coin-or.org/Ipopt)\nis a widely-used software package for the solution of large-scale non-linear\noptimization problems. At its heart, it employs a third-party linear solver to\nsolve a series of sparse symmetric indefinite systems. The speed, accuracy and\nrobustness of the chosen linear solver is critical to the overall performance\nof Ipopt. In some instances, it can be beneficial to scale the linear system\nbefore it is solved.\n  In this paper, different scaling algorithms are employed within Ipopt with a\nnew linear solver HSL_MA97 from the HSL mathematical software library\n(http://www.hsl.rl.ac.uk). An extensive collection of problems from the CUTEr\ntest set (http://www.cuter.rl.ac.uk) is used to illustrate the effects of\nscaling."
},{
    "category": "cs.PL", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.0864v1", 
    "title": "Efficient Generation of Correctness Certificates for the Abstract Domain   of Polyhedra", 
    "arxiv-id": "1304.0864v1", 
    "author": "Micha\u00ebl P\u00e9rin", 
    "publish": "2013-04-03T08:01:42Z", 
    "summary": "Polyhedra form an established abstract domain for inferring runtime\nproperties of programs using abstract interpretation. Computations on them need\nto be certified for the whole static analysis results to be trusted. In this\nwork, we look at how far we can get down the road of a posteriori verification\nto lower the overhead of certification of the abstract domain of polyhedra. We\ndemonstrate methods for making the cost of inclusion certificate generation\nnegligible. From a performance point of view, our single-representation,\nconstraints-based implementation compares with state-of-the-art\nimplementations."
},{
    "category": "cs.MS", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.0878v2", 
    "title": "C Language Extensions for Hybrid CPU/GPU Programming with StarPU", 
    "arxiv-id": "1304.0878v2", 
    "author": "Ludovic Court\u00e8s", 
    "publish": "2013-04-03T09:11:25Z", 
    "summary": "Modern platforms used for high-performance computing (HPC) include machines\nwith both general-purpose CPUs, and \"accelerators\", often in the form of\ngraphical processing units (GPUs). StarPU is a C library to exploit such\nplatforms. It provides users with ways to define \"tasks\" to be executed on CPUs\nor GPUs, along with the dependencies among them, and by automatically\nscheduling them over all the available processing units. In doing so, it also\nrelieves programmers from the need to know the underlying architecture details:\nit adapts to the available CPUs and GPUs, and automatically transfers data\nbetween main memory and GPUs as needed. While StarPU's approach is successful\nat addressing run-time scheduling issues, being a C library makes for a poor\nand error-prone programming interface. This paper presents an effort started in\n2011 to promote some of the concepts exported by the library as C language\nconstructs, by means of an extension of the GCC compiler suite. Our main\ncontribution is the design and implementation of language extensions that map\nto StarPU's task programming paradigm. We argue that the proposed extensions\nmake it easier to get started with StarPU,eliminate errors that can occur when\nusing the C library, and help diagnose possible mistakes. We conclude on future\nwork."
},{
    "category": "cs.MS", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.1356v1", 
    "title": "The Graph Grammar Library - a generic framework for chemical graph   rewrite systems", 
    "arxiv-id": "1304.1356v1", 
    "author": "Christoph Flamm", 
    "publish": "2013-04-04T13:06:34Z", 
    "summary": "Graph rewrite systems are powerful tools to model and study complex problems\nin various fields of research. Their successful application to chemical\nreaction modelling on a molecular level was shown but no appropriate and simple\nsystem is available at the moment.\n  The presented Graph Grammar Library (GGL) implements a generic Double Push\nOut approach for general graph rewrite systems. The framework focuses on a high\nlevel of modularity as well as high performance, using state-of-the-art\nalgorithms and data structures, and comes with extensive documentation. The\nlarge GGL chemistry module enables extensive and detailed studies of chemical\nsystems. It well meets the requirements and abilities envisioned by Yadav et\nal. (2004) for such chemical rewrite systems. Here, molecules are represented\nas undirected labeled graphs while chemical reactions are described by\naccording graph grammar rules. Beside the graph transformation, the GGL offers\nadvanced cheminformatics algorithms for instance to estimate energies\nofmolecules or aromaticity perception. These features are illustrated using a\nset of reactions from polyketide chemistry a huge class of natural compounds of\nmedical relevance.\n  The graph grammar based simulation of chemical reactions offered by the GGL\nis a powerful tool for extensive cheminformatics studies on a molecular level.\nThe GGL already provides rewrite rules for all enzymes listed in the KEGG\nLIGAND database is freely available at\nhttp://www.tbi.univie.ac.at/software/GGL/."
},{
    "category": "cs.CE", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.2272v1", 
    "title": "Algorithms for Large-scale Whole Genome Association Analysis", 
    "arxiv-id": "1304.2272v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2013-04-08T17:13:39Z", 
    "summary": "In order to associate complex traits with genetic polymorphisms, genome-wide\nassociation studies process huge datasets involving tens of thousands of\nindividuals genotyped for millions of polymorphisms. When handling these\ndatasets, which exceed the main memory of contemporary computers, one faces two\ndistinct challenges: 1) Millions of polymorphisms come at the cost of hundreds\nof Gigabytes of genotype data, which can only be kept in secondary storage; 2)\nthe relatedness of the test population is represented by a covariance matrix,\nwhich, for large populations, can only fit in the combined main memory of a\ndistributed architecture. In this paper, we present solutions for both\nchallenges: The genotype data is streamed from and to secondary storage using a\ndouble buffering technique, while the covariance matrix is kept across the main\nmemory of a distributed memory system. We show that these methods sustain\nhigh-performance and allow the analysis of enormous dataset"
},{
    "category": "cs.LG", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.6899v1", 
    "title": "An implementation of the relational k-means algorithm", 
    "arxiv-id": "1304.6899v1", 
    "author": "Bal\u00e1zs Szalkai", 
    "publish": "2013-04-25T12:59:31Z", 
    "summary": "A C# implementation of a generalized k-means variant called relational\nk-means is described here. Relational k-means is a generalization of the\nwell-known k-means clustering method which works for non-Euclidean scenarios as\nwell. The input is an arbitrary distance matrix, as opposed to the traditional\nk-means method, where the clustered objects need to be identified with vectors."
},{
    "category": "cs.MS", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.7053v1", 
    "title": "A GEMM interface and implementation on NVIDIA GPUs for multiple small   matrices", 
    "arxiv-id": "1304.7053v1", 
    "author": "Paul Mullowney", 
    "publish": "2013-04-26T02:22:14Z", 
    "summary": "We present an interface and an implementation of the General Matrix Multiply\n(GEMM) routine for multiple small matrices processed simultaneously on NVIDIA\ngraphics processing units (GPUs). We focus on matrix sizes under 16. The\nimplementation can be easily extended to larger sizes. For single precision\nmatrices, our implementation is 30% to 600% faster than the batched cuBLAS\nimplementation distributed in the CUDA Toolkit 5.0 on NVIDIA Tesla K20c. For\nexample, we obtain 104 GFlop/s and 216 GFlop/s when multiplying 100,000\nindependent matrix pairs of size 10 and 16, respectively. Similar improvement\nin performance is obtained for other sizes, in single and double precision for\nreal and complex types, and when the number of matrices is smaller. Apart from\nour implementation, our different function interface also plays an important\nrole in the improved performance. Applications of this software include Finite\nElement computation on GPUs."
},{
    "category": "cs.MS", 
    "doi": "10.1155/2013/365909", 
    "link": "http://arxiv.org/pdf/1304.7054v1", 
    "title": "Batched Kronecker product for 2-D matrices and 3-D arrays on NVIDIA GPUs", 
    "arxiv-id": "1304.7054v1", 
    "author": "Chetan Jhurani", 
    "publish": "2013-04-26T02:22:25Z", 
    "summary": "We describe an interface and an implementation for performing Kronecker\nproduct actions on NVIDIA GPUs for multiple small 2-D matrices and 3-D arrays\nprocessed in parallel as a batch. This method is suited to cases where the\nKronecker product component matrices are identical but the operands in a\nmatrix-free application vary in the batch. Any batched GEMM (General Matrix\nMultiply) implementation, for example ours [1] or the one in cuBLAS, can also\nbe used for performing batched Kronecker products on GPUs. However, the\nspecialized implementation presented here is faster and uses less memory.\nPartly this is because a simple GEMM based approach would require extra copies\nto and from main memory. We focus on matrix sizes less than or equal to 16,\nsince these are the typical polynomial degrees in Finite Elements, but the\nimplementation can be easily extended for other sizes. We obtain 143 and 285\nGFlop/s for single precision real when processing matrices of size 10 and 16,\nrespectively on NVIDIA Tesla K20c using CUDA 5.0. The corresponding speeds for\n3-D array Kronecker products are 126 and 268 GFlop/s, respectively. Double\nprecision is easily supported using the C++ template mechanism."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1304.7855v1", 
    "title": "Enhancements to ACL2 in Versions 5.0, 6.0, and 6.1", 
    "arxiv-id": "1304.7855v1", 
    "author": "J Strother Moore", 
    "publish": "2013-04-30T04:13:58Z", 
    "summary": "We report on highlights of the ACL2 enhancements introduced in ACL2 releases\nsince the 2011 ACL2 Workshop. Although many enhancements are critical for\nsoundness or robustness, we focus in this paper on those improvements that\ncould benefit users who are aware of them, but that might not be discovered in\neveryday practice."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1305.1422v3", 
    "title": "Somoclu: An Efficient Parallel Library for Self-Organizing Maps", 
    "arxiv-id": "1305.1422v3", 
    "author": "Li Zhao", 
    "publish": "2013-05-07T06:43:26Z", 
    "summary": "Somoclu is a massively parallel tool for training self-organizing maps on\nlarge data sets written in C++. It builds on OpenMP for multicore execution,\nand on MPI for distributing the workload across the nodes in a cluster. It is\nalso able to boost training by using CUDA if graphics processing units are\navailable. A sparse kernel is included, which is useful for high-dimensional\nbut sparse data, such as the vector spaces common in text mining workflows.\nPython, R and MATLAB interfaces facilitate interactive use. Apart from fast\nexecution, memory use is highly optimized, enabling training large emergent\nmaps even on a single computer."
},{
    "category": "cs.NA", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1305.3122v1", 
    "title": "An efficient way to perform the assembly of finite element matrices in   Matlab and Octave", 
    "arxiv-id": "1305.3122v1", 
    "author": "Gilles Scarella", 
    "publish": "2013-05-14T11:52:17Z", 
    "summary": "We describe different optimization techniques to perform the assembly of\nfinite element matrices in Matlab and Octave, from the standard approach to\nrecent vectorized ones, without any low level language used. We finally obtain\na simple and efficient vectorized algorithm able to compete in performance with\ndedicated software such as FreeFEM++. The principle of this assembly algorithm\nis general, we present it for different matrices in the P1 finite elements case\nand in linear elasticity. We present numerical results which illustrate the\ncomputational costs of the different approaches"
},{
    "category": "physics.comp-ph", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1305.3625v1", 
    "title": "Making the case of GPUs in courses on computational physics", 
    "arxiv-id": "1305.3625v1", 
    "author": "Knut Skogstrand Gjerden", 
    "publish": "2013-05-15T20:16:42Z", 
    "summary": "Most relatively modern desktop or even laptop computers contain a graphics\ncard useful for more than showing colors on a screen. In this paper, we make a\ncase for why you should learn enough about GPU (graphics processing unit)\ncomputing to use as an accelerator or even replacement to your CPU code. We\ninclude an example of our own as a case study to show what can be realistically\nexpected."
},{
    "category": "cs.DC", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1305.5120v1", 
    "title": "A Parallel and Scalable Iterative Solver for Sequences of Dense   Eigenproblems Arising in FLAPW", 
    "arxiv-id": "1305.5120v1", 
    "author": "Edoardo Di Napoli", 
    "publish": "2013-05-21T10:08:01Z", 
    "summary": "In one of the most important methods in Density Functional Theory - the\nFull-Potential Linearized Augmented Plane Wave (FLAPW) method - dense\ngeneralized eigenproblems are organized in long sequences. Moreover each\neigenproblem is strongly correlated to the next one in the sequence. We propose\na novel approach which exploits such correlation through the use of an\neigensolver based on subspace iteration and accelerated with Chebyshev\npolynomials. The resulting solver, parallelized using the Elemental library\nframework, achieves excellent scalability and is competitive with current dense\nparallel eigensolvers."
},{
    "category": "cs.DS", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1306.1128v1", 
    "title": "Arithmetic Algorithms for Hereditarily Binary Natural Numbers", 
    "arxiv-id": "1306.1128v1", 
    "author": "Paul Tarau", 
    "publish": "2013-06-05T14:56:04Z", 
    "summary": "We study some essential arithmetic properties of a new tree-based number\nrepresentation, {\\em hereditarily binary numbers}, defined by applying\nrecursively run-length encoding of bijective base-2 digits.\n  Our representation expresses giant numbers like the largest known prime\nnumber and its related perfect number as well as the largest known Woodall,\nCullen, Proth, Sophie Germain and twin primes as trees of small sizes.\n  More importantly, our number representation supports novel algorithms that,\nin the best case, collapse the complexity of various computations by\nsuper-exponential factors and in the worse case are within a constant factor of\ntheir traditional counterparts.\n  As a result, it opens the door to a new world, where arithmetic operations\nare limited by the structural complexity of their operands, rather than their\nbitsizes."
},{
    "category": "astro-ph.IM", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1306.5771v1", 
    "title": "Panphasia: a user guide", 
    "arxiv-id": "1306.5771v1", 
    "author": "Stephen Booth", 
    "publish": "2013-06-24T20:22:29Z", 
    "summary": "We make a very large realisation of a Gaussian white noise field, called\nPANPHASIA, public by releasing software that computes this field. Panphasia is\ndesigned specifically for setting up Gaussian initial conditions for\ncosmological simulations and resimulations of structure formation. We make\navailable both software to compute the field itself and codes to illustrate\napplications including a modified version of a public serial initial conditions\ngenerator. We document the software and present the results of a few basic\ntests of the field. The properties and method of construction of Panphasia are\ndescribed in full in a companion paper Jenkins 2013."
},{
    "category": "math.NA", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1306.6291v4", 
    "title": "A Method for Fast Diagonalization of a 2x2 or 3x3 Real Symmetric Matrix", 
    "arxiv-id": "1306.6291v4", 
    "author": "M. J. Kronenburg", 
    "publish": "2013-06-26T16:57:40Z", 
    "summary": "A method is presented for fast diagonalization of a 2x2 or 3x3 real symmetric\nmatrix, that is determination of its eigenvalues and eigenvectors. The Euler\nangles of the eigenvectors are computed. A small computer algebra program is\nused to compute some of the identities, and a C++ program for testing the\nformulas has been uploaded to arXiv."
},{
    "category": "cs.CE", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1306.6675v1", 
    "title": "Next generation input-output data format for HEP using Google's protocol   buffers", 
    "arxiv-id": "1306.6675v1", 
    "author": "S. V. Chekanov", 
    "publish": "2013-06-27T22:30:55Z", 
    "summary": "We propose a data format for Monte Carlo (MC) events, or any structural data,\nincluding experimental data, in a compact binary form using variable-size\ninteger encoding as implemented in the Google's Protocol Buffers package. This\napproach is implemented in the so-called ProMC library which produces smaller\nfile sizes for MC records compared to the existing input-output libraries used\nin high-energy physics (HEP). Other important features are a separation of\nabstract data layouts from concrete programming implementations,\nself-description and random access. Data stored in ProMC files can be written,\nread and manipulated in a number of programming languages, such C++, Java and\nPython."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.114.1", 
    "link": "http://arxiv.org/pdf/1307.1335v1", 
    "title": "Investigating independent subsets of graphs, with Mathematica", 
    "arxiv-id": "1307.1335v1", 
    "author": "Ottavio M. D'Antona", 
    "publish": "2013-07-04T14:06:46Z", 
    "summary": "With this work we aim to show how Mathematica can be a useful tool to\ninvestigate properties of combinatorial structures. Specifically, we will face\nenumeration problems on independent subsets of powers of paths and cycles,\ntrying to highlight the correspondence with other combinatorial objects with\nthe same cardinality. Then we will study the structures obtained by ordering\nproperly independent subsets of paths and cycles. We will approach some\nenumeration problems on the resulting partially ordered sets, putting in\nevidence the correspondences with structures known as Fibonacci and Lucas\nCubes."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.118", 
    "link": "http://arxiv.org/pdf/1307.1528v1", 
    "title": "Proceedings 10th International Workshop On User Interfaces for Theorem   Provers", 
    "arxiv-id": "1307.1528v1", 
    "author": "Christoph L\u00fcth", 
    "publish": "2013-07-05T06:21:33Z", 
    "summary": "This EPTCS volume collects the post-proceedings of the 10th International\nWorkshop On User Interfaces for Theorem Provers (UITP 2012), held as part of\nthe Conferences on Intelligent Computer Mathematics (CICM 2012) in Bremen on\nJuly 11th 2012. The UITP workshop series aims at bringing together reasearchers\ninterested in designing, developing and evaluating interfaces for interactive\nproof systems, such as theorem provers, formal method tools, and other tools\nmanipulating and presenting mathematical formulae. Started in 1995, it can look\nback on seventeen years of history by now.\n  The papers in the present volume give a good indication of the range of\nquestions currently addressed in the UITP community; this ranges from interface\ndesign (Windsteiger; Dunchev et al) to using technologies such as machine\nlearning to assist the user (Komendantskaya et al). The web features\nprominently (Tankink), and new technology necessitates changes right down to\nthe very basic modes of interaction (Wenzel) - the old REPL (read, evaluate,\nprint, loop) mode of interaction can not take advantage of modern technology,\nsuch as the web and multi-core machines."
},{
    "category": "cs.LO", 
    "doi": "10.4204/EPTCS.118.1", 
    "link": "http://arxiv.org/pdf/1307.1942v1", 
    "title": "PROOFTOOL: a GUI for the GAPT Framework", 
    "arxiv-id": "1307.1942v1", 
    "author": "Bruno Woltzenlogel-Paleo", 
    "publish": "2013-07-08T04:41:18Z", 
    "summary": "This paper introduces PROOFTOOL, the graphical user interface for the General\nArchitecture for Proof Theory (GAPT) framework. Its features are described with\na focus not only on the visualization but also on the analysis and\ntransformation of proofs and related tree-like structures, and its\nimplementation is explained. Finally, PROOFTOOL is compared with three other\ngraphical interfaces for proofs."
},{
    "category": "cs.MS", 
    "doi": "10.4204/EPTCS.118.5", 
    "link": "http://arxiv.org/pdf/1307.1945v1", 
    "title": "Theorema 2.0: A Graphical User Interface for a Mathematical Assistant   System", 
    "arxiv-id": "1307.1945v1", 
    "author": "Wolfgang Windsteiger", 
    "publish": "2013-07-08T04:42:02Z", 
    "summary": "Theorema 2.0 stands for a re-design including a complete re-implementation of\nthe Theorema system, which was originally designed, developed, and implemented\nby Bruno Buchberger and his Theorema group at RISC. In this paper, we present\nthe first prototype of a graphical user interface (GUI) for the new system. It\nheavily relies on powerful interactive capabilities introduced in recent\nreleases of the underlying Mathematica system, most importantly the possibility\nof having dynamic objects connected to interface elements like sliders, menus,\ncheck-boxes, radio-buttons and the like. All these features are fully\nintegrated into the Mathematica programming environment and allow the\nimplementation of a modern user interface."
},{
    "category": "math.OC", 
    "doi": "10.1137/110859129", 
    "link": "http://arxiv.org/pdf/1307.3522v1", 
    "title": "Acceleration of univariate global optimization algorithms working with   Lipschitz functions and Lipschitz first derivatives", 
    "arxiv-id": "1307.3522v1", 
    "author": "Yaroslav D. Sergeyev", 
    "publish": "2013-07-12T17:31:45Z", 
    "summary": "This paper deals with two kinds of the one-dimensional global optimization\nproblems over a closed finite interval: (i) the objective function $f(x)$\nsatisfies the Lipschitz condition with a constant $L$; (ii) the first\nderivative of $f(x)$ satisfies the Lipschitz condition with a constant $M$. In\nthe paper, six algorithms are presented for the case (i) and six algorithms for\nthe case (ii). In both cases, auxiliary functions are constructed and\nadaptively improved during the search. In the case (i), piece-wise linear\nfunctions are constructed and in the case (ii) smooth piece-wise quadratic\nfunctions are used. The constants $L$ and $M$ either are taken as values known\na priori or are dynamically estimated during the search. A recent technique\nthat adaptively estimates the local Lipschitz constants over different zones of\nthe search region is used to accelerate the search. A new technique called the\n\\emph{local improvement} is introduced in order to accelerate the search in\nboth cases (i) and (ii). The algorithms are described in a unique framework,\ntheir properties are studied from a general viewpoint, and convergence\nconditions of the proposed algorithms are given. Numerical experiments executed\non 120 test problems taken from the literature show quite a promising\nperformance of the new accelerating techniques."
},{
    "category": "math.NA", 
    "doi": "10.1137/110859129", 
    "link": "http://arxiv.org/pdf/1307.3529v1", 
    "title": "Solving ordinary differential equations on the Infinity Computer by   working with infinitesimals numerically", 
    "arxiv-id": "1307.3529v1", 
    "author": "Yaroslav D. Sergeyev", 
    "publish": "2013-07-12T17:56:23Z", 
    "summary": "There exists a huge number of numerical methods that iteratively construct\napproximations to the solution $y(x)$ of an ordinary differential equation\n(ODE) $y'(x)=f(x,y)$ starting from an initial value $y_0=y(x_0)$ and using a\nfinite approximation step $h$ that influences the accuracy of the obtained\napproximation. In this paper, a new framework for solving ODEs is presented for\na new kind of a computer -- the Infinity Computer (it has been patented and its\nworking prototype exists). The new computer is able to work numerically with\nfinite, infinite, and infinitesimal numbers giving so the possibility to use\ndifferent infinitesimals numerically and, in particular, to take advantage of\ninfinitesimal values of $h$. To show the potential of the new framework a\nnumber of results is established. It is proved that the Infinity Computer is\nable to calculate derivatives of the solution $y(x)$ and to reconstruct its\nTaylor expansion of a desired order numerically without finding the respective\nderivatives analytically (or symbolically) by the successive derivation of the\nODE as it is usually done when the Taylor method is applied. Methods using\napproximations of derivatives obtained thanks to infinitesimals are discussed\nand a technique for an automatic control of rounding errors is introduced.\nNumerical examples are given."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.cam.2012.02.020", 
    "link": "http://arxiv.org/pdf/1307.4302v1", 
    "title": "Lipschitz gradients for global optimization in a one-point-based   partitioning scheme", 
    "arxiv-id": "1307.4302v1", 
    "author": "Yaroslav D. Sergeyev", 
    "publish": "2013-07-15T13:59:30Z", 
    "summary": "A global optimization problem is studied where the objective function $f(x)$\nis a multidimensional black-box function and its gradient $f'(x)$ satisfies the\nLipschitz condition over a hyperinterval with an unknown Lipschitz constant\n$K$. Different methods for solving this problem by using an a priori given\nestimate of $K$, its adaptive estimates, and adaptive estimates of local\nLipschitz constants are known in the literature. Recently, the authors have\nproposed a one-dimensional algorithm working with multiple estimates of the\nLipschitz constant for $f'(x)$ (the existence of such an algorithm was a\nchallenge for 15 years). In this paper, a new multidimensional geometric method\nevolving the ideas of this one-dimensional scheme and using an efficient\none-point-based partitioning strategy is proposed. Numerical experiments\nexecuted on 800 multidimensional test functions demonstrate quite a promising\nperformance in comparison with popular DIRECT-based methods."
},{
    "category": "math.NA", 
    "doi": "10.1051/proc/201343004", 
    "link": "http://arxiv.org/pdf/1307.4839v1", 
    "title": "FullSWOF_Paral: Comparison of two parallelization strategies (MPI and   SKELGIS) on a software designed for hydrology applications", 
    "arxiv-id": "1307.4839v1", 
    "author": "Georges Sadaka", 
    "publish": "2013-07-18T06:45:12Z", 
    "summary": "In this paper, we perform a comparison of two approaches for the\nparallelization of an existing, free software, FullSWOF 2D (http://www.\nuniv-orleans.fr/mapmo/soft/FullSWOF/ that solves shallow water equations for\napplications in hydrology) based on a domain decomposition strategy. The first\napproach is based on the classical MPI library while the second approach uses\nParallel Algorithmic Skeletons and more precisely a library named SkelGIS\n(Skeletons for Geographical Information Systems). The first results presented\nin this article show that the two approaches are similar in terms of\nperformance and scalability. The two implementation strategies are however very\ndifferent and we discuss the advantages of each one."
},{
    "category": "physics.comp-ph", 
    "doi": "10.3233/SPR-130360", 
    "link": "http://arxiv.org/pdf/1307.6488v1", 
    "title": "From Physics Model to Results: An Optimizing Framework for   Cross-Architecture Code Generation", 
    "arxiv-id": "1307.6488v1", 
    "author": "Jian Tao", 
    "publish": "2013-07-24T16:52:44Z", 
    "summary": "Starting from a high-level problem description in terms of partial\ndifferential equations using abstract tensor notation, the Chemora framework\ndiscretizes, optimizes, and generates complete high performance codes for a\nwide range of compute architectures. Chemora extends the capabilities of\nCactus, facilitating the usage of large-scale CPU/GPU systems in an efficient\nmanner for complex applications, without low-level code tuning. Chemora\nachieves parallelism through MPI and multi-threading, combining OpenMP and\nCUDA. Optimizations include high-level code transformations, efficient loop\ntraversal strategies, dynamically selected data and instruction cache usage\nstrategies, and JIT compilation of GPU code tailored to the problem\ncharacteristics. The discretization is based on higher-order finite differences\non multi-block domains. Chemora's capabilities are demonstrated by simulations\nof black hole collisions. This problem provides an acid test of the framework,\nas the Einstein equations contain hundreds of variables and thousands of terms."
},{
    "category": "cs.MS", 
    "doi": "10.3233/SPR-130360", 
    "link": "http://arxiv.org/pdf/1307.6638v1", 
    "title": "Supporting 64-bit global indices in Epetra and other Trilinos packages   -- Techniques used and lessons learned", 
    "arxiv-id": "1307.6638v1", 
    "author": "James M. Willenbring", 
    "publish": "2013-07-25T06:52:00Z", 
    "summary": "The Trilinos Project is an effort to facilitate the design, development,\nintegration and ongoing support of mathematical software libraries within an\nobject-oriented framework. It is intended for large-scale, complex multiphysics\nengineering and scientific applications. Epetra is one of its basic packages.\nIt provides serial and parallel linear algebra capabilities. Before Trilinos\nversion 11.0, released in 2012, Epetra used the C++ int data-type for storing\nglobal and local indices for degrees of freedom (DOFs). Since int is typically\n32-bit, this limited the largest problem size to be smaller than approximately\ntwo billion DOFs. This was true even if a distributed memory machine could\nhandle larger problems. We have added optional support for C++ long long\ndata-type, which is at least 64-bit wide, for global indices. To save memory,\nmaintain the speed of memory-bound operations, and reduce further changes to\nthe code, the local indices are still 32-bit. We document the changes required\nto achieve this feature and how the new functionality can be used. We also\nreport on the lessons learned in modifying a mature and popular package from\nvarious perspectives -- design goals, backward compatibility, engineering\ndecisions, C++ language features, effects on existing users and other packages,\nand build integration."
},{
    "category": "cs.MS", 
    "doi": "10.3233/SPR-130360", 
    "link": "http://arxiv.org/pdf/1310.0056v2", 
    "title": "Modernizing PHCpack through phcpy", 
    "arxiv-id": "1310.0056v2", 
    "author": "Jan Verschelde", 
    "publish": "2013-09-30T21:05:03Z", 
    "summary": "PHCpack is a large software package for solving systems of polynomial\nequations. The executable phc is menu driven and file oriented. This paper\ndescribes the development of phcpy, a Python interface to PHCpack. Instead of\nnavigating through menus, users of phcpy solve systems in the Python shell or\nvia scripts. Persistent objects replace intermediate files."
},{
    "category": "math.OC", 
    "doi": "10.3233/SPR-130360", 
    "link": "http://arxiv.org/pdf/1310.4716v1", 
    "title": "SOSTOOLS Version 3.00 Sum of Squares Optimization Toolbox for MATLAB", 
    "arxiv-id": "1310.4716v1", 
    "author": "Pablo Parrilo", 
    "publish": "2013-10-17T14:30:42Z", 
    "summary": "SOSTOOLS v3.00 is the latest release of the freely available MATLAB toolbox\nfor formulating and solving sum of squares (SOS) optimization problems. Such\nproblems arise naturally in the analysis and control of nonlinear dynamical\nsystems, but also in other areas such as combinatorial optimization. Highlights\nof the new release include the ability to create polynomial matrices and\nformulate polynomial matrix inequalities, compatibility with MuPAD, the new\nMATLAB symbolic engine, as well as the multipoly toolbox v2.01. SOSTOOLS v3.00\ncan interface with five semidefinite programming solvers, and includes ten\ndemonstration examples."
},{
    "category": "cs.SC", 
    "doi": "10.3233/SPR-130360", 
    "link": "http://arxiv.org/pdf/1310.5551v1", 
    "title": "SymbolicData:SDEval - Benchmarking for Everyone", 
    "arxiv-id": "1310.5551v1", 
    "author": "Andreas Nareike", 
    "publish": "2013-10-18T19:58:03Z", 
    "summary": "In this paper we will present SDeval, a software project that contains tools\nfor creating and running benchmarks with a focus on problems in computer\nalgebra. It is built on top of the Symbolic Data project, able to translate\nproblems in the database into executable code for various computer algebra\nsystems. The included tools are designed to be very flexible to use and to\nextend, such that they can be utilized even in contexts of other communities.\nWith the presentation of SDEval, we will also address particularities of\nbenchmarking in the field of computer algebra. Furthermore, with SDEval, we\nprovide a feasible and automatizable way of reproducing benchmarks published in\ncurrent research works, which appears to be a difficult task in general due to\nthe customizability of the available programs. We will simultaneously present\nthe current developments in the Symbolic Data project."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-642-54479-8_5", 
    "link": "http://arxiv.org/pdf/1310.8455v1", 
    "title": "Composing and Factoring Generalized Green's Operators and Ordinary   Boundary Problems", 
    "arxiv-id": "1310.8455v1", 
    "author": "Georg Regensburger", 
    "publish": "2013-10-31T10:56:22Z", 
    "summary": "We consider solution operators of linear ordinary boundary problems with \"too\nmany\" boundary conditions, which are not always solvable. These generalized\nGreen's operators are a certain kind of generalized inverses of differential\noperators. We answer the question when the product of two generalized Green's\noperators is again a generalized Green's operator for the product of the\ncorresponding differential operators and which boundary problem it solves.\nMoreover, we show that---provided a factorization of the underlying\ndifferential operator---a generalized boundary problem can be factored into\nlower order problems corresponding to a factorization of the respective Green's\noperators. We illustrate our results by examples using the Maple package\nIntDiffOp, where the presented algorithms are implemented."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-642-54479-8_5", 
    "link": "http://arxiv.org/pdf/1312.2266v4", 
    "title": "The deal.II Library, Version 8.1", 
    "arxiv-id": "1312.2266v4", 
    "author": "Toby D. Young", 
    "publish": "2013-12-08T21:31:14Z", 
    "summary": "This paper provides an overview of the new features of the finite element\nlibrary deal.II version 8.1."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-642-54479-8_5", 
    "link": "http://arxiv.org/pdf/1312.2674v1", 
    "title": "Silent error detection in numerical time-stepping schemes", 
    "arxiv-id": "1312.2674v1", 
    "author": "Robert Schreiber", 
    "publish": "2013-12-10T05:31:23Z", 
    "summary": "Errors due to hardware or low level software problems, if detected, can be\nfixed by various schemes, such as recomputation from a checkpoint. Silent\nerrors are errors in application state that have escaped low-level error\ndetection. At extreme scale, where machines can perform astronomically many\noperations per second, silent errors threaten the validity of computed results.\n  We propose a new paradigm for detecting silent errors at the application\nlevel. Our central idea is to frequently compare computed values to those\nprovided by a cheap checking computation, and to build error detectors based on\nthe difference between the two output sequences. Numerical analysis provides us\nwith usable checking computations for the solution of initial-value problems in\nODEs and PDEs, arguably the most common problems in computational science.\nHere, we provide, optimize, and test methods based on Runge-Kutta and linear\nmultistep methods for ODEs, and on implicit and explicit finite difference\nschemes for PDEs. We take the heat equation and Navier-Stokes equations as\nexamples. In tests with artificially injected errors, this approach effectively\ndetects almost all meaningful errors, without significant slowdown."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-642-54479-8_5", 
    "link": "http://arxiv.org/pdf/1312.3020v1", 
    "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", 
    "arxiv-id": "1312.3020v1", 
    "author": "John Canny", 
    "publish": "2013-12-11T02:33:45Z", 
    "summary": "Many large datasets exhibit power-law statistics: The web graph, social\nnetworks, text data, click through data etc. Their adjacency graphs are termed\nnatural graphs, and are known to be difficult to partition. As a consequence\nmost distributed algorithms on these graphs are communication intensive. Many\nalgorithms on natural graphs involve an Allreduce: a sum or average of\npartitioned data which is then shared back to the cluster nodes. Examples\ninclude PageRank, spectral partitioning, and many machine learning algorithms\nincluding regression, factor (topic) models, and clustering. In this paper we\ndescribe an efficient and scalable Allreduce primitive for power-law data. We\npoint out scaling problems with existing butterfly and round-robin networks for\nSparse Allreduce, and show that a hybrid approach improves on both.\nFurthermore, we show that Sparse Allreduce stages should be nested instead of\ncascaded (as in the dense case). And that the optimum throughput Allreduce\nnetwork should be a butterfly of heterogeneous degree where degree decreases\nwith depth into the network. Finally, a simple replication scheme is introduced\nto deal with node failures. We present experiments showing significant\nimprovements over existing systems such as PowerGraph and Hadoop."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-642-54479-8_5", 
    "link": "http://arxiv.org/pdf/1312.6182v1", 
    "title": "Large-Scale Paralleled Sparse Principal Component Analysis", 
    "arxiv-id": "1312.6182v1", 
    "author": "K. Lu", 
    "publish": "2013-12-21T00:38:02Z", 
    "summary": "Principal component analysis (PCA) is a statistical technique commonly used\nin multivariate data analysis. However, PCA can be difficult to interpret and\nexplain since the principal components (PCs) are linear combinations of the\noriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity and\ninterpretability by approximating sparse PCs whose projections capture the\nmaximal variance of original data. In this paper we present an efficient and\nparalleled method of SPCA using graphics processing units (GPUs), which can\nprocess large blocks of data in parallel. Specifically, we construct parallel\nimplementations of the four optimization formulations of the generalized power\nmethod of SPCA (GP-SPCA), one of the most efficient and effective SPCA\napproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)\nis up to eleven times faster than the corresponding CPU implementation (using\nCBLAS), and up to 107 times faster than a MatLab implementation. Extensive\ncomparative experiments in several real-world datasets confirm that SPCA offers\na practical advantage."
},{
    "category": "cs.DC", 
    "doi": "10.1109/CloudCom.2013.7", 
    "link": "http://arxiv.org/pdf/1312.6488v1", 
    "title": "Early Observations on Performance of Google Compute Engine for   Scientific Computing", 
    "arxiv-id": "1312.6488v1", 
    "author": "Miranda Zhang", 
    "publish": "2013-12-23T09:12:09Z", 
    "summary": "Although Cloud computing emerged for business applications in industry,\npublic Cloud services have been widely accepted and encouraged for scientific\ncomputing in academia. The recently available Google Compute Engine (GCE) is\nclaimed to support high-performance and computationally intensive tasks, while\nlittle evaluation studies can be found to reveal GCE's scientific capabilities.\nConsidering that fundamental performance benchmarking is the strategy of\nearly-stage evaluation of new Cloud services, we followed the Cloud Evaluation\nExperiment Methodology (CEEM) to benchmark GCE and also compare it with Amazon\nEC2, to help understand the elementary capability of GCE for dealing with\nscientific problems. The experimental results and analyses show both potential\nadvantages of, and possible threats to applying GCE to scientific computing.\nFor example, compared to Amazon's EC2 service, GCE may better suit applications\nthat require frequent disk operations, while it may not be ready yet for single\nVM-based parallel computing. Following the same evaluation methodology,\ndifferent evaluators can replicate and/or supplement this fundamental\nevaluation of GCE. Based on the fundamental evaluation results, suitable GCE\nenvironments can be further established for case studies of solving real\nscience problems."
},{
    "category": "cs.MS", 
    "doi": "10.1109/CloudCom.2013.7", 
    "link": "http://arxiv.org/pdf/1401.0827v1", 
    "title": "Interaction entre math\u00e9matique et informatique Libre/Open Source par   le logiciel math\u00e9matique", 
    "arxiv-id": "1401.0827v1", 
    "author": "K. I. A. Derouiche", 
    "publish": "2014-01-04T16:42:05Z", 
    "summary": "This article focuses on the application of model development and opening the\nsource code available and implemented by the Free Software and Open Source\nFLOSS to the instructional and teaching has both mathematics and computer by\nthe read-write(R/W) of mathematical software, including the most famous cases\nare numerical and symbolic computation. The article analysis the development of\nthe mathematical model of Free/Open Source(math FLOSS) software has proven its\nimportance in the area of research in mathematics and computer science .\nHowever, although their actual use, is very readable in higher education\ncourses. We discuss the feasibility of this model to the characteristics of the\ndomain, actors, interaction they have and the communities they form during the\ndevelopment of the software. Finally, we propose a mathematical example of\nFree/Open Source(Math FlOSS) software as analysis device ."
},{
    "category": "cs.NA", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.2720v3", 
    "title": "A hierarchically blocked Jacobi SVD algorithm for single and multiple   graphics processing units", 
    "arxiv-id": "1401.2720v3", 
    "author": "Vedran Novakovi\u0107", 
    "publish": "2014-01-13T06:12:17Z", 
    "summary": "We present a hierarchically blocked one-sided Jacobi algorithm for the\nsingular value decomposition (SVD), targeting both single and multiple graphics\nprocessing units (GPUs). The blocking structure reflects the levels of GPU's\nmemory hierarchy. The algorithm may outperform MAGMA's dgesvd, while retaining\nhigh relative accuracy. To this end, we developed a family of parallel pivot\nstrategies on GPU's shared address space, but applicable also to inter-GPU\ncommunication. Unlike common hybrid approaches, our algorithm in a single GPU\nsetting needs a CPU for the controlling purposes only, while utilizing GPU's\nresources to the fullest extent permitted by the hardware. When required by the\nproblem size, the algorithm, in principle, scales to an arbitrary number of GPU\nnodes. The scalability is demonstrated by more than twofold speedup for\nsufficiently large matrices on a Tesla S2050 system with four GPUs vs. a single\nFermi card."
},{
    "category": "cs.MS", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.3013v1", 
    "title": "Resilience in Numerical Methods: A Position on Fault Models and   Methodologies", 
    "arxiv-id": "1401.3013v1", 
    "author": "Frank Mueller", 
    "publish": "2014-01-13T21:18:48Z", 
    "summary": "Future extreme-scale computer systems may expose silent data corruption (SDC)\nto applications, in order to save energy or increase performance. However,\nresilience research struggles to come up with useful abstract programming\nmodels for reasoning about SDC. Existing work randomly flips bits in running\napplications, but this only shows average-case behavior for a low-level,\nartificial hardware model. Algorithm developers need to understand worst-case\nbehavior with the higher-level data types they actually use, in order to make\ntheir algorithms more resilient. Also, we know so little about how SDC may\nmanifest in future hardware, that it seems premature to draw conclusions about\nthe average case. We argue instead that numerical algorithms can benefit from a\nnumerical unreliability fault model, where faults manifest as unbounded\nperturbations to floating-point data. Algorithms can use inexpensive \"sanity\"\nchecks that bound or exclude error in the results of computations. Given a\nselective reliability programming model that requires reliability only when and\nwhere needed, such checks can make algorithms reliable despite unbounded\nfaults. Sanity checks, and in general a healthy skepticism about the\ncorrectness of subroutines, are wise even if hardware is perfectly reliable."
},{
    "category": "cs.MS", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.3301v2", 
    "title": "An efficient way to assemble finite element matrices in vector languages", 
    "arxiv-id": "1401.3301v2", 
    "author": "Gilles Scarella", 
    "publish": "2014-01-14T19:36:39Z", 
    "summary": "Efficient Matlab codes in 2D and 3D have been proposed recently to assemble\nfinite element matrices. In this paper we present simple, compact and efficient\nvectorized algorithms, which are variants of these codes, in arbitrary\ndimension, without the use of any lower level language. They can be easily\nimplemented in many vector languages (e.g. Matlab, Octave, Python, Scilab, R,\nJulia, C++ with STL,...). The principle of these techniques is general, we\npresent it for the assembly of several finite element matrices in arbitrary\ndimension, in the P1 finite element case. We also provide an extension of the\nalgorithms to the case of a system of PDE's. Then we give an extension to\npiecewise polynomials of higher order. We compare numerically the performance\nof these algorithms in Matlab, Octave and Python, with that in FreeFEM++ and in\na compiled language such as C. Examples show that, unlike what is commonly\nbelieved, the performance is not radically worse than that of C : in the\nbest/worst cases, selected vector languages are respectively 2.3/3.5 and\n2.9/4.1 times slower than C in the scalar and vector cases. We also present\nnumerical results which illustrate the computational costs of these algorithms\ncompared to standard algorithms and to other recent ones."
},{
    "category": "cs.MS", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.4950v1", 
    "title": "MRRR-based Eigensolvers for Multi-core Processors and Supercomputers", 
    "arxiv-id": "1401.4950v1", 
    "author": "Matthias Petschow", 
    "publish": "2014-01-20T15:52:03Z", 
    "summary": "The real symmetric tridiagonal eigenproblem is of outstanding importance in\nnumerical computations; it arises frequently as part of eigensolvers for\nstandard and generalized dense Hermitian eigenproblems that are based on a\nreduction to tridiagonal form. For its solution, the algorithm of Multiple\nRelatively Robust Representations (MRRR or MR3 in short) - introduced in the\nlate 1990s - is among the fastest methods. To compute k eigenpairs of a real\nn-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in\ncontrast, all the other practical methods require O(k^2 n) or O(n^3) operations\nin the worst case. This thesis centers around the performance and accuracy of\nMRRR."
},{
    "category": "cs.MS", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.5353v1", 
    "title": "STABLAB Documentation for KdV : Numerical proof of stability of roll   waves in the small-amplitude limit for inclined thin film flow", 
    "arxiv-id": "1401.5353v1", 
    "author": "Blake Barker", 
    "publish": "2014-01-21T15:43:52Z", 
    "summary": "We document the MATLAB code used in the following study: Numerical proof of\nstability of roll waves in the small-amplitude limit for inclined thin film\nflow."
},{
    "category": "cs.SC", 
    "doi": "10.1137/140952429", 
    "link": "http://arxiv.org/pdf/1401.6694v2", 
    "title": "Multivariate sparse interpolation using randomized Kronecker   substitutions", 
    "arxiv-id": "1401.6694v2", 
    "author": "Daniel S. Roche", 
    "publish": "2014-01-26T21:37:51Z", 
    "summary": "We present new techniques for reducing a multivariate sparse polynomial to a\nunivariate polynomial. The reduction works similarly to the classical and\nwidely-used Kronecker substitution, except that we choose the degrees randomly\nbased on the number of nonzero terms in the multivariate polynomial, that is,\nits sparsity. The resulting univariate polynomial often has a significantly\nlower degree than the Kronecker substitution polynomial, at the expense of a\nsmall number of term collisions. As an application, we give a new algorithm for\nmultivariate interpolation which uses these new techniques along with any\nexisting univariate interpolation algorithm."
},{
    "category": "stat.CO", 
    "doi": "10.18637/jss.v071.i02", 
    "link": "http://arxiv.org/pdf/1401.7372v1", 
    "title": "RProtoBuf: Efficient Cross-Language Data Serialization in R", 
    "arxiv-id": "1401.7372v1", 
    "author": "Jeroen Ooms", 
    "publish": "2014-01-28T23:57:02Z", 
    "summary": "Modern data collection and analysis pipelines often involve a sophisticated\nmix of applications written in general purpose and specialized programming\nlanguages. Many formats commonly used to import and export data between\ndifferent programs or systems, such as CSV or JSON, are verbose, inefficient,\nnot type-safe, or tied to a specific programming language. Protocol Buffers are\na popular method of serializing structured data between applications - while\nremaining independent of programming languages or operating systems. They offer\na unique combination of features, performance, and maturity that seems\nparticularly well suited for data-driven applications and numerical computing.\nThe RProtoBuf package provides a complete interface to Protocol Buffers from\nthe R environment for statistical computing. This paper outlines the general\nclass of data serialization requirements for statistical computing, describes\nthe implementation of the RProtoBuf package, and illustrates its use with\nexample applications in large-scale data collection pipelines and web services."
},{
    "category": "cs.MS", 
    "doi": "10.18637/jss.v071.i02", 
    "link": "http://arxiv.org/pdf/1401.8230v2", 
    "title": "Increasing precision of uniform pseudorandom number generators", 
    "arxiv-id": "1401.8230v2", 
    "author": "Alexey Gulov", 
    "publish": "2014-01-29T20:20:18Z", 
    "summary": "A general method to produce uniformly distributed pseudorandom numbers with\nextended precision by combining two pseudorandom numbers with lower precision\nis proposed. In particular, this method can be used for pseudorandom number\ngeneration with extended precision on graphics processing units (GPU), where\nthe performance of single and double precision operations can vary\nsignificantly."
},{
    "category": "math.CA", 
    "doi": "10.1016/j.scico.2013.11.004", 
    "link": "http://arxiv.org/pdf/1403.1200v1", 
    "title": "Recent software developments for special functions in the   Santander-Amsterdam project", 
    "arxiv-id": "1403.1200v1", 
    "author": "N. M. Temme", 
    "publish": "2014-03-05T17:36:34Z", 
    "summary": "We give an overview of published algorithms by our group and of current\nactivities and future plans. In particular, we give details on methods for\ncomputing special functions and discuss in detail two current lines of\nresearch. Firstly, we describe the recent developments for the computation of\ncentral and non-central chi-square cumulative distributions (also called Marcum\nQ-functions), and we present a new quadrature method for computing them.\nSecondly, we describe the fourth-order methods for computing zeros of special\nfunctions recently developed, and we provide an explicit example for the\ncomputation of complex zeros of Bessel functions. We end with an overview of\npublished software by our group for computing special functions."
},{
    "category": "cs.DL", 
    "doi": "10.1016/j.scico.2013.11.004", 
    "link": "http://arxiv.org/pdf/1403.2036v1", 
    "title": "Straightforward Bibliography Management in R with the RefManageR Package", 
    "arxiv-id": "1403.2036v1", 
    "author": "Mathew W. McLean", 
    "publish": "2014-03-09T08:09:02Z", 
    "summary": "This work introduces the R package RefManageR, which provides tools for\nimporting and working with bibliographic references. It extends the bibentry\nclass in R in a number of useful ways, including providing R with previously\nunavailable support for BibLaTeX. BibLaTeX provides a superset of the\nfunctionality of BibTeX, including full Unicode support, no memory limitations,\nadditional fields and entry types, and more sophisticated sorting of\nreferences. RefManageR provides functions for citing and generating a\nbibliography with hyperlinks for documents prepared with RMarkdown or RHTML.\nExisting .bib files can be read into R and converted from BibTeX to BibLaTeX\nand vice versa. References can also be imported via queries to NCBI's Entrez,\nZotero libraries, Google Scholar, and CrossRef. Additionally, references can be\ncreated by reading PDFs stored on the user's machine with the help of Poppler.\nEntries stored in the reference manager can be easily searched by any field, by\ndate ranges, and by various formats for name lists (author by last names,\ntranslator by full names, etc.). Entries can also be updated, combined, sorted,\nprinted in a number of styles, and exported."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.scico.2013.11.004", 
    "link": "http://arxiv.org/pdf/1403.2805v1", 
    "title": "The jsonlite Package: A Practical and Consistent Mapping Between JSON   Data and R Objects", 
    "arxiv-id": "1403.2805v1", 
    "author": "Jeroen Ooms", 
    "publish": "2014-03-12T04:21:10Z", 
    "summary": "A naive realization of JSON data in R maps JSON arrays to an unnamed list,\nand JSON objects to a named list. However, in practice a list is an awkward,\ninefficient type to store and manipulate data. Most statistical applications\nwork with (homogeneous) vectors, matrices or data frames. Therefore JSON\npackages in R typically define certain special cases of JSON structures which\nmap to simpler R types. Currently there exist no formal guidelines, or even\nconsensus between implementations on how R data should be represented in JSON.\nFurthermore, upon closer inspection, even the most basic data structures in R\nactually do not perfectly map to their JSON counterparts and leave some\nambiguity for edge cases. These problems have resulted in different behavior\nbetween implementations and can lead to unexpected output. This paper\nexplicitly describes a mapping between R classes and JSON data, highlights\npotential problems, and proposes conventions that generalize the mapping to\ncover all common structures. We emphasize the importance of type consistency\nwhen using JSON to exchange dynamic data, and illustrate using examples and\nanecdotes. The jsonlite R package is used throughout the paper as a reference\nimplementation."
},{
    "category": "cs.DC", 
    "doi": "10.1007/s11265-014-0878-z", 
    "link": "http://arxiv.org/pdf/1403.4238v1", 
    "title": "Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU   Co-Processing", 
    "arxiv-id": "1403.4238v1", 
    "author": "Joseph R. Cavallaro", 
    "publish": "2014-03-17T18:26:41Z", 
    "summary": "In this paper, we present an OpenCL-based heterogeneous implementation of a\ncomputer vision algorithm -- image inpainting-based object removal algorithm --\non mobile devices. To take advantage of the computation power of the mobile\nprocessor, the algorithm workflow is partitioned between the CPU and the GPU\nbased on the profiling results on mobile devices, so that the\ncomputationally-intensive kernels are accelerated by the mobile GPGPU\n(general-purpose computing using graphics processing units). By exploring the\nimplementation trade-offs and utilizing the proposed optimization strategies at\ndifferent levels including algorithm optimization, parallelism optimization,\nand memory access optimization, we significantly speed up the algorithm with\nthe CPU-GPU heterogeneous implementation, while preserving the quality of the\noutput images. Experimental results show that heterogeneous computing based on\nGPGPU co-processing can significantly speed up the computer vision algorithms\nand makes them practical on real-world mobile devices."
},{
    "category": "hep-lat", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1403.5355v2", 
    "title": "The MIXMAX random number generator", 
    "arxiv-id": "1403.5355v2", 
    "author": "Konstantin G. Savvidy", 
    "publish": "2014-03-21T03:45:08Z", 
    "summary": "In this note, we give a practical solution to the problem of determining the\nmaximal period of matrix generators of pseudo-random numbers which are based on\nan integer-valued unimodular matrix of size NxN known as MIXMAX and arithmetic\ndefined on a Galois field GF[p] with large prime modulus p. The existing theory\nof Galois finite fields is adapted to the present case, and necessary and\nsufficient condition to attain the maximum period is formulated. Three\nefficient algorithms are presented. First, allowing to compute the\nmultiplication by the MIXMAX matrix with O(N) operations. Second, to\nrecursively compute the characteristic polynomial with O(N^2) operations, and\nthird, to apply skips of large number of steps S to the sequence in O(N^2\nlog(S)) operations. It is demonstrated that the dynamical properties of this\ngenerator dramatically improve with the size of the matrix N, as compared to\nthe classes of generators based on sparse matrices and/or sparse characteristic\npolynomials. Finally, we present the implementation details of the generator\nand the results of rigorous statistical testing."
},{
    "category": "q-bio.GN", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1403.6426v1", 
    "title": "High Performance Solutions for Big-data GWAS", 
    "arxiv-id": "1403.6426v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2014-03-25T17:21:55Z", 
    "summary": "In order to associate complex traits with genetic polymorphisms, genome-wide\nassociation studies process huge datasets involving tens of thousands of\nindividuals genotyped for millions of polymorphisms. When handling these\ndatasets, which exceed the main memory of contemporary computers, one faces two\ndistinct challenges: 1) Millions of polymorphisms and thousands of phenotypes\ncome at the cost of hundreds of gigabytes of data, which can only be kept in\nsecondary storage; 2) the relatedness of the test population is represented by\na relationship matrix, which, for large populations, can only fit in the\ncombined main memory of a distributed architecture. In this paper, by using\ndistributed resources such as Cloud or clusters, we address both challenges:\nThe genotype and phenotype data is streamed from secondary storage using a\ndouble buffer- ing technique, while the relationship matrix is kept across the\nmain memory of a distributed memory system. With the help of these solutions,\nwe develop separate algorithms for studies involving only one or a multitude of\ntraits. We show that these algorithms sustain high-performance and allow the\nanalysis of enormous datasets."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1405.0320v1", 
    "title": "Computing all Affine Solution Sets of Binomial Systems", 
    "arxiv-id": "1405.0320v1", 
    "author": "Jan Verschelde", 
    "publish": "2014-05-01T23:00:24Z", 
    "summary": "To compute solutions of sparse polynomial systems efficiently we have to\nexploit the structure of their Newton polytopes. While the application of\npolyhedral methods naturally excludes solutions with zero components, an\nirreducible decomposition of a variety is typically understood in affine space,\nincluding also those components with zero coordinates. For the problem of\ncomputing solution sets in the intersection of some coordinate planes, the\ndirect application of a polyhedral method fails, because the original facial\nstructure of the Newton polytopes may alter completely when selected variables\nbecome zero. Our new proposed method enumerates all factors contributing to a\ngeneralized permanent and toric solutions as a special case of this\nenumeration. For benchmark problems such as the adjacent 2-by-2 minors of a\ngeneral matrix, our methods scale much better than the witness set\nrepresentations of numerical algebraic geometry."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1405.3758v1", 
    "title": "Search Interfaces for Mathematicians", 
    "arxiv-id": "1405.3758v1", 
    "author": "Andrea Kohlhase", 
    "publish": "2014-05-15T06:55:21Z", 
    "summary": "Access to mathematical knowledge has changed dramatically in recent years,\ntherefore changing mathematical search practices. Our aim with this study is to\nscrutinize professional mathematicians' search behavior. With this\nunderstanding we want to be able to reason why mathematicians use which tool\nfor what search problem in what phase of the search process. To gain these\ninsights we conducted 24 repertory grid interviews with mathematically inclined\npeople (ranging from senior professional mathematicians to non-mathematicians).\nFrom the interview data we elicited patterns for the user group\n\"mathematicians\" that can be applied when understanding design issues or\ncreating new designs for mathematical search interfaces."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1405.5668v1", 
    "title": "NLCertify: A Tool for Formal Nonlinear Optimization", 
    "arxiv-id": "1405.5668v1", 
    "author": "Victor Magron", 
    "publish": "2014-05-22T08:41:00Z", 
    "summary": "NLCertify is a software package for handling formal certification of\nnonlinear inequalities involving transcendental multivariate functions. The\ntool exploits sparse semialgebraic optimization techniques with approximation\nmethods for transcendental functions, as well as formal features. Given a box\nand a transcendental multivariate function as input, NLCertify provides OCaml\nlibraries that produce nonnegativity certificates for the function over the\nbox, which can be ultimately proved correct inside the Coq proof assistant."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.06.003", 
    "link": "http://arxiv.org/pdf/1405.7012v4", 
    "title": "A formally verified proof of the Central Limit Theorem", 
    "arxiv-id": "1405.7012v4", 
    "author": "Luke Serafin", 
    "publish": "2014-05-27T19:01:57Z", 
    "summary": "We describe a proof of the Central Limit Theorem that has been formally\nverified in the Isabelle proof assistant. Our formalization builds upon and\nextends Isabelle's libraries for analysis and measure-theoretic probability.\nThe proof of the theorem uses characteristic functions, which are a kind of\nFourier transform, to demonstrate that, under suitable hypotheses, sums of\nrandom variables converge weakly to the standard normal distribution. We also\ndiscuss the libraries and infrastructure that supported the formalization, and\nreflect on some of the lessons we have learned from the effort."
},{
    "category": "cs.CE", 
    "doi": "10.5334/jors.bj", 
    "link": "http://arxiv.org/pdf/1405.7290v2", 
    "title": "PyRDM: A Python-based library for automating the management and online   publication of scientific software and data", 
    "arxiv-id": "1405.7290v2", 
    "author": "Matthew D. Piggott", 
    "publish": "2014-05-28T16:07:12Z", 
    "summary": "The recomputability and reproducibility of results from scientific software\nrequires access to both the source code and all associated input and output\ndata. However, the full collection of these resources often does not accompany\nthe key findings published in journal articles, thereby making it difficult or\nimpossible for the wider scientific community to verify the correctness of a\nresult or to build further research on it. This paper presents a new\nPython-based library, PyRDM, whose functionality aims to automate the process\nof sharing the software and data via online, citable repositories such as\nFigshare. The library is integrated into the workflow of an open-source\ncomputational fluid dynamics package, Fluidity, to demonstrate an example of\nits usage."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1405.7470v1", 
    "title": "Loo.py: transformation-based code generation for GPUs and CPUs", 
    "arxiv-id": "1405.7470v1", 
    "author": "Andreas Kl\u00f6ckner", 
    "publish": "2014-05-29T05:53:56Z", 
    "summary": "Today's highly heterogeneous computing landscape places a burden on\nprogrammers wanting to achieve high performance on a reasonably broad\ncross-section of machines. To do so, computations need to be expressed in many\ndifferent but mathematically equivalent ways, with, in the worst case, one\nvariant per target machine.\n  Loo.py, a programming system embedded in Python, meets this challenge by\ndefining a data model for array-style computations and a library of\ntransformations that operate on this model. Offering transformations such as\nloop tiling, vectorization, storage management, unrolling, instruction-level\nparallelism, change of data layout, and many more, it provides a convenient way\nto capture, parametrize, and re-unify the growth among code variants. Optional,\ndeep integration with numpy and PyOpenCL provides a convenient computing\nenvironment where the transition from prototype to high-performance\nimplementation can occur in a gradual, machine-assisted form."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1407.0904v2", 
    "title": "COFFEE: an Optimizing Compiler for Finite Element Local Assembly", 
    "arxiv-id": "1407.0904v2", 
    "author": "Paul H. J. Kelly", 
    "publish": "2014-07-03T13:01:45Z", 
    "summary": "The numerical solution of partial differential equations using the finite\nelement method is one of the key applications of high performance computing.\nLocal assembly is its characteristic operation. This entails the execution of a\nproblem-specific kernel to numerically evaluate an integral for each element in\nthe discretized problem domain. Since the domain size can be huge, executing\nefficient kernels is fundamental. Their op- timization is, however, a\nchallenging issue. Even though affine loop nests are generally present, the\nshort trip counts and the complexity of mathematical expressions make it hard\nto determine a single or unique sequence of successful transformations.\nTherefore, we present the design and systematic evaluation of COF- FEE, a\ndomain-specific compiler for local assembly kernels. COFFEE manipulates\nabstract syntax trees generated from a high-level domain-specific language for\nPDEs by introducing domain-aware composable optimizations aimed at improving\ninstruction-level parallelism, especially SIMD vectorization, and register\nlocality. It then generates C code including vector intrinsics. Experiments\nusing a range of finite-element forms of increasing complexity show that\nsignificant performance improvement is achieved."
},{
    "category": "cs.SE", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1407.2905v1", 
    "title": "Run-time extensibility and librarization of simulation software", 
    "arxiv-id": "1407.2905v1", 
    "author": "Barry F. Smith", 
    "publish": "2014-07-10T19:08:32Z", 
    "summary": "Build-time configuration and environment assumptions are hampering progress\nand usability in scientific software. That which would be utterly unacceptable\nin non-scientific software somehow passes for the norm in scientific packages.\nThe community needs reusable software packages that are easy use and flexible\nenough to accommodate next-generation simulation and analysis demands."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1407.3262v1", 
    "title": "Elements of Design for Containers and Solutions in the LinBox Library", 
    "arxiv-id": "1407.3262v1", 
    "author": "B. David Saunders", 
    "publish": "2014-06-25T19:38:09Z", 
    "summary": "We describe in this paper new design techniques used in the \\cpp exact linear\nalgebra library \\linbox, intended to make the library safer and easier to use,\nwhile keeping it generic and efficient. First, we review the new simplified\nstructure for containers, based on our \\emph{founding scope allocation} model.\nWe explain design choices and their impact on coding: unification of our matrix\nclasses, clearer model for matrices and submatrices, \\etc Then we present a\nvariation of the \\emph{strategy} design pattern that is comprised of a\ncontroller--plugin system: the controller (solution) chooses among plug-ins\n(algorithms) that always call back the controllers for subtasks. We give\nexamples using the solution \\mul. Finally we present a benchmark architecture\nthat serves two purposes: Providing the user with easier ways to produce\ngraphs; Creating a framework for automatically tuning the library and\nsupporting regression testing."
},{
    "category": "math.NT", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1407.5953v1", 
    "title": "Implementing cryptographic pairings at standard security levels", 
    "arxiv-id": "1407.5953v1", 
    "author": "J\u00e9r\u00f4me Milan", 
    "publish": "2014-07-22T17:42:29Z", 
    "summary": "This study reports on an implementation of cryptographic pairings in a\ngeneral purpose computer algebra system. For security levels equivalent to the\ndifferent AES flavours, we exhibit suitable curves in parametric families and\nshow that optimal ate and twisted ate pairings exist and can be efficiently\nevaluated. We provide a correct description of Miller's algorithm for signed\nbinary expansions such as the NAF and extend a recent variant due to Boxall et\nal. to addition-subtraction chains. We analyse and compare several algorithms\nproposed in the literature for the final exponentiation. Finally, we ive\nrecommendations on which curve and pairing to choose at each security level."
},{
    "category": "math.NA", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1407.8078v1", 
    "title": "Zolotarev Quadrature Rules and Load Balancing for the FEAST Eigensolver", 
    "arxiv-id": "1407.8078v1", 
    "author": "Gautier Viaud", 
    "publish": "2014-07-30T15:13:03Z", 
    "summary": "The FEAST method for solving large sparse eigenproblems is equivalent to\nsubspace iteration with an approximate spectral projector and implicit\northogonalization. This relation allows to characterize the convergence of this\nmethod in terms of the error of a certain rational approximant to an indicator\nfunction. We propose improved rational approximants leading to FEAST variants\nwith faster convergence, in particular, when using rational approximants based\non the work of Zolotarev. Numerical experiments demonstrate the possible\ncomputational savings especially for pencils whose eigenvalues are not well\nseparated and when the dimension of the search space is only slightly larger\nthan the number of wanted eigenvalues. The new approach improves both\nconvergence robustness and load balancing when FEAST runs on multiple search\nintervals in parallel."
},{
    "category": "cs.NE", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1410.0759v3", 
    "title": "cuDNN: Efficient Primitives for Deep Learning", 
    "arxiv-id": "1410.0759v3", 
    "author": "Evan Shelhamer", 
    "publish": "2014-10-03T06:16:43Z", 
    "summary": "We present a library of efficient implementations of deep learning\nprimitives. Deep learning workloads are computationally intensive, and\noptimizing their kernels is difficult and time-consuming. As parallel\narchitectures evolve, kernels must be reoptimized, which makes maintaining\ncodebases difficult over time. Similar issues have long been addressed in the\nHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).\nHowever, there is no analogous library for deep learning. Without such a\nlibrary, researchers implementing deep learning workloads on parallel\nprocessors must create and optimize their own implementations of the main\ncomputational kernels, and this work must be repeated as new parallel\nprocessors emerge. To address this problem, we have created a library similar\nin intent to BLAS, with optimized routines for deep learning workloads. Our\nimplementation contains routines for GPUs, although similarly to the BLAS\nlibrary, these routines could be implemented for other platforms. The library\nis easy to integrate into existing frameworks, and provides optimized\nperformance and memory usage. For example, integrating cuDNN into Caffe, a\npopular framework for convolutional networks, improves performance by 36% on a\nstandard model while also reducing memory consumption."
},{
    "category": "math.NA", 
    "doi": "10.1145/2627373.2627387", 
    "link": "http://arxiv.org/pdf/1410.1387v1", 
    "title": "High-Order Finite-differences on multi-threaded architectures using OCCA", 
    "arxiv-id": "1410.1387v1", 
    "author": "Timothy Warburton", 
    "publish": "2014-10-02T18:15:22Z", 
    "summary": "High-order finite-difference methods are commonly used in wave propagators\nfor industrial subsurface imaging algorithms. Computational aspects of the\nreduced linear elastic vertical transversely isotropic propagator are\nconsidered. Thread parallel algorithms suitable for implementing this\npropagator on multi-core and many-core processing devices are introduced.\nPortability is addressed through the use of the \\OCCA runtime programming\ninterface. Finally, performance results are shown for various architectures on\na representative synthetic test case."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2907944", 
    "link": "http://arxiv.org/pdf/1410.4054v3", 
    "title": "Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing   Units", 
    "arxiv-id": "1410.4054v3", 
    "author": "Tibor Grasser", 
    "publish": "2014-10-15T13:23:31Z", 
    "summary": "We revisit the implementation of iterative solvers on discrete graphics\nprocessing units and demonstrate the benefit of implementations using extensive\nkernel fusion for pipelined formulations over conventional implementations of\nclassical formulations. The proposed implementations with both CUDA and OpenCL\nare freely available in ViennaCL and are shown to be competitive with or even\nsuperior to other solver packages for graphics processing units. Highest\nperformance gains are obtained for small to medium-sized systems, while our\nimplementations are on par with vendor-tuned implementations for very large\nsystems. Our results are especially beneficial for transient problems, where\nmany small to medium-sized systems instead of a single big system need to be\nsolved."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1145/2907944", 
    "link": "http://arxiv.org/pdf/1410.4345v2", 
    "title": "HOPE: A Python Just-In-Time compiler for astrophysical computations", 
    "arxiv-id": "1410.4345v2", 
    "author": "Alexandre Refregier", 
    "publish": "2014-10-16T09:28:18Z", 
    "summary": "The Python programming language is becoming increasingly popular for\nscientific applications due to its simplicity, versatility, and the broad range\nof its libraries. A drawback of this dynamic language, however, is its low\nruntime performance which limits its applicability for large simulations and\nfor the analysis of large data sets, as is common in astrophysics and\ncosmology. While various frameworks have been developed to address this\nlimitation, most focus on covering the complete language set, and either force\nthe user to alter the code or are not able to reach the full speed of an\noptimised native compiled language. In order to combine the ease of Python and\nthe speed of C++, we developed HOPE, a specialised Python just-in-time (JIT)\ncompiler designed for numerical astrophysical applications. HOPE focuses on a\nsubset of the language and is able to translate Python code into C++ while\nperforming numerical optimisation on mathematical expressions at runtime. To\nenable the JIT compilation, the user only needs to add a decorator to the\nfunction definition. We assess the performance of HOPE by performing a series\nof benchmarks and compare its execution speed with that of plain Python, C++\nand the other existing frameworks. We find that HOPE improves the performance\ncompared to plain Python by a factor of 2 to 120, achieves speeds comparable to\nthat of C++, and often exceeds the speed of the existing solutions. We discuss\nthe differences between HOPE and the other frameworks, as well as future\nextensions of its capabilities. The fully documented HOPE package is available\nat http://hope.phys.ethz.ch and is published under the GPLv3 license on PyPI\nand GitHub."
},{
    "category": "math.OC", 
    "doi": "10.1145/2907944", 
    "link": "http://arxiv.org/pdf/1410.4821v1", 
    "title": "Convex Optimization in Julia", 
    "arxiv-id": "1410.4821v1", 
    "author": "Stephen Boyd", 
    "publish": "2014-10-17T18:53:04Z", 
    "summary": "This paper describes Convex, a convex optimization modeling framework in\nJulia. Convex translates problems from a user-friendly functional language into\nan abstract syntax tree describing the problem. This concise representation of\nthe global structure of the problem allows Convex to infer whether the problem\ncomplies with the rules of disciplined convex programming (DCP), and to pass\nthe problem to a suitable solver. These operations are carried out in Julia\nusing multiple dispatch, which dramatically reduces the time required to verify\nDCP compliance and to parse a problem into conic form. Convex then\nautomatically chooses an appropriate backend solver to solve the conic form\nproblem."
},{
    "category": "cs.AR", 
    "doi": "10.1145/2907944", 
    "link": "http://arxiv.org/pdf/1410.8772v1", 
    "title": "Programming the Adapteva Epiphany 64-core Network-on-chip Coprocessor", 
    "arxiv-id": "1410.8772v1", 
    "author": "Alistair P. Rendell", 
    "publish": "2014-10-30T08:29:11Z", 
    "summary": "In the construction of exascale computing systems energy efficiency and power\nconsumption are two of the major challenges. Low-power high performance\nembedded systems are of increasing interest as building blocks for large scale\nhigh- performance systems. However, extracting maximum performance out of such\nsystems presents many challenges. Various aspects from the hardware\narchitecture to the programming models used need to be explored. The Epiphany\narchitecture integrates low-power RISC cores on a 2D mesh network and promises\nup to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of\nmemory per eCore for storing both data and code, and only low level inter-core\ncommunication support, programming the Epiphany system presents several\nchallenges. In this paper we evaluate the performance of the Epiphany system\nfor a variety of basic compute and communication operations. Guided by this\ndata we explore strategies for implementing scientific applications on memory\nconstrained low-powered devices such as the Epiphany. With future systems\nexpected to house thousands of cores in a single chip, the merits of such\narchitectures as a path to exascale is compared to other competing systems."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2907944", 
    "link": "http://arxiv.org/pdf/1411.1830v2", 
    "title": "Introduction to the R package TDA", 
    "arxiv-id": "1411.1830v2", 
    "author": "Cl\u00e9ment Maria", 
    "publish": "2014-11-07T05:10:34Z", 
    "summary": "We present a short tutorial and introduction to using the R package TDA,\nwhich provides some tools for Topological Data Analysis. In particular, it\nincludes implementations of functions that, given some data, provide\ntopological information about the underlying space, such as the distance\nfunction, the distance to a measure, the kNN density estimator, the kernel\ndensity estimator, and the kernel distance. The salient topological features of\nthe sublevel sets (or superlevel sets) of these functions can be quantified\nwith persistent homology. We provide an R interface for the efficient\nalgorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function\nfor the persistent homology of the Rips filtration, and one for the persistent\nhomology of sublevel sets (or superlevel sets) of arbitrary functions evaluated\nover a grid of points. The significance of the features in the resulting\npersistence diagrams can be analyzed with functions that implement recently\ndeveloped statistical methods. The R package TDA also includes the\nimplementation of an algorithm for density clustering, which allows us to\nidentify the spatial organization of the probability mass associated to a\ndensity function and visualize it by means of a dendrogram, the cluster tree."
},{
    "category": "math.NA", 
    "doi": "10.1137/15M1021167", 
    "link": "http://arxiv.org/pdf/1411.2940v3", 
    "title": "Automated generation and symbolic manipulation of tensor product finite   elements", 
    "arxiv-id": "1411.2940v3", 
    "author": "Colin J. Cotter", 
    "publish": "2014-11-11T19:47:45Z", 
    "summary": "We describe and implement a symbolic algebra for scalar and vector-valued\nfinite elements, enabling the computer generation of elements with tensor\nproduct structure on quadrilateral, hexahedral and triangular prismatic cells.\nThe algebra is implemented as an extension to the domain-specific language UFL,\nthe Unified Form Language. This allows users to construct many finite element\nspaces beyond those supported by existing software packages. We have made\ncorresponding extensions to FIAT, the FInite element Automatic Tabulator, to\nenable numerical tabulation of such spaces. This tabulation is consequently\nused during the automatic generation of low-level code that carries out local\nassembly operations, within the wider context of solving finite element\nproblems posed over such function spaces. We have done this work within the\ncode-generation pipeline of the software package Firedrake; we make use of the\nfull Firedrake package to present numerical examples."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2015.05.015", 
    "link": "http://arxiv.org/pdf/1411.3834v1", 
    "title": "Simple, Parallel, High-Performance Virtual Machines for Extreme   Computations", 
    "arxiv-id": "1411.3834v1", 
    "author": "J\u00fcrgen Reuter", 
    "publish": "2014-11-14T09:15:48Z", 
    "summary": "We introduce a high-performance virtual machine (VM) written in a numerically\nfast language like Fortran or C to evaluate very large expressions. We discuss\nthe general concept of how to perform computations in terms of a VM and present\nspecifically a VM that is able to compute tree-level cross sections for any\nnumber of external legs, given the corresponding byte code from the optimal\nmatrix element generator, O'Mega. Furthermore, this approach allows to\nformulate the parallel computation of a single phase space point in a simple\nand obvious way. We analyze hereby the scaling behaviour with multiple threads\nas well as the benefits and drawbacks that are introduced with this method. Our\nimplementation of a VM can run faster than the corresponding native, compiled\ncode for certain processes and compilers, especially for very high\nmultiplicities, and has in general runtimes in the same order of magnitude. By\navoiding the tedious compile and link steps, which may fail for source code\nfiles of gigabyte sizes, new processes or complex higher order corrections that\nare currently out of reach could be evaluated with a VM given enough computing\npower."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1016/j.cpc.2015.05.015", 
    "link": "http://arxiv.org/pdf/1411.4439v1", 
    "title": "Conjugate gradient solvers on Intel Xeon Phi and NVIDIA GPUs", 
    "arxiv-id": "1411.4439v1", 
    "author": "M. Wagner", 
    "publish": "2014-11-17T11:27:55Z", 
    "summary": "Lattice Quantum Chromodynamics simulations typically spend most of the\nruntime in inversions of the Fermion Matrix. This part is therefore frequently\noptimized for various HPC architectures. Here we compare the performance of the\nIntel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate\ngradient solver. By exposing more parallelism to the accelerator through\ninverting multiple vectors at the same time, we obtain a performance greater\nthan 300 GFlop/s on both architectures. This more than doubles the performance\nof the inversions. We also give a short overview of the Knights Corner\narchitecture, discuss some details of the implementation and the effort\nrequired to obtain the achieved performance."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2015.05.015", 
    "link": "http://arxiv.org/pdf/1412.0436v4", 
    "title": "An Infra-Structure for Performance Estimation and Experimental   Comparison of Predictive Models in R", 
    "arxiv-id": "1412.0436v4", 
    "author": "Luis Torgo", 
    "publish": "2014-12-01T11:35:47Z", 
    "summary": "This document describes an infra-structure provided by the R package\nperformanceEstimation that allows to estimate the predictive performance of\ndifferent approaches (workflows) to predictive tasks. The infra-structure is\ngeneric in the sense that it can be used to estimate the values of any\nperformance metrics, for any workflow on different predictive tasks, namely,\nclassification, regression and time series tasks. The package also includes\nseveral standard workflows that allow users to easily set up their experiments\nlimiting the amount of work and information they need to provide. The overall\ngoal of the infra-structure provided by our package is to facilitate the task\nof estimating the predictive performance of different modeling approaches to\npredictive tasks in the R environment."
},{
    "category": "cs.CG", 
    "doi": "10.1016/j.cpc.2015.05.015", 
    "link": "http://arxiv.org/pdf/1412.2562v1", 
    "title": "Minkowski sum of HV-polytopes in Rn", 
    "arxiv-id": "1412.2562v1", 
    "author": "Denis Teissandier", 
    "publish": "2014-12-08T13:56:17Z", 
    "summary": "Minkowski sums cover a wide range of applications in many different fields\nlike algebra, morphing, robotics, mechanical CAD/CAM systems ... This paper\ndeals with sums of polytopes in a n dimensional space provided that both\nH-representation and V-representation are available i.e. the polytopes are\ndescribed by both their half-spaces and vertices. The first method uses the\npolytope normal fans and relies on the ability to intersect dual polyhedral\ncones. Then we introduce another way of considering Minkowski sums of polytopes\nbased on the primal polyhedral cones attached to each vertex."
},{
    "category": "cs.CG", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1412.2564v2", 
    "title": "Minkowski Sum of Polytopes Defined by Their Vertices", 
    "arxiv-id": "1412.2564v2", 
    "author": "Denis Teissandier", 
    "publish": "2014-12-08T13:57:21Z", 
    "summary": "Minkowski sums are of theoretical interest and have applications in fields\nrelated to industrial backgrounds. In this paper we focus on the specific case\nof summing polytopes as we want to solve the tolerance analysis problem\ndescribed in [1]. Our approach is based on the use of linear programming and is\nsolvable in polynomial time. The algorithm we developed can be implemented and\nparallelized in a very easy way."
},{
    "category": "cs.CV", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1412.4564v3", 
    "title": "MatConvNet - Convolutional Neural Networks for MATLAB", 
    "arxiv-id": "1412.4564v3", 
    "author": "Karel Lenc", 
    "publish": "2014-12-15T12:23:35Z", 
    "summary": "MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox."
},{
    "category": "astro-ph.IM", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1412.6367v1", 
    "title": "PyFAI: a Python library for high performance azimuthal integration on   GPU", 
    "arxiv-id": "1412.6367v1", 
    "author": "Giannis Ashiotis", 
    "publish": "2014-12-19T15:06:50Z", 
    "summary": "The pyFAI package has been designed to reduce X-ray diffraction images into\npowder diffraction curves to be further processed by scientists. This\ncontribution describes how to convert an image into a radial profile using the\nNumpy package, how the process was accelerated using Cython. The algorithm was\nparallelised, needing a complete re-design to benefit from massively parallel\ndevices like graphical processing units or accelerators like the Intel Xeon Phi\nusing the PyOpenCL library."
},{
    "category": "cs.MS", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1412.6407v1", 
    "title": "Enhancing SfePy with Isogeometric Analysis", 
    "arxiv-id": "1412.6407v1", 
    "author": "Robert Cimrman", 
    "publish": "2014-12-19T16:03:26Z", 
    "summary": "In the paper a recent enhancement to the open source package SfePy (Simple\nFinite Elements in Python, http://sfepy.org) is introduced, namely the addition\nof another numerical discretization scheme, the isogeometric analysis, to the\noriginal implementation based on the nowadays standard and well-established\nnumerical solution technique, the finite element method. The isogeometric\nremoves the need of the solution domain approximation by a piece-wise polygonal\ndomain covered by the finite element mesh, and allows approximation of unknown\nfields with a higher smoothness then the finite element method, which can be\nadvantageous in many applications. Basic numerical examples illustrating the\nimplementation and use of the isogeometric analysis in SfePy are shown."
},{
    "category": "stat.CO", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1412.6890v2", 
    "title": "Software for Distributed Computation on Medical Databases: A   Demonstration Project", 
    "arxiv-id": "1412.6890v2", 
    "author": "Philip W. Lavori", 
    "publish": "2014-12-22T07:17:01Z", 
    "summary": "Bringing together the information latent in distributed medical databases\npromises to personalize medical care by enabling reliable, stable modeling of\noutcomes with rich feature sets (including patient characteristics and\ntreatments received). However, there are barriers to aggregation of medical\ndata, due to lack of standardization of ontologies, privacy concerns,\nproprietary attitudes toward data, and a reluctance to give up control over end\nuse. Aggregation of data is not always necessary for model fitting. In models\nbased on maximizing a likelihood, the computations can be distributed, with\naggregation limited to the intermediate results of calculations on local data,\nrather than raw data. Distributed fitting is also possible for singular value\ndecomposition. There has been work on the technical aspects of shared\ncomputation for particular applications, but little has been published on the\nsoftware needed to support the \"social networking\" aspect of shared computing,\nto reduce the barriers to collaboration. We describe a set of software tools\nthat allow the rapid assembly of a collaborative computational project, based\non the flexible and extensible R statistical software and other open source\npackages, that can work across a heterogeneous collection of database\nenvironments, with full transparency to allow local officials concerned with\nprivacy protections to validate the safety of the method. We describe the\nprinciples, architecture, and successful test results for the site-stratified\nCox model and rank-k Singular Value Decomposition (SVD)."
},{
    "category": "stat.ML", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1502.06064v1", 
    "title": "MILJS : Brand New JavaScript Libraries for Matrix Calculation and   Machine Learning", 
    "arxiv-id": "1502.06064v1", 
    "author": "Tatsuya Harada", 
    "publish": "2015-02-21T04:29:41Z", 
    "summary": "MILJS is a collection of state-of-the-art, platform-independent, scalable,\nfast JavaScript libraries for matrix calculation and machine learning. Our core\nlibrary offering a matrix calculation is called Sushi, which exhibits far\nbetter performance than any other leading machine learning libraries written in\nJavaScript. Especially, our matrix multiplication is 177 times faster than the\nfastest JavaScript benchmark. Based on Sushi, a machine learning library called\nTempura is provided, which supports various algorithms widely used in machine\nlearning research. We also provide Soba as a visualization library. The\nimplementations of our libraries are clearly written, properly documented and\nthus can are easy to get started with, as long as there is a web browser. These\nlibraries are available from http://mil-tokyo.github.io/ under the MIT license."
},{
    "category": "stat.CO", 
    "doi": "10.4236/jamp.2015.31008", 
    "link": "http://arxiv.org/pdf/1503.00855v1", 
    "title": "How to speed up R code: an introduction", 
    "arxiv-id": "1503.00855v1", 
    "author": "Nathan Uyttendaele", 
    "publish": "2015-03-03T08:21:32Z", 
    "summary": "Most calculations performed by the average R user are unremarkable in the\nsense that nowadays, any computer can crush the related code in a matter of\nseconds. But more and more often, heavy calculations are also performed using\nR, something especially true in some fields such as statistics. The user then\nfaces total execution times of his codes that are hard to work with: hours,\ndays, even weeks. In this paper, how to reduce the total execution time of\nvarious codes will be shown and typical bottlenecks will be discussed. As a\nlast resort, how to run your code on a cluster of computers (most workplaces\nhave one) in order to make use of a larger processing power than the one\navailable on an average computer will also be discussed through two examples."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.01034v2", 
    "title": "Quantomatic: A Proof Assistant for Diagrammatic Reasoning", 
    "arxiv-id": "1503.01034v2", 
    "author": "Vladimir Zamdzhiev", 
    "publish": "2015-03-03T18:20:39Z", 
    "summary": "Monoidal algebraic structures consist of operations that can have multiple\noutputs as well as multiple inputs, which have applications in many areas\nincluding categorical algebra, programming language semantics, representation\ntheory, algebraic quantum information, and quantum groups. String diagrams\nprovide a convenient graphical syntax for reasoning formally about such\nstructures, while avoiding many of the technical challenges of a term-based\napproach. Quantomatic is a tool that supports the (semi-)automatic construction\nof equational proofs using string diagrams. We briefly outline the theoretical\nbasis of Quantomatic's rewriting engine, then give an overview of the core\nfeatures and architecture and give a simple example project that computes\nnormal forms for commutative bialgebras."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.01073v1", 
    "title": "T3PS: Tool for Parallel Processing in Parameter Scans", 
    "arxiv-id": "1503.01073v1", 
    "author": "Vinzenz Maurer", 
    "publish": "2015-02-27T08:48:24Z", 
    "summary": "T3PS is a program that can be used to quickly design and perform parameter\nscans while easily taking advantage of the multi-core architecture of current\nprocessors. It takes an easy to read and write parameter scan definition file\nformat as input. Based on the parameter ranges and other options contained\ntherein, it distributes the calculation of the parameter space over multiple\nprocesses and possibly computers. The derived data is saved in a plain text\nfile format readable by most plotting software. The supported scanning\nstrategies include: grid scan, random scan, Markov Chain Monte Carlo, numerical\noptimization. Several example parameter scans are shown and compared with\nresults in the literature."
},{
    "category": "math.GT", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.04099v1", 
    "title": "Algorithms and complexity for Turaev-Viro invariants", 
    "arxiv-id": "1503.04099v1", 
    "author": "Jonathan Spreer", 
    "publish": "2015-03-13T15:21:06Z", 
    "summary": "The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.04501v1", 
    "title": "Computer Assisted Parallel Program Generation", 
    "arxiv-id": "1503.04501v1", 
    "author": "Shigeo Kawata", 
    "publish": "2015-03-16T02:07:50Z", 
    "summary": "Parallel computation is widely employed in scientific researches, engineering\nactivities and product development. Parallel program writing itself is not\nalways a simple task depending on problems solved. Large-scale scientific\ncomputing, huge data analyses and precise visualizations, for example, would\nrequire parallel computations, and the parallel computing needs the\nparallelization techniques. In this Chapter a parallel program generation\nsupport is discussed, and a computer-assisted parallel program generation\nsystem P-NCAS is introduced. Computer assisted problem solving is one of key\nmethods to promote innovations in science and engineering, and contributes to\nenrich our society and our life toward a programming-free environment in\ncomputing science. Problem solving environments (PSE) research activities had\nstarted to enhance the programming power in 1970's. The P-NCAS is one of the\nPSEs; The PSE concept provides an integrated human-friendly computational\nsoftware and hardware system to solve a target class of problems"
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.05032v2", 
    "title": "CSR5: An Efficient Storage Format for Cross-Platform Sparse   Matrix-Vector Multiplication", 
    "arxiv-id": "1503.05032v2", 
    "author": "Brian Vinter", 
    "publish": "2015-03-17T13:18:49Z", 
    "summary": "Sparse matrix-vector multiplication (SpMV) is a fundamental building block\nfor numerous applications. In this paper, we propose CSR5 (Compressed Sparse\nRow 5), a new storage format, which offers high-throughput SpMV on various\nplatforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is\ninsensitive to the sparsity structure of the input matrix. Thus the single\nformat can support an SpMV algorithm that is efficient both for regular\nmatrices and for irregular matrices. Furthermore, we show that the overhead of\nthe format conversion from the CSR to the CSR5 can be as low as the cost of a\nfew SpMV operations. We compare the CSR5-based SpMV algorithm with 11\nstate-of-the-art formats and algorithms on four mainstream processors using 14\nregular and 10 irregular matrices as a benchmark suite. For the 14 regular\nmatrices in the suite, we achieve comparable or better performance over the\nprevious work. For the 10 irregular matrices, the CSR5 obtains average\nperformance improvement of 17.6\\%, 28.5\\%, 173.0\\% and 293.3\\% (up to 213.3\\%,\n153.6\\%, 405.1\\% and 943.3\\%) over the best existing work on dual-socket Intel\nCPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For\nreal-world applications such as a solver with only tens of iterations, the CSR5\nformat can be more practical because of its low-overhead for format conversion.\nThe source code of this work is downloadable at\nhttps://github.com/bhSPARSE/Benchmark_SpMV_using_CSR5"
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-319-21401-6_22", 
    "link": "http://arxiv.org/pdf/1503.06182v1", 
    "title": "A Multi-Threaded Version of MCFM", 
    "arxiv-id": "1503.06182v1", 
    "author": "Walter T. Giele", 
    "publish": "2015-03-20T18:02:33Z", 
    "summary": "We report on our findings modifying MCFM using OpenMP to implement\nmulti-threading. By using OpenMP, the modified MCFM will execute on any\nprocessor, automatically adjusting to the number of available threads. We\nmodified the integration routine VEGAS to distribute the event evaluation over\nthe threads, while combining all events at the end of every iteration to\noptimize the numerical integration. Special care has been taken that the\nresults of the Monte Carlo integration are independent of the number of threads\nused, to facilitate the validation of the OpenMP version of MCFM."
},{
    "category": "cs.PL", 
    "doi": "10.1145/2774959.2774969", 
    "link": "http://arxiv.org/pdf/1503.07659v2", 
    "title": "Loo.py: From Fortran to performance via transformation and substitution   rules", 
    "arxiv-id": "1503.07659v2", 
    "author": "Andreas Kl\u00f6ckner", 
    "publish": "2015-03-26T09:40:56Z", 
    "summary": "A large amount of numerically-oriented code is written and is being written\nin legacy languages. Much of this code could, in principle, make good use of\ndata-parallel throughput-oriented computer architectures. Loo.py, a\ntransformation-based programming system targeted at GPUs and general\ndata-parallel architectures, provides a mechanism for user-controlled\ntransformation of array programs. This transformation capability is designed to\nnot just apply to programs written specifically for Loo.py, but also those\nimported from other languages such as Fortran. It eases the trade-off between\nachieving high performance, portability, and programmability by allowing the\nuser to apply a large and growing family of transformations to an input\nprogram. These transformations are expressed in and used from Python and may be\napplied from a variety of settings, including a pragma-like manner from other\nlanguages."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1145/2774959.2774969", 
    "link": "http://arxiv.org/pdf/1504.01161v1", 
    "title": "Python bindings for libcloudph++", 
    "arxiv-id": "1504.01161v1", 
    "author": "Davide Del Vento", 
    "publish": "2015-04-05T20:58:18Z", 
    "summary": "This technical note introduces the Python bindings for libcloudph++. The\nlibcloudph++ is a C++ library of algorithms for representing atmospheric cloud\nmicrophysics in numerical models. The bindings expose the complete\nfunctionality of the library to the Python users. The bindings are implemented\nusing the Boost.Python C++ library and use NumPy arrays. This note includes\nlistings with Python scripts exemplifying the use of selected library\ncomponents. An example solution for using the Python bindings to access\nlibcloudph++ from Fortran is presented."
},{
    "category": "stat.CO", 
    "doi": "10.1145/2774959.2774969", 
    "link": "http://arxiv.org/pdf/1504.02914v1", 
    "title": "Representing numeric data in 32 bits while preserving 64-bit precision", 
    "arxiv-id": "1504.02914v1", 
    "author": "Radford M. Neal", 
    "publish": "2015-04-11T20:33:06Z", 
    "summary": "Data files often consist of numbers having only a few significant decimal\ndigits, whose information content would allow storage in only 32 bits. However,\nwe may require that arithmetic operations involving these numbers be done with\n64-bit floating-point precision, which precludes simply representing the data\nas 32-bit floating-point values. Decimal floating point gives a compact and\nexact representation, but requires conversion with a slow division operation\nbefore it can be used. Here, I show that interesting subsets of 64-bit\nfloating-point values can be compactly and exactly represented by the 32 bits\nconsisting of the sign, exponent, and high-order part of the mantissa, with the\nlower-order 32 bits of the mantissa filled in by table lookup, indexed by bits\nfrom the part of the mantissa retained, and possibly from the exponent. For\nexample, decimal data with 4 or fewer digits to the left of the decimal point\nand 2 or fewer digits to the right of the decimal point can be represented in\nthis way using the lower-order 5 bits of the retained part of the mantissa as\nthe index. Data consisting of 6 decimal digits with the decimal point in any of\nthe 7 positions before or after one of the digits can also be represented this\nway, and decoded using 19 bits from the mantissa and exponent as the index.\nEncoding with such a scheme is a simple copy of half the 64-bit value, followed\nif necessary by verification that the value can be represented, by checking\nthat it decodes correctly. Decoding requires only extraction of index bits and\na table lookup. Lookup in a small table will usually reference cache; even with\nlarger tables, decoding is still faster than conversion from decimal floating\npoint with a division operation. I discuss how such schemes perform on recent\ncomputer systems, and how they might be used to automatically compress large\narrays in interpretive languages such as R."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jpdc.2015.06.010", 
    "link": "http://arxiv.org/pdf/1504.05022v2", 
    "title": "A Framework for General Sparse Matrix-Matrix Multiplication on GPUs and   Heterogeneous Processors", 
    "arxiv-id": "1504.05022v2", 
    "author": "Brian Vinter", 
    "publish": "2015-04-20T11:58:05Z", 
    "summary": "General sparse matrix-matrix multiplication (SpGEMM) is a fundamental\nbuilding block for numerous applications such as algebraic multigrid method\n(AMG), breadth first search and shortest path problem. Compared to other sparse\nBLAS routines, an efficient parallel SpGEMM implementation has to handle extra\nirregularity from three aspects: (1) the number of nonzero entries in the\nresulting sparse matrix is unknown in advance, (2) very expensive parallel\ninsert operations at random positions in the resulting sparse matrix dominate\nthe execution time, and (3) load balancing must account for sparse data in both\ninput matrices.\n  In this work we propose a framework for SpGEMM on GPUs and emerging CPU-GPU\nheterogeneous processors. This framework particularly focuses on the above\nthree problems. Memory pre-allocation for the resulting matrix is organized by\na hybrid method that saves a large amount of global memory space and\nefficiently utilizes the very limited on-chip scratchpad memory. Parallel\ninsert operations of the nonzero entries are implemented through the GPU merge\npath algorithm that is experimentally found to be the fastest GPU merge\napproach. Load balancing builds on the number of necessary arithmetic\noperations on the nonzero entries and is guaranteed in all stages.\n  Compared with the state-of-the-art CPU and GPU SpGEMM methods, our approach\ndelivers excellent absolute performance and relative speedups on various\nbenchmarks multiplying matrices with diverse sparsity structures. Furthermore,\non heterogeneous processors, our SpGEMM approach achieves higher throughput by\nusing re-allocatable shared virtual memory.\n  The source code of this work is available at\nhttps://github.com/bhSPARSE/Benchmark_SpGEMM_using_CSR"
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.parco.2015.04.004", 
    "link": "http://arxiv.org/pdf/1504.06474v2", 
    "title": "Speculative Segmented Sum for Sparse Matrix-Vector Multiplication on   Heterogeneous Processors", 
    "arxiv-id": "1504.06474v2", 
    "author": "Brian Vinter", 
    "publish": "2015-04-24T11:23:38Z", 
    "summary": "Sparse matrix-vector multiplication (SpMV) is a central building block for\nscientific software and graph applications. Recently, heterogeneous processors\ncomposed of different types of cores attracted much attention because of their\nflexible core configuration and high energy efficiency. In this paper, we\npropose a compressed sparse row (CSR) format based SpMV algorithm utilizing\nboth types of cores in a CPU-GPU heterogeneous processor. We first\nspeculatively execute segmented sum operations on the GPU part of a\nheterogeneous processor and generate a possibly incorrect results. Then the CPU\npart of the same chip is triggered to re-arrange the predicted partial sums for\na correct resulting vector. On three heterogeneous processors from Intel, AMD\nand nVidia, using 20 sparse matrices as a benchmark suite, the experimental\nresults show that our method obtains significant performance improvement over\nthe best existing CSR-based SpMV algorithms. The source code of this work is\ndownloadable at https://github.com/bhSPARSE/Benchmark_SpMV_using_CSR"
},{
    "category": "cs.PF", 
    "doi": "10.1016/j.parco.2015.04.004", 
    "link": "http://arxiv.org/pdf/1504.08035v1", 
    "title": "The ELAPS Framework: Experimental Linear Algebra Performance Studies", 
    "arxiv-id": "1504.08035v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2015-04-29T21:58:50Z", 
    "summary": "Optimal use of computing resources requires extensive coding, tuning and\nbenchmarking. To boost developer productivity in these time consuming tasks, we\nintroduce the Experimental Linear Algebra Performance Studies framework\n(ELAPS), a multi-platform open source environment for fast yet powerful\nperformance experimentation with dense linear algebra kernels, algorithms, and\nlibraries. ELAPS allows users to construct experiments to investigate how\nperformance and efficiency vary depending on factors such as caching,\nalgorithmic parameters, problem size, and parallelism. Experiments are designed\neither through Python scripts or a specialized GUI, and run on the whole\nspectrum of architectures, ranging from laptops to clusters, accelerators, and\nsupercomputers. The resulting experiment reports provide various metrics and\nstatistics that can be analyzed both numerically and visually. We demonstrate\nthe use of ELAPS in four concrete application scenarios and in as many\ncomputing environments, illustrating its practical value in supporting critical\nperformance decisions."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.00344v1", 
    "title": "Fireflies: New software for interactively exploring dynamical systems   using GPU computing", 
    "arxiv-id": "1505.00344v1", 
    "author": "Robert Merrison-Hort", 
    "publish": "2015-05-02T13:57:16Z", 
    "summary": "In non-linear systems, where explicit analytic solutions usually can't be\nfound, visualisation is a powerful approach which can give insights into the\ndynamical behaviour of models; it is also crucial for teaching this area of\nmathematics. In this paper we present new software, Fireflies, which exploits\nthe power of graphical processing unit (GPU) computing to produce spectacular\ninteractive visualisations of arbitrary systems of ordinary differential\nequations. In contrast to typical phase portraits, Fireflies draws the current\nposition of trajectories (projected onto 2D or 3D space) as single points of\nlight, which move as the system is simulated. Due to the massively parallel\nnature of GPU hardware, Fireflies is able to simulate millions of trajectories\nin parallel (even on standard desktop computer hardware), producing \"swarms\" of\nparticles that move around the screen in real-time according to the equations\nof the system. Particles that move forwards in time reveal stable attractors\n(e.g. fixed points and limit cycles), while the option of integrating another\ngroup of trajectories backwards in time can reveal unstable objects\n(repellers). Fireflies allows the user to change the parameters of the system\nas it is running, in order to see the effect that they have on the dynamics and\nto observe bifurcations. We demonstrate the capabilities of the software with\nthree examples: a two-dimensional \"mean field\" model of neuronal activity, the\nclassical Lorenz system, and a 15-dimensional model of three interacting\nbiologically realistic neurons."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.00383v1", 
    "title": "Tracking Many Solution Paths of a Polynomial Homotopy on a Graphics   Processing Unit", 
    "arxiv-id": "1505.00383v1", 
    "author": "Xiangcheng Yu", 
    "publish": "2015-05-03T00:50:23Z", 
    "summary": "Polynomial systems occur in many areas of science and engineering. Unlike\ngeneral nonlinear systems, the algebraic structure enables to compute all\nsolutions of a polynomial system. We describe our massive parallel\npredictor-corrector algorithms to track many solution paths of a polynomial\nhomotopy. The data parallelism that provides the speedups stems from the\nevaluation and differentiation of the monomials in the same polynomial system\nat different data points, which are the points on the solution paths.\nPolynomial homotopies that have tens of thousands of solution paths can keep a\nsufficiently large amount of threads occupied. Our accelerated code combines\nthe reverse mode of algorithmic differentiation with double double and quad\ndouble precision to compute more accurate results faster."
},{
    "category": "cs.LO", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.01629v1", 
    "title": "LeoPARD --- A Generic Platform for the Implementation of Higher-Order   Reasoners", 
    "arxiv-id": "1505.01629v1", 
    "author": "Christoph Benzm\u00fcller", 
    "publish": "2015-05-07T08:54:19Z", 
    "summary": "LeoPARD supports the implementation of knowledge representation and reasoning\ntools for higher-order logic(s). It combines a sophisticated data structure\nlayer (polymorphically typed {\\lambda}-calculus with nameless spine notation,\nexplicit substitutions, and perfect term sharing) with an ambitious multi-agent\nblackboard architecture (supporting prover parallelism at the term, clause, and\nsearch level). Further features of LeoPARD include a parser for all TPTP\ndialects, a command line interpreter, and generic means for the integration of\nexternal reasoners."
},{
    "category": "math.AG", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.05241v1", 
    "title": "Software for the Gale transform of fewnomial systems and a Descartes   rule for fewnomials", 
    "arxiv-id": "1505.05241v1", 
    "author": "Frank Sottile", 
    "publish": "2015-05-20T04:11:49Z", 
    "summary": "We give a Descartes'-like bound on the number of positive solutions to a\nsystem of fewnomials that holds when its exponent vectors are not in convex\nposition and a sign condition is satisfied. This was discovered while\ndeveloping algorithms and software for computing the Gale transform of a\nfewnomial system, which is our main goal. This software is a component of a\npackage we are developing for Khovanskii-Rolle continuation, which is a\nnumerical algorithm to compute the real solutions to a system of fewnomials."
},{
    "category": "cs.LG", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.06807v1", 
    "title": "MLlib: Machine Learning in Apache Spark", 
    "arxiv-id": "1505.06807v1", 
    "author": "Ameet Talwalkar", 
    "publish": "2015-05-26T05:12:23Z", 
    "summary": "Apache Spark is a popular open-source platform for large-scale data\nprocessing that is well-suited for iterative machine learning tasks. In this\npaper we present MLlib, Spark's open-source distributed machine learning\nlibrary. MLlib provides efficient functionality for a wide range of learning\nsettings and includes several underlying statistical, optimization, and linear\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\nprovides a high-level API that leverages Spark's rich ecosystem to simplify the\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\nrapid growth due to its vibrant open-source community of over 140 contributors,\nand includes extensive documentation to support further growth and to let users\nquickly get up to speed."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.07589v3", 
    "title": "SYM-ILDL: Incomplete $LDL^{T}$ Factorization of Symmetric Indefinite and   Skew-Symmetric Matrices", 
    "arxiv-id": "1505.07589v3", 
    "author": "Paul Liu", 
    "publish": "2015-05-28T08:25:45Z", 
    "summary": "SYM-ILDL is a numerical software package that computes incomplete $LDL^{T}$\n(or `ILDL') factorizations of symmetric indefinite and real skew-symmetric\nmatrices. The core of the algorithm is a Crout variant of incomplete LU (ILU),\noriginally introduced and implemented for symmetric matrices by [Li and Saad,\nCrout versions of ILU factorization with pivoting for sparse symmetric\nmatrices, Transactions on Numerical Analysis 20, pp. 75--85, 2005]. Our code is\neconomical in terms of storage and it deals with real skew-symmetric matrices\nas well, in addition to symmetric ones. The package is written in C++ and it is\ntemplated, open source, and includes a MATLAB interface. The code includes\nbuilt-in RCM and AMD reordering, two equilibration strategies, threshold\nBunch-Kaufman pivoting and rook pivoting, as well as a wrapper to MC64, a\npopular matching based equilibration and reordering algorithm. We also include\ntwo built-in iterative solvers: SQMR preconditioned with ILDL, or MINRES\npreconditioned with a symmetric positive definite preconditioner based on the\nILDL factorization."
},{
    "category": "cs.MS", 
    "doi": "10.1142/S0218127415501813", 
    "link": "http://arxiv.org/pdf/1505.08067v1", 
    "title": "Efficient FFT mapping on GPU for radar processing application: modeling   and implementation", 
    "arxiv-id": "1505.08067v1", 
    "author": "Michel Syska", 
    "publish": "2015-05-29T14:45:03Z", 
    "summary": "General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel\nHaswell) increasingly add GPU computing power to the former multicore\narchitectures. When used for embedded applications (for us, Synthetic aperture\nradar) with intensive signal processing requirements, they must constantly\ncompute convolution algorithms, such as the famous Fast Fourier Transform. Due\nto its \"fractal\" nature (the typical butterfly shape, with larger FFTs defined\nas combination of smaller ones with auxiliary data array transpose functions),\none can hope to compute analytically the size of the largest FFT that can be\nperformed locally on an elementary GPU compute block. Then, the full\napplication must be organized around this given building block size. Now, due\nto phenomena involved in the data transfers between various memory levels\nacross CPUs and GPUs, the optimality of such a scheme is only loosely\npredictable (as communications tend to overcome in time the complexity of\ncomputations). Therefore a mix of (theoretical) analytic approach and\n(practical) runtime validation is here needed. As we shall illustrate, this\noccurs at both stage, first at the level of deciding on a given elementary FFT\nblock size, then at the full application level."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.04462v1", 
    "title": "Accurate computation of Galerkin double surface integrals in the 3-D   boundary element method", 
    "arxiv-id": "1506.04462v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2015-06-15T02:49:10Z", 
    "summary": "Many boundary element integral equation kernels are based on the Green's\nfunctions of the Laplace and Helmholtz equations in three dimensions. These\ninclude, for example, the Laplace, Helmholtz, elasticity, Stokes, and Maxwell's\nequations. Integral equation formulations lead to more compact, but dense\nlinear systems. These dense systems are often solved iteratively via Krylov\nsubspace methods, which may be accelerated via the fast multipole method. There\nare advantages to Galerkin formulations for such integral equations, as they\ntreat problems associated with kernel singularity, and lead to symmetric and\nbetter conditioned matrices. However, the Galerkin method requires each entry\nin the system matrix to be created via the computation of a double surface\nintegral over one or more pairs of triangles. There are a number of\nsemi-analytical methods to treat these integrals, which all have some issues,\nand are discussed in this paper. We present novel methods to compute all the\nintegrals that arise in Galerkin formulations involving kernels based on the\nLaplace and Helmholtz Green's functions to any specified accuracy. Integrals\ninvolving completely geometrically separated triangles are non-singular and are\ncomputed using a technique based on spherical harmonics and multipole\nexpansions and translations, which results in the integration of polynomial\nfunctions over the triangles. Integrals involving cases where the triangles\nhave common vertices, edges, or are coincident are treated via scaling and\nsymmetry arguments, combined with automatic recursive geometric decomposition\nof the integrals. Example results are presented, and the developed software is\navailable as open source."
},{
    "category": "math.NA", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.04463v1", 
    "title": "FEAST Eigensolver for non-Hermitian Problems", 
    "arxiv-id": "1506.04463v1", 
    "author": "Ping Tak Peter Tang", 
    "publish": "2015-06-15T02:55:29Z", 
    "summary": "A detailed new upgrade of the FEAST eigensolver targeting non-Hermitian\neigenvalue problems is presented and thoroughly discussed. It aims at\nbroadening the class of eigenproblems that can be addressed within the\nframework of the FEAST algorithm. The algorithm is ideally suited for computing\nselected interior eigenvalues and their associated right/left bi-orthogonal\neigenvectors,located within a subset of the complex plane. It combines subspace\niteration with efficient contour integration techniques that approximate the\nleft and right spectral projectors. We discuss the various algorithmic choices\nthat have been made to improve the stability and usability of the new\nnon-Hermitian eigensolver. The latter retains the convergence property and\nmulti-level parallelism of Hermitian FEAST, making it a valuable new software\ntool for the scientific community."
},{
    "category": "cs.MS", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.06194v1", 
    "title": "Unstructured Overlapping Mesh Distribution in Parallel", 
    "arxiv-id": "1506.06194v1", 
    "author": "Gerard J. Gorman", 
    "publish": "2015-06-20T02:25:14Z", 
    "summary": "We present a simple mathematical framework and API for parallel mesh and data\ndistribution, load balancing, and overlap generation. It relies on viewing the\nmesh as a Hasse diagram, abstracting away information such as cell shape,\ndimension, and coordinates. The high level of abstraction makes our interface\nboth concise and powerful, as the same algorithm applies to any representable\nmesh, such as hybrid meshes, meshes embedded in higher dimension, and\noverlapped meshes in parallel. We present evidence, both theoretical and\nexperimental, that the algorithms are scalable and efficient. A working\nimplementation can be found in the latest release of the PETSc libraries."
},{
    "category": "cs.GR", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.06745v1", 
    "title": "GraphMaps: Browsing Large Graphs as Interactive Maps", 
    "arxiv-id": "1506.06745v1", 
    "author": "Xiaoji Chen", 
    "publish": "2015-06-22T17:17:34Z", 
    "summary": "Algorithms for laying out large graphs have seen significant progress in the\npast decade. However, browsing large graphs remains a challenge. Rendering\nthousands of graphical elements at once often results in a cluttered image, and\nnavigating these elements naively can cause disorientation. To address this\nchallenge we propose a method called GraphMaps, mimicking the browsing\nexperience of online geographic maps.\n  GraphMaps creates a sequence of layers, where each layer refines the previous\none. During graph browsing, GraphMaps chooses the layer corresponding to the\nzoom level, and renders only those entities of the layer that intersect the\ncurrent viewport. The result is that, regardless of the graph size, the number\nof entities rendered at each view does not exceed a predefined threshold, yet\nall graph elements can be explored by the standard zoom and pan operations.\n  GraphMaps preprocesses a graph in such a way that during browsing, the\ngeometry of the entities is stable, and the viewer is responsive. Our case\nstudies indicate that GraphMaps is useful in gaining an overview of a large\ngraph, and also in exploring a graph on a finer level of detail."
},{
    "category": "cs.CE", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.08258v1", 
    "title": "Trigger detection for adaptive scientific workflows using percentile   sampling", 
    "arxiv-id": "1506.08258v1", 
    "author": "Maher Salloum", 
    "publish": "2015-06-27T04:34:26Z", 
    "summary": "Increasing complexity of scientific simulations and HPC architectures are\ndriving the need for adaptive workflows, where the composition and execution of\ncomputational and data manipulation steps dynamically depend on the\nevolutionary state of the simulation itself. Consider for example, the\nfrequency of data storage. Critical phases of the simulation should be captured\nwith high frequency and with high fidelity for post-analysis, however we cannot\nafford to retain the same frequency for the full simulation due to the high\ncost of data movement. We can instead look for triggers, indicators that the\nsimulation will be entering a critical phase and adapt the workflow\naccordingly.\n  We present a method for detecting triggers and demonstrate its use in direct\nnumerical simulations of turbulent combustion using S3D. We show that chemical\nexplosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator\nfor rapid increase in heat release. However, exhaustive computation of CEMA\nvalues dominates the total simulation, thus is prohibitively expensive. To\novercome this bottleneck, we propose a quantile-sampling approach. Our\nalgorithm comes with provable error/confidence bounds, as a function of the\nnumber of samples. Most importantly, the number of samples is independent of\nthe problem size, thus our proposed algorithm offers perfect scalability. Our\nexperiments on homogeneous charge compression ignition (HCCI) and reactivity\ncontrolled compression ignition (RCCI) simulations show that the proposed\nmethod can detect rapid increases in heat release, and its computational\noverhead is negligible. Our results will be used for dynamic workflow decisions\nabout data storage and mesh resolution in future combustion simulations.\nProposed framework is generalizable and we detail how it could be applied to a\nbroad class of scientific simulation workflows."
},{
    "category": "cs.PF", 
    "doi": "10.1109/TAP.2016.2546951", 
    "link": "http://arxiv.org/pdf/1506.08988v1", 
    "title": "Architecture-Aware Configuration and Scheduling of Matrix Multiplication   on Asymmetric Multicore Processors", 
    "arxiv-id": "1506.08988v1", 
    "author": "Enrique S. Quintana-Ort\u00ed", 
    "publish": "2015-06-30T08:35:15Z", 
    "summary": "Asymmetric multicore processors (AMPs) have recently emerged as an appealing\ntechnology for severely energy-constrained environments, especially in mobile\nappliances where heterogeneity in applications is mainstream. In addition,\ngiven the growing interest for low-power high performance computing, this type\nof architectures is also being investigated as a means to improve the\nthroughput-per-Watt of complex scientific applications.\n  In this paper, we design and embed several architecture-aware optimizations\ninto a multi-threaded general matrix multiplication (gemm), a key operation of\nthe BLAS, in order to obtain a high performance implementation for ARM\nbig.LITTLE AMPs. Our solution is based on the reference implementation of gemm\nin the BLIS library, and integrates a cache-aware configuration as well as\nasymmetric--static and dynamic scheduling strategies that carefully tune and\ndistribute the operation's micro-kernels among the big and LITTLE cores of the\ntarget processor. The experimental results on a Samsung Exynos 5422, a\nsystem-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the\nbig.LITTLE model, expose that our cache-aware versions of gemm with asymmetric\nscheduling attain important gains in performance with respect to its\narchitecture-oblivious counterparts while exploiting all the resources of the\nAMP to deliver considerable energy efficiency."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPEC.2015.7322448", 
    "link": "http://arxiv.org/pdf/1507.01066v2", 
    "title": "Graphulo Implementation of Server-Side Sparse Matrix Multiply in the   Accumulo Database", 
    "arxiv-id": "1507.01066v2", 
    "author": "Adam Fuchs", 
    "publish": "2015-07-04T05:20:22Z", 
    "summary": "The Apache Accumulo database excels at distributed storage and indexing and\nis ideally suited for storing graph data. Many big data analytics compute on\ngraph data and persist their results back to the database. These graph\ncalculations are often best performed inside the database server. The GraphBLAS\nstandard provides a compact and efficient basis for a wide range of graph\napplications through a small number of sparse matrix operations. In this\narticle, we implement GraphBLAS sparse matrix multiplication server-side by\nleveraging Accumulo's native, high-performance iterators. We compare the\nmathematics and performance of inner and outer product implementations, and\nshow how an outer product implementation achieves optimal performance near\nAccumulo's peak write rate. We offer our work as a core component to the\nGraphulo library that will deliver matrix math primitives for graph analytics\nwithin Accumulo."
},{
    "category": "cs.MS", 
    "doi": "10.1137/15M1026171", 
    "link": "http://arxiv.org/pdf/1507.01888v1", 
    "title": "MADNESS: A Multiresolution, Adaptive Numerical Environment for   Scientific Simulation", 
    "arxiv-id": "1507.01888v1", 
    "author": "Yukina Yokoi", 
    "publish": "2015-07-05T15:32:13Z", 
    "summary": "MADNESS (multiresolution adaptive numerical environment for scientific\nsimulation) is a high-level software environment for solving integral and\ndifferential equations in many dimensions that uses adaptive and fast harmonic\nanalysis methods with guaranteed precision based on multiresolution analysis\nand separated representations. Underpinning the numerical capabilities is a\npowerful petascale parallel programming environment that aims to increase both\nprogrammer productivity and code scalability. This paper describes the features\nand capabilities of MADNESS and briefly discusses some current applications in\nchemistry and several areas of physics."
},{
    "category": "math.NA", 
    "doi": "10.1137/15M1026171", 
    "link": "http://arxiv.org/pdf/1508.02219v1", 
    "title": "Using the VBARMS method in parallel computing", 
    "arxiv-id": "1508.02219v1", 
    "author": "Aldo Bonfiglioli", 
    "publish": "2015-08-10T12:26:49Z", 
    "summary": "The paper describes an improved parallel MPI-based implementation of VBARMS,\na variable block variant of the pARMS preconditioner proposed by Li,~Saad and\nSosonkina [NLAA, 2003] for solving general nonsymmetric linear systems. The\nparallel VBARMS solver can detect automatically exact or approximate dense\nstructures in the linear system, and exploits this information to achieve\nimproved reliability and increased throughput during the factorization. A novel\ngraph compression algorithm is discussed that finds these approximate dense\nblocks structures and requires only one simple to use parameter. A complete\nstudy of the numerical and parallel performance of parallel VBARMS is presented\nfor the analysis of large turbulent Navier-Stokes equations on a suite of\nthree-dimensional test cases."
},{
    "category": "cs.NA", 
    "doi": "10.1137/15M1026171", 
    "link": "http://arxiv.org/pdf/1508.03211v1", 
    "title": "Computing accurate Horner form approximations to special functions in   finite precision arithmetic", 
    "arxiv-id": "1508.03211v1", 
    "author": "Tor G. J. Myklebust", 
    "publish": "2015-08-13T13:42:35Z", 
    "summary": "In various applications, computers are required to compute approximations to\nunivariate elementary and special functions such as $\\exp$ and $\\arctan$ to\nmodest accuracy. This paper proposes a new heuristic for automating the design\nof such implementations. This heuristic takes a certain restricted\nspecification of program structure and the desired error properties as input\nand takes explicit account of roundoff error during evaluation."
},{
    "category": "q-fin.GN", 
    "doi": "10.1137/15M1026171", 
    "link": "http://arxiv.org/pdf/1508.07582v1", 
    "title": "Approximating the Sum of Correlated Lognormals: An Implementation", 
    "arxiv-id": "1508.07582v1", 
    "author": "Mitchell Kerman", 
    "publish": "2015-08-30T14:47:50Z", 
    "summary": "Lognormal random variables appear naturally in many engineering disciplines,\nincluding wireless communications, reliability theory, and finance. So, too,\ndoes the sum of (correlated) lognormal random variables. Unfortunately, no\nclosed form probability distribution exists for such a sum, and it requires\napproximation. Some approximation methods date back over 80 years and most take\none of two approaches, either: 1) an approximate probability distribution is\nderived mathematically, or 2) the sum is approximated by a single lognormal\nrandom variable. In this research, we take the latter approach and review a\nfairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang\n(2007), then implement it using C++. The result is applied to a discrete time\nmodel commonly encountered within the field of financial economics."
},{
    "category": "math.NT", 
    "doi": "10.1137/15M1026171", 
    "link": "http://arxiv.org/pdf/1509.00864v1", 
    "title": "Strong Pseudoprimes to Twelve Prime Bases", 
    "arxiv-id": "1509.00864v1", 
    "author": "Jonathan Webster", 
    "publish": "2015-09-02T20:23:07Z", 
    "summary": "Let $\\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases.\nThis value is known for $1 \\leq m \\leq 11$. We extend this by finding\n$\\psi_{12}$ and $\\psi_{13}$. We also present an algorithm to find all integers\n$n\\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a\nreasonable heuristic assumption we can show that it takes at most\n$B^{2/3+o(1)}$ time."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.advengsoft.2016.01.014", 
    "link": "http://arxiv.org/pdf/1509.03604v2", 
    "title": "Fundamental concepts in the Cyclus nuclear fuel cycle simulation   framework", 
    "arxiv-id": "1509.03604v2", 
    "author": "Paul P. H. Wilson", 
    "publish": "2015-09-11T18:39:59Z", 
    "summary": "As nuclear power expands, technical, economic, political, and environmental\nanalyses of nuclear fuel cycles by simulators increase in importance. To date,\nhowever, current tools are often fleet-based rather than discrete and\nrestrictively licensed rather than open source. Each of these choices presents\na challenge to modeling fidelity, generality, efficiency, robustness, and\nscientific transparency. The Cyclus nuclear fuel cycle simulator framework and\nits modeling ecosystem incorporate modern insights from simulation science and\nsoftware architecture to solve these problems so that challenges in nuclear\nfuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle\nsimulator framework and its modeling ecosystem are presented. Additionally, the\nimplementation of each is discussed in the context of motivating challenges in\nnuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are\ndemonstrated for both open and closed fuel cycles."
},{
    "category": "math.OC", 
    "doi": "10.1016/j.cnsns.2014.08.026", 
    "link": "http://arxiv.org/pdf/1509.04518v1", 
    "title": "A deterministic global optimization using smooth diagonal auxiliary   functions", 
    "arxiv-id": "1509.04518v1", 
    "author": "Dmitri E. Kvasov", 
    "publish": "2015-09-15T12:24:58Z", 
    "summary": "In many practical decision-making problems it happens that functions involved\nin optimization process are black-box with unknown analytical representations\nand hard to evaluate. In this paper, a global optimization problem is\nconsidered where both the goal function~$f(x)$ and its gradient $f'(x)$ are\nblack-box functions. It is supposed that $f'(x)$ satisfies the Lipschitz\ncondition over the search hyperinterval with an unknown Lipschitz constant~$K$.\nA new deterministic `Divide-the-Best' algorithm based on efficient diagonal\npartitions and smooth auxiliary functions is proposed in its basic version, its\nconvergence conditions are studied and numerical experiments executed on eight\nhundred test functions are presented."
},{
    "category": "cond-mat.soft", 
    "doi": "10.1016/j.cpc.2016.02.024", 
    "link": "http://arxiv.org/pdf/1509.04692v1", 
    "title": "Scalable Metropolis Monte Carlo for simulation of hard shapes", 
    "arxiv-id": "1509.04692v1", 
    "author": "Sharon C. Glotzer", 
    "publish": "2015-09-15T19:25:33Z", 
    "summary": "We design and implement HPMC, a scalable hard particle Monte Carlo simulation\ntoolkit, and release it open source as part of HOOMD-blue. HPMC runs in\nparallel on many CPUs and many GPUs using domain decomposition. We employ BVH\ntrees instead of cell lists on the CPU for fast performance, especially with\nlarge particle size disparity, and optimize inner loops with SIMD vector\nintrinsics on the CPU. Our GPU kernel proposes many trial moves in parallel on\na checkerboard and uses a block-level queue to redistribute work among threads\nand avoid divergence. HPMC supports a wide variety of shape classes, including\nspheres / disks, unions of spheres, convex polygons, convex spheropolygons,\nconcave polygons, ellipsoids / ellipses, convex polyhedra, convex\nspheropolyhedra, spheres cut by planes, and concave polyhedra. NVT and NPT\nensembles can be run in 2D or 3D triclinic boxes. Additional integration\nschemes permit Frenkel-Ladd free energy computations and implicit depletant\nsimulations. In a benchmark system of a fluid of 4096 pentagons, HPMC performs\n10 million sweeps in 10 minutes on 96 CPU cores on XSEDE Comet. The same\nsimulation would take 7.6 hours in serial. HPMC also scales to large system\nsizes, and the same benchmark with 16.8 million particles runs in 1.4 hours on\n2048 GPUs on OLCF Titan."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.cpc.2016.02.024", 
    "link": "http://arxiv.org/pdf/1509.04706v1", 
    "title": "Direct high-order edge-preserving regularization for tomographic image   reconstruction", 
    "arxiv-id": "1509.04706v1", 
    "author": "Peter D. Lee", 
    "publish": "2015-09-15T18:23:56Z", 
    "summary": "In this paper we present a new two-level iterative algorithm for tomographic\nimage reconstruction. The algorithm uses a regularization technique, which we\ncall edge-preserving Laplacian, that preserves sharp edges between objects\nwhile damping spurious oscillations in the areas where the reconstructed image\nis smooth. Our numerical simulations demonstrate that the proposed method\noutperforms total variation (TV) regularization and it is competitive with the\ncombined TV-L2 penalty. Obtained reconstructed images show increased\nsignal-to-noise ratio and visually appealing structural features. Computer\nimplementation and parameter control of the proposed technique is\nstraightforward, which increases the feasibility of it across many tomographic\napplications. In this paper, we applied our method to the under-sampled\ncomputed tomography (CT) projection data and also considered a case of\nreconstruction in emission tomography The MATLAB code is provided to support\nobtained results."
},{
    "category": "cs.SI", 
    "doi": "10.1016/j.cpc.2016.02.024", 
    "link": "http://arxiv.org/pdf/1509.06397v2", 
    "title": "SnapVX: A Network-Based Convex Optimization Solver", 
    "arxiv-id": "1509.06397v2", 
    "author": "Jure Leskovec", 
    "publish": "2015-09-21T20:44:12Z", 
    "summary": "SnapVX is a high-performance Python solver for convex optimization problems\ndefined on networks. For these problems, it provides a fast and scalable\nsolution with guaranteed global convergence. SnapVX combines the capabilities\nof two open source software packages: Snap.py and CVXPY. Snap.py is a large\nscale graph processing library, and CVXPY provides a general modeling framework\nfor small-scale subproblems. SnapVX offers a customizable yet easy-to-use\ninterface with out-of-the-box functionality. Based on the Alternating Direction\nMethod of Multipliers (ADMM), it is able to efficiently store, analyze, and\nsolve large optimization problems from a variety of different applications.\nDocumentation, examples, and more can be found on the SnapVX website at\nhttp://snap.stanford.edu/snapvx."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.02.024", 
    "link": "http://arxiv.org/pdf/1509.06935v2", 
    "title": "A shared memory implementation of pipelined Parareal", 
    "arxiv-id": "1509.06935v2", 
    "author": "Daniel Ruprecht", 
    "publish": "2015-09-23T12:04:23Z", 
    "summary": "The paper introduces an OpenMP implementation of pipelined Parareal and\ncompares it to a standard MPI-based implementation. Both versions yield\nessentially identical runtimes, but, depending on the compiler, the OpenMP\nvariant consumes about 7% less energy. However, its key advantage is a\nsignificantly smaller memory footprint. The higher implementation complexity,\nincluding manual control of locks, might make it difficult to use in legacy\ncodes, though."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2016.02.024", 
    "link": "http://arxiv.org/pdf/1509.07919v1", 
    "title": "Analysis of A Splitting Approach for the Parallel Solution of Linear   Systems on GPU Cards", 
    "arxiv-id": "1509.07919v1", 
    "author": "Dan Negrut", 
    "publish": "2015-09-25T23:04:17Z", 
    "summary": "We discuss an approach for solving sparse or dense banded linear systems\n${\\bf A} {\\bf x} = {\\bf b}$ on a Graphics Processing Unit (GPU) card. The\nmatrix ${\\bf A} \\in {\\mathbb{R}}^{N \\times N}$ is possibly nonsymmetric and\nmoderately large; i.e., $10000 \\leq N \\leq 500000$. The ${\\it split\\ and\\\nparallelize}$ (${\\tt SaP}$) approach seeks to partition the matrix ${\\bf A}$\ninto diagonal sub-blocks ${\\bf A}_i$, $i=1,\\ldots,P$, which are independently\nfactored in parallel. The solution may choose to consider or to ignore the\nmatrices that couple the diagonal sub-blocks ${\\bf A}_i$. This approach, along\nwith the Krylov subspace-based iterative method that it preconditions, are\nimplemented in a solver called ${\\tt SaP::GPU}$, which is compared in terms of\nefficiency with three commonly used sparse direct solvers: ${\\tt PARDISO}$,\n${\\tt SuperLU}$, and ${\\tt MUMPS}$. ${\\tt SaP::GPU}$, which runs entirely on\nthe GPU except several stages involved in preliminary row-column permutations,\nis robust and compares well in terms of efficiency with the aforementioned\ndirect solvers. In a comparison against Intel's ${\\tt MKL}$, ${\\tt SaP::GPU}$\nalso fares well when used to solve dense banded systems that are close to being\ndiagonally dominant. ${\\tt SaP::GPU}$ is publicly available and distributed as\nopen source under a permissive BSD3 license."
},{
    "category": "math.NA", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1510.08642v1", 
    "title": "Performance evaluation of multiple precision matrix multiplications   using parallelized Strassen and Winograd algorithms", 
    "arxiv-id": "1510.08642v1", 
    "author": "Tomonori Kouya", 
    "publish": "2015-10-29T10:58:55Z", 
    "summary": "It is well known that Strassen and Winograd algorithms can reduce the\ncomputational costs associated with dense matrix multiplication. We have\nalready shown that they are also very effective for software-based multiple\nprecision floating-point arithmetic environments such as the MPFR/GMP library.\nIn this paper, we show that we can obtain the same effectiveness for\ndouble-double (DD) and quadruple-double (QD) environments supported by the QD\nlibrary, and that parallelization can increase the speed of these multiple\nprecision matrix multiplications. Finally, we demonstrate that our implemented\nparallelized Strassen and Winograd algorithms can increase the speed of\nparallelized LU decomposition."
},{
    "category": "cs.CE", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1511.02134v1", 
    "title": "A quantitative performance analysis for Stokes solvers at the extreme   scale", 
    "arxiv-id": "1511.02134v1", 
    "author": "Barbara Wohlmuth", 
    "publish": "2015-11-06T16:07:04Z", 
    "summary": "This article presents a systematic quantitative performance analysis for\nlarge finite element computations on extreme scale computing systems. Three\nparallel iterative solvers for the Stokes system, discretized by low order\ntetrahedral elements, are compared with respect to their numerical efficiency\nand their scalability running on up to $786\\,432$ parallel threads. A genuine\nmultigrid method for the saddle point system using an Uzawa-type smoother\nprovides the best overall performance with respect to memory consumption and\ntime-to-solution. The largest system solved on a Blue Gene/Q system has more\nthan ten trillion ($1.1 \\cdot 10 ^{13}$) unknowns and requires about 13 minutes\ncompute time. Despite the matrix free and highly optimized implementation, the\nmemory requirement for the solution vector and the auxiliary vectors is about\n200 TByte. Brandt's notion of \"textbook multigrid efficiency\" is employed to\nstudy the algorithmic performance of iterative solvers. A recent extension of\nthis paradigm to \"parallel textbook multigrid efficiency\" makes it possible to\nassess also the efficiency of parallel iterative solvers for a given hardware\narchitecture in absolute terms. The efficiency of the method is demonstrated\nfor simulating incompressible fluid flow in a pipe filled with spherical\nobstacles."
},{
    "category": "cs.DC", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1511.02166v1", 
    "title": "Evaluation of the Intel Xeon Phi and NVIDIA K80 as accelerators for   two-dimensional panel codes", 
    "arxiv-id": "1511.02166v1", 
    "author": "Lukas Einkemmer", 
    "publish": "2015-11-06T17:17:36Z", 
    "summary": "To predict the properties of fluid flow over a solid geometry is an important\nengineering problem. In many applications so-called panel methods (or boundary\nelement methods) have become the standard approach to solve the corresponding\npartial differential equation. Since panel methods in two dimensions are\ncomputationally cheap, they are well suited as the inner solver in an\noptimization algorithm.\n  In this paper we evaluate the performance of the Intel Xeon Phi 7120 and the\nNVIDIA K80 to accelerate such an optimization algorithm. For that purpose, we\nhave implemented an optimized version of the algorithm on the CPU and Xeon Phi\n(based on OpenMP, vectorization, and the Intel MKL library) and on the GPU\n(based on CUDA and the MAGMA library). We present timing results for all codes\nand discuss the similarities and differences between the three implementations.\nOverall, we observe a speedup of approximately $2.5$ for adding a Intel Xeon\nPhi 7120 to a dual socket workstation and a speedup between $3$ and $3.5$ for\nadding a NVIDIA K80 to a dual socket workstation."
},{
    "category": "cs.MS", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1511.06487v3", 
    "title": "mplrs: A scalable parallel vertex/facet enumeration code", 
    "arxiv-id": "1511.06487v3", 
    "author": "Charles Jordan", 
    "publish": "2015-11-20T04:54:22Z", 
    "summary": "We describe a new parallel implementation, mplrs, of the vertex enumeration\ncode lrs that uses the MPI parallel environment and can be run on a network of\ncomputers. The implementation makes use of a C wrapper that essentially uses\nthe existing lrs code with only minor modifications. mplrs was derived from the\nearlier parallel implementation plrs, written by G. Roumanis in C++. plrs uses\nthe Boost library and runs on a shared memory machine. In developing mplrs we\ndiscovered a method of balancing the parallel tree search, called budgeting,\nthat greatly improves parallelization beyond the bottleneck encountered\npreviously at around 32 cores.\n  This method can be readily adapted for use in other reverse search\nenumeration codes. We also report some preliminary computational results\ncomparing parallel and sequential codes for vertex/facet enumeration problems\nfor convex polyhedra. The problems chosen span the range from simple to highly\ndegenerate polytopes. For most problems tested, the results clearly show the\nadvantage of using the parallel implementation mplrs of the reverse search\nbased code lrs, even when as few as 8 cores are available. For some problems\nalmost linear speedup was observed up to 1200 cores, the largest number of\ncores tested."
},{
    "category": "cs.DC", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1512.01274v1", 
    "title": "MXNet: A Flexible and Efficient Machine Learning Library for   Heterogeneous Distributed Systems", 
    "arxiv-id": "1512.01274v1", 
    "author": "Zheng Zhang", 
    "publish": "2015-12-03T22:49:21Z", 
    "summary": "MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines."
},{
    "category": "cs.CG", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1512.02960v1", 
    "title": "Ensembles of Cycles Programmed in GiNaC", 
    "arxiv-id": "1512.02960v1", 
    "author": "Vladimir V. Kisil", 
    "publish": "2015-12-09T17:32:30Z", 
    "summary": "This library manipulates ensembles of cycles (quadrics), which are\ninterrelated through certain geometric relations (to be orthogonal, to be\ntangent, etc.). The code operates with numeric and symbolic data of cycles in\nspaces of arbitrary dimensionality and metrics with any signatures. In the\ntwo-dimensional case illustrations and animations can be produced."
},{
    "category": "hep-lat", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1512.03487v1", 
    "title": "Grid: A next generation data parallel C++ QCD library", 
    "arxiv-id": "1512.03487v1", 
    "author": "Antonin Portelli", 
    "publish": "2015-12-10T23:51:19Z", 
    "summary": "In this proceedings we discuss the motivation, implementation details, and\nperformance of a new physics code base called Grid. It is intended to be more\nperformant, more general, but similar in spirit to QDP++\\cite{QDP}. Our\napproach is to engineer the basic type system to be consistently fast, rather\nthan bolt on a few optimised routines, and we are attempt to write all our\noptimised routines directly in the Grid framework. It is hoped this will\ndeliver best known practice performance across the next generation of\nsupercomputers, which will provide programming challenges to traditional scalar\ncodes.\n  We illustrate the programming patterns used to implement our goals, and\nadvances in productivity that have been enabled by using new features in C++11."
},{
    "category": "q-bio.QM", 
    "doi": "10.14495/jsiaml.8.21", 
    "link": "http://arxiv.org/pdf/1601.04458v1", 
    "title": "Reducing local minima in fitness landscapes of parameter estimation by   using piecewise evaluation and state estimation", 
    "arxiv-id": "1601.04458v1", 
    "author": "Sven Sahle", 
    "publish": "2016-01-18T10:38:52Z", 
    "summary": "Ordinary differential equations (ODE) are widely used for modeling in Systems\nBiology. As most commonly only some of the kinetic parameters are measurable or\nprecisely known, parameter estimation techniques are applied to parametrize the\nmodel to experimental data. A main challenge for the parameter estimation is\nthe complexity of the parameter space, especially its high dimensionality and\nlocal minima.\n  Parameter estimation techniques consist of an objective function, measuring\nhow well a certain parameter set describes the experimental data, and an\noptimization algorithm that optimizes this objective function. A lot of effort\nhas been spent on developing highly sophisticated optimization algorithms to\ncope with the complexity in the parameter space, but surprisingly few articles\naddress the influence of the objective function on the computational complexity\nin finding global optima. We extend a recently developed multiple shooting for\nstochastic systems (MSS) objective function for parameter estimation of\nstochastic models and apply it to parameter estimation of ODE models. This MSS\nobjective function treats the intervals between measurement points separately.\nThis separate treatment allows the ODE trajectory to stay closer to the data\nand we show that it reduces the complexity of the parameter space.\n  We use examples from Systems Biology, namely a Lotka-Volterra model, a\nFitzHugh-Nagumo oscillator and a Calcium oscillation model, to demonstrate the\npower of the MSS approach for reducing the complexity and the number of local\nminima in the parameter space. The approach is fully implemented in the COPASI\nsoftware package and, therefore, easily accessible for a wide community of\nresearchers."
},{
    "category": "cs.DC", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1601.07944v1", 
    "title": "Discontinuous Galerkin methods on graphics processing units for   nonlinear hyperbolic conservation laws", 
    "arxiv-id": "1601.07944v1", 
    "author": "Lilia Krivodonova", 
    "publish": "2016-01-28T22:49:50Z", 
    "summary": "We present a novel implementation of the modal discontinuous Galerkin (DG)\nmethod for hyperbolic conservation laws in two dimensions on graphics\nprocessing units (GPUs) using NVIDIA's Compute Unified Device Architecture\n(CUDA). Both flexible and highly accurate, DG methods accommodate parallel\narchitectures well as their discontinuous nature produces element-local\napproximations. High performance scientific computing suits GPUs well, as these\npowerful, massively parallel, cost-effective devices have recently included\nsupport for double-precision floating point numbers. Computed examples for\nEuler equations over unstructured triangle meshes demonstrate the effectiveness\nof our implementation on an NVIDIA GTX 580 device. Profiling of our method\nreveals performance comparable to an existing nodal DG-GPU implementation for\nlinear problems."
},{
    "category": "cs.NA", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1602.01376v1", 
    "title": "Inv-ASKIT: A Parallel Fast Diret Solver for Kernel Matrices", 
    "arxiv-id": "1602.01376v1", 
    "author": "George Biros", 
    "publish": "2016-02-03T17:23:24Z", 
    "summary": "We present a parallel algorithm for computing the approximate factorization\nof an $N$-by-$N$ kernel matrix. Once this factorization has been constructed\n(with $N \\log^2 N $ work), we can solve linear systems with this matrix with $N\n\\log N $ work. Kernel matrices represent pairwise interactions of points in\nmetric spaces. They appear in machine learning, approximation theory, and\ncomputational physics. Kernel matrices are typically dense (matrix\nmultiplication scales quadratically with $N$) and ill-conditioned (solves can\nrequire 100s of Krylov iterations). Thus, fast algorithms for matrix\nmultiplication and factorization are critical for scalability.\n  Recently we introduced ASKIT, a new method for approximating a kernel matrix\nthat resembles N-body methods. Here we introduce INV-ASKIT, a factorization\nscheme based on ASKIT. We describe the new method, derive complexity estimates,\nand conduct an empirical study of its accuracy and scalability. We report\nresults on real-world datasets including \"COVTYPE\" ($0.5$M points in 54\ndimensions), \"SUSY\" ($4.5$M points in 8 dimensions) and \"MNIST\" (2M points in\n784 dimensions) using shared and distributed memory parallelism. In our largest\nrun we approximately factorize a dense matrix of size 32M $\\times$ 32M\n(generated from points in 64 dimensions) on 4,096 Sandy-Bridge cores. To our\nknowledge these results improve the state of the art by several orders of\nmagnitude."
},{
    "category": "stat.CO", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.00293v2", 
    "title": "RWebData: A High-Level Interface to the Programmable Web", 
    "arxiv-id": "1603.00293v2", 
    "author": "Ulrich Matter", 
    "publish": "2016-03-01T14:44:47Z", 
    "summary": "The rise of the programmable web offers new opportunities for the empirically\ndriven social sciences. The access, compilation and preparation of data from\nthe programmable web for statistical analysis can, however, involve substantial\nup-front costs for the practical researcher. The R-package RWebData provides a\nhigh-level framework that allows data to be easily collected from the\nprogrammable web in a format that can directly be used for statistical analysis\nin R (R Core Team 2013) without bothering about the data's initial format and\nnesting structure. It was developed specifically for users who have no\nexperience with web technologies and merely use R as a statistical software.\nThe core idea and methodological contribution of the package are the\ndisentangling of parsing web data and mapping them with a generic algorithm\n(independent of the initial data structure) to a flat table-like\nrepresentation. This paper provides an overview of the high-level functions for\nR-users, explains the basic architecture of the package, and illustrates the\nimplemented data mapping algorithm."
},{
    "category": "cs.MS", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.02297v1", 
    "title": "TTC: A high-performance Compiler for Tensor Transpositions", 
    "arxiv-id": "1603.02297v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-03-07T21:13:00Z", 
    "summary": "We present TTC, an open-source parallel compiler for multidimensional tensor\ntranspositions. In order to generate high-performance C++ code, TTC explores a\nnumber of optimizations, including software prefetching, blocking,\nloop-reordering, and explicit vectorization. To evaluate the performance of\nmultidimensional transpositions across a range of possible use-cases, we also\nrelease a benchmark covering arbitrary transpositions of up to six dimensions.\nPerformance results show that the routines generated by TTC achieve close to\npeak memory bandwidth on both the Intel Haswell and the AMD Steamroller\narchitectures, and yield significant performance gains over modern compilers.\nBy implementing a set of pruning heuristics, TTC allows users to limit the\nnumber of potential solutions; this option is especially useful when dealing\nwith high-dimensional tensors, as the search space might become prohibitively\nlarge. Experiments indicate that when only 100 potential solutions are\nconsidered, the resulting performance is about 99% of that achieved with\nexhaustive search."
},{
    "category": "cs.DC", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.02526v1", 
    "title": "Testing fine-grained parallelism for the ADMM on a factor-graph", 
    "arxiv-id": "1603.02526v1", 
    "author": "Jos\u00e9 Bento", 
    "publish": "2016-03-08T14:13:38Z", 
    "summary": "There is an ongoing effort to develop tools that apply distributed\ncomputational resources to tackle large problems or reduce the time to solve\nthem. In this context, the Alternating Direction Method of Multipliers (ADMM)\narises as a method that can exploit distributed resources like the dual ascent\nmethod and has the robustness and improved convergence of the augmented\nLagrangian method. Traditional approaches to accelerate the ADMM using multiple\ncores are problem-specific and often require multi-core programming. By\ncontrast, we propose a problem-independent scheme of accelerating the ADMM that\ndoes not require the user to write any parallel code. We show that this scheme,\nan interpretation of the ADMM as a message-passing algorithm on a factor-graph,\ncan automatically exploit fine-grained parallelism both in GPUs and\nshared-memory multi-core computers and achieves significant speedup in such\ndiverse application domains as combinatorial optimization, machine learning,\nand optimal control. Specifically, we obtain 10-18x speedup using a GPU, and\n5-9x using multiple CPU cores, over a serial, optimized C-version of the ADMM,\nwhich is similar to the typical speedup reported for existing GPU-accelerated\nlibraries, including cuFFT (19x), cuBLAS (17x), and cuRAND (8x)."
},{
    "category": "cs.MS", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.03236v4", 
    "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic   Differentiation", 
    "arxiv-id": "1603.03236v4", 
    "author": "Sebastian Weichwald", 
    "publish": "2016-03-10T12:23:12Z", 
    "summary": "Optimization on manifolds is a class of methods for optimization of an\nobjective function, subject to constraints which are smooth, in the sense that\nthe set of points which satisfy the constraints admits the structure of a\ndifferentiable manifold. While many optimization problems are of the described\nform, technicalities of differential geometry and the laborious calculation of\nderivatives pose a significant barrier for experimenting with these methods.\n  We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox\nfor optimization on manifolds, implemented in Python, that---similarly to the\nManopt Matlab toolbox---implements several manifold geometries and optimization\nalgorithms. Moreover, we lower the barriers to users further by using automated\ndifferentiation for calculating derivative information, saving users time and\nsaving them from potential calculation and implementation errors."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.04787v1", 
    "title": "States and channels in quantum mechanics without complex numbers", 
    "arxiv-id": "1603.04787v1", 
    "author": "J. A. Miszczak", 
    "publish": "2016-03-15T18:00:52Z", 
    "summary": "In the presented note we aim at exploring the possibility of abandoning\ncomplex numbers in the representation of quantum states and operations. We\ndemonstrate a simplified version of quantum mechanics in which the states are\nrepresented using real numbers only. The main advantage of this approach is\nthat the simulation of the $n$-dimensional quantum system requires $n^2$ real\nnumbers, in contrast to the standard case where $n^4$ real numbers are\nrequired. The main disadvantage is the lack of hermicity in the representation\nof quantum states. Using Mathematica computer algebra system we develop a set\nof functions for manipulating real-only quantum states. With the help of this\ntool we study the properties of the introduced representation and the induced\nrepresentation of quantum channels."
},{
    "category": "math.OC", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.05835v2", 
    "title": "A Flexible Primal-Dual Toolbox", 
    "arxiv-id": "1603.05835v2", 
    "author": "Hendrik Dirks", 
    "publish": "2016-03-18T11:01:23Z", 
    "summary": "\\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex\nvariational problems in image processing and beyond. Such problems often\nconsist of non-differentiable parts and involve linear operators. The toolbox\nuses a primal-dual scheme to avoid (computationally) inefficient operator\ninversion and to get reliable error estimates. From the user-side,\n\\textbf{FlexBox} expects the primal formulation of the problem, automatically\ndecouples operators and dualizes the problem. For large-scale problems,\n\\textbf{FlexBox} also comes with a \\cpp-module, which can be used stand-alone\nor together with MATLAB via MEX-interfaces. Besides various pre-implemented\ndata-fidelities and regularization-terms, \\textbf{FlexBox} is able to handle\narbitrary operators while being easily extendable, due to its object-oriented\ndesign. The toolbox is available at\n\\href{http://www.flexbox.im}{http://www.flexbox.im}"
},{
    "category": "cs.FL", 
    "doi": "10.1002/fld.3963", 
    "link": "http://arxiv.org/pdf/1603.06017v1", 
    "title": "Automatic Theorem Proving in Walnut", 
    "arxiv-id": "1603.06017v1", 
    "author": "Hamoon Mousavi", 
    "publish": "2016-03-18T23:53:10Z", 
    "summary": "Walnut is a software package that implements a mechanical decision procedure\nfor deciding certain combinatorial properties of some special words referred to\nas automatic words or automatic sequences. Walnut is written in Java and is\nopen source. It is licensed under GNU General Public License."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPCSim.2016.7568318", 
    "link": "http://arxiv.org/pdf/1603.07008v1", 
    "title": "A mixed precision semi-Lagrangian algorithm and its performance on   accelerators", 
    "arxiv-id": "1603.07008v1", 
    "author": "Lukas Einkemmer", 
    "publish": "2016-03-22T22:07:18Z", 
    "summary": "In this paper we propose a mixed precision algorithm in the context of the\nsemi-Lagrangian discontinuous Galerkin method. The performance of this approach\nis evaluated on a traditional dual socket workstation as well as on a Xeon Phi\nand an NVIDIA K80. We find that the mixed precision algorithm can be\nimplemented efficiently on these architectures. This implies that, in addition\nto the considerable reduction in memory, a substantial increase in performance\ncan be observed as well. Moreover, we discuss the relative performance of our\nimplementations."
},{
    "category": "cs.NE", 
    "doi": "10.1109/HPCSim.2016.7568318", 
    "link": "http://arxiv.org/pdf/1604.01416v1", 
    "title": "dMath: A Scalable Linear Algebra and Math Library for Heterogeneous   GP-GPU Architectures", 
    "arxiv-id": "1604.01416v1", 
    "author": "Anthony Skjellum", 
    "publish": "2016-04-05T20:28:26Z", 
    "summary": "A new scalable parallel math library, dMath, is presented in this paper that\ndemonstrates leading scaling when using intranode, or internode,\nhybrid-parallelism for deep-learning. dMath provides easy-to-use distributed\nbase primitives and a variety of domain-specific algorithms. These include\nmatrix multiplication, convolutions, and others allowing for rapid development\nof highly scalable applications, including Deep Neural Networks (DNN), whereas\npreviously one was restricted to libraries that provided effective primitives\nfor only a single GPU, like Nvidia cublas and cudnn or DNN primitives from\nNervana neon framework. Development of HPC software is difficult,\nlabor-intensive work, requiring a unique skill set. dMath allows a wide range\nof developers to utilize parallel and distributed hardware easily. One\ncontribution of this approach is that data is stored persistently on the GPU\nhardware, avoiding costly transfers between host and device. Advanced memory\nmanagement techniques are utilized, including caching of transferred data and\nmemory reuse through pooling. A key contribution of dMath is that it delivers\nperformance, portability, and productivity to its specific domain of support.\nIt enables algorithm and application programmers to quickly solve problems\nwithout managing the significant complexity associated with multi-level\nparallelism."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPCSim.2016.7568318", 
    "link": "http://arxiv.org/pdf/1604.08079v2", 
    "title": "UBL: an R package for Utility-based Learning", 
    "arxiv-id": "1604.08079v2", 
    "author": "Luis Torgo", 
    "publish": "2016-04-27T14:13:11Z", 
    "summary": "This document describes the R package UBL that allows the use of several\nmethods for handling utility-based learning problems. Classification and\nregression problems that assume non-uniform costs and/or benefits pose serious\nchallenges to predictive analytic tasks. In the context of meteorology,\nfinance, medicine, ecology, among many other, specific domain information\nconcerning the preference bias of the users must be taken into account to\nenhance the models predictive performance. To deal with this problem, a large\nnumber of techniques was proposed by the research community for both\nclassification and regression tasks. The main goal of UBL package is to\nfacilitate the utility-based predictive analytic task by providing a set of\nmethods to deal with this type of problems in the R environment. It is a\nversatile tool that provides mechanisms to handle both regression and\nclassification (binary and multiclass) tasks. Moreover, UBL package allows the\nuser to specify his domain preferences, but it also provides some automatic\nmethods that try to infer those preference bias from the domain, considering\nsome common known settings."
},{
    "category": "cs.MS", 
    "doi": "10.1145/2930889.2930937", 
    "link": "http://arxiv.org/pdf/1605.00410v1", 
    "title": "Computing Real Roots of Real Polynomials ... and now For Real!", 
    "arxiv-id": "1605.00410v1", 
    "author": "Michael Sagraloff", 
    "publish": "2016-05-02T09:47:10Z", 
    "summary": "Very recent work introduces an asymptotically fast subdivision algorithm,\ndenoted ANewDsc, for isolating the real roots of a univariate real polynomial.\nThe method combines Descartes' Rule of Signs to test intervals for the\nexistence of roots, Newton iteration to speed up convergence against clusters\nof roots, and approximate computation to decrease the required precision. It\nachieves record bounds on the worst-case complexity for the considered problem,\nmatching the complexity of Pan's method for computing all complex roots and\nimproving upon the complexity of other subdivision methods by several\nmagnitudes.\n  In the article at hand, we report on an implementation of ANewDsc on top of\nthe RS root isolator. RS is a highly efficient realization of the classical\nDescartes method and currently serves as the default real root solver in Maple.\nWe describe crucial design changes within ANewDsc and RS that led to a\nhigh-performance implementation without harming the theoretical complexity of\nthe underlying algorithm.\n  With an excerpt of our extensive collection of benchmarks, available online\nat http://anewdsc.mpi-inf.mpg.de/, we illustrate that the theoretical gain in\nperformance of ANewDsc over other subdivision methods also transfers into\npractice. These experiments also show that our new implementation outperforms\nboth RS and mature competitors by magnitudes for notoriously hard instances\nwith clustered roots. For all other instances, we avoid almost any overhead by\nintegrating additional optimizations and heuristics."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jcp.2016.09.037", 
    "link": "http://arxiv.org/pdf/1605.00492v2", 
    "title": "High level implementation of geometric multigrid solvers for finite   element problems: applications in atmospheric modelling", 
    "arxiv-id": "1605.00492v2", 
    "author": "Eike Hermann M\u00fcller", 
    "publish": "2016-05-02T14:06:24Z", 
    "summary": "The implementation of efficient multigrid preconditioners for elliptic\npartial differential equations (PDEs) is a challenge due to the complexity of\nthe resulting algorithms and corresponding computer code. For sophisticated\nfinite element discretisations on unstructured grids an efficient\nimplementation can be very time consuming and requires the programmer to have\nin-depth knowledge of the mathematical theory, parallel computing and\noptimisation techniques on manycore CPUs. In this paper we show how the\ndevelopment of bespoke multigrid preconditioners can be simplified\nsignificantly by using a framework which allows the expression of the each\ncomponent of the algorithm at the correct abstraction level. Our approach (1)\nallows the expression of the finite element problem in a language which is\nclose to the mathematical formulation of the problem, (2) guarantees the\nautomatic generation and efficient execution of parallel optimised low-level\ncomputer code and (3) is flexible enough to support different abstraction\nlevels and give the programmer control over details of the preconditioner. We\nuse the composable abstractions of the Firedrake/PyOP2 package to demonstrate\nthe efficiency of this approach for the solution of strongly anisotropic PDEs\nin atmospheric modelling. The weak formulation of the PDE is expressed in\nUnified Form Language (UFL) and the lower PyOP2 abstraction layer allows the\nmanual design of computational kernels for a bespoke geometric multigrid\npreconditioner. We compare the performance of this preconditioner to a\nsingle-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code\nis inherently parallel and we present a detailed performance analysis for a\nsingle node (24 cores) on the ARCHER supercomputer. Our implementation utilises\na significant fraction of the available memory bandwidth and shows very good\nweak scaling on up to 6,144 compute cores."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jcp.2016.09.037", 
    "link": "http://arxiv.org/pdf/1605.02688v1", 
    "title": "Theano: A Python framework for fast computation of mathematical   expressions", 
    "arxiv-id": "1605.02688v1", 
    "author": "Ying Zhang", 
    "publish": "2016-05-09T18:32:34Z", 
    "summary": "Theano is a Python library that allows to define, optimize, and evaluate\nmathematical expressions involving multi-dimensional arrays efficiently. Since\nits introduction, it has been one of the most used CPU and GPU mathematical\ncompilers - especially in the machine learning community - and has shown steady\nperformance improvements. Theano is being actively and continuously developed\nsince 2008, multiple frameworks have been built on top of it and it has been\nused to produce many state-of-the-art machine learning models.\n  The present article is structured as follows. Section I provides an overview\nof the Theano software and its community. Section II presents the principal\nfeatures of Theano and how to use them, and compares them with other similar\nprojects. Section III focuses on recently-introduced functionalities and\nimprovements. Section IV compares the performance of Theano against Torch7 and\nTensorFlow on several machine learning models. Section V discusses current\nlimitations of Theano and potential ways of improving it."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.jcp.2016.09.037", 
    "link": "http://arxiv.org/pdf/1606.00094v2", 
    "title": "Boda-RTC: Productive Generation of Portable, Efficient Code for   Convolutional Neural Networks on Mobile Computing Platforms", 
    "arxiv-id": "1606.00094v2", 
    "author": "Kurt Keutzer", 
    "publish": "2016-06-01T02:17:26Z", 
    "summary": "The popularity of neural networks (NNs) spans academia, industry, and popular\nculture. In particular, convolutional neural networks (CNNs) have been applied\nto many image based machine learning tasks and have yielded strong results. The\navailability of hardware/software systems for efficient training and deployment\nof large and/or deep CNN models has been, and continues to be, an important\nconsideration for the field. Early systems for NN computation focused on\nleveraging existing dense linear algebra techniques and libraries. Current\napproaches use low-level machine specific programming and/or closed-source,\npurpose-built vendor libraries. In this work, we present an open source system\nthat, compared to existing approaches, achieves competitive computational speed\nwhile achieving higher portability. We achieve this by targeting the\nvendor-neutral OpenCL platform using a code-generation approach. We argue that\nour approach allows for both: (1) the rapid development of new computational\nkernels for existing hardware targets, and (2) the rapid tuning of existing\ncomputational kernels for new hardware targets. Results are presented for a\ncase study of targeting the Qualcomm Snapdragon 820 mobile computing platform\nfor CNN deployment."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jcp.2016.09.037", 
    "link": "http://arxiv.org/pdf/1606.05563v1", 
    "title": "Computing all Space Curve Solutions of Polynomial Systems by Polyhedral   Methods", 
    "arxiv-id": "1606.05563v1", 
    "author": "Jan Verschelde", 
    "publish": "2016-06-17T15:28:40Z", 
    "summary": "A polyhedral method to solve a system of polynomial equations exploits its\nsparse structure via the Newton polytopes of the polynomials. We propose a\nhybrid symbolic-numeric method to compute a Puiseux series expansion for every\nspace curve that is a solution of a polynomial system. The focus of this paper\nconcerns the difficult case when the leading powers of the Puiseux series of\nthe space curve are contained in the relative interior of a higher dimensional\ncone of the tropical prevariety. We show that this difficult case does not\noccur for polynomials with generic coefficients. To resolve this case, we\npropose to apply polyhedral end games to recover tropisms hidden in the\ntropical prevariety."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761646", 
    "link": "http://arxiv.org/pdf/1606.05790v2", 
    "title": "Mathematical Foundations of the GraphBLAS", 
    "arxiv-id": "1606.05790v2", 
    "author": "Timothy Mattson", 
    "publish": "2016-06-18T18:46:20Z", 
    "summary": "The GraphBLAS standard (GraphBlas.org) is being developed to bring the\npotential of matrix based graph algorithms to the broadest possible audience.\nMathematically the Graph- BLAS defines a core set of matrix-based graph\noperations that can be used to implement a wide class of graph algorithms in a\nwide range of programming environments. This paper provides an introduction to\nthe mathematics of the GraphBLAS. Graphs represent connections between vertices\nwith edges. Matrices can represent a wide range of graphs using adjacency\nmatrices or incidence matrices. Adjacency matrices are often easier to analyze\nwhile incidence matrices are often better for representing data. Fortunately,\nthe two are easily connected by matrix mul- tiplication. A key feature of\nmatrix mathematics is that a very small number of matrix operations can be used\nto manipulate a very wide range of graphs. This composability of small number\nof operations is the foundation of the GraphBLAS. A standard such as the\nGraphBLAS can only be effective if it has low performance overhead. Performance\nmeasurements of prototype GraphBLAS implementations indicate that the overhead\nis low."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-319-39639-2_15", 
    "link": "http://arxiv.org/pdf/1606.06604v1", 
    "title": "Stochastic Runge-Kutta Software Package for Stochastic Differential   Equations", 
    "arxiv-id": "1606.06604v1", 
    "author": "L. A. Sevastyanov", 
    "publish": "2016-06-21T14:51:11Z", 
    "summary": "As a result of the application of a technique of multistep processes\nstochastic models construction the range of models, implemented as a\nself-consistent differential equations, was obtained. These are partial\ndifferential equations (master equation, the Fokker--Planck equation) and\nstochastic differential equations (Langevin equation). However, analytical\nmethods do not always allow to research these equations adequately. It is\nproposed to use the combined analytical and numerical approach studying these\nequations. For this purpose the numerical part is realized within the framework\nof symbolic computation. It is recommended to apply stochastic Runge--Kutta\nmethods for numerical study of stochastic differential equations in the form of\nthe Langevin. Under this approach, a program complex on the basis of analytical\ncalculations metasystem Sage is developed. For model verification logarithmic\nwalks and Black--Scholes two-dimensional model are used. To illustrate the\nstochastic \"predator--prey\" type model is used. The utility of the combined\nnumerical-analytical approach is demonstrated."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-39639-2_15", 
    "link": "http://arxiv.org/pdf/1606.06977v2", 
    "title": "Computing hypergeometric functions rigorously", 
    "arxiv-id": "1606.06977v2", 
    "author": "Fredrik Johansson", 
    "publish": "2016-06-22T15:07:11Z", 
    "summary": "We present an efficient implementation of hypergeometric functions in\narbitrary-precision interval arithmetic. The functions ${}_0F_1$, ${}_1F_1$,\n${}_2F_1$ and ${}_2F_0$ (or the Kummer $U$-function) are supported for\nunrestricted complex parameters and argument, and by extension, we cover\nexponential and trigonometric integrals, error functions, Fresnel integrals,\nincomplete gamma and beta functions, Bessel functions, Airy functions, Legendre\nfunctions, Jacobi polynomials, complete elliptic integrals, and other special\nfunctions. The output can be used directly for interval computations or to\ngenerate provably correct floating-point approximations in any format.\nPerformance is competitive with earlier arbitrary-precision software, and\nsometimes orders of magnitude faster. We also partially cover the generalized\nhypergeometric function ${}_pF_q$ and computation of high-order parameter\nderivatives."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1606.07085v2", 
    "title": "From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph   Algorithms inside a BigTable Database", 
    "arxiv-id": "1606.07085v2", 
    "author": "Bill Howe", 
    "publish": "2016-06-22T20:08:47Z", 
    "summary": "Google BigTable's scale-out design for distributed key-value storage inspired\na generation of NoSQL databases. Recently the NewSQL paradigm emerged in\nresponse to analytic workloads that demand distributed computation local to\ndata storage. Many such analytics take the form of graph algorithms, a trend\nthat motivated the GraphBLAS initiative to standardize a set of matrix math\nkernels for building graph algorithms. In this article we show how it is\npossible to implement the GraphBLAS kernels in a BigTable database by\npresenting the design of Graphulo, a library for executing graph algorithms\ninside the Apache Accumulo database. We detail the Graphulo implementation of\ntwo graph algorithms and conduct experiments comparing their performance to two\nmain-memory matrix math systems. Our results shed insight into the conditions\nthat determine when executing a graph algorithm is faster inside a database\nversus an external system---in short, that memory requirements and relative I/O\nare critical factors."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1607.00291v3", 
    "title": "High-Performance Tensor Contraction without Transposition", 
    "arxiv-id": "1607.00291v3", 
    "author": "Devin A. Matthews", 
    "publish": "2016-07-01T15:37:59Z", 
    "summary": "Tensor computations--in particular tensor contraction (TC)--are important\nkernels in many scientific computing applications. Due to the fundamental\nsimilarity of TC to matrix multiplication (MM) and to the availability of\noptimized implementations such as the BLAS, tensor operations have\ntraditionally been implemented in terms of BLAS operations, incurring both a\nperformance and a storage overhead. Instead, we implement TC using the flexible\nBLIS framework, which allows for transposition (reshaping) of the tensor to be\nfused with internal partitioning and packing operations, requiring no explicit\ntransposition operations or additional workspace. This implementation, TBLIS,\nachieves performance approaching that of MM, and in some cases considerably\nhigher than that of traditional TC. Our implementation supports multithreading\nusing an approach identical to that used for MM in BLIS, with similar\nperformance characteristics. The complexity of managing tensor-to-matrix\ntransformations is also handled automatically in our approach, greatly\nsimplifying its use in scientific applications."
},{
    "category": "math.NA", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1607.00346v4", 
    "title": "Distributed-memory Hierarchical Interpolative Factorization", 
    "arxiv-id": "1607.00346v4", 
    "author": "Lexing Ying", 
    "publish": "2016-07-01T18:37:34Z", 
    "summary": "The hierarchical interpolative factorization (HIF) offers an efficient way\nfor solving or preconditioning elliptic partial differential equations. By\nexploiting locality and low-rank properties of the operators, the HIF achieves\nquasi-linear complexity for factorizing the discrete positive definite elliptic\noperator and linear complexity for solving the associated linear system. In\nthis paper, the distributed-memory HIF (DHIF) is introduced as a parallel and\ndistributed-memory implementation of the HIF. The DHIF organizes the processes\nin a hierarchical structure and keep the communication as local as possible.\nThe computation complexity is $O\\left(\\frac{N\\log N}{P}\\right)$ and\n$O\\left(\\frac{N}{P}\\right)$ for constructing and applying the DHIF,\nrespectively, where $N$ is the size of the problem and $P$ is the number of\nprocesses. The communication complexity is $O\\left(\\sqrt{P}\\log^3\nP\\right)\\alpha + O\\left(\\frac{N^{2/3}}{\\sqrt{P}}\\right)\\beta$ where $\\alpha$ is\nthe latency and $\\beta$ is the inverse bandwidth. Extensive numerical examples\nare performed on the NERSC Edison system with up to 8192 processes. The\nnumerical results agree with the complexity analysis and demonstrate the\nefficiency and scalability of the DHIF."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1607.01404v2", 
    "title": "PRIMME_SVDS: A High-Performance Preconditioned SVD Solver for Accurate   Large-Scale Computations", 
    "arxiv-id": "1607.01404v2", 
    "author": "Andreas Stathopoulos", 
    "publish": "2016-07-05T20:15:56Z", 
    "summary": "The increasing number of applications requiring the solution of large scale\nsingular value problems have rekindled interest in iterative methods for the\nSVD. Some promising recent ad- vances in large scale iterative methods are\nstill plagued by slow convergence and accuracy limitations for computing\nsmallest singular triplets. Furthermore, their current implementations in\nMATLAB cannot address the required large problems. Recently, we presented a\npreconditioned, two-stage method to effectively and accurately compute a small\nnumber of extreme singular triplets. In this research, we present a\nhigh-performance software, PRIMME SVDS, that implements our hybrid method based\non the state-of-the-art eigensolver package PRIMME for both largest and\nsmallest singular values. PRIMME SVDS fills a gap in production level software\nfor computing the partial SVD, especially with preconditioning. The numerical\nexperiments demonstrate its superior performance compared to other\nstate-of-the-art software and its good parallel performance under strong and\nweak scaling."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1607.01477v2", 
    "title": "Accelerating eigenvector and pseudospectra computation using blocked   multi-shift triangular solves", 
    "arxiv-id": "1607.01477v2", 
    "author": "Jack Poulson", 
    "publish": "2016-07-06T04:19:04Z", 
    "summary": "Multi-shift triangular solves are basic linear algebra calculations with\napplications in eigenvector and pseudospectra computation. We propose blocked\nalgorithms that efficiently exploit Level 3 BLAS to perform multi-shift\ntriangular solves and safe multi-shift triangular solves. Numerical experiments\nindicate that computing triangular eigenvectors with a safe multi-shift\ntriangular solve achieves speedups by a factor of 60 relative to LAPACK. This\nalgorithm accelerates the calculation of general eigenvectors threefold. When\nusing multi-shift triangular solves to compute pseudospectra, we report\nninefold speedups relative to EigTool."
},{
    "category": "cs.CE", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1607.02904v1", 
    "title": "The Vectorization of the Tersoff Multi-Body Potential: An Exercise in   Performance Portability", 
    "arxiv-id": "1607.02904v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-07-11T11:23:04Z", 
    "summary": "Molecular dynamics simulations, an indispensable research tool in\ncomputational chemistry and materials science, consume a significant portion of\nthe supercomputing cycles around the world. We focus on multi-body potentials\nand aim at achieving performance portability. Compared with well-studied pair\npotentials, multibody potentials deliver increased simulation accuracy but are\ntoo complex for effective compiler optimization. Because of this, achieving\ncross-platform performance remains an open question. By abstracting from target\narchitecture and computing precision, we develop a vectorization scheme\napplicable to both CPUs and accelerators. We present results for the Tersoff\npotential within the molecular dynamics code LAMMPS on several architectures,\ndemonstrating efficiency gains not only for computational kernels, but also for\nlarge-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver\nis between 3 and 5 times faster than the pure MPI reference."
},{
    "category": "stat.CO", 
    "doi": "10.1109/HPEC.2016.7761577", 
    "link": "http://arxiv.org/pdf/1608.02148v2", 
    "title": "Randomized Matrix Decompositions using R", 
    "arxiv-id": "1608.02148v2", 
    "author": "J. Nathan Kutz", 
    "publish": "2016-08-06T19:47:48Z", 
    "summary": "The singular value decomposition (SVD) is among the most ubiquitous matrix\nfactorizations. Specifically, it is a cornerstone algorithm for data analysis,\ndimensionality reduction and data compression. However, despite modern computer\npower, massive datasets pose a computational challenge for traditional SVD\nalgorithms. We present the R package rsvd, which enables the fast computation\nof the SVD and related methods, facilitated by randomized algorithms. The rsvd\npackage provides routines for computing the randomized singular value\ndecomposition, randomized principal component analysis and randomized robust\nprincipal component analysis. Randomized algorithms provide an efficient\ncomputational framework for reducing the computational demands of traditional\n(deterministic) matrix factorizations. The key idea is to compute a compressed\nrepresentation of the data using random sampling. This smaller (compressed)\nmatrix captures the essential information that can then be used to obtain a\nlow-rank matrix approximation. Several numerical examples demonstrate the usage\nof the rsvd package. The results show substantial accelerated computational\ntimes using the powerful concept of randomization."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761626", 
    "link": "http://arxiv.org/pdf/1608.04041v1", 
    "title": "Julia Implementation of the Dynamic Distributed Dimensional Data Model", 
    "arxiv-id": "1608.04041v1", 
    "author": "Dylan Hutchison", 
    "publish": "2016-08-14T00:58:41Z", 
    "summary": "Julia is a new language for writing data analysis programs that are easy to\nimplement and run at high performance. Similarly, the Dynamic Distributed\nDimensional Data Model (D4M) aims to clarify data analysis operations while\nretaining strong performance. D4M accomplishes these goals through a\ncomposable, unified data model on associative arrays. In this work, we present\nan implementation of D4M in Julia and describe how it enables and facilitates\ndata analysis. Several experiments showcase scalable performance in our new\nJulia version as compared to the original Matlab implementation."
},{
    "category": "cs.DC", 
    "doi": "10.1109/HPEC.2016.7761626", 
    "link": "http://arxiv.org/pdf/1608.07573v2", 
    "title": "Containers for portable, productive and performant scientific computing", 
    "arxiv-id": "1608.07573v2", 
    "author": "Garth N. Wells", 
    "publish": "2016-08-26T11:58:00Z", 
    "summary": "Containers are an emerging technology that hold promise for improving\nproductivity and code portability in scientific computing. We examine Linux\ncontainer technology for the distribution of a non-trivial scientific computing\nsoftware stack and its execution on a spectrum of platforms from laptop\ncomputers through to high performance computing (HPC) systems. We show on a\nworkstation and a leadership-class HPC system that when deployed appropriately\nthere are no performance penalties running scientific programs inside\ncontainers. For Python code run on large parallel computers, the run time is\nreduced inside a container due to faster library imports. The software\ndistribution approach and data that we present will help developers and users\ndecide on whether container technology is appropriate for them. We also provide\nguidance for the vendors of HPC systems that rely on proprietary libraries for\nperformance on what they can do to make containers work seamlessly and without\nperformance penalty."
},{
    "category": "cs.NA", 
    "doi": "10.1109/HPEC.2016.7761626", 
    "link": "http://arxiv.org/pdf/1609.00829v1", 
    "title": "Efficient computation of Laguerre polynomials", 
    "arxiv-id": "1609.00829v1", 
    "author": "N. M. Temme", 
    "publish": "2016-09-03T13:59:43Z", 
    "summary": "An efficient algorithm and a Fortran 90 module (LaguerrePol) for computing\nLaguerre polynomials $L^{(\\alpha)}_n(z)$ are presented. The standard three-term\nrecurrence relation satisfied by the polynomials and different types of\nasymptotic expansions valid for $n$ large and $\\alpha$ small, are used\ndepending on the parameter region.\n  Based on tests of contiguous relations in the parameter $\\alpha$ and the\ndegree $n$ satisfied by the polynomials, we claim that a relative accuracy\nclose or better than $10^{-12}$ can be obtained using the module LaguerrePol\nfor computing the functions $L^{(\\alpha)}_n(z)$ in the parameter range $z \\ge\n0$, $-1 < \\alpha \\le 5$, $n \\ge 0$."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2016.7761626", 
    "link": "http://arxiv.org/pdf/1609.01088v1", 
    "title": "GTApprox: surrogate modeling for industrial design", 
    "arxiv-id": "1609.01088v1", 
    "author": "Dmitry Yarotsky", 
    "publish": "2016-09-05T10:41:14Z", 
    "summary": "We describe GTApprox - a new tool for medium-scale surrogate modeling in\nindustrial design. Compared to existing software, GTApprox brings several\ninnovations: a few novel approximation algorithms, several advanced methods of\nautomated model selection, novel options in the form of hints. We demonstrate\nthe efficiency of GTApprox on a large collection of test problems. In addition,\nwe describe several applications of GTApprox to real engineering problems."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.jocs.2016.11.001", 
    "link": "http://arxiv.org/pdf/1609.01277v2", 
    "title": "OpenSBLI: A framework for the automated derivation and parallel   execution of finite difference solvers on a range of computer architectures", 
    "arxiv-id": "1609.01277v2", 
    "author": "Neil D. Sandham", 
    "publish": "2016-09-05T10:11:31Z", 
    "summary": "Exascale computing will feature novel and potentially disruptive hardware\narchitectures. Exploiting these to their full potential is non-trivial.\nNumerical modelling frameworks involving finite difference methods are\ncurrently limited by the 'static' nature of the hand-coded discretisation\nschemes and repeatedly may have to be re-written to run efficiently on new\nhardware. In contrast, OpenSBLI uses code generation to derive the model's code\nfrom a high-level specification. Users focus on the equations to solve, whilst\nnot concerning themselves with the detailed implementation. Source-to-source\ntranslation is used to tailor the code and enable its execution on a variety of\nhardware."
},{
    "category": "physics.ins-det", 
    "doi": "10.1186/s40679-017-0039-0", 
    "link": "http://arxiv.org/pdf/1609.02831v1", 
    "title": "Nanosurveyor: a framework for real-time data processing", 
    "arxiv-id": "1609.02831v1", 
    "author": "Stefano Marchesini", 
    "publish": "2016-09-09T15:21:19Z", 
    "summary": "Scientists are drawn to synchrotrons and accelerator based light sources\nbecause of their brightness, coherence and flux. The rate of improvement in\nbrightness and detector technology has outpaced Moore's law growth seen for\ncomputers, networks, and storage, and is enabling novel observations and\ndiscoveries with faster frame rates, larger fields of view, higher resolution,\nand higher dimensionality. Here we present an integrated software/algorithmic\nframework designed to capitalize on high throughput experiments, and describe\nthe streamlined processing pipeline of ptychography data analysis. The pipeline\nprovides throughput, compression, and resolution as well as rapid feedback to\nthe microscope operators."
},{
    "category": "cs.DL", 
    "doi": "10.1186/s40679-017-0039-0", 
    "link": "http://arxiv.org/pdf/1609.03457v1", 
    "title": "Math-Aware Search Engines: Physics Applications and Overview", 
    "arxiv-id": "1609.03457v1", 
    "author": "Deanna C. Pineau", 
    "publish": "2016-09-08T06:07:23Z", 
    "summary": "Search engines for equations now exist, which return results matching the\nquery's mathematical meaning or structural presentation. Operating over\nscientific papers, online encyclopedias, and math discussion forums, their\ncontent includes physics, math, and other sciences. They enable physicists to\navoid jargon and more easily target mathematical content within and across\ndisciplines. As a natural extension of keyword-based search, they open up a new\nworld for discovering both exact and approximate mathematical solutions;\nphysical systems' analogues and alternative models; and physics' patterns.\n  This review presents the existing math-aware search engines, discusses\nmethods for maximizing their search success, and overviews their math-matching\ncapabilities. Proposed applications to physics are also given, to contribute\ntowards developers' and physicists' exploration of the newly available search\nhorizons."
},{
    "category": "cs.DC", 
    "doi": "10.1186/s40679-017-0039-0", 
    "link": "http://arxiv.org/pdf/1609.07008v1", 
    "title": "Betweenness Centrality is more Parallelizable than Dense Matrix   Multiplication", 
    "arxiv-id": "1609.07008v1", 
    "author": "Torsten Hoefler", 
    "publish": "2016-09-22T15:01:30Z", 
    "summary": "General infrastructure and scalable algorithms for sparse matrix\nmultiplication enable succinct high-performance implementation of numerical\nmethods and graph algorithms. We showcase the theoretical and practical quality\nof novel sparse matrix multiplication routines in Cyclops Tensor Framework\n(CTF) via MFBC: a Maximal Frontier Betweenness Centrality algorithm. Our sparse\nmatrix multiplication algorithms and consequently MFBC perform asymptotically\nless communication than previous approaches. For graphs with $n$ vertices and\naverage degree $k$, we show that on $p$ processors, MFBC performs a factor of\n$p^{1/3}$ less communication than known alternatives when $k=n/p^{2/3}$. If $p$\nprocessors are needed to fit the problem in memory, all costs associated with\nthe algorithm can be reduced by a factor of $s=(n/k)\\sqrt{p}$ when using $sp$\nprocessors. For multiplication of square dense matrices only $s=\\sqrt{p}$ is\nachievable.\n  We formulate and implement MFBC for weighted graphs, by leveraging\nspecially-designed monoids and functions. We prove the correctness of the new\nformulation. CTF allows a parallelism-oblivious C++ implementation of MFBC to\nachieve good scalability for both extremely sparse and relatively dense graphs.\nThe library automatically searches a space of distributed data decompositions\nand sparse matrix multiplication algorithms. The resulting code outperforms the\nwell-known CombBLAS library by factors of up to 8 and shows more robust\nperformance. Our design methodology is general and readily extensible to other\ngraph problems."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPEC.2016.7761640", 
    "link": "http://arxiv.org/pdf/1609.08642v1", 
    "title": "Benchmarking the Graphulo Processing Framework", 
    "arxiv-id": "1609.08642v1", 
    "author": "Jeremy Kepner", 
    "publish": "2016-09-27T20:09:03Z", 
    "summary": "Graph algorithms have wide applicablity to a variety of domains and are often\nused on massive datasets. Recent standardization efforts such as the GraphBLAS\nspecify a set of key computational kernels that hardware and software\ndevelopers can adhere to. Graphulo is a processing framework that enables\nGraphBLAS kernels in the Apache Accumulo database. In our previous work, we\nhave demonstrated a core Graphulo operation called \\textit{TableMult} that\nperforms large-scale multiplication operations of database tables. In this\narticle, we present the results of scaling the Graphulo engine to larger\nproblems and scalablity when a greater number of resources is used.\nSpecifically, we present two experiments that demonstrate Graphulo scaling\nperformance is linear with the number of available resources. The first\nexperiment demonstrates cluster processing rates through Graphulo's TableMult\noperator on two large graphs, scaled between $2^{17}$ and $2^{19}$ vertices.\nThe second experiment uses TableMult to extract a random set of rows from a\nlarge graph ($2^{19}$ nodes) to simulate a cued graph analytic. These\nbenchmarking results are of relevance to Graphulo users who wish to apply\nGraphulo to their graph problems."
},{
    "category": "math.NA", 
    "doi": "10.1109/HPEC.2016.7761640", 
    "link": "http://arxiv.org/pdf/1609.09841v1", 
    "title": "GPU Acceleration of Hermite Methods for the Simulation of Wave   Propagation", 
    "arxiv-id": "1609.09841v1", 
    "author": "Timothy Warburton", 
    "publish": "2016-09-30T18:02:54Z", 
    "summary": "The Hermite methods of Goodrich, Hagstrom, and Lorenz (2006) use Hermite\ninterpolation to construct high order numerical methods for hyperbolic initial\nvalue problems. The structure of the method has several favorable features for\nparallel computing. In this work, we propose algorithms that take advantage of\nthe many-core architecture of Graphics Processing Units. The algorithm exploits\nthe compact stencil of Hermite methods and uses data structures that allow for\nefficient data load and stores. Additionally the highly localized evolution\noperator of Hermite methods allows us to combine multi-stage time-stepping\nmethods within the new algorithms incurring minimal accesses of global memory.\nUsing a scalar linear wave equation, we study the algorithm by considering\nHermite interpolation and evolution as individual kernels and alternatively\ncombined them into a monolithic kernel. For both approaches we demonstrate\nstrategies to increase performance. Our numerical experiments show that\nalthough a two kernel approach allows for better performance on the hardware, a\nmonolithic kernel can offer a comparable time to solution with less global\nmemory usage."
},{
    "category": "stat.ME", 
    "doi": "10.1109/HPEC.2016.7761640", 
    "link": "http://arxiv.org/pdf/1610.00345v1", 
    "title": "Density Estimation with Distribution Element Trees", 
    "arxiv-id": "1610.00345v1", 
    "author": "Daniel W. Meyer", 
    "publish": "2016-10-02T20:07:28Z", 
    "summary": "The estimation of probability densities based on available data is a central\ntask in many statistical applications. Especially in the case of large\nensembles with many samples or high-dimensional sample spaces, computationally\nefficient methods are needed. We propose a new method that is based on a\ndecomposition of the distribution to be estimated in terms of so-called\ndistribution elements (DEs). These elements enable an adaptive and hierarchical\ndiscretization of the sample space with small or large elements in regions with\nhigh and variable or low densities, respectively. The refinement strategy that\nwe propose is based on statistical goodness-of-fit and independence tests that\nevaluate the local approximation of the distribution in terms of DEs. The\ncapabilities of our new method are inspected based on several low and\nhigh-dimensional examples."
},{
    "category": "math.AG", 
    "doi": "10.1109/HPEC.2016.7761640", 
    "link": "http://arxiv.org/pdf/1610.03034v1", 
    "title": "Numerical Implicitization for Macaulay2", 
    "arxiv-id": "1610.03034v1", 
    "author": "Joe Kileel", 
    "publish": "2016-10-10T19:14:59Z", 
    "summary": "We present the Macaulay2 package NumericalImplicitization, which allows for\nuser-friendly computation of the basic invariants of the image of a polynomial\nmap, such as dimension, degree, and Hilbert function values. This package\nrelies on methods of numerical algebraic geometry, such as homotopy\ncontinuation and monodromy."
},{
    "category": "cs.DS", 
    "doi": "10.1109/HPEC.2016.7761640", 
    "link": "http://arxiv.org/pdf/1610.05141v1", 
    "title": "Efficient Random Sampling - Parallel, Vectorized, Cache-Efficient, and   Online", 
    "arxiv-id": "1610.05141v1", 
    "author": "Carsten Dachsbacher", 
    "publish": "2016-10-17T14:38:02Z", 
    "summary": "We consider the problem of sampling $n$ numbers from the range\n$\\{1,\\ldots,N\\}$ without replacement on modern architectures. The main result\nis a simple divide-and-conquer scheme that makes sequential algorithms more\ncache efficient and leads to a parallel algorithm running in expected time\n$\\mathcal{O}\\left(n/p+\\log p\\right)$ on $p$ processors. The amount of\ncommunication between the processors is very small and independent of the\nsample size. We also discuss modifications needed for load balancing, reservoir\nsampling, online sampling, sampling with replacement, Bernoulli sampling, and\nvectorization on SIMD units or GPUs."
},{
    "category": "cond-mat.quant-gas", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1610.05329v1", 
    "title": "OpenMP, OpenMP/MPI, and CUDA/MPI C programs for solving the   time-dependent dipolar Gross-Pitaevskii equation", 
    "arxiv-id": "1610.05329v1", 
    "author": "Antun Balaz", 
    "publish": "2016-10-17T20:07:09Z", 
    "summary": "We present new versions of the previously published C and CUDA programs for\nsolving the dipolar Gross-Pitaevskii equation in one, two, and three spatial\ndimensions, which calculate stationary and non-stationary solutions by\npropagation in imaginary or real time. Presented programs are improved and\nparallelized versions of previous programs, divided into three packages\naccording to the type of parallelization. First package contains improved and\nthreaded version of sequential C programs using OpenMP. Second package\nadditionally parallelizes three-dimensional variants of the OpenMP programs\nusing MPI, allowing them to be run on distributed-memory systems. Finally,\nprevious three-dimensional CUDA-parallelized programs are further parallelized\nusing MPI, similarly as the OpenMP programs. We also present speedup test\nresults obtained using new versions of programs in comparison with the previous\nsequential C and parallel CUDA programs. The improvements to the sequential\nversion yield a speedup of 1.1 to 1.9, depending on the program. OpenMP\nparallelization yields further speedup of 2 to 12 on a 16-core workstation,\nwhile OpenMP/MPI version demonstrates a speedup of 11.5 to 16.5 on a computer\ncluster with 32 nodes used. CUDA/MPI version shows a speedup of 9 to 10 on a\ncomputer cluster with 32 nodes."
},{
    "category": "stat.CO", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1610.07310v1", 
    "title": "Large Scale Parallel Computations in R through Elemental", 
    "arxiv-id": "1610.07310v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2016-10-24T07:30:27Z", 
    "summary": "Even though in recent years the scale of statistical analysis problems has\nincreased tremendously, many statistical software tools are still limited to\nsingle-node computations. However, statistical analyses are largely based on\ndense linear algebra operations, which have been deeply studied, optimized and\nparallelized in the high-performance-computing community. To make\nhigh-performance distributed computations available for statistical analysis,\nand thus enable large scale statistical computations, we introduce RElem, an\nopen source package that integrates the distributed dense linear algebra\nlibrary Elemental into R. While on the one hand, RElem provides direct wrappers\nof Elemental's routines, on the other hand, it overloads various operators and\nfunctions to provide an entirely native R experience for distributed\ncomputations. We showcase how simple it is to port existing R programs to Relem\nand demonstrate that Relem indeed allows to scale beyond the single-node\nlimitation of R with the full performance of Elemental without any overhead."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1610.08128v1", 
    "title": "The Reverse Cuthill-McKee Algorithm in Distributed-Memory", 
    "arxiv-id": "1610.08128v1", 
    "author": "Esmond G. Ng", 
    "publish": "2016-10-26T00:26:44Z", 
    "summary": "Ordering vertices of a graph is key to minimize fill-in and data structure\nsize in sparse direct solvers, maximize locality in iterative solvers, and\nimprove performance in graph algorithms. Except for naturally parallelizable\nordering methods such as nested dissection, many important ordering methods\nhave not been efficiently mapped to distributed-memory architectures. In this\npaper, we present the first-ever distributed-memory implementation of the\nreverse Cuthill-McKee (RCM) algorithm for reducing the profile of a sparse\nmatrix. Our parallelization uses a two-dimensional sparse matrix decomposition.\nWe achieve high performance by decomposing the problem into a small number of\nprimitives and utilizing optimized implementations of these primitives. Our\nimplementation shows strong scaling up to 1024 cores for smaller matrices and\nup to 4096 cores for larger matrices."
},{
    "category": "cs.SE", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1610.08713v1", 
    "title": "The Probabilistic Model Checker Storm (Extended Abstract)", 
    "arxiv-id": "1610.08713v1", 
    "author": "Matthias Volk", 
    "publish": "2016-10-27T11:14:53Z", 
    "summary": "We present a new probabilistic model checker Storm. Using state-of-the-art\nlibraries, we aim for both high performance and versatility. This extended\nabstract gives a brief overview of the features of Storm."
},{
    "category": "math.NA", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1610.09874v1", 
    "title": "Anisotropic mesh adaptation in Firedrake with PETSc DMPlex", 
    "arxiv-id": "1610.09874v1", 
    "author": "Gerard J. Gorman", 
    "publish": "2016-10-31T11:34:29Z", 
    "summary": "Despite decades of research in this area, mesh adaptation capabilities are\nstill rarely found in numerical simulation software. We postulate that the\nprimary reason for this is lack of usability. Integrating mesh adaptation into\nexisting software is difficult as non-trivial operators, such as error metrics\nand interpolation operators, are required, and integrating available adaptive\nremeshers is not straightforward. Our approach presented here is to first\nintegrate Pragmatic, an anisotropic mesh adaptation library, into DMPlex, a\nPETSc object that manages unstructured meshes and their interactions with\nPETSc's solvers and I/O routines. As PETSc is already widely used, this will\nmake anisotropic mesh adaptation available to a much larger community. As a\ndemonstration of this we describe the integration of anisotropic mesh\nadaptation into Firedrake, an automated Finite Element based system for the\nportable solution of partial differential equations which already uses PETSc\nsolvers and I/O via DMPlex. We present a proof of concept of this integration\nwith a three-dimensional advection test case."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.cpc.2016.07.029", 
    "link": "http://arxiv.org/pdf/1611.00675v1", 
    "title": "emgr - The Empirical Gramian Framework", 
    "arxiv-id": "1611.00675v1", 
    "author": "Christian Himpe", 
    "publish": "2016-11-02T16:39:06Z", 
    "summary": "Gramian matrices are a well-known encoding for properties of input-output\nsystems such as controllability, observability or minimality. These so called\nsystem Gramian matrices were developed in linear system theory for applications\nsuch as model order reduction of control systems. Empirical Gramian matrices\nare an extension to the system Gramians for parametric and nonlinear systems as\nwell as a data-driven method of computation. The empirical Gramian framework\n\\emgr implements the empirical Gramians in a uniform and configurable manner,\nwith applications such as Gramian-based (nonlinear) model reduction,\ndecentralized control, sensitivity analysis, parameter identification and\ncombined state and parameter reduction."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.02274v1", 
    "title": "GPU-Based Parallel Integration of Large Numbers of Independent ODE   Systems", 
    "arxiv-id": "1611.02274v1", 
    "author": "Chih-Jen Sung", 
    "publish": "2016-11-06T18:11:16Z", 
    "summary": "The task of integrating a large number of independent ODE systems arises in\nvarious scientific and engineering areas. For nonstiff systems, common explicit\nintegration algorithms can be used on GPUs, where individual GPU threads\nconcurrently integrate independent ODEs with different initial conditions or\nparameters. One example is the fifth-order adaptive Runge-Kutta-Cash-Karp\n(RKCK) algorithm. In the case of stiff ODEs, standard explicit algorithms\nrequire impractically small time-step sizes for stability reasons, and implicit\nalgorithms are therefore commonly used instead to allow larger time steps and\nreduce the computational expense. However, typical high-order implicit\nalgorithms based on backwards differentiation formulae (e.g., VODE, LSODE)\ninvolve complex logical flow that causes severe thread divergence when\nimplemented on GPUs, limiting the performance. Therefore, alternate algorithms\nare needed. A GPU-based Runge-Kutta-Chebyshev (RKC) algorithm can handle\nmoderate levels of stiffness and performs significantly faster than not only an\nequivalent CPU version but also a CPU-based implicit algorithm (VODE) based on\nresults shown in the literature. In this chapter, we present the mathematical\nbackground, implementation details, and source code for the RKCK and RKC\nalgorithms for use integrating large numbers of independent systems of ODEs on\nGPUs. In addition, brief performance comparisons are shown for each algorithm,\ndemonstrating the potential benefit of moving to GPU-based ODE integrators."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.02831v1", 
    "title": "Arb: Efficient Arbitrary-Precision Midpoint-Radius Interval Arithmetic", 
    "arxiv-id": "1611.02831v1", 
    "author": "Fredrik Johansson", 
    "publish": "2016-11-09T06:23:37Z", 
    "summary": "Arb is a C library for arbitrary-precision interval arithmetic using the\nmidpoint-radius representation, also known as ball arithmetic. It supports real\nand complex numbers, polynomials, power series, matrices, and evaluation of\nmany special functions. The core number types are designed for versatility and\nspeed in a range of scenarios, allowing performance that is competitive with\nnon-interval arbitrary-precision types such as MPFR and MPC floating-point\nnumbers. We discuss the low-level number representation, strategies for\nprecision and error bounds, and the implementation of efficient polynomial\narithmetic with interval coefficients."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.03410v1", 
    "title": "Binomial Checkpointing for Arbitrary Programs with No User Annotation", 
    "arxiv-id": "1611.03410v1", 
    "author": "Barak A. Pearlmutter", 
    "publish": "2016-11-10T17:29:24Z", 
    "summary": "Heretofore, automatic checkpointing at procedure-call boundaries, to reduce\nthe space complexity of reverse mode, has been provided by systems like\nTapenade. However, binomial checkpointing, or treeverse, has only been provided\nin Automatic Differentiation (AD) systems in special cases, e.g., through\nuser-provided pragmas on DO loops in Tapenade, or as the nested taping\nmechanism in adol-c for time integration processes, which requires that user\ncode be refactored. We present a framework for applying binomial checkpointing\nto arbitrary code with no special annotation or refactoring required. This is\naccomplished by applying binomial checkpointing directly to a program trace.\nThis trace is produced by a general-purpose checkpointing mechanism that is\northogonal to AD."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.06365v1", 
    "title": "A Case for Malleable Thread-Level Linear Algebra Libraries: The LU   Factorization with Partial Pivoting", 
    "arxiv-id": "1611.06365v1", 
    "author": "Robert van de Geijn", 
    "publish": "2016-11-19T13:55:29Z", 
    "summary": "We propose two novel techniques for overcoming load-imbalance encountered\nwhen implementing so-called look-ahead mechanisms in relevant dense matrix\nfactorizations for the solution of linear systems. Both techniques target the\nscenario where two thread teams are created/activated during the factorization,\nwith each team in charge of performing an independent task/branch of execution.\nThe first technique promotes worker sharing (WS) between the two tasks,\nallowing the threads of the task that completes first to be reallocated for use\nby the costlier task. The second technique allows a fast task to alert the\nslower task of completion, enforcing the early termination (ET) of the second\ntask, and a smooth transition of the factorization procedure into the next\niteration.\n  The two mechanisms are instantiated via a new malleable thread-level\nimplementation of the Basic Linear Algebra Subprograms (BLAS), and their\nbenefits are illustrated via an implementation of the LU factorization with\npartial pivoting enhanced with look-ahead. Concretely, our experimental results\non a six core Intel-Xeon processor show the benefits of combining WS+ET,\nreporting competitive performance in comparison with a task-parallel\nruntime-based solution."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.06892v1", 
    "title": "Bidiagonalization with Parallel Tiled Algorithms", 
    "arxiv-id": "1611.06892v1", 
    "author": "Jack Dongarra", 
    "publish": "2016-11-18T15:11:48Z", 
    "summary": "We consider algorithms for going from a \"full\" matrix to a condensed \"band\nbidiagonal\" form using orthogonal transformations. We use the framework of\n\"algorithms by tiles\". Within this framework, we study: (i) the tiled\nbidiagonalization algorithm BiDiag, which is a tiled version of the standard\nscalar bidiagonalization algorithm; and (ii) the R-bidiagonalization algorithm\nR-BiDiag, which is a tiled version of the algorithm which consists in first\nperforming the QR factorization of the initial matrix, then performing the\nband-bidiagonalization of the R-factor. For both bidiagonalization algorithms\nBiDiag and R-BiDiag, we use four main types of reduction trees, namely FlatTS,\nFlatTT, Greedy, and a newly introduced auto-adaptive tree, Auto. We provide a\nstudy of critical path lengths for these tiled algorithms, which shows that (i)\nR-BiDiag has a shorter critical path length than BiDiag for tall and skinny\nmatrices, and (ii) Greedy based schemes are much better than earlier proposed\nvariants with unbounded resources. We provide experiments on a single multicore\nnode, and on a few multicore nodes of a parallel distributed shared-memory\nsystem, to show the superiority of the new algorithms on a variety of matrix\nsizes, matrix shapes and core counts."
},{
    "category": "cs.NE", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.06945v1", 
    "title": "A Metaprogramming and Autotuning Framework for Deploying Deep Learning   Applications", 
    "arxiv-id": "1611.06945v1", 
    "author": "Kurt Keutzer", 
    "publish": "2016-11-21T18:49:23Z", 
    "summary": "In recent years, deep neural networks (DNNs), have yielded strong results on\na wide range of applications. Graphics Processing Units (GPUs) have been one\nkey enabling factor leading to the current popularity of DNNs. However, despite\nincreasing hardware flexibility and software programming toolchain maturity,\nhigh efficiency GPU programming remains difficult: it suffers from high\ncomplexity, low productivity, and low portability. GPU vendors such as NVIDIA\nhave spent enormous effort to write special-purpose DNN libraries. However, on\nother hardware targets, especially mobile GPUs, such vendor libraries are not\ngenerally available. Thus, the development of portable, open, high-performance,\nenergy-efficient GPU code for DNN operations would enable broader deployment of\nDNN-based algorithms. Toward this end, this work presents a framework to enable\nproductive, high-efficiency GPU programming for DNN computations across\nhardware platforms and programming models. In particular, the framework\nprovides specific support for metaprogramming, autotuning, and DNN-tailored\ndata types. Using our framework, we explore implementing DNN operations on\nthree different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA\nGPUs, we show both portability between OpenCL and CUDA as well competitive\nperformance compared to the vendor library. On Qualcomm GPUs, we show that our\nframework enables productive development of target-specific optimizations, and\nachieves reasonable absolute performance. Finally, On AMD GPUs, we show initial\nresults that indicate our framework can yield reasonable performance on a new\nplatform with minimal effort."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.07819v1", 
    "title": "dMath: Distributed Linear Algebra for DL", 
    "arxiv-id": "1611.07819v1", 
    "author": "Trevor Gale", 
    "publish": "2016-11-19T00:24:12Z", 
    "summary": "The paper presents a parallel math library, dMath, that demonstrates leading\nscaling when using intranode, internode, and hybrid-parallelism for deep\nlearning (DL). dMath provides easy-to-use distributed primitives and a variety\nof domain-specific algorithms including matrix multiplication, convolutions,\nand others allowing for rapid development of scalable applications like deep\nneural networks (DNNs). Persistent data stored in GPU memory and advanced\nmemory management techniques avoid costly transfers between host and device.\ndMath delivers performance, portability, and productivity to its specific\ndomain of support."
},{
    "category": "math.OC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1611.08832v1", 
    "title": "Verifying Integer Programming Results", 
    "arxiv-id": "1611.08832v1", 
    "author": "Daniel E. Steffy", 
    "publish": "2016-11-27T12:27:03Z", 
    "summary": "Software for mixed-integer linear programming can return incorrect results\nfor a number of reasons, one being the use of inexact floating-point\narithmetic. Even solvers that employ exact arithmetic may suffer from\nprogramming or algorithmic errors, motivating the desire for a way to produce\nindependently verifiable certificates of claimed results. Due to the complex\nnature of state-of-the-art MILP solution algorithms, the ideal form of such a\ncertificate is not entirely clear. This paper proposes such a certificate\nformat, illustrating its capabilities and structure through examples. The\ncertificate format is designed with simplicity in mind and is composed of a\nlist of statements that can be sequentially verified using a limited number of\nsimple yet powerful inference rules. We present a supplementary verification\ntool for compressing and checking these certificates independently of how they\nwere created. We report computational results on a selection of mixed-integer\nlinear programming instances from the literature. To this end, we have extended\nthe exact rational version of the MIP solver SCIP to produce such certificates."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1612.02495v2", 
    "title": "An initial investigation of the performance of GPU-based swept   time-space decomposition", 
    "arxiv-id": "1612.02495v2", 
    "author": "Kyle E Niemeyer", 
    "publish": "2016-12-08T00:19:54Z", 
    "summary": "Simulations of physical phenomena are essential to the expedient design of\nprecision components in aerospace and other high-tech industries. These\nphenomena are often described by mathematical models involving partial\ndifferential equations (PDEs) without exact solutions. Modern design problems\nrequire simulations with a level of resolution that is difficult to achieve in\na reasonable amount of time even in effectively parallelized solvers. Though\nthe scale of the problem relative to available computing power is the greatest\nimpediment to accelerating these applications, significant performance gains\ncan be achieved through careful attention to the details of memory accesses.\nParallelized PDE solvers are subject to a trade-off in memory management: store\nthe solution for each timestep in abundant, global memory with high access\ncosts or in a limited, private memory with low access costs that must be passed\nbetween nodes. The GPU implementation of swept time-space decomposition\npresented here mitigates this dilemma by using private (shared) memory,\navoiding internode communication, and overwriting unnecessary values. It shows\nsignificant improvement in the execution time of the PDE solvers in one\ndimension achieving speedups of 6-2x for large and small problem sizes\nrespectively compared to naive GPU versions and 7-300x compared to parallel CPU\nversions."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1612.03772v1", 
    "title": "SimTensor: A synthetic tensor data generator", 
    "arxiv-id": "1612.03772v1", 
    "author": "Joao Gama", 
    "publish": "2016-12-09T19:13:03Z", 
    "summary": "SimTensor is a multi-platform, open-source software for generating artificial\ntensor data (either with CP/PARAFAC or Tucker structure) for reproducible\nresearch on tensor factorization algorithms. SimTensor is a stand-alone\napplication based on MATALB. It provides a wide range of facilities for\ngenerating tensor data with various configurations. It comes with a\nuser-friendly graphical user interface, which enables the user to generate\ntensors with complicated settings in an easy way. It also has this facility to\nexport generated data to universal formats such as CSV and HDF5, which can be\nimported via a wide range of programming languages (C, C++, Java, R, Fortran,\nMATLAB, Perl, Python, and many more). The most innovative part of SimTensor is\nthis that can generate temporal tensors with periodic waves, seasonal effects\nand streaming structure. it can apply constraints such as non-negativity and\ndifferent kinds of sparsity to the data. SimTensor also provides this facility\nto simulate different kinds of change-points and inject various types of\nanomalies. The source code and binary versions of SimTensor is available for\ndownload in http://www.simtensor.org."
},{
    "category": "math.NA", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1612.05313v1", 
    "title": "The Method of Gauss-Newton to Compute Power Series Solutions of   Polynomial Homotopies", 
    "arxiv-id": "1612.05313v1", 
    "author": "Jan Verschelde", 
    "publish": "2016-12-15T23:52:29Z", 
    "summary": "We consider the extension of the method of Gauss-Newton from complex\nfloating-point arithmetic to the field of truncated power series with complex\nfloating-point coefficients. With linearization we formulate a linear system\nwhere the coefficient matrix is a series with matrix coefficients. The\nstructure of the linear system leads in the regular case to a block triangular\nsystem. When one or several of the matrix coefficients in the series are\nsingular, the linear system has a block banded structure. We show that in the\nregular case, the solution of the linear system satisfies the conditions of the\nHermite interpolation problem. In general, we solve a Hermite-Laurent\ninterpolation problem, via a lower triangular echelon form on the coefficient\nmatrix. Hermite interpolation also applies to Newton's method where we compute\nthe coefficients of the power series via its Maclaurin expansion. With a couple\nof illustrative examples, we demonstrate the application to polynomial homotopy\ncontinuation."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1612.07526v1", 
    "title": "An efficient hybrid tridiagonal divide-and-conquer algorithm on   distributed memory architectures", 
    "arxiv-id": "1612.07526v1", 
    "author": "Xuebin Chi", 
    "publish": "2016-12-22T10:19:09Z", 
    "summary": "In this paper, an efficient divide-and-conquer (DC) algorithm is proposed for\nthe symmetric tridiagonal matrices based on ScaLAPACK and the hierarchically\nsemiseparable (HSS) matrices. HSS is an important type of rank-structured\nmatrices.Most time of the DC algorithm is cost by computing the eigenvectors\nvia the matrix-matrix multiplications (MMM). In our parallel hybrid DC (PHDC)\nalgorithm, MMM is accelerated by using the HSS matrix techniques when the\nintermediate matrix is large. All the HSS algorithms are done via the package\nSTRUMPACK. PHDC has been tested by using many different matrices. Compared with\nthe DC implementation in MKL, PHDC can be faster for some matrices with few\ndeflations when using hundreds of processes. However, the gains decrease as the\nnumber of processes increases. The comparisons of PHDC with ELPA (the\nEigenvalue soLvers for Petascale Applications library) are similar. PHDC is\nusually slower than MKL and ELPA when using 300 or more processes on Tianhe-2\nsupercomputer."
},{
    "category": "stat.ML", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1701.03980v1", 
    "title": "DyNet: The Dynamic Neural Network Toolkit", 
    "arxiv-id": "1701.03980v1", 
    "author": "Pengcheng Yin", 
    "publish": "2017-01-15T01:53:23Z", 
    "summary": "We describe DyNet, a toolkit for implementing neural network models based on\ndynamic declaration of network structure. In the static declaration strategy\nthat is used in toolkits like Theano, CNTK, and TensorFlow, the user first\ndefines a computation graph (a symbolic representation of the computation), and\nthen examples are fed into an engine that executes this computation and\ncomputes its derivatives. In DyNet's dynamic declaration strategy, computation\ngraph construction is mostly transparent, being implicitly constructed by\nexecuting procedural code that computes the network outputs, and the user is\nfree to use different network structures for each input. Dynamic declaration\nthus facilitates the implementation of more complicated network architectures,\nand DyNet is specifically designed to allow users to implement their models in\na way that is idiomatic in their preferred programming language (C++ or\nPython). One challenge with dynamic declaration is that because the symbolic\ncomputation graph is defined anew for every training example, its construction\nmust have low overhead. To achieve this, DyNet has an optimized C++ backend and\nlightweight graph representation. Experiments show that DyNet's speeds are\nfaster than or comparable with static declaration toolkits, and significantly\nfaster than Chainer, another dynamic declaration toolkit. DyNet is released\nopen-source under the Apache 2.0 license and available at\nhttp://github.com/clab/dynet."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1701.05431v1", 
    "title": "A task-driven implementation of a simple numerical solver for hyperbolic   conservation laws", 
    "arxiv-id": "1701.05431v1", 
    "author": "Vincent Perrier", 
    "publish": "2017-01-19T14:29:17Z", 
    "summary": "This article describes the implementation of an all-in-one numerical\nprocedure within the runtime StarPU. In order to limit the complexity of the\nmethod, for the sake of clarity of the presentation of the non-classical\ntask-driven programming environnement, we have limited the numerics to first\norder in space and time. Results show that the task distribution is efficient\nif the tasks are numerous and individually large enough so that the task heap\ncan be saturated by tasks which computational time covers the task management\noverhead. Next, we also see that even though they are mostly faster on graphic\ncards, not all the tasks are suitable for GPUs, which brings forward the\nimportance of the task scheduler. Finally, we look at a more realistic system\nof conservation laws with an expensive source term, what allows us to conclude\nand open on future works involving higher local arithmetic intensity, by\nincreasing the order of the numerical method or by enriching the model\n(increased number of parameters and therefore equations)."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1702.06898v1", 
    "title": "Enhancing speed and scalability of the ParFlow simulation code", 
    "arxiv-id": "1702.06898v1", 
    "author": "Stefan Kollet", 
    "publish": "2017-02-22T17:12:49Z", 
    "summary": "Regional hydrology studies are often supported by high resolution simulations\nof subsurface flow that require expensive and extensive computations. Efficient\nusage of the latest high performance parallel computing systems becomes a\nnecessity. The simulation software ParFlow has been demonstrated to meet this\nrequirement and shown to have excellent solver scalability for up to 16,384\nprocesses. In the present work we show that the code requires further\nenhancements in order to fully take advantage of current petascale machines. We\nidentify ParFlow's way of parallelization of the computational mesh as a\ncentral bottleneck. We propose to reorganize this subsystem using fast mesh\npartition algorithms provided by the parallel adaptive mesh refinement library\np4est. We realize this in a minimally invasive manner by modifying selected\nparts of the code to reinterpret the existing mesh data structures. We evaluate\nthe scaling performance of the modified version of ParFlow, demonstrating good\nweak and strong scaling up to 458k cores of the Juqueen supercomputer, and test\nan example application at large scale."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1703.04036v1", 
    "title": "A Java library to perform S-expansions of Lie algebras", 
    "arxiv-id": "1703.04036v1", 
    "author": "F. Nadal", 
    "publish": "2017-03-11T22:46:19Z", 
    "summary": "The contraction method is a procedure that allows to establish non-trivial\nrelations between Lie algebras and have had succesful applications in both\nmathematics and theoretical physics. This work deals with generalizations of\nthe contraction procedure with a main focus in the so called S-expansion method\nas it includes most of the other generalized contractions. Basically, the\nS-exansion combines a Lie algebra $\\mathcal{G}$ with a finite abelian semigroup\n$S$ in order to define new S-expanded algebras. After giving a description of\nthe main ingredients used in this paper, we present a Java library that\nautomatizes the S-expansion procedure. With this computational tool we are able\nto represent Lie algebras and semigroups, so we can perform S-expansions of Lie\nalgebras using arbitrary semigroups. We explain how the library methods has\nbeen constructed and how they work; then we give a set of example programs\naimed to solve different problems. They are presented so that any user can\neasily modify them to perform his own calculations, without being necessarily\nan expert in Java. Finally, some comments about further developements and\npossible new applications are made."
},{
    "category": "cs.LG", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1703.05298v2", 
    "title": "Neural Networks for Beginners. A fast implementation in Matlab, Torch,   TensorFlow", 
    "arxiv-id": "1703.05298v2", 
    "author": "Andrea Zugarini", 
    "publish": "2017-03-10T18:01:20Z", 
    "summary": "This report provides an introduction to some Machine Learning tools within\nthe most common development environments. It mainly focuses on practical\nproblems, skipping any theoretical introduction. It is oriented to both\nstudents trying to approach Machine Learning and experts looking for new\nframeworks."
},{
    "category": "cs.CE", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/0704.0090v1", 
    "title": "Real Options for Project Schedules (ROPS)", 
    "arxiv-id": "0704.0090v1", 
    "author": "Lester Ingber", 
    "publish": "2007-04-01T14:35:40Z", 
    "summary": "Real Options for Project Schedules (ROPS) has three recursive\nsampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)\noptimization shell optimizes parameters of strategic Plans containing multiple\nProjects containing ordered Tasks. A middle shell samples probability\ndistributions of durations of Tasks. An inner shell samples probability\ndistributions of costs of Tasks. PATHTREE is used to develop options on\nschedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to\ndevelop a relative risk analysis among projects."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/0712.4126v1", 
    "title": "TRUST-TECH based Methods for Optimization and Learning", 
    "arxiv-id": "0712.4126v1", 
    "author": "Chandan K. Reddy", 
    "publish": "2007-12-25T03:14:32Z", 
    "summary": "Many problems that arise in machine learning domain deal with nonlinearity\nand quite often demand users to obtain global optimal solutions rather than\nlocal optimal ones. Optimization problems are inherent in machine learning\nalgorithms and hence many methods in machine learning were inherited from the\noptimization literature. Popularly known as the initialization problem, the\nideal set of parameters required will significantly depend on the given\ninitialization values. The recently developed TRUST-TECH (TRansformation Under\nSTability-reTaining Equilibria CHaracterization) methodology systematically\nexplores the subspace of the parameters to obtain a complete set of local\noptimal solutions. In this thesis work, we propose TRUST-TECH based methods for\nsolving several optimization and machine learning problems. Two stages namely,\nthe local stage and the neighborhood-search stage, are repeated alternatively\nin the solution space to achieve improvements in the quality of the solutions.\nOur methods were tested on both synthetic and real datasets and the advantages\nof using this novel framework are clearly manifested. This framework not only\nreduces the sensitivity to initialization, but also allows the flexibility for\nthe practitioners to use various global and local methods that work well for a\nparticular problem of interest. Other hierarchical stochastic algorithms like\nevolutionary algorithms and smoothing algorithms are also studied and\nframeworks for combining these methods with TRUST-TECH have been proposed and\nevaluated on several test systems."
},{
    "category": "cs.LO", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/0808.0540v1", 
    "title": "Executable Set Theory and Arithmetic Encodings in Prolog", 
    "arxiv-id": "0808.0540v1", 
    "author": "Paul Tarau", 
    "publish": "2008-08-05T04:59:56Z", 
    "summary": "The paper is organized as a self-contained literate Prolog program that\nimplements elements of an executable finite set theory with focus on\ncombinatorial generation and arithmetic encodings. The complete Prolog code is\navailable at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First,\nranking and unranking functions for some \"mathematically elegant\" data types in\nthe universe of Hereditarily Finite Sets with Urelements are provided,\nresulting in arithmetic encodings for powersets, hypergraphs, ordinals and\nchoice functions. After implementing a digraph representation of Hereditarily\nFinite Sets we define {\\em decoration functions} that can recover well-founded\nsets from encodings of their associated acyclic digraphs. We conclude with an\nencoding of arbitrary digraphs and discuss a concept of duality induced by the\nset membership relation. In the process, we uncover the surprising possibility\nof internally sharing isomorphic objects, independently of their language level\ntypes and meanings."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1005.1320v1", 
    "title": "The myth of equidistribution for high-dimensional simulation", 
    "arxiv-id": "1005.1320v1", 
    "author": "Richard P. Brent", 
    "publish": "2010-05-08T05:14:00Z", 
    "summary": "A pseudo-random number generator (RNG) might be used to generate w-bit random\nsamples in d dimensions if the number of state bits is at least dw. Some RNGs\nperform better than others and the concept of equidistribution has been\nintroduced in the literature in order to rank different RNGs. We define what it\nmeans for a RNG to be (d,w)-equidistributed, and then argue that\n(d,w)-equidistribution is not necessarily a desirable property."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1101.4369v2", 
    "title": "Univariate real root isolation in an extension field", 
    "arxiv-id": "1101.4369v2", 
    "author": "Elias Tsigaridas", 
    "publish": "2011-01-23T13:38:46Z", 
    "summary": "We present algorithmic, complexity and implementation results for the problem\nof isolating the real roots of a univariate polynomial in $B_{\\alpha} \\in\nL[y]$, where $L=\\QQ(\\alpha)$ is a simple algebraic extension of the rational\nnumbers. We consider two approaches for tackling the problem. In the first\napproach using resultant computations we perform a reduction to a polynomial\nwith integer coefficients. We compute separation bounds for the roots, and\nusing them we deduce that we can isolate the real roots of $B_{\\alpha}$ in\n$\\sOB(N^{10})$, where $N$ is an upper bound on all the quantities (degree and\nbitsize) of the input polynomials. In the second approach we isolate the real\nroots working directly on the polynomial of the input. We compute improved\nseparation bounds for real roots and we prove that they are optimal, under mild\nassumptions. For isolating the roots we consider a modified Sturm's algorithm,\nand a modified version of \\func{descartes}' algorithm introduced by Sagraloff.\nFor the former we prove a complexity bound of $\\sOB(N^8)$ and for the latter a\nbound of $\\sOB(N^{7})$. We implemented the algorithms in \\func{C} as part of\nthe core library of \\mathematica and we illustrate their efficiency over\nvarious data sets. Finally, we present complexity results for the general case\nof the first approach, where the coefficients belong to multiple extensions."
},{
    "category": "math.OC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1103.2695v1", 
    "title": "Software for Generation of Classes of Test Functions with Known Local   and Global Minima for Global Optimization", 
    "arxiv-id": "1103.2695v1", 
    "author": "Yaroslav D. Sergeyev", 
    "publish": "2011-03-14T15:48:52Z", 
    "summary": "A procedure for generating non-differentiable, continuously differentiable,\nand twice continuously differentiable classes of test functions for\nmultiextremal multidimensional box-constrained global optimization and a\ncorresponding package of C subroutines are presented. Each test class consists\nof 100 functions. Test functions are generated by defining a convex quadratic\nfunction systematically distorted by polynomials in order to introduce local\nminima. To determine a class, the user defines the following parameters: (i)\nproblem dimension, (ii) number of local minima, (iii) value of the global\nminimum, (iv) radius of the attraction region of the global minimizer, (v)\ndistance from the global minimizer to the vertex of the quadratic function.\nThen, all other necessary parameters are generated randomly for all 100\nfunctions of the class. Full information about each test function including\nlocations and values of all local minima is supplied to the user. Partial\nderivatives are also generated where possible."
},{
    "category": "cs.NA", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1103.3076v2", 
    "title": "PyDEC: Software and Algorithms for Discretization of Exterior Calculus", 
    "arxiv-id": "1103.3076v2", 
    "author": "Anil N. Hirani", 
    "publish": "2011-03-16T01:50:33Z", 
    "summary": "This paper describes the algorithms, features and implementation of PyDEC, a\nPython library for computations related to the discretization of exterior\ncalculus. PyDEC facilitates inquiry into both physical problems on manifolds as\nwell as purely topological problems on abstract complexes. We describe\nefficient algorithms for constructing the operators and objects that arise in\ndiscrete exterior calculus, lowest order finite element exterior calculus and\nin related topological problems. Our algorithms are formulated in terms of\nhigh-level matrix operations which extend to arbitrary dimension. As a result,\nour implementations map well to the facilities of numerical libraries such as\nNumPy and SciPy. The availability of such libraries makes Python suitable for\nprototyping numerical methods. We demonstrate how PyDEC is used to solve\nphysical and topological problems through several concise examples."
},{
    "category": "cs.SC", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1103.4697v1", 
    "title": "Arrangement Computation for Planar Algebraic Curves", 
    "arxiv-id": "1103.4697v1", 
    "author": "Michael Sagraloff", 
    "publish": "2011-03-24T08:26:01Z", 
    "summary": "We present a new certified and complete algorithm to compute arrangements of\nreal planar algebraic curves. Our algorithm provides a geometric-topological\nanalysis of the decomposition of the plane induced by a finite number of\nalgebraic curves in terms of a cylindrical algebraic decomposition of the\nplane. Compared to previous approaches, we improve in two main aspects:\nFirstly, we significantly reduce the amount of exact operations, that is, our\nalgorithms only uses resultant and gcd as purely symbolic operations. Secondly,\nwe introduce a new hybrid method in the lifting step of our algorithm which\ncombines the usage of a certified numerical complex root solver and information\nderived from the resultant computation. Additionally, we never consider any\ncoordinate transformation and the output is also given with respect to the\ninitial coordinate system. We implemented our algorithm as a prototypical\npackage of the C++-library CGAL. Our implementation exploits graphics hardware\nto expedite the resultant and gcd computation. We also compared our\nimplementation with the current reference implementation, that is, CGAL's curve\nanalysis and arrangement for algebraic curves. For various series of\nchallenging instances, our experiments show that the new implementation\noutperforms the existing one."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1201.1548v1", 
    "title": "Exact Symbolic-Numeric Computation of Planar Algebraic Curves", 
    "arxiv-id": "1201.1548v1", 
    "author": "Michael Sagraloff", 
    "publish": "2012-01-07T11:57:44Z", 
    "summary": "We present a novel certified and complete algorithm to compute arrangements\nof real planar algebraic curves. It provides a geometric-topological analysis\nof the decomposition of the plane induced by a finite number of algebraic\ncurves in terms of a cylindrical algebraic decomposition. From a high-level\nperspective, the overall method splits into two main subroutines, namely an\nalgorithm denoted Bisolve to isolate the real solutions of a zero-dimensional\nbivariate system, and an algorithm denoted GeoTop to analyze a single algebraic\ncurve.\n  Compared to existing approaches based on elimination techniques, we\nconsiderably improve the corresponding lifting steps in both subroutines. As a\nresult, generic position of the input system is never assumed, and thus our\nalgorithm never demands for any change of coordinates. In addition, we\nsignificantly limit the types of involved exact operations, that is, we only\nuse resultant and gcd computations as purely symbolic operations. The latter\nresults are achieved by combining techniques from different fields such as\n(modular) symbolic computation, numerical analysis and algebraic geometry.\n  We have implemented our algorithms as prototypical contributions to the\nC++-project CGAL. They exploit graphics hardware to expedite the symbolic\ncomputations. We have also compared our implementation with the current\nreference implementations, that is, LGP and Maple's Isolate for polynomial\nsystem solving, and CGAL's bivariate algebraic kernel for analyses and\narrangement computations of algebraic curves. For various series of challenging\ninstances, our exhaustive experiments show that the new implementations\noutperform the existing ones."
},{
    "category": "math.AG", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1202.4384v1", 
    "title": "Computable Hilbert Schemes", 
    "arxiv-id": "1202.4384v1", 
    "author": "Paolo Lella", 
    "publish": "2012-02-20T17:05:30Z", 
    "summary": "In this PhD thesis we propose an algorithmic approach to the study of the\nHilbert scheme. Developing algorithmic methods, we also obtain general results\nabout Hilbert schemes. In Chapter 1 we discuss the equations defining the\nHilbert scheme as subscheme of a suitable Grassmannian and in Chapter 5 we\ndetermine a new set of equations of degree lower than the degree of equations\nknown so far. In Chapter 2 we study the most important objects used to project\nalgorithmic techniques, namely Borel-fixed ideals. We determine an algorithm\ncomputing all the saturated Borel-fixed ideals with Hilbert polynomial assigned\nand we investigate their combinatorial properties. In Chapter 3 we show a new\ntype of flat deformations of Borel-fixed ideals which lead us to give a new\nproof of the connectedness of the Hilbert scheme. In Chapter 4 we construct\nfamilies of ideals that generalize the notion of family of ideals sharing the\nsame initial ideal with respect to a fixed term ordering. Some of these\nfamilies correspond to open subsets of the Hilbert scheme and can be used to a\nlocal study of the Hilbert scheme. In Chapter 6 we deal with the problem of the\nconnectedness of the Hilbert scheme of locally Cohen-Macaulay curves in the\nprojective 3-space. We show that one of the Hilbert scheme considered a \"good\"\ncandidate to be non-connected, is instead connected. Moreover there are three\nappendices that present and explain how to use the implementations of the\nalgorithms proposed."
},{
    "category": "cs.MS", 
    "doi": "10.1007/978-3-319-06548-9_8", 
    "link": "http://arxiv.org/pdf/1206.1187v2", 
    "title": "Parallel random variates generator for GPUs based on normal numbers", 
    "arxiv-id": "1206.1187v2", 
    "author": "Tim Wilkin", 
    "publish": "2012-06-06T11:30:42Z", 
    "summary": "Pseudorandom number generators are required for many computational tasks,\nsuch as stochastic modelling and simulation. This paper investigates the serial\nCPU and parallel GPU implementation of a Linear Congruential Generator based on\nthe binary representation of the normal number $\\alpha_{2,3}$. We adapted two\nmethods of modular reduction which allowed us to perform most operations in\n64-bit integer arithmetic, improving on the original implementation based on\n106-bit double-double operations. We found that our implementation is faster\nthan existing methods in literature, and our generation rate is close to the\nlimiting rate imposed by the efficiency of writing to a GPU's global memory."
},{
    "category": "astro-ph.EP", 
    "doi": "10.1016/j.newast.2013.01.002", 
    "link": "http://arxiv.org/pdf/1208.1157v2", 
    "title": "Swarm-NG: a CUDA Library for Parallel n-body Integrations with focus on   Simulations of Planetary Systems", 
    "arxiv-id": "1208.1157v2", 
    "author": "Jorg Peters", 
    "publish": "2012-08-06T13:11:28Z", 
    "summary": "We present Swarm-NG, a C++ library for the efficient direct integration of\nmany n-body systems using highly-parallel Graphics Processing Unit (GPU), such\nas NVIDIA's Tesla T10 and M2070 GPUs. While previous studies have demonstrated\nthe benefit of GPUs for n-body simulations with thousands to millions of\nbodies, Swarm-NG focuses on many few-body systems, e.g., thousands of systems\nwith 3...15 bodies each, as is typical for the study of planetary systems.\nSwarm-NG parallelizes the simulation, including both the numerical integration\nof the equations of motion and the evaluation of forces using NVIDIA's \"Compute\nUnified Device Architecture\" (CUDA) on the GPU. Swarm-NG includes optimized\nimplementations of 4th order time-symmetrized Hermite integration and mixed\nvariable symplectic integration, as well as several sample codes for other\nalgorithms to illustrate how non-CUDA-savvy users may themselves introduce\ncustomized integrators into the Swarm-NG framework. To optimize performance, we\nanalyze the effect of GPU-specific parameters on performance under double\nprecision.\n  Applications of Swarm-NG include studying the late stages of planet\nformation, testing the stability of planetary systems and evaluating the\ngoodness-of-fit between many planetary system models and observations of\nextrasolar planet host stars (e.g., radial velocity, astrometry, transit\ntiming). While Swarm-NG focuses on the parallel integration of many planetary\nsystems,the underlying integrators could be applied to a wide variety of\nproblems that require repeatedly integrating a set of ordinary differential\nequations many times using different initial conditions and/or parameter\nvalues."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.newast.2013.01.002", 
    "link": "http://arxiv.org/pdf/1210.7683v1", 
    "title": "Computing Petaflops over Terabytes of Data: The Case of Genome-Wide   Association Studies", 
    "arxiv-id": "1210.7683v1", 
    "author": "Paolo Bientinesi", 
    "publish": "2012-10-29T14:58:03Z", 
    "summary": "In many scientific and engineering applications, one has to solve not one but\na sequence of instances of the same problem. Often times, the problems in the\nsequence are linked in a way that allows intermediate results to be reused. A\ncharacteristic example for this class of applications is given by the\nGenome-Wide Association Studies (GWAS), a widely spread tool in computational\nbiology. GWAS entails the solution of up to trillions ($10^{12}$) of correlated\ngeneralized least-squares problems, posing a daunting challenge: the\nperformance of petaflops ($10^{15}$ floating-point operations) over terabytes\nof data.\n  In this paper, we design an algorithm for performing GWAS on multi-core\narchitectures. This is accomplished in three steps. First, we show how to\nexploit the relation among successive problems, thus reducing the overall\ncomputational complexity. Then, through an analysis of the required data\ntransfers, we identify how to eliminate any overhead due to input/output\noperations. Finally, we study how to decompose computation into tasks to be\ndistributed among the available cores, to attain high performance and\nscalability. With our algorithm, a GWAS that currently requires the use of a\nsupercomputer may now be performed in matter of hours on a single multi-core\nnode.\n  The discussion centers around the methodology to develop the algorithm rather\nthan the specific application. We believe the paper contributes valuable\nguidelines of general applicability for computational scientists on how to\ndevelop and optimize numerical algorithms."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.newast.2013.01.002", 
    "link": "http://arxiv.org/pdf/1308.1536v2", 
    "title": "A Parallel Algorithm for Calculation of Large Determinants with High   Accuracy for GPUs and MPI clusters", 
    "arxiv-id": "1308.1536v2", 
    "author": "Yuri Matiyasevich", 
    "publish": "2013-08-07T11:24:07Z", 
    "summary": "We present a parallel algorithm for calculating very large determinants with\narbitrary precision on computer clusters. This algorithm minimises data\nmovements between the nodes and computes not only the determinant but also all\nminors corresponding to a particular row or column at a little extra cost, and\nalso the determinants and minors of all submatrices in the top left corner at\nno extra cost. We implemented the algorithm in arbitrary precision arithmetic,\nsuitable for very ill conditioned matrices, and empirically estimated the loss\nof precision. The algorithm was applied to studies of Riemann's zeta function."
},{
    "category": "cs.AI", 
    "doi": "10.1016/j.newast.2013.01.002", 
    "link": "http://arxiv.org/pdf/1309.4962v1", 
    "title": "HOL(y)Hammer: Online ATP Service for HOL Light", 
    "arxiv-id": "1309.4962v1", 
    "author": "Josef Urban", 
    "publish": "2013-09-19T13:22:31Z", 
    "summary": "HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)\nmathematics encoded in the HOL Light system. The service allows its users to\nupload and automatically process an arbitrary formal development (project)\nbased on HOL Light, and to attack arbitrary conjectures that use the concepts\ndefined in some of the uploaded projects. For that, the service uses several\nautomated reasoning systems combined with several premise selection methods\ntrained on all the project proofs. The projects that are readily available on\nthe server for such query answering include the recent versions of the\nFlyspeck, Multivariate Analysis and Complex Analysis libraries. The service\nruns on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP\ncombinations and 4 decision procedures that contribute to its overall\nperformance. The system is also available for local installation by interested\nusers, who can customize it for their own proof development. An Emacs interface\nallowing parallel asynchronous queries to the service is also provided. The\noverall structure of the service is outlined, problems that arise and their\nsolutions are discussed, and an initial account of using the system is given."
},{
    "category": "cs.CE", 
    "doi": "10.1016/j.newast.2013.01.002", 
    "link": "http://arxiv.org/pdf/1311.4533v3", 
    "title": "A Study of Speed of the Boundary Element Method as applied to the   Realtime Computational Simulation of Biological Organs", 
    "arxiv-id": "1311.4533v3", 
    "author": "Kirana Kumara P", 
    "publish": "2013-11-18T20:54:26Z", 
    "summary": "In this work, possibility of simulating biological organs in realtime using\nthe Boundary Element Method (BEM) is investigated. Biological organs are\nassumed to follow linear elastostatic material behavior, and constant boundary\nelement is the element type used. First, a Graphics Processing Unit (GPU) is\nused to speed up the BEM computations to achieve the realtime performance.\nNext, instead of the GPU, a computer cluster is used. Results indicate that BEM\nis fast enough to provide for realtime graphics if biological organs are\nassumed to follow linear elastostatic material behavior. Although the present\nwork does not conduct any simulation using nonlinear material models, results\nfrom using the linear elastostatic material model imply that it would be\ndifficult to obtain realtime performance if highly nonlinear material models\nthat properly characterize biological organs are used. Although the use of BEM\nfor the simulation of biological organs is not new, the results presented in\nthe present study are not found elsewhere in the literature."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPEC.2014.7040945", 
    "link": "http://arxiv.org/pdf/1406.4923v1", 
    "title": "Achieving 100,000,000 database inserts per second using Accumulo and D4M", 
    "arxiv-id": "1406.4923v1", 
    "author": "Charles Yee", 
    "publish": "2014-06-19T00:44:12Z", 
    "summary": "The Apache Accumulo database is an open source relaxed consistency database\nthat is widely used for government applications. Accumulo is designed to\ndeliver high performance on unstructured data such as graphs of network data.\nThis paper tests the performance of Accumulo using data from the Graph500\nbenchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is\nused to implement the benchmark on a 216-node cluster running the MIT\nSuperCloud software stack. A peak performance of over 100,000,000 database\ninserts per second was achieved which is 100x larger than the highest\npreviously published value for any other database. The performance scales\nlinearly with the number of ingest clients, number of database servers, and\ndata size. The performance was achieved by adapting several supercomputing\ntechniques to this application: distributed arrays, domain decomposition,\nadaptive load balancing, and single-program-multiple-data programming."
},{
    "category": "cs.MS", 
    "doi": "10.1109/HPEC.2014.7040945", 
    "link": "http://arxiv.org/pdf/1406.5597v1", 
    "title": "Transpose-free Fast Fourier Transform for Turbulence Simulation", 
    "arxiv-id": "1406.5597v1", 
    "author": "M. Chaudhuri", 
    "publish": "2014-06-21T11:19:59Z", 
    "summary": "Pseudo-spectral method is one of the most accurate techniques for simulating\nturbulent flows. Fast Fourier transform (FFT) is an integral part of this\nmethod. In this paper, we present a new procedure to compute FFT in which we\nsave operations during interprocess communications by avoiding transpose of the\narray. As a result, our transpose-free FFT is 15\\% to 20\\% faster than FFTW."
},{
    "category": "math.DS", 
    "doi": "10.1016/j.amc.2015.05.120", 
    "link": "http://arxiv.org/pdf/1406.6900v3", 
    "title": "Efficient Gluing of Numerical Continuation and a Multiple Solution   Method for Elliptic PDEs", 
    "arxiv-id": "1406.6900v3", 
    "author": "Christian Kuehn", 
    "publish": "2014-06-26T14:37:40Z", 
    "summary": "Numerical continuation calculations for ordinary differential equations\n(ODEs) are, by now, an established tool for bifurcation analysis in dynamical\nsystems theory as well as across almost all natural and engineering sciences.\nAlthough several excellent standard software packages are available for ODEs,\nthere are - for good reasons - no standard numerical continuation toolboxes\navailable for partial differential equations (PDEs), which cover a broad range\nof different classes of PDEs automatically. A natural approach to this problem\nis to look for efficient gluing computation approaches, with independent\ncomponents developed by researchers in numerical analysis, dynamical systems,\nscientific computing and mathematical modelling. In this paper, we shall study\nseveral elliptic PDEs (Lane-Emden-Fowler, Lane-Emden-Fowler with microscopic\nforce, Caginalp) via the numerical continuation software pde2path and develop a\ngluing component to determine a set of starting solutions for the continuation\nby exploting the variational structures of the PDEs. In particular, we solve\nthe initialization problem of numerical continuation for PDEs via a minimax\nalgorithm to find multiple unstable solution. Furthermore, for the Caginalp\nsystem, we illustrate the efficient gluing link of pde2path to the underlying\nmesh generation and the FEM MatLab pdetoolbox. Even though the approach works\nefficiently due to the high-level programming language and without developing\nany new algorithms, we still obtain interesting bifurcation diagrams and\ndirectly applicable conclusions about the three elliptic PDEs we study, in\nparticular with respect to symmetry-breaking. In particular, we show for a\nmodified Lane-Emden-Fowler equation with an asymmetric microscopic force, how a\nfully connected bifurcation diagram splits up into C-shaped isolas on which\nlocalized pattern deformation appears towards two different regimes."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.amc.2015.05.120", 
    "link": "http://arxiv.org/pdf/1406.6924v1", 
    "title": "Strongly stable ideals and Hilbert polynomials", 
    "arxiv-id": "1406.6924v1", 
    "author": "Paolo Lella", 
    "publish": "2014-06-26T15:35:00Z", 
    "summary": "The \\textit{StronglyStableIdeals} package for \\textit{Macaulay2} provides a\nmethod to compute all the saturated strongly stable ideals in a given\npolynomial ring defining schemes with a fixed Hilbert polynomial. A description\nof the main method and the auxiliary tools is given."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.05.120", 
    "link": "http://arxiv.org/pdf/1408.1727v1", 
    "title": "Cluster-level tuning of a shallow water equation solver on the Intel MIC   architecture", 
    "arxiv-id": "1408.1727v1", 
    "author": "Cliff Addison", 
    "publish": "2014-08-07T22:53:51Z", 
    "summary": "The paper demonstrates the optimization of the execution environment of a\nhybrid OpenMP+MPI computational fluid dynamics code (shallow water equation\nsolver) on a cluster enabled with Intel Xeon Phi coprocessors. The discussion\nincludes: (1) Controlling the number and affinity of OpenMP threads to optimize\naccess to memory bandwidth; (2) Tuning the inter-operation of OpenMP and MPI to\npartition the problem for better data locality; (3) Ordering the MPI ranks in a\nway that directs some of the traffic into faster communication channels; (4)\nUsing efficient peer-to-peer communication between Xeon Phi coprocessors based\non the InfiniBand fabric.\n  With tuning, the application has 90% percent efficiency of parallel scaling\nup to 8 Intel Xeon Phi coprocessors in 2 compute nodes. For larger problems,\nscalability is even better, because of the greater computation to communication\nratio. However, problems of that size do not fit in the memory of one\ncoprocessor. The performance of the solver on one Intel Xeon Phi coprocessor\n7120P exceeds the performance on a dual-socket Intel Xeon E5-2697 v2 CPU by a\nfactor of 1.6x. In a 2-node cluster with 4 coprocessors per compute node, the\nMIC architecture yields 5.8x more performance than the CPUs. Only one line of\nlegacy Fortran code had to be changed in order to achieve the reported\nperformance on the MIC architecture (not counting changes to the command-line\ninterface). The methodology discussed in this paper is directly applicable to\nother bandwidth-bound stencil algorithms utilizing a hybrid OpenMP+MPI\napproach."
},{
    "category": "cs.DC", 
    "doi": "10.1016/j.amc.2015.05.120", 
    "link": "http://arxiv.org/pdf/1408.4587v1", 
    "title": "EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI   version of DPSNN-STDP (distributed plastic spiking neural net) and   improvements to other DAL codes", 
    "arxiv-id": "1408.4587v1", 
    "author": "Piero Vicini", 
    "publish": "2014-08-20T10:00:15Z", 
    "summary": "The EURETILE project required the selection and coding of a set of dedicated\nbenchmarks. The project is about the software and hardware architecture of\nfuture many-tile distributed fault-tolerant systems. We focus on dynamic\nworkloads characterised by heavy numerical processing requirements. The\nambition is to identify common techniques that could be applied to both the\nEmbedded Systems and HPC domains. This document is the first public deliverable\nof Work Package 7: Challenging Tiled Applications."
},{
    "category": "cs.MS", 
    "doi": "10.1016/j.amc.2015.05.120", 
    "link": "http://arxiv.org/pdf/1501.06625v3", 
    "title": "Accelerating Polynomial Homotopy Continuation on a Graphics Processing   Unit with Double Double and Quad Double Arithmetic", 
    "arxiv-id": "1501.06625v3", 
    "author": "Xiangcheng Yu", 
    "publish": "2015-01-26T23:54:20Z", 
    "summary": "Numerical continuation methods track a solution path defined by a homotopy.\nThe systems we consider are defined by polynomials in several variables with\ncomplex coefficients. For larger dimensions and degrees, the numerical\nconditioning worsens and hardware double precision becomes often insufficient\nto reach the end of the solution path. With double double and quad double\narithmetic, we can solve larger problems that we could not solve with hardware\ndouble arithmetic, but at a higher computational cost. This cost overhead can\nbe compensated by acceleration on a Graphics Processing Unit (GPU). We describe\nour implementation and report on computational results on benchmark polynomial\nsystems."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.amc.2010.01.019", 
    "link": "http://arxiv.org/pdf/1104.1698v1", 
    "title": "About the generalized LM-inverse and the weighted Moore-Penrose inverse", 
    "arxiv-id": "1104.1698v1", 
    "author": "Selver H. Pep\u00ed", 
    "publish": "2011-04-09T12:09:08Z", 
    "summary": "The recursive method for computing the generalized LM-inverse of a constant\nrectangular matrix augmented by a column vector is proposed in Udwadia and\nPhohomsiri (2007) [16] and [17]. The corresponding algorithm for the sequential\ndetermination of the generalized LM-inverse is established in the present\npaper. We prove that the introduced algorithm for computing the generalized\nLM-inverse and the algorithm for the computation of the weighted Moore-Penrose\ninverse developed by Wang and Chen (1986) in [23] are equivalent algorithms.\nBoth of the algorithms are implemented in the present paper using the package\nMATHEMATICA. Several rational test matrices and randomly generated constant\nmatrices are tested and the CPU time is compared and discussed."
},{
    "category": "q-bio.NC", 
    "doi": "10.1007/s12021-013-9186-1", 
    "link": "http://arxiv.org/pdf/1305.2550v2", 
    "title": "HERMES: towards an integrated toolbox to characterize functional and   effective brain connectivity", 
    "arxiv-id": "1305.2550v2", 
    "author": "Francisco del-Pozo", 
    "publish": "2013-05-12T01:04:55Z", 
    "summary": "The analysis of the interdependence between time series has become an\nimportant field of research in the last years, mainly as a result of advances\nin the characterization of dynamical systems from the signals they produce, the\nintroduction of concepts such as generalized and phase synchronization and the\napplication of information theory to time series analysis. In neurophysiology,\ndifferent analytical tools stemming from these concepts have added to the\n'traditional' set of linear methods, which includes the cross-correlation and\nthe coherency function in the time and frequency domain, respectively, or more\nelaborated tools such as Granger Causality. This increase in the number of\napproaches to tackle the existence of functional (FC) or effective connectivity\n(EC) between two (or among many) neural networks, along with the mathematical\ncomplexity of the corresponding time series analysis tools, makes it desirable\nto arrange them into a unified-easy-to-use software package. The goal is to\nallow neuroscientists, neurophysiologists and researchers from related fields\nto easily access and make use of these analysis methods from a single\nintegrated toolbox. Here we present HERMES (http://hermes.ctb.upm.es), a\ntoolbox for the Matlab environment (The Mathworks, Inc), which is designed for\nthe analysis functional and effective brain connectivity from\nneurophysiological data such as multivariate EEG and/or MEG records. It\nincludes also visualization tools and statistical methods to address the\nproblem of multiple comparisons. We believe that this toolbox will be very\nhelpful to all the researchers working in the emerging field of brain\nconnectivity analysis."
},{
    "category": "cs.MS", 
    "doi": "10.1007/s12021-013-9186-1", 
    "link": "http://arxiv.org/pdf/1306.1295v3", 
    "title": "MathGR: a tensor and GR computation package to keep it simple", 
    "arxiv-id": "1306.1295v3", 
    "author": "Yi Wang", 
    "publish": "2013-06-06T05:08:44Z", 
    "summary": "We introduce the MathGR package, written in Mathematica. The package can\nmanipulate tensor and GR calculations with either abstract or explicit indices,\nsimplify tensors with permutational symmetries, decompose tensors from abstract\nindices to partially or completely explicit indices and convert partial\nderivatives into total derivatives. Frequently used GR tensors and a model of\nFRW universe with ADM type perturbations are predefined. The package is built\naround the philosophy to \"keep it simple\", and makes use of latest tensor\ntechnologies of Mathematica."
},{
    "category": "cs.AI", 
    "doi": "10.1007/s12021-013-9186-1", 
    "link": "http://arxiv.org/pdf/1310.2805v1", 
    "title": "MizAR 40 for Mizar 40", 
    "arxiv-id": "1310.2805v1", 
    "author": "Josef Urban", 
    "publish": "2013-10-10T13:24:07Z", 
    "summary": "As a present to Mizar on its 40th anniversary, we develop an AI/ATP system\nthat in 30 seconds of real time on a 14-CPU machine automatically proves 40% of\nthe theorems in the latest official version of the Mizar Mathematical Library\n(MML). This is a considerable improvement over previous performance of large-\ntheory AI/ATP methods measured on the whole MML. To achieve that, a large suite\nof AI/ATP methods is employed and further developed. We implement the most\nuseful methods efficiently, to scale them to the 150000 formulas in MML. This\nreduces the training times over the corpus to 1-3 seconds, allowing a simple\npractical deployment of the methods in the online automated reasoning service\nfor the Mizar users (MizAR)."
},{
    "category": "cs.NA", 
    "doi": "10.1007/s12021-013-9186-1", 
    "link": "http://arxiv.org/pdf/1405.7879v2", 
    "title": "Kahler: An Implementation of Discrete Exterior Calculus on Hermitian   Manifolds", 
    "arxiv-id": "1405.7879v2", 
    "author": "Alex Eftimiades", 
    "publish": "2014-05-30T14:36:33Z", 
    "summary": "This paper details the techniques and algorithms implemented in Kahler, a\nPython library that implements discrete exterior calculus on arbitrary\nHermitian manifolds. Borrowing techniques and ideas first implemented in PyDEC,\nKahler provides a uniquely general framework for computation using discrete\nexterior calculus. Manifolds can have arbitrary dimension, topology, bilinear\nHermitian metrics, and embedding dimension. Kahler comes equipped with tools\nfor generating triangular meshes in arbitrary dimensions with arbitrary\ntopology. Kahler can also generate discrete sharp operators and implement de\nRham maps. Computationally intensive tasks are automatically parallelized over\nthe number of cores detected. The program itself is written in Cython--a\nsuperset of the Python language that is translated to C and compiled for extra\nspeed. Kahler is applied to several example problems: normal modes of a\nvibrating membrane, electromagnetic resonance in a cavity, the quantum harmonic\noscillator, and the Dirac-Kahler equation. Convergence is demonstrated on\nrandom meshes."
},{
    "category": "math.NA", 
    "doi": "10.1007/s12021-013-9186-1", 
    "link": "http://arxiv.org/pdf/1407.7786v2", 
    "title": "Numerical Methods for the Computation of the Confluent and Gauss   Hypergeometric Functions", 
    "arxiv-id": "1407.7786v2", 
    "author": "Mason A. Porter", 
    "publish": "2014-07-29T17:30:23Z", 
    "summary": "The two most commonly used hypergeometric functions are the confluent\nhypergeometric function and the Gauss hypergeometric function. We review the\navailable techniques for accurate, fast, and reliable computation of these two\nhypergeometric functions in different parameter and variable regimes. The\nmethods that we investigate include Taylor and asymptotic series computations,\nGauss-Jacobi quadrature, numerical solution of differential equations,\nrecurrence relations, and others. We discuss the results of numerical\nexperiments used to determine the best methods, in practice, for each parameter\nand variable regime considered. We provide 'roadmaps' with our recommendation\nfor which methods should be used in each situation."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1063/1.4903712", 
    "link": "http://arxiv.org/pdf/1412.7160v1", 
    "title": "The NIFTY way of Bayesian signal inference", 
    "arxiv-id": "1412.7160v1", 
    "author": "Marco Selig", 
    "publish": "2014-12-22T21:00:07Z", 
    "summary": "We introduce NIFTY, \"Numerical Information Field Theory\", a software package\nfor the development of Bayesian signal inference algorithms that operate\nindependently from any underlying spatial grid and its resolution. A large\nnumber of Bayesian and Maximum Entropy methods for 1D signal reconstruction, 2D\nimaging, as well as 3D tomography, appear formally similar, but one often finds\nindividualized implementations that are neither flexible nor easily\ntransferable. Signal inference in the framework of NIFTY can be done in an\nabstract way, such that algorithms, prototyped in 1D, can be applied to real\nworld problems in higher-dimensional settings. NIFTY as a versatile library is\napplicable and already has been applied in 1D, 2D, 3D and spherical settings. A\nrecent application is the D3PO algorithm targeting the non-trivial task of\ndenoising, deconvolving, and decomposing photon observations in high energy\nastronomy."
},{
    "category": "cs.DC", 
    "doi": "10.1063/1.4903712", 
    "link": "http://arxiv.org/pdf/1503.05743v1", 
    "title": "Implementation of a Practical Distributed Calculation System with   Browsers and JavaScript, and Application to Distributed Deep Learning", 
    "arxiv-id": "1503.05743v1", 
    "author": "Tatsuya Harada", 
    "publish": "2015-03-19T12:41:29Z", 
    "summary": "Deep learning can achieve outstanding results in various fields. However, it\nrequires so significant computational power that graphics processing units\n(GPUs) and/or numerous computers are often required for the practical\napplication. We have developed a new distributed calculation framework called\n\"Sashimi\" that allows any computer to be used as a distribution node only by\naccessing a website. We have also developed a new JavaScript neural network\nframework called \"Sukiyaki\" that uses general purpose GPUs with web browsers.\nSukiyaki performs 30 times faster than a conventional JavaScript library for\ndeep convolutional neural networks (deep CNNs) learning. The combination of\nSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the\ndistributed deep learning of deep CNNs only with web browsers on various\ndevices. The libraries that comprise the proposed methods are available under\nMIT license at http://mil-tokyo.github.io/."
},{
    "category": "cs.MS", 
    "doi": "10.1063/1.4903712", 
    "link": "http://arxiv.org/pdf/1506.02618v1", 
    "title": "Solving Polynomial Systems in the Cloud with Polynomial Homotopy   Continuation", 
    "arxiv-id": "1506.02618v1", 
    "author": "Xiangcheng Yu", 
    "publish": "2015-06-08T19:05:49Z", 
    "summary": "Polynomial systems occur in many fields of science and engineering.\nPolynomial homotopy continuation methods apply symbolic-numeric algorithms to\nsolve polynomial systems. We describe the design and implementation of our web\ninterface and reflect on the application of polynomial homotopy continuation\nmethods to solve polynomial systems in the cloud. Via the graph isomorphism\nproblem we organize and classify the polynomial systems we solved. The\nclassification with the canonical form of a graph identifies newly submitted\nsystems with systems that have already been solved."
},{
    "category": "cs.AI", 
    "doi": "10.1007/978-3-319-23219-5_32", 
    "link": "http://arxiv.org/pdf/1510.04914v1", 
    "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing   Difficult Problems", 
    "arxiv-id": "1510.04914v1", 
    "author": "Jean-Marc Alliot", 
    "publish": "2015-10-16T15:18:42Z", 
    "summary": "The only rigorous approaches for achieving a numerical proof of optimality in\nglobal optimization are interval-based methods that interleave branching of the\nsearch-space and pruning of the subdomains that cannot contain an optimal\nsolution. State-of-the-art solvers generally integrate local optimization\nalgorithms to compute a good upper bound of the global minimum over each\nsubspace. In this document, we propose a cooperative framework in which\ninterval methods cooperate with evolutionary algorithms. The latter are\nstochastic algorithms in which a population of candidate solutions iteratively\nevolves in the search-space to reach satisfactory solutions.\n  Within our cooperative solver Charibde, the evolutionary algorithm and the\ninterval-based algorithm run in parallel and exchange bounds, solutions and\nsearch-space in an advanced manner via message passing. A comparison of\nCharibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and\nNLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows\nthat Charibde is highly competitive against non-rigorous solvers and converges\nfaster than rigorous solvers by an order of magnitude."
},{
    "category": "cs.CG", 
    "doi": "10.1007/978-3-319-23219-5_32", 
    "link": "http://arxiv.org/pdf/1606.01289v2", 
    "title": "Conforming restricted Delaunay mesh generation for piecewise smooth   complexes", 
    "arxiv-id": "1606.01289v2", 
    "author": "Darren Engwirda", 
    "publish": "2016-06-03T22:06:58Z", 
    "summary": "A Frontal-Delaunay refinement algorithm for mesh generation in piecewise\nsmooth domains is described. Built using a restricted Delaunay framework, this\nnew algorithm combines a number of novel features, including: (i) an\nunweighted, conforming restricted Delaunay representation for domains specified\nas a (non-manifold) collection of piecewise smooth surface patches and curve\nsegments, (ii) a protection strategy for domains containing curve segments that\nsubtend sharply acute angles, and (iii) a new class of off-centre refinement\nrules designed to achieve high-quality point-placement along embedded curve\nfeatures. Experimental comparisons show that the new Frontal-Delaunay algorithm\noutperforms a classical (statically weighted) restricted Delaunay-refinement\ntechnique for a number of three-dimensional benchmark problems."
},{
    "category": "cs.CE", 
    "doi": "10.1007/978-3-319-23219-5_32", 
    "link": "http://arxiv.org/pdf/1607.03252v1", 
    "title": "Scheduling massively parallel multigrid for multilevel Monte Carlo   methods", 
    "arxiv-id": "1607.03252v1", 
    "author": "Barbara Wohlmuth", 
    "publish": "2016-07-12T07:47:45Z", 
    "summary": "The computational complexity of naive, sampling-based uncertainty\nquantification for 3D partial differential equations is extremely high.\nMultilevel approaches, such as multilevel Monte Carlo (MLMC), can reduce the\ncomplexity significantly, but to exploit them fully in a parallel environment,\nsophisticated scheduling strategies are needed. Often fast algorithms that are\nexecuted in parallel are essential to compute fine level samples in 3D, whereas\nto compute individual coarse level samples only moderate numbers of processors\ncan be employed efficiently. We make use of multiple instances of a parallel\nmultigrid solver combined with advanced load balancing techniques. In\nparticular, we optimize the concurrent execution across the three layers of the\nMLMC method: parallelization across levels, across samples, and across the\nspatial grid. The overall efficiency and performance of these methods will be\nanalyzed. Here the scalability window of the multigrid solver is revealed as\nbeing essential, i.e., the property that the solution can be computed with a\nrange of process numbers while maintaining good parallel efficiency. We\nevaluate the new scheduling strategies in a series of numerical tests, and\nconclude the paper demonstrating large 3D scaling experiments."
},{
    "category": "cs.DS", 
    "doi": "10.1007/978-3-319-23219-5_32", 
    "link": "http://arxiv.org/pdf/1610.09146v1", 
    "title": "Performance evaluation of explicit finite difference algorithms with   varying amounts of computational and memory intensity", 
    "arxiv-id": "1610.09146v1", 
    "author": "Neil D. Sandham", 
    "publish": "2016-10-28T09:45:31Z", 
    "summary": "Future architectures designed to deliver exascale performance motivate the\nneed for novel algorithmic changes in order to fully exploit their\ncapabilities. In this paper, the performance of several numerical algorithms,\ncharacterised by varying degrees of memory and computational intensity, are\nevaluated in the context of finite difference methods for fluid dynamics\nproblems. It is shown that, by storing some of the evaluated derivatives as\nsingle thread- or process-local variables in memory, or recomputing the\nderivatives on-the-fly, a speed-up of ~2 can be obtained compared to\ntraditional algorithms that store all derivatives in global arrays."
},{
    "category": "cs.DC", 
    "doi": "10.1007/978-3-319-23219-5_32", 
    "link": "http://arxiv.org/pdf/cs/0303031v1", 
    "title": "A Bird's eye view of Matrix Distributed Processing", 
    "arxiv-id": "cs/0303031v1", 
    "author": "Massimo Di Pierro", 
    "publish": "2003-03-28T20:47:50Z", 
    "summary": "We present Matrix Distributed Processing, a C++ library for fast development\nof efficient parallel algorithms. MDP is based on MPI and consists of a\ncollection of C++ classes and functions such as lattice, site and field. Once\nan algorithm is written using these components the algorithm is automatically\nparallel and no explicit call to communication functions is required. MDP is\nparticularly suitable for implementing parallel solvers for multi-dimensional\ndifferential equations and mesh-like problems."
},{
    "category": "hep-ph", 
    "doi": "10.1016/j.cpc.2005.04.013", 
    "link": "http://arxiv.org/pdf/hep-ph/0411100v2", 
    "title": "LSJK - a C++ library for arbitrary-precision numeric evaluation of the   generalized log-sine functions", 
    "arxiv-id": "hep-ph/0411100v2", 
    "author": "A. Sheplyakov", 
    "publish": "2004-11-07T17:13:45Z", 
    "summary": "Generalized log-sine functions appear in higher order epsilon-expansion of\ndifferent Feynman diagrams. We present an algorithm for numerical evaluation of\nthese functions of real argument. This algorithm is implemented as C++ library\nwith arbitrary-precision arithmetics for integer 0 < k < 9 and j > 1. Some new\nrelations and representations for the generalized log-sine functions are given."
},{
    "category": "math.CA", 
    "doi": "10.1016/j.cpc.2005.04.013", 
    "link": "http://arxiv.org/pdf/math/0608789v7", 
    "title": "One method for proving inequalities by computer", 
    "arxiv-id": "math/0608789v7", 
    "author": "Branko J. Malesevic", 
    "publish": "2006-08-31T14:59:20Z", 
    "summary": "In this article we consider a method for proving a class of analytical\ninequalities via minimax rational approximations. All numerical calculations in\nthis paper are given by Maple computer program."
},{
    "category": "nlin.SI", 
    "doi": "10.1016/j.cpc.2005.04.013", 
    "link": "http://arxiv.org/pdf/nlin/0407062v1", 
    "title": "Construction of Single-valued Solutions for Nonintegrable Systems with   the Help of the Painleve Test", 
    "arxiv-id": "nlin/0407062v1", 
    "author": "S. Yu. Vernov", 
    "publish": "2004-07-27T12:51:53Z", 
    "summary": "The Painleve test is very useful to construct not only the Laurent-series\nsolutions but also the elliptic and trigonometric ones. Such single-valued\nfunctions are solutions of some polynomial first order differential equations.\nTo find the elliptic solutions we transform an initial nonlinear differential\nequation in a nonlinear algebraic system in parameters of the Laurent-series\nsolutions of the initial equation. The number of unknowns in the obtained\nnonlinear system does not depend on number of arbitrary coefficients of the\nused first order equation. In this paper we describe the corresponding\nalgorithm, which has been realized in REDUCE and Maple."
},{
    "category": "math-ph", 
    "doi": "10.1016/j.cpc.2009.11.007", 
    "link": "http://arxiv.org/pdf/0907.2557v2", 
    "title": "The Multiple Zeta Value Data Mine", 
    "arxiv-id": "0907.2557v2", 
    "author": "J. A. M. Vermaseren", 
    "publish": "2009-07-15T11:14:37Z", 
    "summary": "We provide a data mine of proven results for multiple zeta values (MZVs) of\nthe form $\\zeta(s_1,s_2,...,s_k)=\\sum_{n_1>n_2>...>n_k>0}^\\infty \\{1/(n_1^{s_1}\n>... n_k^{s_k})\\}$ with weight $w=\\sum_{i=1}^k s_i$ and depth $k$ and for Euler\nsums of the form $\\sum_{n_1>n_2>...>n_k>0}^\\infty t\\{(\\epsilon_1^{n_1}\n>...\\epsilon_1 ^{n_k})/ (n_1^{s_1} ... n_k^{s_k}) \\}$ with signs\n$\\epsilon_i=\\pm1$. Notably, we achieve explicit proven reductions of all MZVs\nwith weights $w\\le22$, and all Euler sums with weights $w\\le12$, to bases whose\ndimensions, bigraded by weight and depth, have sizes in precise agreement with\nthe Broadhurst--Kreimer and Broadhurst conjectures. Moreover, we lend further\nsupport to these conjectures by studying even greater weights ($w\\le30$), using\nmodular arithmetic. To obtain these results we derive a new type of relation\nfor Euler sums, the Generalized Doubling Relations. We elucidate the \"pushdown\"\nmechanism, whereby the ornate enumeration of primitive MZVs, by weight and\ndepth, is reconciled with the far simpler enumeration of primitive Euler sums.\nThere is some evidence that this pushdown mechanism finds its origin in\ndoubling relations. We hope that our data mine, obtained by exploiting the\nunique power of the computer algebra language {\\sc form}, will enable the study\nof many more such consequences of the double-shuffle algebra of MZVs, and their\nEuler cousins, which are already the subject of keen interest, to practitioners\nof quantum field theory, and to mathematicians alike."
},{
    "category": "physics.comp-ph", 
    "doi": "10.1371/journal.pone.0012528", 
    "link": "http://arxiv.org/pdf/0912.0161v4", 
    "title": "Community landscapes: an integrative approach to determine overlapping   network module hierarchy, identify key nodes and predict network dynamics", 
    "arxiv-id": "0912.0161v4", 
    "author": "Peter Csermely", 
    "publish": "2009-12-01T10:36:18Z", 
    "summary": "Background: Network communities help the functional organization and\nevolution of complex networks. However, the development of a method, which is\nboth fast and accurate, provides modular overlaps and partitions of a\nheterogeneous network, has proven to be rather difficult. Methodology/Principal\nFindings: Here we introduce the novel concept of ModuLand, an integrative\nmethod family determining overlapping network modules as hills of an influence\nfunction-based, centrality-type community landscape, and including several\nwidely used modularization methods as special cases. As various adaptations of\nthe method family, we developed several algorithms, which provide an efficient\nanalysis of weighted and directed networks, and (1) determine pervasively\noverlapping modules with high resolution; (2) uncover a detailed hierarchical\nnetwork structure allowing an efficient, zoom-in analysis of large networks;\n(3) allow the determination of key network nodes and (4) help to predict\nnetwork dynamics. Conclusions/Significance: The concept opens a wide range of\npossibilities to develop new approaches and applications including network\nrouting, classification, comparison and prediction."
},{
    "category": "cs.CE", 
    "doi": "10.1371/journal.pone.0012528", 
    "link": "http://arxiv.org/pdf/1001.3213v2", 
    "title": "Using Premia and Nsp for Constructing a Risk Management Benchmark for   Testing Parallel Architecture", 
    "arxiv-id": "1001.3213v2", 
    "author": "Bernard Lapeyre", 
    "publish": "2010-01-19T07:54:16Z", 
    "summary": "Financial institutions have massive computations to carry out overnight which\nare very demanding in terms of the consumed CPU. The challenge is to price many\ndifferent products on a cluster-like architecture. We have used the Premia\nsoftware to valuate the financial derivatives. In this work, we explain how\nPremia can be embedded into Nsp, a scientific software like Matlab, to provide\na powerful tool to valuate a whole portfolio. Finally, we have integrated an\nMPI toolbox into Nsp to enable to use Premia to solve a bunch of pricing\nproblems on a cluster. This unified framework can then be used to test\ndifferent parallel architectures."
},{
    "category": "cs.CG", 
    "doi": "10.1371/journal.pone.0012528", 
    "link": "http://arxiv.org/pdf/1112.4523v1", 
    "title": "Complexity and Algorithms for Euler Characteristic of Simplicial   Complexes", 
    "arxiv-id": "1112.4523v1", 
    "author": "Eduardo S\u00e1enz de Cabez\u00f3n", 
    "publish": "2011-12-19T22:51:21Z", 
    "summary": "We consider the problem of computing the Euler characteristic of an abstract\nsimplicial complex given by its vertices and facets. We show that this problem\nis #P-complete and present two new practical algorithms for computing Euler\ncharacteristic. The two new algorithms are derived using combinatorial\ncommutative algebra and we also give a second description of them that requires\nno algebra. We present experiments showing that the two new algorithms can be\nimplemented to be faster than previous Euler characteristic implementations by\na large margin."
},{
    "category": "cs.SC", 
    "doi": "10.1016/j.jsc.2008.04.009", 
    "link": "http://arxiv.org/pdf/1203.1017v1", 
    "title": "On the asymptotic and practical complexity of solving bivariate systems   over the reals", 
    "arxiv-id": "1203.1017v1", 
    "author": "Elias P. Tsigaridas", 
    "publish": "2012-03-05T19:39:05Z", 
    "summary": "This paper is concerned with exact real solving of well-constrained,\nbivariate polynomial systems. The main problem is to isolate all common real\nroots in rational rectangles, and to determine their intersection\nmultiplicities. We present three algorithms and analyze their asymptotic bit\ncomplexity, obtaining a bound of $\\sOB(N^{14})$ for the purely projection-based\nmethod, and $\\sOB(N^{12})$ for two subresultant-based methods: this notation\nignores polylogarithmic factors, where $N$ bounds the degree and the bitsize of\nthe polynomials. The previous record bound was $\\sOB(N^{14})$.\n  Our main tool is signed subresultant sequences. We exploit recent advances on\nthe complexity of univariate root isolation, and extend them to sign evaluation\nof bivariate polynomials over two algebraic numbers, and real root counting for\npolynomials over an extension field. Our algorithms apply to the problem of\nsimultaneous inequalities; they also compute the topology of real plane\nalgebraic curves in $\\sOB(N^{12})$, whereas the previous bound was\n$\\sOB(N^{14})$.\n  All algorithms have been implemented in MAPLE, in conjunction with numeric\nfiltering. We compare them against FGB/RS, system solvers from SYNAPS, and\nMAPLE libraries INSULATE and TOP, which compute curve topology. Our software is\namong the most robust, and its runtimes are comparable, or within a small\nconstant factor, with respect to the C/C++ libraries.\n  Key words: real solving, polynomial systems, complexity, MAPLE software"
},{
    "category": "cs.IT", 
    "doi": "10.3389/frobt.2014.00011", 
    "link": "http://arxiv.org/pdf/1408.3270v2", 
    "title": "JIDT: An information-theoretic toolkit for studying the dynamics of   complex systems", 
    "arxiv-id": "1408.3270v2", 
    "author": "Joseph T. Lizier", 
    "publish": "2014-08-14T13:11:15Z", 
    "summary": "Complex systems are increasingly being viewed as distributed information\nprocessing systems, particularly in the domains of computational neuroscience,\nbioinformatics and Artificial Life. This trend has resulted in a strong uptake\nin the use of (Shannon) information-theoretic measures to analyse the dynamics\nof complex systems in these fields. We introduce the Java Information Dynamics\nToolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3\nlicensed) open-source code implementation for empirical estimation of\ninformation-theoretic measures from time-series data. While the toolkit\nprovides classic information-theoretic measures (e.g. entropy, mutual\ninformation, conditional mutual information), it ultimately focusses on\nimplementing higher-level measures for information dynamics. That is, JIDT\nfocusses on quantifying information storage, transfer and modification, and the\ndynamics of these operations in space and time. For this purpose, it includes\nimplementations of the transfer entropy and active information storage, their\nmultivariate extensions and local or pointwise variants. JIDT provides\nimplementations for both discrete and continuous-valued data for each measure,\nincluding various types of estimator for continuous data (e.g. Gaussian,\nbox-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped at run-time\ndue to Java's object-oriented polymorphism. Furthermore, while written in Java,\nthe toolkit can be used directly in MATLAB, GNU Octave, Python and other\nenvironments. We present the principles behind the code design, and provide\nseveral examples to guide users."
},{
    "category": "physics.data-an", 
    "doi": "10.3389/frobt.2014.00011", 
    "link": "http://arxiv.org/pdf/1410.6910v3", 
    "title": "SPIKY: A graphical user interface for monitoring spike train synchrony", 
    "arxiv-id": "1410.6910v3", 
    "author": "Nebojsa Bozanic", 
    "publish": "2014-10-25T11:02:26Z", 
    "summary": "Techniques for recording large-scale neuronal spiking activity are developing\nvery fast. This leads to an increasing demand for algorithms capable of\nanalyzing large amounts of experimental spike train data. One of the most\ncrucial and demanding tasks is the identification of similarity patterns with a\nvery high temporal resolution and across different spatial scales. To address\nthis task, in recent years three time-resolved measures of spike train\nsynchrony have been proposed, the ISI-distance, the SPIKE-distance, and event\nsynchronization. The Matlab source codes for calculating and visualizing these\nmeasures have been made publicly available. However, due to the many different\npossible representations of the results the use of these codes is rather\ncomplicated and their application requires some basic knowledge of Matlab. Thus\nit became desirable to provide a more user-friendly and interactive interface.\nHere we address this need and present SPIKY, a graphical user interface which\nfacilitates the application of time-resolved measures of spike train synchrony\nto both simulated and real data. SPIKY includes implementations of the\nISI-distance, the SPIKE-distance and SPIKE-synchronization (an improved and\nsimplified extension of event synchronization) which have been optimized with\nrespect to computation speed and memory demand. It also comprises a spike train\ngenerator and an event detector which makes it capable of analyzing continuous\ndata. Finally, the SPIKY package includes additional complementary programs\naimed at the analysis of large numbers of datasets and the estimation of\nsignificance levels."
},{
    "category": "cs.NA", 
    "doi": "10.1109/CEC.2015.7257223", 
    "link": "http://arxiv.org/pdf/1504.02366v1", 
    "title": "A Collection of Challenging Optimization Problems in Science,   Engineering and Economics", 
    "arxiv-id": "1504.02366v1", 
    "author": "Crina Grosan", 
    "publish": "2015-04-09T16:31:25Z", 
    "summary": "Function optimization and finding simultaneous solutions of a system of\nnonlinear equations (SNE) are two closely related and important optimization\nproblems. However, unlike in the case of function optimization in which one is\nrequired to find the global minimum and sometimes local minima, a database of\nchallenging SNEs where one is required to find stationary points (extrama and\nsaddle points) is not readily available. In this article, we initiate building\nsuch a database of important SNE (which also includes related function\noptimization problems), arising from Science, Engineering and Economics. After\nproviding a short review of the most commonly used mathematical and\ncomputational approaches to find solutions of such systems, we provide a\npreliminary list of challenging problems by writing the Mathematical\nformulation down, briefly explaning the origin and importance of the problem\nand giving a short account on the currently known results, for each of the\nproblems. We anticipate that this database will not only help benchmarking\nnovel numerical methods for solving SNEs and function optimization problems but\nalso will help advancing the corresponding research areas."
},{
    "category": "astro-ph.IM", 
    "doi": "10.1051/0004-6361/201321236", 
    "link": "http://arxiv.org/pdf/1301.4499v2", 
    "title": "NIFTY - Numerical Information Field Theory - a versatile Python library   for signal inference", 
    "arxiv-id": "1301.4499v2", 
    "author": "Torsten A. En\u00dflin", 
    "publish": "2013-01-18T21:00:01Z", 
    "summary": "NIFTY, \"Numerical Information Field Theory\", is a software package designed\nto enable the development of signal inference algorithms that operate\nregardless of the underlying spatial grid and its resolution. Its\nobject-oriented framework is written in Python, although it accesses libraries\nwritten in Cython, C++, and C for efficiency. NIFTY offers a toolkit that\nabstracts discretized representations of continuous spaces, fields in these\nspaces, and operators acting on fields into classes. Thereby, the correct\nnormalization of operations on fields is taken care of automatically without\nconcerning the user. This allows for an abstract formulation and programming of\ninference algorithms, including those derived within information field theory.\nThus, NIFTY permits its user to rapidly prototype algorithms in 1D, and then\napply the developed code in higher-dimensional settings of real world problems.\nThe set of spaces on which NIFTY operates comprises point sets, n-dimensional\nregular grids, spherical spaces, their harmonic counterparts, and product\nspaces constructed as combinations of those. The functionality and diversity of\nthe package is demonstrated by a Wiener filter code example that successfully\nruns without modification regardless of the space on which the inference\nproblem is defined."
},lol]