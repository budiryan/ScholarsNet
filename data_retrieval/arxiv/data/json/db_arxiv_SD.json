[{
    "category": "cs.SD", 
    "doi": "10.1109/89.876310", 
    "link": "http://arxiv.org/pdf/cs/0005022v2", 
    "title": "Fractionally-addressed delay lines", 
    "arxiv-id": "cs/0005022v2", 
    "author": "Davide Rocchesso", 
    "publish": "2000-05-17T14:30:01Z", 
    "summary": "While traditional implementations of variable-length digital delay lines are\nbased on a circular buffer accessed by two pointers, we propose an\nimplementation where a single fractional pointer is used both for read and\nwrite operations. On modern general-purpose architectures, the proposed method\nis nearly as efficient as the popularinterpolated circular buffer, and it\nbehaves well for delay-length modulations commonly found in digital audio\neffects. The physical interpretation of the new implementation shows that it is\nsuitable for simulating tension or density modulations in wave-propagating\nmedia."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.409214", 
    "link": "http://arxiv.org/pdf/cs/0007014v1", 
    "title": "The Sound Manifesto", 
    "arxiv-id": "cs/0007014v1", 
    "author": "Ilia Bisnovatyi", 
    "publish": "2000-07-09T00:34:07Z", 
    "summary": "Computing practice today depends on visual output to drive almost all user\ninteraction. Other senses, such as audition, may be totally neglected, or used\ntangentially, or used in highly restricted specialized ways. We have excellent\naudio rendering through D-A conversion, but we lack rich general facilities for\nmodeling and manipulating sound comparable in quality and flexibility to\ngraphics. We need co-ordinated research in several disciplines to improve the\nuse of sound as an interactive information channel.\n  Incremental and separate improvements in synthesis, analysis, speech\nprocessing, audiology, acoustics, music, etc. will not alone produce the\nradical progress that we seek in sonic practice. We also need to create a new\ncentral topic of study in digital audio research. The new topic will assimilate\nthe contributions of different disciplines on a common foundation. The key\ncentral concept that we lack is sound as a general-purpose information channel.\nWe must investigate the structure of this information channel, which is driven\nby the co-operative development of auditory perception and physical sound\nproduction. Particular audible encodings, such as speech and music, illuminate\nsonic information by example, but they are no more sufficient for a\ncharacterization than typography is sufficient for a characterization of visual\ninformation."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/cs/0010017v2", 
    "title": "Generalization of a 3-D resonator model for the simulation of spherical   enclosures", 
    "arxiv-id": "cs/0010017v2", 
    "author": "Pierre Dutilleux", 
    "publish": "2000-10-10T13:08:28Z", 
    "summary": "A rectangular enclosure has such an even distribution of resonances that it\ncan be accurately and efficiently modelled using a feedback delay network.\nConversely, a non rectangular shape such as a sphere has a distribution of\nresonances that challenges the construction of an efficient model. This work\nproposes an extension of the already known feedback delay network structure to\nmodel the resonant properties of a sphere. A specific frequency distribution of\nresonances can be approximated, up to a certain frequency, by inserting an\nallpass filter of moderate order after each delay line of a feedback delay\nnetwork. The structure used for rectangular boxes is therefore augmented with a\nset of allpass filters allowing parametric control over the enclosure size and\nthe boundary properties. This work was motivated by informal listening tests\nwhich have shown that it is possible to identify a basic shape just from the\ndistribution of its audible resonances."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/cs/0103005v1", 
    "title": "Source-Filter Decomposition of Harmonic Sounds", 
    "arxiv-id": "cs/0103005v1", 
    "author": "Michael J. O'Donnell", 
    "publish": "2001-03-05T04:41:12Z", 
    "summary": "This paper describes a method for decomposing steady-state instrument data\ninto excitation and formant filter components. The input data, taken from\nseveral series of recordings of acoustical instruments is analyzed in the\nfrequency domain, and for each series a model is built, which most accurately\nrepresents the data as a source-filter system. The source part is taken to be a\nharmonic excitation system with frequency-invariant magnitudes, and the filter\npart is considered to be responsible for all spectral inhomogenieties. This\nmethod has been applied to the SHARC database of steady state instrument data\nto create source-filter models for a large number of acoustical instruments.\nSubsequent use of such models can have a wide variety of applications,\nincluding improvements to wavetable and physical modeling synthesis, high\nquality pitch shifting, and creation of \"hybrid\" instrument timbres."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/cs/0103006v1", 
    "title": "Flexible Software Framework for Modal Synthesis", 
    "arxiv-id": "cs/0103006v1", 
    "author": "Ilia Bisnovatyi", 
    "publish": "2001-03-05T05:05:15Z", 
    "summary": "Modal synthesis is an important area of physical modeling whose exploration\nin the past has been held back by a large number of control parameters, the\nscarcity of general-purpose design tools and the difficulty of obtaining the\ncomputational power required for real-time synthesis. This paper presents an\noverview of a flexible software framework facilitating the design and control\nof instruments based on modal synthesis. The framework is designed as a\nhierarchy of polymorphic synthesis objects, representing modal structures of\nvarious complexity. As a method of generalizing all interactions among the\nelements of a modal system, an abstract notion of {\\it energy} is introduced,\nand a set of energy transfer functions is provided. Such abstraction leads to a\ndesign where the dynamics of interactions can be largely separated from the\nspecifics of particular modal structures, yielding an easily configurable and\nexpandable system. A real-time version of the framework has been implemented as\na set of C++ classes along with an integrating shell and a GUI, and is\ncurrently being used to design and play modal instruments, as well as to survey\nfundamental properties of various modal algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/cs/0701177v4", 
    "title": "Pitch Tracking of Acoustic Signals based on Average Squared Mean   Difference Function", 
    "arxiv-id": "cs/0701177v4", 
    "author": "Sagnik Sinha", 
    "publish": "2007-01-26T20:07:43Z", 
    "summary": "In this paper, a method of pitch tracking based on variance minimization of\nlocally periodic subsamples of an acoustic signal is presented. Replicates\nalong the length of the periodically sampled data of the signal vector are\ntaken and locally averaged sample variances are minimized to estimate the\nfundamental frequency. Using this method, pitch tracking of any text\nindependent voiced signal is possible for different speakers."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0803.0197v1", 
    "title": "DSP Based System for Real time Voice Synthesis Applications Development", 
    "arxiv-id": "0803.0197v1", 
    "author": "Costin Miron", 
    "publish": "2008-03-03T09:04:46Z", 
    "summary": "This paper describes an experimental system designed for development of real\ntime voice synthesis applications. The system is composed from a DSP\ncoprocessor card, equipped with an TMS320C25 or TMS320C50 chip, voice\nacquisition module (ADDA2),host computer (IBM-PC compatible), software specific\ntools."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0812.0706v1", 
    "title": "Which notes are Vadi-Samvadi in Raga Rageshree?", 
    "arxiv-id": "0812.0706v1", 
    "author": "Kartik Mahto", 
    "publish": "2008-12-03T12:47:08Z", 
    "summary": "The notes which play the most important and second most important roles in\nexpressing a raga are called Vadi and Samvadi swars respectively in (North)\nIndian Classical music. Like Bageshree, Bhairavi, Shankara, Hamir and Kalingra,\nRageshree is another controversial raga so far as the choice of Vadi-Samvadi\nselection is concerned where there are two different opinions. In the present\nwork, a two minute vocal recording of raga Rageshree is subjected to a careful\nstatistical analysis. Our analysis is broken into three phases: first half,\nmiddle half and last half. Under a multinomial model set up holding appreciably\nin the first two phases, only one opinion is found acceptable. In the last\nphase the distribution seems to be quasi multinomial, characterized by an\nunstable nature of relative occurrence of pitch of all the notes and although\nthe note whose relative occurrence of pitch suddenly shoots is the Vadi swar\nselected from our analysis of the first two phases, we take it as an outlier\ndemanding a separate treatment like any other in statistics. Selection of\nVadi-Samvadi notes in a quasi-multinomial set up is still an open research\nproblem. An interesting musical cocktail is proposed, however, embedding\nseveral ideas like melodic property of notes, note combinations and pitch\nmovements between notes, using some weighted combination of psychological and\nstatistical stability of notes along with watching carefully the sudden shoot\nof one or more notes whenever there is enough evidence that multinomial model\nhas broken down."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0901.2416v1", 
    "title": "TR01: Time-continuous Sparse Imputation", 
    "arxiv-id": "0901.2416v1", 
    "author": "B. Cranen", 
    "publish": "2009-01-16T08:24:14Z", 
    "summary": "An effective way to increase the noise robustness of automatic speech\nrecognition is to label noisy speech features as either reliable or unreliable\n(missing) prior to decoding, and to replace the missing ones by clean speech\nestimates. We present a novel method to obtain such clean speech estimates.\nUnlike previous imputation frameworks which work on a frame-by-frame basis, our\nmethod focuses on exploiting information from a large time-context. Using a\nsliding window approach, denoised speech representations are constructed using\na sparse representation of the reliable features in an overcomplete basis of\nfixed-length exemplar fragments. We demonstrate the potential of our approach\nwith experiments on the AURORA-2 connected digit database."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0901.3902v1", 
    "title": "iKlax: a New Musical Audio Format for Active Listening", 
    "arxiv-id": "0901.3902v1", 
    "author": "Sylvain Marchand", 
    "publish": "2009-01-25T14:57:55Z", 
    "summary": "In this paper, we are presenting a new model for interactive music. Unlike\nmost interactive systems, our model is based on file organization, but does not\nrequire digital audio treatments. This model includes a definition of a\nconstraints system and its solver. The products of this project are intended\nfor the general public, inexperienced users, as well as professional musicians,\nand will be distributed commercially. We are here presenting three products of\nthis project. The difficulty of this project is to design a technology and\nsoftware products for interactive music which must be easy to use by the\ngeneral public and by professional composers."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0902.2783v2", 
    "title": "New Ica-Beamforming Method to Under-Determined BSS", 
    "arxiv-id": "0902.2783v2", 
    "author": "Seyed Mohammad Ahadi", 
    "publish": "2009-02-16T21:03:30Z", 
    "summary": "This paper has been withdrawn by the author ali pourmohammad."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0903.3080v1", 
    "title": "A Unified Theory of Time-Frequency Reassignment", 
    "arxiv-id": "0903.3080v1", 
    "author": "Sean A. Fulop", 
    "publish": "2009-03-18T03:33:48Z", 
    "summary": "Time-frequency representations such as the spectrogram are commonly used to\nanalyze signals having a time-varying distribution of spectral energy, but the\nspectrogram is constrained by an unfortunate tradeoff between resolution in\ntime and frequency. A method of achieving high-resolution spectral\nrepresentations has been independently introduced by several parties. The\ntechnique has been variously named reassignment and remapping, but while the\nimplementations have differed in details, they are all based on the same\ntheoretical and mathematical foundation. In this work, we present a brief\nhistory of work on the method we will call the method of time-frequency\nreassignment, and present a unified mathematical description of the technique\nand its derivation. We will focus on the development of time-frequency\nreassignment in the context of the spectrogram, and conclude with a discussion\nof some current applications of the reassigned spectrogram."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0903.3198v1", 
    "title": "TR02: State dependent oracle masks for improved dynamical features", 
    "arxiv-id": "0903.3198v1", 
    "author": "B. Cranen", 
    "publish": "2009-03-18T16:51:25Z", 
    "summary": "Using the AURORA-2 digit recognition task, we show that recognition\naccuracies obtained with classical, SNR based oracle masks can be substantially\nimproved by using a state-dependent mask estimation technique."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0907.3220v1", 
    "title": "Inter Genre Similarity Modelling For Automatic Music Genre   Classification", 
    "arxiv-id": "0907.3220v1", 
    "author": "Engin Erzin", 
    "publish": "2009-07-18T13:25:59Z", 
    "summary": "Music genre classification is an essential tool for music information\nretrieval systems and it has been finding critical applications in various\nmedia platforms. Two important problems of the automatic music genre\nclassification are feature extraction and classifier design. This paper\ninvestigates inter-genre similarity modelling (IGS) to improve the performance\nof automatic music genre classification. Inter-genre similarity information is\nextracted over the mis-classified feature population. Once the inter-genre\nsimilarity is modelled, elimination of the inter-genre similarity reduces the\ninter-genre confusion and improves the identification rates. Inter-genre\nsimilarity modelling is further improved with iterative IGS modelling(IIGS) and\nscore modelling for IGS elimination(SMIGS). Experimental results with promising\nclassification improvements are provided."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0909.2363v1", 
    "title": "Improvement of Text Dependent Speaker Identification System Using   Neuro-Genetic Hybrid Algorithm in Office Environmental Conditions", 
    "arxiv-id": "0909.2363v1", 
    "author": "Md. Fayzur Rahman", 
    "publish": "2009-09-12T20:36:35Z", 
    "summary": "In this paper, an improved strategy for automated text dependent speaker\nidentification system has been proposed in noisy environment. The\nidentification process incorporates the Neuro- Genetic hybrid algorithm with\ncepstral based features. To remove the background noise from the source\nutterance, wiener filter has been used. Different speech pre-processing\ntechniques such as start-end point detection algorithm, pre-emphasis filtering,\nframe blocking and windowing have been used to process the speech utterances.\nRCC, MFCC, MFCC, MFCC, LPC and LPCC have been used to extract the features.\nAfter feature extraction of the speech, Neuro-Genetic hybrid algorithm has been\nused in the learning and identification purposes. Features are extracted by\nusing different techniques to optimize the performance of the identification.\nAccording to the VALID speech database, the highest speaker identification rate\nof 100.000 percent for studio environment and 82.33 percent for office\nenvironmental conditions have been achieved in the close set text dependent\nspeaker identification system."
},{
    "category": "cs.SD", 
    "doi": "10.1155/S1110865701000105", 
    "link": "http://arxiv.org/pdf/0911.3538v1", 
    "title": "Noise Speech wavelet analyzing in special time ranges", 
    "arxiv-id": "0911.3538v1", 
    "author": "Amin Daneshmand Malayeri", 
    "publish": "2009-11-18T13:36:20Z", 
    "summary": "Speech analyzing in special periods of time has been presented in this paper.\nOne of the most important periods in signal processing is near to Zero. By this\npaper, we analyze noise speech signals when these signals are near to Zero. Our\nstrategy is defining some subfunctions and compress histograms when a noise\nspeech signal is in a special period. It can be so useful for wavelet signal\nprocessing and spoken systems analyzing."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/0911.5171v1", 
    "title": "Untangling Phase and Time in Monophonic Sounds", 
    "arxiv-id": "0911.5171v1", 
    "author": "Henning Thielemann", 
    "publish": "2009-11-26T23:59:44Z", 
    "summary": "We are looking for a mathematical model of monophonic sounds with independent\ntime and phase dimensions. With such a model we can resynthesise a sound with\narbitrarily modulated frequency and progress of the timbre. We propose such a\nmodel and show that it exactly fulfils some natural properties, like a kind of\ntime-invariance, robustness against non-harmonic frequencies, envelope\npreservation, and inclusion of plain resampling as a special case. The\nresulting algorithm is efficient and allows to process data in a streaming\nmanner with phase and shape modulation at sample rate, what we demonstrate with\nan implementation in the functional language Haskell. It allows a wide range of\napplications, namely pitch shifting and time scaling, creative FM synthesis\neffects, compression of monophonic sounds, generating loops for sampled sounds,\nsynthesise sounds similar to wavetable synthesis, or making ultrasound audible."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/0912.0745v1", 
    "title": "A Digital Guitar Tuner", 
    "arxiv-id": "0912.0745v1", 
    "author": "Anjali Kuppayil Saji", 
    "publish": "2009-12-04T20:27:14Z", 
    "summary": "The objective of this paper is to understand the critical parameters that\nneed to be addressed while designing a guitar tuner. The focus of the design\nlies in developing a suitable algorithm to accurately detect the fundamental\nfrequency of a plucked guitar string from its frequency spectrum. A\nuserfriendly graphical interface is developed using Matlab to allow any user to\neasily tune his guitar using the developed program."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/1001.4190v1", 
    "title": "Speech Recognition of the letter 'zha' in Tamil Language using HMM", 
    "arxiv-id": "1001.4190v1", 
    "author": "D. Narasimhan", 
    "publish": "2010-01-23T19:04:50Z", 
    "summary": "Speech signals of the letter 'zha' in Tamil language of 3 males and 3 females\nwere coded using an improved version of Linear Predictive Coding (LPC). The\nsampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,\nwhere the original bit rate was at 128000 bits per second with the help of wave\nsurfer audio tool. The output LPC cepstrum is implemented in first order three\nstate Hidden Markov Model(HMM) chain."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/1003.4908v1", 
    "title": "Perceptual analyses of action-related impact sounds", 
    "arxiv-id": "1003.4908v1", 
    "author": "Stephen Mcadams", 
    "publish": "2010-03-25T14:24:52Z", 
    "summary": "Among environmental sounds, we have chosen to study a class of action-related\nimpact sounds: automobile door closure sounds. We propose to describe these\nsounds using a model composed of perceptual properties. The development of the\nperceptual model was derived from the evaluation of many door closure sounds\nmeasured under controlled laboratory listening conditions. However, listening\nto such sounds normally occurs within a natural context, which probably\nmodifies their perception. We therefore need to study differences between the\nreal situation and the laboratory situation by following standard practices in\norder to specify the precise listening conditions and observe the influence of\nprevious learning, expectations, action-perception interactions, and attention\ngiven to sounds. Our process consists in doing in situ experiments that are\ncompared with specific laboratory experiments in order to isolate certain\ninfluential, context dependent components."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/1004.4478v1", 
    "title": "Intelligent System for Speaker Identification using Lip features with   PCA and ICA", 
    "arxiv-id": "1004.4478v1", 
    "author": "Ritu Tiwari", 
    "publish": "2010-04-26T10:58:19Z", 
    "summary": "Biometric authentication techniques are more consistent and efficient than\nconventional authentication techniques and can be used in monitoring,\ntransaction authentication, information retrieval, access control, forensics,\netc. In this paper, we have presented a detailed comparative analysis between\nPrinciple Component Analysis (PCA) and Independent Component Analysis (ICA)\nwhich are used for feature extraction on the basis of different Artificial\nNeural Network (ANN) such as Back Propagation (BP), Radial Basis Function (RBF)\nand Learning Vector Quantization (LVQ). In this paper, we have chosen \"TULIPS1\ndatabase, (Movellan, 1995)\" which is a small audiovisual database of 12\nsubjects saying the first 4 digits in English for the incorporation of above\nmethods. The six geometric lip features i.e. height of the outer corners of the\nmouth, width of the outer corners of the mouth, height of the inner corners of\nthe mouth, width of the inner corners of the mouth, height of the upper lip,\nand height of the lower lip which extracts the identity relevant information\nare considered for the research work. After the comprehensive analysis and\nevaluation a maximum of 91.07% accuracy in speaker recognition is achieved\nusing PCA and RBF and 87.36% accuracy is achieved using ICA and RBF. Speaker\nidentification has a wide scope of applications such as access control,\nmonitoring, transaction authentication, information retrieval, forensics, etc."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/1005.2465v2", 
    "title": "Dichotic harmony for the musical practice", 
    "arxiv-id": "1005.2465v2", 
    "author": "Vadim R. Madgazin", 
    "publish": "2010-05-14T07:26:49Z", 
    "summary": "The dichotic method of hearing sound adapts in the region of musical harmony.\nThe algorithm of the separation of the being dissonant voices into several\nseparate groups is proposed. For an increase in the pleasantness of chords the\ndifferent groups of voices are heard out through the different channels of\nheadphones. Is created two demonstration program for PC. Keywords: music,\nharmony, chord, dichotic listening, dissonance, consonance, headphones,\npleasantness, midi."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2010.11001", 
    "link": "http://arxiv.org/pdf/1006.0831v1", 
    "title": "Treatment the Effects of Studio Wall Resonance and Coincidence Phenomena   for Recording Noisy Speech Via FPGA Digital Filter", 
    "arxiv-id": "1006.0831v1", 
    "author": "Mahmoud I. A. Abdalla", 
    "publish": "2010-06-04T09:58:27Z", 
    "summary": "This work introduces an economic solution for the problems of sound\ninsulation of recording studios. Sound insulation at wall resonance frequency\nis weak. Instead of acoustical treatment, a digital filter is used to eliminate\nthe effects of wall resonance and coincidence phenomena on recording of speech.\nSound insulation of studio is measured to calculate the wall resonance\nfrequency and the coincidence frequency. Pole /zero placement technique is used\nto calculate the IIR filter coefficients. The digital filter is designed,\nsimulated and implemented. The proposed system is used to treat these problems\nand it is shown to be effective in recording the noisy speech. In this work\ndigital signal processing is used instead of the acoustic treatment to\neliminate the effect of noise at the studio wall resonance. This technique is\ncheap and effective in canceling the noise at the desired frequencies. Field\nProgrammable Gate Array (FPGA) is used for hardware implementation of the\nproposed filter structure which provides fast and cheap solution for processing\nreal time audio signals. The implementation is carried out using Spartan chip\nfrom Xinlinx achieving higher performance than commercially available software\nsolutions."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1006.0866v1", 
    "title": "A Study on the Interactive \"HOPSCOTCH\" Game for the Children Using   Computer Music Techniques", 
    "arxiv-id": "1006.0866v1", 
    "author": "Chih-Fang Huang", 
    "publish": "2010-06-04T11:20:08Z", 
    "summary": "\"Hopscotch\" is a world-wide game for children to play since the times in the\nancient Roman Empire and China. Here we present a study mainly focused on the\nresearch and discussion of the application on the children's well-know\nedutainment via the physical interactive design to provide the sensing of the\ntimes for the conventional hopscotch, which is a new type of experiment for the\ntechnology aided edutainment. The innovated hopscotch music game involves the\nsound samples of various animals and the characters of cartoon, and the\nalgorithmic composition via the development of the music technology based\ninteractive game, to gradually make the children perceive the world of digits,\nsound, and music. It can guide the growing children's personality and character\nfrom disorder into clarity. Furthermore, the traditional teaching materials can\nbe improved via the implementation of the electrical sensing devices,\nelectrical I/O module, and the computer music program Max/MSP, to integrate the\ninteractive computer music with the interactive and immersive soundscapes\ncomposition, and the teaching tool with educational gaming is completely\naccomplished eventually."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1009.2796v1", 
    "title": "Estimation of Infants' Cry Fundamental Frequency using a Modified SIFT   algorithm", 
    "arxiv-id": "1009.2796v1", 
    "author": "Dror Lederman", 
    "publish": "2010-09-14T21:36:25Z", 
    "summary": "This paper addresses the problem of infants' cry fundamental frequency\nestimation. The fundamental frequency is estimated using a modified simple\ninverse filtering tracking (SIFT) algorithm. The performance of the modified\nSIFT is studied using a real database of infants' cry. It is shown that the\nalgorithm is capable of overcoming the problem of under-estimation and\nover-estimation of the cry fundamental frequency, with an estimation accuracy\nof 6.15% and 3.75%, for hyperphonated and phonated cry segments, respectively.\nSome typical examples of the fundamental frequency contour in typical cases of\npathological and healthy cry signals are presented and discussed."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1012.2797v1", 
    "title": "Should Corpora be Big, Rich, or Dense?", 
    "arxiv-id": "1012.2797v1", 
    "author": "Ryan Shosted", 
    "publish": "2010-12-13T16:44:46Z", 
    "summary": "In this paper, we ask what properties makes a large corpus more or less\nuseful. We suggest that size, by itself, should not be the ultimate goal of\nbuilding a corpus. Large-scale corpora are considered desirable because they\noffer statistical stability and rich variation. But this rich variation means\nmore factors to control and evaluate, which can limit the advantages of size.\nWe discuss the use of multi-channel data to complement large-scale speech\ncorpora. Even though multi-channel data may limit the scale of a corpus (due to\nthe complex and labor-intensive nature of data collection) they can offer\ninformation that allows us to tease apart various factors related to speech\nproduction."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1101.1682v1", 
    "title": "Detecting gross alignment errors in the Spoken British National Corpus", 
    "arxiv-id": "1101.1682v1", 
    "author": "Greg Kochanski", 
    "publish": "2011-01-09T23:02:52Z", 
    "summary": "The paper presents methods for evaluating the accuracy of alignments between\ntranscriptions and audio recordings. The methods have been applied to the\nSpoken British National Corpus, which is an extensive and varied corpus of\nnatural unscripted speech. Early results show good agreement with human ratings\nof alignment accuracy. The methods also provide an indication of the location\nof likely alignment problems; this should allow efficient manual examination of\nlarge corpora. Automatic checking of such alignments is crucial when analysing\nany very large corpus, since even the best current speech alignment systems\nwill occasionally make serious errors. The methods described here use a hybrid\napproach based on statistics of the speech signal itself, statistics of the\nlabels being evaluated, and statistics linking the two."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1103.4118v1", 
    "title": "Sampling-rate-aware noise generation", 
    "arxiv-id": "1103.4118v1", 
    "author": "Henning Thielemann", 
    "publish": "2011-03-21T19:32:00Z", 
    "summary": "In this paper we consider the generation of discrete white noise. Despite\nthis seems to be a simple problem, common noise generator implementations do\nnot deliver comparable results at different sampling rates. First we define\nwhat we mean with \"comparable results\". From this we conclude, that the\nvariance of the random variables shall grow proportionally to the sampling\nrate. Eventually we consider how noise behaves under common signal\ntransformations, such as frequency filters, quantisation and impulse generation\nand we explore how these signal transformations must be designed in order\ngenerate sampling-rate-aware results when applied to white noise."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1106.0844v1", 
    "title": "A Fast Affine Projection Algorithm Based on Matching Pursuit in Adaptive   Noise Cancellation for Speech Enhancement", 
    "arxiv-id": "1106.0844v1", 
    "author": "N. Sonbolestan", 
    "publish": "2011-06-04T18:10:09Z", 
    "summary": "In many application of noise cancellation, the changes in signal\ncharacteristics could be quite fast. This requires the utilization of adaptive\nalgorithms, which converge rapidly. Least Mean Squares (LMS) adaptive filters\nhave been used in a wide range of signal processing application. The Recursive\nLeast Squares (RLS) algorithm has established itself as the \"ultimate\" adaptive\nfiltering algorithm in the sense that it is the adaptive filter exhibiting the\nbest convergence behavior. Unfortunately, practical implementations of the\nalgorithm are often associated with high computational complexity and/or poor\nnumerical properties. Recently adaptive filtering was presented that was based\non Matching Pursuits, have a nice tradeoff between complexity and the\nconvergence speed. This paper describes a new approach for noise cancellation\nin speech enhancement using the new adaptive filtering algorithm named fast\naffine projection algorithm (FAPA). The simulation results demonstrate the good\nperformance of the FAPA in attenuating the noise."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1106.0846v1", 
    "title": "A Family of Adaptive Filter Algorithms in Noise Cancellation for Speech   Enhancement", 
    "arxiv-id": "1106.0846v1", 
    "author": "M. lotfizad", 
    "publish": "2011-06-04T18:25:34Z", 
    "summary": "In many application of noise cancellation, the changes in signal\ncharacteristics could be quite fast. This requires the utilization of adaptive\nalgorithms, which converge rapidly. Least Mean Squares (LMS) and Normalized\nLeast Mean Squares (NLMS) adaptive filters have been used in a wide range of\nsignal processing application because of its simplicity in computation and\nimplementation. The Recursive Least Squares (RLS) algorithm has established\nitself as the \"ultimate\" adaptive filtering algorithm in the sense that it is\nthe adaptive filter exhibiting the best convergence behavior. Unfortunately,\npractical implementations of the algorithm are often associated with high\ncomputational complexity and/or poor numerical properties. Recently adaptive\nfiltering was presented, have a nice tradeoff between complexity and the\nconvergence speed. This paper describes a new approach for noise cancellation\nin speech enhancement using the two new adaptive filtering algorithms named\nfast affine projection algorithm and fast Euclidean direction search algorithms\nfor attenuating noise in speech signals. The simulation results demonstrate the\ngood performance of the two new algorithms in attenuating the noise."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijma.2010.2203", 
    "link": "http://arxiv.org/pdf/1106.1199v1", 
    "title": "Open-loop multi-channel inversion of room impulse response", 
    "arxiv-id": "1106.1199v1", 
    "author": "Mark A. Hasegawa-Johnson", 
    "publish": "2011-06-06T21:03:31Z", 
    "summary": "This paper considers methods for audio display in a CAVE-type virtual reality\ntheater, a 3 m cube with displays covering all six rigid faces. Headphones are\npossible since the user's headgear continuously measures ear positions, but\nloudspeakers are preferable since they enhance the sense of total immersion.\nThe proposed solution consists of open-loop acoustic point control. The\ntransfer function, a matrix of room frequency responses from the loudspeakers\nto the ears of the user, is inverted using multi-channel inversion methods, to\ncreate exactly the desired sound field at the user's ears. The inverse transfer\nfunction is constructed from impulse responses simulated by the image source\nmethod. This technique is validated by measuring a 2x2 matrix transfer\nfunction, simulating a transfer function with the same geometry, and filtering\nthe measured transfer function through the inverse of the simulation. Since\naccuracy of the image source method decreases with time, inversion performance\nis improved by windowing the simulated response prior to inversion. Parameters\nof the simulation and inversion are adjusted to minimize residual reverberant\nenergy; the best-case dereverberation ratio is 10 dB."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij.2011.2203", 
    "link": "http://arxiv.org/pdf/1107.4185v1", 
    "title": "Estimation of Severity of Speech Disability through Speech Envelope", 
    "arxiv-id": "1107.4185v1", 
    "author": "H. C. Nagaraj", 
    "publish": "2011-07-21T07:17:34Z", 
    "summary": "In this paper, envelope detection of speech is discussed to distinguish the\npathological cases of speech disabled children. The speech signal samples of\nchildren of age between five to eight years are considered for the present\nstudy. These speech signals are digitized and are used to determine the speech\nenvelope. The envelope is subjected to ratio mean analysis to estimate the\ndisability. This analysis is conducted on ten speech signal samples which are\nrelated to both place of articulation and manner of articulation. Overall\nspeech disability of a pathological subject is estimated based on the results\nof above analysis."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij", 
    "link": "http://arxiv.org/pdf/1107.5492v1", 
    "title": "Application of Gammachirp Auditory Filter as a Continuous Wavelet   Analysis", 
    "arxiv-id": "1107.5492v1", 
    "author": "Kais Ouni", 
    "publish": "2011-07-27T14:42:52Z", 
    "summary": "This paper presents a new method on the use of the gammachirp auditory filter\nbased on a continuous wavelet analysis. The gammachirp auditory filter is\ndesigned to provide a spectrum reflecting the spectral properties of the\ncochlea, which is responsible for frequency analysis in the human auditory\nsystem. The impulse response of the theoretical gammachirp auditory filter that\nhas been developed by Irino and Patterson can be used as the kernel for wavelet\ntransform which approximates the frequency response of the cochlea. This study\nimplements the gammachirp auditory filter described by Irino as an analytical\nwavelet and examines its application to a different speech signals.\n  The obtained results will be compared with those obtained by two other\npredefined wavelet families that are Morlet and Mexican Hat. The results show\nthat the gammachirp wavelet family gives results that are comparable to ones\nobtained by Morlet and Mexican Hat wavelet family."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij", 
    "link": "http://arxiv.org/pdf/1112.1368v1", 
    "title": "Discovering novel computer music techniques by exploring the space of   short computer programs", 
    "arxiv-id": "1112.1368v1", 
    "author": "Ville-Matias Heikkil\u00e4", 
    "publish": "2011-12-06T18:30:13Z", 
    "summary": "Very short computer programs, sometimes consisting of as few as three\narithmetic operations in an infinite loop, can generate data that sounds like\nmusic when output as raw PCM audio. The space of such programs was recently\nexplored by dozens of individuals within various on-line communities. This\npaper discusses the programs resulting from this exploratory work and\nhighlights some rather unusual methods they use for synthesizing sound and\ngenerating musical structure."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij", 
    "link": "http://arxiv.org/pdf/1112.6178v1", 
    "title": "A general framework for online audio source separation", 
    "arxiv-id": "1112.6178v1", 
    "author": "Emmanuel Vincent", 
    "publish": "2011-12-28T20:29:13Z", 
    "summary": "We consider the problem of online audio source separation. Existing\nalgorithms adopt either a sliding block approach or a stochastic gradient\napproach, which is faster but less accurate. Also, they rely either on spatial\ncues or on spectral cues and cannot separate certain mixtures. In this paper,\nwe design a general online audio source separation framework that combines both\napproaches and both types of cues. The model parameters are estimated in the\nMaximum Likelihood (ML) sense using a Generalised Expectation Maximisation\n(GEM) algorithm with multiplicative updates. The separation performance is\nevaluated as a function of the block size and the step size and compared to\nthat of an offline algorithm."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij", 
    "link": "http://arxiv.org/pdf/1202.4212v2", 
    "title": "Harmony Explained: Progress Towards A Scientific Theory of Music", 
    "arxiv-id": "1202.4212v2", 
    "author": "Daniel Shawcross Wilkerson", 
    "publish": "2012-02-20T03:16:20Z", 
    "summary": "Most music theory books are like medieval medical textbooks: they contain\nunjustified superstition, non-reasoning, and funny symbols glorified by Latin\nphrases. How does music, in particular harmony, actually work, presented as a\nreal, scientific theory of music?\n  The core to our approach is to consider not only the Physical phenomena of\nnature but also the Computational phenomena of any machine that must make sense\nof sound, such as the human brain. In particular we derive the following three\nfundamental phenomena of music:\n  * the Major Scale,\n  * the Standard Chord Dictionary, and\n  * the difference in feeling between the Major and Minor Triads.\n  While the Major Scale has been independently derived before by others in a\nsimilar manner [Helmholtz1863, Birkhoff1933], I believe the derivation of the\nStandard Chord Dictionary as well as the difference in feeling between the\nMajor and Minor Triads to be original.\n  We show to be incomplete the theory of the heretofore agreed-upon authority\non this subject, 19th-century Physicist Hermann Helmholtz [Helmholtz1863]: he\nsays notes are in \"concord\" because the sound playing them together is \"less\nworse\" than that of some other notes. But note that, in this theory, more notes\ncan only penalize, some merely less than others, and so the most harmonious\nsound should be a single note by itself(!) and harmony would not exist as a\nphenomenon of music at all.\n  I intend this article to be satisfying to scientists as an original\ncontribution to science and art, yet I also intend it to be approachable by\nmusicians and other curious members of the general public who may have long\nwondered at the curious properties of tonal music and been frustrated by the\nlack of satisfying, readable exposition on the subject. Therefore I have\nwritten in a deliberately plain and conversational style, avoiding\nunnecessarily formal language."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij", 
    "link": "http://arxiv.org/pdf/1206.1450v1", 
    "title": "A comparative study of performance of fpga based mel filter bank & bark   filter bank", 
    "arxiv-id": "1206.1450v1", 
    "author": "Saikat Bose", 
    "publish": "2012-06-07T11:08:25Z", 
    "summary": "The sensitivity of human ear is dependent on frequency which is nonlinearly\nresolved across the audio spectrum .Now to improve the recognition performance\nin a similar non linear approach requires a front -end design, suggested by\nempirical evidences. A popular alternative to linear prediction based analysis\nis therefore filter bank analysis since this provides a much more\nstraightforward route to obtain the desired non-linear frequency resolution.\nMEL filter bank and BARK filter bank are two popular filter bank analysis\ntechniques. This paper presents FPGA based implementation of MEL filter bank\nand BARK filter bank with different bandwidths and different signal spectrum\nranges. The designs have been implemented using VHDL, simulated and verified\nusing Xilinx 11.1.For each filter bank, the basic building block is implemented\nin Spartan 3E. A comparative study among these two mentioned filter banks is\nalso done in this paper."
},{
    "category": "cs.SD", 
    "doi": "10.5120/7896-1235", 
    "link": "http://arxiv.org/pdf/1208.1418v1", 
    "title": "Analysis of a Modern Voice Morphing Approach using Gaussian Mixture   Models for Laryngectomees", 
    "arxiv-id": "1208.1418v1", 
    "author": "Jay Padhya", 
    "publish": "2012-08-07T13:33:40Z", 
    "summary": "This paper proposes a voice morphing system for people suffering from\nLaryngectomy, which is the surgical removal of all or part of the larynx or the\nvoice box, particularly performed in cases of laryngeal cancer. A primitive\nmethod of achieving voice morphing is by extracting the source's vocal\ncoefficients and then converting them into the target speaker's vocal\nparameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping\nthe coefficients from source to destination. However, the use of the\ntraditional/conventional GMM-based mapping approach results in the problem of\nover-smoothening of the converted voice. Thus, we hereby propose a unique\nmethod to perform efficient voice morphing and conversion based on GMM,which\novercomes the traditional-method effects of over-smoothening. It uses a\ntechnique of glottal waveform separation and prediction of excitations and\nhence the result shows that not only over-smoothening is eliminated but also\nthe transformed vocal tract parameters match with the target. Moreover, the\nsynthesized speech thus obtained is found to be of a sufficiently high quality.\nThus, voice morphing based on a unique GMM approach has been proposed and also\ncritically evaluated based on various subjective and objective evaluation\nparameters. Further, an application of voice morphing for Laryngectomees which\ndeploys this unique approach has been recommended by this paper."
},{
    "category": "cs.SD", 
    "doi": "10.5120/7896-1235", 
    "link": "http://arxiv.org/pdf/1210.0171v1", 
    "title": "A novel method for obtaining a better quality speech signal for cochlear   Implants using kalman with drnl and ssb technique", 
    "arxiv-id": "1210.0171v1", 
    "author": "K Padmaraju", 
    "publish": "2012-09-30T04:58:45Z", 
    "summary": "Cochlear implant devices are known to exist since a long time. The purpose of\nthe present work is to develop a speech algorithm for obtaining robust speech.\nIn this paper, the technique of cochlear implant is first introduced, followed\nby discussions of some of the existing techniques available for obtaining\nspeech. The next section introduces a new technique for obtaining robust\nspeech. The key feature of this technique lies in the use of the advantages of\nan integrated approach involving the use of an estimation technique such as a\nkalman filter with non linear filter bank strategy, using Dual Resonance Non\nLinear(DRNL) and Single Side Band(SSB) Encoding method. A comparative study of\nthe proposed method with the existing method indicates that the proposed method\nperforms well compared to the existing method."
},{
    "category": "cs.SD", 
    "doi": "10.5120/7896-1235", 
    "link": "http://arxiv.org/pdf/1210.3778v1", 
    "title": "Blind speech separation based on undecimated wavelet packet-perceptual   filterbanks and independent component analysis", 
    "arxiv-id": "1210.3778v1", 
    "author": "Zied Lachiri", 
    "publish": "2012-10-14T10:36:52Z", 
    "summary": "In this paper, we address the problem of blind separation of speech mixtures.\nWe propose a new blind speech separation system, which integrates a perceptual\nfilterbank and independent component analysis (ICA) and using kurtosis\ncriterion. The perceptual filterbank was designed by adjusting undecimated\nwavelet packet decomposition (UWPD) tree in order to accord to critical band\ncharacteristics of psycho-acoustic model. Our proposed technique consists on\ntransforming the observations signals into an adequate representation using\nUWPD and Kurtosis maximization criterion in a new preprocessing step in order\nto increase the non-Gaussianity which is a pre-requirement for ICA. Experiments\nwere carried out with the instantaneous mixture of two speech sources using two\nsensors. The obtained results show that the proposed method gives a\nconsiderable improvement when compared with FastICA and other techniques."
},{
    "category": "cs.SD", 
    "doi": "10.5120/7896-1235", 
    "link": "http://arxiv.org/pdf/1212.3119v1", 
    "title": "A nuclear-norm based convex formulation for informed source separation", 
    "arxiv-id": "1212.3119v1", 
    "author": "P. -A. Absil", 
    "publish": "2012-12-13T10:39:26Z", 
    "summary": "We study the problem of separating audio sources from a single linear\nmixture. The goal is to find a decomposition of the single channel spectrogram\ninto a sum of individual contributions associated to a certain number of\nsources. In this paper, we consider an informed source separation problem in\nwhich the input spectrogram is partly annotated. We propose a convex\nformulation that relies on a nuclear norm penalty to induce low rank for the\ncontributions. We show experimentally that solving this model with a simple\nsubgradient method outperforms a previously introduced nonnegative matrix\nfactorization (NMF) technique, both in terms of source separation quality and\ncomputation time."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2012.32032", 
    "link": "http://arxiv.org/pdf/1212.6903v1", 
    "title": "About Multichannel Speech Signal Extraction and Separation Techniques", 
    "arxiv-id": "1212.6903v1", 
    "author": "Hamid Amiri", 
    "publish": "2012-12-31T15:01:37Z", 
    "summary": "The extraction of a desired speech signal from a noisy environment has become\na challenging issue. In the recent years, the scientific community has\nparticularly focused on multichannel techniques which are dealt with in this\nreview. In fact, this study tries to classify these multichannel techniques\ninto three main ones: Beamforming, Independent Com-ponent Analysis (ICA) and\nTime Frequency (T-F) masking. This paper also highlights their advantages and\ndrawbacks. However these previously mentioned techniques could not afford\nsatisfactory results. This fact leads to the idea that a combination of those\ntechniques, which is depicted along this study, may probably provide more\nefficient results. In-deed, giving the fact that those approaches are still be\nconsidered as being not totally efficient, has led us to review these mentioned\nabove in the hope that further researches will provide this domain with\nsuitable innovations."
},{
    "category": "cs.SD", 
    "doi": "10.4236/jsip.2012.32032", 
    "link": "http://arxiv.org/pdf/1302.0136v1", 
    "title": "Maximum a posteriori estimation of piecewise arcs in tempo time-series", 
    "arxiv-id": "1302.0136v1", 
    "author": "Elaine Chew", 
    "publish": "2013-02-01T10:51:20Z", 
    "summary": "In musical performances with expressive tempo modulation, the tempo variation\ncan be modelled as a sequence of tempo arcs. Previous authors have used this\nidea to estimate series of piecewise arc segments from data. In this paper we\ndescribe a probabilistic model for a time-series process of this nature, and\nuse this to perform inference of single- and multi-level arc processes from\ndata. We describe an efficient Viterbi-like process for MAP inference of arcs.\nOur approach is score-agnostic, and together with efficient inference allows\nfor online analysis of performances including improvisations, and can predict\nimmediate future tempo trajectories."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2013.6637691", 
    "link": "http://arxiv.org/pdf/1302.3462v2", 
    "title": "Improved multiple birdsong tracking with distribution derivative method   and Markov renewal process clustering", 
    "arxiv-id": "1302.3462v2", 
    "author": "Mark D. Plumbley", 
    "publish": "2013-02-14T16:44:10Z", 
    "summary": "Segregating an audio mixture containing multiple simultaneous bird sounds is\na challenging task. However, birdsong often contains rapid pitch modulations,\nand these modulations carry information which may be of use in automatic\nrecognition. In this paper we demonstrate that an improved spectrogram\nrepresentation, based on the distribution derivative method, leads to improved\nperformance of a segregation algorithm which uses a Markov renewal process\nmodel to track vocalisation patterns consisting of singing and silences."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2013.6637691", 
    "link": "http://arxiv.org/pdf/1303.1023v2", 
    "title": "Consistent Iterative Hard Thresholding For Signal Declipping", 
    "arxiv-id": "1303.1023v2", 
    "author": "Christophe De Vleeschouwer", 
    "publish": "2013-03-05T13:15:16Z", 
    "summary": "Clipping or saturation in audio signals is a very common problem in signal\nprocessing, for which, in the severe case, there is still no satisfactory\nsolution. In such case, there is a tremendous loss of information, and\ntraditional methods fail to appropriately recover the signal. We propose a\nnovel approach for this signal restoration problem based on the framework of\nIterative Hard Thresholding. This approach, which enforces the consistency of\nthe reconstructed signal with the clipped observations, shows superior\nperformance in comparison to the state-of-the-art declipping algorithms. This\nis confirmed on synthetic and on actual high-dimensional audio data processing,\nboth on SNR and on subjective user listening evaluations."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2013.6637691", 
    "link": "http://arxiv.org/pdf/1311.0819v1", 
    "title": "Phoneme discrimination using neurons with symmetric nonlinear response   over a spectral range", 
    "arxiv-id": "1311.0819v1", 
    "author": "Martin Klimo", 
    "publish": "2013-11-04T19:22:39Z", 
    "summary": "We consider the ability of a very simple feed-forward neural network to\ndiscriminate phonemes based on just relative power spectrum. The network\nconsists of two neurons with symmetric nonlinear response over a spectral\nrange. The output of the neurons is subsequently fed to a comparator. We show\nthat often this is enough to achieve complete separation of data. We compare\nthe performance of found discriminants with that of more general neurons. Our\nconclusion is that not much is gained in passing to real-valued weights. More\nlikely higher number of neurons and preprocessing of input will yield better\ndiscrimination results. The networks considered are directly amenable to\nhardware (neuromorphic) designs. Other advantages include interpretability,\nguarantees of performance on unseen data and low Kolmogorov complexity."
},{
    "category": "cs.SD", 
    "doi": "10.7323/ijaet/v6_iss5", 
    "link": "http://arxiv.org/pdf/1311.0842v1", 
    "title": "An Intuitive Design Approach For Implementing Real Time Audio Effects", 
    "arxiv-id": "1311.0842v1", 
    "author": "Om Ranjan", 
    "publish": "2013-11-04T20:43:47Z", 
    "summary": "Audio effect implementation on random musical signal is a basic application\nof digital signal processors. In this paper, the compatibility features of\nMATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop\nSimulink models which when emulated on TMS320C6713 DSK generate real time audio\neffects. Each design has been done by two different asynchronous scheduling\ntechniques: (i) Idle task Scheduling and (ii) DSP/BIOS task Scheduling. A basic\nCOCOMO analysis has been done for the generated code to justify the industrial\nviability of this design approach.\n  KEYWORDS: Musical signal processing, Real time Audio effects, Echo, Stress\nGeneration, Reverberation, Reverberated Chorus, Real Time Scheduling."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2014.2317989", 
    "link": "http://arxiv.org/pdf/1311.1047v2", 
    "title": "A Geometric Approach to Sound Source Localization from Time-Delay   Estimates", 
    "arxiv-id": "1311.1047v2", 
    "author": "Radu Horaud", 
    "publish": "2013-11-05T13:38:52Z", 
    "summary": "This paper addresses the problem of sound-source localization from time-delay\nestimates using arbitrarily-shaped non-coplanar microphone arrays. A novel\ngeometric formulation is proposed, together with a thorough algebraic analysis\nand a global optimization solver. The proposed model is thoroughly described\nand evaluated. The geometric analysis, stemming from the direct acoustic\npropagation model, leads to necessary and sufficient conditions for a set of\ntime delays to correspond to a unique position in the source space. Such sets\nof time delays are referred to as feasible sets. We formally prove that every\nfeasible set corresponds to exactly one position in the source space, whose\nvalue can be recovered using a closed-form localization mapping. Therefore we\nseek for the optimal feasible set of time delays given, as input, the received\nmicrophone signals. This time delay estimation problem is naturally cast into a\nprogramming task, constrained by the feasibility conditions derived from the\ngeometric analysis. A global branch-and-bound optimization technique is\nproposed to solve the problem at hand, hence estimating the best set of\nfeasible time delays and, subsequently, localizing the sound source. Extensive\nexperiments with both simulated and real data are reported; we compare our\nmethodology to four state-of-the-art techniques. This comparison clearly shows\nthat the proposed method combined with the branch-and-bound algorithm\noutperforms existing methods. These in-depth geometric understanding, practical\nalgorithms, and encouraging results, open several opportunities for future\nwork."
},{
    "category": "cs.SD", 
    "doi": "10.1111/2041-210X.12223", 
    "link": "http://arxiv.org/pdf/1311.4764v1", 
    "title": "Large-scale analysis of frequency modulation in birdsong databases", 
    "arxiv-id": "1311.4764v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2013-11-19T15:02:55Z", 
    "summary": "Birdsong often contains large amounts of rapid frequency modulation (FM). It\nis believed that the use or otherwise of FM is adaptive to the acoustic\nenvironment, and also that there are specific social uses of FM such as trills\nin aggressive territorial encounters. Yet temporal fine detail of FM is often\nabsent or obscured in standard audio signal analysis methods such as Fourier\nanalysis or linear prediction. Hence it is important to consider high\nresolution signal processing techniques for analysis of FM in bird\nvocalisations. If such methods can be applied at big data scales, this offers a\nfurther advantage as large datasets become available.\n  We introduce methods from the signal processing literature which can go\nbeyond spectrogram representations to analyse the fine modulations present in a\nsignal at very short timescales. Focusing primarily on the genus Phylloscopus,\nwe investigate which of a set of four analysis methods most strongly captures\nthe species signal encoded in birdsong. In order to find tools useful in\npractical analysis of large databases, we also study the computational time\ntaken by the methods, and their robustness to additive noise and MP3\ncompression.\n  We find three methods which can robustly represent species-correlated FM\nattributes, and that the simplest method tested also appears to perform the\nbest. We find that features representing the extremes of FM encode species\nidentity supplementary to that captured in frequency features, whereas\nbandwidth features do not encode additional information.\n  Large-scale FM analysis can efficiently extract information useful for\nbioacoustic studies, in addition to measures more commonly used to characterise\nvocalisations."
},{
    "category": "cs.SD", 
    "doi": "10.1111/2041-210X.12223", 
    "link": "http://arxiv.org/pdf/1311.5924v1", 
    "title": "Objets Sonores: Une Repr\u00e9sentation Bio-Inspir\u00e9e Hi\u00e9rarchique   Parcimonieuse \u00c0 Tr\u00e8s Grandes Dimensions Utilisable En Reconnaissance;   Auditory Objects: Bio-Inspired Hierarchical Sparse High Dimensional   Representation for Recognition", 
    "arxiv-id": "1311.5924v1", 
    "author": "Jean Rouat", 
    "publish": "2013-11-22T22:54:45Z", 
    "summary": "L'accent est plac\\'e dans cet article sur la structure hi\\'erarchique,\nl'aspect parcimonieux de la repr\\'esentation de l'information sonore, la tr\\`es\ngrande dimension des caract\\'eristiques ainsi que sur l'ind\\'ependance des\ncaract\\'eristiques permettant de d\\'efinir les composantes des objets sonores.\nLes notions d'objet sonore et de repr\\'esentation neuronale sont d'abord\nintroduites, puis illustr\\'ees avec une application en analyse de signaux\nsonores vari\\'es: parole, musique et environnements naturels ext\\'erieurs.\nFinalement, un nouveau syst\\`eme de reconnaissance automatique de parole est\npropos\\'e. Celui-ci est compar\\'e \\`a un syst\\`eme statistique conventionnel.\nIl montre tr\\`es clairement que l'analyse par objets sonores introduit une\ngrande polyvalence et robustesse en reconnaissance de parole. Cette\nint\\'egration des connaissances en neurosciences et traitement des signaux\nacoustiques ouvre de nouvelles perspectives dans le domaine de la\nreconnaissance de signaux acoustiques.\n  The emphasis is put on the hierarchical structure, independence and\nsparseness aspects of auditory signal representations in high-dimensional\nspaces, so as to define the components of auditory objects. The concept of an\nauditory object and its neural representation is introduced. An illustrative\napplication then follows, consisting in the analysis of various auditory\nsignals: speech, music and natural outdoor environments. A new automatic speech\nrecognition (ASR) system is then proposed and compared to a conventional\nstatistical system. The proposed system clearly shows that an object-based\nanalysis introduces a great flexibility and robustness for the task of speech\nrecognition. The integration of knowledge from neuroscience and acoustic signal\nprocessing brings new ways of thinking to the field of classification of\nacoustic signals."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1402.2683v3", 
    "title": "Acoustic Space Learning for Sound Source Separation and Localization on   Binaural Manifolds", 
    "arxiv-id": "1402.2683v3", 
    "author": "Radu Horaud", 
    "publish": "2014-02-11T22:01:19Z", 
    "summary": "In this paper we address the problems of modeling the acoustic space\ngenerated by a full-spectrum sound source and of using the learned model for\nthe localization and separation of multiple sources that simultaneously emit\nsparse-spectrum sounds. We lay theoretical and methodological grounds in order\nto introduce the binaural manifold paradigm. We perform an in-depth study of\nthe latent low-dimensional structure of the high-dimensional interaural\nspectral data, based on a corpus recorded with a human-like audiomotor robot\nhead. A non-linear dimensionality reduction technique is used to show that\nthese data lie on a two-dimensional (2D) smooth manifold parameterized by the\nmotor states of the listener, or equivalently, the sound source directions. We\npropose a probabilistic piecewise affine mapping model (PPAM) specifically\ndesigned to deal with high-dimensional data exhibiting an intrinsic piecewise\nlinear structure. We derive a closed-form expectation-maximization (EM)\nprocedure for estimating the model parameters, followed by Bayes inversion for\nobtaining the full posterior density function of a sound source direction. We\nextend this solution to deal with missing data and redundancy in real world\nspectrograms, and hence for 2D localization of natural sound sources such as\nspeech. We further generalize the model to the challenging case of multiple\nsound sources and we propose a variational EM framework. The associated\nalgorithm, referred to as variational EM for source separation and localization\n(VESSL) yields a Bayesian estimation of the 2D locations and time-frequency\nmasks of all the sources. Comparisons of the proposed approach with several\nexisting methods reveal that the combination of acoustic-space learning with\nBayesian inference enables our method to outperform state-of-the-art methods."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1402.4160v1", 
    "title": "Maximizing the Signal-to-Alias Ratio in Non-Uniform Filter Banks for   Acoustic Echo Cancellation", 
    "arxiv-id": "1402.4160v1", 
    "author": "D. J. Shpak", 
    "publish": "2014-02-14T03:11:53Z", 
    "summary": "A new method for designing non-uniform filter-banks for acoustic echo\ncancellation is proposed. In the method, the analysis prototype filter design\nis framed as a convex optimization problem that maximizes the signal-to-alias\nratio (SAR) in the analysis banks. Since each sub-band has a different\nbandwidth, the contribution to the overall SAR from each analysis bank is taken\ninto account during optimization. To increase the degrees of freedom during\noptimization, no constraints are imposed on the phase or group delay of the\nfilters; at the same time, low delay is achieved by ensuring that the resulting\nfilters are minimum phase. Experimental results show that the filter bank\ndesigned using the proposed method results in a sub-band adaptive filter with a\nmuch better echo return loss enhancement (ERLE) when compared with existing\ndesign methods."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1404.6881v1", 
    "title": "Improving Blind Source Separation Performance By Adaptive Array   Geometries For Humanoid Robots", 
    "arxiv-id": "1404.6881v1", 
    "author": "Walter Kellermann", 
    "publish": "2014-04-28T07:17:18Z", 
    "summary": "In this paper, the concept of an adaptation algorithm is proposed, which can\nbe used to blindly adapt the microphone array geometry of a humanoid robot such\nthat the performance of the underlying signal separation algorithm is improved.\nAs a decisive feature, an online performance measure for blind source\nseparation is introduced which allows a robust and reliable estimation of the\ninstantaneous separation performance based on currently observable data.\nExperimental results from a simulated environment confirm the efficacy of the\nconcept."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1409.0117v1", 
    "title": "Computerized Multi Microphone Test System", 
    "arxiv-id": "1409.0117v1", 
    "author": "A. M. Dorman", 
    "publish": "2014-08-30T14:51:33Z", 
    "summary": "An acoustic testing approach based on the concept of a microphone sensor\nsurrounding the product under test is proposed. Microphone signals are\nprocessed simultaneously by a test system computer, according to the objective\nof the test. The spatial and frequency domain selectivity features of this\nmethod are examined. Sound-spatial visualization algorithm is observed. A test\nsystem design based on the concept of a microphone surrounding the tested\nproduct has the potential to improve distortion measurement accuracy."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1409.3206v1", 
    "title": "DSP.Ear: Leveraging Co-Processor Support for Continuous Audio Sensing on   Smartphones", 
    "arxiv-id": "1409.3206v1", 
    "author": "Cecilia Mascolo", 
    "publish": "2014-09-10T19:30:58Z", 
    "summary": "The rapidly growing adoption of sensor-enabled smartphones has greatly fueled\nthe proliferation of applications that use phone sensors to monitor user\nbehavior. A central sensor among these is the microphone which enables, for\ninstance, the detection of valence in speech, or the identification of\nspeakers. Deploying multiple of these applications on a mobile device to\ncontinuously monitor the audio environment allows for the acquisition of a\ndiverse range of sound-related contextual inferences. However, the cumulative\nprocessing burden critically impacts the phone battery.\n  To address this problem, we propose DSP.Ear - an integrated sensing system\nthat takes advantage of the latest low-power DSP co-processor technology in\ncommodity mobile devices to enable the continuous and simultaneous operation of\nmultiple established algorithms that perform complex audio inferences. The\nsystem extracts emotions from voice, estimates the number of people in a room,\nidentifies the speakers, and detects commonly found ambient sounds, while\ncritically incurring little overhead to the device battery. This is achieved\nthrough a series of pipeline optimizations that allow the computation to remain\nlargely on the DSP. Through detailed evaluation of our prototype implementation\nwe show that, by exploiting a smartphone's co-processor, DSP.Ear achieves a 3\nto 7 times increase in the battery lifetime compared to a solution that uses\nonly the phone's main processor. In addition, DSP.Ear is 2 to 3 times more\npower efficient than a naive DSP solution without optimizations. We further\nanalyze a large-scale dataset from 1320 Android users to show that in about\n80-90% of the daily usage instances DSP.Ear is able to sustain a full day of\noperation (even in the presence of other smartphone workloads) with a single\nbattery charge."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1409.6554v1", 
    "title": "A Single-Processor Approach to Speech Processing Pipeline of Bilateral   Cochlear Implants", 
    "arxiv-id": "1409.6554v1", 
    "author": "Taher Shahbazi Mirzahasanloo", 
    "publish": "2014-09-23T14:22:16Z", 
    "summary": "This dissertation covers a single-processor approach to the speech processing\npipeline of bilateral Cochlear Implants (CIs). The use of only a single\nprocessor to provide binaural stimulation signals overcomes the synchronization\nproblem, which is an existing challenging problem in the deployment of\nbilateral CI devices. The developed single-processor speech processing pipeline\nprovides CI users with a sense of directionality. Its non-synchronization\nfeature as well as low computational and memory requirements make it a suitable\nsolution for actual deployment. A speech enhancement framework is developed\nthat incorporates different non-Euclidean speech distortion criteria and\ndifferent noise environments. This framework not only allows the design of\nenvironment-optimized parameters but also enables a user-specific solution\nwhere the anthropometric measurements of an individual user are incorporated\ninto the training process to obtain individualized bilateral parameters. The\ndeveloped techniques are primarily meant for bilateral CIs, however, they are\ngeneral purpose in the sense that they are also applicable to binaural hearing\naids, bimodal devices having hearing aid in one ear and cochlear implant in the\nother ear as well as dual-channel speech enhancement applications. Extensive\nexperiments have shown the effectiveness of the developed solution in six\ncommonly encountered noise environments compared to a similar one-channel\npipeline when using two separate processors or when using independent\nsequential processing."
},{
    "category": "cs.SD", 
    "doi": "10.1142/S0129065714400036", 
    "link": "http://arxiv.org/pdf/1501.04981v1", 
    "title": "Listening to features", 
    "arxiv-id": "1501.04981v1", 
    "author": "Laurent Daudet", 
    "publish": "2015-01-19T19:41:35Z", 
    "summary": "This work explores nonparametric methods which aim at synthesizing audio from\nlow-dimensionnal acoustic features typically used in MIR frameworks. Several\nissues prevent this task to be straightforwardly achieved. Such features are\ndesigned for analysis and not for synthesis, thus favoring high-level\ndescription over easily inverted acoustic representation. Whereas some previous\nstudies already considered the problem of synthesizing audio from features such\nas Mel-Frequency Cepstral Coefficients, they mainly relied on the explicit\nformula used to compute those features in order to inverse them. Here, we\ninstead adopt a simple blind approach, where arbitrary sets of features can be\nused during synthesis and where reconstruction is exemplar-based. After testing\nthe approach on a speech synthesis from well known features problem, we apply\nit to the more complex task of inverting songs from the Million Song Dataset.\nWhat makes this task harder is twofold. First, that features are irregularly\nspaced in the temporal domain according to an onset-based segmentation. Second\nthe exact method used to compute these features is unknown, although the\nfeatures for new audio can be computed using their API as a black-box. In this\npaper, we detail these difficulties and present a framework to nonetheless\nattempting such synthesis by concatenating audio samples from a training\ndataset, whose features have been computed beforehand. Samples are selected at\nthe segment level, in the feature space with a simple nearest neighbor search.\nAdditionnal constraints can then be defined to enhance the synthesis\npertinence. Preliminary experiments are presented using RWC and GTZAN audio\ndatasets to synthesize tracks from the Million Song Dataset."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1104.3544v2", 
    "title": "An automatic volume control for preserving intelligibility", 
    "arxiv-id": "1104.3544v2", 
    "author": "Franklin Felber", 
    "publish": "2011-03-29T22:29:39Z", 
    "summary": "A new method has been developed to adjust volume automatically on all audio\ndevices equipped with at least one microphone, including mobile phones,\npersonal media players, headsets, and car radios, that might be used in noisy\nenvironments, such as crowds, cars, and outdoors. The method uses a patented\nset of algorithms, implemented on the chips in such devices, to preserve\nconstant intelligibility of speech in noisy environments, rather than constant\nsignal-to-noise ratio. The algorithms analyze the noise background in real time\nand compensate only for fluctuating noise in the frequency domain and the time\ndomain that interferes with intelligibility of speech. Advantages of this\nmethod of controlling volume include: Controlling volume without sacrificing\nclarity; adjusting only for persistent speech-interference noise; smoothing\nvolume fluctuations; and eliminating static-like bursts caused by noise spikes.\nPractical human-factors approaches to implementing these algorithms in mobile\nphones are discussed."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1109.5876v1", 
    "title": "R\u00e9nyi Information Measures for Spectral Change Detection", 
    "arxiv-id": "1109.5876v1", 
    "author": "Xavier Rodet", 
    "publish": "2011-09-27T13:09:11Z", 
    "summary": "Change detection within an audio stream is an important task in several\ndomains, such as classification and segmentation of a sound or of a music\npiece, as well as indexing of broadcast news or surveillance applications. In\nthis paper we propose two novel methods for spectral change detection without\nany assumption about the input sound: they are both based on the evaluation of\ninformation measures applied to a time- frequency representation of the signal,\nand in particular to the spectrogram. The class of measures we consider, the\nR\\'enyi entropies, are obtained by extending the Shannon entropy definition: a\nbiasing of the spectrogram coefficients is realized through the dependence of\nsuch measures on a parameter, which allows refined results compared to those\nobtained with standard divergences. These methods provide a low computational\ncost and are well-suited as a support for higher level analysis, segmentation\nand classification algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1109.6313v1", 
    "title": "A Reduced Multiple Gabor Frame for Local Time Adaptation of the   Spectrogram", 
    "arxiv-id": "1109.6313v1", 
    "author": "X. Rodet", 
    "publish": "2011-09-27T10:36:01Z", 
    "summary": "In this paper we propose a method for automatic local time adap- tation of\nthe spectrogram of an audio signal, based on its decomposition within a Gabor\nmulti-frame. The sparsity of the analyses within each individual frame is\nevaluated through the R\\'enyi entropies measures. According to the sparsity of\nthe decompositions, an optimal resolution and a reduced multi-frame are\ndetermined, defining an adapted spectrogram with variable resolution and hop\nsize. The composition of such a reduced multi-frame allows an immediate\ndefinition of a dual frame: re-synthesis techniques for this adapted analysis\nare easily derived by the traditional phase vocoder scheme."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1109.6314v1", 
    "title": "An Entropy Based Method for Local Time-Adaptation of the Spectrogram", 
    "arxiv-id": "1109.6314v1", 
    "author": "X. Rodet", 
    "publish": "2011-09-27T08:32:50Z", 
    "summary": "We propose a method for automatic local time-adaptation of the spectrogram of\naudio signals: it is based on the decomposition of a signal within a Gabor\nmulti-frame through the STFT operator. The sparsity of the analysis in every\nindividual frame of the multi-frame is evaluated through the R\\'enyi entropy\nmeasures: the best local resolution is determined minimizing the entropy\nvalues. The overall spectrogram of the signal we obtain thus provides local\noptimal resolution adaptively evolving over time. We give examples of the\nperformance of our algorithm with an instrumental sound and a synthetic one,\nshowing the improvement in spectrogram displaying obtained with an automatic\nadaptation of the resolution. The analysis operator is invertible, thus leading\nto a perfect reconstruction of the original signal through the analysis\ncoefficients."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1109.6651v1", 
    "title": "Sound Analysis and Synthesis Adaptive in Time and Two Frequency Bands", 
    "arxiv-id": "1109.6651v1", 
    "author": "Axel R\u00f6bel", 
    "publish": "2011-09-29T14:06:15Z", 
    "summary": "We present an algorithm for sound analysis and resynthesis with local\nautomatic adaptation of time-frequency resolution. There exists several\nalgorithms allowing to adapt the analysis window depending on its time or\nfrequency location; in what follows we propose a method which select the\noptimal resolution depending on both time and frequency. We consider an\napproach that we denote as analysis-weighting, from the point of view of Gabor\nframe theory. We analyze in particular the case of different adaptive\ntime-varying resolutions within two complementary frequency bands; this is a\ntypical case where perfect signal reconstruction cannot in general be achieved\nwith fast algorithms, causing a certain error to be minimized. We provide\nexamples of adaptive analyses of a music sound, and outline several\npossibilities that this work opens."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1207.5104v1", 
    "title": "Analysis of speech under stress using Linear techniques and Non-Linear   techniques for emotion recognition system", 
    "arxiv-id": "1207.5104v1", 
    "author": "Prof. B. V. Pathak", 
    "publish": "2012-07-21T05:25:27Z", 
    "summary": "Analysis of speech for recognition of stress is important for identification\nof emotional state of person. This can be done using 'Linear Techniques', which\nhas different parameters like pitch, vocal tract spectrum, formant frequencies,\nDuration, MFCC etc. which are used for extraction of features from speech.\nTEO-CB-Auto-Env is the method which is non-linear method of features\nextraction. Analysis is done using TU-Berlin (Technical University of Berlin)\nGerman database. Here emotion recognition is done for different emotions like\nneutral, happy, disgust, sad, boredom and anger. Emotion recognition is used in\nlie detector, database access systems, and in military for recognition of\nsoldiers' emotion identification during the war."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SARNOF.2011.5876448", 
    "link": "http://arxiv.org/pdf/1207.5827v1", 
    "title": "Algorithm to suppress scanner noise in recorded speech during functional   magnetic resonance imaging", 
    "arxiv-id": "1207.5827v1", 
    "author": "Satrajit S. Ghosh", 
    "publish": "2012-07-24T21:08:35Z", 
    "summary": "The high-intensity, repetitive noise associated with functional magnetic\nresonance imaging hinders on-line monitoring of subjects' speech and/or\nrecording speech signals suitable for off-line analysis. The proposed algorithm\nenhances the speech signal by suppressing the scanner noise in the signal\nrecorded by a single-channel microphone. Significant increases in\nsignal-to-noise ratio are achieved using an adaptive filter that combines time\nand frequency domain elements. In addition to providing a recording suitable\nfor speech analysis, such a real-time system provides an alternative means (to,\ne.g., the \"panic ball\") for communication between the patient and the operator\nduring image acquisition."
},{
    "category": "cs.SD", 
    "doi": "10.5120/9646-4381", 
    "link": "http://arxiv.org/pdf/1301.0265v1", 
    "title": "Usable Speech Assignment for Speaker Identification under Co-Channel   Situation", 
    "arxiv-id": "1301.0265v1", 
    "author": "Ezzedine Ben Braiek", 
    "publish": "2013-01-02T17:06:58Z", 
    "summary": "Usable speech criteria are proposed to extract minimally corrupted speech for\nspeaker identification (SID) in co-channel speech. In co-channel speech, either\nspeaker can randomly appear as the stronger speaker or the weaker one at a\ntime. Hence, the extracted usable segments are separated in time and need to be\norganized into speaker streams for SID. In this paper, we focus to organize\nextracted usable speech segment into a single stream for the same speaker by\nspeaker assignment system. For this, we develop model-based speaker assignment\nmethod based on posterior probability and exhaustive search algorithm.\nEvaluation of this method is performed on TIMIT database. The system is\nevaluated on co-channel speech and results show a significant improvement."
},{
    "category": "cs.SD", 
    "doi": "10.5120/9646-4381", 
    "link": "http://arxiv.org/pdf/1301.0278v1", 
    "title": "Evaluation of a Multi-Resolution Dyadic Wavelet Transform Method for   usable Speech Detection", 
    "arxiv-id": "1301.0278v1", 
    "author": "Ezzedine Ben Braiek", 
    "publish": "2013-01-02T17:50:00Z", 
    "summary": "Many applications of speech communication and speaker identification suffer\nfrom the problem of co-channel speech. This paper deals with a multi-resolution\ndyadic wavelet transform method for usable segments of co-channel speech\ndetection that could be processed by a speaker identification system.\nEvaluation of this method is performed on TIMIT database referring to the\nTarget to Interferer Ratio measure. Co-channel speech is constructed by mixing\nall possible gender speakers. Results do not show much difference for different\nmixtures. For the overall mixtures 95.76% of usable speech is correctly\ndetected with false alarms of 29.65%."
},{
    "category": "cs.SD", 
    "doi": "10.5120/9646-4381", 
    "link": "http://arxiv.org/pdf/1304.0969v1", 
    "title": "Toward Evolution Strategies Application in Automatic Polyphonic Music   Transcription using Electronic Synthesis", 
    "arxiv-id": "1304.0969v1", 
    "author": "Herve Kabamba Mbikayi", 
    "publish": "2013-04-03T14:54:10Z", 
    "summary": "We present in this paper a new approach for polyphonic music transcription\nusing evolution strategies (ES). Automatic music transcription is a complex\nprocess that still remains an open challenge. Using an audio signal to be\ntranscribed as target for our ES, information needed to generate a MIDI file\ncan be extracted from this latter one. Many techniques presented in the\nliterature at present exist and a few of them have applied evolutionary\nalgorithms to address this problem in the context of considering it as a search\nspace problem. However, ES have never been applied until now. The experiments\nshowed that by using these machines learning tools, some shortcomings presented\nby other evolutionary algorithms based approaches for transcription can be\nsolved. They include the computation cost and the time for convergence. As\nevolution strategies use self-adapting parameters, we show in this paper that\nby correctly tuning the value of its strategy parameter that controls the\nstandard deviation, a fast convergence can be triggered toward the optima,\nwhich from the results performs the transcription of the music with good\naccuracy and in a short time. In the same context, the computation task is\ntackled using parallelization techniques thus reducing the computation time and\nthe transcription time in the overall."
},{
    "category": "cs.SD", 
    "doi": "10.5120/9646-4381", 
    "link": "http://arxiv.org/pdf/1305.1141v1", 
    "title": "Acoustic Echo Cancellation Postfilter Design Issues For Speech   Recognition System", 
    "arxiv-id": "1305.1141v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-06T10:33:06Z", 
    "summary": "In this paper a generalized postfilter algorithm design issues are presented.\nThis postfilter is used to jointly suppress late reverberation, residual echo,\nand background noise. When residual echo and noise are suppressed, the best\nresult obtains by suppressing both interferences together after the Acoustic\necho cancellation (AEC). The main advantage of this approach is that the\nresidual echo and noise suppression does not suffer from the existence of a\nstrong acoustic echo component. Furthermore, the Acoustic echo cancellation\n(AEC) does not suffer from the time-varying noise suppression. A disadvantage\nis that the input signal of the Acoustic echo cancellation (AEC) has a low\nsignal-to-noise ratio (SNR). To overcome this problem, algorithms have been\nproposed where, apart from the joint suppression, a noise-reduced signal is\nused to adapt the echo canceller."
},{
    "category": "cs.SD", 
    "doi": "10.1080/09298215.2014.894533", 
    "link": "http://arxiv.org/pdf/1306.1461v2", 
    "title": "The GTZAN dataset: Its contents, its faults, their effects on   evaluation, and its future use", 
    "arxiv-id": "1306.1461v2", 
    "author": "Bob L. Sturm", 
    "publish": "2013-06-06T16:30:44Z", 
    "summary": "The GTZAN dataset appears in at least 100 published works, and is the\nmost-used public dataset for evaluation in machine listening research for music\ngenre recognition (MGR). Our recent work, however, shows GTZAN has several\nfaults (repetitions, mislabelings, and distortions), which challenge the\ninterpretability of any result derived using it. In this article, we disprove\nthe claims that all MGR systems are affected in the same ways by these faults,\nand that the performances of MGR systems in GTZAN are still meaningfully\ncomparable since they all face the same faults. We identify and analyze the\ncontents of GTZAN, and provide a catalog of its faults. We review how GTZAN has\nbeen used in MGR research, and find few indications that its faults have been\nknown and considered. Finally, we rigorously study the effects of its faults on\nevaluating five different MGR systems. The lesson is not to banish GTZAN, but\nto use it with consideration of its contents."
},{
    "category": "cs.SD", 
    "doi": "10.1080/17459737.2015.1033024", 
    "link": "http://arxiv.org/pdf/1306.6458v5", 
    "title": "Harmony Perception by Periodicity Detection", 
    "arxiv-id": "1306.6458v5", 
    "author": "Frieder Stolzenburg", 
    "publish": "2013-06-27T10:27:22Z", 
    "summary": "The perception of consonance/dissonance of musical harmonies is strongly\ncorrelated with their periodicity. This is shown in this article by\nconsistently applying recent results from psychophysics and neuroacoustics,\nnamely that the just noticeable difference between pitches for humans is about\n1% for the musically important low frequency range and that periodicities of\ncomplex chords can be detected in the human brain. Based thereon, the concepts\nof relative and logarithmic periodicity with smoothing are introduced as\npowerful measures of harmoniousness. The presented results correlate\nsignificantly with empirical investigations on the perception of chords. Even\nfor scales, plausible results are obtained. For example, all classical church\nmodes appear in the front ranks of all theoretically possible seven-tone\nscales."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2014.2303135", 
    "link": "http://arxiv.org/pdf/1312.2795v2", 
    "title": "Reverberant Audio Source Separation via Sparse and Low-Rank Modeling", 
    "arxiv-id": "1312.2795v2", 
    "author": "Pierre Vandergheynst", 
    "publish": "2013-12-10T13:45:33Z", 
    "summary": "The performance of audio source separation from underdetermined convolutive\nmixture assuming known mixing filters can be significantly improved by using an\nanalysis sparse prior optimized by a reweighting l1 scheme and a wideband\ndatafidelity term, as demonstrated by a recent article. In this letter, we show\nthat the performance can be improved even more significantly by exploiting a\nlow-rank prior on the source spectrograms.We present a new algorithm to\nestimate the sources based on i) an analysis sparse prior, ii) a reweighting\nscheme so as to increase the sparsity, iii) a wideband data-fidelity term in a\nconstrained form, and iv) a low-rank constraint on the source spectrograms.\nEvaluation on reverberant music mixtures shows that the resulting algorithm\nimproves state-of-the-art methods by more than 2 dB of signal-to-distortion\nratio."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2014.2303135", 
    "link": "http://arxiv.org/pdf/1312.4014v1", 
    "title": "A Simple Method to Produce Algorithmic MIDI Music based on Randomness,   Simple Probabilities and Multi-Threading", 
    "arxiv-id": "1312.4014v1", 
    "author": "Yannis Tzitzikas", 
    "publish": "2013-12-14T07:55:23Z", 
    "summary": "This paper introduces a simple method for producing multichannel MIDI music\nthat is based on randomness and simple probabilities. One distinctive feature\nof the method is that it produces and sends in parallel to the sound card more\nthan one unsynchronized channels by exploiting the multi-threading capabilities\nof general purpose programming languages. As consequence the derived sound\noffers a quite ``full\" and ``unpredictable\" acoustic experience to the\nlistener. Subsequently the paper reports the results of an evaluation with\nusers. The results were very surprising: the majority of users responded that\nthey could tolerate this music in various occasions."
},{
    "category": "cs.SD", 
    "doi": "10.14569/IJACSA.2013.040721", 
    "link": "http://arxiv.org/pdf/1312.4127v1", 
    "title": "A Hybrid Approach for Co-Channel Speech Segregation based on CASA, HMM   Multipitch Tracking, and Medium Frame Harmonic Model", 
    "arxiv-id": "1312.4127v1", 
    "author": "Aliaa A. A. Youssif", 
    "publish": "2013-12-15T09:40:37Z", 
    "summary": "This paper proposes a hybrid approach for co-channel speech segregation. HMM\n(hidden Markov model) is used to track the pitches of 2 talkers. The resulting\npitch tracks are then enriched with the prominent pitch. The enriched tracks\nare correctly grouped using pitch continuity. Medium frame harmonics are used\nto extract the second pitch for frames with only one pitch deduced using the\nprevious steps. Finally, the pitch tracks are input to CASA (computational\nauditory scene analysis) to segregate the mixed speech. The center frequency\nrange of the gamma tone filter banks is maximized to reduce the overlap between\nthe channels filtered for better segregation. Experiments were conducted using\nthis hybrid approach on the speech separation challenge database and compared\nto the single (non-hybrid) approaches, i.e. signal processing and CASA. Results\nshow that using the hybrid approach outperforms the single approaches."
},{
    "category": "cs.SD", 
    "doi": "10.14569/IJACSA.2013.040721", 
    "link": "http://arxiv.org/pdf/1403.1501v1", 
    "title": "Sparse DOA Estimation of Wideband Sound Sources Using Circular Harmonics", 
    "arxiv-id": "1403.1501v1", 
    "author": "Martin Kleinsteuber", 
    "publish": "2014-03-06T17:29:39Z", 
    "summary": "Sparse signal models are in the focus of recent developments in narrowband\nDOA estimation. Applying these methods to localizing audio sources, however, is\nchallenging due to the wideband nature of the signals. The common approach of\nprocessing all frequency bands separately and fusing the results is costly and\ncan introduce errors in the solution. We show how these problems can be\novercome by decomposing the wavefield of a circular microphone array and using\ncircular harmonic coefficients instead of time-frequency data for sparse DOA\nestimation. As a result, we present the super-resolution localization method\nWASCHL (Wideband Audio Sparse Circular Harmonics Localizer) that is inherently\nfrequency-coherent and highly efficient from a computational point of view."
},{
    "category": "cs.SD", 
    "doi": "10.14569/IJACSA.2013.040721", 
    "link": "http://arxiv.org/pdf/1403.2180v2", 
    "title": "Optimal Window and Lattice in Gabor Transform Application to Audio   Analysis", 
    "arxiv-id": "1403.2180v2", 
    "author": "Darian M. Onchis", 
    "publish": "2014-03-10T09:18:54Z", 
    "summary": "This article deals with the use of optimal lattice and optimal window in\nDiscrete Gabor Transform computation. In the case of a generalized Gaussian\nwindow, extending earlier contributions, we introduce an additional local\nwindow adaptation technique for non-stationary signals. We illustrate our\napproach and the earlier one by addressing three time-frequency analysis\nproblems to show the improvements achieved by the use of optimal lattice and\nwindow: close frequencies distinction, frequency estimation and SNR estimation.\nThe results are presented, when possible, with real world audio signals."
},{
    "category": "cs.SD", 
    "doi": "10.14569/IJACSA.2013.040721", 
    "link": "http://arxiv.org/pdf/1405.6945v1", 
    "title": "Sparsity-Aware Filtered-X Affine Projection Algorithms for Active Noise   Control", 
    "arxiv-id": "1405.6945v1", 
    "author": "R. C. de Lamare", 
    "publish": "2014-05-25T22:22:04Z", 
    "summary": "This paper describes a novel technique for promoting sparsity in the modified\nfiltered-x algorithms required for active noise control. The proposed\nalgorithms are based on recent techniques incorporating approximations to the\n\\ell_0-norm in the cost functions that are used to derive adaptive filtering\nalgorithms. In particular, zero-attracting and reweighted zero-attracting\nfiltered-x adaptive algorithms are developed and considered for active noise\ncontrol problems. The results of simulations indicate that the proposed\ntechniques improve the convergence of the existing modified algorithm in the\ncase where the primary and secondary paths exhibit a degree of sparsity."
},{
    "category": "cs.SD", 
    "doi": "10.14569/IJACSA.2013.040721", 
    "link": "http://arxiv.org/pdf/1405.7866v1", 
    "title": "Vocal signal digital processing. Instrument for analog to digital   conversion study", 
    "arxiv-id": "1405.7866v1", 
    "author": "Felicia-Florentina Giza", 
    "publish": "2014-05-30T13:58:36Z", 
    "summary": "The goal of this article is to present interactive didactic software for\nanalog to digital conversion using PCM method. After a short introduction\nregarding vocal signal processing we present some method for analog to digital\nconversion. The didactic software is an applet that can be direct accessed by\nany interested person."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2014.2385864", 
    "link": "http://arxiv.org/pdf/1407.2351v3", 
    "title": "Efficient Steered-Response Power Methods for Sound Source Localization   Using Microphone Arrays", 
    "arxiv-id": "1407.2351v3", 
    "author": "Bowon Lee", 
    "publish": "2014-07-09T04:42:25Z", 
    "summary": "This paper proposes an efficient method based on the steered-response power\n(SRP) technique for sound source localization using microphone arrays: the\nvolumetric SRP (V-SRP). As compared to the SRP, by deploying a sparser\nvolumetric grid, the V-SRP achieves a significant reduction of the\ncomputational complexity without sacrificing the accuracy of the location\nestimates. By appending a fine search step to the V-SRP, its refined version\n(RV-SRP) improves on the compromise between complexity and accuracy.\nExperiments conducted in both simulated- and real-data scenarios demonstrate\nthe benefits of the proposed approaches. Specifically, the RV-SRP is shown to\noutperform the SRP in accuracy at a computational cost of about ten times\nlower."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2014.2385864", 
    "link": "http://arxiv.org/pdf/1407.3398v2", 
    "title": "Speech Polarity Detection Using Hilbert Phase Information", 
    "arxiv-id": "1407.3398v2", 
    "author": "Aguthu Smily", 
    "publish": "2014-07-12T16:30:30Z", 
    "summary": "The objective of the present work is to propose a method to automatically\ndetect polarity of the speech signals by estimating instants of significant\nexcitation of the vocaltract and the cosine phase of the analytic signal\nrepresentation. The phase changes in the analytic signal around the Hilbert\nenvelope (HE) peaks are found to vary according to the polarity of the given\nspeech signal. The relevant HE peaks for the Hilbert phase analysis are\nselected by estimating the instants of significant excitation in speech. The\nspeech polarity identification rate obtained for the proposed method is almost\nequal to the state of the art residual skewness method for speech polarity\ndetection. The proposed method also provides the same results for the polarity\ndetection in electro-glottogram signals. Finally, the robustness of the\nproposed method is confirmed from the reduced detection error rates obtained in\nnoisy environments with various signal to noise ratios (SNRs). The MATLAB codes\nused for implementing the proposed method are available for download from the\nfollowing link: http://nlp.amrita.edu:8080/TTS/polarityprograms.zip"
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2014.2385864", 
    "link": "http://arxiv.org/pdf/1410.2430v1", 
    "title": "Phase-Optimized K-SVD for Signal Extraction from Underdetermined   Multichannel Sparse Mixtures", 
    "arxiv-id": "1410.2430v1", 
    "author": "Walter Kellermann", 
    "publish": "2014-10-09T11:46:02Z", 
    "summary": "We propose a novel sparse representation for heavily underdetermined\nmultichannel sound mixtures, i.e., with much more sources than microphones. The\nproposed approach operates in the complex Fourier domain, thus preserving\nspatial characteristics carried by phase differences. We derive a\ngeneralization of K-SVD which jointly estimates a dictionary capturing both\nspectral and spatial features, a sparse activation matrix, and all\ninstantaneous source phases from a set of signal examples. The dictionary can\nthen be used to extract the learned signal from a new input mixture. The method\nis applied to the challenging problem of ego-noise reduction for robot\naudition. We demonstrate its superiority relative to conventional\ndictionary-based techniques using recordings made in a real room."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TENCON.2009.5396003", 
    "link": "http://arxiv.org/pdf/1410.6905v1", 
    "title": "On the use of Stress information in Speech for Speaker Recognition", 
    "arxiv-id": "1410.6905v1", 
    "author": "Sunil Kumar Kopparapu", 
    "publish": "2014-10-25T09:53:39Z", 
    "summary": "The performance of a speaker recognition system decreases when the speaker is\nunder stress or emotion. In this paper we explore and identify a mechanism that\nenables use of inherent stress-in-speech or speaking style information present\nin speech of a person as additional cues for speaker recognition. We quantify\nthe the inherent stress present in the speech of a speaker mainly using 3\nfeatures, namely, pitch, amplitude and duration (together called PAD) We\nexperimentally observe that the PAD vectors of similar phones in different\nwords of a speaker are close to each other in the three dimensional (PAD) space\nconfirming that the way a speaker stresses different syllables in their speech\nis unique to them, thus we propose the use of PAD based speaking style of a\nspeaker as an additional feature for speaker recognition applications."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TENCON.2009.5396003", 
    "link": "http://arxiv.org/pdf/1411.0370v1", 
    "title": "Detection of transitions between broad phonetic classes in a speech   signal", 
    "arxiv-id": "1411.0370v1", 
    "author": "A G Ramakrishnan", 
    "publish": "2014-11-03T06:27:13Z", 
    "summary": "Detection of transitions between broad phonetic classes in a speech signal is\nan important problem which has applications such as landmark detection and\nsegmentation. The proposed hierarchical method detects silence to non-silence\ntransitions, high amplitude (mostly sonorants) to low ampli- tude (mostly\nfricatives/affricates/stop bursts) transitions and vice-versa. A subset of the\nextremum (minimum or maximum) samples between every pair of successive\nzero-crossings is selected above a second pass threshold, from each bandpass\nfiltered speech signal frame. Relative to the mid-point (reference) of a frame,\nlocations of the first and the last extrema lie on either side, if the speech\nsignal belongs to a homogeneous segment; else, both these locations lie on the\nleft or the right side of the reference, indicating a transition frame. When\ntested on the entire TIMIT database, of the transitions detected, 93.6% are\nwithin a tolerance of 20 ms from the hand labeled boundaries. Sonorant,\nunvoiced non-sonorant and silence classes and their respective onsets are\ndetected with an accuracy of about 83.5% for the same tolerance. The results\nare as good as, and in some respects better than the state-of-the-art methods\nfor similar tasks."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TENCON.2009.5396003", 
    "link": "http://arxiv.org/pdf/1411.1267v1", 
    "title": "An Interesting Property of LPCs for Sonorant Vs Fricative Discrimination", 
    "arxiv-id": "1411.1267v1", 
    "author": "Pradeep Balachandran", 
    "publish": "2014-11-05T13:20:56Z", 
    "summary": "Linear prediction (LP) technique estimates an optimum all-pole filter of a\ngiven order for a frame of speech signal. The coefficients of the all-pole\nfilter, 1/A(z) are referred to as LP coefficients (LPCs). The gain of the\ninverse of the all-pole filter, A(z) at z = 1, i.e, at frequency = 0, A(1)\ncorresponds to the sum of LPCs, which has the property of being lower (higher)\nthan a threshold for the sonorants (fricatives). When the inverse-tan of A(1),\ndenoted as T(1), is used a feature and tested on the sonorant and fricative\nframes of the entire TIMIT database, an accuracy of 99.07% is obtained. Hence,\nwe refer to T(1) as sonorant-fricative discrimination index (SFDI). This\nproperty has also been tested for its robustness for additive white noise and\non the telephone quality speech of the NTIMIT database. These results are\ncomparable to, or in some respects, better than the state-of-the-art methods\nproposed for a similar task. Such a property may be used for segmenting a\nspeech signal or for non-uniform frame-rate analysis."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TENCON.2009.5396003", 
    "link": "http://arxiv.org/pdf/1411.1898v1", 
    "title": "A Novel Uncertainty Parameter SR (Signal To Residual Spectrum Ratio)   Evaluation Approach For Speech Enhancement", 
    "arxiv-id": "1411.1898v1", 
    "author": "B. Ravi Teja", 
    "publish": "2014-11-07T13:09:39Z", 
    "summary": "Usually, hearing impaired people use hearing aids which are implemented with\nspeech enhancement algorithms. Estimation of speech and estimation of nose are\nthe components in single channel speech enhancement system. The main objective\nof any speech enhancement algorithm is estimation of noise power spectrum for\nnon stationary environment. VAD (Voice Activity Detector) is used to identify\nspeech pauses and during these pauses only estimation of noise. MMSE (Minimum\nMean Square Error) speech enhancement algorithm did not enhance the\nintelligibility, quality and listener fatigues are the perceptual aspects of\nspeech. Novel evaluation approach SR (Signal to Residual spectrum ratio) based\non uncertainty parameter introduced for the benefits of hearing impaired people\nin non stationary environments to control distortions. By estimation and\nupdating of noise based on division of original pure signal into three parts\nsuch as pure speech, quasi speech and non speech frames based on multiple\nthreshold conditions. Different values of SR and LLR demonstrate the amount of\nattenuation and amplification distortions. The proposed method will compared\nwith any one method WAT(Weighted Average Technique) Hence by using parameters\nSR (signal to residual spectrum ratio) and LLR (log like hood ratio), MMSE\n(Minim Mean Square Error) in terms of segmented SNR and LLR."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2425213", 
    "link": "http://arxiv.org/pdf/1411.2744v4", 
    "title": "Spatial Source Subtraction Based on Incomplete Measurements of Relative   Transfer Function", 
    "arxiv-id": "1411.2744v4", 
    "author": "Sharon Gannot", 
    "publish": "2014-11-11T09:43:31Z", 
    "summary": "Relative impulse responses between microphones are usually long and dense due\nto the reverberant acoustic environment. Estimating them from short and noisy\nrecordings poses a long-standing challenge of audio signal processing. In this\npaper we apply a novel strategy based on ideas of Compressed Sensing. Relative\ntransfer function (RTF) corresponding to the relative impulse response can\noften be estimated accurately from noisy data but only for certain frequencies.\nThis means that often only an incomplete measurement of the RTF is available. A\ncomplete RTF estimate can be obtained through finding its sparsest\nrepresentation in the time-domain: that is, through computing the sparsest\namong the corresponding relative impulse responses. Based on this approach, we\npropose to estimate the RTF from noisy data in three steps. First, the RTF is\nestimated using any conventional method such as the non-stationarity-based\nestimator by Gannot et al. or through Blind Source Separation. Second,\nfrequencies are determined for which the RTF estimate appears to be accurate.\nThird, the RTF is reconstructed through solving a weighted $\\ell_1$ convex\nprogram, which we propose to solve via a computationally efficient variant of\nthe SpaRSA (Sparse Reconstruction by Separable Approximation) algorithm. An\nextensive experimental study with real-world recordings has been conducted. It\nhas been shown that the proposed method is capable of improving many\nconventional estimators used as the first step in most situations."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2425213", 
    "link": "http://arxiv.org/pdf/1411.6741v1", 
    "title": "A Complex Matrix Factorization approach to Joint Modeling of Magnitude   and Phase for Source Separation", 
    "arxiv-id": "1411.6741v1", 
    "author": "Rajesh M. Hegde", 
    "publish": "2014-11-25T06:18:45Z", 
    "summary": "Conventional NMF methods for source separation factorize the matrix of\nspectral magnitudes. Spectral Phase is not included in the decomposition\nprocess of these methods. However, phase of the speech mixture is generally\nused in reconstructing the target speech signal. This results in undesired\ntraces of interfering sources in the target signal. In this paper the spectral\nphase is incorporated in the decomposition process itself. Additionally, the\ncomplex matrix factorization problem is reduced to an NMF problem using simple\ntransformations. This results in effective separation of speech mixtures since\nboth magnitude and phase are utilized jointly in the separation process.\nImprovement in source separation results are demonstrated using objective\nquality evaluations on the GRID corpus."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2425213", 
    "link": "http://arxiv.org/pdf/1502.03162v1", 
    "title": "Sparse Head-Related Impulse Response for Efficient Direct Convolution", 
    "arxiv-id": "1502.03162v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2015-02-11T00:47:22Z", 
    "summary": "Head-related impulse responses (HRIRs) are subject-dependent and\ndirection-dependent filters used in spatial audio synthesis. They describe the\nscattering response of the head, torso, and pinnae of the subject. We propose a\nstructural factorization of the HRIRs into a product of non-negative and\nToeplitz matrices; the factorization is based on a novel extension of a\nnon-negative matrix factorization algorithm. As a result, the HRIR becomes\nexpressible as a convolution between a direction-independent \\emph{resonance}\nfilter and a direction-dependent \\emph{reflection} filter. Further, the\nreflection filter can be made \\emph{sparse} with minimal HRIR distortion. The\ndescribed factorization is shown to be applicable to the arbitrary source\nsignal case and allows one to employ time-domain convolution at a computational\ncost lower than using convolution in the frequency domain."
},{
    "category": "cs.SD", 
    "doi": "10.5540/03.2015.003.01.0468", 
    "link": "http://arxiv.org/pdf/1502.03387v1", 
    "title": "A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation   Recording", 
    "arxiv-id": "1502.03387v1", 
    "author": "R. M. Campello de Souza", 
    "publish": "2015-02-11T17:30:48Z", 
    "summary": "This paper presents a new approach for a vocoder design based on full\nfrequency masking by octaves in addition to a technique for spectral filling\nvia beta probability distribution. Some psycho-acoustic characteristics of\nhuman hearing - inaudibility masking in frequency and phase - are used as a\nbasis for the proposed algorithm. The results confirm that this technique may\nbe useful to save bandwidth in applications requiring intelligibility. It is\nrecommended for the legal eavesdropping of long voice conversations."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2418571", 
    "link": "http://arxiv.org/pdf/1502.03784v2", 
    "title": "Coherent-to-Diffuse Power Ratio Estimation for Dereverberation", 
    "arxiv-id": "1502.03784v2", 
    "author": "Walter Kellermann", 
    "publish": "2015-02-12T19:38:50Z", 
    "summary": "The estimation of the time- and frequency-dependent coherent-to-diffuse power\nratio (CDR) from the measured spatial coherence between two omnidirectional\nmicrophones is investigated. Known CDR estimators are formulated in a common\nframework, illustrated using a geometric interpretation in the complex plane,\nand investigated with respect to bias and robustness towards model errors.\nSeveral novel unbiased CDR estimators are proposed, and it is shown that\nknowledge of either the direction of arrival (DOA) of the target source or the\ncoherence of the noise field is sufficient for unbiased CDR estimation. The\nvalidity of the model for the application of CDR estimates to dereverberation\nis investigated using measured and simulated impulse responses. A CDR-based\ndereverberation system is presented and evaluated using signal-based quality\nmeasures as well as automatic speech recognition accuracy. The results show\nthat the proposed unbiased estimators have a practical advantage over existing\nestimators, and that the proposed DOA-independent estimator can be used for\neffective blind dereverberation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2418571", 
    "link": "http://arxiv.org/pdf/1502.04300v1", 
    "title": "Mandarin Singing Voice Synthesis Based on Harmonic Plus Noise Model and   Singing Expression Analysis", 
    "arxiv-id": "1502.04300v1", 
    "author": "Hsin-Min Wang", 
    "publish": "2015-02-15T10:22:47Z", 
    "summary": "The purpose of this study is to investigate how humans interpret musical\nscores expressively, and then design machines that sing like humans. We\nconsider six factors that have a strong influence on the expression of human\nsinging. The factors are related to the acoustic, phonetic, and musical\nfeatures of a real singing signal. Given real singing voices recorded following\nthe MIDI scores and lyrics, our analysis module can extract the expression\nparameters from the real singing signals semi-automatically. The expression\nparameters are used to control the singing voice synthesis (SVS) system for\nMandarin Chinese, which is based on the harmonic plus noise model (HNM). The\nresults of perceptual experiments show that integrating the expression factors\ninto the SVS system yields a notable improvement in perceptual naturalness,\nclearness, and expressiveness. By one-to-one mapping of the real singing signal\nand expression controls to the synthesizer, our SVS system can simulate the\ninterpretation of a real singer with the timbre of a speaker."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1503.00586v2", 
    "title": "Evaluation of spatial audio reproduction schemes for application in   hearing aid research", 
    "arxiv-id": "1503.00586v2", 
    "author": "Volker Hohmann", 
    "publish": "2015-03-02T16:01:56Z", 
    "summary": "Loudspeaker-based spatial audio reproduction schemes are increasingly used\nfor evaluating hearing aids in complex acoustic conditions. To further\nestablish the feasibility of this approach, this study investigated the\ninteraction between spatial resolution of different reproduction methods and\ntechnical and perceptual hearing aid performance measures using computer\nsimulations. Three spatial audio reproduction methods -- discrete speakers,\nvector base amplitude panning and higher order ambisonics -- were compared in\nregular circular loudspeaker arrays with 4 to 72 channels. The influence of\nreproduction method and array size on performance measures of representative\nmulti-microphone hearing aid algorithm classes with spatially distributed\nmicrophones and a representative single channel noise-reduction algorithm was\nanalyzed. Algorithm classes differed in their way of analyzing and exploiting\nspatial properties of the sound field, requiring different accuracy of sound\nfield reproduction. Performance measures included beam pattern analysis,\nsignal-to-noise ratio analysis, perceptual localization prediction, and quality\nmodeling. The results show performance differences and interaction effects\nbetween reproduction method and algorithm class that may be used for guidance\nwhen selecting the appropriate method and number of speakers for specific tasks\nin hearing aid research."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1503.07015v2", 
    "title": "Online Monaural Speech Enhancement Based on Periodicity Analysis and A   Priori SNR Estimation", 
    "arxiv-id": "1503.07015v2", 
    "author": "Volker Hohmann", 
    "publish": "2015-03-24T12:40:13Z", 
    "summary": "This paper describes an online algorithm for enhancing monaural noisy speech.\nFirstly, a novel phase-corrected low-delay gammatone filterbank is derived for\nsignal subband decomposition and resynthesis; the subband signals are then\nanalyzed frame by frame. Secondly, a novel feature named periodicity degree\n(PD) is proposed to be used for detecting and estimating the fundamental period\n(P0) in each frame and for estimating the signal-to-noise ratio (SNR) in each\nframe-subband signal unit. The PD is calculated in each unit as the\nmultiplication of the normalized autocorrelation and the comb filter ratio, and\nshown to be robust in various low-SNR conditions. Thirdly, the noise energy\nlevel in each signal unit is estimated recursively based on the estimated SNR\nfor units with high PD and based on the noisy signal energy level for units\nwith low PD. Then the a priori SNR is estimated using a decision-directed\napproach with the estimated noise level. Finally, a revised Wiener gain is\ncalculated, smoothed, and applied to each unit; the processed units are summed\nacross subbands and frames to form the enhanced signal. The P0 detection\naccuracy of the algorithm was evaluated on two corpora and showed comparable\nperformance on one corpus and better performance on the other corpus when\ncompared to a recently published pitch detection algorithm. The speech\nenhancement effect of the algorithm was evaluated on one corpus with two\nobjective criteria and showed better performance in one highly non-stationary\nnoise and comparable performance in two other noises when compared to a\nstate-of-the-art statistical-model based algorithm."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1503.07150v2", 
    "title": "Acoustic event detection for multiple overlapping similar sources", 
    "arxiv-id": "1503.07150v2", 
    "author": "David Clayton", 
    "publish": "2015-03-24T19:45:02Z", 
    "summary": "Many current paradigms for acoustic event detection (AED) are not adapted to\nthe organic variability of natural sounds, and/or they assume a limit on the\nnumber of simultaneous sources: often only one source, or one source of each\ntype, may be active. These aspects are highly undesirable for applications such\nas bird population monitoring. We introduce a simple method modelling the\nonsets, durations and offsets of acoustic events to avoid intrinsic limits on\npolyphony or on inter-event temporal patterns. We evaluate the method in a case\nstudy with over 3000 zebra finch calls. In comparison against a HMM-based\nmethod we find it more accurate at recovering acoustic events, and more robust\nfor estimating calling rates."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1504.03128v1", 
    "title": "Absolute Geometry Calibration of Distributed Microphone Arrays in an   Audio-Visual Sensor Network", 
    "arxiv-id": "1504.03128v1", 
    "author": "Reinhold Haeb-Umbach", 
    "publish": "2015-04-13T11:08:00Z", 
    "summary": "Joint audio-visual speaker tracking requires that the locations of\nmicrophones and cameras are known and that they are given in a common\ncoordinate system. Sensor self-localization algorithms, however, are usually\nseparately developed for either the acoustic or the visual modality and return\ntheir positions in a modality specific coordinate system, often with an unknown\nrotation, scaling and translation between the two. In this paper we propose two\ntechniques to determine the positions of acoustic sensors in a common\ncoordinate system, based on audio-visual correlates, i.e., events that are\nlocalized by both, microphones and cameras separately. The first approach maps\nthe output of an acoustic self-calibration algorithm by estimating rotation,\nscale and translation to the visual coordinate system, while the second solves\na joint system of equations with acoustic and visual directions of arrival as\ninput. The evaluation of the two strategies reveals that joint calibration\noutperforms the mapping approach and achieves an overall calibration error of\n0.20m even in reverberant environments."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1504.07372v1", 
    "title": "Time-Frequency Trade-offs for Audio Source Separation with Binary Masks", 
    "arxiv-id": "1504.07372v1", 
    "author": "Andrew J. R. Simpson", 
    "publish": "2015-04-28T08:18:55Z", 
    "summary": "The short-time Fourier transform (STFT) provides the foundation of\nbinary-mask based audio source separation approaches. In computing a\nspectrogram, the STFT window size parameterizes the trade-off between time and\nfrequency resolution. However, it is not yet known how this parameter affects\nthe operation of the binary mask in terms of separation quality for real-world\nsignals such as speech or music. Here, we demonstrate that the trade-off\nbetween time and frequency in the STFT, used to perform ideal binary mask\nseparation, depends upon the types of source that are to be separated. In\nparticular, we demonstrate that different window sizes are optimal for\nseparating different combinations of speech and musical signals. Our findings\nhave broad implications for machine audition and machine learning in general."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1505.00289v1", 
    "title": "Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural   Network", 
    "arxiv-id": "1505.00289v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2015-05-01T22:10:58Z", 
    "summary": "Audio source separation is a difficult machine learning problem and\nperformance is measured by comparing extracted signals with the component\nsource signals. However, if separation is motivated by the ultimate goal of\nre-mixing then complete separation is not necessary and hence separation\ndifficulty and separation quality are dependent on the nature of the re-mix.\nHere, we use a convolutional deep neural network (DNN), trained to estimate\n'ideal' binary masks for separating voice from music, to perform re-mixing of\nthe vocal balance by operating directly on the individual magnitude components\nof the musical mixture spectrogram. Our results demonstrate that small changes\nin vocal gain may be applied with very little distortion to the ultimate\nre-mix. Our method may be useful for re-mixing existing mixes."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1505.04385v1", 
    "title": "An Efficient Parameterization of the Room Transfer Function", 
    "arxiv-id": "1505.04385v1", 
    "author": "Terence Betlehem", 
    "publish": "2015-05-17T12:09:48Z", 
    "summary": "This paper proposes an efficient parameterization of the Room Transfer\nFunction (RTF). Typically, the RTF rapidly varies with varying source and\nreceiver positions, hence requires an impractical number of point to point\nmeasurements to characterize a given room. Therefore, we derive a novel RTF\nparameterization that is robust to both receiver and source variations with the\nfollowing salient features: (i) The parameterization is given in terms of a\nmodal expansion of 3D basis functions. (ii) The aforementioned modal expansion\ncan be truncated at a finite number of modes given that the source and receiver\nlocations are from two sizeable spatial regions, which are arbitrarily\ndistributed. (iii) The parameter weights/coefficients are independent of the\nsource/receiver positions. Therefore, a finite set of coefficients is shown to\nbe capable of accurately calculating the RTF between any two arbitrary points\nfrom a predefined spatial region where the source(s) lie and a pre-defined\nspatial region where the receiver(s) lie. A practical method to measure the RTF\ncoefficients is also provided, which only requires a single microphone unit and\na single loudspeaker unit, given that the room characteristics remain\nstationary over time. The accuracy of the above parameterization is verified\nusing appropriate simulation examples."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1506.01830v2", 
    "title": "Sparsity and cosparsity for audio declipping: a flexible non-convex   approach", 
    "arxiv-id": "1506.01830v2", 
    "author": "R\u00e9mi Gribonval", 
    "publish": "2015-06-05T09:20:00Z", 
    "summary": "This work investigates the empirical performance of the sparse synthesis\nversus sparse analysis regularization for the ill-posed inverse problem of\naudio declipping. We develop a versatile non-convex heuristics which can be\nreadily used with both data models. Based on this algorithm, we report that, in\nmost cases, the two models perform almost similarly in terms of signal\nenhancement. However, the analysis version is shown to be amenable for real\ntime audio processing, when certain analysis operators are considered. Both\nversions outperform state-of-the-art methods in the field, especially for the\nseverely saturated signals."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1506.03604v1", 
    "title": "Binaural coherent-to-diffuse-ratio estimation for dereverberation using   an ITD model", 
    "arxiv-id": "1506.03604v1", 
    "author": "Xiaodong Li", 
    "publish": "2015-06-11T09:55:59Z", 
    "summary": "Most previously proposed dual-channel coherent-to-diffuse-ratio (CDR)\nestimators are based on a free-field model. When used for binaural signals,\ne.g., for dereverberation in binaural hearing aids, their performance may\ndegrade due to the influence of the head, even when the direction-of-arrival of\nthe desired speaker is exactly known. In this paper, the head shadowing effect\nis taken into account for CDR estimation by using a simplified model for the\nfrequency-dependent interaural time difference and a model for the binaural\ncoherence of the diffuse noise field. Evaluation of CDR-based dereverberation\nwith measured binaural impulse responses indicates that the proposed binaural\nCDR estimators can improve PESQ scores."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1506.03701v3", 
    "title": "Channel Interaction and Current Level Affect Across-Electrode   Integration of Interaural Time Differences in Bilateral Cochlear-Implant   Listeners", 
    "arxiv-id": "1506.03701v3", 
    "author": "Bernhard Laback", 
    "publish": "2015-06-11T15:06:03Z", 
    "summary": "Sensitivity to ITDs is important for sound localization. Normal-hearing\nlisteners benefit from across-frequency processing, as seen with improved ITD\nthresholds when consistent ITD cues are presented over a range of frequency\nchannels compared to when ITD information is only presented in a single\nfrequency channel. This study aimed to clarify whether cochlear-implant (CI)\nlisteners can make use of similar processing when being stimulated with\nmultiple interaural electrode pairs transmitting consistent ITD information.\nITD thresholds for unmodulated, 100-pulse-per-second pulse trains were measured\nin seven bilateral CI listeners using research interfaces. Consistent ITDs were\npresented at either one or two electrode pairs at different current levels,\nallowing for comparisons at either constant level per component electrode or\nequal overall loudness. Different tonotopic distances between the pairs were\ntested in order to clarify the potential influence of channel interaction.\nComparison of ITD thresholds between double pairs and the respective single\npairs revealed systematic effects of tonotopic separation and current level. At\nconstant levels, performance with double-pair stimulation improved compared to\nsingle-pair stimulation, but only for large tonotopic separation. Comparisons\nat equal overall loudness revealed no benefit from presenting ITD information\nat two electrode pairs for any tonotopic spacing. Irrespective of\nelectrode-pair configuration, ITD sensitivity improved with increasing current\nlevel. Hence, the improved ITD sensitivity for double pairs found for a large\ntonotopic separation and constant current levels seems to be due to increased\nloudness. The overall data suggest that CI listeners can benefit from combining\nconsistent ITD information across multiple electrodes, provided sufficient\nstimulus levels and that stimulating electrode pairs are widely spaced."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1507.00201v1", 
    "title": "Towards a Generalization of Relative Transfer Functions to More Than One   Source", 
    "arxiv-id": "1507.00201v1", 
    "author": "Walter Kellermann", 
    "publish": "2015-07-01T12:13:10Z", 
    "summary": "We propose a natural way to generalize relative transfer functions (RTFs) to\nmore than one source. We first prove that such a generalization is not possible\nusing a single multichannel spectro-temporal observation, regardless of the\nnumber of microphones. We then introduce a new transform for multichannel\nmulti-frame spectrograms, i.e., containing several channels and time frames in\neach time-frequency bin. This transform allows a natural generalization which\nsatisfies the three key properties of RTFs, namely, they can be directly\nestimated from observed signals, they capture spatial properties of the sources\nand they do not depend on emitted signals. Through simulated experiments, we\nshow how this new method can localize multiple simultaneously active sound\nsources using short spectro-temporal windows, without relying on source\nseparation."
},{
    "category": "cs.SD", 
    "doi": "10.3813/AAA.918878", 
    "link": "http://arxiv.org/pdf/1507.05143v1", 
    "title": "Cover Song Identification with Timbral Shape Sequences", 
    "arxiv-id": "1507.05143v1", 
    "author": "Paul Bendich", 
    "publish": "2015-07-18T03:55:50Z", 
    "summary": "We introduce a novel low level feature for identifying cover songs which\nquantifies the relative changes in the smoothed frequency spectrum of a song.\nOur key insight is that a sliding window representation of a chunk of audio can\nbe viewed as a time-ordered point cloud in high dimensions. For corresponding\nchunks of audio between different versions of the same song, these point clouds\nare approximately rotated, translated, and scaled copies of each other. If we\ntreat MFCC embeddings as point clouds and cast the problem as a relative shape\nsequence, we are able to correctly identify 42/80 cover songs in the \"Covers\n80\" dataset. By contrast, all other work to date on cover songs exclusively\nrelies on matching note sequences from Chroma derived features."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4929733", 
    "link": "http://arxiv.org/pdf/1507.07348v1", 
    "title": "A model for the temporal evolution of the spatial coherence in decaying   reverberant sound fields", 
    "arxiv-id": "1507.07348v1", 
    "author": "Walter Kellermann", 
    "publish": "2015-07-27T10:08:24Z", 
    "summary": "Reverberant sound fields are often modeled as isotropic. However, it has been\nobserved that spatial properties change during the decay of the sound field\nenergy, due to non-isotropic attenuation in non-ideal rooms. In this letter, a\nmodel for the spatial coherence between two sensors in a decaying reverberant\nsound field is developed for rectangular rooms. The modeled coherence function\ndepends on room dimensions, surface reflectivity and orientation of the sensor\npair, but is independent of the position of source and sensors in the room. The\nmodel includes the spherically isotropic (diffuse) and cylindrically isotropic\nsound field models as special cases."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4929733", 
    "link": "http://arxiv.org/pdf/1508.03148v1", 
    "title": "Semi-Supervised Sound Source Localization Based on Manifold   Regularization", 
    "arxiv-id": "1508.03148v1", 
    "author": "Sharon Gannot", 
    "publish": "2015-08-13T09:02:48Z", 
    "summary": "Conventional speaker localization algorithms, based merely on the received\nmicrophone signals, are often sensitive to adverse conditions, such as: high\nreverberation or low signal to noise ratio (SNR). In some scenarios, e.g. in\nmeeting rooms or cars, it can be assumed that the source position is confined\nto a predefined area, and the acoustic parameters of the environment are\napproximately fixed. Such scenarios give rise to the assumption that the\nacoustic samples from the region of interest have a distinct geometrical\nstructure. In this paper, we show that the high dimensional acoustic samples\nindeed lie on a low dimensional manifold and can be embedded into a low\ndimensional space. Motivated by this result, we propose a semi-supervised\nsource localization algorithm which recovers the inverse mapping between the\nacoustic samples and their corresponding locations. The idea is to use an\noptimization framework based on manifold regularization, that involves\nsmoothness constraints of possible solutions with respect to the manifold. The\nproposed algorithm, termed Manifold Regularization for Localization (MRL), is\nimplemented in an adaptive manner. The initialization is conducted with only\nfew labelled samples attached with their respective source locations, and then\nthe system is gradually adapted as new unlabelled samples (with unknown source\nlocations) are received. Experimental results show superior localization\nperformance when compared with a recently presented algorithm based on a\nmanifold learning approach and with the generalized cross-correlation (GCC)\nalgorithm as a baseline."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4929733", 
    "link": "http://arxiv.org/pdf/1508.07739v4", 
    "title": "Transposition of Notations in Just Intonation", 
    "arxiv-id": "1508.07739v4", 
    "author": "David Ryan", 
    "publish": "2015-08-31T09:39:02Z", 
    "summary": "A notation system was previously presented which can notate any rational\nfrequency in free Just Intonation. Transposition of music is carried out by\nmultiplying each member of a set of frequencies by a single frequency.\nTransposition of JI notations up by a fixed amount requires multiplication to\nbe defined for any two notations. Transposition down requires inversion to be\ndefined for any notation, which allows division to also be defined for any two\nnotations. Each notation splits into four components which in decreasing size\norder are octave, diatonic scale note, sharps or flats, rational comma\nadjustment. Multiplication can be defined for each of the four notation\ncomponents. Since rational number multiplication is commutative, this leads to\na definition of multiplication for frequencies and thus notations. Examples of\nnotation inversion and multiplication are given. Examples of transposing\nmelodies are given. These are checked for accuracy using the rational numbers\nwhich each notation represents. Calculation shortcuts are considered which make\nnotation operations quicker to carry out by hand. A question regarding whether\nrational commas should be extended from 5-rough rational numbers to all\nrational numbers is considered which would greatly simplify notation\nmultiplication. This approach is rejected since it leads to confusion about\noctave number. The four component notation system is recommended instead.\nExtensions to computer notation systems and stave representations are briefly\nmentioned."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4929733", 
    "link": "http://arxiv.org/pdf/1509.00334v1", 
    "title": "Transform\u00e9e en scattering sur la spirale temps-chroma-octave", 
    "arxiv-id": "1509.00334v1", 
    "author": "St\u00e9phane Mallat", 
    "publish": "2015-09-01T15:04:16Z", 
    "summary": "We introduce a scattering representation for the analysis and classification\nof sounds. It is locally translation-invariant, stable to deformations in time\nand frequency, and has the ability to capture harmonic structures. The\nscattering representation can be interpreted as a convolutional neural network\nwhich cascades a wavelet transform in time and along a harmonic spiral. We\nstudy its application for the analysis of the deformations of the source-filter\nmodel."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2598319", 
    "link": "http://arxiv.org/pdf/1509.03205v3", 
    "title": "Estimation of the Direct-Path Relative Transfer Function for Supervised   Sound-Source Localization", 
    "arxiv-id": "1509.03205v3", 
    "author": "Sharon Gannot", 
    "publish": "2015-09-10T15:57:28Z", 
    "summary": "This paper addresses the problem of binaural localization of a single speech\nsource in noisy and reverberant environments. For a given binaural microphone\nsetup, the binaural response corresponding to the direct-path propagation of a\nsingle source is a function of the source direction. In practice, this response\nis contaminated by noise and reverberations. The direct-path relative transfer\nfunction (DP-RTF) is defined as the ratio between the direct-path acoustic\ntransfer function of the two channels. We propose a method to estimate the\nDP-RTF from the noisy and reverberant microphone signals in the short-time\nFourier transform domain. First, the convolutive transfer function\napproximation is adopted to accurately represent the impulse response of the\nsensors in the STFT domain. Second, the DP-RTF is estimated by using the auto-\nand cross-power spectral densities at each frequency and over multiple frames.\nIn the presence of stationary noise, an inter-frame spectral subtraction\nalgorithm is proposed, which enables to achieve the estimation of noise-free\nauto- and cross-power spectral densities. Finally, the estimated DP-RTFs are\nconcatenated across frequencies and used as a feature vector for the\nlocalization of speech source. Experiments with both simulated and real data\nshow that the proposed localization method performs well, even under severe\nadverse acoustic conditions, and outperforms state-of-the-art localization\nmethods under most of the acoustic conditions."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SLT.2014.7078560", 
    "link": "http://arxiv.org/pdf/1509.04934v1", 
    "title": "Background-tracking Acoustic Features for Genre Identification of   Broadcast Shows", 
    "arxiv-id": "1509.04934v1", 
    "author": "Thomas Hain", 
    "publish": "2015-09-16T14:28:49Z", 
    "summary": "This paper presents a novel method for extracting acoustic features that\ncharacterise the background environment in audio recordings. These features are\nbased on the output of an alignment that fits multiple parallel\nbackground--based Constrained Maximum Likelihood Linear Regression\ntransformations asynchronously to the input audio signal. With this setup, the\nresulting features can track changes in the audio background like appearance\nand disappearance of music, applause or laughter, independently of the speakers\nin the foreground of the audio. The ability to provide this type of acoustic\ndescription in audiovisual data has many potential applications, including\nautomatic classification of broadcast archives or improving automatic\ntranscription and subtitling. In this paper, the performance of these features\nin a genre identification task in a set of 332 BBC shows is explored. The\nproposed background--tracking features outperform short--term Perceptual Linear\nPrediction features in this task using Gaussian Mixture Model classifiers (62%\nvs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and\nSupport Vector Machines, increases the performance of the system with the novel\nbackground--tracking features to 79% and 81% in accuracy respectively."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SLT.2014.7078560", 
    "link": "http://arxiv.org/pdf/1509.06882v1", 
    "title": "Robust coherence-based spectral enhancement for distant speech   recognition", 
    "arxiv-id": "1509.06882v1", 
    "author": "Walter Kellermann", 
    "publish": "2015-09-23T08:34:18Z", 
    "summary": "In this contribution to the 3rd CHiME Speech Separation and Recognition\nChallenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline\nspeech recognition system by a coherence-based Wiener filter which is applied\nto the output signal of the baseline beamformer. To compute the time- and\nfrequency-dependent postfilter gains the ratio between direct and diffuse\nsignal components at the output of the baseline beamformer is estimated and\nused as approximation of the short-time signal-to-noise ratio. The proposed\nspectral enhancement technique is evaluated with respect to word error rates of\nthe CHiME-3 challenge baseline speech recognition system using real speech\nrecorded in public environments. Results confirm the effectiveness of the\ncoherence-based postfilter when integrated into the front-end signal\nenhancement."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SLT.2014.7078560", 
    "link": "http://arxiv.org/pdf/1509.07298v4", 
    "title": "An Investigation of Universal Background Sparse Coding Based Speaker   Verification on TIMIT", 
    "arxiv-id": "1509.07298v4", 
    "author": "Xiao-Lei Zhang", 
    "publish": "2015-09-24T10:16:09Z", 
    "summary": "In this paper, we propose a universal background model, named universal\nbackground sparse coding (UBSC), for speaker verification. The proposed method\ntrains an ensemble of clusterings by data resampling, and produces sparse codes\nfrom the clusterings by one-nearest-neighbor optimization plus binarization.\nThe main advantage of UBSC is that it does not suffer from local minima and\ndoes not make Gaussian assumptions on data distributions. We evaluated UBSC on\na clean speech corpus---TIMIT. We used the cosine similarity and inner product\nsimilarity as the scoring methods of a trial. Experimental results show that\nUBSC is comparable to Gaussian mixture model."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SLT.2014.7078560", 
    "link": "http://arxiv.org/pdf/1509.07411v1", 
    "title": "Speech Dereverberation in the STFT Domain", 
    "arxiv-id": "1509.07411v1", 
    "author": "Mike Brookes", 
    "publish": "2015-09-24T15:46:55Z", 
    "summary": "Reverberation is damaging to both the quality and the intelligibility of a\nspeech signal. We propose a novel single-channel method of dereverberation\nbased on a linear filter in the Short Time Fourier Transform domain. Each\nenhanced frame is constructed from a linear sum of nearby frames based on the\nchannel impulse response. The results show that the method can resolve any\nreverberant signal with knowledge of the impulse response to a non-reverberant\nsignal."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.00266v1", 
    "title": "Noise robust integration for blind and non-blind reverberation time   estimation", 
    "arxiv-id": "1510.00266v1", 
    "author": "Peter H\u00e4ndel", 
    "publish": "2015-10-01T14:53:29Z", 
    "summary": "The estimation of the decay rate of a signal section is an integral component\nof both blind and non-blind reverberation time estimation methods. Several\ndecay rate estimators have previously been proposed, based on, e.g., linear\nregression and maximum-likelihood estimation. Unfortunately, most approaches\nare sensitive to background noise, and/or are fairly demanding in terms of\ncomputational complexity. This paper presents a low complexity decay rate\nestimator, robust to stationary noise, for reverberation time estimation.\nSimulations using artificial signals, and experiments with speech in\nventilation noise, demonstrate the performance and noise robustness of the\nproposed method."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.00268v1", 
    "title": "The ICSTM+TUM+UP Approach to the 3rd CHIME Challenge: Single-Channel   LSTM Speech Enhancement with Multi-Channel Correlation Shaping   Dereverberation and LSTM Language Models", 
    "arxiv-id": "1510.00268v1", 
    "author": "Bj\u00f6rn Schuller", 
    "publish": "2015-10-01T14:57:08Z", 
    "summary": "This paper presents our contribution to the 3rd CHiME Speech Separation and\nRecognition Challenge. Our system uses Bidirectional Long Short-Term Memory\n(BLSTM) Recurrent Neural Networks (RNNs) for Single-channel Speech Enhancement\n(SSE). Networks are trained to predict clean speech as well as noise features\nfrom noisy speech features. In addition, the system applies two methods of\ndereverberation on the 6-channel recordings of the challenge. The first is the\nPhase-Error based Filtering (PEF) that uses time-varying phase-error filters\nbased on estimated time-difference of arrival of the speech source and the\nphases of the microphone signals. The second is the Correlation Shaping (CS)\nthat applies a reduction of the long-term correlation energy in reverberant\nspeech. The Linear Prediction (LP) residual is processed to suppress the\nlong-term correlation. Furthermore, the system employs a LSTM Language Model\n(LM) to perform N-best rescoring of recognition hypotheses. Using the proposed\nmethods, an improved Word Error Rate (WER) of 24.38% is achieved over the real\neval test set. This is around 25% relative improvement over the challenge\nbaseline."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.00383v1", 
    "title": "Proceedings of the ACE Challenge Workshop - a satellite event of   IEEE-WASPAA (2015)", 
    "arxiv-id": "1510.00383v1", 
    "author": "Patrick A. Naylor", 
    "publish": "2015-10-01T14:44:34Z", 
    "summary": "Several established parameters and metrics have been used to characterize the\nacoustics of a room. The most important are the Direct-To-Reverberant Ratio\n(DRR), the Reverberation Time (T60) and the reflection coefficient. The\nacoustic characteristics of a room based on such parameters can be used to\npredict the quality and intelligibility of speech signals in that room.\nRecently, several important methods in speech enhancement and speech\nrecognition have been developed that show an increase in performance compared\nto the predecessors but do require knowledge of one or more fundamental\nacoustical parameters such as the T60. Traditionally, these parameters have\nbeen estimated using carefully measured Acoustic Impulse Responses (AIRs).\nHowever, in most applications it is not practical or even possible to measure\nthe acoustic impulse response. Consequently, there is increasing research\nactivity in the estimation of such parameters directly from speech and audio\nsignals. The aim of this challenge was to evaluate state-of-the-art algorithms\nfor blind acoustic parameter estimation from speech and to promote the emerging\narea of research in this field. Participants evaluated their algorithms for T60\nand DRR estimation against the 'ground truth' values provided with the\ndata-sets and presented the results in a paper describing the method used."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.01193v1", 
    "title": "Reverberation time estimation on the ACE corpus using the SDD method", 
    "arxiv-id": "1510.01193v1", 
    "author": "Patrick A. Naylor", 
    "publish": "2015-10-05T15:42:45Z", 
    "summary": "Reverberation Time (T60) is an important measure for characterizing the\nproperties of a room. The author's T60 estimation algorithm was previously\ntested on simulated data where the noise is artificially added to the speech\nafter convolution with a impulse responses simulated using the image method. We\ntest the algorithm on speech convolved with real recorded impulse responses and\nnoise from the same rooms from the Acoustic Characterization of Environments\n(ACE) corpus and achieve results comparable results to those using simulated\ndata."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.01806v3", 
    "title": "Music Viewed by its Entropy Content: A Novel Window for Comparative   Analysis", 
    "arxiv-id": "1510.01806v3", 
    "author": "Klaus Jaffe", 
    "publish": "2015-10-07T03:00:52Z", 
    "summary": "Polyphonic music files were analyzed using the set of symbols that produced\nthe Minimal Entropy Description which we call the Fundamental Scale. This\nallowed us to create a novel space to represent music pieces by developing: a)\na method to adjust a description from its original scale of observation to a\ngeneral scale, b) the concept of higher order entropy as the entropy associated\nto the deviations of a frequency ranked symbol profile from a perfect Zipf\nprofile. We called this diversity index the \"2nd Order Entropy\". Applying these\nmethods to a variety of musical pieces showed how the space of \"symbolic\nspecific diversity-entropy\" and that of \"2nd order entropy\" captures\ncharacteristics that are unique to each music type, style, composer and genre.\nSome clustering of these properties around each musical category is shown. This\nmethod allows to visualize a historic trajectory of academic music across this\nspace, from medieval to contemporary academic music. We show that description\nof musical structures using entropy and symbolic diversity allows to\ncharacterize traditional and popular expressions of music. These classification\ntechniques promise to be useful in other disciplines for pattern recognition\nand machine learning, for example."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177931", 
    "link": "http://arxiv.org/pdf/1510.04205v1", 
    "title": "Reducing one-to-many problem in Voice Conversion by equalizing the   formant locations using dynamic frequency warping", 
    "arxiv-id": "1510.04205v1", 
    "author": "Seyed Hamidreza Mohammadi", 
    "publish": "2015-10-14T17:17:33Z", 
    "summary": "In this study, we investigate a solution to reduce the effect of one-to-many\nproblem in voice conversion. One-to-many problem in VC happens when two very\nsimilar speech segments in source speaker have corresponding speech segments in\ntarget speaker that are not similar to each other. As a result, the mapper\nfunction usually over-smoothes the generated features in order to be similar to\nboth target speech segments. In this study, we propose to equalize the formant\nlocation of source-target frame pairs using dynamic frequency warping in order\nto reduce the complexity. After the conversion, another dynamic frequency\nwarping is further applied to reverse the effect of formant location\nequalization during the training. The subjective experiments showed that the\nproposed approach improves the speech quality significantly."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.04595v3", 
    "title": "A Variational EM Algorithm for the Separation of Time-Varying   Convolutive Audio Mixtures", 
    "arxiv-id": "1510.04595v3", 
    "author": "Radu Horaud", 
    "publish": "2015-10-15T15:48:19Z", 
    "summary": "This paper addresses the problem of separating audio sources from\ntime-varying convolutive mixtures. We propose a probabilistic framework based\non the local complex-Gaussian model combined with non-negative matrix\nfactorization. The time-varying mixing filters are modeled by a continuous\ntemporal stochastic process. We present a variational expectation-maximization\n(VEM) algorithm that employs a Kalman smoother to estimate the time-varying\nmixing matrix, and that jointly estimate the source parameters. The sound\nsources are then separated by Wiener filters constructed with the estimators\nprovided by the VEM algorithm. Extensive experiments on simulated data show\nthat the proposed method outperforms a block-wise version of a state-of-the-art\nbaseline method."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.04616v1", 
    "title": "Evaluating the Non-Intrusive Room Acoustics Algorithm with the ACE   Challenge", 
    "arxiv-id": "1510.04616v1", 
    "author": "Patrick A. Naylor", 
    "publish": "2015-10-15T16:42:16Z", 
    "summary": "We present a single channel data driven method for non-intrusive estimation\nof full-band reverberation time and full-band direct-to-reverberant ratio. The\nmethod extracts a number of features from reverberant speech and builds a model\nusing a recurrent neural network to estimate the reverberant acoustic\nparameters. We explore three configurations by including different data and\nalso by combining the recurrent neural network estimates using a support vector\nmachine. Our best method to estimate DRR provides a Root Mean Square Deviation\n(RMSD) of 3.84 dB and a RMSD of 43.19 % for T60 estimation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.04620v1", 
    "title": "Joint Estimation of Reverberation Time and Direct-to-Reverberation Ratio   from Speech using Auditory-Inspired Features", 
    "arxiv-id": "1510.04620v1", 
    "author": "Bernd T. Meyer", 
    "publish": "2015-10-15T16:48:48Z", 
    "summary": "Blind estimation of acoustic room parameters such as the reverberation time\n$T_\\mathrm{60}$ and the direct-to-reverberation ratio ($\\mathrm{DRR}$) is still\na challenging task, especially in case of blind estimation from reverberant\nspeech signals. In this work, a novel approach is proposed for joint estimation\nof $T_\\mathrm{60}$ and $\\mathrm{DRR}$ from wideband speech in noisy conditions.\n2D Gabor filters arranged in a filterbank are exploited for extracting\nfeatures, which are then used as input to a multi-layer perceptron (MLP). The\nMLP output neurons correspond to specific pairs of $(T_\\mathrm{60},\n\\mathrm{DRR})$ estimates; the output is integrated over time, and a simple\ndecision rule results in our estimate. The approach is applied to\nsingle-microphone fullband speech signals provided by the Acoustic\nCharacterization of Environments (ACE) Challenge. Our approach outperforms the\nbaseline systems with median errors of close-to-zero and -1.5 dB for the\n$T_\\mathrm{60}$ and $\\mathrm{DRR}$ estimates, respectively, while the\ncalculation of estimates is 5.8 times faster compared to the baseline."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.04707v1", 
    "title": "SRMR variants for improved blind room acoustics characterization", 
    "arxiv-id": "1510.04707v1", 
    "author": "T. H. Falk", 
    "publish": "2015-10-15T20:18:29Z", 
    "summary": "Reverberation, especially in large rooms, severely degrades speech\nrecognition performance and speech intelligibility. Since direct measurement of\nroom characteristics is usually not possible, blind estimation of\nreverberation-related metrics such as the reverberation time (RT) and the\ndirect-to-reverberant energy ratio (DRR) can be valuable information to speech\nrecognition and enhancement algorithms operating in enclosed environments. The\nobjective of this work is to evaluate the performance of five variants of blind\nRT and DRR estimators based on a modulation spectrum representation of\nreverberant speech with single- and multi-channel speech data. These models are\nall based on variants of the so-called Speech-to-Reverberation Modulation\nEnergy Ratio (SRMR). We show that these measures outperform a state-of-the-art\nbaseline based on maximum-likelihood estimation of sound decay rates in terms\nof root-mean square error (RMSE), as well as Pearson correlation. Compared to\nthe baseline, the best proposed measure, called NSRMR_k , achieves a 23%\nrelative improvement in terms of RMSE and allows for relative correlation\nimprovements ranging from 13% to 47% for RT prediction."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.07315v1", 
    "title": "A Hybrid Approach for Speech Enhancement Using MoG Model and Neural   Network Phoneme Classifier", 
    "arxiv-id": "1510.07315v1", 
    "author": "Sharon Gannot", 
    "publish": "2015-10-25T22:24:37Z", 
    "summary": "In this paper we present a single-microphone speech enhancement algorithm. A\nhybrid approach is proposed merging the generative mixture of Gaussians (MoG)\nmodel and the discriminative neural network (NN). The proposed algorithm is\nexecuted in two phases, the training phase, which does not recur, and the test\nphase. First, the noise-free speech power spectral density (PSD) is modeled as\na MoG, representing the phoneme based diversity in the speech signal. An NN is\nthen trained with phoneme labeled database for phoneme classification with\nmel-frequency cepstral coefficients (MFCC) as the input features. Given the\nphoneme classification results, a speech presence probability (SPP) is obtained\nusing both the generative and discriminative models. Soft spectral subtraction\nis then executed while simultaneously, the noise estimation is updated. The\ndiscriminative NN maintain the continuity of the speech and the generative\nphoneme-based MoG preserves the speech spectral structure. Extensive\nexperimental study using real speech and noise signals is provided. We also\ncompare the proposed algorithm with alternative speech enhancement algorithms.\nWe show that we obtain a significant improvement over previous methods in terms\nof both speech quality measures and speech recognition results."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.07546v1", 
    "title": "Direct-to-Reverberant Ratio Estimation on the ACE Corpus Using a   Two-channel Beamformer", 
    "arxiv-id": "1510.07546v1", 
    "author": "Patrick A. Naylor", 
    "publish": "2015-10-26T16:50:26Z", 
    "summary": "Direct-to-Reverberant Ratio (DRR) is an important measure for characterizing\nthe properties of a room. The recently proposed DRR Estimation using a\nNull-Steered Beamformer (DENBE) algorithm was originally tested on simulated\ndata where noise was artificially added to the speech after convolution with\nimpulse responses simulated using the image-source method. This paper evaluates\nthe performance of this algorithm on speech convolved with measured impulse\nresponses and noise using the Acoustic Characterization of Environments (ACE)\nEvaluation corpus. The fullband DRR estimation performance of the DENBE\nalgorithm exceeds that of the baselines in all Signal-to-Noise Ratios (SNRs)\nand noise types. In addition, estimation of the DRR in one third-octave ISO\nfrequency bands is demonstrated."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.07774v1", 
    "title": "A dictionary learning and source recovery based approach to classify   diverse audio sources", 
    "arxiv-id": "1510.07774v1", 
    "author": "A G Ramakrishnan", 
    "publish": "2015-10-27T05:25:11Z", 
    "summary": "A dictionary learning based audio source classification algorithm is proposed\nto classify a sample audio signal as one amongst a finite set of different\naudio sources. Cosine similarity measure is used to select the atoms during\ndictionary learning. Based on three objective measures proposed, namely, signal\nto distortion ratio (SDR), the number of non-zero weights and the sum of\nweights, a frame-wise source classification accuracy of 98.2% is obtained for\ntwelve different sources. Cent percent accuracy has been obtained using moving\nSDR accumulated over six successive frames for ten of the audio sources tested,\nwhile the two other sources require accumulation of 10 and 14 frames."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.08484v1", 
    "title": "MUSAN: A Music, Speech, and Noise Corpus", 
    "arxiv-id": "1510.08484v1", 
    "author": "Daniel Povey", 
    "publish": "2015-10-28T20:59:04Z", 
    "summary": "This report introduces a new corpus of music, speech, and noise. This dataset\nis suitable for training models for voice activity detection (VAD) and\nmusic/speech discrimination. Our corpus is released under a flexible Creative\nCommons license. The dataset consists of music from several genres, speech from\ntwelve languages, and a wide assortment of technical and non-technical noises.\nWe demonstrate use of this corpus for music/speech discrimination on Broadcast\nnews and VAD for speaker identification."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.08950v1", 
    "title": "Estimation of the direct-to-reverberant Energy Ratio using a spherical   microphone array", 
    "arxiv-id": "1510.08950v1", 
    "author": "Wen Zhang", 
    "publish": "2015-10-30T01:52:31Z", 
    "summary": "This paper proposes a practical approach to estimate the\ndirect-to-reverberant energy ratio (DRR) using a spherical microphone array\nwithout having knowledge of the source signal. We base our estimation on a\ntheoretical relationship between the DRR and the coherence estimation function\nbetween coincident pressure and particle velocity. We discuss the proposed\nmethod's ability to estimate the DRR in a wide variety of room sizes,\nreverberation times and source receiver distances with appropriate examples.\nTest results show that the method can estimate the room DRR for frequencies\nbetween 199 - 2511 Hz, with $\\pm$ 3 dB accuracy."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2554286", 
    "link": "http://arxiv.org/pdf/1510.08963v1", 
    "title": "PSD estimation in Beamspace for Estimating Direct-to-Reverberant Ratio   from A Reverberant Speech Signal", 
    "arxiv-id": "1510.08963v1", 
    "author": "Kenta Niwa", 
    "publish": "2015-10-30T03:45:41Z", 
    "summary": "A method for estimation of direct-to-reverberant ratio (DRR) using a\nmicrophone array is proposed. The proposed method estimates the power spectral\ndensity (PSD) of the direct sound and the reverberation using the algorithm\n\\textit{PSD estimation in beamspace} with a microphone array and calculates the\nDRR of the observed signal. The speech corpus of the ACE (Acoustic\nCharacterisation of Environments) Challenge was utilised for evaluating the\npractical feasibility of the proposed method. The experimental results revealed\nthat the proposed method was able to effectively estimate the DRR from a\nrecording of a reverberant speech signal which included various environmental\nnoise."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ymssp.2015.11.027", 
    "link": "http://arxiv.org/pdf/1511.00067v1", 
    "title": "Sparsity-based Algorithm for Detecting Faults in Rotating Machines", 
    "arxiv-id": "1511.00067v1", 
    "author": "Ivan W. Selesnick", 
    "publish": "2015-10-31T03:48:31Z", 
    "summary": "This paper addresses the detection of periodic transients in vibration\nsignals for detecting faults in rotating machines. For this purpose, we present\na method to estimate periodic-group-sparse signals in noise. The method is\nbased on the formulation of a convex optimization problem. A fast iterative\nalgorithm is given for its solution. A simulated signal is formulated to verify\nthe performance of the proposed approach for periodic feature extraction. The\ndetection performance of comparative methods is compared with that of the\nproposed approach via RMSE values and receiver operating characteristic (ROC)\ncurves. Finally, the proposed approach is applied to compound faults diagnosis\nof motor bearings. The non-stationary vibration data were acquired from a\nSpectraQuest's machinery fault simulator. The processed results show the\nproposed approach can effectively detect and extract the useful features of\nbearing outer race and inner race defect."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2016.07.004", 
    "link": "http://arxiv.org/pdf/1511.00393v2", 
    "title": "Detection of Faults in Rotating Machinery Using Periodic Time-Frequency   Sparsity", 
    "arxiv-id": "1511.00393v2", 
    "author": "Ivan W. Selesnick", 
    "publish": "2015-11-02T06:08:24Z", 
    "summary": "This paper addresses the problem of extracting periodic oscillatory features\nin vibration sig- nals for detecting faults in rotating machinery. To extract\nthe feature, we propose an approach in the short-time Fourier transform (STFT)\ndomain where the periodic oscillatory feature man- ifests itself as a\nrelatively sparse grid. To estimate the sparse grid, we formulate an\noptimization problem using customized binary weights in the regularizer, where\nthe weights are formulated to promote periodicity. In order to solve the\nproposed optimization problem, we develop an algorithm called augmented\nLagrangian majorization-minimization algorithm, which combines the split\naugmented Lagrangian shrinkage algorithm (SALSA) with majorization-minimization\n(MM), and is guaranteed to converge for both convex and non-convex formulation.\nAs examples, the proposed approach is applied to simulated data, and used as a\ntool for diagnosing faults in bearings and gearboxes for real data, and\ncompared to some state-of-the-art methods. The results show the proposed\napproach can effectively detect and extract the periodical oscillatory\nfeatures."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2016.07.004", 
    "link": "http://arxiv.org/pdf/1511.03440v1", 
    "title": "Combination of binaural and harmonic masking release effects in the   detection of a single component in complex tones", 
    "arxiv-id": "1511.03440v1", 
    "author": "Volker Hohmann", 
    "publish": "2015-11-11T10:26:57Z", 
    "summary": "Harmonic and binaural signal features are relevant for auditory scene\nanalysis, i.e., the segregation and grouping of sound sources in complex\nacoustic scenes. To further investigate how these features combine in the\nauditory system, detection thresholds for an 800-Hz tone masked by a diotic\nharmonic complex tone were measured in six normal-hearing subjects. The target\ntone was presented diotically or with an interaural phase difference (IPD) of\n180{\\deg} and in harmonic or \"mistuned\" relationship to the diotic masker.\nThree different maskers were used, a resolved and an unresolved complex tone\n(fundamental frequency: 160 and 40 Hz) with four components below and above the\ntarget frequency and a broadband unresolved complex tone with 12 additional\ncomponents. The target IPD provided release from masking in all masker\nconditions, whereas mistuning led to a significant release from masking only in\nthe diotic conditions with the resolved and the narrowband unresolved maskers.\nA significant effect of mistuning was neither found in the diotic condition\nwith the wideband unresolved masker nor in any of the dichotic conditions. An\nauditory model with a single analysis frequency band and different binaural\nprocessing schemes was employed to predict the data of the unresolved masker\nconditions. Sensitivity to modulation cues was achieved by including an\nauditory-motivated modulation filter in the processing pathway. The predictions\nof the diotic data were in line with the experimental results and literature\ndata in the narrowband condition, but not in the broadband condition. The\nexperimental and model results in the dichotic conditions hint at a parallel\nprocessing scheme with a binaural processor that has only limited access to\nmodulation information."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2016.07.004", 
    "link": "http://arxiv.org/pdf/1511.04063v1", 
    "title": "Single-Channel Maximum-Likelihood T60 Estimation Exploiting Subband   Information", 
    "arxiv-id": "1511.04063v1", 
    "author": "Walter Kellermann", 
    "publish": "2015-11-12T20:41:40Z", 
    "summary": "This contribution presents four algorithms developed by the authors for\nsingle-channel fullband and subband T60 estimation within the ACE challenge.\nThe blind estimation of the fullband reverberation time (RT) by\nmaximum-likelihood (ML) estimation based on [15] is considered as baseline\napproach. An improvement of this algorithm is devised where an energy-weighted\naveraging of the upper subband RT estimates is performed using either a DCT or\n1/3-octave filter-bank. The evaluation results show that this approach leads to\na lower variance for the estimation error in comparison to the baseline\napproach at the price of an increased computational complexity. Moreover, a new\nalgorithm to estimate the subband RT is devised, where the RT estimates for the\nlower octave subbands are extrapolated from the RT estimates of the upper\nsubbands by means of a simple model for the frequency-dependency of the subband\nRT. The evaluation results of the ACE challenge reveal that this approach\nallows to estimate the subband RT with an estimation error which is in a\nsimilar range as for the presented fullband RT estimators."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2016.07.004", 
    "link": "http://arxiv.org/pdf/1511.04867v2", 
    "title": "Quality assessment of voice converted speech using articulatory features", 
    "arxiv-id": "1511.04867v2", 
    "author": "Hemant A. Patil", 
    "publish": "2015-11-16T08:53:37Z", 
    "summary": "We propose a novel application based on acoustic-to-articulatory inversion\ntowards quality assessment of voice converted speech. The ability of humans to\nspeak effortlessly requires coordinated movements of various articulators,\nmuscles, etc. This effortless movement contributes towards naturalness,\nintelligibility and speakers identity which is partially present in voice\nconverted speech. Hence, during voice conversion, the information related to\nspeech production is lost. In this paper, this loss is quantified for male\nvoice, by showing increase in RMSE error for voice converted speech followed by\nshowing decrease in mutual information. Similar results are obtained in case of\nfemale voice. This observation is extended by showing that articulatory\nfeatures can be used as an objective measure. The effectiveness of proposed\nmeasure over MCD is illustrated by comparing their correlation with Mean\nOpinion Score."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2016.07.004", 
    "link": "http://arxiv.org/pdf/1511.07008v1", 
    "title": "Real Time Vowel Tremolo Detection Using Low Level Audio Descriptors", 
    "arxiv-id": "1511.07008v1", 
    "author": "Marta Gentilucci", 
    "publish": "2015-11-22T13:05:05Z", 
    "summary": "This paper resumes the results of a research conducted in a music production\nsituation Therefore, it is more a final lab report, a prospective methodology\nthen a scientific experience. The methodology we are presenting was developed\nas an answer to a musical problem raised by the Italian composer Marta\nGentilucci. The problem was \"how to extract a temporal structure from a vowel\ntremolo, on a tenuto (steady state) pitch.\" The musical goal was to apply, in a\ncompositional context the vowel tremolo time structure on a tenuto pitch chord,\nas a transposition control.In this context we decide to follow, to explore the\npotential of low-level MPEG7 audio descriptors to build event detection\nfunctions. One of the main problems using low-level audio descriptors in audio\nanalysis is the redundancy of information among them. We describe an \"ad hoc\"\ninteractive methodology, based on side effect use of dimensionality reduction\nby PCA, to choose a feature from a set of low-level audio descriptors, to be\nused to detect a vowel tremolo rhythm. This methodology is supposed to be\ninteractive and easy enough to be used in a live creative context."
},{
    "category": "cs.SD", 
    "doi": "10.1007/s11042-015-3039-x", 
    "link": "http://arxiv.org/pdf/1512.01809v1", 
    "title": "High quality voice conversion using prosodic and high-resolution   spectral features", 
    "arxiv-id": "1512.01809v1", 
    "author": "Eng Siong Chng", 
    "publish": "2015-12-06T17:26:52Z", 
    "summary": "Voice conversion methods have advanced rapidly over the last decade. Studies\nhave shown that speaker characteristics are captured by spectral feature as\nwell as various prosodic features. Most existing conversion methods focus on\nthe spectral feature as it directly represents the timbre characteristics,\nwhile some conversion methods have focused only on the prosodic feature\nrepresented by the fundamental frequency. In this paper, a comprehensive\nframework using deep neural networks to convert both timbre and prosodic\nfeatures is proposed. The timbre feature is represented by a high-resolution\nspectral feature. The prosodic features include F0, intensity and duration. It\nis well known that DNN is useful as a tool to model high-dimensional features.\nIn this work, we show that DNN initialized by our proposed autoencoder\npretraining yields good quality DNN conversion models. This pretraining is\ntailor-made for voice conversion and leverages on autoencoder to capture the\ngeneric spectral shape of source speech. Additionally, our framework uses\nsegmental DNN models to capture the evolution of the prosodic features over\ntime. To reconstruct the converted speech, the spectral feature produced by the\nDNN model is combined with the three prosodic features produced by the DNN\nsegmental models. Our experimental results show that the application of both\nprosodic and high-resolution spectral features leads to quality converted\nspeech as measured by objective evaluation and subjective listening tests."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2015.7324385", 
    "link": "http://arxiv.org/pdf/1512.02125v1", 
    "title": "Joint Time-Frequency Scattering for Audio Classification", 
    "arxiv-id": "1512.02125v1", 
    "author": "St\u00e9phane Mallat", 
    "publish": "2015-12-07T17:11:01Z", 
    "summary": "We introduce the joint time-frequency scattering transform, a time shift\ninvariant descriptor of time-frequency structure for audio classification. It\nis obtained by applying a two-dimensional wavelet transform in time and\nlog-frequency to a time-frequency wavelet scalogram. We show that this\ndescriptor successfully characterizes complex time-frequency phenomena such as\ntime-varying filters and frequency modulated excitations. State-of-the-art\nresults are achieved for signal reconstruction and phone segment classification\non the TIMIT dataset."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1512.03261v3", 
    "title": "Exploiting a Geometrically Sampled Grid in the Steered Response Power   Algorithm for Localization Improvement", 
    "arxiv-id": "1512.03261v3", 
    "author": "Gian Luca Foresti", 
    "publish": "2015-12-10T14:12:51Z", 
    "summary": "The steered response power phase transform (SRP-PHAT) is a beamformer method\nvery attractive in acoustic localization applications due to its robustness in\nreverberant environments. This paper presents a spatial grid design procedure,\ncalled the geometrically sampled grid (GSG), which aims at computing the\nspatial grid by taking into account the discrete sampling of time difference of\narrival (TDOA) functions and the desired spatial resolution. A new SRP-PHAT\nlocalization algorithm based on the GSG method is also introduced. The proposed\nmethod exploits the intersections of the discrete hyperboloids representing the\nTDOA information domain of the sensor array, and projects the whole TDOA\ninformation on the space search grid. The GSG method thus allows to design the\nsampled spatial grid which represents the best search grid for a given sensor\narray, it allows to perform a sensitivity analysis of the array and to\ncharacterize its spatial localization accuracy, and it may assist the system\ndesigner in the reconfiguration of the array. Experimental results using both\nsimulated data and real recordings show that the localization accuracy is\nsubstantially improved both for high and for low spatial resolution, and that\nit is closely related to the proposed power response sensitivity measure."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1512.05811v1", 
    "title": "Spectral Study of the Vocal Tract in Vowel Synthesis: A Comparison   between 1D and 3D Acoustic Analysis", 
    "arxiv-id": "1512.05811v1", 
    "author": "Sidney Fels", 
    "publish": "2015-12-17T22:06:51Z", 
    "summary": "A state-of-the-art 1D acoustic synthesizer has been previously developed, and\ncoupled to speaker-specific biomechanical models of oropharynx in ArtiSynth. As\nexpected, the formant frequencies of the synthesized vowel sounds were shown to\nbe different from those of the recorded audio. Such discrepancy was\nhypothesized to be due to the simplified geometry of the vocal tract model as\nwell as the one dimensional implementation of Navier-Stokes equations. In this\npaper, we calculate Helmholtz resonances of our vocal tract geometries using 3D\nfinite element method (FEM), and compare them with the formant frequencies\nobtained from the 1D method and audio. We hope such comparison helps with\nclarifying the limitations of our current models and/or speech synthesizer."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1512.08075v1", 
    "title": "Multichannel audio signal source separation based on an Interchannel   Loudness Vector Sum", 
    "arxiv-id": "1512.08075v1", 
    "author": "Taejin Lee", 
    "publish": "2015-12-26T05:25:00Z", 
    "summary": "In this paper, a Blind Source Separation (BSS) algorithm for multichannel\naudio contents is proposed. Unlike common BSS algorithms targeting stereo audio\ncontents or microphone array signals, our technique is targeted at multichannel\naudio such as 5.1 and 7.1ch audio. Since most multichannel audio object sources\nare panned using the Inter-channel Loudness Difference (ILD), we employ the\nILVS (Inter-channel Loudness Vector Sum) concept to cluster common signals\n(such as background music) from each channel. After separating the common\nsignals from each channel, we employ an Expectation Maximization (EM) algorithm\nwith a von-Mises distribution to successfully classify the clustering of sound\nsource objects and separate the audio signals from the original mixture. Our\nproposed method can therefore separate common audio signals and object source\nsignals from multiple channels with reasonable quality. Our multichannel audio\ncontent separation technique can be applied to an upmix system or a cinema\naudio system requiring multichannel audio source separation."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1601.00833v1", 
    "title": "An Analysis of Rhythmic Staccato-Vocalization Based on Frequency   Demodulation for Laughter Detection in Conversational Meetings", 
    "arxiv-id": "1601.00833v1", 
    "author": "B. B. Chaudhuri", 
    "publish": "2016-01-05T14:13:28Z", 
    "summary": "Human laugh is able to convey various kinds of meanings in human\ncommunications. There exists various kinds of human laugh signal, for example:\nvocalized laugh and non vocalized laugh. Following the theories of psychology,\namong all the vocalized laugh type, rhythmic staccato-vocalization\nsignificantly evokes the positive responses in the interactions. In this paper\nwe attempt to exploit this observation to detect human laugh occurrences, i.e.,\nthe laughter, in multiparty conversations from the AMI meeting corpus. First,\nwe separate the high energy frames from speech, leaving out the low energy\nframes through power spectral density estimation. We borrow the algorithm of\nrhythm detection from the area of music analysis to use that on the high energy\nframes. Finally, we detect rhythmic laugh frames, analyzing the candidate\nrhythmic frames using statistics. This novel approach for detection of\n`positive' rhythmic human laughter performs better than the standard laughter\nclassification baseline."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1601.01577v1", 
    "title": "Gender Identification using MFCC for Telephone Applications - A   Comparative Study", 
    "arxiv-id": "1601.01577v1", 
    "author": "Sung Wook Baik", 
    "publish": "2016-01-07T15:57:45Z", 
    "summary": "Gender recognition is an essential component of automatic speech recognition\nand interactive voice response systems. Determining gender of the speaker\nreduces the computational burden of such systems for any further processing.\nTypical methods for gender recognition from speech largely depend on features\nextraction and classification processes. The purpose of this study is to\nevaluate the performance of various state-of-the-art classification methods\nalong with tuning their parameters for helping selection of the optimal\nclassification methods for gender recognition tasks. Five classification\nschemes including k-nearest neighbor, na\\\"ive Bayes, multilayer perceptron,\nrandom forest, and support vector machine are comprehensively evaluated for\ndetermination of gender from telephonic speech using the Mel-frequency cepstral\ncoefficients. Different experiments were performed to determine the effects of\ntraining data sizes, length of the speech streams, and parameter tuning on\nclassification performance. Results suggest that SVM is the best classifier\namong all the five schemes for gender recognition."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4974289", 
    "link": "http://arxiv.org/pdf/1601.02069v1", 
    "title": "Dynamic Transposition of Melodic Sequences on Digital Devices", 
    "arxiv-id": "1601.02069v1", 
    "author": "Andrei V Smirnov", 
    "publish": "2016-01-09T03:06:18Z", 
    "summary": "A method is proposed which enables one to produce musical compositions by\nusing transposition in place of harmonic progression. A transposition scale is\nintroduced to provide a set of intervals commensurate with the musical scale,\nsuch as chromatic or just intonation scales. A sequence of intervals selected\nfrom the transposition scale is used to shift instrument frequency at\npredefined times during the composition which serves as a harmonic sequence of\na composition. A transposition sequence constructed in such a way can be\nextended to a hierarchy of sequences. The fundamental sound frequency of an\ninstrument is obtained as a product of the base frequency, instrument key\nfactor, and a cumulative product of respective factors from all the harmonic\nsequences. The multiplication factors are selected from subsets of rational\nnumbers, which form instrument scales and transposition scales of different\nlevels. Each harmonic sequence can be related to its own transposition scale,\nor a single scale can be used for all levels. When composing for an orchestra\nof instruments, harmonic sequences and instrument scales can be assigned\nindependently to each musical instrument. The method solves the problem of\nusing just intonation scale across multiple octaves as well as simplifies\nwriting of instrument scores."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2571727", 
    "link": "http://arxiv.org/pdf/1601.02309v1", 
    "title": "Wavelet speech enhancement based on nonnegative matrix factorization", 
    "arxiv-id": "1601.02309v1", 
    "author": "Borching Su", 
    "publish": "2016-01-11T02:53:25Z", 
    "summary": "For most of the state-of-the-art speech enhancement techniques, a spectrogram\nis usually preferred than the respective time-domain raw data since it reveals\nmore compact presentation together with conspicuous temporal information over a\nlong time span. However, the short-time Fourier transform (STFT) that creates\nthe spectrogram in general distorts the original signal and thereby limits the\ncapability of the associated speech enhancement techniques. In this study, we\npropose a novel speech enhancement method that adopts the algorithms of\ndiscrete wavelet packet transform (DWPT) and nonnegative matrix factorization\n(NMF) in order to conquer the aforementioned limitation. In brief, the DWPT is\nfirst applied to split a time-domain speech signal into a series of subband\nsignals without introducing any distortion. Then we exploit NMF to highlight\nthe speech component for each subband. Finally, the enhanced subband signals\nare joined together via the inverse DWPT to reconstruct a noise-reduced signal\nin time domain. We evaluate the proposed DWPT-NMF based speech enhancement\nmethod on the MHINT task. Experimental results show that this new method\nbehaves very well in prompting speech quality and intelligibility and it\noutperforms the convnenitional STFT-NMF based method."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ymssp.2016.06.035", 
    "link": "http://arxiv.org/pdf/1601.02339v2", 
    "title": "Repetitive Transients Extraction Algorithm for Detecting Bearing Faults", 
    "arxiv-id": "1601.02339v2", 
    "author": "Ivan W. Selesnick", 
    "publish": "2016-01-11T06:45:58Z", 
    "summary": "This paper addresses the problem of noise reduction with simultaneous\ncomponents extrac- tion in vibration signals for faults diagnosis of bearing.\nThe observed vibration signal is modeled as a summation of two components\ncontaminated by noise, and each component composes of repetitive transients. To\nextract the two components simultaneously, an approach by solving an\noptimization problem is proposed in this paper. The problem adopts convex\nsparsity-based regularization scheme for decomposition, and non-convex\nregularization is used to further promote the sparsity but preserving the\nglobal convexity. A synthetic example is presented to illustrate the\nperformance of the proposed approach for repetitive feature extraction. The\nperformance and effectiveness of the proposed method are further demonstrated\nby applying to compound faults and single fault diagnosis of a locomotive\nbearing. The results show the proposed approach can effectively extract the\nfeatures of outer and inner race defects."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ymssp.2016.06.035", 
    "link": "http://arxiv.org/pdf/1601.02489v1", 
    "title": "Categorization of Tablas by Wavelet Analysis", 
    "arxiv-id": "1601.02489v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-01-03T06:24:56Z", 
    "summary": "Tabla, a percussion instrument, mainly used to accompany vocalists,\ninstrumentalists and dancers in every style of music from classical to light in\nIndia, mainly used for keeping rhythm. This percussion instrument consists of\ntwo drums played by two hands, structurally different and produces different\nharmonic sounds. Earlier work has done labeling tabla strokes from real time\nperformances by testing neural networks and tree based classification methods.\nThe current work extends previous work by C. V. Raman and S. Kumar in 1920 on\nspectrum modeling of tabla strokes. In this paper we have studied spectral\ncharacteristics (by wavelet analysis by sub band coding method and using\ntorrence wavelet tool) of nine strokes from each of five tablas using Wavelet\ntransform. Wavelet analysis is now a common tool for analyzing localized\nvariations of power within a time series and to find the frequency distribution\nin time frequency space. Statistically, we will look into the patterns depicted\nby harmonics of different sub bands and the tablas. Distribution of dominant\nfrequencies at different sub-band of stroke signals, distribution of power and\nbehavior of harmonics are the important features, leads to categorization of\ntabla."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ymssp.2016.06.035", 
    "link": "http://arxiv.org/pdf/1601.02546v1", 
    "title": "Automatic Determination of Chord Roots", 
    "arxiv-id": "1601.02546v1", 
    "author": "Samuel Rupprechter", 
    "publish": "2016-01-11T18:14:03Z", 
    "summary": "Even though chord roots constitute a fundamental concept in music theory,\nexisting models do not explain and determine them to full satisfaction. We\npresent a new method which takes sequential context into account to resolve\nambiguities and detect nonharmonic tones. We extract features from chord pairs\nand use a decision tree to determine chord roots. This leads to a quantitative\nimprovement in correctness of the predicted roots in comparison to other\nmodels. All this raises the question how much harmonic and nonharmonic tones\nactually contribute to the perception of chord roots."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ymssp.2016.06.035", 
    "link": "http://arxiv.org/pdf/1601.06652v1", 
    "title": "A Perceptually Motivated Filter Bank with Perfect Reconstruction for   Audio Signal Processing", 
    "arxiv-id": "1601.06652v1", 
    "author": "Zdenek Prusa", 
    "publish": "2016-01-25T16:13:37Z", 
    "summary": "Many audio applications rely on filter banks (FBs) to analyze, process, and\nre-synthesize sounds. To approximate the auditory frequency resolution in the\nsignal chain, some applications rely on perceptually motivated FBs, the\ngammatone FB being a popular example. However, most perceptually motivated FBs\nonly allow partial signal reconstruction at high redundancies and/or do not\nhave good resistance to sub-channel processing. This paper introduces an\noversampled perceptually motivated FB enabling perfect reconstruction,\nefficient FB design, and adaptable redundancy. The filters are directly\nconstructed in the frequency domain and linearly distributed on a perceptual\nfrequency scale (e.g. ERB, Bark, or Mel scale). The proposed design allows for\nvarious filter shapes, uniform or non-uniform FB setting, and large\ndown-sampling factors. For redundancies $\\geq$ 3 perfect reconstruction is\nachieved by computing the canonical dual FB analytically. For lower\nredundancies perfect reconstruction is achieved using an iterative method.\nExperiments show performance improvements of the proposed approach when\ncompared to the gammatone FB in terms of reconstruction error and resistance to\nsub-channel processing, especially at low redundancies."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.compeleceng.2008.11.005", 
    "link": "http://arxiv.org/pdf/1602.05900v1", 
    "title": "An Iterative Linearised Solution to the Sinusoidal Parameter Estimation   Problem", 
    "arxiv-id": "1602.05900v1", 
    "author": "Timothy B. Terriberry", 
    "publish": "2016-02-17T18:15:36Z", 
    "summary": "Signal processing applications use sinusoidal modelling for speech synthesis,\nspeech coding, and audio coding. Estimation of the model parameters involves\nnon-linear optimisation methods, which can be very costly for real-time\napplications. We propose a low-complexity iterative method that starts from\ninitial frequency estimates and converges rapidly. We show that for N sinusoids\nin a frame of length L, the proposed method has a complexity of O(LN), which is\nsignificantly less than the matching pursuits method. Furthermore, the proposed\nmethod is shown to be more accurate than the matching pursuits and\ntime-frequency reassignment methods in our experiments."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.specom.2016.07.002", 
    "link": "http://arxiv.org/pdf/1602.06582v2", 
    "title": "Near-field signal acquisition for smartglasses using two acoustic   vector-sensors", 
    "arxiv-id": "1602.06582v2", 
    "author": "Sharon Gannot", 
    "publish": "2016-02-21T21:20:13Z", 
    "summary": "Smartglasses, in addition to their visual-output capabilities, often contain\nacoustic sensors for receiving the user's voice. However, operation in noisy\nenvironments may lead to significant degradation of the received signal. To\naddress this issue, we propose employing an acoustic sensor array which is\nmounted on the eyeglasses frames. The signals from the array are processed by\nan algorithm with the purpose of acquiring the user's desired near-filed speech\nsignal while suppressing noise signals originating from the environment. The\narray is comprised of two AVSs which are located at the fore of the glasses'\ntemples. Each AVS consists of four collocated subsensors: one pressure sensor\n(with an omnidirectional response) and three particle-velocity sensors (with\ndipole responses) oriented in mutually orthogonal directions. The array\nconfiguration is designed to boost the input power of the desired signal, and\nto ensure that the characteristics of the noise at the different channels are\nsufficiently diverse (lending towards more effective noise suppression). Since\nchanges in the array's position correspond to the desired speaker's movement,\nthe relative source-receiver position remains unchanged; hence, the need to\ntrack fluctuations of the steering vector is avoided. Conversely, the spatial\nstatistics of the noise are subject to rapid and abrupt changes due to sudden\nmovement and rotation of the user's head. Consequently, the algorithm must be\ncapable of rapid adaptation. We propose an algorithm which incorporates\ndetection of the desired speech in the time-frequency domain, and employs this\ninformation to adaptively update estimates of the noise statistics. Speech\ndetection plays a key role in ensuring the quality of the output signal. We\nconduct controlled measurements of the array in noisy scenarios. The proposed\nalgorithm preforms favorably with respect to conventional algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.specom.2016.07.002", 
    "link": "http://arxiv.org/pdf/1602.07767v1", 
    "title": "Breath Activity Detection Algorithm", 
    "arxiv-id": "1602.07767v1", 
    "author": "Manel Ramon-Martinez", 
    "publish": "2016-02-25T01:41:34Z", 
    "summary": "This report describes the use of a support vector machines with a novel\nkernel, to determine the breathing rate and inhalation duration of a fire\nfighter wearing a Self-Contained Breathing Apparatus. With this information, an\nincident commander can monitor the firemen in his command for exhaustion and\nensure timely rotation of personnel to ensure overall fire fighter safety"
},{
    "category": "cs.SD", 
    "doi": "10.1109/SCFT.2000.878425", 
    "link": "http://arxiv.org/pdf/1602.08215v1", 
    "title": "Bandwidth Extension of Narrowband Speech for Low Bit-Rate Wideband   Coding", 
    "arxiv-id": "1602.08215v1", 
    "author": "Roch Lefebvre", 
    "publish": "2016-02-26T06:47:06Z", 
    "summary": "Wireless telephone speech is usually limited to the 300-3400 Hz band, which\nreduces its quality. There is thus a growing demand for wideband speech systems\nthat transmit from 50 Hz to 8000 Hz. This paper presents an algorithm to\ngenerate wideband speech from narrowband speech using as low as 500 bits/s of\nside information. The 50-300 Hz band is predicted from the narrowband signal. A\nsource-excitation model is used for the 3400-8000 Hz band, where the excitation\nis extrapolated at the receiver, and the spectral envelope is transmitted.\nThough some artifacts are present, the resulting wideband speech has enhanced\nquality compared to narrowband speech."
},{
    "category": "cs.SD", 
    "doi": "10.1109/SCFT.2000.878425", 
    "link": "http://arxiv.org/pdf/1602.08507v1", 
    "title": "Occupancy Estimation in Smart Buildings using Audio-Processing   Techniques", 
    "arxiv-id": "1602.08507v1", 
    "author": "Chao Lu", 
    "publish": "2016-02-24T04:52:01Z", 
    "summary": "In the past few years, several case studies have illustrated that the use of\noccupancy information in buildings leads to energy-efficient and low-cost HVAC\noperation. The widely presented techniques for occupancy estimation include\ntemperature, humidity, CO2 concentration, image camera, motion sensor and\npassive infrared (PIR) sensor. So far little studies have been reported in\nliterature to utilize audio and speech processing as indoor occupancy\nprediction technique. With rapid advances of audio and speech processing\ntechnologies, nowadays it is more feasible and attractive to integrate\naudio-based signal processing component into smart buildings. In this work, we\npropose to utilize audio processing techniques (i.e., speaker recognition and\nbackground audio energy estimation) to estimate room occupancy (i.e., the\nnumber of people inside a room). Theoretical analysis and simulation results\ndemonstrate the accuracy and effectiveness of this proposed occupancy\nestimation technique. Based on the occupancy estimation, smart buildings will\nadjust the thermostat setups and HVAC operations, thus, achieving greater\nquality of service and drastic cost savings."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HSCMA.2008.4538718", 
    "link": "http://arxiv.org/pdf/1602.08633v1", 
    "title": "Perceptually-Motivated Nonlinear Channel Decorrelation For Stereo   Acoustic Echo Cancellation", 
    "arxiv-id": "1602.08633v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-02-27T19:32:25Z", 
    "summary": "Acoustic echo cancellation with stereo signals is generally an\nunder-determined problem because of the high coherence between the left and\nright channels. In this paper, we present a novel method of significantly\nreducing inter-channel coherence without affecting the audio quality. Our work\ntakes into account psychoacoustic masking and binaural auditory cues. The\nproposed non-linear processing combines a shaped comb-allpass (SCAL) filter\nwith the injection of psychoacoustically masked noise. We show that the\nproposed method performs significantly better than other known methods for\nreducing inter-channel coherence."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HSCMA.2008.4538718", 
    "link": "http://arxiv.org/pdf/1602.08668v1", 
    "title": "Speex: A Free Codec For Free Speech", 
    "arxiv-id": "1602.08668v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-02-28T04:38:33Z", 
    "summary": "The Speex project has been started in 2002 to address the need for a free,\nopen-source speech codec. Speex is based on the Code Excited Linear Prediction\n(CELP) algorithm and, unlike the previously existing Vorbis codec, is optimised\nfor transmitting speech for low latency communication over an unreliable packet\nnetwork. This paper presents an overview of Speex, the technology involved in\nit and how it can be used in applications. The most recent developments in\nSpeex, such as the fixed-point port, acoustic echo cancellation and noise\nsuppression are also addressed."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HSCMA.2008.4538718", 
    "link": "http://arxiv.org/pdf/1603.01824v1", 
    "title": "Low-Complexity Iterative Sinusoidal Parameter Estimation", 
    "arxiv-id": "1603.01824v1", 
    "author": "Timothy B. Terriberry", 
    "publish": "2016-03-06T13:17:27Z", 
    "summary": "Sinusoidal parameter estimation is a computationally-intensive task, which\ncan pose problems for real-time implementations. In this paper, we propose a\nlow-complexity iterative method for estimating sinusoidal parameters that is\nbased on the linearisation of the model around an initial frequency estimate.\nWe show that for N sinusoids in a frame of length L, the proposed method has a\ncomplexity of O(LN), which is significantly less than the matching pursuits\nmethod. Furthermore, the proposed method is shown to be more accurate than the\nmatching pursuits and time frequency reassignment methods in our experiments."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HSCMA.2008.4538718", 
    "link": "http://arxiv.org/pdf/1603.01863v1", 
    "title": "Improved Noise Weighting in CELP Coding of Speech - Applying the Vorbis   Psychoacoustic Model To Speex", 
    "arxiv-id": "1603.01863v1", 
    "author": "Christopher Montgomery", 
    "publish": "2016-03-06T19:19:22Z", 
    "summary": "One key aspect of the CELP algorithm is that it shapes the coding noise using\na simple, yet effective, weighting filter. In this paper, we improve the noise\nshaping of CELP using a more modern psychoacoustic model. This has the\nsignificant advantage of improving the quality of an existing codec without the\nneed to change the bit-stream. More specifically, we improve the Speex CELP\ncodec by using the psychoacoustic model used in the Vorbis audio codec. The\nresults show a significant increase in quality, especially at high bit-rates,\nwhere the improvement is equivalent to a 20% reduction in bit-rate. The\ntechnique itself is not specific to Speex and could be applied to other CELP\ncodecs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2004.1325962", 
    "link": "http://arxiv.org/pdf/1603.03215v1", 
    "title": "Microphone array post-filter for separation of simultaneous   non-stationary sources", 
    "arxiv-id": "1603.03215v1", 
    "author": "Fran\u00e7ois Michaud", 
    "publish": "2016-03-10T10:42:47Z", 
    "summary": "Microphone array post-filters have demonstrated their ability to greatly\nreduce noise at the output of a beamformer. However, current techniques only\nconsider a single source of interest, most of the time assuming stationary\nbackground noise. We propose a microphone array post-filter that enhances the\nsignals produced by the separation of simultaneous sources using common source\nseparation algorithms. Our method is based on a loudness-domain optimal\nspectral estimator and on the assumption that the noise can be described as the\nsum of a stationary component and of a transient component that is due to\nleakage between the channels of the initial source separation algorithm. The\nsystem is evaluated in the context of mobile robotics and is shown to produce\nbetter results than current post-filtering techniques, greatly reducing\ninterference while causing little distortion to the signal of interest, even at\nvery low SNR."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2004.1325962", 
    "link": "http://arxiv.org/pdf/1603.03364v1", 
    "title": "Channel Decorrelation For Stereo Acoustic Echo Cancellation In   High-Quality Audio Communication", 
    "arxiv-id": "1603.03364v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-03-10T18:51:24Z", 
    "summary": "In this paper, we address an important problem in high-quality audio\ncommunication systems. Acoustic echo cancellation with stereo signals is\ngenerally an under-determined problem because of the generally important\ncorrelation that exists between the left and right channels. In this paper, we\npresent a novel method of significantly reducing that correlation without\naffecting the audio quality. This method is perceptually motivated and combines\na shaped comb-allpass (SCAL) filter with the injection of psychoacoustically\nmasked noise. We show that the proposed method performs significantly better\nthan other known methods for channel decorrelation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2004.1325962", 
    "link": "http://arxiv.org/pdf/1603.03947v3", 
    "title": "Spoofing Detection Goes Noisy: An Analysis of Synthetic Speech Detection   in the Presence of Additive Noise", 
    "arxiv-id": "1603.03947v3", 
    "author": "Aleksandr Sizov", 
    "publish": "2016-03-12T17:44:48Z", 
    "summary": "Automatic speaker verification (ASV) technology is recently finding its way\nto end-user applications for secure access to personal data, smart services or\nphysical facilities. Similar to other biometric technologies, speaker\nverification is vulnerable to spoofing attacks where an attacker masquerades as\na particular target speaker via impersonation, replay, text-to-speech (TTS) or\nvoice conversion (VC) techniques to gain illegitimate access to the system. We\nfocus on TTS and VC that represent the most flexible, high-end spoofing\nattacks. Most of the prior studies on synthesized or converted speech detection\nreport their findings using high-quality clean recordings. Meanwhile, the\nperformance of spoofing detectors in the presence of additive noise, an\nimportant consideration in practical ASV implementations, remains largely\nunknown. To this end, we analyze the suitability of state-of-the-art synthetic\nspeech detectors under additive noise with a special focus on front-end\nfeatures. Our comparison includes eight acoustic feature sets, five related to\nspectral magnitude and three to spectral phase information. Our extensive\nexperiments on ASVSpoof 2015 corpus reveal several important findings. Firstly,\nall the countermeasures break down even at relatively high signal-to-noise\nratios (SNRs) and fail to generalize to noisy conditions. Secondly, speech\nenhancement is not found helpful. Thirdly, GMM back-end generally outperforms\nthe more involved i-vector back-end. Fourthly, concerning the compared\nfeatures, the Mel-frequency cepstral coefficients (MFCCs) and subband spectral\ncentroid magnitude coefficients (SCMCs) perform the best on average though the\nwinner method depends on SNR and noise type. Finally, a study with two score\nfusion strategies shows that combining different feature based systems improves\nrecognition accuracy for known and unknown attacks in both clean and noisy\nconditions."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.04979v1", 
    "title": "Guitar Solos as Networks", 
    "arxiv-id": "1603.04979v1", 
    "author": "Stefano Ferretti", 
    "publish": "2016-03-16T06:59:11Z", 
    "summary": "This paper presents an approach to model melodies (and music pieces in\ngeneral) as networks. Notes of a melody can be seen as nodes of a network that\nare connected whenever these are played in sequence. This creates a directed\ngraph. By using complex network theory, it is possible to extract some main\nmetrics, typical of networks, that characterize the piece. Using this\nframework, we provide an analysis on a set of guitar solos performed by main\nmusicians. The results of this study indicate that this model can have an\nimpact on multimedia applications such as music classification, identification,\nand automatic music generation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.05435v1", 
    "title": "Modified Group Delay Based MultiPitch Estimation in Co-Channel Speech", 
    "arxiv-id": "1603.05435v1", 
    "author": "Hema A. Murthy", 
    "publish": "2016-03-17T11:35:09Z", 
    "summary": "Phase processing has been replaced by group delay processing for the\nextraction of source and system parameters from speech. Group delay functions\nare ill-behaved when the transfer function has zeros that are close to unit\ncircle in the z-domain. The modified group delay function addresses this\nproblem and has been successfully used for formant and monopitch estimation. In\nthis paper, modified group delay functions are used for multipitch estimation\nin concurrent speech. The power spectrum of the speech is first flattened in\norder to annihilate the system characteristics, while retaining the source\ncharacteristics. Group delay analysis on this flattened spectrum picks the\npredominant pitch in the first pass and a comb filter is used to filter out the\nestimated pitch along with its harmonics. The residual spectrum is again\nanalyzed for the next candidate pitch estimate in the second pass. The final\npitch trajectories of the constituent speech utterances are formed using pitch\ngrouping and post processing techniques. The performance of the proposed\nalgorithm was evaluated on standard datasets using two metrics; pitch accuracy\nand standard deviation of fine pitch error. Our results show that the proposed\nalgorithm is a promising pitch detection method in multipitch environment for\nreal speech recordings."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.06065v1", 
    "title": "A pairwise approach to simultaneous onset/offset detection for singing   voice using correntropy", 
    "arxiv-id": "1603.06065v1", 
    "author": "Kyogu Lee", 
    "publish": "2016-03-19T08:45:21Z", 
    "summary": "In this paper, we propose a novelmethod to search for precise locations of\npaired note onset and offset in a singing voice signal. In comparison with the\nexisting onset detection algorithms,our approach differs in two key respects.\nFirst, we employ Correntropy, a generalized correlation function inspired from\nReyni's entropy, as a detection function to capture the instantaneous flux\nwhile preserving insensitiveness to outliers. Next, a novel peak picking\nalgorithm is specially designed for this detection function. By calculating the\nfitness of a pre-defined inverse hyperbolic kernel to a detection function, it\nis possible to find an onset and its corresponding offset simultaneously.\nExperimental results show that the proposed method achieves performance\nsignificantly better than or comparable to other state-of-the-art techniques\nfor onset detection in singing voice."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.07173v1", 
    "title": "Deductive Refinement of Species Labelling in Weakly Labelled Birdsong   Recordings", 
    "arxiv-id": "1603.07173v1", 
    "author": "Dan Stowell", 
    "publish": "2016-03-23T13:21:12Z", 
    "summary": "Many approaches have been used in bird species classification from their\nsound in order to provide labels for the whole of a recording. However, a more\nprecise classification of each bird vocalization would be of great importance\nto the use and management of sound archives and bird monitoring. In this work,\nwe introduce a technique that using a two step process can first automatically\ndetect all bird vocalizations and then, with the use of 'weakly' labelled\nrecordings, classify them. Evaluations of our proposed method show that it\nachieves a correct classification of 61% when used in a synthetic dataset, and\nup to 89% when the synthetic dataset only consists of vocalizations larger than\n1000 pixels."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.07236v2", 
    "title": "Individual identity in songbirds: signal representations and metric   learning for locating the information in complex corvid calls", 
    "arxiv-id": "1603.07236v2", 
    "author": "Lisa F. Gill", 
    "publish": "2016-03-23T15:29:39Z", 
    "summary": "Bird calls range from simple tones to rich dynamic multi-harmonic structures.\nThe more complex calls are very poorly understood at present, such as those of\nthe scientifically important corvid family (jackdaws, crows, ravens, etc.).\nIndividual birds can recognise familiar individuals from calls, but where in\nthe signal is this identity encoded? We studied the question by applying a\ncombination of feature representations to a dataset of jackdaw calls, including\nlinear predictive coding (LPC) and adaptive discrete Fourier transform (aDFT).\nWe demonstrate through a classification paradigm that we can strongly\noutperform a standard spectrogram representation for identifying individuals,\nand we apply metric learning to determine which time-frequency regions\ncontribute most strongly to robust individual identification. Computational\nmethods can help to direct our search for understanding of these complex\nbiological signals."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.08740v1", 
    "title": "On the Impact of Localization Errors on HRTF-based Robust Least-Squares   Beamforming", 
    "arxiv-id": "1603.08740v1", 
    "author": "Walter Kellermann", 
    "publish": "2016-03-29T12:21:36Z", 
    "summary": "In this work, a recently proposed Head-Related Transfer Function (HRTF)-based\nRobust Least-Squares Frequency-Invariant (RLSFI) beamformer design is analyzed\nwith respect to its robustness against localization errors, which lead to a\nmismatch between the HRTFs corresponding to the actual target source position\nand the HRTFs which have been used for the beamformer design. The impact of\nthis mismatch on the performance of the HRTF-based RLSFI beamformer is\nevaluated, including a comparison to the free-field-based beamformer design,\nusing signal-based measures and word error rates for an off-the-shelf speech\nrecognizer."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1603.08904v4", 
    "title": "Mathematical Harmony Analysis", 
    "arxiv-id": "1603.08904v4", 
    "author": "David Ryan", 
    "publish": "2016-03-29T19:43:32Z", 
    "summary": "Musical chords, harmonies or melodies in Just Intonation have note\nfrequencies which are described by a base frequency multiplied by rational\nnumbers. For any local section, these notes can be converted to some base\nfrequency multiplied by whole positive numbers. The structure of the chord can\nbe analysed mathematically by finding functions which are unchanged upon chord\ntransposition. These functions are are denoted invariant, and are important for\nunderstanding the structure of harmony. Each chord described by whole numbers\nhas a greatest common divisor, GCD, and a lowest common multiple, LCM. The\nratio of these is denoted Complexity which is a positive whole number. The set\nof divisors of Complexity give a subset of a p limit tone lattice and have both\na natural ordering and a multiplicative structure. The position and orientation\nof the original chord, on the ordered set or on the lattice, give rise to many\nother invariant functions including measures for otonality and utonality. Other\ninvariant functions can be constructed from: ratios between note pairs, prime\nprojections, weighted chords which incorporate loudness. Given a set of\nconditions described by invariant functions, algorithms can be developed to\nfind all scales or chords meeting those conditions, allowing the classification\nof consonant harmonies up to specified limits."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1604.02243v1", 
    "title": "Multifractal Detrended Cross Correlation Analysis A Tool for the   Assessment of Raga in Bollywood Music", 
    "arxiv-id": "1604.02243v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-04-08T07:04:56Z", 
    "summary": "Since the start of Indian cinema, a number of films have been made where a\nparticular song is based on a certain raga. These songs have been taking a\nmajor role in spreading the essence of classical music to the common people,\nwho have no formal exposure to classical music. In this paper, we look to\nexplore what are the particular features of a certain raga which make it\nunderstandable to common people and enrich the song to a great extent. For\nthis, we chose two common ragas of Hindustani classical music, namely Bhairav\nand Mian ki Malhar which are known to have widespread application in popular\nfilm music. We have taken 3 minute clips of these two ragas from the renderings\nof two eminent maestros of Hindustani classical music. 3 min clips of ten (10)\nwidely popular songs of Bollywood films were selected for analysis. These were\nanalyzed with the help of a latest non linear analysis technique called\nMultifractal Detrended Cross correlation Analysis (MFDXA). With this technique,\nall parts of the Film music and the renderings from the eminent maestros are\nanalyzed to find out a cross correlation coefficient ({\\gamma}x) which gives\nthe degree of correlation between these two signals. We hypothesize that the\nparts which have the highest degree of cross correlation are the parts in which\nthat particular raga is established in the song. Also the variation of cross\ncorrelation coefficient in the different parts of the two samples gives a\nmeasure of the modulation that is executed by the singer. Thus, in nutshell we\ntry to study scientifically the amount of correlation that exists between the\nraga and the same raga being utilized in Film music. This will help in\ngenerating an automated algorithm through which a naive listener will relish\nthe flavor of a particular raga in a popular film song. The results are\ndiscussed in detail."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICME.2016.7553001", 
    "link": "http://arxiv.org/pdf/1604.02250v2", 
    "title": "How Do the Singing Styles vary over Generations in different Gharanas of   Hindustani Classical Music A Comparative Non Linear Study", 
    "arxiv-id": "1604.02250v2", 
    "author": "Dipak Ghosh", 
    "publish": "2016-04-08T07:20:38Z", 
    "summary": "Hindustani classical music is entirely based on the Raga structures. In\nHindustani music, a Gharana or school refers to the adherence of a group of\nmusicians to a particular musical style of performing a certain raga. The\nobjective of this work was to find out if any characteristic acoustic cues\nexist which discriminates a particular gharana from the other. Another\nintriguing fact is if the artists of the same gharana keep their singing style\nunchanged over generations or evolution of music takes place like everything\nelse in nature. In this work, we chose to study the similarities and\ndifferences in singing style of some artists from at least four consecutive\ngenerations representing four different gharanas using robust non-linear\nmethods. For this, alap parts of a particular raga sung by all the artists were\nanalyzed with the help of non linear multifractal analysis (MFDFA) technique.\nThe spectral width obtained from the MFDFA method gives an estimate of the\ncomplexity of the signal. The observations give a cue in the direction to the\nscientific recognition of guru-shisya parampara (teacher-student tradition) a\nhitherto much-heard philosophical term. Moreover the variation in the\ncomplexity patterns among various gharanas will give a hint of the\ncharacteristic feature of that particular gharana as well as the effect of\nglobalization in the field of classical music happening through past few\ndecades."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1604.03393v3", 
    "title": "Robust coherence-based spectral enhancement for speech recognition in   adverse real-world environments", 
    "arxiv-id": "1604.03393v3", 
    "author": "Walter Kellermann", 
    "publish": "2016-04-12T13:11:12Z", 
    "summary": "Speech recognition in adverse real-world environments is highly affected by\nreverberation and nonstationary background noise. A well-known strategy to\nreduce such undesired signal components in multi-microphone scenarios is\nspatial filtering of the microphone signals. In this article, we demonstrate\nthat an additional coherence-based postfilter, which is applied to the\nbeamformer output signal to remove diffuse interference components from the\nlatter, is an effective means to further improve the recognition accuracy of\nmodern deep learning speech recognition systems. To this end, the recently\nupdated 3rd CHiME Speech Separation and Recognition Challenge (CHiME-3)\nbaseline speech recognition system is extended by a coherence-based postfilter\nand the postfilter's impact on the word error rates is investigated for the\nnoisy environments provided by CHiME-3. To determine the time- and\nfrequency-dependent postfilter gains, we use a Direction-of-Arrival\n(DOA)-dependent and a DOA-independent estimator of the coherent-to-diffuse\npower ratio as an approximation of the short-time signal-to-noise ratio. Our\nexperiments show that incorporating coherence-based postfiltering into the\nCHiME-3 baseline speech recognition system leads to a significant reduction of\nthe word error rate scores for the noisy and reverberant environments provided\nas part of CHiME-3."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1604.04669v1", 
    "title": "Two Pairwise Iterative Schemes For High Dimensional Blind Source   Separation", 
    "arxiv-id": "1604.04669v1", 
    "author": "Fathi M. Salem", 
    "publish": "2016-04-16T00:26:59Z", 
    "summary": "This paper addresses the high dimensionality problem in blind source\nseparation (BSS), where the number of sources is greater than two. Two pairwise\niterative schemes are proposed to tackle this high dimensionality problem. The\ntwo pairwise schemes realize nonparametric independent component analysis (ICA)\nalgorithms based on a new high-performance Convex CauchySchwarz Divergence\n(CCSDIV). These two schemes enable fast and efficient demixing of sources in\nreal-world high dimensional source applications. Finally, the performance\nsuperiority of the proposed schemes is demonstrated in metric-comparison with\nFastICA, RobustICA, convex ICA (CICA), and other leading existing algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1604.08516v1", 
    "title": "Robust Joint Alignment of Multiple Versions of a Piece of Music", 
    "arxiv-id": "1604.08516v1", 
    "author": "Simon Dixon", 
    "publish": "2016-04-28T17:03:27Z", 
    "summary": "Large music content libraries often comprise multiple versions of a piece of\nmusic. To establish a link between different versions, automatic music\nalignment methods map each position in one version to a corresponding position\nin another version. Due to the leeway in interpreting a piece, any two versions\ncan differ significantly, for example, in terms of local tempo, articulation,\nor playing style. For a given pair of versions, these differences can be\nsignificant such that even state-of-the-art methods fail to identify a correct\nalignment. In this paper, we present a novel method that increases the\nrobustness for difficult to align cases. Instead of aligning only pairs of\nversions as done in previous methods, our method aligns multiple versions in a\njoint manner. This way, the alignment can be computed by comparing each version\nnot only with one but with several versions, which stabilizes the comparison\nand leads to an increase in alignment robustness. Using recordings from the\nMazurka Project, the alignment error for our proposed method was 14% lower on\naverage compared to a state-of-the-art method, with significantly less outliers\n(standard deviation 53% lower)."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1605.00810v1", 
    "title": "Diagonal Unloading Beamforming for Source Localization", 
    "arxiv-id": "1605.00810v1", 
    "author": "Gian Luca Foresti", 
    "publish": "2016-05-03T09:31:09Z", 
    "summary": "In sensor array beamforming methods, a class of algorithms commonly used to\nestimate the position of a radiating source, the diagonal loading of the\nbeamformer covariance matrix is generally used to improve computational\naccuracy and localization robustness. This paper proposes a diagonal unloading\n(DU) method which extends the conventional response power beamforming method by\nimposing an additional constraint to the covariance matrix of the array output\nvector. The regularization is obtained by subtracting a given amount of white\nnoise from the main diagonal of the covariance matrix. Specifically, the DU\nbeamformer aims at subtracting the signal subspace from the noisy signal space\nand it is computed by constraining the regularized covariance matrix to be\nnegative definite. It is hence a data-dependent covariance matrix conditioning\nmethod. We show how to calculate precisely the unloading parameter, and we\npresent an eigenvalue analysis for comparing the proposed DU beamforming, the\nminimum variance distortionless response (MVDR) filter and the multiple signal\nclassification (MUSIC) method. Theoretical analysis and experiments with\nacoustic sources demonstrate that the DU beamformer localization performance is\ncomparable to that of MVDR and MUSIC. Since the DU beamformer computational\ncost is comparable to that of a conventional beamformer, the proposed method\ncan be attractive in array processing due to its simplicity, effectiveness and\ncomputational efficiency."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1605.02427v1", 
    "title": "Speech Enhancement In Multiple-Noise Conditions using Deep Neural   Networks", 
    "arxiv-id": "1605.02427v1", 
    "author": "Dinei Florencio", 
    "publish": "2016-05-09T06:13:37Z", 
    "summary": "In this paper we consider the problem of speech enhancement in real-world\nlike conditions where multiple noises can simultaneously corrupt speech. Most\nof the current literature on speech enhancement focus primarily on presence of\nsingle noise in corrupted speech which is far from real-world environments.\nSpecifically, we deal with improving speech quality in office environment where\nmultiple stationary as well as non-stationary noises can be simultaneously\npresent in speech. We propose several strategies based on Deep Neural Networks\n(DNN) for speech enhancement in these scenarios. We also investigate a DNN\ntraining strategy based on psychoacoustic models from speech coding for\nenhancement of noisy speech"
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1605.03724v1", 
    "title": "Sub-vector Extraction and Cascade Post-Processing for Speaker   Verification Using MLLR Super-vectors", 
    "arxiv-id": "1605.03724v1", 
    "author": "D. Matrouf", 
    "publish": "2016-05-12T08:44:47Z", 
    "summary": "In this paper, we propose a speaker-verification system based on maximum\nlikelihood linear regression (MLLR) super-vectors, for which speakers are\ncharacterized by m-vectors. These vectors are obtained by a uniform\nsegmentation of the speaker MLLR super-vector using an overlapped sliding\nwindow. We consider three approaches for MLLR transformation, based on the\nconventional $1$-best automatic transcription, on the lattice word\ntranscription, or on a simple global universal background model (UBM). Session\nvariability compensation is performed in a post-processing module with\nprobabilistic linear discriminant analysis (PLDA) or the eigen factor radial\n(EFR). Alternatively, we propose a cascade post-processing for the MLLR\nsuper-vector based speaker-verification system.\n  In this case, the m-vectors or MLLR super-vectors are first projected onto a\nlower-dimensional vector space generated by linear discriminant analysis (LDA).\nNext, PLDA session variability compensation and scoring is applied to the\nreduced-dimensional vectors. This approach combines the advantages of both\ntechniques and makes the estimation of PLDA parameters easier. Experimental\nresults on telephone conversations of the NIST 2008 and 2010 speaker\nrecognition evaluation (SRE) indicate that the proposed m-vector system\nperforms significantly better than the conventional system based on the full\nMLLR super-vectors. Cascade post-processing further reduces the error rate in\nall cases. Finally, we present the results of fusion with a standard i-vector\nsystem in the feature, as well as in the score domain, demonstrating that the\nm-vector system is both competitive and complementary with it."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1605.06644v3", 
    "title": "Deep convolutional networks on the pitch spiral for musical instrument   recognition", 
    "arxiv-id": "1605.06644v3", 
    "author": "Carmine-Emanuele Cella", 
    "publish": "2016-05-21T13:49:33Z", 
    "summary": "Musical performance combines a wide range of pitches, nuances, and expressive\ntechniques. Audio-based classification of musical instruments thus requires to\nbuild signal representations that are invariant to such transformations. This\narticle investigates the construction of learned convolutional architectures\nfor instrument recognition, given a limited amount of annotated training data.\nIn this context, we benchmark three different weight sharing strategies for\ndeep convolutional networks in the time-frequency domain: temporal kernels;\ntime-frequency kernels; and a linear combination of time-frequency kernels\nwhich are one octave apart, akin to a Shepard pitch spiral. We provide an\nacoustical interpretation of these strategies within the source-filter\nframework of quasi-harmonic sounds with a fixed spectral envelope, which are\narchetypal of musical notes. The best classification accuracy is obtained by\nhybridizing all three convolutional layers into a single deep learning\narchitecture."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2017.02.005", 
    "link": "http://arxiv.org/pdf/1605.07008v1", 
    "title": "madmom: a new Python Audio and Music Signal Processing Library", 
    "arxiv-id": "1605.07008v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-05-23T13:29:09Z", 
    "summary": "In this paper, we present madmom, an open-source audio processing and music\ninformation retrieval (MIR) library written in Python. madmom features a\nconcise, NumPy-compatible, object oriented design with simple calling\nconventions and sensible default values for all parameters, which facilitates\nfast prototyping of MIR applications. Prototypes can be seamlessly converted\ninto callable processing pipelines through madmom's concept of Processors,\ncallable objects that run transparently on multiple cores. Processors can also\nbe serialised, saved, and re-run to allow results to be easily reproduced\nanywhere. Apart from low-level audio processing, madmom puts emphasis on\nmusically meaningful high-level features. Many of these incorporate machine\nlearning techniques and madmom provides a module that implements some in MIR\ncommonly used methods such as hidden Markov models and neural networks.\nAdditionally, madmom comes with several state-of-the-art MIR algorithms for\nonset detection, beat, downbeat and meter tracking, tempo estimation, and piano\ntranscription. These can easily be incorporated into bigger MIR systems or run\nas stand-alone programs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2016.7471634", 
    "link": "http://arxiv.org/pdf/1605.07466v1", 
    "title": "Complex NMF under phase constraints based on signal modeling:   application to audio source separation", 
    "arxiv-id": "1605.07466v1", 
    "author": "Bertrand David", 
    "publish": "2016-05-24T14:04:48Z", 
    "summary": "Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing\nmixtures of audio signals in the Time-Frequency (TF) domain. In the source\nseparation framework, the phase recovery for each extracted component is\nnecessary for synthesizing time-domain signals. The Complex NMF (CNMF) model\naims to jointly estimate the spectrogram and the phase of the sources, but\nrequires to constrain the phase in order to produce satisfactory sounding\nresults. We propose to incorporate phase constraints based on signal models\nwithin the CNMF framework: a \\textit{phase unwrapping} constraint that enforces\na form of temporal coherence, and a constraint based on the \\textit{repetition}\nof audio events, which models the phases of the sources within onset frames. We\nalso provide an algorithm for estimating the model parameters. The experimental\nresults highlight the interest of including such constraints in the CNMF\nframework for separating overlapping components in complex audio mixtures."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2016.7471634", 
    "link": "http://arxiv.org/pdf/1605.07467v1", 
    "title": "Phase reconstruction of spectrograms with linear unwrapping: application   to audio signal restoration", 
    "arxiv-id": "1605.07467v1", 
    "author": "Bertrand David", 
    "publish": "2016-05-24T14:05:14Z", 
    "summary": "This paper introduces a novel technique for reconstructing the phase of\nmodified spectrograms of audio signals. From the analysis of mixtures of\nsinusoids we obtain relationships between phases of successive time frames in\nthe Time-Frequency (TF) domain. To obtain similar relationships over\nfrequencies, in particular within onset frames, we study an impulse model.\nInstantaneous frequencies and attack times are estimated locally to encompass\nthe class of non-stationary signals such as vibratos. These techniques ensure\nboth the vertical coherence of partials (over frequencies) and the horizontal\ncoherence (over time). The method is tested on a variety of data and\ndemonstrates better performance than traditional consistency-based approaches.\nWe also introduce an audio restoration framework and observe that our technique\noutperforms traditional methods."
},{
    "category": "cs.SD", 
    "doi": "10.1109/WASPAA.2015.7336935", 
    "link": "http://arxiv.org/pdf/1605.07468v1", 
    "title": "Phase reconstruction of spectrograms based on a model of repeated audio   events", 
    "arxiv-id": "1605.07468v1", 
    "author": "Bertrand David", 
    "publish": "2016-05-24T14:05:35Z", 
    "summary": "Phase recovery of modified spectrograms is a major issue in audio signal\nprocessing applications, such as source separation. This paper introduces a\nnovel technique for estimating the phases of components in complex mixtures\nwithin onset frames in the Time-Frequency (TF) domain. We propose to exploit\nthe phase repetitions from one onset frame to another. We introduce a reference\nphase which characterizes a component independently of its activation times.\nThe onset phases of a component are then modeled as the sum of this reference\nand an offset which is linearly dependent on the frequency. We derive a complex\nmixture model within onset frames and we provide two algorithms for the\nestimation of the model phase parameters. The model is estimated on\nexperimental data and this technique is integrated into an audio source\nseparation framework. The results demonstrate that this model is a promising\ntool for exploiting phase repetitions, and point out its potential for\nseparating overlapping components in complex mixtures."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1605.07469v1", 
    "title": "Phase recovery in NMF for audio source separation: an insightful   benchmark", 
    "arxiv-id": "1605.07469v1", 
    "author": "Bertrand David", 
    "publish": "2016-05-24T14:05:51Z", 
    "summary": "Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing\nmixtures of audio signals in the Time-Frequency (TF) domain. In applications\nsuch as source separation, the phase recovery for each extracted component is a\nmajor issue since it often leads to audible artifacts. In this paper, we\npresent a methodology for evaluating various NMF-based source separation\ntechniques involving phase reconstruction. For each model considered, a\ncomparison between two approaches (blind separation without prior information\nand oracle separation with supervised model learning) is performed, in order to\ninquire about the room for improvement for the estimation methods. Experimental\nresults show that the High Resolution NMF (HRNMF) model is particularly\npromising, because it is able to take phases and correlations over time into\naccount with a great expressive power."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1605.07809v2", 
    "title": "Using instantaneous frequency and aperiodicity detection to estimate F0   for high-quality speech synthesis", 
    "arxiv-id": "1605.07809v2", 
    "author": "Heiga Zen", 
    "publish": "2016-05-25T10:20:07Z", 
    "summary": "This paper introduces a general and flexible framework for F0 and\naperiodicity (additive non periodic component) analysis, specifically intended\nfor high-quality speech synthesis and modification applications. The proposed\nframework consists of three subsystems: instantaneous frequency estimator and\ninitial aperiodicity detector, F0 trajectory tracker, and F0 refinement and\naperiodicity extractor. A preliminary implementation of the proposed framework\nsubstantially outperformed (by a factor of 10 in terms of RMS F0 estimation\nerror) existing F0 extractors in tracking ability of temporally varying F0\ntrajectories. The front end aperiodicity detector consists of a complex-valued\nwavelet analysis filter with a highly selective temporal and spectral envelope.\nThis front end aperiodicity detector uses a new measure that quantifies the\ndeviation from periodicity. The measure is less sensitive to slow FM and AM and\nclosely correlates with the signal to noise ratio."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1605.08450v1", 
    "title": "The Implementation of Low-cost Urban Acoustic Monitoring Devices", 
    "arxiv-id": "1605.08450v1", 
    "author": "Juan Pablo Bello", 
    "publish": "2016-05-26T20:41:28Z", 
    "summary": "The urban sound environment of New York City (NYC) can be, amongst other\nthings: loud, intrusive, exciting and dynamic. As indicated by the large\nmajority of noise complaints registered with the NYC 311 information/complaints\nline, the urban sound environment has a profound effect on the quality of life\nof the city's inhabitants. To monitor and ultimately understand these sonic\nenvironments, a process of long-term acoustic measurement and analysis is\nrequired. The traditional method of environmental acoustic monitoring utilizes\nshort term measurement periods using expensive equipment, setup and operated by\nexperienced and costly personnel. In this paper a different approach is\nproposed to this application which implements a smart, low-cost, static,\nacoustic sensing device based around consumer hardware. These devices can be\ndeployed in numerous and varied urban locations for long periods of time,\nallowing for the collection of longitudinal urban acoustic data. The varied\nenvironmental conditions of urban settings make for a challenge in gathering\ncalibrated sound pressure level data for prospective stakeholders. This paper\ndetails the sensors' design, development and potential future applications,\nwith a focus on the calibration of the devices' Microelectromechanical systems\n(MEMS) microphone in order to generate reliable decibel levels at the\ntype/class 2 level."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.00037v1", 
    "title": "Nonnegative tensor factorization with frequency modulation cues for   blind audio source separation", 
    "arxiv-id": "1606.00037v1", 
    "author": "Philippe Depalle", 
    "publish": "2016-05-31T20:40:50Z", 
    "summary": "We present Vibrato Nonnegative Tensor Factorization, an algorithm for\nsingle-channel unsupervised audio source separation with an application to\nseparating instrumental or vocal sources with nonstationary pitch from music\nrecordings. Our approach extends Nonnegative Matrix Factorization for audio\nmodeling by including local estimates of frequency modulation as cues in the\nseparation. This permits the modeling and unsupervised separation of vibrato or\nglissando musical sources, which is not possible with the basic matrix\nfactorization formulation.\n  The algorithm factorizes a sparse nonnegative tensor comprising the audio\nspectrogram and local frequency-slope-to-frequency ratios, which are estimated\nat each time-frequency bin using the Distributed Derivative Method. The use of\nlocal frequency modulations as separation cues is motivated by the principle of\ncommon fate partial grouping from Auditory Scene Analysis, which hypothesizes\nthat each latent source in a mixture is characterized perceptually by coherent\nfrequency and amplitude modulations shared by its component partials. We derive\nmultiplicative factor updates by Minorization-Maximization, which guarantees\nconvergence to a local optimum by iteration. We then compare our method to the\nbaseline on two separation tasks: one considers synthetic vibrato notes, while\nthe other considers vibrato string instrument recordings."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.01368v1", 
    "title": "Modelling Symbolic Music: Beyond the Piano Roll", 
    "arxiv-id": "1606.01368v1", 
    "author": "Christian Walder", 
    "publish": "2016-06-04T10:51:24Z", 
    "summary": "In this paper, we consider the problem of probabilistically modelling\nsymbolic music data. We introduce a representation which reduces polyphonic\nmusic to a univariate categorical sequence. In this way, we are able to apply\nstate of the art natural language processing techniques, namely the long\nshort-term memory sequence model. The representation we employ permits\narbitrary rhythmic structure, which we assume to be given. We show that our\nmodel is effective on four out of four piano roll based benchmark datasets. We\nfurther improve our model by augmenting our training data set with\ntranspositions of the original pieces through all musical keys, thereby\nconvincingly advancing the state of the art on these benchmark problems. We\nalso fit models to music which is unconstrained in its rhythmic structure,\ndiscuss the properties of this model, and provide musical samples which are\nmore sophisticated than previously possible with this class of recurrent neural\nnetwork sequence models. We also provide our newly preprocessed data set of non\npiano-roll music data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.03365v2", 
    "title": "Acoustic Characterization of Environments (ACE) Challenge Results   Technical Report", 
    "arxiv-id": "1606.03365v2", 
    "author": "Patrick A. Naylor", 
    "publish": "2015-12-17T10:40:22Z", 
    "summary": "This document provides the results of the tests of acoustic parameter\nestimation algorithms on the Acoustic Characterization of Environments (ACE)\nChallenge Evaluation dataset which were subsequently submitted and written up\ninto papers for the Proceedings of the ACE Challenge. This document is\nsupporting material for a forthcoming journal paper on the ACE Challenge which\nwill provide further analysis of the results."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.06258v1", 
    "title": "Uncalibrated 3D Room Reconstruction from Sound", 
    "arxiv-id": "1606.06258v1", 
    "author": "Alessio Del Bue", 
    "publish": "2016-06-20T19:21:46Z", 
    "summary": "This paper presents a method to reconstruct the 3D structure of generic\nconvex rooms from sound signals. Differently from most of the previous\napproaches, the method is fully uncalibrated in the sense that no knowledge\nabout the microphones and sources position is needed. Moreover, we demonstrate\nthat it is possible to bypass the well known echo labeling problem, allowing to\nreconstruct the room shape in a reasonable computation time without the need of\nadditional hypotheses on the echoes order of arrival. Finally, the method is\nintrinsically robust to outliers and missing data in the echoes detection,\nallowing to work also in low SNR conditions. The proposed pipeline formalises\nthe problem in different steps such as time of arrival estimation, microphones\nand sources localization and walls estimation. After providing a solution to\nthese different problems we present a global optimization approach that links\ntogether all the problems in a single optimization function. The accuracy and\nrobustness of the method is assessed on a wide set of simulated setups and in a\nchallenging real scenario. Moreover we make freely available for a challenging\ndataset for 3D room reconstruction with accurate ground truth in a real\nscenario."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.07136v1", 
    "title": "A Speaker Diarization System for Studying Peer-Led Team Learning Groups", 
    "arxiv-id": "1606.07136v1", 
    "author": "John H. L. Hansen", 
    "publish": "2016-06-22T23:35:06Z", 
    "summary": "Peer-led team learning (PLTL) is a model for teaching STEM courses where\nsmall student groups meet periodically to collaboratively discuss coursework.\nAutomatic analysis of PLTL sessions would help education researchers to get\ninsight into how learning outcomes are impacted by individual participation,\ngroup behavior, team dynamics, etc.. Towards this, speech and language\ntechnology can help, and speaker diarization technology will lay the foundation\nfor analysis. In this study, a new corpus is established called CRSS-PLTL, that\ncontains speech data from 5 PLTL teams over a semester (10 sessions per team\nwith 5-to-8 participants in each team). In CRSS-PLTL, every participant wears a\nLENA device (portable audio recorder) that provides multiple audio recordings\nof the event. Our proposed solution is unsupervised and contains a new online\nspeaker change detection algorithm, termed G 3 algorithm in conjunction with\nHausdorff-distance based clustering to provide improved detection accuracy.\nAdditionally, we also exploit cross channel information to refine our\ndiarization hypothesis. The proposed system provides good improvements in\ndiarization error rate (DER) over the baseline LIUM system. We also present\nhigher level analysis such as the number of conversational turns taken in a\nsession, and speaking-time duration (participation) for each speaker."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1606.07598v1", 
    "title": "An Active Machine Hearing System for Auditory Stream Segregation", 
    "arxiv-id": "1606.07598v1", 
    "author": "Dorothea Kolossa", 
    "publish": "2016-06-24T08:28:41Z", 
    "summary": "This study describes a binaural machine hearing system that is capable of\nperforming auditory stream segregation in scenarios where multiple sound\nsources are present. The process of stream segregation refers to the capability\nof human listeners to group acoustic signals into sets of distinct auditory\nstreams, corresponding to individual sound sources. The proposed computational\nframework mimics this ability via a probabilistic clustering scheme for joint\nlocalization and segregation. This scheme is based on mixtures of von Mises\ndistributions to model the angular positions of the sound sources surrounding\nthe listener. The distribution parameters are estimated using block-wise\nprocessing of auditory cues extracted from binaural signals. Additionally, the\nproposed system can conduct rotational head movements to improve localization\nand stream segregation performance. Evaluation of the system is conducted in\nscenarios containing multiple simultaneously active speech and non-speech\nsounds placed at different positions relative to the listener."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.00211v1", 
    "title": "Spherical Harmonic Signal Covariance and Sound Field Diffuseness", 
    "arxiv-id": "1607.00211v1", 
    "author": "Craig T. Jin", 
    "publish": "2016-07-01T11:49:40Z", 
    "summary": "Characterizing sound field diffuseness has many practical applications, from\nroom acoustics analysis to speech enhancement and sound field reproduction. In\nthis paper we investigate how spherical microphone arrays (SMAs) can be used to\ncharacterize diffuseness. Due to their specific geometry, SMAs are particularly\nwell suited for analyzing the spatial properties of sound fields. In\nparticular, the signals recorded by an SMA can be analyzed in the spherical\nharmonic (SH) domain, which has special and desirable mathematical properties\nwhen it comes to analyzing diffuse sound fields. We present a new measure of\ndiffuseness, the COMEDIE diffuseness estimate, which is based on the analysis\nof the SH signal covariance matrix. This algorithm is suited for the estimation\nof diffuseness arising either from the presence of multiple sources distributed\naround the SMA or from the presence of a diffuse noise background. As well, we\nintroduce the concept of a diffuseness profile, which consists in measuring the\ndiffuseness for several SH orders simultaneously. Experimental results indicate\nthat diffuseness profiles better describe the properties of the sound field\nthan a single diffuseness measurement."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.02383v1", 
    "title": "Acoustic scene classification using convolutional neural network and   multiple-width frequency-delta data augmentation", 
    "arxiv-id": "1607.02383v1", 
    "author": "Kyogu Lee", 
    "publish": "2016-07-08T14:33:58Z", 
    "summary": "In recent years, neural network approaches have shown superior performance to\nconventional hand-made features in numerous application areas. In particular,\nconvolutional neural networks (ConvNets) exploit spatially local correlations\nacross input data to improve the performance of audio processing tasks, such as\nspeech recognition, musical chord recognition, and onset detection. Here we\napply ConvNet to acoustic scene classification, and show that the error rate\ncan be further decreased by using delta features in the frequency domain. We\npropose a multiple-width frequency-delta (MWFD) data augmentation method that\nuses static mel-spectrogram and frequency-delta features as individual input\nexamples. In addition, we describe a ConvNet output aggregation method designed\nfor MWFD augmentation, folded mean aggregation, which combines output\nprobabilities of static and MWFD features from the same analysis window using\nmultiplication first, rather than taking an average of all output\nprobabilities. We describe calculation results using the DCASE 2016 challenge\ndataset, which shows that ConvNet outperforms both of the baseline system with\nhand-crafted features and a deep neural network approach by around 7%. The\nperformance was further improved (by 5.7%) using the MWFD augmentation together\nwith folded mean aggregation. The system exhibited a classification accuracy of\n0.831 when classifying 15 acoustic scenes."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.06642v2", 
    "title": "HRTF-based Robust Least-Squares Frequency-Invariant Polynomial   Beamforming", 
    "arxiv-id": "1607.06642v2", 
    "author": "Walter Kellermann", 
    "publish": "2016-07-22T11:56:52Z", 
    "summary": "In this work, we propose a robust Head-Related Transfer Function (HRTF)-based\npolynomial beamformer design which accounts for the influence of a humanoid\nrobot's head on the sound field. In addition, it allows for a flexible steering\nof our previously proposed robust HRTF-based beamformer design. We evaluate the\nHRTF-based polynomial beamformer design and compare it to the original\nHRTF-based beamformer design by means of signal-independent measures as well as\nword error rates of an off-the-shelf speech recognition system. Our results\nconfirm the effectiveness of the polynomial beamformer design, which makes it a\npromising approach to robust beamforming for robot audition."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.06706v2", 
    "title": "Experiments on the DCASE Challenge 2016: Acoustic Scene Classification   and Sound Event Detection in Real Life Recording", 
    "arxiv-id": "1607.06706v2", 
    "author": "Ian Lane", 
    "publish": "2016-07-22T15:15:53Z", 
    "summary": "In this paper we present our work on Task 1 Acoustic Scene Classi- fication\nand Task 3 Sound Event Detection in Real Life Recordings. Among our experiments\nwe have low-level and high-level features, classifier optimization and other\nheuristics specific to each task. Our performance for both tasks improved the\nbaseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9%\ncompared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based\nError Rate of 0.76 compared to the baseline of 0.91."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.07801v1", 
    "title": "ABROA : Audio-Based Room-Occupancy Analysis using Gaussian Mixtures and   Hidden Markov Models", 
    "arxiv-id": "1607.07801v1", 
    "author": "Rafael Valle", 
    "publish": "2016-06-22T05:51:40Z", 
    "summary": "This paper outlines preliminary steps towards the development of an audio-\nbased room-occupancy analysis model. Our approach borrows from speech\nrecognition tradition and is based on Gaussian Mixtures and Hidden Markov\nModels. We analyze possible challenges encountered in the development of such a\nmodel, and offer several solutions including feature design and prediction\nstrategies. We provide results obtained from experiments with audio data from a\nretail store in Palo Alto, California. Model assessment is done via\nleave-two-out Bootstrap and model convergence achieves good accuracy, thus\nrepresenting a contribution to multimodal people counting algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1607.08482v1", 
    "title": "Early and Late Time Acoustic Measures for Underwater Seismic Airgun   Signals In Long-Term Acoustic Data Sets", 
    "arxiv-id": "1607.08482v1", 
    "author": "Christopher W. Clark", 
    "publish": "2016-05-05T18:03:41Z", 
    "summary": "This work presents a new toolkit for describing the acoustic properties of\nthe ocean environment before, during and after a sound event caused by an\nunderwater seismic air-gun. The toolkit uses existing sound measures, but\nuniquely applies these to capture the early time period (actual pulse) and late\ntime period (reverberation and multiple arrivals). In total, 183 features are\nproduced for each air-gun sound. This toolkit was utilized on data retrieved\nfrom a field deployment encompassing five marine autonomous recording units\nduring a 46-day seismic air-gun survey in Baffin Bay, Greenland. Using this\ntoolkit, a total of 147 million data points were identified from the Greenland\ndeployment recordings. The feasibility of extracting a large number of features\nwas then evaluated using two separate methods: a serial computer and a high\nperformance system. Results indicate that data extraction performance took an\nestimated 216 hours for the serial system, and 18 hours for the high\nperformance computer. This paper provides an analytical description of the new\ntoolkit along with details for using it to identify relevant data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1608.01844v2", 
    "title": "L\u00e9vy NMF for robust nonnegative source separation", 
    "arxiv-id": "1608.01844v2", 
    "author": "Antoine Liutkus", 
    "publish": "2016-08-05T11:48:58Z", 
    "summary": "Source separation, which consists in decomposing data into meaningful\nstructured components, is an active research topic in many areas, such as music\nand image signal processing, applied physics and text mining. In this paper, we\nintroduce the Positive $\\alpha$-stable (P$\\alpha$S) distributions to model the\nlatent sources, which are a subclass of the stable distributions family. They\nnotably permit us to model random variables that are both nonnegative and\nimpulsive. Considering the L\\'evy distribution, the only P$\\alpha$S\ndistribution whose density is tractable, we propose a mixture model called\nL\\'evy Nonnegative Matrix Factorization (L\\'evy NMF). This model accounts for\nlow-rank structures in nonnegative data that possibly has high variability or\nis corrupted by very adverse noise. The model parameters are estimated in a\nmaximum-likelihood sense. We also derive an estimator of the sources given the\nparameters, which extends the validity of the generalized Wiener filtering to\nthe P$\\alpha$S case. Experiments on synthetic data show that L\\'evy NMF\ncompares favorably with state-of-the art techniques in terms of robustness to\nimpulsive noise. The analysis of two types of realistic signals is also\nconsidered: musical spectrograms and fluorescence spectra of chemical species.\nThe results highlight the potential of the L\\'evy NMF model for decomposing\nnonnegative data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1608.01953v1", 
    "title": "Phase recovery by unwrapping: applications to music signal processing", 
    "arxiv-id": "1608.01953v1", 
    "author": "Bertrand David", 
    "publish": "2016-08-05T17:45:05Z", 
    "summary": "This paper introduces a novel technique for reconstructing the phase of\nmodified spectrograms of audio signals. From the analysis of mixtures of\nsinusoids we obtain relationships between phases of successive time frames in\nthe Time-Frequency (TF) domain. Instantaneous frequencies are estimated locally\nto encompass the class of non-stationary signals such as vibratos. This\ntechnique ensures the horizontal coherence (over time) of the partials. The\nmethod is tested on a variety of data and demonstrates better performance than\ntraditional consistency-based approaches. We also introduce an audio\nrestoration framework and obtain results that compete with other\nstate-of-the-art methods. Finally, we apply this phase recovery method to an\naudio source separation task where the spectrograms of the isolated components\nare known. We propose to use the phase unwrapping estimate to initialize a\nsource separation iterative procedure. Experiments conducted on realistic music\npieces demonstrate the effectiveness of such a method for various music signal\nprocessing tasks."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7177936", 
    "link": "http://arxiv.org/pdf/1608.03417v1", 
    "title": "Bird detection in audio: a survey and a challenge", 
    "arxiv-id": "1608.03417v1", 
    "author": "Herv\u00e9 Glotin", 
    "publish": "2016-08-11T11:01:21Z", 
    "summary": "Many biological monitoring projects rely on acoustic detection of birds.\nDespite increasingly large datasets, this detection is often manual or\nsemi-automatic, requiring manual tuning/postprocessing. We review the state of\nthe art in automatic bird sound detection, and identify a widespread need for\ntuning-free and species-agnostic approaches. We introduce new datasets and an\nIEEE research challenge to address this need, to make possible the development\nof fully automatic algorithms for bird sound detection."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1608.05179v2", 
    "title": "Improving the Efficiency of DAMAS for Sound Source Localization via   Wavelet Compression Computational Grid", 
    "arxiv-id": "1608.05179v2", 
    "author": "Xun Liu", 
    "publish": "2016-08-18T05:16:38Z", 
    "summary": "Phased microphone arrays are used widely in the applications for acoustic\nsource localization. Deconvolution approaches such as DAMAS successfully\novercome the spatial resolution limit of the conventional delay-and-sum (DAS)\nbeamforming method. However deconvolution approaches require high computational\neffort compared to conventional DAS beamforming method. This paper presents a\nnovel method that serves to improve the efficiency of DAMAS via wavelet\ncompression computational grid rather than via optimizing DAMAS algorithm. In\nthis method, the efficiency of DAMAS increases with compression ratio. This\nmethod can thus save lots of run time in industrial applications for sound\nsource localization, particularly when sound sources are just located in a\nsmall extent compared with scanning plane and a band of angular frequency needs\nto be calculated. In addition, this method largely retains the spatial\nresolution of DAMAS on original computational grid, although with a minor\ndeficiency that the occurrence probability of aliasing increasing slightly for\ncomplicated sound source."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1608.07713v1", 
    "title": "Diffuse-field coherence of sensors with arbitrary directional responses", 
    "arxiv-id": "1608.07713v1", 
    "author": "Archontis Politis", 
    "publish": "2016-08-27T14:23:51Z", 
    "summary": "Knowledge of the diffuse-field coherence between array sensors is a basic\nassumption for a wide range of array processing applications. Explicit\nrelations previously existed only for omnidirectional and first-order\ndirectional sensors, or a restricted arrangement of differential patterns. We\npresent a closed-form formulation of the theoretical coherence function between\narbitrary directionally band-limited sensors for the general cases that a) the\nresponses of the individual sensors are known or estimated, and the coherence\nneeds to be known for an arbitrary arrangement, and b) that no information on\nthe sensor directionality or on array geometry exists, but calibration\nmeasurements around the array are available."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.00291v1", 
    "title": "A Non-iterative Method for (Re)Construction of Phase from STFT Magnitude", 
    "arxiv-id": "1609.00291v1", 
    "author": "Peter L. S\u00f8ndergaard", 
    "publish": "2016-09-01T15:52:58Z", 
    "summary": "A non-iterative method for the construction of the Short-Time Fourier\nTransform (STFT) phase from the magnitude is presented. The method is based on\nthe direct relationship between the partial derivatives of the phase and the\nlogarithm of the magnitude of the un-sampled STFT with respect to the Gaussian\nwindow. Although the theory holds in the continuous setting only, the\nexperiments show that the algorithm performs well even in the discretized\nsetting (Discrete Gabor transform) with low redundancy using the sampled\nGaussian window, the truncated Gaussian window and even other compactly\nsupported windows like the Hann window.\n  Due to the non-iterative nature, the algorithm is very fast and it is\nsuitable for long audio signals. Moreover, solutions of iterative phase\nreconstruction algorithms can be improved considerably by initializing them\nwith the phase estimate provided by the present algorithm.\n  We present an extensive comparison with the state-of-the-art algorithms in a\nreproducible manner."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.01678v2", 
    "title": "Discriminative Enhancement for Single Channel Audio Source Separation   using Deep Neural Networks", 
    "arxiv-id": "1609.01678v2", 
    "author": "Mark D. Plumbley", 
    "publish": "2016-09-06T18:03:33Z", 
    "summary": "The sources separated by most single channel audio source separation\ntechniques are usually distorted and each separated source contains residual\nsignals from the other sources. To tackle this problem, we propose to enhance\nthe separated sources to decrease the distortion and interference between the\nseparated sources using deep neural networks (DNNs). Two different DNNs are\nused in this work. The first DNN is used to separate the sources from the mixed\nsignal. The second DNN is used to enhance the separated signals. To consider\nthe interactions between the separated sources, we propose to use a single DNN\nto enhance all the separated sources together. To reduce the residual signals\nof one source from the other separated sources (interference), we train the DNN\nfor enhancement discriminatively to maximize the dissimilarity between the\npredicted sources. The experimental results show that using discriminative\nenhancement decreases the distortion and interference between the separated\nsources."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.03296v1", 
    "title": "A Neural Network Alternative to Non-Negative Audio Models", 
    "arxiv-id": "1609.03296v1", 
    "author": "Shrikant Venkataramani", 
    "publish": "2016-09-12T07:50:41Z", 
    "summary": "We present a neural network that can act as an equivalent to a Non-Negative\nMatrix Factorization (NMF), and further show how it can be used to perform\nsupervised source separation. Due to the extensibility of this approach we show\nhow we can achieve better source separation performance as compared to\nNMF-based methods, and propose a variety of derivative architectures that can\nbe used for further improvements."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.03409v2", 
    "title": "Acoustic intensity, energy-density and diffuseness estimation in a   directionally-constrained region", 
    "arxiv-id": "1609.03409v2", 
    "author": "Ville Pulkki", 
    "publish": "2016-09-12T14:07:05Z", 
    "summary": "This work presents a method for estimation of the acoustic intensity, the\nenergy density and the associated sound field diffuseness around the origin,\nwhen the sound field is weighted with a spatial filter. The method permits\nenergetic DOA estimation and sound field characterization focused in a specific\nangular region determined by the beam pattern of the spatial filter. The\nformulation of the estimators is presented and their behavior is analyzed for\nthe fundamental cases useful in parametric sound field models of a single plane\nwave, a uniform diffuse field and a mixture of the two."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.06210v2", 
    "title": "Interference Reduction in Music Recordings Combining Kernel Additive   Modelling and Non-Negative Matrix Factorization", 
    "arxiv-id": "1609.06210v2", 
    "author": "Mark Sandler", 
    "publish": "2016-09-20T15:09:36Z", 
    "summary": "In live and studio recordings unexpected sound events often lead to\ninterferences in the signal. For non-stationary interferences, sound source\nseparation techniques can be used to reduce the interference level in the\nrecording. In this context, we present a novel approach combining the strengths\nof two algorithmic families: NMF and KAM. The recent KAM approach applies\nrobust statistics on frames selected by a source-specific kernel to perform\nsource separation. Based on semi-supervised NMF, we extend this approach in two\nways. First, we locate the interference in the recording based on detected NMF\nactivity. Second, we improve the kernel-based frame selection by incorporating\nan NMF-based estimate of the clean music signal. Further, we introduce a\ntemporal context in the kernel, taking some musical structure into account. Our\nexperiments show improved separation quality for our proposed method over a\nstate-of-the-art approach for interference reduction."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.jsv.2017.02.005", 
    "link": "http://arxiv.org/pdf/1609.08211v1", 
    "title": "A Robust Diarization System for Measuring Dominance in Peer-Led Team   Learning Groups", 
    "arxiv-id": "1609.08211v1", 
    "author": "John H. L. Hansen", 
    "publish": "2016-09-26T22:18:49Z", 
    "summary": "Peer-Led Team Learning (PLTL) is a structured learning model where a team\nleader is appointed to facilitate collaborative problem solving among students\nfor Science, Technology, Engineering and Mathematics (STEM) courses. This paper\npresents an informed HMM-based speaker diarization system. The minimum duration\nof short conversationalturns and number of participating students were fed as\nside information to the HMM system. A modified form of Bayesian Information\nCriterion (BIC) was used for iterative merging and re-segmentation. Finally, we\nused the diarization output to compute a novel dominance score based on\nunsupervised acoustic analysis."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1609.09231v1", 
    "title": "Low Rank and Sparsity Analysis Applied to Speech Enhancement via Online   Estimated Dictionary", 
    "arxiv-id": "1609.09231v1", 
    "author": "Jun Qin", 
    "publish": "2016-09-29T07:24:27Z", 
    "summary": "We propose an online estimated dictionary based single channel speech\nenhancement algorithm, which focuses on low rank and sparse matrix\ndecomposition. In this proposed algorithm, a noisy speech spectral matrix is\nconsidered as the summation of low rank background noise components and an\nactivation of the online speech dictionary, on which both low rank and sparsity\nconstraints are imposed. This decomposition takes the advantage of local\nestimated dictionary high expressiveness on speech components. The local\ndictionary can be obtained through estimating the speech presence probability\nby applying Expectation Maximal algorithm, in which a generalized Gamma prior\nfor speech magnitude spectrum is used. The evaluation results show that the\nproposed algorithm achieves significant improvements when compared to four\nother speech enhancement algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1609.09390v1", 
    "title": "Measurement of Sound Fields Using Moving Microphones", 
    "arxiv-id": "1609.09390v1", 
    "author": "Alfred Mertins", 
    "publish": "2016-09-29T15:33:09Z", 
    "summary": "The sampling of sound fields involves the measurement of spatially dependent\nroom impulse responses, where the Nyquist-Shannon sampling theorem applies in\nboth the temporal and spatial domain. Therefore, sampling inside a volume of\ninterest requires a huge number of sampling points in space, which comes along\nwith further difficulties such as exact microphone positioning and calibration\nof multiple microphones. In this paper, we present a method for measuring sound\nfields using moving microphones whose trajectories are known to the algorithm.\nAt that, the number of microphones is customizable by trading measurement\neffort against sampling time. Through spatial interpolation of the dynamic\nmeasurements, a system of linear equations is set up which allows for the\nreconstruction of the entire sound field inside the volume of interest."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1609.09443v2", 
    "title": "Semi-supervised Speech Enhancement in Envelop and Details Subspaces", 
    "arxiv-id": "1609.09443v2", 
    "author": "Jun Qin", 
    "publish": "2016-09-29T17:54:51Z", 
    "summary": "In this study, we propose a modulation decoupling based single channel speech\nenhancement subspace framework, in which the spectrogram of noisy speech is\ndecoupled as the product of a spectral envelop subspace and a spectral details\nsubspace. This decoupling approach provides a method to specifically work on\nelimination of those noises that greatly affect the intelligibility. Two\nsupervised low-rank and sparse decomposition schemes are developed in the\nspectral envelop subspace to obtain a robust recovery of speech components. A\nBayesian formulation of non-negative factorization is used to learn the speech\ndictionary from the spectral envelop subspace of clean speech samples. In the\nspectral details subspace, a standard robust principal component analysis is\nimplemented to extract the speech components. The validation results show that\ncompared with four speech enhancement algorithms, including MMSE-SPP, NMF-RPCA,\nRPCA, and LARC, the proposed MS based algorithms achieve satisfactory\nperformance on improving perceptual quality, and especially speech\nintelligibility."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1609.09747v1", 
    "title": "Hearing in a shoe-box : Binaural source position and wall absorption   estimation using virtually supervised learning", 
    "arxiv-id": "1609.09747v1", 
    "author": "Antoine Deleforge", 
    "publish": "2016-09-30T14:20:56Z", 
    "summary": "This paper introduces a new framework for supervised sound source\nlocalization referred to as virtually-supervised learning. An acoustic shoe-box\nroom simulator is used to generate a large number of binaural single-source\naudio scenes. These scenes are used to build a dataset of spatial binaural\nfeatures annotated with acoustic properties such as the 3D source position and\nthe walls' absorption coefficients. A probabilis-tic high-to low-dimensional\nregression framework is used to learn a mapping from these features to the\nacoustic properties. Results indicate that this mapping successfully estimates\nthe azimuth and elevation of new sources, but also their range and even the\nwalls' absorption coefficients solely based on binau-ral signals. Results also\nreveal that incorporating random-diffusion effects in the data significantly\nimproves the estimation of all parameters."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1609.09764v2", 
    "title": "Adaptive dictionary based approach for background noise and speaker   classification and subsequent source separation", 
    "arxiv-id": "1609.09764v2", 
    "author": "T V Ananthapadmanabha", 
    "publish": "2016-09-30T14:57:07Z", 
    "summary": "A judicious combination of dictionary learning methods, block sparsity and\nsource recovery algorithm are used in a hierarchical manner to identify the\nnoises and the speakers from a noisy conversation between two people.\nConversations are simulated using speech from two speakers, each with a\ndifferent background noise, with varied SNR values, down to -10 dB. Ten each of\nrandomly chosen male and female speakers from the TIMIT database and all the\nnoise sources from the NOISEX database are used for the simulations. For\nspeaker identification, the relative value of weights recovered is used to\nselect an appropriately small subset of the test data, assumed to contain\nspeech. This novel choice of using varied amounts of test data results in an\nimprovement in the speaker recognition rate of around 15% at SNR of 0 dB.\nSpeech and noise are separated using dictionaries of the estimated speaker and\nnoise, and an improvement of signal to distortion ratios of up to 10% is\nachieved at SNR of 0 dB. K-medoid and cosine similarity based dictionary\nlearning methods lead to better recognition of the background noise and the\nspeaker. Experiments are also conducted on cases, where either the background\nnoise or the speaker is outside the set of trained dictionaries. In such cases,\nadaptive dictionary learning leads to performance comparable to the other case\nof complete dictionaries."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.00644v2", 
    "title": "Speech Enhancement via Two-Stage Dual Tree Complex Wavelet Packet   Transform with a Speech Presence Probability Estimator", 
    "arxiv-id": "1610.00644v2", 
    "author": "Jun Qin", 
    "publish": "2016-10-03T17:39:01Z", 
    "summary": "In this paper, a two-stage dual tree complex wavelet packet transform\n(DTCWPT) based speech enhancement algorithm has been proposed, in which a\nspeech presence probability (SPP) estimator and a generalized minimum mean\nsquared error (MMSE) estimator are developed. To overcome the drawback of\nsignal distortions caused by down sampling of WPT, a two-stage analytic\ndecomposition concatenating undecimated WPT (UWPT) and decimated WPT is\nemployed. An SPP estimator in the DTCWPT domain is derived based on a\ngeneralized Gamma distribution of speech, and Gaussian noise assumption. The\nvalidation results show that the proposed algorithm can obtain enhanced\nperceptual evaluation of speech quality (PESQ), and segmental signal-to-noise\nratio (SegSNR) at low SNR nonstationary noise, compared with other four\nstate-of-the-art speech enhancement algorithms, including optimally modified\nLSA (OM-LSA), soft masking using a posteriori SNR uncertainty (SMPO), a\nposteriori SPP based MMSE estimation (MMSE-SPP), and adaptive Bayesian wavelet\nthresholding (BWT)."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.01797v1", 
    "title": "A Joint Detection-Classification Model for Audio Tagging of Weakly   Labelled Data", 
    "arxiv-id": "1610.01797v1", 
    "author": "Mark Plumbley", 
    "publish": "2016-10-06T09:51:12Z", 
    "summary": "Audio tagging aims to assign one or several tags to an audio clip. Most of\nthe datasets are weakly labelled, which means only the tags of the clip are\nknown, without knowing the occurrence time of the tags. The labeling of an\naudio clip is often based on the audio events in the clip and no event level\nlabel is provided to the user. Previous works have used the bag of frames model\nassume the tags occur all the time, which is not the case in practice. We\npropose a joint detection-classification (JDC) model to detect and classify the\naudio clip simultaneously. The JDC model has the ability to attend to\ninformative and ignore uninformative sounds. Then only informative regions are\nused for classification. Experimental results on the \"CHiME Home\" dataset show\nthat the JDC model reduces the equal error rate (EER) from 19.0% to 16.9%. More\ninterestingly, the audio event detector is trained successfully without needing\nthe event level label."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.02392v1", 
    "title": "An Automatic System for Acoustic Microphone Geometry Calibration based   on Minimal Solvers", 
    "arxiv-id": "1610.02392v1", 
    "author": "Kalle \u00c5str\u00f6m", 
    "publish": "2016-10-07T19:57:41Z", 
    "summary": "In this paper, robust detection, tracking and geometry estimation methods are\ndeveloped and combined into a system for estimating time-difference estimates,\nmicrophone localization and sound source movement. No assumptions on the 3D\nlocations of the microphones and sound sources are made. The system is capable\nof tracking continuously moving sound sources in an reverberant environment.\nThe multi-path components are explicitly tracked and used in the geometry\nestimation parts. The system is based on matching between pairs of channels\nusing GCC-PHAT. Instead of taking a single maximum at each time instant from\neach such pair, we select the four strongest local maxima. This produce a set\nof hypothesis to work with in the subsequent steps, where consistency\nconstraints between the channels and time-continuity constraints are exploited.\nIn the paper it demonstrated how such detections can be used to estimate\nmicrophone positions, sound source movement and room geometry. The methods are\ntested and verified using real data from several reverberant environments. The\nevaluation demonstrated accuracy in the order of few millimeters."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.02831v2", 
    "title": "Domain adaptation based Speaker Recognition on Short Utterances", 
    "arxiv-id": "1610.02831v2", 
    "author": "Clinton Fookes", 
    "publish": "2016-10-10T10:09:49Z", 
    "summary": "This paper explores how the in- and out-domain probabilistic linear\ndiscriminant analysis (PLDA) speaker verification behave when enrolment and\nverification lengths are reduced. Experiment studies have found that when\nfull-length utterance is used for evaluation, in-domain PLDA approach shows\nmore than 28% improvement in EER and DCF values over out-domain PLDA approach\nand when short utterances are used for evaluation, the performance gain of\nin-domain speaker verification reduces at an increasing rate. Novel modified\ninter dataset variability (IDV) compensation is used to compensate the mismatch\nbetween in- and out-domain data and IDV-compensated out-domain PLDA shows\nrespectively 26% and 14% improvement over out-domain PLDA speaker verification\nwhen SWB and NIST data are respectively used for S normalization. When the\nevaluation utterance length is reduced, the performance gain by IDV also\nreduces as short utterance evaluation data i-vectors have more variations due\nto phonetic variations when compared to the dataset mismatch between in- and\nout-domain data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.03190v1", 
    "title": "DNN based Speaker Recognition on Short Utterances", 
    "arxiv-id": "1610.03190v1", 
    "author": "Clinton Fookes", 
    "publish": "2016-10-11T05:04:25Z", 
    "summary": "This paper investigates the effects of limited speech data in the context of\nspeaker verification using deep neural network (DNN) approach. Being able to\nreduce the length of required speech data is important to the development of\nspeaker verification system in real world applications. The experimental\nstudies have found that DNN-senone-based Gaussian probabilistic linear\ndiscriminant analysis (GPLDA) system respectively achieves above 50% and 18%\nimprovements in EER values over GMM-UBM GPLDA system on NIST 2010\ncoreext-coreext and truncated 15sec-15sec evaluation conditions. Further when\nGPLDA model is trained on short-length utterances (30sec) rather than\nfull-length utterances (2min), DNN-senone GPLDA system achieves above 7%\nimprovement in EER values on truncated 15sec-15sec condition. This is because\nshort length development i-vectors have speaker, session and phonetic variation\nand GPLDA is able to robustly model those variations. For several real world\napplications, longer utterances (2min) can be used for enrollment and shorter\nutterances (15sec) are required for verification, and in those conditions,\nDNN-senone GPLDA system achieves above 26% improvement in EER values over\nGMM-UBM GPLDA systems."
},{
    "category": "cs.SD", 
    "doi": "10.1109/LSP.2016.2627029", 
    "link": "http://arxiv.org/pdf/1610.03772v1", 
    "title": "RAVEN X High Performance Data Mining Toolbox for Bioacoustic Data   Analysis", 
    "arxiv-id": "1610.03772v1", 
    "author": "Tyler A. Helble", 
    "publish": "2016-10-12T16:24:54Z", 
    "summary": "Objective of this work is to integrate high performance computing (HPC)\ntechnologies and bioacoustics data-mining capabilities by offering a\nMATLAB-based toolbox called Raven-X. Raven-X will provide a\nhardware-independent solution, for processing large acoustic datasets - the\ntoolkit will be available to the community at no cost. This goal will be\nachieved by leveraging prior work done which successfully deployed MATLAB based\nHPC tools within Cornell University's Bioacoustics Research Program (BRP).\nThese tools enabled commonly available multi-core computers to process data at\naccelerated rates to detect and classify whale sounds in large multi-channel\nsound archives. Through this collaboration, we will expand on this effort which\nwas featured through Mathworks research and industry forums incorporate new\ncutting-edge detectors and classifiers, and disseminate Raven-X to the broader\nbioacoustics community."
},{
    "category": "cs.SD", 
    "doi": "10.1049/el.2015.2665", 
    "link": "http://arxiv.org/pdf/1610.04695v1", 
    "title": "Non-negative matrix factorization-based subband decomposition for   acoustic source localization", 
    "arxiv-id": "1610.04695v1", 
    "author": "Hanseok Ko", 
    "publish": "2016-10-15T06:49:15Z", 
    "summary": "A novel non-negative matrix factorization (NMF) based subband decomposition\nin frequency spatial domain for acoustic source localization using a microphone\narray is introduced. The proposed method decomposes source and noise subband\nand emphasises source dominant frequency bins for more accurate source\nrepresentation. By employing NMF, delay basis vectors and their subband\ninformation in frequency spatial domain for each frame is extracted. The\nproposed algorithm is evaluated in both simulated noise and real noise with a\nspeech corpus database. Experimental results clearly indicate that the\nalgorithm performs more accurately than other conventional algorithms under\nboth reverberant and noisy acoustic environments."
},{
    "category": "cs.SD", 
    "doi": "10.1049/el.2015.2665", 
    "link": "http://arxiv.org/pdf/1610.04770v1", 
    "title": "Semi-Supervised Source Localization on Multiple-Manifolds with   Distributed Microphones", 
    "arxiv-id": "1610.04770v1", 
    "author": "Sharon Gannot", 
    "publish": "2016-10-15T18:14:57Z", 
    "summary": "The problem of source localization with ad hoc microphone networks in noisy\nand reverberant enclosures, given a training set of prerecorded measurements,\nis addressed in this paper. The training set is assumed to consist of a limited\nnumber of labelled measurements, attached with corresponding positions, and a\nlarger amount of unlabelled measurements from unknown locations. However,\nmicrophone calibration is not required. We use a Bayesian inference approach\nfor estimating a function that maps measurement-based feature vectors to the\ncorresponding positions. The central issue is how to combine the information\nprovided by the different microphones in a unified statistical framework. To\naddress this challenge, we model this function using a Gaussian process with a\ncovariance function that encapsulates both the connections between pairs of\nmicrophones and the relations among the samples in the training set. The\nparameters of the process are estimated by optimizing a maximum likelihood (ML)\ncriterion. In addition, a recursive adaptation mechanism is derived where the\nnew streaming measurements are used to update the model. Performance is\ndemonstrated for 2-D localization of both simulated data and real-life\nrecordings in a variety of reverberation and noise levels."
},{
    "category": "cs.SD", 
    "doi": "10.5281/zenodo.50364", 
    "link": "http://arxiv.org/pdf/1610.04922v1", 
    "title": "Making Mainstream Synthesizers with Csound", 
    "arxiv-id": "1610.04922v1", 
    "author": "Ivan Osipenko", 
    "publish": "2016-10-16T22:27:33Z", 
    "summary": "For more than the past twenty years, Csound has been one of the leaders in\nthe world of the computer music research, implementing innovative synthesis\nmethods and making them available beyond the academic environments from which\nthey often arise, and into the hands of musicians and sound designers\nthroughout the world. In its present state, Csound offers an efficient\nenvironment for sound experimentation, allowing the user to work with almost\nany known sound synthesis or signal processing method through its vast\ncollection of ready-made opcodes. But despite all this potential, the shared\nresource of Csound instruments still lacks quality reproductions of well-known\nsynthesizers; even with its ability to generate commercial standard user\ninterfaces and with the possibility to compile Csound instruments in such as\nfashion so that they can be used with no knowledge of Csound code. To fill this\ngap, the authors have implemented two commercial-style synthesizers as VST\nplug-ins using the Csound front-end \"Cabbage\". This paper describes their\narchitecture and some of the Csound specific challenges involved in the\ndevelopment of fully featured synthesizers."
},{
    "category": "cs.SD", 
    "doi": "10.5281/zenodo.50364", 
    "link": "http://arxiv.org/pdf/1610.04965v1", 
    "title": "Improving Short Utterance PLDA Speaker Verification using SUV Modelling   and Utterance Partitioning Approach", 
    "arxiv-id": "1610.04965v1", 
    "author": "Clinton Fookes", 
    "publish": "2016-10-17T03:36:42Z", 
    "summary": "This paper analyses the short utterance probabilistic linear discriminant\nanalysis (PLDA) speaker verification with utterance partitioning and short\nutterance variance (SUV) modelling approaches. Experimental studies have found\nthat instead of using single long-utterance as enrolment data, if long enrolled\nutterance is partitioned into multiple short utterances and average of short\nutterance i-vectors is used as enrolled data, that improves the Gaussian PLDA\n(GPLDA) speaker verification. This is because short utterance i-vectors have\nspeaker, session and utterance variations, and utterance-partitioning approach\ncompensates the utterance variation. Subsequently, SUV-PLDA is also studied\nwith utterance partitioning approach, and utterance partitioning-based\nSUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008\nand NIST 2010 truncated 10sec-10sec evaluation condition as utterance\npartitioning approach compensates the utterance variation and SUV modelling\napproach compensates the mismatch between full-length development data and\nshort-length evaluation data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1610.05653v2", 
    "title": "Acoustic Reflector Localization: Novel Image Source Reversion and Direct   Localization Methods", 
    "arxiv-id": "1610.05653v2", 
    "author": "Wenwu Wang", 
    "publish": "2016-10-18T14:48:06Z", 
    "summary": "Acoustic reflector localization is an important issue in audio signal\nprocessing, with direct applications in spatial audio, scene reconstruction,\nand source separation. Several methods have recently been proposed to estimate\nthe 3D positions of acoustic reflectors given room impulse responses (RIRs). In\nthis article, we categorize these methods as \"image-source reversion\", which\nlocalizes the image source before finding the reflector position, and \"direct\nlocalization\", which localizes the reflector without intermediate steps. We\npresent five new contributions. First, an onset detector, called the clustered\ndynamic programming projected phase-slope algorithm, is proposed to\nautomatically extract the time of arrival for early reflections within the RIRs\nof a compact microphone array. Second, we propose an image-source reversion\nmethod that uses the RIRs from a single loudspeaker. It is constructed by\ncombining an image source locator (the image source direction and range (ISDAR)\nalgorithm), and a reflector locator (using the loudspeaker-image bisection\n(LIB) algorithm). Third, two variants of it, exploiting multiple loudspeakers,\nare proposed. Fourth, we present a direct localization method, the ellipsoid\ntangent sample consensus (ETSAC), exploiting ellipsoid properties to localize\nthe reflector. Finally, systematic experiments on simulated and measured RIRs\nare presented, comparing the proposed methods with the state-of-the-art. ETSAC\ngenerates errors lower than the alternative methods compared through our\ndatasets. Nevertheless, the ISDAR-LIB combination performs well and has a run\ntime 200 times faster than ETSAC."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1610.06214v1", 
    "title": "A model of infant speech perception and learning", 
    "arxiv-id": "1610.06214v1", 
    "author": "Philip Zurbuchen", 
    "publish": "2016-10-19T20:56:05Z", 
    "summary": "Infant speech perception and learning is modeled using Echo State Network\nclassification and Reinforcement Learning. Ambient speech for the modeled\ninfant learner is created using the speech synthesizer Vocaltractlab. An\nauditory system is trained to recognize vowel sounds from a series of speakers\nof different anatomies in Vocaltractlab. Having formed perceptual targets, the\ninfant uses Reinforcement Learning to imitate his ambient speech. A possible\nway of bridging the problem of speaker normalisation is proposed, using direct\nimitation but also including a caregiver who listens to the infants sounds and\nimitates those that sound vowel-like."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.01172v1", 
    "title": "Multiple-Speaker Localization Based on Direct-Path Features and   Likelihood Maximization with Spatial Sparsity Regularization", 
    "arxiv-id": "1611.01172v1", 
    "author": "Radu Horaud", 
    "publish": "2016-11-03T20:08:40Z", 
    "summary": "This paper addresses the problem of multiple-speaker localization in noisy\nand reverberant environments, using binaural recordings of an acoustic scene. A\nGaussian mixture model (GMM) is adopted, whose components correspond to all the\npossible candidate source locations defined on a grid. After optimizing the\nGMM-based objective function, given an observed set of binaural features, both\nthe number of sources and their locations are estimated by selecting the GMM\ncomponents with the largest priors. This is achieved by enforcing a sparse\nsolution, thus favoring a small number of speakers with respect to the large\nnumber of initial candidate source locations. An entropy-based penalty term is\nadded to the likelihood, thus imposing sparsity over the set of GMM priors. In\naddition, the direct-path relative transfer function (DP-RTF) is used to build\nrobust binaural features. The DP-RTF, recently proposed for single-source\nlocalization, was shown to be robust to reverberations, since it encodes\ninter-channel information corresponding to the direct-path of sound\npropagation. In this paper, we extend the DP-RTF estimation to the case of\nmultiple sources. In the short-time Fourier transform domain, a consistency\ntest is proposed to check whether a set of consecutive frames is associated to\nthe same source or not. Reliable DP-RTF features are selected from the frames\nthat pass the consistency test to be used for source localization. Experiments\ncarried out using both simulation data and real data gathered with a robotic\nhead confirm the efficiency of the proposed multi-source localization method."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.03178v1", 
    "title": "Noise reduction combining microphone and piezoelectric device", 
    "arxiv-id": "1611.03178v1", 
    "author": "Shuji Hashimoto", 
    "publish": "2016-11-10T03:51:42Z", 
    "summary": "It is often required to extract the sound of an objective instrument played\nin concert with other instruments. Microphone array is one of the effective\nways to enhance a sound from a specific direction. However it is not effective\nin an echoic room such as concert hall. The pickup microphone attached on the\nspecific musical instrument is often employed to obtain the sound exclusively\nfrom other instrumental sounds. The obtained timbre differ from the one we hear\nat the usual listening position. The purpose of this paper is to propose a new\nmethod of sound separation that utilizes the piezoelectric device attached on\nthe body of the instrument. The signal from the attached device has a different\nspectrum from the sound heard by the audience but has the same frequency\ncomponents as the instrumental sound. Our idea is to use the device signal as a\nmodifier of the sound focusing filter applied to the microphone sound at the\nlistening position. The proposed method firstly estimates the frequency\ncomponents of the signal from the piezoelectric device. The frequency\ncharacteristics for filtering the microphone sound are changed so that it pass\nthe estimated frequency components. Thus we can extractthe target sound without\ndistortion. The proposed method is a sort of dynamic sparseness approach. It\nwas found that SNR is improved by 8.7dB through the experiments."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.04947v1", 
    "title": "Detection of north atlantic right whale upcalls using local binary   patterns in a two-stage strategy", 
    "arxiv-id": "1611.04947v1", 
    "author": "Edmund Gerstein", 
    "publish": "2016-11-15T17:23:17Z", 
    "summary": "In this paper, we investigate the effectiveness of two-stage classification\nstrategies in detecting north Atlantic right whale upcalls. Time-frequency\nmeasurements of data from passive acoustic monitoring devices are evaluated as\nimages. Vocalization spectrograms are preprocessed for noise reduction and tone\nremoval. First stage of the algorithm eliminates non-upcalls by an energy\ndetection algorithm. In the second stage, two sets of features are extracted\nfrom the remaining signals using contour-based and texture based methods. The\nformer is based on extraction of time-frequency features from upcall contours,\nand the latter employs a Local Binary Pattern operator to extract\ndistinguishing texture features of the upcalls. Subsequently evaluation phase\nis carried out by using several classifiers to assess the effectiveness of both\nthe contour-based and texture-based features for upcall detection. Experimental\nresults with the data set provided by the Cornell University Bioacoustics\nResearch Program reveal that classifiers show accuracy improvements of 3% to 4%\nwhen using LBP features over time-frequency features. Classifiers such as the\nLinear Discriminant Analysis, Support Vector Machine, and TreeBagger achieve\nhigh upcall detection rates with LBP features."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.05182v1", 
    "title": "Detecting tala Computationally in Polyphonic Context - A Novel Approach", 
    "arxiv-id": "1611.05182v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-11-16T08:15:00Z", 
    "summary": "In North-Indian-Music-System(NIMS),tabla is mostly used as percussive\naccompaniment for vocal-music in polyphonic-compositions. The human auditory\nsystem uses perceptual grouping of musical-elements and easily filters the\ntabla component, thereby decoding prominent rhythmic features like tala, tempo\nfrom a polyphonic composition. For Western music, lots of work have been\nreported for automated drum analysis of polyphonic composition. However,\nattempts at computational analysis of tala by separating the tabla-signal from\nmixed signal in NIMS have not been successful. Tabla is played with two\ncomponents - right and left. The right-hand component has frequency overlap\nwith voice and other instruments. So, tala analysis of polyphonic-composition,\nby accurately separating the tabla-signal from the mixture is a baffling task,\ntherefore an area of challenge. In this work we propose a novel technique for\nsuccessfully detecting tala using left-tabla signal, producing meaningful\nresults because the left-tabla normally doesn't have frequency overlap with\nvoice and other instruments. North-Indian-rhythm follows complex cyclic\npattern, against linear approach of Western-rhythm. We have exploited this\ncyclic property along with stressed and non-stressed methods of playing\ntabla-strokes to extract a characteristic pattern from the left-tabla strokes,\nwhich, after matching with the grammar of tala-system, determines the tala and\ntempo of the composition. A large number of\npolyphonic(vocal+tabla+other-instruments) compositions has been analyzed with\nthe methodology and the result clearly reveals the effectiveness of proposed\ntechniques."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.06505v1", 
    "title": "Decision-Based Transcription of Jazz Guitar Solos Using a Harmonic   Bident Analysis Filter Bank and Spectral Distribution Weighting", 
    "arxiv-id": "1611.06505v1", 
    "author": "Fran\u00e7ois Pachet", 
    "publish": "2016-11-20T12:11:58Z", 
    "summary": "Jazz guitar solos are improvised melody lines played on one instrument on top\nof a chordal accompaniment (comping). As the improvisation happens\nspontaneously, a reference score is non-existent, only a lead sheet. There are\nsituations, however, when one would like to have the original melody lines in\nthe form of notated music, see the Real Book. The motivation is either for the\npurpose of practice and imitation or for musical analysis. In this work, an\nautomatic transcriber for jazz guitar solos is developed. It resorts to a very\nintuitive representation of tonal music signals: the pitchgram. No\ninstrument-specific modeling is involved, so the transcriber should be\napplicable to other pitched instruments as well. Neither is there the need to\nlearn any note profiles prior to or during the transcription. Essentially, the\nproposed transcriber is a decision tree, thus a classifier, with a depth of 3.\nIt has a (very) low computational complexity and can be run on-line. The\ndecision rules can be refined or extended with no or little musical education.\nThe transcriber's performance is evaluated on a set of ten jazz solo excerpts\nand compared with a state-of-the-art transcription system for the guitar plus\nPYIN. We achieve an improvement of 34% w.r.t. the reference system and 19%\nw.r.t. PYIN in terms of the F-measure. Another measure of accuracy, the error\nscore, attests that the number of erroneous pitch detections is reduced by more\nthan 50% w.r.t. the reference system and by 45% w.r.t. PYIN."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.08749v2", 
    "title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on   Animal calls and Speech", 
    "arxiv-id": "1611.08749v2", 
    "author": "Randall Balestriero", 
    "publish": "2016-11-26T22:16:35Z", 
    "summary": "The scattering framework offers an optimal hierarchical convolutional\ndecomposition according to its kernels. Convolutional Neural Net (CNN) can be\nseen as an optimal kernel decomposition, nevertheless it requires large amount\nof training data to learn its kernels. We propose a trade-off between these two\napproaches: a Chirplet kernel as an efficient Q constant bioacoustic\nrepresentation to pretrain CNN. First we motivate Chirplet bioinspired auditory\nrepresentation. Second we give the first algorithm (and code) of a Fast\nChirplet Transform (FCT). Third, we demonstrate the computation efficiency of\nFCT on large environmental data base: months of Orca recordings, and 1000 Birds\nspecies from the LifeClef challenge. Fourth, we validate FCT on the vowels\nsubset of the Speech TIMIT dataset. The results show that FCT accelerates CNN\nwhen it pretrains low level layers: it reduces training duration by -28\\% for\nbirds classification, and by -26% for vowels classification. Scores are also\nenhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average\nPrecision on birds, and +2.3\\% of vowel accuracy against raw audio CNN. We\nconclude on perspectives on tonotopic FCT deep machine listening, and\ninter-species bioacoustic transfer learning to generalise the representation of\nanimal communication systems."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.08905v1", 
    "title": "SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based   on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction", 
    "arxiv-id": "1611.08905v1", 
    "author": "Fran\u00e7ois Pachet", 
    "publish": "2016-11-27T20:29:53Z", 
    "summary": "Research in collaborative music learning is subject to unresolved problems\ndemanding new technological solutions. One such problem poses the suppression\nof the accompaniment in a live recording of a performance during practice,\nwhich can be for the purposes of self-assessment or further machine-aided\nanalysis. Being able to separate a solo from the accompaniment allows to create\nlearning agents that may act as personal tutors and help the apprentice improve\nhis or her technique. First, we start from the classical adaptive noise\ncancelling approach, and adjust it to the problem at hand. In a second step, we\ncompare some adaptive and Wiener filtering approaches and assess their\nperformances on the task. Our findings underpin that adaptive filtering is\ninapt of dealing with music signals and that Wiener filtering in the short-time\nFourier transform domain is a much more effective approach. In addition, it is\nvery cheap if carried out in the frequency bands of auditory filters. A\ndouble-output extension based on maximal-ratio combining is also proposed."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2633802", 
    "link": "http://arxiv.org/pdf/1611.09524v1", 
    "title": "Understanding Audio Pattern Using Convolutional Neural Network From Raw   Waveforms", 
    "arxiv-id": "1611.09524v1", 
    "author": "Samarjit Das", 
    "publish": "2016-11-29T08:33:48Z", 
    "summary": "One key step in audio signal processing is to transform the raw signal into\nrepresentations that are efficient for encoding the original information.\nTraditionally, people transform the audio into spectral representations, as a\nfunction of frequency, amplitude and phase transformation. In this work, we\ntake a purely data-driven approach to understand the temporal dynamics of audio\nat the raw signal level. We maximize the information extracted from the raw\nsignal through a deep convolutional neural network (CNN) model. Our CNN model\nis trained on the urbansound8k dataset. We discover that salient audio patterns\nembedded in the raw waveforms can be efficiently extracted through a\ncombination of nonlinear filters learned by the CNN model."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2899004", 
    "link": "http://arxiv.org/pdf/1611.09733v1", 
    "title": "Getting Closer to the Essence of Music: The Con Espressione Manifesto", 
    "arxiv-id": "1611.09733v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-11-29T17:19:45Z", 
    "summary": "This text offers a personal and very subjective view on the current situation\nof Music Information Research (MIR). Motivated by the desire to build systems\nwith a somewhat deeper understanding of music than the ones we currently have,\nI try to sketch a number of challenges for the next decade of MIR research,\ngrouped around six simple truths about music that are probably generally agreed\non, but often ignored in everyday research."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2899004", 
    "link": "http://arxiv.org/pdf/1612.00876v1", 
    "title": "FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts", 
    "arxiv-id": "1612.00876v1", 
    "author": "Martin Vetterli", 
    "publish": "2016-12-02T22:02:04Z", 
    "summary": "In this paper we present FRIDA---an algorithm for estimating directions of\narrival of multiple wideband sound sources. FRIDA combines multi-band\ninformation coherently and achieves state-of-the-art resolution at extremely\nlow signal-to-noise ratios. It works for arbitrary array layouts, but unlike\nthe various steered response power and subspace methods, it does not require a\ngrid search. FRIDA leverages recent advances in sampling signals with a finite\nrate of innovation. It is based on the insight that for any array layout, the\nentries of the spatial covariance matrix can be linearly transformed into a\nuniformly sampled sum of sinusoids."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2899004", 
    "link": "http://arxiv.org/pdf/1612.01860v4", 
    "title": "An algorithm to assign musical prime commas to every prime number and   construct a universal and compact free Just Intonation musical notation", 
    "arxiv-id": "1612.01860v4", 
    "author": "David Ryan", 
    "publish": "2016-12-05T12:48:03Z", 
    "summary": "Musical frequencies in Just Intonation are comprised of rational numbers. The\nstructure of rational numbers is determined by prime factorisations. Just\nIntonation frequencies can be split into two components. The larger component\nuses only integer powers of the first two primes, 2 and 3. The smaller\ncomponent decomposes into a series of microtonal adjustments, one for each\nprime number 5 and above present in the original frequency. The larger 3-limit\ncomponent can be notated using scientific pitch notation modified to use\nPythagorean tuning. The microtonal adjustments can be notated using rational\ncommas which are built up from prime commas. This gives a notation system for\nthe whole of free-JI, called Rational Comma Notation. RCN is compact since all\nmicrotonal adjustments can be represented by a single notational unit based on\na rational number. RCN has different versions depending on the choice of\nalgorithm to assign a prime comma to each prime number. Two existing algorithms\nSAG and KG are found in the literature. A novel algorithm DR is developed based\non discussion of mathematical and musical criteria for algorithm design.\nResults for DR are presented for primes below 1400. Some observations are made\nabout these results and their applications, including shorthand notation and\npitch class lattices. Results for DR are compared with those for SAG and KG.\nTranslation is possible between any two free-JI notations and any two versions\nof RCN since they all represent the same underlying set of rational numbers."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2899004", 
    "link": "http://arxiv.org/pdf/1612.03505v1", 
    "title": "Convolutional Neural Networks for Passive Monitoring of a Shallow Water   Environment using a Single Sensor", 
    "arxiv-id": "1612.03505v1", 
    "author": "Craig T. Jin", 
    "publish": "2016-12-12T00:13:35Z", 
    "summary": "A cost effective approach to remote monitoring of protected areas such as\nmarine reserves and restricted naval waters is to use passive sonar to detect,\nclassify, localize, and track marine vessel activity (including small boats and\nautonomous underwater vehicles). Cepstral analysis of underwater acoustic data\nenables the time delay between the direct path arrival and the first multipath\narrival to be measured, which in turn enables estimation of the instantaneous\nrange of the source (a small boat). However, this conventional method is\nlimited to ranges where the Lloyd's mirror effect (interference pattern formed\nbetween the direct and first multipath arrivals) is discernible. This paper\nproposes the use of convolutional neural networks (CNNs) for the joint\ndetection and ranging of broadband acoustic noise sources such as marine\nvessels in conjunction with a data augmentation approach for improving network\nperformance in varied signal-to-noise ratio (SNR) situations. Performance is\ncompared with a conventional passive sonar ranging method for monitoring marine\nvessel activity using real data from a single hydrophone mounted above the sea\nfloor. It is shown that CNNs operating on cepstrum data are able to detect the\npresence and estimate the range of transiting vessels at greater distances than\nthe conventional method."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.04028v2", 
    "title": "Adaptive DCTNet for Audio Signal Classification", 
    "arxiv-id": "1612.04028v2", 
    "author": "Andrew Thompson", 
    "publish": "2016-12-13T04:56:47Z", 
    "summary": "In this paper, we investigate DCTNet for audio signal classification. Its\noutput feature is related to Cohen's class of time-frequency distributions. We\nintroduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature\nextraction. The A-DCTNet applies the idea of constant-Q transform, with its\ncenter frequencies of filterbanks geometrically spaced. The A-DCTNet is\nadaptive to different acoustic scales, and it can better capture low frequency\nacoustic information that is sensitive to human audio perception than features\nsuch as Mel-frequency spectral coefficients (MFSC). We use features extracted\nby the A-DCTNet as input for classifiers. Experimental results show that the\nA-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art\nperformance in bird song classification rate, and improve artist identification\naccuracy in music data. They demonstrate A-DCTNet's applicability to signal\nprocessing problems."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.04919v1", 
    "title": "Combination of Linear Prediction and Phase Decomposition for Glottal   Source Analysis on Voiced Speech", 
    "arxiv-id": "1612.04919v1", 
    "author": "John N. Gowdy", 
    "publish": "2016-12-15T04:22:39Z", 
    "summary": "Some glottal analysis approaches based upon linear prediction or complex\ncepstrum approaches have been proved to be effective to estimate glottal source\nfrom real speech utterances. We propose a new approach employing both an\nall-pole odd-order linear prediction to provide a coarse estimation and phase\ndecomposition based causality/anti-causality separation to generate further\nrefinements. The obtained measures show that this method improved performance\nin terms of reducing source-filter separation in estimation of glottal flow\npulses (GFP). No glottal model fitting is required by this method, thus it has\nwide and flexible adaptation to retain fidelity of speakers's vocal features\nwith computationally affordable resource. The method is evaluated on real\nspeech utterances to validate it."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.04928v1", 
    "title": "Music Generation with Deep Learning", 
    "arxiv-id": "1612.04928v1", 
    "author": "Srikanth Grandhe", 
    "publish": "2016-12-15T05:06:40Z", 
    "summary": "The use of deep learning to solve problems in literary arts has been a recent\ntrend that has gained a lot of attention and automated generation of music has\nbeen an active area. This project deals with the generation of music using raw\naudio files in the frequency domain relying on various LSTM architectures.\nFully connected and convolutional layers are used along with LSTM's to capture\nrich features in the frequency domain and increase the quality of music\ngenerated. The work is focused on unconstrained music generation and uses no\ninformation about musical structure(notes or chords) to aid learning.The music\ngenerated from various architectures are compared using blind fold tests. Using\nthe raw audio to train models is the direction to tapping the enormous amount\nof mp3 files that exist over the internet without requiring the manual effort\nto make structured MIDI files. Moreover, not all audio files can be represented\nwith MIDI files making the study of these models an interesting prospect to the\nfuture of such models."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.05076v1", 
    "title": "Live Score Following on Sheet Music Images", 
    "arxiv-id": "1612.05076v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-15T14:16:56Z", 
    "summary": "In this demo we show a novel approach to score following. Instead of relying\non some symbolic representation, we are using a multi-modal convolutional\nneural network to match the incoming audio stream directly to sheet music\nimages. This approach is in an early stage and should be seen as proof of\nconcept. Nonetheless, the audience will have the opportunity to test our\nimplementation themselves via 3 simple piano pieces."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.05156v1", 
    "title": "A Phase Vocoder based on Nonstationary Gabor Frames", 
    "arxiv-id": "1612.05156v1", 
    "author": "Monika D\u00f6rfler", 
    "publish": "2016-12-15T19:43:54Z", 
    "summary": "We propose a new algorithm for time stretching music signals based on the\ntheory of nonstationary Gabor frames. The algorithm extends the techniques of\nthe classical phase vocoder by incorporating adaptive time-frequency\nrepresentations and adaptive phase locking. Applying a preliminary onset\ndetection algorithm, the obtained time-frequency representation implies good\ntime resolution for the onsets and good frequency resolution for the sinusoidal\ncomponents.\n  The phase estimates are done only at peak channels using quadratic\ninterpolation and the remaining phases are then locked to the values of the\npeaks in an adaptive manner. In contrast to previous attempts we let the number\nof frequency channels vary over time in order to obtain a low redundancy of the\ncorresponding transform. We show that with a redundancy comparable to that of\nthe phase vocoder we can greatly reduce artefacts such as phasiness and\ntransient smearing. The algorithm is tested on both synthetic and real world\nsignals and compared with state of the art algorithms in a reproducible manner."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.05168v1", 
    "title": "LIA system description for NIST SRE 2016", 
    "arxiv-id": "1612.05168v1", 
    "author": "Jean-Fran\u00e7ois Bonastre", 
    "publish": "2016-12-15T17:59:05Z", 
    "summary": "This paper describes the LIA speaker recognition system developed for the\nSpeaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed,\nall based on a state-of-the-art approach: i-vector/PLDA which represents the\nmainstream technique in text-independent speaker recognition. These sub-systems\ndiffer: on the acoustic feature extraction front-end (MFCC, PLP), at the\ni-vector extraction stage (UBM, DNN or two-feats posteriors) and finally on the\ndata-shifting (IDVC, mean-shifting). The submitted system is a fusion at the\nscore-level of these eight sub-systems."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.05432v1", 
    "title": "Basis-Function Modeling of Loudness Variations in Ensemble Performance", 
    "arxiv-id": "1612.05432v1", 
    "author": "Carlos Eduardo Cancino Chac\u00f3n", 
    "publish": "2016-12-16T11:05:52Z", 
    "summary": "This paper describes a computational model of loudness variations in\nexpressive ensemble performance. The model predicts and explains the continuous\nvariation of loudness as a function of information extracted automatically from\nthe written score. Although such models have been proposed for expressive\nperformance in solo instruments, this is (to the best of our knowledge) the\nfirst attempt to define a model for expressive performance in ensembles. To\nthat end, we extend an existing model that was designed to model expressive\npiano performances, and describe the additional steps necessary for the model\nto deal with scores of arbitrary instrumentation, including orchestral scores.\nWe test both linear and non-linear variants of the extended model n a data set\nof audio recordings of symphonic music, in a leave-one-out setting. The\nexperiments reveal that the most successful model variant is a recurrent,\nnon-linear model. Even if the accuracy of the predicted loudness varies from\none recording to another, in several cases the model explains well over 50% of\nthe variance in loudness."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.05489v1", 
    "title": "On-bird Sound Recordings: Automatic Acoustic Recognition of Activities   and Contexts", 
    "arxiv-id": "1612.05489v1", 
    "author": "Lisa F. Gill", 
    "publish": "2016-12-16T14:40:43Z", 
    "summary": "We introduce a novel approach to studying animal behaviour and the context in\nwhich it occurs, through the use of microphone backpacks carried on the backs\nof individual free-flying birds. These sensors are increasingly used by animal\nbehaviour researchers to study individual vocalisations of freely behaving\nanimals, even in the field. However such devices may record more than an\nanimals vocal behaviour, and have the potential to be used for investigating\nspecific activities (movement) and context (background) within which\nvocalisations occur. To facilitate this approach, we investigate the automatic\nannotation of such recordings through two different sound scene analysis\nparadigms: a scene-classification method using feature learning, and an\nevent-detection method using probabilistic latent component analysis (PLCA). We\nanalyse recordings made with Eurasian jackdaws (Corvus monedula) in both\ncaptive and field settings. Results are comparable with the state of the art in\nsound scene analysis; we find that the current recognition quality level\nenables scalable automatic annotation of audio logger data, given partial\nannotation, but also find that individual differences between animals and/or\ntheir backpacks limit the generalisation from one individual to another. we\nconsider the interrelation of 'scenes' and 'events' in this particular task,\nand issues of temporal resolution."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.06151v3", 
    "title": "HRTF-based two-dimensional robust least-squares frequency-invariant   beamformer design for robot audition", 
    "arxiv-id": "1612.06151v3", 
    "author": "Walter Kellermann", 
    "publish": "2016-12-19T12:19:25Z", 
    "summary": "In this work, we propose a two-dimensional Head-Related Transfer Function\n(HRTF)-based robust beamformer design for robot audition, which allows for\nexplicit control of the beamformer response for the entire three-dimensional\nsound field surrounding a humanoid robot. We evaluate the proposed method by\nmeans of both signal-independent and signal-dependent measures in a robot\naudition scenario. Our results confirm the effectiveness of the proposed\ntwo-dimensional HRTF-based beamformer design, compared to our previously\npublished one-dimensional HRTF-based beamformer design, which was carried out\nfor a fixed elevation angle only."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.06642v1", 
    "title": "Efficient Target Activity Detection based on Recurrent Neural Networks", 
    "arxiv-id": "1612.06642v1", 
    "author": "Walter Kellermann", 
    "publish": "2016-12-20T13:04:33Z", 
    "summary": "This paper addresses the problem of Target Activity Detection (TAD) for\nbinaural listening devices. TAD denotes the problem of robustly detecting the\nactivity of a target speaker in a harsh acoustic environment, which comprises\ninterfering speakers and noise (cocktail party scenario). In previous work, it\nhas been shown that employing a Feed-forward Neural Network (FNN) for detecting\nthe target speaker activity is a promising approach to combine the advantage of\ndifferent TAD features (used as network inputs). In this contribution, we\nexploit a larger context window for TAD and compare the performance of FNNs and\nRecurrent Neural Networks (RNNs) with an explicit focus on small network\ntopologies as desirable for embedded acoustic signal processing systems. More\nspecifically, the investigations include a comparison between three different\ntypes of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent\nUnits. The results indicate that all versions of RNNs outperform FNNs for the\ntask of TAD."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.09089v2", 
    "title": "What Makes Audio Event Detection Harder than Classification?", 
    "arxiv-id": "1612.09089v2", 
    "author": "Alfred Mertins", 
    "publish": "2016-12-29T10:24:57Z", 
    "summary": "There is a common observation that audio event classification is easier to\ndeal with than detection. So far, this observation has been accepted as a fact\nand we lack a careful analysis. In this paper, we reason the rationale behind\nthis fact and, more importantly, leverage them to benefit the audio event\ndetection task. We present an improved detection pipeline in which a\nverification step is appended to augment a detection system. This step employs\na high-quality event classifier to postprocess the benign event hypotheses\noutputted by the detection system and reject false alarms. To demonstrate the\neffectiveness of the proposed pipeline, we implement and pair up different\nevent detectors based on the most common detection schemes and various event\nclassifiers, ranging from the standard bag-of-words model to the\nstate-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst\ndataset show significant improvements to detection performance. More\nimportantly, these improvements are consistent for all detector-classifier\ncombinations."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1612.09150v2", 
    "title": "Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian   Process Latent Variable Model", 
    "arxiv-id": "1612.09150v2", 
    "author": "Jia-Ching Wang", 
    "publish": "2016-12-29T14:05:56Z", 
    "summary": "Traditional speech enhancement techniques modify the magnitude of a speech in\ntime-frequency domain, and use the phase of a noisy speech to resynthesize a\ntime domain speech. This work proposes a complex-valued Gaussian process latent\nvariable model (CGPLVM) to enhance directly the complex-valued noisy spectrum,\nmodifying not only the magnitude but also the phase. The main idea that\nunderlies the developed method is the modeling of short-time Fourier transform\n(STFT) coefficients across the time frames of a speech as a proper complex\nGaussian process (GP) with noise added. The proposed method is based on\nprojecting the spectrum into a low-dimensional subspace. The likelihood\ncriterion is used to optimize the hyperparameters of the model. Experiments\nwere carried out on the CHTTL database, which contains the digits zero to nine\nin Mandarin. Several standard measures are used to demonstrate that the\nproposed method outperforms baseline methods."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1701.03834v1", 
    "title": "On Higher Order Positive Differential Energy Operator", 
    "arxiv-id": "1701.03834v1", 
    "author": "Mohammad Bagher Shamsollahi", 
    "publish": "2017-01-09T15:10:38Z", 
    "summary": "The higher order differential energy operator (DEO), denoted via\n$\\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser\noperator. The DEO helps measuring the higher order gauge of energy of a signal\nwhich is useful for AM-FM demodulation. However, the energy criterion defined\nby the DEO is not compliant with the presumption of positivity of energy. In\nthis paper we introduce a higher order operator called Positive Differential\nEnergy Operator (PDEO). This operator which can be obtained using alternative\nrecursive relations, resolves the energy sign problem. The simulations\ndemonstrate that the proposed operator can outperform DEOs in terms of Average\nSignal to Error Ratio (ASER) in AM/FM demodulation."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.00025v1", 
    "title": "An Experimental Analysis of the Entanglement Problem in   Neural-Network-based Music Transcription Systems", 
    "arxiv-id": "1702.00025v1", 
    "author": "Gerhard Widmer", 
    "publish": "2017-01-31T19:21:41Z", 
    "summary": "Several recent polyphonic music transcription systems have utilized deep\nneural networks to achieve state of the art results on various benchmark\ndatasets, pushing the envelope on framewise and note-level performance\nmeasures. Unfortunately we can observe a sort of glass ceiling effect. To\ninvestigate this effect, we provide a detailed analysis of the particular kinds\nof errors that state of the art deep neural transcription systems make, when\ntrained and tested on a piano transcription task. We are ultimately forced to\ndraw a rather disheartening conclusion: the networks seem to learn combinations\nof notes, and have a hard time generalizing to unseen combinations of notes.\nFurthermore, we speculate on various means to alleviate this situation."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.01999v1", 
    "title": "Identification of Voice Utterance with Aging Factor Using the Method of   MFCC Multichannel", 
    "arxiv-id": "1702.01999v1", 
    "author": "Agfianto Eko Putra", 
    "publish": "2017-02-07T13:19:08Z", 
    "summary": "This research was conducted to develop a method to identify voice utterance.\nFor voice utterance that encounters change caused by aging factor, with the\ninterval of 10 to 25 years. The change of voice utterance influenced by aging\nfactor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).\nHowever, the level of the compatibility of the feature may be dropped down to\n55%. While the ones which do not encounter it may reach 95%. To improve the\ncompatibility of the changing voice feature influenced by aging factor, then\nthe method of the more specific feature extraction is developed: which is by\nseparating the voice into several channels, suggested as MFCC multichannel,\nconsisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank\n(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that\nfor model M5FB and M2FB have the highest score in the level of compatibility\nwith 85% and 82% with 25 years interval. While model M5FB gets the highest\nscore of 86% for 10 years time interval."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.02130v1", 
    "title": "On the Importance of Temporal Context in Proximity Kernels: A Vocal   Separation Case Study", 
    "arxiv-id": "1702.02130v1", 
    "author": "Mark Sandler", 
    "publish": "2017-02-07T18:41:31Z", 
    "summary": "Musical source separation methods exploit source-specific spectral\ncharacteristics to facilitate the decomposition process. Kernel Additive\nModelling (KAM) models a source applying robust statistics to time-frequency\nbins as specified by a source-specific kernel, a function defining similarity\nbetween bins. Kernels in existing approaches are typically defined using\nmetrics between single time frames. In the presence of noise and other sound\nsources information from a single-frame, however, turns out to be unreliable\nand often incorrect frames are selected as similar. In this paper, we\nincorporate a temporal context into the kernel to provide additional\ninformation stabilizing the similarity search. Evaluated in the context of\nvocal separation, our simple extension led to a considerable improvement in\nseparation quality compared to previous kernels."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.02285v1", 
    "title": "Speaker Change Detection Using Features through A Neural Network Speaker   Classifier", 
    "arxiv-id": "1702.02285v1", 
    "author": "Aravind Ganapathiraju", 
    "publish": "2017-02-08T04:37:40Z", 
    "summary": "The mechanism proposed here is for real-time speaker change detection in\nconversations, which firstly trains a neural network text-independent speaker\nclassifier using in-domain speaker data. Through the network, features of\nconversational speech from out-of-domain speakers are then converted into\nlikelihood vectors, i.e. similarity scores comparing to the in-domain speakers.\nThese transformed features demonstrate very distinctive patterns, which\nfacilitates differentiating speakers and enable speaker change detection with\nsome straight-forward distance metrics. The speaker classifier and the speaker\nchange detector are trained/tested using speech of the first 200 (in-domain)\nand the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For\nthe speaker classification, 100% accuracy at a 200 speaker size is achieved on\nany testing file, given the speech duration is at least 0.97 seconds. For the\nspeaker change detection using speaker classification outputs, performance\nbased on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms\nof error rate and F1 score, using synthesized data by concatenating speech from\nvarious speakers. It captures close to 97% of the changes by comparing the\ncurrent second of speech with the previous second, which is very competitive\namong literature using other methods."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.02289v1", 
    "title": "Neural Network Based Speaker Classification and Verification Systems   with Enhanced Features", 
    "arxiv-id": "1702.02289v1", 
    "author": "Aravind Ganapathiraju", 
    "publish": "2017-02-08T04:59:00Z", 
    "summary": "This work presents a novel framework based on feed-forward neural network for\ntext-independent speaker classification and verification, two related systems\nof speaker recognition. With optimized features and model training, it achieves\n100% classification rate in classification and less than 6% Equal Error Rate\n(ERR), using merely about 1 second and 5 seconds of data respectively. Features\nwith stricter Voice Active Detection (VAD) than the regular one for speech\nrecognition ensure extracting stronger voiced portion for speaker recognition,\nspeaker-level mean and variance normalization helps to eliminate the\ndiscrepancy between samples from the same speaker. Both are proven to improve\nthe system performance. In building the neural network speaker classifier, the\nnetwork structure parameters are optimized with grid search and dynamically\nreduced regularization parameters are used to avoid training terminated in\nlocal minimum. It enables the training goes further with lower cost. In speaker\nverification, performance is improved with prediction score normalization,\nwhich rewards the speaker identity indices with distinct peaks and penalizes\nthe weak ones with high scores but more competitors, and speaker-specific\nthresholding, which significantly reduces ERR in the ROC curve. TIMIT corpus\nwith 8K sampling rate is used here. First 200 male speakers are used to train\nand test the classification performance. The testing files of them are used as\nin-domain registered speakers, while data from the remaining 126 male speakers\nare used as out-of-domain speakers, i.e. imposters in speaker verification."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1702.06724v1", 
    "title": "A new cosine series antialiasing function and its application to   aliasing-free glottal source models for speech and singing synthesis", 
    "arxiv-id": "1702.06724v1", 
    "author": "Toshio Irino", 
    "publish": "2017-02-22T09:36:48Z", 
    "summary": "We formulated and implemented a procedure to generate aliasing-free\nexcitation source signals based on the Fujisaki- Ljungqvist model. It uses a\nnew antialiasing filter in the contin- uous time domain followed by an IIR\ndigital filter for response equalization. We introduced a general designing\nprocedure of cosine series to design the new antialiasing function. We also\napplied this new procedure to revise out previous implementa- tion of the\nantialiased Fant-Liljencrants model. Combination of these signals and a lattice\nimplementation of time varying vocal tract model provides a reliable and\nflexible basis to test F0 ex- tractors and source aperiodicity analysis\nmethods. MATLAB implementation of these antialiased excitation source models\nare available as part of our open source tools for speech science"
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1703.00009v1", 
    "title": "Nonlinear Model and its Inverse of an Audio System", 
    "arxiv-id": "1703.00009v1", 
    "author": "Alessandro Loriga", 
    "publish": "2017-02-28T18:26:44Z", 
    "summary": "This computer science master thesis aims at modelling the nonlinearities of a\nloudspeaker. A piecewise linear approximation is initially explored and then we\npresent a nonlinear Volterra model to simulate the behavior of the system. The\ngeneral theory of continuous and discrete Volterra series is summarised. A\nNormalized Least Mean Square algorithm is used to determine the Volterra series\nto third order. We also present as inverted system which is trained with the\nsame algorithm. Training data for the models were collected measuring a\nphysical speaker using a laser interferometer. Results indicate a decrease in\nMean Squared Error compared to the linear model with a dependency on the\nparticular test signal, the order and the parameters of the model."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1703.00384v1", 
    "title": "Nonlinear Volterra model of a loudspeaker behavior based on Laser   Doppler Vibrometry", 
    "arxiv-id": "1703.00384v1", 
    "author": "Elisabeth Dumont", 
    "publish": "2017-02-28T11:00:19Z", 
    "summary": "We demonstrate the capabilities of nonlinear Volterra models to simulate the\nbehavior of an audio system and compare them to linear filters. In this paper a\nnonlinear model of an audio system based on Volterra series is presented and\nNormalized Least Mean Square algorithm is used to determine the Volterra series\nto third order. Training data for the models were collected measuring a\nphysical speaker using a laser interferometer. We explore several training\nsignals and filter's parameters. Results indicate a decrease in Mean Squared\nError compared to the linear model with a dependency on the particular test\nsignal, the order and the parameters of the model."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1703.02318v1", 
    "title": "Linear and Circular Microphone Array for Remote Surveillance: Simulated   Performance Analysis", 
    "arxiv-id": "1703.02318v1", 
    "author": "Luis Weruaga", 
    "publish": "2017-03-07T10:36:50Z", 
    "summary": "Acoustic beamforming with a microphone array represents an adequate\ntechnology for remote acoustic surveillance, as the system has no mechanical\nparts and it has moderate size. However, in order to accomplish real\nimplementation, several challenges need to be addressed, such as array\ngeometry, microphone characteristics, and the digital beamforming algorithms.\nThis paper presents a simulated analysis on the effect of the array geometry in\nthe beamforming response. Two geometries are considered, namely, the linear and\nthe circular geometry. The analysis is performed with computer simulations to\nmimic reality. The future steps comprise the construction of the physical\nmicrophone array, and the software implementation on a multichannel digital\nsignal processing (DSP) system."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1703.05003v1", 
    "title": "On the Importance of Super-Gaussian Speech Priors for Pre-Trained Speech   Enhancement", 
    "arxiv-id": "1703.05003v1", 
    "author": "Timo Gerkmann", 
    "publish": "2017-03-15T08:33:38Z", 
    "summary": "For enhancing noisy signals, pre-trained single-channel speech enhancement\nschemes exploit prior knowledge about the shape of typical speech structures.\nThis knowledge is obtained from training data for which methods from machine\nlearning are used, e.g., Mixtures of Gaussians, nonnegative matrix\nfactorization, and deep neural networks. If only speech envelopes are employed\nas prior speech knowledge, e.g., to meet requirements in terms of computational\ncomplexity and memory consumption, Wiener-like enhancement filters will not be\nable to reduce noise components between speech spectral harmonics. In this\npaper, we highlight the role of clean speech estimators that employ\nsuper-Gaussian speech priors in particular for pre- trained approaches when\nspectral envelope models are used. In the 2000s, such estimators have been\nconsidered by many researchers for improving non-trained enhancement schemes.\nHowever, while the benefit of super-Gaussian clean speech estimators in\nnon-trained enhancement schemes is limited, we point out that these estimators\nmake a much larger difference for enhancement schemes that employ pre-trained\nenvelope models. We show that for such pre-trained enhancements schemes super-\nGaussian estimators allow for a suppression of annoying residual noises which\nare not reduced using Gaussian filters such as the Wiener filter. As a\nconsequence, considerable improvements in terms of Perceptual Evaluation of\nSpeech Quality and segmental signal-to-noise ratios are achieved."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/1703.06052v1", 
    "title": "Attention and Localization based on a Deep Convolutional Recurrent Model   for Weakly Supervised Audio Tagging", 
    "arxiv-id": "1703.06052v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2017-03-17T15:31:58Z", 
    "summary": "Audio tagging aims to perform multi-label classification on audio chunks and\nit is a newly proposed task in the Detection and Classification of Acoustic\nScenes and Events 2016 (DCASE 2016) challenge. This task encourages research\nefforts to better analyze and understand the content of the huge amounts of\naudio data on the web. The difficulty in audio tagging is that it only has a\nchunk-level label without a frame-level label. This paper presents a weakly\nsupervised method to not only predict the tags but also indicate the temporal\nlocations of the occurred acoustic events. The attention scheme is found to be\neffective in identifying the important frames while ignoring the unrelated\nframes. The proposed framework is a deep convolutional recurrent model with two\nauxiliary modules: an attention module and a localization module. The proposed\nalgorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art\nperformance was achieved on the evaluation set with equal error rate (EER)\nreduced from 0.13 to 0.11, compared with the convolutional recurrent baseline\nsystem."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0006026v2", 
    "title": "Online Correction of Dispersion Error in 2D Waveguide Meshes", 
    "arxiv-id": "cs/0006026v2", 
    "author": "Davide Rocchesso", 
    "publish": "2000-06-12T06:08:57Z", 
    "summary": "An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by\nmodels discretizing the wave equation on the time-space grid (finite difference\nmethods), or locally discretizing the solution of the wave equation (waveguide\nmeshes). The two approaches provide equivalent computational structures, and\nintroduce numerical dispersion that induces a misalignment of the modes from\ntheir theoretical positions. Prior literature shows that dispersion can be\narbitrarily reduced by oversizing and oversampling the mesh, or by adpting\noffline warping techniques. In this paper we propose to reduce numerical\ndispersion by embedding warping elements, i.e., properly tuned allpass filters,\nin the structure. The resulting model exhibits a significant reduction in\ndispersion, and requires less computational resources than a regular mesh\nstructure having comparable accuracy."
},{
    "category": "cs.NE", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0006039v1", 
    "title": "Orthogonal Least Squares Algorithm for the Approximation of a Map and   its Derivatives with a RBF Network", 
    "arxiv-id": "cs/0006039v1", 
    "author": "Davide Rocchesso", 
    "publish": "2000-06-28T10:17:43Z", 
    "summary": "Radial Basis Function Networks (RBFNs) are used primarily to solve\ncurve-fitting problems and for non-linear system modeling. Several algorithms\nare known for the approximation of a non-linear curve from a sparse data set by\nmeans of RBFNs. However, there are no procedures that permit to define\nconstrains on the derivatives of the curve. In this paper, the Orthogonal Least\nSquares algorithm for the identification of RBFNs is modified to provide the\napproximation of a non-linear 1-in 1-out map along with its derivatives, given\na set of training data. The interest on the derivatives of non-linear functions\nconcerns many identification and control tasks where the study of system\nstability and robustness is addressed. The effectiveness of the proposed\nalgorithm is demonstrated by a study on the stability of a single loop feedback\nsystem."
},{
    "category": "cs.NA", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0101003v1", 
    "title": "Signal-Theoretic Characterization of Waveguide Mesh Geometries for   Models of Two--Dimensional Wave Propagation in Elastic Media", 
    "arxiv-id": "cs/0101003v1", 
    "author": "Davide Rocchesso", 
    "publish": "2001-01-04T07:41:59Z", 
    "summary": "Waveguide Meshes are efficient and versatile models of wave propagation along\na multidimensional ideal medium. The choice of the mesh geometry affects both\nthe computational cost and the accuracy of simulations. In this paper, we focus\non 2D geometries and use multidimensional sampling theory to compare the\nsquare, triangular, and hexagonal meshes in terms of sampling efficiency and\ndispersion error under conditions of critical sampling. The analysis shows that\nthe triangular geometry exhibits the most desirable tradeoff between accuracy\nand computational cost."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0203015v1", 
    "title": "Towards Experimental Nanosound Using Almost Disjoint Set Theory", 
    "arxiv-id": "cs/0203015v1", 
    "author": "Cameron L Jones", 
    "publish": "2002-03-12T09:21:39Z", 
    "summary": "Music composition using digital audio sequence editors is increasingly\nperformed in a visual workspace where sound complexes are built from discrete\nsound objects, called gestures that are arranged in time and space to generate\na continuous composition. The visual workspace, common to most industry\nstandard audio loop sequencing software, is premised on the arrangement of\ngestures defined with geometric shape properties. Here, one aspect of fractal\nset theory was validated using audio-frequency sets to evaluate self-affine\nscaling behavior when new sound complexes are built through union and\nintersection operations on discrete musical gestures. Results showed that\nintersection of two sets revealed lower complexity compared with the union\noperator, meaning that the intersection of two sound gestures is an almost\ndisjoint set, and in accord with formal logic. These results are also discussed\nwith reference to fuzzy sets, cellular automata, nanotechnology and\nself-organization to further explore the link between sequenced notation and\ncomplexity."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0204004v1", 
    "title": "Models and Tools for Collaborative Annotation", 
    "arxiv-id": "cs/0204004v1", 
    "author": "Kazuaki Maeda", 
    "publish": "2002-04-03T17:25:39Z", 
    "summary": "The Annotation Graph Toolkit (AGTK) is a collection of software which\nfacilitates development of linguistic annotation tools. AGTK provides a\ndatabase interface which allows applications to use a database server for\npersistent storage. This paper discusses various modes of collaborative\nannotation and how they can be supported with tools built using AGTK and its\ndatabase interface. We describe the relational database schema and API, and\ndescribe a version of the TableTrans tool which supports collaborative\nannotation. The remainder of the paper discusses a high-level query language\nfor annotation graphs, along with optimizations, in support of expressive and\nefficient access to the annotations held on a large central server. The paper\ndemonstrates that it is straightforward to support a variety of different\nlevels of collaborative annotation with existing AGTK-based tools, with a\nminimum of additional programming effort."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0204005v1", 
    "title": "Creating Annotation Tools with the Annotation Graph Toolkit", 
    "arxiv-id": "cs/0204005v1", 
    "author": "Haejoong Lee", 
    "publish": "2002-04-03T17:32:53Z", 
    "summary": "The Annotation Graph Toolkit is a collection of software supporting the\ndevelopment of annotation tools based on the annotation graph model. The\ntoolkit includes application programming interfaces for manipulating annotation\ngraph data and for importing data from other formats. There are interfaces for\nthe scripting languages Tcl and Python, a database interface, specialized\ngraphical user interfaces for a variety of annotation tasks, and several sample\napplications. This paper describes all the toolkit components for the benefit\nof would-be application developers."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4970932", 
    "link": "http://arxiv.org/pdf/cs/0204006v1", 
    "title": "TableTrans, MultiTrans, InterTrans and TreeTrans: Diverse Tools Built on   the Annotation Graph Toolkit", 
    "arxiv-id": "cs/0204006v1", 
    "author": "Salim Zayat", 
    "publish": "2002-04-03T17:18:49Z", 
    "summary": "Four diverse tools built on the Annotation Graph Toolkit are described. Each\ntool associates linguistic codes and structures with time-series data. All are\nbased on the same software library and tool architecture. TableTrans is for\nobservational coding, using a spreadsheet whose rows are aligned to a signal.\nMultiTrans is for transcribing multi-party communicative interactions recorded\nusing multi-channel signals. InterTrans is for creating interlinear text\naligned to audio. TreeTrans is for creating and manipulating syntactic trees.\nThis work demonstrates that the development of diverse tools and re-use of\nsoftware components is greatly facilitated by a common high-level application\nprogramming interface for representing the data and managing input/output,\ntogether with a common architecture for managing the interaction of multiple\ncomponents."
},{
    "category": "cs.HC", 
    "doi": "10.1145/503376.503454", 
    "link": "http://arxiv.org/pdf/cs/0205053v1", 
    "title": "Sotto Voce: Exploring the Interplay of Conversation and Mobile Audio   Spaces", 
    "arxiv-id": "cs/0205053v1", 
    "author": "Allison Woodruff", 
    "publish": "2002-05-21T02:55:20Z", 
    "summary": "In addition to providing information to individual visitors, electronic\nguidebooks have the potential to facilitate social interaction between visitors\nand their companions. However, many systems impede visitor interaction. By\ncontrast, our electronic guidebook, Sotto Voce, has social interaction as a\nprimary design goal. The system enables visitors to share audio information -\nspecifically, they can hear each other's guidebook activity using a\ntechnologically mediated audio eavesdropping mechanism. We conducted a study of\nvisitors using Sotto Voce while touring a historic house. The results indicate\nthat visitors are able to use the system effectively, both as a conversational\nresource and as an information appliance. More surprisingly, our results\nsuggest that the technologically mediated audio often cohered the visitors'\nconversation and activity to a far greater degree than audio delivered through\nthe open air."
},{
    "category": "cs.HC", 
    "doi": "10.1145/642611.642686", 
    "link": "http://arxiv.org/pdf/cs/0304002v1", 
    "title": "The Mad Hatter&acute;s Cocktail Party: A Social Mobile Audio Space   Supporting Multiple Simultaneous Conversations", 
    "arxiv-id": "cs/0304002v1", 
    "author": "Allison Woodruff", 
    "publish": "2003-04-01T05:15:05Z", 
    "summary": "This paper presents a mobile audio space intended for use by gelled social\ngroups. In face-to-face interactions in such social groups, conversational\nfloors change frequently, e.g., two participants split off to form a new\nconversational floor, a participant moves from one conversational floor to\nanother, etc. To date, audio spaces have provided little support for such\ndynamic regroupings of participants, either requiring that the participants\nexplicitly specify with whom they wish to talk or simply presenting all\nparticipants as though they are in a single floor. By contrast, the audio space\ndescribed here monitors participant behavior to identify conversational floors\nas they emerge. The system dynamically modifies the audio delivered to each\nparticipant to enhance the salience of the participants with whom they are\ncurrently conversing. We report a user study of the system, focusing on\nconversation analytic results."
},{
    "category": "cs.LG", 
    "doi": "10.1063/1.2826943", 
    "link": "http://arxiv.org/pdf/cs/0612096v1", 
    "title": "Using state space differential geometry for nonlinear blind source   separation", 
    "arxiv-id": "cs/0612096v1", 
    "author": "David N. Levin", 
    "publish": "2006-12-19T22:32:18Z", 
    "summary": "Given a time series of multicomponent measurements of an evolving stimulus,\nnonlinear blind source separation (BSS) seeks to find a \"source\" time series,\ncomprised of statistically independent combinations of the measured components.\nIn this paper, we seek a source time series with local velocity cross\ncorrelations that vanish everywhere in stimulus state space. However, in an\nearlier paper the local velocity correlation matrix was shown to constitute a\nmetric on state space. Therefore, nonlinear BSS maps onto a problem of\ndifferential geometry: given the metric observed in the measurement coordinate\nsystem, find another coordinate system in which the metric is diagonal\neverywhere. We show how to determine if the observed data are separable in this\nway, and, if they are, we show how to construct the required transformation to\nthe source coordinate system, which is essentially unique except for an unknown\nrotation that can be found by applying the methods of linear BSS. Thus, the\nproposed technique solves nonlinear BSS in many situations or, at least,\nreduces it to linear BSS, without the use of probabilistic, parametric, or\niterative procedures. This paper also describes a generalization of this\nmethodology that performs nonlinear independent subspace separation. In every\ncase, the resulting decomposition of the observed data is an intrinsic property\nof the stimulus' evolution in the sense that it does not depend on the way the\nobserver chooses to view it (e.g., the choice of the observing machine's\nsensors). In other words, the decomposition is a property of the evolution of\nthe \"real\" stimulus that is \"out there\" broadcasting energy to the observer.\nThe technique is illustrated with analytic and numerical examples."
},{
    "category": "cs.SD", 
    "doi": "10.1063/1.2826943", 
    "link": "http://arxiv.org/pdf/cs/0612107v1", 
    "title": "Voiced speech as secondary response of a self-consistent fundamental   drive", 
    "arxiv-id": "cs/0612107v1", 
    "author": "Friedhelm R. Drepper", 
    "publish": "2006-12-21T12:39:45Z", 
    "summary": "Voiced segments of speech are assumed to be composed of non-stationary\nacoustic objects which can be described as stationary response of a\nnon-stationary fundamental drive (FD) process and which are furthermore suited\nto reconstruct the hidden FD by using a voice adapted (self-consistent)\npart-tone decomposition of the speech signal. The universality and robustness\nof human pitch perception encourages the reconstruction of a band-limited FD in\nthe frequency range of the pitch. The self-consistent decomposition of voiced\ncontinuants generates several part-tones which can be confirmed to be\ntopologically equivalent to corresponding acoustic modes of the excitation on\nthe transmitter side. As topologically equivalent image of a glottal master\noscillator, the self-consistent FD is suited to serve as low frequency part of\nthe basic time-scale separation of auditive perception and to describe the\nbroadband voiced excitation as entrained (synchronized) and/or modulated\nprimary response. Being guided by the acoustic correlates of pitch and loudness\nperception, the time-scale separation avoids the conventional assumption of\nstationary excitation and represents the basic decoding step of an advanced\nprecision transmission protocol of self-consistent (voiced) acoustic objects.\nThe present study is focussed on the adaptation of the trajectories (contours)\nof the centre filter frequency of the part-tones to the chirp of the glottal\nmaster oscillator."
},{
    "category": "cs.SD", 
    "doi": "10.1063/1.2826943", 
    "link": "http://arxiv.org/pdf/cs/0612138v1", 
    "title": "Accommodating Sample Size Effect on Similarity Measures in Speaker   Clustering", 
    "arxiv-id": "cs/0612138v1", 
    "author": "John R. Kender", 
    "publish": "2006-12-28T06:39:55Z", 
    "summary": "We investigate the symmetric Kullback-Leibler (KL2) distance in speaker\nclustering and its unreported effects for differently-sized feature matrices.\nSpeaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)\nvectors, and features are compared using the KL2 metric to form clusters of\nspeech segments for each speaker. We make two observations with respect to\nclustering based on KL2: 1.) The accuracy of clustering is strongly dependent\non the absolute lengths of the speech segments and their extracted feature\nvectors. 2.) The accuracy of the similarity measure strongly degrades with the\nlength of the shorter of the two speech segments. These effects of length can\nbe attributed to the measure of covariance used in KL2. We demonstrate an\nempirical correction of this sample-size effect that increases clustering\naccuracy. We draw parallels to two Vector Quantization-based (VQ) similarity\nmeasures, one which exhibits an equivalent effect of sample size, and the\nsecond being less influenced by it."
},{
    "category": "cs.SD", 
    "doi": "10.1063/1.2826943", 
    "link": "http://arxiv.org/pdf/cs/0612139v1", 
    "title": "Alignment of Speech to Highly Imperfect Text Transcriptions", 
    "arxiv-id": "cs/0612139v1", 
    "author": "John R. Kender", 
    "publish": "2006-12-28T06:45:43Z", 
    "summary": "We introduce a novel and inexpensive approach for the temporal alignment of\nspeech to highly imperfect transcripts from automatic speech recognition (ASR).\nTranscripts are generated for extended lecture and presentation videos, which\nin some cases feature more than 30 speakers with different accents, resulting\nin highly varying transcription qualities. In our approach we detect a subset\nof phonemes in the speech track, and align them to the sequence of phonemes\nextracted from the transcript. We report on the results for 4 speech-transcript\nsets ranging from 22 to 108 minutes. The alignment performance is promising,\nshowing a correct matching of phonemes within 10, 20, 30 second error margins\nfor more than 60%, 75%, 90% of text, respectively, on average."
},{
    "category": "cs.SD", 
    "doi": "10.1063/1.2826943", 
    "link": "http://arxiv.org/pdf/cs/0703049v1", 
    "title": "Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem", 
    "arxiv-id": "cs/0703049v1", 
    "author": "Olga A. Savenkova", 
    "publish": "2007-03-10T23:59:55Z", 
    "summary": "Speech recognition based on the syllable segment is discussed in this paper.\nThe principal search methods in space of states for the speech recognition\nproblem by segment-syllabic parameters trajectory synthesis are investigated.\nRecognition as comparison the parameters trajectories in chosen speech units on\nthe sections of the segmented speech is realized. Some experimental results are\ngiven and discussed."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0705.4654v1", 
    "title": "Local Area Damage Detection in Composite Structures Using Piezoelectric   Transducers", 
    "arxiv-id": "0705.4654v1", 
    "author": "Donald A. Sofge", 
    "publish": "2007-05-31T17:19:17Z", 
    "summary": "An integrated and automated smart structures approach for structural health\nmonitoring is presented, utilizing an array of piezoelectric transducers\nattached to or embedded within the structure for both actuation and sensing.\nThe system actively interrogates the structure via broadband excitation of\nmultiple actuators across a desired frequency range. The structure's vibration\nsignature is then characterized by computing the transfer functions between\neach actuator/sensor pair, and compared to the baseline signature. Experimental\nresults applying the system to local area damage detection in a MD Explorer\nrotorcraft composite flexbeam are presented."
},{
    "category": "cs.MM", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0801.0625v1", 
    "title": "On the Robustness of the Delay-Based Fingerprint Embedding Scheme", 
    "arxiv-id": "0801.0625v1", 
    "author": "Shiguo Lian", 
    "publish": "2008-01-04T03:15:56Z", 
    "summary": "The delay-based fingerprint embedding was recently proposed to support more\nusers in secure media distribution scenario. In this embedding scheme, some\nusers are assigned the same fingerprint code with only different embedding\ndelay. The algorithm's robustness against collusion attacks is investigated.\nHowever, its robustness against common desynchronization attacks, e.g.,\ncropping and time shifting, is not considered. In this paper, desynchronization\nattacks are used to break the delay-based fingerprint embedding algorithm. To\nimprove the robustness, two means are proposed to keep the embedded fingerprint\ncodes synchronized, i.e., adding a synchronization fingerprint and adopting the\nrelative delay to detect users. Analyses and experiments are given to show the\nimprovements."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0804.3241v2", 
    "title": "A Synthesizer Based on Frequency-Phase Analysis and Square Waves", 
    "arxiv-id": "0804.3241v2", 
    "author": "Sossio Vergara", 
    "publish": "2008-04-21T06:32:23Z", 
    "summary": "This article introduces an effective generalization of the polar flavor of\nthe Fourier Theorem based on a new method of analysis. Under the premises of\nthe new theory an ample class of functions become viable as bases, with the\nfurther advantage of using the same basis for analysis and reconstruction. In\nfact other tools, like the wavelets, admit specially built nonorthogonal bases\nbut require different bases for analysis and reconstruction (biorthogonal and\ndual bases) and vectorial coordinates; this renders those systems unintuitive\nand computing intensive. As an example of the advantages of the new\ngeneralization of the Fourier Theorem, this paper introduces a novel method for\nthe synthesis that is based on frequency-phase series of square waves (the\nequivalent of the polar Fourier Theorem but for nonorthogonal bases). The\nresulting synthesizer is very efficient needing only few components, frugal in\nterms of computing needs, and viable for many applications."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0809.3214v2", 
    "title": "A Statistical Approach to Modeling Indian Classical Music Performance", 
    "arxiv-id": "0809.3214v2", 
    "author": "Kartik Mahto", 
    "publish": "2008-09-18T17:38:57Z", 
    "summary": "A raga is a melodic structure with fixed notes and a set of rules\ncharacterizing a certain mood endorsed through performance. By a vadi swar is\nmeant that note which plays the most significant role in expressing the raga. A\nsamvadi swar similarly is the second most significant note. However, the\ndetermination of their significance has an element of subjectivity and hence we\nare motivated to find some truths through an objective analysis. The paper\nproposes a probabilistic method of note detection and demonstrates how the\nrelative frequency (relative number of occurrences of the pitch) of the more\nimportant notes stabilize far more quickly than that of others. In addition, a\ncount for distinct transitory and similar looking non-transitory (fundamental)\nfrequency movements (but possibly embedding distinct emotions!) between the\nnotes is also taken depicting the varnalankars or musical ornaments decorating\nthe notes and note sequences as rendered by the artist. They reflect certain\nstructural properties of the ragas. Several case studies are presented."
},{
    "category": "cs.CV", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0809.4501v1", 
    "title": "Audio Classification from Time-Frequency Texture", 
    "arxiv-id": "0809.4501v1", 
    "author": "Jean-Jacques Slotine", 
    "publish": "2008-09-25T20:54:29Z", 
    "summary": "Time-frequency representations of audio signals often resemble texture\nimages. This paper derives a simple audio classification algorithm based on\ntreating sound spectrograms as texture images. The algorithm is inspired by an\nearlier visual classification scheme particularly efficient at classifying\ntextures. While solely based on time-frequency texture features, the algorithm\nachieves surprisingly good performance in musical instrument classification\nexperiments."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.310667", 
    "link": "http://arxiv.org/pdf/0905.3678v3", 
    "title": "Major and minor. The formula of musical emotions", 
    "arxiv-id": "0905.3678v3", 
    "author": "Vadim R. Madgazin", 
    "publish": "2009-05-22T13:40:36Z", 
    "summary": "The new formulas, which determine sign and amplitude of utilitarian emotions,\nare proposed on the basis of the information theory of emotions. In area of\nperception of musical chords the force of emotions depends on the relative\npitch of sounds of major and minor chords.\n  Is advanced hypothesis that in the perception of a musical chord in the\npsyche caused by the subject value of some objective function L. This function\nis expressed directly through the proportion of the pitch of chord. Major\nchords are expressed as the straight proportions, which generate idea about an\nincrease in the objective function (L>1) and are caused positive utilitarian\nemotions. Minor chords are expressed as the inverse proportion, which generate\nidea about the decrease of objective function (L<1) and are caused negative\nutilitarian emotions.\n  The formula of musical emotions is advanced: Pwe = log(L) =\n(1/M)*log(n1*n2*n3* ... *nM), where M is a quantity of voices of chord, ni -\ninteger number (or reciprocal fraction) from the pitch proportion, which\ncorresponds to the i-th voice of chord.\n  Confined experimental check is produced. The limits of the applicability of\nthe formula of musical emotions are investigated.\n  Keywords: sound, music, chord, major, minor, emotions, the formula of musical\nemotions, the information theory of emotions."
},{
    "category": "math.NA", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/0906.5202v2", 
    "title": "Superposition frames for adaptive time-frequency analysis and fast   reconstruction", 
    "arxiv-id": "0906.5202v2", 
    "author": "Patrick J. Wolfe", 
    "publish": "2009-06-29T19:49:30Z", 
    "summary": "In this article we introduce a broad family of adaptive, linear\ntime-frequency representations termed superposition frames, and show that they\nadmit desirable fast overlap-add reconstruction properties akin to standard\nshort-time Fourier techniques. This approach stands in contrast to many\nadaptive time-frequency representations in the extant literature, which, while\nmore flexible than standard fixed-resolution approaches, typically fail to\nprovide efficient reconstruction and often lack the regular structure necessary\nfor precise frame-theoretic analysis. Our main technical contributions come\nthrough the development of properties which ensure that this construction\nprovides for a numerically stable, invertible signal representation. Our\nprimary algorithmic contributions come via the introduction and discussion of\nspecific signal adaptation criteria in deterministic and stochastic settings,\nbased respectively on time-frequency concentration and nonstationarity\ndetection. We conclude with a short speech enhancement example that serves to\nhighlight potential applications of our approach."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/0909.0599v1", 
    "title": "Codebook Design Method for Noise Robust Speaker Identification based on   Genetic Algorithm", 
    "arxiv-id": "0909.0599v1", 
    "author": "Md. Fayzur Rahman", 
    "publish": "2009-09-03T08:49:54Z", 
    "summary": "In this paper, a novel method of designing a codebook for noise robust\nspeaker identification purpose utilizing Genetic Algorithm has been proposed.\nWiener filter has been used to remove the background noises from the source\nspeech utterances. Speech features have been extracted using standard speech\nparameterization method such as LPC, LPCC, RCC, MFCC, (delta)MFCC and\n(delta)(delta) MFCC. For each of these techniques, the performance of the\nproposed system has been compared. In this codebook design method, Genetic\nAlgorithm has the capability of getting global optimal result and hence\nimproves the quality of the codebook. Comparing with the NOIZEOUS speech\ndatabase, the experimental result shows that 79.62 percent accuracy has been\nachieved."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/0909.3976v2", 
    "title": "The Information Theory of Emotions of Musical Chords", 
    "arxiv-id": "0909.3976v2", 
    "author": "Vadim R. Madgazin", 
    "publish": "2009-09-22T12:24:19Z", 
    "summary": "The paper offers a solution to the centuries-old puzzle - why the major\nchords are perceived as happy and the minor chords as sad - based on the\ninformation theory of emotions. A theory and a formula of musical emotions were\ncreated. They define the sign and the amplitude of the utilitarian emotional\ncoloration of separate major and minor chords through relative pitches of\nconstituent sounds. Keywords: chord, major, minor, the formula of musical\nemotions, the information theory of emotions."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1003.0659v2", 
    "title": "Particle Filtering on the Audio Localization Manifold", 
    "arxiv-id": "1003.0659v2", 
    "author": "Yoav Freund", 
    "publish": "2010-03-02T19:50:38Z", 
    "summary": "We present a novel particle filtering algorithm for tracking a moving sound\nsource using a microphone array. If there are N microphones in the array, we\ntrack all $N \\choose 2$ delays with a single particle filter over time. Since\nit is known that tracking in high dimensions is rife with difficulties, we\ninstead integrate into our particle filter a model of the low dimensional\nmanifold that these delays lie on. Our manifold model is based off of work on\nmodeling low dimensional manifolds via random projection trees [1]. In\naddition, we also introduce a new weighting scheme to our particle filtering\nalgorithm based on recent advancements in online learning. We show that our\nnovel TDOA tracking algorithm that integrates a manifold model can greatly\noutperform standard particle filters on this audio tracking task."
},{
    "category": "cs.OH", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1003.1509v1", 
    "title": "A New Variable Threshold and Dynamic Step Size Based Active Noise   Control System for Improving Performance", 
    "arxiv-id": "1003.1509v1", 
    "author": "A. Krishnan", 
    "publish": "2010-03-07T18:18:46Z", 
    "summary": "Several approaches have been introduced in literature for active noise\ncontrol (ANC) systems. Since FxLMS algorithm appears to be the best choice as a\ncontroller filter, researchers tend to improve performance of ANC systems by\nenhancing and modifying this algorithm. In this paper, modification is done in\nthe existing FxLMS algorithm that provides a new structure for improving the\ntracking performance and convergence rate. The secondary signal y(n) is dynamic\nthresholded by Wavelet transform to improve tracking. The convergence rate is\nimproved by dynamically varying the step size of the error signal."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1003.2441v1", 
    "title": "Up-sampling and Natural Sample Value Computation for Digital Pulse Width   Modulators", 
    "arxiv-id": "1003.2441v1", 
    "author": "Dilip V. Sarwate", 
    "publish": "2010-03-11T23:00:15Z", 
    "summary": "Digital pulse width modulation has been considered for high-fidelity and\nhigh-efficiency audio amplifiers for several years. It has been shown that the\ndistortion can be reduced and the implementation of the system can be\nsimplified if the switching frequency is much higher than the Nyquist rate of\nthe modulating waveform. Hence, the input digital source is normally upsampled\nto a higher frequency. It was also proved that converting uniform samples to\nnatural samples will decrease the harmonic distortion. Thus, in this paper, we\nexamine a new approach that combines upsampling, digital interpolation and\nnatural sampling conversion. This approach uses poly-phase implementation of\nthe digital interpolation filter and digital differentiators. We will show that\nthe structure consists of an FIR-type linear stage and a nonlinear stage. Some\nspectral simulation results of a pulse width modulation system based on this\napproach will also be presented. Finally, we will discuss the improvement of\nthe new approach over old algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1003.5623v1", 
    "title": "Spoken Language Identification Using Hybrid Feature Extraction Methods", 
    "arxiv-id": "1003.5623v1", 
    "author": "Mahesh Chandra", 
    "publish": "2010-03-29T17:48:22Z", 
    "summary": "This paper introduces and motivates the use of hybrid robust feature\nextraction technique for spoken language identification (LID) system. The\nspeech recognizers use a parametric form of a signal to get the most important\ndistinguishable features of speech signal for recognition task. In this paper\nMel-frequency cepstral coefficients (MFCC), Perceptual linear prediction\ncoefficients (PLP) along with two hybrid features are used for language\nIdentification. Two hybrid features, Bark Frequency Cepstral Coefficients\n(BFCC) and Revised Perceptual Linear Prediction Coefficients (RPLP) were\nobtained from combination of MFCC and PLP. Two different classifiers, Vector\nQuantization (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model\n(GMM) were used for classification. The experiment shows better identification\nrate using hybrid feature extraction techniques compared to conventional\nfeature extraction methods.BFCC has shown better performance than MFCC with\nboth classifiers. RPLP along with GMM has shown best identification performance\namong all feature extraction techniques."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1003.5627v1", 
    "title": "Wavelet-Based Mel-Frequency Cepstral Coefficients for Speaker   Identification using Hidden Markov Models", 
    "arxiv-id": "1003.5627v1", 
    "author": "Hanaa S. Ali", 
    "publish": "2010-03-29T17:54:55Z", 
    "summary": "To improve the performance of speaker identification systems, an effective\nand robust method is proposed to extract speech features, capable of operating\nin noisy environment. Based on the time-frequency multi-resolution property of\nwavelet transform, the input speech signal is decomposed into various frequency\nchannels. For capturing the characteristic of the signal, the Mel-Frequency\nCepstral Coefficients (MFCCs) of the wavelet channels are calculated. Hidden\nMarkov Models (HMMs) were used for the recognition stage as they give better\nrecognition for the speaker's features than Dynamic Time Warping (DTW).\nComparison of the proposed approach with the MFCCs conventional feature\nextraction method shows that the proposed method not only effectively reduces\nthe influence of noise, but also improves recognition. A recognition rate of\n99.3% was obtained using the proposed feature extraction technique compared to\n98.7% using the MFCCs. When the test patterns were corrupted by additive white\nGaussian noise with 20 dB S/N ratio, the recognition rate was 97.3% using the\nproposed method compared to 93.3% using the MFCCs."
},{
    "category": "cs.PL", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1004.4796v2", 
    "title": "Compiling Signal Processing Code embedded in Haskell via LLVM", 
    "arxiv-id": "1004.4796v2", 
    "author": "Henning Thielemann", 
    "publish": "2010-04-27T13:31:41Z", 
    "summary": "We discuss a programming language for real-time audio signal processing that\nis embedded in the functional language Haskell and uses the Low-Level Virtual\nMachine as back-end. With that framework we can code with the comfort and type\nsafety of Haskell while achieving maximum efficiency of fast inner loops and\nfull vectorisation. This way Haskell becomes a valuable alternative to special\npurpose signal processing languages."
},{
    "category": "cs.HC", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1005.3182v2", 
    "title": "Haptics in computer music : a paradigm shift", 
    "arxiv-id": "1005.3182v2", 
    "author": "Annie Luciani", 
    "publish": "2010-05-18T13:03:47Z", 
    "summary": "With an historical point of view combined with a bibliographic overview, the\narticle discusses the idea that haptic force feedback transducers correspond\nwith a paradigm shift in our real-time tools for creating music. So doing, il\nshows that computer music may be regarded as a major field of research and\napplication for haptics."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1005.5137v1", 
    "title": "Improved Method for Individualization of Head-Related Transfer Functions   on Horizontal Plane Using Reduced Number of Anthropometric Measurements", 
    "arxiv-id": "1005.5137v1", 
    "author": "D. Gunawan", 
    "publish": "2010-05-27T18:02:13Z", 
    "summary": "An important problem to be solved in modeling head-related impulse responses\n(HRIRs) is how to individualize HRIRs so that they are suitable for a listener.\nWe modeled the entire magnitude head-related transfer functions (HRTFs), in\nfrequency domain, for sound sources on horizontal plane of 37 subjects using\nprincipal components analysis (PCA). The individual magnitude HRTFs could be\nmodeled adequately well by a linear combination of only ten orthonormal basis\nfunctions. The goal of this research was to establish multiple linear\nregression (MLR) between weights of basis functions obtained from PCA and fewer\nanthropometric measurements in order to individualize a given listener's HRTFs\nwith his or her own anthropomety. We proposed here an improved\nindividualization method based on MLR of weights of basis functions by\nutilizing 8 chosen out of 27 anthropometric measurements. Our objective\nexperiments' results show a superior performance than that of our previous work\non individualizing minimum phase HRIRs and also better than similar research.\nThe proposed individualization method shows that the individualized magnitude\nHRTFs could approximated well the the original ones with small error. Moving\nsound employing the reconstructed HRIRs could be perceived as if it was moving\naround the horizontal plane."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1008.4658v2", 
    "title": "A high speed unsupervised speaker retrieval using vector quantization   and second-order statistics", 
    "arxiv-id": "1008.4658v2", 
    "author": "Konstantin Biatov", 
    "publish": "2010-08-27T08:29:45Z", 
    "summary": "This paper describes an effective unsupervised method for query-by-example\nspeaker retrieval. We suppose that only one speaker is in each audio file or in\naudio segment. The audio data are modeled using a common universal codebook.\nThe codebook is based on bag-of-frames (BOF). The features corresponding to the\naudio frames are extracted from all audio files. These features are grouped\ninto clusters using the K-means algorithm. The individual audio files are\nmodeled by the normalized distribution of the numbers of cluster bins\ncorresponding to this file. In the first level the k-nearest to the query files\nare retrieved using vector space representation. In the second level the\nsecond-order statistical measure is applied to obtained k-nearest files to find\nthe final result of the retrieval. The described method is evaluated on the\nsubset of Ester corpus of French broadcast news."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2010.2041604", 
    "link": "http://arxiv.org/pdf/1009.4719v1", 
    "title": "A Fast Audio Clustering Using Vector Quantization and Second Order   Statistics", 
    "arxiv-id": "1009.4719v1", 
    "author": "Konstantin Biatov", 
    "publish": "2010-09-23T20:38:06Z", 
    "summary": "This paper describes an effective unsupervised speaker indexing approach. We\nsuggest a two stage algorithm to speed-up the state-of-the-art algorithm based\non the Bayesian Information Criterion (BIC). In the first stage of the merging\nprocess a computationally cheap method based on the vector quantization (VQ) is\nused. Then in the second stage a more computational expensive technique based\non the BIC is applied. In the speaker indexing task a turning parameter or a\nthreshold is used. We suggest an on-line procedure to define the value of a\nturning parameter without using development data. The results are evaluated\nusing 10 hours of audio data."
},{
    "category": "cs.LG", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1009.4972v1", 
    "title": "Speaker Identification using MFCC-Domain Support Vector Machine", 
    "arxiv-id": "1009.4972v1", 
    "author": "Md. Emdadul Haque", 
    "publish": "2010-09-25T05:32:44Z", 
    "summary": "Speech recognition and speaker identification are important for\nauthentication and verification in security purpose, but they are difficult to\nachieve. Speaker identification methods can be divided into text-independent\nand text-dependent. This paper presents a technique of text-dependent speaker\nidentification using MFCC-domain support vector machine (SVM). In this work,\nmelfrequency cepstrum coefficients (MFCCs) and their statistical distribution\nproperties are used as features, which will be inputs to the neural network.\nThis work firstly used sequential minimum optimization (SMO) learning technique\nfor SVM that improve performance over traditional techniques Chunking, Osuna.\nThe cepstrum coefficients representing the speaker characteristics of a speech\nsegment are computed by nonlinear filter bank analysis and discrete cosine\ntransform. The speaker identification ability and convergence speed of the SVMs\nare investigated for different combinations of features. Extensive experimental\nresults on several samples show the effectiveness of the proposed approach."
},{
    "category": "cs.SD", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1009.5761v1", 
    "title": "Approximate Maximum A Posteriori Inference with Entropic Priors", 
    "arxiv-id": "1009.5761v1", 
    "author": "Matthew D. Hoffman", 
    "publish": "2010-09-29T03:20:40Z", 
    "summary": "In certain applications it is useful to fit multinomial distributions to\nobserved data with a penalty term that encourages sparsity. For example, in\nprobabilistic latent audio source decomposition one may wish to encode the\nassumption that only a few latent sources are active at any given time. The\nstandard heuristic of applying an L1 penalty is not an option when fitting the\nparameters to a multinomial distribution, which are constrained to sum to 1. An\nalternative is to use a penalty term that encourages low-entropy solutions,\nwhich corresponds to maximum a posteriori (MAP) parameter estimation with an\nentropic prior. The lack of conjugacy between the entropic prior and the\nmultinomial distribution complicates this approach. In this report I propose a\nsimple iterative algorithm for MAP estimation of multinomial distributions with\nsparsity-inducing entropic priors."
},{
    "category": "cs.MM", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1010.3951v1", 
    "title": "Alternatives to speech in low bit rate communication systems", 
    "arxiv-id": "1010.3951v1", 
    "author": "Pedro M. Q. Aguiar", 
    "publish": "2010-10-19T15:24:33Z", 
    "summary": "This paper describes a framework and a method with which speech communication\ncan be analyzed. The framework consists of a set of low bit rate, short-range\nacoustic communication systems, such as speech, but that are quite different\nfrom speech. The method is to systematically compare these systems according to\ndifferent objective functions such as data rate, computational overhead,\npsychoacoustic effects and semantics. One goal of this study is to better\nunderstand the nature of human communication. Another goal is to identify\nacoustic communication systems that are more efficient than human speech for\nsome specific purposes."
},{
    "category": "math.CO", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1103.0596v1", 
    "title": "Music By Numbers", 
    "arxiv-id": "1103.0596v1", 
    "author": "Shawn Fowers", 
    "publish": "2011-03-03T02:26:38Z", 
    "summary": "In this paper we present a mathematical way of defining musical modes, we\nderive a formula for the total number of modes and define the musicality of a\nmode as the total number of harmonic chords whithin the mode. We also give an\nalgorithm for the construction of a duet of melodic lines given a sequence of\nnumbers and a mode. We attach the .mus files of the counterpoints obtained by\nusing the sequence of primes and several musical modes."
},{
    "category": "cs.SD", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1105.1383v1", 
    "title": "Topological Considerations for Tuning and Fingering Stringed Instruments", 
    "arxiv-id": "1105.1383v1", 
    "author": "Camille Goudeseune", 
    "publish": "2011-05-06T20:25:36Z", 
    "summary": "We present a formal language for assigning pitches to strings for fingered\nmulti-string instruments, particularly the six-string guitar. Given the\ninstrument's tuning (the strings' open pitches) and the compass of the fingers\nof the hand stopping the strings, the formalism yields a framework for\nsimultaneously optimizing three things: the mapping of pitches to strings, the\nchoice of instrument tuning, and the key of the composition. Final optimization\nrelies on heuristics idiomatic to the tuning, the particular musical style, and\nthe performer's proficiency."
},{
    "category": "cs.SD", 
    "doi": "10.3923/ijepe.2007.274.278", 
    "link": "http://arxiv.org/pdf/1105.2770v2", 
    "title": "Improving Performance of Speaker Identification System Using   Complementary Information Fusion", 
    "arxiv-id": "1105.2770v2", 
    "author": "Goutam Saha", 
    "publish": "2011-05-13T16:32:44Z", 
    "summary": "Feature extraction plays an important role as a front-end processing block in\nspeaker identification (SI) process. Most of the SI systems utilize like\nMel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP),\nLinear Predictive Cepstral Coefficients (LPCC), as a feature for representing\nspeech signal. Their derivations are based on short term processing of speech\nsignal and they try to capture the vocal tract information ignoring the\ncontribution from the vocal cord. Vocal cord cues are equally important in SI\ncontext, as the information like pitch frequency, phase in the residual signal,\netc could convey important speaker specific attributes and are complementary to\nthe information contained in spectral feature sets. In this paper we propose a\nnovel feature set extracted from the residual signal of LP modeling.\nHigher-order statistical moments are used here to find the nonlinear\nrelationship in residual signal. To get the advantages of complementarity vocal\ncord based decision score is fused with the vocal tract based score. The\nexperimental results on two public databases show that fused mode system\noutperforms single spectral features."
},{
    "category": "stat.AP", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1107.0076v1", 
    "title": "KARMA: Kalman-based autoregressive moving average modeling and inference   for formant and antiformant tracking", 
    "arxiv-id": "1107.0076v1", 
    "author": "Patrick J. Wolfe", 
    "publish": "2011-06-30T22:15:07Z", 
    "summary": "Vocal tract resonance characteristics in acoustic speech signals are\nclassically tracked using frame-by-frame point estimates of formant frequencies\nfollowed by candidate selection and smoothing using dynamic programming methods\nthat minimize ad hoc cost functions. The goal of the current work is to provide\nboth point estimates and associated uncertainties of center frequencies and\nbandwidths in a statistically principled state-space framework. Extended Kalman\n(K) algorithms take advantage of a linearized mapping to infer formant and\nantiformant parameters from frame-based estimates of autoregressive moving\naverage (ARMA) cepstral coefficients. Error analysis of KARMA, WaveSurfer, and\nPraat is accomplished in the all-pole case using a manually marked formant\ndatabase and synthesized speech waveforms. KARMA formant tracks exhibit lower\noverall root-mean-square error relative to the two benchmark algorithms, with\nthird formant tracking more challenging. Antiformant tracking performance of\nKARMA is illustrated using synthesized and spoken nasal phonemes. The\nsimultaneous tracking of uncertainty levels enables practitioners to recognize\ntime-varying confidence in parameters of interest and adjust algorithmic\nsettings accordingly."
},{
    "category": "math.OC", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1110.6002v1", 
    "title": "Optimization of frequency quantization", 
    "arxiv-id": "1110.6002v1", 
    "author": "V. N. Tibabishev", 
    "publish": "2011-10-27T08:07:04Z", 
    "summary": "We obtain the functional defining the price and quality of sample readings of\nthe generalized velocities. It is shown that the optimal sampling frequency, in\nthe sense of minimizing the functional quality and price depends on the\nsampling of the upper cutoff frequency of the analog signal of the order of the\ngeneralized velocities measured by the generalized coordinates, the frequency\nproperties of the analog input filter and a maximum sampling rate for\nanalog-digital converter (ADC). An example of calculating the frequency\nquantization for two-tier ADC with an input RC filter."
},{
    "category": "physics.data-an", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1112.4410v1", 
    "title": "Ricean Shadowed Statistical Characterization of Shallow Water Acoustic   Channels for Wireless Communications", 
    "arxiv-id": "1112.4410v1", 
    "author": "J. F. Paris", 
    "publish": "2011-12-19T17:17:10Z", 
    "summary": "In this letter, the statistical behaviour of the shallow water acoustic\nchannel for wireless communications is shown to be well characterized by the\nRicean shadowed distribution, which has never been proposed for communication\npurposes on this type of channel. This characterization is clearly motivated\nfrom statistical and physical perspectives and has both theoretical and\npractical advantages compared to previously proposed models."
},{
    "category": "math.CO", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1201.2654v1", 
    "title": "Musical Modes, Their Associated Chords and Their Musicality", 
    "arxiv-id": "1201.2654v1", 
    "author": "Kent Kidman", 
    "publish": "2012-01-12T19:43:06Z", 
    "summary": "In this paper we present a mathematical way of defining musical modes and we\ndefine the musicality of a mode as a product of three different factors. We\nconclude by classifying the modes which are most musical according to our\ndefinition."
},{
    "category": "cs.PL", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1202.4269v1", 
    "title": "Live-Musikprogrammierung in Haskell", 
    "arxiv-id": "1202.4269v1", 
    "author": "Henning Thielemann", 
    "publish": "2012-02-20T09:44:47Z", 
    "summary": "We aim to compose algorithmic music in an interactive way with multiple\nparticipants. To this end we develop an interpreter for a sub-language of the\nnon-strict functional programming language Haskell that allows to modify the\nprogram during its execution. Our system can be used both for musical\nlive-coding and for demonstration and education of functional programming."
},{
    "category": "cs.NE", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1202.5953v1", 
    "title": "On an Ethical Use of Neural Networks: A Case Study on a North Indian   Raga", 
    "arxiv-id": "1202.5953v1", 
    "author": "Soubhik Chakraborty", 
    "publish": "2012-02-27T14:36:43Z", 
    "summary": "The paper gives an artificial neural network (ANN) approach to time series\nmodeling, the data being instance versus notes (characterized by pitch)\ndepicting the structure of a North Indian raga, namely, Bageshree. Respecting\nthe sentiments of the artists' community, the paper argues why it is more\nethical to model a structure than try and \"manufacture\" an artist by training\nthe neural network to copy performances of artists. Indian Classical Music\ncenters on the ragas, where emotion and devotion are both important and neither\ncan be substituted by such \"calculated artistry\" which the ANN generated copies\nare ultimately up to."
},{
    "category": "cs.SE", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1208.1956v1", 
    "title": "MIDI-LAB, a Powerful Visual Basic Program for Creating MIDI Music", 
    "arxiv-id": "1208.1956v1", 
    "author": "Xi Zhou", 
    "publish": "2012-08-09T15:43:58Z", 
    "summary": "Creating MIDI music can be a practical challenge. In the past, working with\nit was difficult and frustrating to all but the most accomplished and\ndetermined. Now, however, we are offering a powerful Visual Basic program\ncalled MIDI-LAB, that is easy to learn, and instantly rewarding to even the\nnewest users. MIDI-LAB has been developed to give users the ability to quickly\ncreate music with a limitless variety of tunes, tempos, speeds, volumes,\ninstruments, rhythms and major scales. This program has a simple, intuitive,\nand user-friendly interface, which provides a straightforward way to enter\nmusical data with Numbered Musical Notation (NMN) and immediately create MIDI\nmusic. The key feature of this program is the digitalization of music input. It\nvastly simplifies creating, editing, and saving MIDI music. MIDI-LAB can be\nused virtually anywhere to write music for entertainment, teaching, computer\ngames, and mobile phone ringtones."
},{
    "category": "math.DS", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1208.5963v2", 
    "title": "How far are vowel formants from computed vocal tract resonances?", 
    "arxiv-id": "1208.5963v2", 
    "author": "Martti Vainio", 
    "publish": "2012-08-29T16:38:04Z", 
    "summary": "We compare numerically computed resonances of the human vocal tract with\nformants that have been extracted from speech during vowel pronunciation. The\ngeometry of the vocal tract has been obtained by MRI from a male subject, and\nthe corresponding speech has been recorded simultaneously. The resonances are\ncomputed by solving the Helmholtz partial differential equation with the Finite\nElement Method (FEM).\n  Despite a rudimentary exterior space acoustics model, i.e., the Dirichlet\nboundary condition at the mouth opening, the computed resonance structure\ndiffers from the measured formant structure by $\\approx$ 0.7 semitones for [i]\nand [u] having small mouth opening area, and by $\\approx$ 3 semitones for\nvowels [a] and [ae] that have a larger mouth opening. The contribution of the\npossibly open velar port has not been taken into considaration at all which\nadds the discrepancy for [a] in the present data set. We conclude that by\nimproving the exterior space model and properly treating the velar port\nopening, it is possible to computationally attain four lowest vowel formants\nwith an error less than a semitone. The corresponding wave equation model on\nMRI-produced vocal tract geometries is expected to have a comparable accuracy."
},{
    "category": "cs.MM", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1210.0297v2", 
    "title": "Comparison of Speech Activity Detection Techniques for Speaker   Recognition", 
    "arxiv-id": "1210.0297v2", 
    "author": "Goutam Saha", 
    "publish": "2012-10-01T07:10:12Z", 
    "summary": "Speech activity detection (SAD) is an essential component for a variety of\nspeech processing applications. It has been observed that performances of\nvarious speech based tasks are very much dependent on the efficiency of the\nSAD. In this paper, we have systematically reviewed some popular SAD techniques\nand their applications in speaker recognition. Speaker verification system\nusing different SAD technique are experimentally evaluated on NIST speech\ncorpora using Gaussian mixture model- universal background model (GMM-UBM)\nbased classifier for clean and noisy conditions. It has been found that two\nGaussian modeling based SAD is comparatively better than other SAD techniques\nfor different types of noises."
},{
    "category": "q-bio.NC", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1210.2944v1", 
    "title": "Spatial Auditory BCI Paradigm Utilizing N200 and P300 Responses", 
    "arxiv-id": "1210.2944v1", 
    "author": "Tomasz M. Rutkowski", 
    "publish": "2012-10-10T14:59:26Z", 
    "summary": "The paper presents our recent results obtained with a new auditory spatial\nlocalization based BCI paradigm in which the ERP shape differences at early\nlatencies are employed to enhance the traditional P300 responses in an oddball\nexperimental setting. The concept relies on the recent results in auditory\nneuroscience showing a possibility to differentiate early anterior\ncontralateral responses to attended spatial sources. Contemporary\nstimuli-driven BCI paradigms benefit mostly from the P300 ERP latencies in so\ncalled \"aha-response\" settings. We show the further enhancement of the\nclassification results in spatial auditory paradigms by incorporating the N200\nlatencies, which differentiate the brain responses to lateral, in relation to\nthe subject head, sound locations in the auditory space. The results reveal\nthat those early spatial auditory ERPs boost online classification results of\nthe BCI application. The online BCI experiments with the multi-command BCI\nprototype support our research hypothesis with the higher classification\nresults and the improved information-transfer-rates."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1210.6766v1", 
    "title": "Structured Sparsity Models for Multiparty Speech Recovery from   Reverberant Recordings", 
    "arxiv-id": "1210.6766v1", 
    "author": "Volkan Cevher", 
    "publish": "2012-10-25T09:22:59Z", 
    "summary": "We tackle the multi-party speech recovery problem through modeling the\nacoustic of the reverberant chambers. Our approach exploits structured sparsity\nmodels to perform room modeling and speech recovery. We propose a scheme for\ncharacterizing the room acoustic from the unknown competing speech sources\nrelying on localization of the early images of the speakers by sparse\napproximation of the spatial spectra of the virtual sources in a free-space\nmodel. The images are then clustered exploiting the low-rank structure of the\nspectro-temporal components belonging to each source. This enables us to\nidentify the early support of the room impulse response function and its unique\nmap to the room geometry. To further tackle the ambiguity of the reflection\nratios, we propose a novel formulation of the reverberation model and estimate\nthe absorption coefficients through a convex optimization exploiting joint\nsparsity model formulated upon spatio-spectral sparsity of concurrent speech\nrepresentation. The acoustic parameters are then incorporated for separating\nindividual speech signals through either structured sparse recovery or inverse\nfiltering the acoustic channels. The experiments conducted on real data\nrecordings demonstrate the effectiveness of the proposed approach for\nmulti-party speech recovery and recognition."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1212.5091v1", 
    "title": "Maximally Informative Observables and Categorical Perception", 
    "arxiv-id": "1212.5091v1", 
    "author": "Elaine Tsiang", 
    "publish": "2012-12-19T17:40:07Z", 
    "summary": "We formulate the problem of perception in the framework of information\ntheory, and prove that categorical perception is equivalent to the existence of\nan observable that has the maximum possible information on the target of\nperception. We call such an observable maximally informative. Regardless\nwhether categorical perception is real, maximally informative observables can\nform the basis of a theory of perception. We conclude with the implications of\nsuch a theory for the problem of speech perception."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4739462", 
    "link": "http://arxiv.org/pdf/1212.6350v1", 
    "title": "Single-sided Real-time PESQ Score Estimation", 
    "arxiv-id": "1212.6350v1", 
    "author": "Mart\u00edn Varela", 
    "publish": "2012-12-27T11:31:16Z", 
    "summary": "For several years now, the ITU-T's Perceptual Evaluation of Speech Quality\n(PESQ) has been the reference for objective speech quality assessment. It is\nwidely deployed in commercial QoE measurement products, and it has been well\nstudied in the literature. While PESQ does provide reasonably good correlation\nwith subjective scores for VoIP applications, the algorithm itself is not\nusable in a real-time context, since it requires a reference signal, which is\nusually not available in normal conditions. In this paper we provide an\nalternative technique for estimating PESQ scores in a single-sided fashion,\nbased on the Pseudo Subjective Quality Assessment (PSQA) technique."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4803889", 
    "link": "http://arxiv.org/pdf/1302.4382v1", 
    "title": "Finite element computation of elliptical vocal tract impedances using   the two-microphone transfer function method", 
    "arxiv-id": "1302.4382v1", 
    "author": "Oriol Guasch", 
    "publish": "2013-02-15T16:44:17Z", 
    "summary": "The experimental two-microphone transfer function method (TMTF) is adapted to\nthe numerical framework to compute the radiation and input impedances of\nthree-dimensional vocal tracts of elliptical cross section. In its simplest\nversion, the TMTF method only requires measuring the acoustic pressure at two\npoints in an impedance duct and the postprocessing of the corresponding\ntransfer function. However, some considerations are to be taken into account\nwhen using the TMTF method in the numerical context, which constitute the main\nobjective of this paper. In particular, the importance of including absorption\nat the impedance duct walls to avoid lengthy numerical simulations is discussed\nand analytical complex axial wave numbers for elliptical ducts are derived for\nthis purpose. It is also shown how the plane wave restriction of the TMTF\nmethod can be circumvented to some extent by appropriate location of the\nvirtual microphones, thus extending the method frequency range of validity.\nVirtual microphone spacing is also discussed on the basis of the so called\nsingularity factor. Numerical examples include the computation of the radiation\nimpedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for\nsimplified vocal tracts of circular and elliptical cross sections."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4803889", 
    "link": "http://arxiv.org/pdf/1303.5513v1", 
    "title": "Parameters Optimization for Improving ASR Performance in Adverse Real   World Noisy Environmental Conditions", 
    "arxiv-id": "1303.5513v1", 
    "author": "Vilas Thakare", 
    "publish": "2013-03-22T04:20:16Z", 
    "summary": "From the existing research it has been observed that many techniques and\nmethodologies are available for performing every step of Automatic Speech\nRecognition (ASR) system, but the performance (Minimization of Word Error\nRecognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology\nis not dependent on the only technique applied in that method. The research\nwork indicates that, performance mainly depends on the category of the noise,\nthe level of the noise and the variable size of the window, frame, frame\noverlap etc is considered in the existing methods. The main aim of the work\npresented in this paper is to use variable size of parameters like window size,\nframe size and frame overlap percentage to observe the performance of\nalgorithms for various categories of noise with different levels and also train\nthe system for all size of parameters and category of real world noisy\nenvironment to improve the performance of the speech recognition system. This\npaper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by\napplying variable size of parameters. It is observed that, it is really very\nhard to evaluate test results and decide parameter size for ASR performance\nimprovement for its resultant optimization. Hence, this study further suggests\nthe feasible and optimum parameter size using Fuzzy Inference System (FIS) for\nenhancing resultant accuracy in adverse real world noisy environmental\nconditions. This work will be helpful to give discriminative training of\nubiquitous ASR system for better Human Computer Interaction (HCI)."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4803889", 
    "link": "http://arxiv.org/pdf/1303.5515v1", 
    "title": "Adverse Conditions and ASR Techniques for Robust Speech User Interface", 
    "arxiv-id": "1303.5515v1", 
    "author": "VM Thakare", 
    "publish": "2013-03-22T04:44:37Z", 
    "summary": "The main motivation for Automatic Speech Recognition (ASR) is efficient\ninterfaces to computers, and for the interfaces to be natural and truly useful,\nit should provide coverage for a large group of users. The purpose of these\ntasks is to further improve man-machine communication. ASR systems exhibit\nunacceptable degradations in performance when the acoustical environments used\nfor training and testing the system are not the same. The goal of this research\nis to increase the robustness of the speech recognition systems with respect to\nchanges in the environment. A system can be labeled as environment-independent\nif the recognition accuracy for a new environment is the same or higher than\nthat obtained when the system is retrained for that environment. Attaining such\nperformance is the dream of the researchers. This paper elaborates some of the\ndifficulties with Automatic Speech Recognition (ASR). These difficulties are\nclassified into Speakers characteristics and environmental conditions, and\ntried to suggest some techniques to compensate variations in speech signal.\nThis paper focuses on the robustness with respect to speakers variations and\nchanges in the acoustical environment. We discussed several different external\nfactors that change the environment and physiological differences that affect\nthe performance of a speech recognition system followed by techniques that are\nhelpful to design a robust ASR system."
},{
    "category": "cs.PL", 
    "doi": "10.1121/1.4803889", 
    "link": "http://arxiv.org/pdf/1303.5768v1", 
    "title": "Live music programming in Haskell", 
    "arxiv-id": "1303.5768v1", 
    "author": "Henning Thielemann", 
    "publish": "2013-03-22T20:02:38Z", 
    "summary": "We aim for composing algorithmic music in an interactive way with multiple\nparticipants. To this end we have developed an interpreter for a sub-language\nof the non-strict functional programming language Haskell that allows the\nmodification of a program during its execution. Our system can be used both for\nmusical live-coding and for demonstration and education of functional\nprogramming."
},{
    "category": "cs.MM", 
    "doi": "10.5121/sipij.2013.4411", 
    "link": "http://arxiv.org/pdf/1309.2359v1", 
    "title": "Speech Enhancement using Kernel and Normalized Kernel Affine Projection   Algorithm", 
    "arxiv-id": "1309.2359v1", 
    "author": "T. Kishore Kumar", 
    "publish": "2013-09-10T01:57:14Z", 
    "summary": "The goal of this paper is to investigate the speech signal enhancement using\nKernel Affine Projection Algorithm (KAPA) and Normalized KAPA. The removal of\nbackground noise is very important in many applications like speech\nrecognition, telephone conversations, hearing aids, forensic, etc. Kernel\nadaptive filters shown good performance for removal of noise. If the evaluation\nof background noise is more slowly than the speech, i.e., noise signal is more\nstationary than the speech, we can easily estimate the noise during the pauses\nin speech. Otherwise it is more difficult to estimate the noise which results\nin degradation of speech. In order to improve the quality and intelligibility\nof speech, unlike time and frequency domains, we can process the signal in new\ndomain like Reproducing Kernel Hilbert Space (RKHS) for high dimensional to\nyield more powerful nonlinear extensions. For experiments, we have used the\ndatabase of noisy speech corpus (NOIZEUS). From the results, we observed the\nremoval noise in RKHS has great performance in signal to noise ratio values in\ncomparison with conventional adaptive filters."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij.2013.4411", 
    "link": "http://arxiv.org/pdf/1309.5275v2", 
    "title": "An open dataset for research on audio field recording archives:   freefield1010", 
    "arxiv-id": "1309.5275v2", 
    "author": "Mark D. Plumbley", 
    "publish": "2013-09-20T14:12:04Z", 
    "summary": "We introduce a free and open dataset of 7690 audio clips sampled from the\nfield-recording tag in the Freesound audio archive. The dataset is designed for\nuse in research related to data mining in audio archives of field recordings /\nsoundscapes. Audio is standardised, and audio and metadata are Creative Commons\nlicensed. We describe the data preparation process, characterise the dataset\ndescriptively, and illustrate its use through an auto-tagging experiment."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij.2013.4411", 
    "link": "http://arxiv.org/pdf/1309.6047v1", 
    "title": "Non-negative Matrix Factorization with Linear Constraints for   Single-Channel Speech Enhancement", 
    "arxiv-id": "1309.6047v1", 
    "author": "Mikhail Kotov", 
    "publish": "2013-09-24T05:14:53Z", 
    "summary": "This paper investigates a non-negative matrix factorization (NMF)-based\napproach to the semi-supervised single-channel speech enhancement problem where\nonly non-stationary additive noise signals are given. The proposed method\nrelies on sinusoidal model of speech production which is integrated inside NMF\nframework using linear constraints on dictionary atoms. This method is further\ndeveloped to regularize harmonic amplitudes. Simple multiplicative algorithms\nare presented. The experimental evaluation was made on TIMIT corpus mixed with\nvarious types of noise. It has been shown that the proposed method outperforms\nsome of the state-of-the-art noise suppression techniques in terms of\nsignal-to-noise ratio."
},{
    "category": "stat.AP", 
    "doi": "10.5121/sipij.2013.4411", 
    "link": "http://arxiv.org/pdf/1311.0407v1", 
    "title": "Audio Texture Synthesis with Scattering Moments", 
    "arxiv-id": "1311.0407v1", 
    "author": "St\u00e9phane Mallat", 
    "publish": "2013-11-02T20:37:34Z", 
    "summary": "We introduce an audio texture synthesis algorithm based on scattering\nmoments. A scattering transform is computed by iteratively decomposing a signal\nwith complex wavelet filter banks and computing their amplitude envelop.\nScattering moments provide general representations of stationary processes\ncomputed as expected values of scattering coefficients. They are estimated with\nlow variance estimators from single realizations. Audio signals having\nprescribed scattering moments are synthesized with a gradient descent\nalgorithms. Audio synthesis examples show that scattering representation\nprovide good synthesis of audio textures with much fewer coefficients than the\nstate of the art."
},{
    "category": "q-bio.NC", 
    "doi": "10.3389/fncom.2014.00026", 
    "link": "http://arxiv.org/pdf/1311.0607v2", 
    "title": "Efficient coding of spectrotemporal binaural sounds leads to emergence   of the auditory space representation", 
    "arxiv-id": "1311.0607v2", 
    "author": "Wiktor Mlynarski", 
    "publish": "2013-11-04T08:51:14Z", 
    "summary": "To date a number of studies have shown that receptive field shapes of early\nsensory neurons can be reproduced by optimizing coding efficiency of natural\nstimulus ensembles. A still unresolved question is whether the efficient coding\nhypothesis explains formation of neurons which explicitly represent\nenvironmental features of different functional importance. This paper proposes\nthat the spatial selectivity of higher auditory neurons emerges as a direct\nconsequence of learning efficient codes for natural binaural sounds. Firstly,\nit is demonstrated that a linear efficient coding transform - Independent\nComponent Analysis (ICA) trained on spectrograms of naturalistic simulated\nbinaural sounds extracts spatial information present in the signal. A simple\nhierarchical ICA extension allowing for decoding of sound position is proposed.\nFurthermore, it is shown that units revealing spatial selectivity can be\nlearned from a binaural recording of a natural auditory scene. In both cases a\nrelatively small subpopulation of learned spectrogram features suffices to\nperform accurate sound localization. Representation of the auditory space is\ntherefore learned in a purely unsupervised way by maximizing the coding\nefficiency and without any task-specific constraints. This results imply that\nefficient coding is a useful strategy for learning structures which allow for\nmaking behaviorally vital inferences about the environment."
},{
    "category": "cs.CL", 
    "doi": "10.3389/fncom.2014.00026", 
    "link": "http://arxiv.org/pdf/1402.3080v1", 
    "title": "Software Requirement Specification Using Reverse Speech Technology", 
    "arxiv-id": "1402.3080v1", 
    "author": "Sajeer Karattil", 
    "publish": "2014-02-13T10:28:43Z", 
    "summary": "Speech analysis had been taken to a new level with the discovery of Reverse\nSpeech (RS). RS is the discovery of hidden messages, referred as reversals, in\nnormal speech. Works are in progress for exploiting the relevance of RS in\ndifferent real world applications such as investigation, medical field etc. In\nthis paper we represent an innovative method for preparing a reliable Software\nRequirement Specification (SRS) document with the help of reverse speech. As\nSRS act as the backbone for the successful completion of any project, a\nreliable method is needed to overcome the inconsistencies. Using RS such a\nreliable method for SRS documentation was developed."
},{
    "category": "cs.CL", 
    "doi": "10.5120/15302-4039", 
    "link": "http://arxiv.org/pdf/1402.3648v1", 
    "title": "Auto Spell Suggestion for High Quality Speech Synthesis in Hindi", 
    "arxiv-id": "1402.3648v1", 
    "author": "Ritika Agarwal", 
    "publish": "2014-02-15T05:11:35Z", 
    "summary": "The goal of Text-to-Speech (TTS) synthesis in a particular language is to\nconvert arbitrary input text to intelligible and natural sounding speech.\nHowever, for a particular language like Hindi, which is a highly confusing\nlanguage (due to very close spellings), it is not an easy task to identify\nerrors/mistakes in input text and an incorrect text degrade the quality of\noutput speech hence this paper is a contribution to the development of high\nquality speech synthesis with the involvement of Spellchecker which generates\nspell suggestions for misspelled words automatically. Involvement of\nspellchecker would increase the efficiency of speech synthesis by providing\nspell suggestions for incorrect input text. Furthermore, we have provided the\ncomparative study for evaluating the resultant effect on to phonetic text by\nadding spellchecker on to input text."
},{
    "category": "cs.SD", 
    "doi": "10.5120/15302-4039", 
    "link": "http://arxiv.org/pdf/1402.3689v1", 
    "title": "Sound Representation and Classification Benchmark for Domestic Robots", 
    "arxiv-id": "1402.3689v1", 
    "author": "Radu Horaud", 
    "publish": "2014-02-15T13:27:01Z", 
    "summary": "We address the problem of sound representation and classification and present\nresults of a comparative study in the context of a domestic robotic scenario. A\ndataset of sounds was recorded in realistic conditions (background noise,\npresence of several sound sources, reverberations, etc.) using the humanoid\nrobot NAO. An extended benchmark is carried out to test a variety of\nrepresentations combined with several classifiers. We provide results obtained\nwith the annotated dataset and we assess the methods quantitatively on the\nbasis of their classification scores, computation times and memory\nrequirements. The annotated dataset is publicly available at\nhttps://team.inria.fr/perception/nard/."
},{
    "category": "q-bio.NC", 
    "doi": "10.5120/15302-4039", 
    "link": "http://arxiv.org/pdf/1402.4648v2", 
    "title": "Natural statistics of binaural sounds", 
    "arxiv-id": "1402.4648v2", 
    "author": "J\u00fcrgen Jost", 
    "publish": "2014-02-19T12:47:05Z", 
    "summary": "Binaural sound localization is usually considered a discrimination task,\nwhere interaural time (ITD) and level (ILD) disparities at pure frequency\nchannels are utilized to identify a position of a sound source. In natural\nconditions binaural circuits are exposed to a stimulation by sound waves\noriginating from multiple, often moving and overlapping sources. Therefore\nstatistics of binaural cues depend on acoustic properties and the spatial\nconfiguration of the environment. In order to process binaural sounds\nefficiently, the auditory system should be adapted to naturally encountered cue\ndistributions. Statistics of cues encountered naturally and their dependence on\nthe physical properties of an auditory scene have not been studied before.\nHere, we performed binaural recordings of three auditory scenes with varying\nspatial properties. We have analyzed empirical cue distributions from each\nscene by fitting them with parametric probability density functions which\nallowed for an easy comparison of different scenes. Higher order statistics of\nbinaural waveforms were analyzed by performing Independent Component Analysis\n(ICA) and studying properties of learned basis functions. Obtained results can\nbe related to known neuronal mechanisms and suggest how binaural hearing can be\nunderstood in terms of adaptation to the natural signal statistics."
},{
    "category": "cs.SD", 
    "doi": "10.1371/journal.pone.0119032", 
    "link": "http://arxiv.org/pdf/1404.2037v1", 
    "title": "Idealized computational models for auditory receptive fields", 
    "arxiv-id": "1404.2037v1", 
    "author": "Anders Friberg", 
    "publish": "2014-04-08T08:18:01Z", 
    "summary": "This paper presents a theory by which idealized models of auditory receptive\nfields can be derived in a principled axiomatic manner, from a set of\nstructural properties to enable invariance of receptive field responses under\nnatural sound transformations and ensure internal consistency between\nspectro-temporal receptive fields at different temporal and spectral scales.\n  For defining a time-frequency transformation of a purely temporal sound\nsignal, it is shown that the framework allows for a new way of deriving the\nGabor and Gammatone filters as well as a novel family of generalized Gammatone\nfilters, with additional degrees of freedom to obtain different trade-offs\nbetween the spectral selectivity and the temporal delay of time-causal temporal\nwindow functions.\n  When applied to the definition of a second-layer of receptive fields from a\nspectrogram, it is shown that the framework leads to two canonical families of\nspectro-temporal receptive fields, in terms of spectro-temporal derivatives of\neither spectro-temporal Gaussian kernels for non-causal time or the combination\nof a time-causal generalized Gammatone filter over the temporal domain and a\nGaussian filter over the logspectral domain. For each filter family, the\nspectro-temporal receptive fields can be either separable over the\ntime-frequency domain or be adapted to local glissando transformations that\nrepresent variations in logarithmic frequencies over time. Within each domain\nof either non-causal or time-causal time, these receptive field families are\nderived by uniqueness from the assumptions.\n  It is demonstrated how the presented framework allows for computation of\nbasic auditory features for audio processing and that it leads to predictions\nabout auditory receptive fields with good qualitative similarity to biological\nreceptive fields measured in the inferior colliculus (ICC) and primary auditory\ncortex (A1) of mammals."
},{
    "category": "cs.AI", 
    "doi": "10.1080/09298215.2014.884145", 
    "link": "http://arxiv.org/pdf/1404.2313v1", 
    "title": "Outer-Product Hidden Markov Model and Polyphonic MIDI Score Following", 
    "arxiv-id": "1404.2313v1", 
    "author": "Shigeki Sagayama", 
    "publish": "2014-04-08T21:48:13Z", 
    "summary": "We present a polyphonic MIDI score-following algorithm capable of following\nperformances with arbitrary repeats and skips, based on a probabilistic model\nof musical performances. It is attractive in practical applications of score\nfollowing to handle repeats and skips which may be made arbitrarily during\nperformances, but the algorithms previously described in the literature cannot\nbe applied to scores of practical length due to problems with large\ncomputational complexity. We propose a new type of hidden Markov model (HMM) as\na performance model which can describe arbitrary repeats and skips including\nperformer tendencies on distributed score positions before and after them, and\nderive an efficient score-following algorithm that reduces computational\ncomplexity without pruning. A theoretical discussion on how much such\ninformation on performer tendencies improves the score-following results is\ngiven. The proposed score-following algorithm also admits performance mistakes\nand is demonstrated to be effective in practical situations by carrying out\nevaluations with human performances. The proposed HMM is potentially valuable\nfor other topics in information processing and we also provide a detailed\ndescription of inference algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1080/09298215.2015.1078819", 
    "link": "http://arxiv.org/pdf/1404.2314v2", 
    "title": "A Stochastic Temporal Model of Polyphonic MIDI Performance with   Ornaments", 
    "arxiv-id": "1404.2314v2", 
    "author": "Kenji Watanabe", 
    "publish": "2014-04-08T21:48:21Z", 
    "summary": "We study indeterminacies in realization of ornaments and how they can be\nincorporated in a stochastic performance model applicable for music information\nprocessing such as score-performance matching. We point out the importance of\ntemporal information, and propose a hidden Markov model which describes it\nexplicitly and represents ornaments with several state types. Following a\nreview of the indeterminacies, they are carefully incorporated into the model\nthrough its topology and parameters, and the state construction for quite\ngeneral polyphonic scores is explained in detail. By analyzing piano\nperformance data, we find significant overlaps in inter-onset-interval\ndistributions of chordal notes, ornaments, and inter-chord events, and the data\nis used to determine details of the model. The model is applied for score\nfollowing and offline score-performance matching, yielding highly accurate\nmatching for performances with many ornaments and relatively frequent errors,\nrepeats, and skips."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ISIEA.2010.5679370", 
    "link": "http://arxiv.org/pdf/1406.2464v1", 
    "title": "Music and Vocal Separation Using Multi-Band Modulation Based Features", 
    "arxiv-id": "1406.2464v1", 
    "author": "G Sita", 
    "publish": "2014-06-10T08:29:58Z", 
    "summary": "The potential use of non-linear speech features has not been investigated for\nmusic analysis although other commonly used speech features like Mel Frequency\nCeptral Coefficients (MFCC) and pitch have been used extensively. In this\npaper, we assume an audio signal to be a sum of modulated sinusoidal and then\nuse the energy separation algorithm to decompose the audio into amplitude and\nfrequency modulation components using the non-linear Teager-Kaiser energy\noperator. We first identify the distribution of these non-linear features for\nmusic only and voice only segments in the audio signal in different Mel spaced\nfrequency bands and show that they have the ability to discriminate. The\nproposed method based on Kullback-Leibler divergence measure is evaluated using\na set of Indian classical songs from three different artists. Experimental\nresults show that the discrimination ability is evident in certain low and mid\nfrequency bands (200 - 1500 Hz)."
},{
    "category": "cs.HC", 
    "doi": "10.5121/csit.2014.4526", 
    "link": "http://arxiv.org/pdf/1406.2775v1", 
    "title": "Realization and design of a pilot assist decision-making system based on   speech recognition", 
    "arxiv-id": "1406.2775v1", 
    "author": "Zhengfa Liang", 
    "publish": "2014-06-11T04:46:27Z", 
    "summary": "A system based on speech recognition is proposed for pilot assist\ndecision-making. It is based on a HIL aircraft simulation platform and uses the\nmicrocontroller SPCE061A as the central processor to achieve better reliability\nand higher cost-effect performance. Technologies of LPCC (linear predictive\ncepstral coding) and DTW (Dynamic Time Warping) are applied for isolated-word\nspeech recognition to gain a smaller amount of calculation and a better\nreal-time performance. Besides, we adopt the PWM (Pulse Width Modulation)\nregulation technology to effectively regulate each control surface by speech,\nand thus to assist the pilot to make decisions. By trial and error, it is\nproved that we have a satisfactory accuracy rate of speech recognition and\ncontrol effect. More importantly, our paper provides a creative idea for\nintelligent human-computer interaction and applications of speech recognition\nin the field of aviation control. Our system is also very easy to be extended\nand applied."
},{
    "category": "cs.SD", 
    "doi": "10.5121/csit.2014.4526", 
    "link": "http://arxiv.org/pdf/1406.3884v1", 
    "title": "Learning An Invariant Speech Representation", 
    "arxiv-id": "1406.3884v1", 
    "author": "Tomaso Poggio", 
    "publish": "2014-06-16T02:03:29Z", 
    "summary": "Recognition of speech, and in particular the ability to generalize and learn\nfrom small sets of labelled examples like humans do, depends on an appropriate\nrepresentation of the acoustic input. We formulate the problem of finding\nrobust speech features for supervised learning with small sample complexity as\na problem of learning representations of the signal that are maximally\ninvariant to intraclass transformations and deformations. We propose an\nextension of a theory for unsupervised learning of invariant visual\nrepresentations to the auditory domain and empirically evaluate its validity\nfor voiced speech sound classification. Our version of the theory requires the\nmemory-based, unsupervised storage of acoustic templates -- such as specific\nphones or words -- together with all the transformations of each that normally\noccur. A quasi-invariant representation for a speech segment can be obtained by\nprojecting it to each template orbit, i.e., the set of transformed signals, and\ncomputing the associated one-dimensional empirical probability distributions.\nThe computations can be performed by modules of filtering and pooling, and\nextended to hierarchical architectures. In this paper, we apply a single-layer,\nmulticomponent representation for phonemes and demonstrate improved accuracy\nand decreased sample complexity for vowel classification compared to standard\nspectral, cepstral and perceptual features."
},{
    "category": "cs.SD", 
    "doi": "10.5121/csit.2014.4526", 
    "link": "http://arxiv.org/pdf/1406.4447v1", 
    "title": "Automatic Fado Music Classification", 
    "arxiv-id": "1406.4447v1", 
    "author": "Isabel Trancoso", 
    "publish": "2014-06-17T17:44:45Z", 
    "summary": "In late 2011, Fado was elevated to the oral and intangible heritage of\nhumanity by UNESCO. This study aims to develop a tool for automatic detection\nof Fado music based on the audio signal. To do this, frequency spectrum-related\ncharacteristics were captured form the audio signal: in addition to the Mel\nFrequency Cepstral Coefficients (MFCCs) and the energy of the signal, the\nsignal was further analysed in two frequency ranges, providing additional\ninformation. Tests were run both in a 10-fold cross-validation setup (97.6%\naccuracy), and in a traditional train/test setup (95.8% accuracy). The good\nresults reflect the fact that Fado is a very distinctive musical style."
},{
    "category": "cs.MM", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1406.6473v1", 
    "title": "Performance Comparison of Linear Prediction based Vocoders in Linux   Platform", 
    "arxiv-id": "1406.6473v1", 
    "author": "Sakuntala S. Pillai", 
    "publish": "2014-06-25T06:45:02Z", 
    "summary": "Linear predictive coders form an important class of speech coders. This paper\ndescribes the software level implementation of linear prediction based\nvocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)\nand Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s\nand 2.4 kb/s respectively. The C programs of the vocoders have been compiled\nand executed in Linux platform. Subjective testing with the help of Mean\nOpinion Score test has been performed. Waveform analysis has been done using\nPraat and Adobe Audition software. The results show that MELP and CELP produce\ncomparable quality while the quality of LD-CELP coder is much higher, at the\nexpense of higher bit rate."
},{
    "category": "cs.LG", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1408.0193v1", 
    "title": "A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures", 
    "arxiv-id": "1408.0193v1", 
    "author": "Fathi M. Salem", 
    "publish": "2014-08-01T14:47:33Z", 
    "summary": "We propose a frequency domain method based on robust independent component\nanalysis (RICA) to address the multichannel Blind Source Separation (BSS)\nproblem of convolutive speech mixtures in highly reverberant environments. We\nimpose regularization processes to tackle the ill-conditioning problem of the\ncovariance matrix and to mitigate the performance degradation in the frequency\ndomain. We apply an algorithm to separate the source signals in adverse\nconditions, i.e. high reverberation conditions when short observation signals\nare available. Furthermore, we study the impact of several parameters on the\nperformance of separation, e.g. overlapping ratio and window type of the\nfrequency domain method. We also compare different techniques to solve the\nfrequency-domain permutation ambiguity. Through simulations and real world\nexperiments, we verify the superiority of the presented convolutive algorithm\namong other BSS algorithms, including recursive regularized ICA (RR ICA),\nindependent vector analysis (IVA)."
},{
    "category": "cs.SY", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1408.2294v3", 
    "title": "Digital Filter Designs for Recursive Frequency Analysis", 
    "arxiv-id": "1408.2294v3", 
    "author": "Hugh L. Kennedy", 
    "publish": "2014-08-11T02:13:20Z", 
    "summary": "Digital filters for recursively computing the discrete Fourier transform\n(DFT) and estimating the frequency spectrum of sampled signals are examined,\nwith an emphasis on magnitude-response and numerical stability. In this\ntutorial-style treatment, existing recursive techniques are reviewed, explained\nand compared within a coherent framework; some fresh insights are provided and\nnew enhancements/modifications are proposed. It is shown that the replacement\nof resonators by (non-recursive) modulators in sliding DFT (SDFT) analyzers\nwith either a finite impulse response (FIR), or an infinite impulse response\n(IIR), does improve performance somewhat; however stability is not guaranteed,\nas the cancellation of marginally stable poles by zeros is still involved. The\nFIR deadbeat observer is shown to be more reliable than the SDFT methods, an\nIIR variant is presented, and ways of fine-tuning its response are discussed. A\nnovel technique for stabilizing IIR SDFT analyzers with a fading memory, so\nthat all poles are inside the unit circle, is also derived. Slepian and\nsum-of-cosine windows are adapted to improve the frequency responses for the\nvarious FIR and IIR DFT methods."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1409.0203v1", 
    "title": "Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix   Completion Algorithm and Theoretical Guarantees", 
    "arxiv-id": "1409.0203v1", 
    "author": "Afsaneh Asaei", 
    "publish": "2014-08-31T10:33:45Z", 
    "summary": "This paper addresses the problem of ad hoc microphone array calibration where\nonly partial information about the distances between microphones is available.\nWe construct a matrix consisting of the pairwise distances and propose to\nestimate the missing entries based on a novel Euclidean distance matrix\ncompletion algorithm by alternative low-rank matrix completion and projection\nonto the Euclidean distance space. This approach confines the recovered matrix\nto the EDM cone at each iteration of the matrix completion algorithm. The\ntheoretical guarantees of the calibration performance are obtained considering\nthe random and locally structured missing entries as well as the measurement\nnoise on the known distances. This study elucidates the links between the\ncalibration error and the number of microphones along with the noise level and\nthe ratio of missing distances. Thorough experiments on real data recordings\nand simulated setups are conducted to demonstrate these theoretical insights. A\nsignificant improvement is achieved by the proposed Euclidean distance matrix\ncompletion algorithm over the state-of-the-art techniques for ad hoc microphone\narray calibration."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1501.07866v1", 
    "title": "A Comparison of Classifiers in Performing Speaker Accent Recognition   Using MFCCs", 
    "arxiv-id": "1501.07866v1", 
    "author": "Ernest Fokoue", 
    "publish": "2015-01-28T21:58:51Z", 
    "summary": "An algorithm involving Mel-Frequency Cepstral Coefficients (MFCCs) is\nprovided to perform signal feature extraction for the task of speaker accent\nrecognition. Then different classifiers are compared based on the MFCC feature.\nFor each signal, the mean vector of MFCC matrix is used as an input vector for\npattern recognition. A sample of 330 signals, containing 165 US voice and 165\nnon-US voice, is analyzed. By comparison, k-nearest neighbors yield the highest\naverage test accuracy, after using a cross-validation of size 500, and least\ntime being used in the computation"
},{
    "category": "cs.SD", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1109.6270v1", 
    "title": "Fractal String Generation and Its Application in Music Composition", 
    "arxiv-id": "1109.6270v1", 
    "author": "P. Pal Choudhury", 
    "publish": "2011-09-27T10:04:45Z", 
    "summary": "Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re,\nKomal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their\nharmonics. Each note corresponds to a particular frequency. When such strings\nare encoded to form discrete sequences, different frequencies present in the\nmusic corresponds to different amplitude levels (value) of the discrete\nsequence. Initially, a class of discrete sequences has been generated using\nlogistic map. All these discrete sequences have at most n-different amplitude\nlevels (value) (depending on the particular raga). Without loss of generality,\nwe have chosen two discrete sequences of two types of Indian raga viz. Bhairabi\nand Bhupali having same number of amplitude levels to obtain/search close\nrelatives from the class. The relative / closeness can be assured through\ncorrelation coefficient.The search is unbiased, random and non-adaptive. The\nobtained string is that which maximally resembles the given two sequences. The\nsame can be thought of as a music composition of the given two strings. It is\nto be noted that all these string are fractal string which can be persuaded by\nfractal dimension."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1204.2541v1", 
    "title": "Employing Subsequence Matching in Audio Data Processing", 
    "arxiv-id": "1204.2541v1", 
    "author": "Pavel Zezula", 
    "publish": "2012-04-11T07:01:31Z", 
    "summary": "We overview current problems of audio retrieval and time-series subsequence\nmatching. We discuss the usage of subsequence matching approaches in audio data\nprocessing, especially in automatic speech recognition (ASR) area and we aim at\nimproving performance of the retrieval process. To overcome the problems known\nfrom the time-series area like the occurrence of implementation bias and data\nbias we present a Subsequence Matching Framework as a tool for fast\nprototyping, building, and testing similarity search subsequence matching\napplications. The framework is build on top of MESSIF (Metric Similarity Search\nImplementation Framework) and thus the subsequence matching algorithms can\nexploit advanced similarity indexes in order to significantly increase their\nquery processing performance. To prove our concept we provide a design of\nquery-by-example spoken term detection type of application with the usage of\nphonetic posteriograms and subsequence matching approach."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1204.3236v1", 
    "title": "Using Mimicry to Learn about Mental Representations", 
    "arxiv-id": "1204.3236v1", 
    "author": "Greg Kochanski", 
    "publish": "2012-04-15T04:49:08Z", 
    "summary": "Phonology typically describes speech in terms of discrete signs like\nfeatures. The field of intonational phonology uses discrete accents to describe\nintonation and prosody. But, are such representations useful? The results of\nmimicry experiments indicate that discrete signs are not a useful\nrepresentation of the shape of intonation contours. Human behaviour seems to be\nbetter represented by a attractors where memory retains substantial fine detail\nabout an utterance. There is no evidence that discrete abstract representations\nthat might be formed that have an effect on the speech that is subsequently\nproduced. This paper also discusses conditions under which a discrete phonology\ncan arise from an attractor model and why - for intonation - attractors can be\ninferred without the implying a discrete phonology."
},{
    "category": "physics.class-ph", 
    "doi": "10.14445/22315381/IJETT-V10P310", 
    "link": "http://arxiv.org/pdf/1207.5490v2", 
    "title": "External Tonehole Interactions in Woodwind Instruments", 
    "arxiv-id": "1207.5490v2", 
    "author": "Jean Kergomard", 
    "publish": "2012-07-23T19:13:37Z", 
    "summary": "The classical Transfer-Matrix Method (TMM) is often used to calculate the\ninput impedance of woodwind instruments. However, the TMM ignores the possible\ninfluence of the radiated sound from toneholes on other open holes. In this\npaper a method is proposed to account for external tonehole interactions. We\ndescribe the Transfer-Matrix Method with external Interaction (TMMI) and then\ncompare results using this approach with the Finite Element Method (FEM) and\nTMM, as well as with experimental data. It is found that the external tonehole\ninteractions increase the amount of radiated energy, reduce slightly the lower\nresonance frequencies, and modify significantly the response near and above the\ntonehole lattice cutoff frequency. In an appendix, a simple perturbation of the\nTMM to account for external interactions is investigated, though it is found to\nbe inadequate at low frequencies and for holes spaced far apart."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1301.1932v1", 
    "title": "An Approach for Classification of Dysfluent and Fluent Speech Using K-NN   And SVM", 
    "arxiv-id": "1301.1932v1", 
    "author": "D. S. Vinod", 
    "publish": "2013-01-09T17:42:59Z", 
    "summary": "This paper presents a new approach for classification of dysfluent and fluent\nspeech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent\nwhen person's speech flows easily and smoothly. Sounds combine into syllable,\nsyllables mix together into words and words link into sentences with little\neffort. When someone's speech is dysfluent, it is irregular and does not flow\neffortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow\nof speech. Stuttering is one such disorder in which the fluent flow of speech\nis disrupted by occurrences of dysfluencies such as repetitions, prolongations,\ninterjections and so on. In this work we have considered three types of\ndysfluencies such as repetition, prolongation and interjection to characterize\ndysfluent speech. After obtaining dysfluent and fluent speech, the speech\nsignals are analyzed in order to extract MFCC features. The k-Nearest Neighbor\n(k-NN) and Support Vector Machine (SVM) classifiers are used to classify the\nspeech as dysfluent and fluent speech. The 80% of the data is used for training\nand 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for\ndysfluent and fluent speech respectively."
},{
    "category": "cs.AI", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1301.2306v1", 
    "title": "A Mixed Graphical Model for Rhythmic Parsing", 
    "arxiv-id": "1301.2306v1", 
    "author": "Christopher S Raphael", 
    "publish": "2013-01-10T16:26:12Z", 
    "summary": "A method is presented for the rhythmic parsing problem: Given a sequence of\nobserved musical note onset times, we estimate the corresponding notated rhythm\nand tempo process. A graphical model is developed that represents the\nsimultaneous evolution of tempo and rhythm and relates these hidden quantities\nto observations. The rhythm variables are discrete and the tempo and\nobservation variables are continuous. We show how to compute the globally most\nlikely configuration of the tempo and rhythm variables given an observation of\nnote onset times. Preliminary experiments are presented on a small data set. A\ngeneralization to arbitrary conditional Gaussian distributions is outlined."
},{
    "category": "stat.AP", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1304.1199v3", 
    "title": "The distribution of calibrated likelihood-ratios in speaker recognition", 
    "arxiv-id": "1304.1199v3", 
    "author": "Niko Br\u00fcmmer", 
    "publish": "2013-04-03T22:00:17Z", 
    "summary": "This paper studies properties of the score distributions of calibrated\nlog-likelihood-ratios that are used in automatic speaker recognition. We derive\nthe essential condition for calibration that the log likelihood ratio of the\nlog-likelihood-ratio is the log-likelihood-ratio. We then investigate what the\nconsequence of this condition is to the probability density functions (PDFs) of\nthe log-likelihood-ratio score. We show that if the PDF of the non-target\ndistribution is Gaussian, then the PDF of the target distribution must be\nGaussian as well. The means and variances of these two PDFs are interrelated,\nand determined completely by the discrimination performance of the recognizer\ncharacterized by the equal error rate. These relations allow for a new way of\ncomputing the offset and scaling parameters for linear calibration, and we\nderive closed-form expressions for these and show that for modern i-vector\nsystems with PLDA scoring this leads to good calibration, comparable to\ntraditional logistic regression, over a wide range of system performance."
},{
    "category": "cs.LG", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1304.7851v2", 
    "title": "North Atlantic Right Whale Contact Call Detection", 
    "arxiv-id": "1304.7851v2", 
    "author": "Osamah Rawashdeh", 
    "publish": "2013-04-30T03:41:14Z", 
    "summary": "The North Atlantic right whale (Eubalaena glacialis) is an endangered\nspecies. These whales continuously suffer from deadly vessel impacts alongside\nthe eastern coast of North America. There have been countless efforts to save\nthe remaining 350 - 400 of them. One of the most prominent works is done by\nMarinexplore and Cornell University. A system of hydrophones linked to\nsatellite connected-buoys has been deployed in the whales habitat. These\nhydrophones record and transmit live sounds to a base station. These recording\nmight contain the right whale contact call as well as many other noises. The\nnoise rate increases rapidly in vessel-busy areas such as by the Boston harbor.\nThis paper presents and studies the problem of detecting the North Atlantic\nright whale contact call with the presence of noise and other marine life\nsounds. A novel algorithm was developed to preprocess the sound waves before a\ntree based hierarchical classifier is used to classify the data and provide a\nscore. The developed model was trained with 30,000 data points made available\nthrough the Cornell University Whale Detection Challenge program. Results\nshowed that the developed algorithm had close to 85% success rate in detecting\nthe presence of the North Atlantic right whale."
},{
    "category": "cs.HC", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1304.8013v1", 
    "title": "Exploration of Speech enabled System for English", 
    "arxiv-id": "1304.8013v1", 
    "author": "T. V. Prasad", 
    "publish": "2013-03-29T10:19:11Z", 
    "summary": "This paper presents exploration of speech enable operating systems, software,\nand applications. It begins with a description of how such systems work, and\nthe level of accuracy that can be expected. It explains the applications of\nspeech recognition technology in different areas education, medical, mobile\ncomputing, railway reservation, dictation, and web browsing. A brief comparison\nof the operating systems supported for voice, speech recognition software or\ntool. It gives the brief introduction about the potential of voice/speech\nrecognition software. It explains the feature of different speech enable\nOperating system and speech recognition software. Windows speech recognition\nhave many innovative features for Windows operating system and efficiently\nassist the computer to control, dictate, navigate, selecting the words, sending\nemails and correcting the words or sentences. It also explains the benefits and\nissue related to speech technology. In last era speech recognition technology\ngrew tremendously. There are large number of companies who are working in these\narea and developing software for the people who are not able to control the\nsystem through keyboard or mouse such as physically impaired and senior\ncitizens. This paper gives a brief introduction of speech enabled OS and speech\nrecognition software."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.1145v1", 
    "title": "Techniques for Feature Extraction In Speech Recognition System : A   Comparative Study", 
    "arxiv-id": "1305.1145v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-06T10:42:34Z", 
    "summary": "The time domain waveform of a speech signal carries all of the auditory\ninformation. From the phonological point of view, it little can be said on the\nbasis of the waveform itself. However, past research in mathematics, acoustics,\nand speech technology have provided many methods for converting data that can\nbe considered as information if interpreted correctly. In order to find some\nstatistically relevant information from incoming data, it is important to have\nmechanisms for reducing the information of each segment in the audio signal\ninto a relatively small number of parameters, or features. These features\nshould describe each segment in such a characteristic way that other similar\nsegments can be grouped together by comparing their features. There are\nenormous interesting and exceptional ways to describe the speech signal in\nterms of parameters. Though, they all have their strengths and weaknesses, we\nhave presented some of the most used methods with their importance."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.1426v1", 
    "title": "Speech Enhancement Modeling Towards Robust Speech Recognition System", 
    "arxiv-id": "1305.1426v1", 
    "author": "V. M. Thakare", 
    "publish": "2013-05-07T07:21:06Z", 
    "summary": "Form about four decades human beings have been dreaming of an intelligent\nmachine which can master the natural speech. In its simplest form, this machine\nshould consist of two subsystems, namely automatic speech recognition (ASR) and\nspeech understanding (SU). The goal of ASR is to transcribe natural speech\nwhile SU is to understand the meaning of the transcription. Recognizing and\nunderstanding a spoken sentence is obviously a knowledge-intensive process,\nwhich must take into account all variable information about the speech\ncommunication process, from acoustics to semantics and pragmatics. While\ndeveloping an Automatic Speech Recognition System, it is observed that some\nadverse conditions degrade the performance of the Speech Recognition System. In\nthis contribution, speech enhancement system is introduced for enhancing speech\nsignals corrupted by additive noise and improving the performance of Automatic\nSpeech Recognizers in noisy conditions. Automatic speech recognition\nexperiments show that replacing noisy speech signals by the corresponding\nenhanced speech signals leads to an improvement in the recognition accuracies.\nThe amount of improvement varies with the type of the corrupting noise."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.2352v1", 
    "title": "Speech Enhancement Using Pitch Detection Approach For Noisy Environment", 
    "arxiv-id": "1305.2352v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-09T08:39:11Z", 
    "summary": "Acoustical mismatch among training and testing phases degrades outstandingly\nspeech recognition results. This problem has limited the development of\nreal-world nonspecific applications, as testing conditions are highly variant\nor even unpredictable during the training process. Therefore the background\nnoise has to be removed from the noisy speech signal to increase the signal\nintelligibility and to reduce the listener fatigue. Enhancement techniques\napplied, as pre-processing stages; to the systems remarkably improve\nrecognition results. In this paper, a novel approach is used to enhance the\nperceived quality of the speech signal when the additive noise cannot be\ndirectly controlled. Instead of controlling the background noise, we propose to\nreinforce the speech signal so that it can be heard more clearly in noisy\nenvironments. The subjective evaluation shows that the proposed method improves\nperceptual quality of speech in various noisy environments. As in some cases\nspeaking may be more convenient than typing, even for rapid typists: many\nmathematical symbols are missing from the keyboard but can be easily spoken and\nrecognized. Therefore, the proposed system can be used in an application\ndesigned for mathematical symbol recognition (especially symbols not available\non the keyboard) in schools."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.2680v1", 
    "title": "A study for the effect of the Emphaticness and language and dialect for   Voice Onset Time (VOT) in Modern Standard Arabic (MSA)", 
    "arxiv-id": "1305.2680v1", 
    "author": "Sulaiman S. AlDahri", 
    "publish": "2013-05-13T06:38:34Z", 
    "summary": "The signal sound contains many different features, including Voice Onset Time\n(VOT), which is a very important feature of stop sounds in many languages. The\nonly application of VOT values is stopping phoneme subsets. This subset of\nconsonant sounds is stop phonemes exist in the Arabic language, and in fact,\nall languages. The pronunciation of these sounds is hard and unique especially\nfor less-educated Arabs and non-native Arabic speakers. VOT can be utilized by\nthe human auditory system to distinguish between voiced and unvoiced stops such\nas /p/ and /b/ in English.This search focuses on computing and analyzing VOT of\nModern Standard Arabic (MSA), within the Arabic language, for all pairs of\nnon-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/)\ndepending on carrier words. This research uses a database built by ourselves,\nand uses the carrier words syllable structure: CV-CV-CV. One of the main\noutcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% of\nnon-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used to\nclassify or detect for a dialect ina language."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.2846v1", 
    "title": "Opportunities & Challenges In Automatic Speech Recognition", 
    "arxiv-id": "1305.2846v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-09T08:42:26Z", 
    "summary": "Automatic speech recognition enables a wide range of current and emerging\napplications such as automatic transcription, multimedia content analysis, and\nnatural human-computer interfaces. This paper provides a glimpse of the\nopportunities and challenges that parallelism provides for automatic speech\nrecognition and related application research from the point of view of speech\nresearchers. The increasing parallelism in computing platforms opens three\nmajor possibilities for speech recognition systems: improving recognition\naccuracy in non-ideal, everyday noisy environments; increasing recognition\nthroughput in batch processing of speech data; and reducing recognition latency\nin realtime usage scenarios. This paper describes technical challenges,\napproaches taken, and possible directions for future research to guide the\ndesign of efficient parallel software and hardware infrastructures."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.2847v1", 
    "title": "An Overview of Hindi Speech Recognition", 
    "arxiv-id": "1305.2847v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-09T08:44:58Z", 
    "summary": "In this age of information technology, information access in a convenient\nmanner has gained importance. Since speech is a primary mode of communication\namong human beings, it is natural for people to expect to be able to carry out\nspoken dialogue with computer. Speech recognition system permits ordinary\npeople to speak to the computer to retrieve information. It is desirable to\nhave a human computer dialogue in local language. Hindi being the most widely\nspoken Language in India is the natural primary human language candidate for\nhuman machine interaction. There are five pairs of vowels in Hindi languages;\none member is longer than the other one. This paper describes an overview of\nspeech recognition system that includes how speech is produced and the\nproperties and characteristics of Hindi Phoneme."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1305.2959v1", 
    "title": "Automatic Speech Recognition Using Template Model for Man-Machine   Interface", 
    "arxiv-id": "1305.2959v1", 
    "author": "V M Thakare", 
    "publish": "2013-05-09T08:47:47Z", 
    "summary": "Speech is a natural form of communication for human beings, and computers\nwith the ability to understand speech and speak with a human voice are expected\nto contribute to the development of more natural man-machine interfaces.\nComputers with this kind of ability are gradually becoming a reality, through\nthe evolution of speech recognition technologies. Speech is being an important\nmode of interaction with computers. In this paper Feature extraction is\nimplemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern\nmatching is done using Dynamic time warping (DTW) algorithm."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1306.2593v1", 
    "title": "A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure (A   framework for probabilistic modeling of spoken languages and their phonology)", 
    "arxiv-id": "1306.2593v1", 
    "author": "Elaine Tsiang", 
    "publish": "2013-06-11T17:48:01Z", 
    "summary": "We formulate a phonetic-prosodic space based on attributes as perceptual\nobservables, rather than articulatory specifications. We propose an alphabet as\nmarkers in the phonetic subspace, aiming for a resolution sufficient to support\nrecognition of all spoken languages. The prosodic subspace is made up of\ndirectly measurable physical variables. With the proposed alphabet, traditional\ndiphthongs naturally generalize to a broader class of language-neutral\nphonotactic constraints, indicating a correlation structure similar to that of\nthe traditional sonority-based syllable. We define a stochastic structure on\nthe phone strings based on this diphthongal constraint, and show how a specific\nspoken language can be defined as a specific set of probability distributions\nof this stochastic structure. Furthermore, phonological variations within a\nspoken language can be modeled as varying probability distributions restricted\nto the phonetic subspace, conditioned on different values in the prosodic\nsubspace."
},{
    "category": "math.HO", 
    "doi": "10.5121/ijcsea.2012.2603", 
    "link": "http://arxiv.org/pdf/1306.2859v1", 
    "title": "On the Mathematics of Music: From Chords to Fourier Analysis", 
    "arxiv-id": "1306.2859v1", 
    "author": "Deanna Needell", 
    "publish": "2013-06-11T17:08:35Z", 
    "summary": "Mathematics is a far reaching discipline and its tools appear in many\napplications. In this paper we discuss its role in music and signal processing\nby revisiting the use of mathematics in algorithms that can extract chord\ninformation from recorded music. We begin with a light introduction to the\ntheory of music and motivate the use of Fourier analysis in audio processing.\nWe introduce the discrete and continuous Fourier transforms and investigate\ntheir use in extracting important information from audio data."
},{
    "category": "stat.AP", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1403.4513v1", 
    "title": "A quantitative approach to evolution of music and philosophy", 
    "arxiv-id": "1403.4513v1", 
    "author": "Luciano da Fontoura Costa", 
    "publish": "2013-11-14T01:18:31Z", 
    "summary": "The development of new statistical and computational methods is increasingly\nmaking it possible to bridge the gap between hard sciences and humanities. In\nthis study, we propose an approach based on a quantitative evaluation of\nattributes of objects in fields of humanities, from which concepts such as\ndialectics and opposition are formally defined mathematically. As case studies,\nwe analyzed the temporal evolution of classical music and philosophy by\nobtaining data for 8 features characterizing the corresponding fields for 7\nwell-known composers and philosophers, which were treated with multivariate\nstatistics and pattern recognition methods. A bootstrap method was applied to\navoid statistical bias caused by the small sample data set, with which hundreds\nof artificial composers and philosophers were generated, influenced by the 7\nnames originally chosen. Upon defining indices for opposition, skewness and\ncounter-dialectics, we confirmed the intuitive analysis of historians in that\nclassical music evolved according to a master-apprentice tradition, while in\nphilosophy changes were driven by opposition. Though these case studies were\nmeant only to show the possibility of treating phenomena in humanities\nquantitatively, including a quantitative measure of concepts such as dialectics\nand opposition the results are encouraging for further application of the\napproach presented here to many other areas, since it is entirely generic."
},{
    "category": "stat.ML", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1403.7084v2", 
    "title": "Constrained speaker linking", 
    "arxiv-id": "1403.7084v2", 
    "author": "Niko Br\u00fcmmer", 
    "publish": "2014-03-26T14:51:31Z", 
    "summary": "In this paper we study speaker linking (a.k.a.\\ partitioning) given\nconstraints of the distribution of speaker identities over speech recordings.\nSpecifically, we show that the intractable partitioning problem becomes\ntractable when the constraints pre-partition the data in smaller cliques with\nnon-overlapping speakers. The surprisingly common case where speakers in\ntelephone conversations are known, but the assignment of channels to identities\nis unspecified, is treated in a Bayesian way. We show that for the Dutch CGN\ndatabase, where this channel assignment task is at hand, a lightweight speaker\nrecognition system can quite effectively solve the channel assignment problem,\nwith 93% of the cliques solved. We further show that the posterior distribution\nover channel assignment configurations is well calibrated."
},{
    "category": "cs.LG", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1403.7746v1", 
    "title": "Multi-label Ferns for Efficient Recognition of Musical Instruments in   Recordings", 
    "arxiv-id": "1403.7746v1", 
    "author": "Alicja A. Wieczorkowska", 
    "publish": "2014-03-30T12:22:36Z", 
    "summary": "In this paper we introduce multi-label ferns, and apply this technique for\nautomatic classification of musical instruments in audio recordings. We compare\nthe performance of our proposed method to a set of binary random ferns, using\njazz recordings as input data. Our main result is obtaining much faster\nclassification and higher F-score. We also achieve substantial reduction of the\nmodel size."
},{
    "category": "cs.IR", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1403.7923v1", 
    "title": "Using perceptually defined music features in music information retrieval", 
    "arxiv-id": "1403.7923v1", 
    "author": "Anders Elowsson", 
    "publish": "2014-03-31T09:19:54Z", 
    "summary": "In this study, the notion of perceptual features is introduced for describing\ngeneral music properties based on human perception. This is an attempt at\nrethinking the concept of features, in order to understand the underlying human\nperception mechanisms. Instead of using concepts from music theory such as\ntones, pitches, and chords, a set of nine features describing overall\nproperties of the music was selected. They were chosen from qualitative\nmeasures used in psychology studies and motivated from an ecological approach.\nThe selected perceptual features were rated in two listening experiments using\ntwo different data sets. They were modeled both from symbolic (MIDI) and audio\ndata using different sets of computational features. Ratings of emotional\nexpression were predicted using the perceptual features. The results indicate\nthat (1) at least some of the perceptual features are reliable estimates; (2)\nemotion ratings could be predicted by a small combination of perceptual\nfeatures with an explained variance up to 90%; (3) the perceptual features\ncould only to a limited extent be modeled using existing audio features. The\nresults also clearly indicated that a small number of dedicated features were\nsuperior to a 'brute force' model using a large number of general audio\nfeatures."
},{
    "category": "cs.CL", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1405.0049v2", 
    "title": "Exemplar Dynamics Models of the Stability of Phonological Categories", 
    "arxiv-id": "1405.0049v2", 
    "author": "P. F. Tupper", 
    "publish": "2014-04-30T22:44:59Z", 
    "summary": "We develop a model for the stability and maintenance of phonological\ncategories. Examples of phonological categories are vowel sounds such as \"i\"\nand \"e\". We model such categories as consisting of collections of labeled\nexemplars that language users store in their memory. Each exemplar is a\ndetailed memory of an instance of the linguistic entity in question. Starting\nfrom an exemplar-level model we derive integro-differential equations for the\nlong-term evolution of the density of exemplars in different portions of\nphonetic space. Using these latter equations we investigate under what\nconditions two phonological categories merge or not. Our main conclusion is\nthat for the preservation of distinct phonological categories, it is necessary\nthat anomalous speech tokens of a given category are discarded, and not merely\nstored in memory as an exemplar of another category."
},{
    "category": "cs.SD", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1405.1379v1", 
    "title": "Design and Optimization of a Speech Recognition Front-End for   Distant-Talking Control of a Music Playback Device", 
    "arxiv-id": "1405.1379v1", 
    "author": "Joshua Atkins", 
    "publish": "2014-05-05T14:37:47Z", 
    "summary": "This paper addresses the challenging scenario for the distant-talking control\nof a music playback device, a common portable speaker with four small\nloudspeakers in close proximity to one microphone. The user controls the device\nthrough voice, where the speech-to-music ratio can be as low as -30 dB during\nmusic playback. We propose a speech enhancement front-end that relies on known\nrobust methods for echo cancellation, double-talk detection, and noise\nsuppression, as well as a novel adaptive quasi-binary mask that is well suited\nfor speech recognition. The optimization of the system is then formulated as a\nlarge scale nonlinear programming problem where the recognition rate is\nmaximized and the optimal values for the system parameters are found through a\ngenetic algorithm. We validate our methodology by testing over the TIMIT\ndatabase for different music playback levels and noise types. Finally, we show\nthat the proposed front-end allows a natural interaction with the device for\nlimited-vocabulary voice commands."
},{
    "category": "cs.SD", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1405.4843v1", 
    "title": "Trends and Perspectives for Signal Processing in Consumer Audio", 
    "arxiv-id": "1405.4843v1", 
    "author": "Daniele Giacobello", 
    "publish": "2014-05-19T19:10:15Z", 
    "summary": "The trend in media consumption towards streaming and portability offers new\nchallenges and opportunities for signal processing in audio and acoustics. The\nmost significant embodiment of this trend is that most music consumption now\nhappens on-the-go which has recently led to an explosion in headphone sales and\nsmall portable speakers. In particular, premium headphones offer a gateway for\na younger generation to experience high quality sound. Additionally, through\ntechnologies incorporating head-related transfer functions headphones can also\noffer unique new experiences in gaming, augmented reality, and surround sound\nlistening. Home audio has also seen a transition to smaller sound systems in\nthe form of sound bars. This speaker configuration offers many exciting\nchallenges for surround sound reproduction which has traditionally used five\nspeakers surrounding the listener. Furthermore, modern home entertainment\nsystems offer more than just content delivery; users now expect wireless and\nconnected smart devices with video conferencing, gaming, and other interactive\ncapabilities. With this comes challenges for voice interaction at a distance\nand in demanding conditions, e.g., during content playback, and opportunities\nfor new smart interactive experiences based on awareness of environment and\nuser biometrics."
},{
    "category": "cs.SD", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1405.6524v1", 
    "title": "Automatic large-scale classification of bird sounds is strongly improved   by unsupervised feature learning", 
    "arxiv-id": "1405.6524v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2014-05-26T09:58:20Z", 
    "summary": "Automatic species classification of birds from their sound is a computational\ntool of increasing importance in ecology, conservation monitoring and vocal\ncommunication studies. To make classification useful in practice, it is crucial\nto improve its accuracy while ensuring that it can run at big data scales. Many\napproaches use acoustic measures based on spectrogram-type data, such as the\nMel-frequency cepstral coefficient (MFCC) features which represent a\nmanually-designed summary of spectral information. However, recent work in\nmachine learning has demonstrated that features learnt automatically from data\ncan often outperform manually-designed feature transforms. Feature learning can\nbe performed at large scale and \"unsupervised\", meaning it requires no manual\ndata labelling, yet it can improve performance on \"supervised\" tasks such as\nclassification. In this work we introduce a technique for feature learning from\nlarge volumes of bird sound recordings, inspired by techniques that have proven\nuseful in other domains. We experimentally compare twelve different feature\nrepresentations derived from the Mel spectrum (of which six use this\ntechnique), using four large and diverse databases of bird vocalisations, with\na random forest classifier. We demonstrate that MFCCs are of limited power in\nthis context, leading to worse performance than the raw Mel spectral data.\nConversely, we demonstrate that unsupervised feature learning provides a\nsubstantial boost over MFCCs and Mel spectra without adding computational\ncomplexity after the model has been trained. The boost is particularly notable\nfor single-label classification tasks at large scale. The spectro-temporal\nactivations learned through our procedure resemble spectro-temporal receptive\nfields calculated from avian primary auditory forebrain."
},{
    "category": "cs.SD", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1407.0380v1", 
    "title": "A Multi Level Data Fusion Approach for Speaker Identification on   Telephone Speech", 
    "arxiv-id": "1407.0380v1", 
    "author": "Dorra Ben Ayed", 
    "publish": "2014-06-27T20:34:05Z", 
    "summary": "Several speaker identification systems are giving good performance with clean\nspeech but are affected by the degradations introduced by noisy audio\nconditions. To deal with this problem, we investigate the use of complementary\ninformation at different levels for computing a combined match score for the\nunknown speaker. In this work, we observe the effect of two supervised machine\nlearning approaches including support vectors machines (SVM) and na\\\"ive bayes\n(NB). We define two feature vector sets based on mel frequency cepstral\ncoefficients (MFCC) and relative spectral perceptual linear predictive\ncoefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture\nModel (GMM). Several ways of combining these information sources give\nsignificant improvements in a text-independent speaker identification task\nusing a very large telephone degraded NTIMIT database."
},{
    "category": "cs.MM", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1407.2221v1", 
    "title": "Sonic interaction with a virtual orchestra of factory machinery", 
    "arxiv-id": "1407.2221v1", 
    "author": "Val\u00e9rie Gouranton", 
    "publish": "2014-07-06T06:29:34Z", 
    "summary": "This paper presents an immersive application where users receive sound and\nvisual feedbacks on their interactions with a virtual environment. In this\napplication, the users play the part of conductors of an orchestra of factory\nmachines since each of their actions on interaction devices triggers a pair of\nvisual and audio responses. Audio stimuli were spatialized around the listener.\nThe application was exhibited during the 2013 Science and Music day and\ndesigned to be used in a large immersive system with head tracking, shutter\nglasses and a 10.2 loudspeaker configuration."
},{
    "category": "cs.IR", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1410.0001v1", 
    "title": "On Evaluation Validity in Music Autotagging", 
    "arxiv-id": "1410.0001v1", 
    "author": "Thibault Langlois", 
    "publish": "2014-09-30T14:57:52Z", 
    "summary": "Music autotagging, an established problem in Music Information Retrieval,\naims to alleviate the human cost required to manually annotate collections of\nrecorded music with textual labels by automating the process. Many autotagging\nsystems have been proposed and evaluated by procedures and datasets that are\nnow standard (used in MIREX, for instance). Very little work, however, has been\ndedicated to determine what these evaluations really mean about an autotagging\nsystem, or the comparison of two systems, for the problem of annotating music\nin the real world. In this article, we are concerned with explaining the figure\nof merit of an autotagging system evaluated with a standard approach.\nSpecifically, does the figure of merit, or a comparison of figures of merit,\nwarrant a conclusion about how well autotagging systems have learned to\ndescribe music with a specific vocabulary? The main contributions of this paper\nare a formalization of the notion of validity in autotagging evaluation, and a\nmethod to test it in general. We demonstrate the practical use of our method in\nexperiments with three specific state-of-the-art autotagging systems --all of\nwhich are reproducible using the linked code and data. Our experiments show for\nthese specific systems in a simple and objective two-class task that the\nstandard evaluation approach does not provide valid indicators of their\nperformance."
},{
    "category": "cs.SD", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1410.6903v1", 
    "title": "Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech", 
    "arxiv-id": "1410.6903v1", 
    "author": "Sunil Kumar Kopparapu", 
    "publish": "2014-10-25T09:40:46Z", 
    "summary": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\nspeech features in most speech and speaker recognition applications. In this\npaper, we study the effect of resampling a speech signal on these speech\nfeatures. We first derive a relationship between the MFCC param- eters of the\nresampled speech and the MFCC parameters of the original speech. We propose six\nmethods of calculating the MFCC parameters of downsampled speech by\ntransforming the Mel filter bank used to com- pute MFCC of the original speech.\nWe then experimentally compute the MFCC parameters of the down sampled speech\nusing the proposed meth- ods and compute the Pearson coefficient between the\nMFCC parameters of the downsampled speech and that of the original speech to\nidentify the most effective choice of Mel-filter band that enables the computed\nMFCC of the resampled speech to be as close as possible to the original speech\nsample MFCC."
},{
    "category": "cs.CL", 
    "doi": "10.7717/peerj.488", 
    "link": "http://arxiv.org/pdf/1410.7382v1", 
    "title": "Modified Mel Filter Bank to Compute MFCC of Subsampled Speech", 
    "arxiv-id": "1410.7382v1", 
    "author": "Sunil Kumar Kopparapu", 
    "publish": "2014-10-25T10:00:14Z", 
    "summary": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\nspeech features in most speech and speaker recognition applications. In this\nwork, we propose a modified Mel filter bank to extract MFCCs from subsampled\nspeech. We also propose a stronger metric which effectively captures the\ncorrelation between MFCCs of original speech and MFCC of resampled speech. It\nis found that the proposed method of filter bank construction performs\ndistinguishably well and gives recognition performance on resampled speech\nclose to recognition accuracies on original speech."
},{
    "category": "cs.SD", 
    "doi": "10.5121/sipij.2014.5503", 
    "link": "http://arxiv.org/pdf/1411.2795v1", 
    "title": "Speaker Identification From Youtube Obtained Data", 
    "arxiv-id": "1411.2795v1", 
    "author": "Nitesh Kumar Chaudhary", 
    "publish": "2014-11-11T13:20:19Z", 
    "summary": "An efficient, and intuitive algorithm is presented for the identification of\nspeakers from a long dataset (like YouTube long discussion, Cocktail party\nrecorded audio or video).The goal of automatic speaker identification is to\nidentify the number of different speakers and prepare a model for that speaker\nby extraction, characterization and speaker-specific information contained in\nthe speech signal. It has many diverse application specially in the field of\nSurveillance, Immigrations at Airport, cyber security, transcription in\nmulti-source of similar sound source, where it is difficult to assign\ntranscription arbitrary. The most commonly speech parametrization used in\nspeaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixture\nmodeling, which is the speaker modeling technique is then explained. Gaussian\nmixture models (GMM), perhaps the most robust machine learning algorithm has\nbeen introduced examine and judge carefully speaker identification in text\nindependent. The application or employment of Gaussian mixture models for\nmonitoring & Analysing speaker identity is encouraged by the familiarity,\nawareness, or understanding gained through experience that Gaussian spectrum\ndepict the characteristics of speaker's spectral conformational pattern and\nremarkable ability of GMM to construct capricious densities after that we\nillustrate 'Expectation maximization' an iterative algorithm which takes some\narbitrary value in initial estimation and carry on the iterative process until\nthe convergence of value is observed,so by doing various number of experiments\nwe are able to obtain 79 ~ 82% of identification rate using Vector quantization\nand 85 ~ 92.6% of identification rate using GMM modeling by Expectation\nmaximization parameter estimation depending on variation of parameter."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1411.3715v1", 
    "title": "Acoustic Scene Classification", 
    "arxiv-id": "1411.3715v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2014-11-13T16:03:09Z", 
    "summary": "In this article we present an account of the state-of-the-art in acoustic\nscene classification (ASC), the task of classifying environments from the\nsounds they produce. Starting from a historical review of previous research in\nthis area, we define a general framework for ASC and present different imple-\nmentations of its components. We then describe a range of different algorithms\nsubmitted for a data challenge that was held to provide a general and fair\nbenchmark for ASC techniques. The dataset recorded for this purpose is\npresented, along with the performance metrics that are used to evaluate the\nalgorithms and statistical significance tests to compare the submitted methods.\nWe use a baseline method that employs MFCCS, GMMS and a maximum likelihood\ncriterion as a benchmark, and only find sufficient evidence to conclude that\nthree algorithms significantly outperform it. We also evaluate the human\nclassification accuracy in performing a similar classification task. The best\nperforming algorithm achieves a mean accuracy that matches the median accuracy\nobtained by humans, and common pairs of classes are misclassified by both\ncomputers and humans. However, all acoustic scenes are correctly classified by\nat least some individuals, while there are scenes that are misclassified by all\nalgorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1411.4890v1", 
    "title": "Which Are You In A Photo?", 
    "arxiv-id": "1411.4890v1", 
    "author": "Xing Zhang", 
    "publish": "2014-11-17T05:06:50Z", 
    "summary": "Automatic image tagging has been a long standing problem, it mainly relies on\nimage recognition techniques of which the accuracy is still not satisfying.\nThis paper attempts to explore out-of-band sensing base on the mobile phone to\nsense the people in a picture while the picture is being taken and create name\ntags on-the-fly. The major challenges pertain to two aspects - \"Who\" and\n\"Which\". (1) \"Who\": discriminating people who are in the picture from those\nthat are not; (2) \"Which\": correlating each name tag with its corresponding\npeople in the picture. We propose an accurate acoustic scheme applying on the\nmobile phones, which leverages the Doppler effect of sound wave to address\nthese two challenges. As a proof of concept, we implement the scheme on 7\nandroid phones and take pictures in various real-life scenarios with people\npositioning in different ways. Extensive experiments show that the accuracy of\ntag correlation is above 85% within 3m for picturing."
},{
    "category": "cs.AI", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1412.3079v1", 
    "title": "Computoser - rule-based, probability-driven algorithmic music   composition", 
    "arxiv-id": "1412.3079v1", 
    "author": "Bozhidar Bozhanov", 
    "publish": "2014-12-09T20:06:10Z", 
    "summary": "This paper presents the Computoser hybrid probability/rule based algorithm\nfor music composition (http://computoser.com) and provides a reference\nimplementation. It addresses the issues of unpleasantness and lack of variation\nexhibited by many existing approaches by combining the two methods (basing the\nparameters of the rules on data obtained from preliminary analysis).\n  A sample of 500+ musical pieces was analyzed to derive probabilities for\nmusical characteristics and events (e.g. scale, tempo, intervals). The\nalgorithm was constructed to produce musical pieces using the derived\nprobabilities combined with a large set of composition rules, which were\nobtained and structured after studying established composition practices.\nGenerated pieces were published on the Computoser website where evaluation was\nperformed by listeners. The feedback was positive (58.4% approval), asserting\nthe merits of the undertaken approach.\n  The paper compares this hybrid approach to other approaches to algorithmic\ncomposition and presents a survey of the pleasantness of the resulting music."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1412.4052v2", 
    "title": "The bag-of-frames approach: a not so sufficient model for urban   soundscapes", 
    "arxiv-id": "1412.4052v2", 
    "author": "Jean-Julien Aucouturier", 
    "publish": "2014-12-11T05:11:54Z", 
    "summary": "The \"bag-of-frames\" approach (BOF), which encodes audio signals as the\nlong-term statistical distribution of short-term spectral features, is commonly\nregarded as an effective and sufficient way to represent environmental sound\nrecordings (soundscapes) since its introduction in an influential 2007 article.\nThe present paper describes a concep-tual replication of this seminal article\nusing several new soundscape datasets, with results strongly questioning the\nadequacy of the BOF approach for the task. We show that the good accuracy\noriginally re-ported with BOF likely result from a particularly thankful\ndataset with low within-class variability, and that for more realistic\ndatasets, BOF in fact does not perform significantly better than a mere\none-point av-erage of the signal's features. Soundscape modeling, therefore,\nmay not be the closed case it was once thought to be. Progress, we ar-gue,\ncould lie in reconsidering the problem of considering individual acoustical\nevents within each soundscape."
},{
    "category": "cs.CL", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1412.4616v1", 
    "title": "A Broadcast News Corpus for Evaluation and Tuning of German LVCSR   Systems", 
    "arxiv-id": "1412.4616v1", 
    "author": "Gerhard Rigoll", 
    "publish": "2014-12-15T14:34:38Z", 
    "summary": "Transcription of broadcast news is an interesting and challenging application\nfor large-vocabulary continuous speech recognition (LVCSR). We present in\ndetail the structure of a manually segmented and annotated corpus including\nover 160 hours of German broadcast news, and propose it as an evaluation\nframework of LVCSR systems. We show our own experimental results on the corpus,\nachieved with a state-of-the-art LVCSR decoder, measuring the effect of\ndifferent feature sets and decoding parameters, and thereby demonstrate that\nreal-time decoding of our test set is feasible on a desktop PC at 9.2% word\nerror rate."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1412.6853v1", 
    "title": "Psychophysics of musical elements in the discrete-time representation of   sound", 
    "arxiv-id": "1412.6853v1", 
    "author": "D\u00e9bora Cristina Corr\u00eaa", 
    "publish": "2014-12-22T01:04:53Z", 
    "summary": "Notes, ornaments and intervals are examples of basic musical elements. Their\nrepresentation as discrete-time digital audio plays a central role in software\nfor music creation and design. Nevertheless, there is no systematic relation,\nin analytical terms, of these musical elements to the sonic samples. Such a\ncompendium enables scientific experiments in precise and trustful ways, among\neducational and artistic uses. This paper presents a comprehensive description\nof music in digital audio, within an unified approach. Musical elements, like\npitch, duration and timbre are expressed by equations on sample level. This\nquantitatively relates characteristics of the discrete-time signal to musical\nqualities. Internal variations, e.g. tremolos, vibratos and spectral\nfluctuations, are also considered to operate within a note. Moreover, the\ngeneration of musical structures such as rhythmic meter, pitch intervals and\ncycles, are attained canonically with notes. The availability of these\nresources in scripts is provided in public domain within the \\massa\\ toolbox -\nMusic and Audio in Sequences and Samples. Authors observe that the\nimplementation of sample-domain analytical results as open source can encourage\nconcise research. As further illustrated in the paper, \\massa\\ has already been\nemployed by users for diverse purposes, including acoustics experimentation,\nart and education. The efficacy of these physical descriptions was confirmed by\nthe synthesis of small musical pieces. As shown, it is possible to synthesize\nwhole albums through collage of scripts and parametrization."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1412.7022v3", 
    "title": "Audio Source Separation with Discriminative Scattering Networks", 
    "arxiv-id": "1412.7022v3", 
    "author": "Yann LeCun", 
    "publish": "2014-12-22T15:15:44Z", 
    "summary": "In this report we describe an ongoing line of research for solving\nsingle-channel source separation problems. Many monaural signal decomposition\ntechniques proposed in the literature operate on a feature space consisting of\na time-frequency representation of the input data. A challenge faced by these\napproaches is to effectively exploit the temporal dependencies of the signals\nat scales larger than the duration of a time-frame. In this work we propose to\ntackle this problem by modeling the signals using a time-frequency\nrepresentation with multiple temporal resolutions. The proposed representation\nconsists of a pyramid of wavelet scattering operators, which generalizes\nConstant Q Transforms (CQT) with extra layers of convolution and complex\nmodulus. We first show that learning standard models with this multi-resolution\nsetting improves source separation results over fixed-resolution methods. As\nstudy case, we use Non-Negative Matrix Factorizations (NMF) that has been\nwidely considered in many audio application. Then, we investigate the inclusion\nof the proposed multi-resolution setting into a discriminative training regime.\nWe discuss several alternatives using different deep neural network\narchitectures."
},{
    "category": "stat.ML", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1502.00141v1", 
    "title": "An evaluation framework for event detection using a morphological model   of acoustic scenes", 
    "arxiv-id": "1502.00141v1", 
    "author": "Axel Roebel", 
    "publish": "2015-01-31T18:12:34Z", 
    "summary": "This paper introduces a model of environmental acoustic scenes which adopts a\nmorphological approach by ab-stracting temporal structures of acoustic scenes.\nTo demonstrate its potential, this model is employed to evaluate the\nperformance of a large set of acoustic events detection systems. This model\nallows us to explicitly control key morphological aspects of the acoustic scene\nand isolate their impact on the performance of the system under evaluation.\nThus, more information can be gained on the behavior of evaluated systems,\nproviding guidance for further improvements. The proposed model is validated\nusing submitted systems from the IEEE DCASE Challenge; results indicate that\nthe proposed scheme is able to successfully build datasets useful for\nevaluating some aspects the performance of event detection systems, more\nparticularly their robustness to new listening conditions and the increasing\nlevel of background sounds."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MSP.2014.2326181", 
    "link": "http://arxiv.org/pdf/1502.01707v1", 
    "title": "CS reconstruction of the speech and musical signals", 
    "arxiv-id": "1502.01707v1", 
    "author": "Radoje Albijanic", 
    "publish": "2015-02-05T20:40:41Z", 
    "summary": "The application of Compressive sensing approach to the speech and musical\nsignals is considered in this paper. Compressive sensing (CS) is a new approach\nto the signal sampling that allows signal reconstruction from a small set of\nrandomly acquired samples. This method is developed for the signals that\nexhibit the sparsity in a certain domain. Here we have observed two sparsity\ndomains: discrete Fourier and discrete cosine transform domain. Furthermore,\ntwo different types of audio signals are analyzed in terms of sparsity and CS\nperformance - musical and speech signals. Comparative analysis of the CS\nreconstruction using different number of signal samples is performed in the two\ndomains of sparsity. It is shown that the CS can be successfully applied to\nboth, musical and speech signals, but the speech signals are more demanding in\nterms of the number of observations. Also, our results show that discrete\ncosine transform domain allows better reconstruction using lower number of\nobservations, compared to the Fourier transform domain, for both types of\nsignals."
},{
    "category": "physics.soc-ph", 
    "doi": "10.1098/rsos.150081", 
    "link": "http://arxiv.org/pdf/1502.05417v1", 
    "title": "The Evolution of Popular Music: USA 1960-2010", 
    "arxiv-id": "1502.05417v1", 
    "author": "Armand M. Leroi", 
    "publish": "2015-02-17T18:32:39Z", 
    "summary": "In modern societies, cultural change seems ceaseless. The flux of fashion is\nespecially obvious for popular music. While much has been written about the\norigin and evolution of pop, most claims about its history are anecdotal rather\nthan scientific in nature. To rectify this we investigate the US Billboard Hot\n100 between 1960 and 2010. Using Music Information Retrieval (MIR) and\ntext-mining tools we analyse the musical properties of ~17,000 recordings that\nappeared in the charts and demonstrate quantitative trends in their harmonic\nand timbral properties. We then use these properties to produce an audio-based\nclassification of musical styles and study the evolution of musical diversity\nand disparity, testing, and rejecting, several classical theories of cultural\nchange. Finally, we investigate whether pop musical evolution has been gradual\nor punctuated. We show that, although pop music has evolved continuously, it\ndid so with particular rapidity during three stylistic \"revolutions\" around\n1964, 1983 and 1991. We conclude by discussing how our study points the way to\na quantitative science of cultural change."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1502.05751v2", 
    "title": "Efficient Synthesis of Room Acoustics via Scattering Delay Networks", 
    "arxiv-id": "1502.05751v2", 
    "author": "Julius O. Smith III", 
    "publish": "2015-02-19T23:58:36Z", 
    "summary": "An acoustic reverberator consisting of a network of delay lines connected via\nscattering junctions is proposed. All parameters of the reverberator are\nderived from physical properties of the enclosure it simulates. It allows for\nsimulation of unequal and frequency-dependent wall absorption, as well as\ndirectional sources and microphones. The reverberator renders the first-order\nreflections exactly, while making progressively coarser approximations of\nhigher-order reflections. The rate of energy decay is close to that obtained\nwith the image method (IM) and consistent with the predictions of Sabine and\nEyring equations. The time evolution of the normalized echo density, which was\npreviously shown to be correlated with the perceived texture of reverberation,\nis also close to that of IM. However, its computational complexity is one to\ntwo orders of magnitude lower, comparable to the computational complexity of a\nfeedback delay network (FDN), and its memory requirements are negligible."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1502.06811v1", 
    "title": "A Review of Audio Features and Statistical Models Exploited for Voice   Pattern Design", 
    "arxiv-id": "1502.06811v1", 
    "author": "Hien-Thanh Duong", 
    "publish": "2015-02-24T13:47:45Z", 
    "summary": "Audio fingerprinting, also named as audio hashing, has been well-known as a\npowerful technique to perform audio identification and synchronization. It\nbasically involves two major steps: fingerprint (voice pattern) design and\nmatching search. While the first step concerns the derivation of a robust and\ncompact audio signature, the second step usually requires knowledge about\ndatabase and quick-search algorithms. Though this technique offers a wide range\nof real-world applications, to the best of the authors' knowledge, a\ncomprehensive survey of existing algorithms appeared more than eight years ago.\nThus, in this paper, we present a more up-to-date review and, for emphasizing\non the audio signal processing aspect, we focus our state-of-the-art survey on\nthe fingerprint design step for which various audio features and their\ntractable statistical models are discussed."
},{
    "category": "stat.AP", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1503.03022v1", 
    "title": "A novel method based on cross correlation maximization, for pattern   matching by means of a single parameter. Application to the human voice", 
    "arxiv-id": "1503.03022v1", 
    "author": "Leonardo Bennun", 
    "publish": "2015-03-06T15:31:37Z", 
    "summary": "This work develops a cross correlation maximization technique, based on\nstatistical concepts, for pattern matching purposes in time series. The\ntechnique analytically quantifies the extent of similitude between a known\nsignal within a group of data, by means of a single parameter. Specifically,\nthe method was applied to voice recognition problem, by selecting samples from\na given individual recordings of the 5 vowels, in Spanish. The frequency of\nacquisition of the data was 11.250 Hz. A certain distinctive interval was\nestablished from each vowel time series as a representative test function and\nit was compared both to itself and to the rest of the vowels by means of an\nalgorithm, for a subsequent graphic illustration of the results.\n  We conclude that for a minimum distinctive length, the method meets\nresemblance between every vowel with itself, and also an irrefutable difference\nwith the rest of the vowels for an estimate length of 30 points (~2 10-3 s)."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1504.08021v1", 
    "title": "Who Spoke What? A Latent Variable Framework for the Joint Decoding of   Multiple Speakers and their Keywords", 
    "arxiv-id": "1504.08021v1", 
    "author": "Thippur V. Sreenivas", 
    "publish": "2015-04-29T20:56:42Z", 
    "summary": "In this paper, we present a latent variable (LV) framework to identify all\nthe speakers and their keywords given a multi-speaker mixture signal. We\nintroduce two separate LVs to denote active speakers and the keywords uttered.\nThe dependency of a spoken keyword on the speaker is modeled through a\nconditional probability mass function. The distribution of the mixture signal\nis expressed in terms of the LV mass functions and speaker-specific-keyword\nmodels. The proposed framework admits stochastic models, representing the\nprobability density function of the observation vectors given that a particular\nspeaker uttered a specific keyword, as speaker-specific-keyword models. The LV\nmass functions are estimated in a Maximum Likelihood framework using the\nExpectation Maximization (EM) algorithm. The active speakers and their keywords\nare detected as modes of the joint distribution of the two LVs. In mixture\nsignals, containing two speakers uttering the keywords simultaneously, the\nproposed framework achieves an accuracy of 82% for detecting both the speakers\nand their respective keywords, using Student's-t mixture models as\nspeaker-specific-keyword models."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1506.02170v1", 
    "title": "Hybridized Feature Extraction and Acoustic Modelling Approach for   Dysarthric Speech Recognition", 
    "arxiv-id": "1506.02170v1", 
    "author": "D. Shivakrishna", 
    "publish": "2015-06-06T17:05:00Z", 
    "summary": "Dysarthria is malfunctioning of motor speech caused by faintness in the human\nnervous system. It is characterized by the slurred speech along with physical\nimpairment which restricts their communication and creates the lack of\nconfidence and affects the lifestyle. This paper attempt to increase the\nefficiency of Automatic Speech Recognition (ASR) system for unimpaired speech\nsignal. It describes state of art of research into improving ASR for speakers\nwith dysarthria by means of incorporated knowledge of their speech production.\nHybridized approach for feature extraction and acoustic modelling technique\nalong with evolutionary algorithm is proposed for increasing the efficiency of\nthe overall system. Here number of feature vectors are varied and tested the\nsystem performance. It is observed that system performance is boosted by\ngenetic algorithm. System with 16 acoustic features optimized with genetic\nalgorithm has obtained highest recognition rate of 98.28% with training time of\n5:30:17."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1506.04828v2", 
    "title": "Significance of the levels of spectral valleys with application to   front/back distinction of vowel sounds", 
    "arxiv-id": "1506.04828v2", 
    "author": "Shubham Sharma", 
    "publish": "2015-06-16T04:03:06Z", 
    "summary": "An objective critical distance (OCD) has been defined as that spacing between\nadjacent formants, when the level of the valley between them reaches the mean\nspectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) as\nthe critical distance determined by subjective experiments for similar\nexperimental conditions. The level of spectral valley serves a purpose similar\nto that of the spacing between the formants with an added advantage that it can\nbe measured from the spectral envelope without an explicit knowledge of formant\nfrequencies. Based on the relative spacing of formant frequencies, the level of\nthe spectral valley, VI (between F1 and F2) is much higher than the level of\nVII (spectral valley between F2 and F3) for back vowels and vice-versa for\nfront vowels. Classification of vowels into front/back distinction with the\ndifference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamil\nand Kannada language databases gives, on the average, an accuracy of about 95%,\nwhich is comparable to the accuracy (90.6%) obtained using a neural network\nclassifier trained and tested using MFCC as the feature vector for TIMIT\ndatabase. The acoustic feature (VI-VII) has also been tested for its robustness\non the TIMIT database for additive white and babble noise and an accuracy of\nabout 95% has been obtained for SNRs down to 25 dB for both types of noise."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1506.05268v1", 
    "title": "Deep Denoising Auto-encoder for Statistical Speech Synthesis", 
    "arxiv-id": "1506.05268v1", 
    "author": "Junichi Yamagishi", 
    "publish": "2015-06-17T10:17:59Z", 
    "summary": "This paper proposes a deep denoising auto-encoder technique to extract better\nacoustic features for speech synthesis. The technique allows us to\nautomatically extract low-dimensional features from high dimensional spectral\nfeatures in a non-linear, data-driven, unsupervised way. We compared the new\nstochastic feature extractor with conventional mel-cepstral analysis in\nanalysis-by-synthesis and text-to-speech experiments. Our results confirm that\nthe proposed method increases the quality of synthetic speech in both\nexperiments."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1507.04019v1", 
    "title": "Feature Normalisation for Robust Speech Recognition", 
    "arxiv-id": "1507.04019v1", 
    "author": "D. S. Pavan Kumar", 
    "publish": "2015-07-14T20:34:16Z", 
    "summary": "Speech recognition system performance degrades in noisy environments. If the\nacoustic models are built using features of clean utterances, the features of a\nnoisy test utterance would be acoustically mismatched with the trained model.\nThis gives poor likelihoods and poor recognition accuracy. Model adaptation and\nfeature normalisation are two broad areas that address this problem. While the\nformer often gives better performance, the latter involves estimation of lesser\nnumber of parameters, making the system feasible for practical implementations.\n  This research focuses on the efficacies of various subspace, statistical and\nstereo based feature normalisation techniques. A subspace projection based\nmethod has been investigated as a standalone and adjunct technique involving\nreconstruction of noisy speech features from a precomputed set of clean speech\nbuilding-blocks. The building blocks are learned using non-negative matrix\nfactorisation (NMF) on log-Mel filter bank coefficients, which form a basis for\nthe clean speech subspace. The work provides a detailed study on how the method\ncan be incorporated into the extraction process of Mel-frequency cepstral\ncoefficients. Experimental results show that the new features are robust to\nnoise, and achieve better results when combined with the existing techniques.\n  The work also proposes a modification to the training process of SPLICE\nalgorithm for noise robust speech recognition. It is based on feature\ncorrelations, and enables this stereo-based algorithm to improve the\nperformance in all noise conditions, especially in unseen cases. Further, the\nmodified framework is extended to work for non-stereo datasets where clean and\nnoisy training utterances, but not stereo counterparts, are required. An\nMLLR-based computationally efficient run-time noise adaptation method in SPLICE\nframework has been proposed."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1507.05546v1", 
    "title": "Automatic Identification of Animal Breeds and Species Using Bioacoustics   and Artificial Neural Networks", 
    "arxiv-id": "1507.05546v1", 
    "author": "Arlene A. Mendoza", 
    "publish": "2015-07-20T16:06:23Z", 
    "summary": "In this research endeavor, it was hypothesized that the sound produced by\nanimals during their vocalizations can be used as identifiers of the animal\nbreed or species even if they sound the same to unaided human ear. To test this\nhypothesis, three artificial neural networks (ANNs) were developed using\nbioacoustics properties as inputs for the respective automatic identification\nof 13 bird species, eight dog breeds, and 11 frog species. Recorded\nvocalizations of these animals were collected and processed using several known\nsignal processing techniques to convert the respective sounds into computable\nbioacoustics values. The converted values of the vocalizations, together with\nthe breed or species identifications, were used to train the ANNs following a\nten-fold cross validation technique. Tests show that the respective ANNs can\ncorrectly identify 71.43\\% of the birds, 94.44\\% of the dogs, and 90.91\\% of\nthe frogs. This result show that bioacoustics and ANN can be used to\nautomatically determine animal breeds and species, which together could be a\npromising automated tool for animal identification, biodiversity determination,\nanimal conservation, and other animal welfare efforts."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1507.06711v2", 
    "title": "The SYSU System for the Interspeech 2015 Automatic Speaker Verification   Spoofing and Countermeasures Challenge", 
    "arxiv-id": "1507.06711v2", 
    "author": "Ming Li", 
    "publish": "2015-07-24T00:36:01Z", 
    "summary": "Many existing speaker verification systems are reported to be vulnerable\nagainst different spoofing attacks, for example speaker-adapted speech\nsynthesis, voice conversion, play back, etc. In order to detect these spoofed\nspeech signals as a countermeasure, we propose a score level fusion approach\nwith several different i-vector subsystems. We show that the acoustic level\nMel-frequency cepstral coefficients (MFCC) features, the phase level modified\ngroup delay cepstral coefficients (MGDCC) and the phonetic level phoneme\nposterior probability (PPP) tandem features are effective for the\ncountermeasure. Furthermore, feature level fusion of these features before\ni-vector modeling also enhance the performance. A polynomial kernel support\nvector machine is adopted as the supervised classifier. In order to enhance the\ngeneralizability of the countermeasure, we also adopted the cosine similarity\nand PLDA scoring as one-class classifications methods. By combining the\nproposed i-vector subsystems with the OpenSMILE baseline which covers the\nacoustic and prosodic information further improves the final performance. The\nproposed fusion system achieves 0.29% and 3.26% EER on the development and test\nset of the database provided by the INTERSPEECH 2015 automatic speaker\nverification spoofing and countermeasures challenge."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1508.00354v1", 
    "title": "Significance of Maximum Spectral Amplitude in Sub-bands for Spectral   Envelope Estimation and Its Application to Statistical Parametric Speech   Synthesis", 
    "arxiv-id": "1508.00354v1", 
    "author": "Suryakanth V. Gangashetty", 
    "publish": "2015-08-03T09:28:22Z", 
    "summary": "In this paper we propose a technique for spectral envelope estimation using\nmaximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most\nother methods in the literature parametrize spectral envelope in cepstral\ndomain such as Mel-generalized cepstrum etc. Such cepstral domain\nrepresentations, although compact, are not readily interpretable. This\ndifficulty is overcome by our method which parametrizes in the spectral domain\nitself. In our experiments, spectral envelope estimated using MSASB method was\nincorporated in the STRAIGHT vocoder. Both objective and subjective results of\nanalysis-by-synthesis indicate that the proposed method is comparable to\nSTRAIGHT. We also evaluate the effectiveness of the proposed parametrization in\na statistical parametric speech synthesis framework using deep neural networks."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1508.04909v1", 
    "title": "Histogram of gradients of Time-Frequency Representations for Audio scene   detection", 
    "arxiv-id": "1508.04909v1", 
    "author": "Gilles Gasso", 
    "publish": "2015-08-20T08:07:10Z", 
    "summary": "This paper addresses the problem of audio scenes classification and\ncontributes to the state of the art by proposing a novel feature. We build this\nfeature by considering histogram of gradients (HOG) of time-frequency\nrepresentation of an audio scene. Contrarily to classical audio features like\nMFCC, we make the hypothesis that histogram of gradients are able to encode\nsome relevant informations in a time-frequency {representation:} namely, the\nlocal direction of variation (in time and frequency) of the signal spectral\npower. In addition, in order to gain more invariance and robustness, histogram\nof gradients are locally pooled. We have evaluated the relevance of {the novel\nfeature} by comparing its performances with state-of-the-art competitors, on\nseveral datasets, including a novel one that we provide, as part of our\ncontribution. This dataset, that we make publicly available, involves $19$\nclasses and contains about $900$ minutes of audio scene recording. We thus\nbelieve that it may be the next standard dataset for evaluating audio scene\nclassification algorithms. Our comparison results clearly show that our\nHOG-based features outperform its competitors"
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2438547", 
    "link": "http://arxiv.org/pdf/1508.06056v1", 
    "title": "A Novel Reconfigurable Hardware Design for Speech Enhancement Based on   Multi-Band Spectral Subtraction Involving Magnitude and Phase Components", 
    "arxiv-id": "1508.06056v1", 
    "author": "Amlan Chakrabarti", 
    "publish": "2015-08-25T08:02:38Z", 
    "summary": "This paper proposes an efficient reconfigurable hardware design for speech\nenhancement based on multi band spectral subtraction algorithm and involving\nboth magnitude and phase components. Our proposed design is novel as it\nestimates environmental noise from speech adaptively utilizing both magnitude\nand phase components of the speech spectrum. We performed multi-band spectrum\nsubtraction by dividing the noisy speech spectrum into different non-uniform\nfrequency bands having varying signal to noise ratio (SNR) and subtracting the\nestimated noise from each of these frequency bands. This results to the\nelimination of noise from both high SNR and low SNR signal components for all\nthe frequency bands. We have coined our proposed speech enhancement technique\nas Multi Band Magnitude Phase Spectral Subtraction (MBMPSS). The magnitude and\nphase operations are executed concurrently exploiting the parallel logic blocks\nof Field Programmable Gate Array (FPGA), thus increasing the throughput of the\nsystem to a great extent. We have implemented our design on Spartan6 Lx45 FPGA\nand presented the implementation result in terms of resource utilization and\ndelay information for the different blocks of our design. To the best of our\nbest knowledge, this is a new type of hardware design for speech enhancement\napplication and also a first of its kind implementation on reconfigurable\nhardware. We have used benchmark audio data for the evaluation of the proposed\nhardware and the experimental results show that our hardware shows a better SNR\nvalue compared to the existing state of the art research works."
},{
    "category": "q-bio.NC", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1508.06226v1", 
    "title": "Study of the characteristic parameters of the normal voices of   Argentinian speakers", 
    "arxiv-id": "1508.06226v1", 
    "author": "M. R. Mu\u00f1\u00f3z", 
    "publish": "2014-12-18T17:11:56Z", 
    "summary": "The voice laboratory permits to study the human voices using a method that is\nobjective and noninvasive. In this work, we have studied the parameters of the\nhuman voice such as pitch, formant, jitter, shimmer and harmonic-noise ratio of\na group of young people. This statistical information of parameters is obtained\nfrom Argentinian speakers."
},{
    "category": "cs.SD", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1509.04956v1", 
    "title": "Melodic Contour and Mid-Level Global Features Applied to the Analysis of   Flamenco Cantes", 
    "arxiv-id": "1509.04956v1", 
    "author": "Jos\u00e9 Miguel D\u00edaz-B\u00e1\u00f1ez", 
    "publish": "2015-09-16T15:56:22Z", 
    "summary": "This work focuses on the topic of melodic characterization and similarity in\na specific musical repertoire: a cappella flamenco singing, more specifically\nin debla and martinete styles. We propose the combination of manual and\nautomatic description. First, we use a state-of-the-art automatic transcription\nmethod to account for general melodic similarity from music recordings. Second,\nwe define a specific set of representative mid-level melodic features, which\nare manually labeled by flamenco experts. Both approaches are then contrasted\nand combined into a global similarity measure. This similarity measure is\nassessed by inspecting the clusters obtained through phylogenetic algorithms\nalgorithms and by relating similarity to categorization in terms of style.\nFinally, we discuss the advantage of combining automatic and expert annotations\nas well as the need to include repertoire-specific descriptions for meaningful\nmelodic characterization in traditional music collections."
},{
    "category": "cs.SD", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1509.05254v5", 
    "title": "Post-processing speech recordings during MRI", 
    "arxiv-id": "1509.05254v5", 
    "author": "Antti Ojalammi", 
    "publish": "2015-09-17T13:45:44Z", 
    "summary": "We discuss post-processing of speech that has been recorded during Magnetic\nResonance Imaging (MRI) of the vocal tract. Such speech recordings are\ncontaminated by high levels of acoustic noise from the MRI scanner. Also, the\nfrequency response of the sound signal path is not flat as a result of severe\nrestrictions on recording instrumentation due to MRI technology.\n  The post-processing algorithm for noise reduction is based on adaptive\nspectral filtering. The speech material consists of samples of prolonged vowel\nproductions that are used for validation of the post-processing algorithm. The\ncomparison data is recorded in anechoic chamber from the same test subject.\nFormant analysis is carried out for the post-processed speech and the\ncomparison data. Artificially noise-contaminated vowel samples are used for\nvalidation experiments to determine performance of the algorithm where using\ntrue data would be difficult.\n  The properties of recording instrumentation or the post-processing algorithm\ndo not explain the consistent frequency dependent discrepancy between formant\ndata from experiments during MRI and in anechoic chamber. It is shown that the\ndiscrepancy is statistically significant, in particular, where it is largest at\n1 kHz and 2 kHz. The reflecting surfaces of the MRI head and neck coil are\nsuspected to change the speech acoustics which results in \"external formants\"\nat these frequencies. However, the role of test subject adaptation to noise and\nconstrained space acoustics during an MRI examination cannot be ruled out."
},{
    "category": "cs.LG", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1509.06095v1", 
    "title": "Multilayer bootstrap network for unsupervised speaker recognition", 
    "arxiv-id": "1509.06095v1", 
    "author": "Xiao-Lei Zhang", 
    "publish": "2015-09-21T02:28:44Z", 
    "summary": "We apply multilayer bootstrap network (MBN), a recent proposed unsupervised\nlearning method, to unsupervised speaker recognition. The proposed method first\nextracts supervectors from an unsupervised universal background model, then\nreduces the dimension of the high-dimensional supervectors by multilayer\nbootstrap network, and finally conducts unsupervised speaker recognition by\nclustering the low-dimensional data. The comparison results with 2 unsupervised\nand 1 supervised speaker recognition techniques demonstrate the effectiveness\nand robustness of the proposed method."
},{
    "category": "cs.SD", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1509.06103v1", 
    "title": "Noise Robust IOA/CAS Speech Separation and Recognition System For The   Third 'CHIME' Challenge", 
    "arxiv-id": "1509.06103v1", 
    "author": "Yonghong Yan", 
    "publish": "2015-09-21T03:37:11Z", 
    "summary": "This paper presents the contribution to the third 'CHiME' speech separation\nand recognition challenge including both front-end signal processing and\nback-end speech recognition. In the front-end, Multi-channel Wiener filter\n(MWF) is designed to achieve background noise reduction. Different from\ntraditional MWF, optimized parameter for the tradeoff between noise reduction\nand target signal distortion is built according to the desired noise reduction\nlevel. In the back-end, several techniques are taken advantage to improve the\nnoisy Automatic Speech Recognition (ASR) performance including Deep Neural\nNetwork (DNN), Convolutional Neural Network (CNN) and Long short-term memory\n(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary\nlanguage model finite state transducer, and ROVER scheme. Experimental results\nshow the proposed system combining front-end and back-end is effective to\nimprove the ASR performance."
},{
    "category": "cs.SD", 
    "doi": "10.4279/PIP.060002", 
    "link": "http://arxiv.org/pdf/1509.07211v1", 
    "title": "Noise-Robust ASR for the third 'CHiME' Challenge Exploiting   Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent   Neural Network", 
    "arxiv-id": "1509.07211v1", 
    "author": "Fengyun Zhu", 
    "publish": "2015-09-24T02:16:11Z", 
    "summary": "In this paper, the Lingban entry to the third 'CHiME' speech separation and\nrecognition challenge is presented. A time-frequency masking based speech\nenhancement front-end is proposed to suppress the environmental noise utilizing\nmulti-channel coherence and spatial cues. The state-of-the-art speech\nrecognition techniques, namely recurrent neural network based acoustic and\nlanguage modeling, state space minimum Bayes risk based discriminative acoustic\nmodeling, and i-vector based acoustic condition modeling, are carefully\nintegrated into the speech recognition back-end. To further improve the system\nperformance by fully exploiting the advantages of different technologies, the\nfinal recognition results are obtained by lattice combination and rescoring.\nEvaluations carried out on the official dataset prove the effectiveness of the\nproposed systems. Comparing with the best baseline result, the proposed system\nobtains consistent improvements with over 57% relative word error rate\nreduction on the real-data test set."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1509.07659v2", 
    "title": "A dedicated greedy pursuit algorithm for sparse spectral representation   of music sound", 
    "arxiv-id": "1509.07659v2", 
    "author": "Gagan Aggarwal", 
    "publish": "2015-09-25T10:03:17Z", 
    "summary": "A dedicated algorithm for sparse spectral representation of music sound is\npresented. The goal is to enable the representation of a piece of music signal,\nas a linear superposition of as few spectral components as possible. A\nrepresentation of this nature is said to be sparse. In the present context\nsparsity is accomplished by greedy selection of the spectral components, from\nan overcomplete set called a dictionary. The proposed algorithm is tailored to\nbe applied with trigonometric dictionaries. Its distinctive feature being that\nit avoids the need for the actual construction of the whole dictionary, by\nimplementing the required operations via the Fast Fourier Transform. The\nachieved sparsity is theoretically equivalent to that rendered by the\nOrthogonal Matching Pursuit method. The contribution of the proposed dedicated\nimplementation is to extend the applicability of the standard Orthogonal\nMatching Pursuit algorithm, by reducing its storage and computational demands.\nThe suitability of the approach for producing sparse spectral models is\nillustrated by comparison with the traditional method, in the line of the Short\nTime Fourier Transform, involving only the corresponding orthonormal\ntrigonometric basis."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1509.08062v1", 
    "title": "End-to-End Text-Dependent Speaker Verification", 
    "arxiv-id": "1509.08062v1", 
    "author": "Noam Shazeer", 
    "publish": "2015-09-27T07:43:36Z", 
    "summary": "In this paper we present a data-driven, integrated approach to speaker\nverification, which maps a test utterance and a few reference utterances\ndirectly to a single score for verification and jointly optimizes the system's\ncomponents using the same evaluation protocol and metric as at test time. Such\nan approach will result in simple and efficient systems, requiring little\ndomain-specific knowledge and making few model assumptions. We implement the\nidea by formulating the problem as a single neural network architecture,\nincluding the estimation of a speaker model on only a few utterances, and\nevaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker\nverification. The proposed approach appears to be very effective for big data\napplications like ours that require highly accurate, easy-to-maintain systems\nwith a small footprint."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1509.09113v1", 
    "title": "Processing of acoustical signals via a wavelet-based analysis", 
    "arxiv-id": "1509.09113v1", 
    "author": "Evangelos Matsinos", 
    "publish": "2015-09-30T10:33:41Z", 
    "summary": "In the present paper, details are given on the implementation of a\nwavelet-based analysis tailored to the processing of acoustical signals. The\nfamily of the suitable wavelets (`Reimann wavelets') are obtained in the time\ndomain from a Fourier transform, extracted in Ref.~\\cite{r1} after invoking\ntheoretical principles and time-frequency localisation constraints. A scheme is\nset forth to determine the optimal values of the parameters of this type of\nwavelet on the basis of the goodness of the reproduction of a $30$-s audio file\ncontaining harmonic signals corresponding to six successive $A$ notes of the\nchromatic musical scale, from $A_2$ to $A_7$. The quality of the reproduction\nover about six and a half octaves is investigated. Finally, details are given\non the incorporation of the re-assignment method in the analysis framework, as\nthe means a) to determine the important contributions of the wavelet transforms\nand b) to suppress noise present in the signal."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.01443v1", 
    "title": "A Waveform Representation Framework for High-quality Statistical   Parametric Speech Synthesis", 
    "arxiv-id": "1510.01443v1", 
    "author": "Minghui Dong", 
    "publish": "2015-10-06T06:12:31Z", 
    "summary": "State-of-the-art statistical parametric speech synthesis (SPSS) generally\nuses a vocoder to represent speech signals and parameterize them into features\nfor subsequent modeling. Magnitude spectrum has been a dominant feature over\nthe years. Although perceptual studies have shown that phase spectrum is\nessential to the quality of synthesized speech, it is often ignored by using a\nminimum phase filter during synthesis and the speech quality suffers. To bypass\nthis bottleneck in vocoded speech, this paper proposes a phase-embedded\nwaveform representation framework and establishes a magnitude-phase joint\nmodeling platform for high-quality SPSS. Our experiments on waveform\nreconstruction show that the performance is better than that of the widely-used\nSTRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms\na leading-edge, vocoded, deep bidirectional long short-term memory recurrent\nneural network (DBLSTM-RNN)-based baseline system in various objective\nevaluation metrics conducted."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.01949v1", 
    "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", 
    "arxiv-id": "1510.01949v1", 
    "author": "Martti Vainio", 
    "publish": "2015-10-07T14:08:13Z", 
    "summary": "Prominences and boundaries are the essential constituents of prosodic\nstructure in speech. They provide for means to chunk the speech stream into\nlinguistically relevant units by providing them with relative saliences and\ndemarcating them within coherent utterance structures. Prominences and\nboundaries have both been widely used in both basic research on prosody as well\nas in text-to-speech synthesis. However, there are no representation schemes\nthat would provide for both estimating and modelling them in a unified fashion.\nHere we present an unsupervised unified account for estimating and representing\nprosodic prominences and boundaries using a scale-space analysis based on\ncontinuous wavelet transform. The methods are evaluated and compared to earlier\nwork using the Boston University Radio News corpus. The results show that the\nproposed method is comparable with the best published supervised annotation\nmethods."
},{
    "category": "cs.MM", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.03090v1", 
    "title": "An Extension of Interactive Scores for Multimedia Scenarios with   Temporal Relations for Micro and Macro Controls", 
    "arxiv-id": "1510.03090v1", 
    "author": "Julien Castet", 
    "publish": "2015-10-11T19:15:56Z", 
    "summary": "Software to design multimedia scenarios is usually based either on a fixed\ntimeline or on cue lists, but both models are unrelated temporally. On the\ncontrary, the formalism of interactive scores can describe multimedia scenarios\nwith flexible and fixed temporal relations among the objects of the scenario,\nbut cannot express neither temporal relations for micro controls nor signal\nprocessing. We extend interactive scores with such relations and with sound\nprocessing. We show some applications and we describe how they can be\nimplemented in Pure Data. Our implementation has low average relative jitter\neven under high cpu load."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.03602v1", 
    "title": "A language model based approach towards large scale and lightweight   language identification systems", 
    "arxiv-id": "1510.03602v1", 
    "author": "Manish Shrivastava", 
    "publish": "2015-10-13T09:51:23Z", 
    "summary": "Multilingual spoken dialogue systems have gained prominence in the recent\npast necessitating the requirement for a front-end Language Identification\n(LID) system. Most of the existing LID systems rely on modeling the language\ndiscriminative information from low-level acoustic features. Due to the\nvariabilities of speech (speaker and emotional variabilities, etc.),\nlarge-scale LID systems developed using low-level acoustic features suffer from\na degradation in the performance. In this approach, we have attempted to model\nthe higher level language discriminative phonotactic information for developing\nan LID system. In this paper, the input speech signal is tokenized to phone\nsequences by using a language independent phone recognizer. The language\ndiscriminative phonotactic information in the obtained phone sequences are\nmodeled using statistical and recurrent neural network based language modeling\napproaches. As this approach, relies on higher level phonotactical information\nit is more robust to variabilities of speech. Proposed approach is\ncomputationally light weight, highly scalable and it can be used in complement\nwith the existing LID systems."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4964342", 
    "link": "http://arxiv.org/pdf/1510.04029v1", 
    "title": "Corpus COFLA: A research corpus for the Computational study of Flamenco   Music", 
    "arxiv-id": "1510.04029v1", 
    "author": "Emilia G\u00f3mez", 
    "publish": "2015-10-14T10:14:29Z", 
    "summary": "Flamenco is a music tradition from Southern Spain which attracts a growing\ncommunity of enthusiasts around the world. Its unique melodic and rhythmic\nelements, the typically spontaneous and improvised interpretation and its\ndiversity regarding styles make this still largely undocumented art form a\nparticularly interesting material for musicological studies. In prior works it\nhas already been demonstrated that research on computational analysis of\nflamenco music, despite it being a relatively new field, can provide powerful\ntools for the discovery and diffusion of this genre. In this paper we present\ncorpusCOFLA, a data framework for the development of such computational tools.\nThe proposed collection of audio recordings and meta-data serves as a pool for\ncreating annotated subsets which can be used in development and evaluation of\nalgorithms for specific music information retrieval tasks. First, we describe\nthe design criteria for the corpus creation and then provide various examples\nof subsets drawn from the corpus. We showcase possible research applications in\nthe context of computational study of flamenco music and give perspectives\nregarding further development of the corpus."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1510.04039v1", 
    "title": "Automatic Transcription of Flamenco Singing from Polyphonic Music   Recordings", 
    "arxiv-id": "1510.04039v1", 
    "author": "Emilia G\u00f3mez", 
    "publish": "2015-10-14T10:53:00Z", 
    "summary": "Automatic note-level transcription is considered one of the most challenging\ntasks in music information retrieval. The specific case of flamenco singing\ntranscription poses a particular challenge due to its complex melodic\nprogressions, intonation inaccuracies, the use of a high degree of\nornamentation and the presence of guitar accompaniment. In this study, we\nexplore the limitations of existing state of the art transcription systems for\nthe case of flamenco singing and propose a specific solution for this genre: We\nfirst extract the predominant melody and apply a novel contour filtering\nprocess to eliminate segments of the pitch contour which originate from the\nguitar accompaniment. We formulate a set of onset detection functions based on\nvolume and pitch characteristics to segment the resulting vocal pitch contour\ninto discrete note events. A quantised pitch label is assigned to each note\nevent by combining global pitch class probabilities with local pitch contour\nstatistics. The proposed system outperforms state of the art singing\ntranscription systems with respect to voicing accuracy, onset detection and\noverall performance when evaluated on flamenco singing datasets."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1510.04880v1", 
    "title": "Harmonic and Timbre Analysis of Tabla Strokes", 
    "arxiv-id": "1510.04880v1", 
    "author": "Dipak Ghosh", 
    "publish": "2015-10-15T18:42:04Z", 
    "summary": "Indian twin drums mainly bayan and dayan (tabla) are the most important\npercussion instruments in India popularly used for keeping rhythm. It is a twin\npercussion/drum instrument of which the right hand drum is called dayan and the\nleft hand drum is called bayan. Tabla strokes are commonly called as `bol',\nconstitutes a series of syllables. In this study we have studied the timbre\ncharacteristics of nine strokes from each of five different tablas. Timbre\nparameters were calculated from LTAS of each stroke signals. Study of timbre\ncharacteristics is one of the most important deterministic approach for\nanalyzing tabla and its stroke characteristics. Statistical correlations among\ntimbre parameters were measured and also through factor analysis we get to know\nabout the parameters of timbre analysis which are closely related. Tabla\nstrokes have unique harmonic and timbral characteristics at mid frequency range\nand have no uniqueness at low frequency ranges."
},{
    "category": "cs.SY", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1510.05073v1", 
    "title": "Block Sparse Memory Improved Proportionate Affine Projection Sign   Algorithm", 
    "arxiv-id": "1510.05073v1", 
    "author": "Steven L. Grant", 
    "publish": "2015-10-17T05:51:01Z", 
    "summary": "A block sparse memory improved proportionate affine projection sign algorithm\n(BS-MIP-APSA) is proposed for block sparse system identification under\nimpulsive noise. The new BS-MIP-APSA not only inherits the performance\nimprovement for block-sparse system identification, but also achieves\nrobustness to impulsive noise and the efficiency of the memory improved\nproportionate affine projection sign algorithm (MIP-APSA). Simulations indicate\nthat it can provide both faster convergence rate and better tracking ability\nunder impulsive interference for block sparse system identification as compared\nto APSA and MIP-APSA."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1510.05937v2", 
    "title": "Binary Speaker Embedding", 
    "arxiv-id": "1510.05937v2", 
    "author": "Thomas Fang Zheng", 
    "publish": "2015-10-20T15:49:59Z", 
    "summary": "The popular i-vector model represents speakers as low-dimensional continuous\nvectors (i-vectors), and hence it is a way of continuous speaker embedding. In\nthis paper, we investigate binary speaker embedding, which transforms i-vectors\nto binary vectors (codes) by a hash function. We start from locality sensitive\nhashing (LSH), a simple binarization approach where binary codes are derived\nfrom a set of random hash functions. A potential problem of LSH is that the\nrandomly sampled hash functions might be suboptimal. We therefore propose an\nimproved Hamming distance learning approach, where the hash function is learned\nby a variable-sized block training that projects each dimension of the original\ni-vectors to variable-sized binary codes independently. Our experiments show\nthat binary speaker embedding can deliver competitive or even better results on\nboth speaker verification and identification tasks, while the memory usage and\nthe computation cost are significantly reduced."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1510.05940v2", 
    "title": "Max-margin Metric Learning for Speaker Recognition", 
    "arxiv-id": "1510.05940v2", 
    "author": "Thomas Fang Zheng", 
    "publish": "2015-10-20T16:01:05Z", 
    "summary": "Probabilistic linear discriminant analysis (PLDA) is a popular normalization\napproach for the i-vector model, and has delivered state-of-the-art performance\nin speaker recognition. A potential problem of the PLDA model, however, is that\nit essentially assumes Gaussian distributions over speaker vectors, which is\nnot always true in practice. Additionally, the objective function is not\ndirectly related to the goal of the task, e.g., discriminating true speakers\nand imposters. In this paper, we propose a max-margin metric learning approach\nto solve the problems. It learns a linear transform with a criterion that the\nmargin between target and imposter trials are maximized. Experiments conducted\non the SRE08 core test show that compared to PLDA, the new approach can obtain\ncomparable or even better performance, though the scoring is simply a cosine\ncomputation."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2531284", 
    "link": "http://arxiv.org/pdf/1511.00360v1", 
    "title": "Automatic Prosody Prediction for Chinese Speech Synthesis using   BLSTM-RNN and Embedding Features", 
    "arxiv-id": "1511.00360v1", 
    "author": "Yang Liu", 
    "publish": "2015-11-02T02:34:12Z", 
    "summary": "Prosody affects the naturalness and intelligibility of speech. However,\nautomatic prosody prediction from text for Chinese speech synthesis is still a\ngreat challenge and the traditional conditional random fields (CRF) based\nmethod always heavily relies on feature engineering. In this paper, we propose\nto use neural networks to predict prosodic boundary labels directly from\nChinese characters without any feature engineering. Experimental results show\nthat stacking feed-forward and bidirectional long short-term memory (BLSTM)\nrecurrent network layers achieves superior performance over the CRF-based\nmethod. The embedding features learned from raw text further enhance the\nperformance."
},{
    "category": "cs.CV", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1511.02503v5", 
    "title": "Bearing fault diagnosis based on spectrum images of vibration signals", 
    "arxiv-id": "1511.02503v5", 
    "author": "Gongbo Zhou", 
    "publish": "2015-11-08T16:51:22Z", 
    "summary": "Bearing fault diagnosis has been a challenge in the monitoring activities of\nrotating machinery, and it's receiving more and more attention. The\nconventional fault diagnosis methods usually extract features from the\nwaveforms or spectrums of vibration signals in order to realize fault\nclassification. In this paper, a novel feature in the form of images is\npresented, namely the spectrum images of vibration signals. The spectrum images\nare simply obtained by doing fast Fourier transformation. Such images are\nprocessed with two-dimensional principal component analysis (2DPCA) to reduce\nthe dimensions, and then a minimum distance method is applied to classify the\nfaults of bearings. The effectiveness of the proposed method is verified with\nexperimental data."
},{
    "category": "cs.SD", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1511.03174v3", 
    "title": "Fault Diagnosis of Rolling Element Bearings with a Spectrum Searching   Method", 
    "arxiv-id": "1511.03174v3", 
    "author": "Gongbo Zhou", 
    "publish": "2015-11-10T16:39:01Z", 
    "summary": "Rolling element bearing faults in rotating systems are observed as impulses\nin the vibration signals, which are usually buried in noises. In order to\neffectively detect the fault of bearings, a novel spectrum searching method is\nproposed. The structural information of spectrum (SIOS) on a predefined basis\nis constructed through a searching algorithm, such that the harmonics of\nimpulses generated by faults can be clearly identified and analyzed. Local\npeaks of the spectrum are located on a certain bin of the basis, and then the\nSIOS can interpret the spectrum via the number and energy of harmonics related\nto frequency bins of the basis. Finally bearings can be diagnosed based on the\nSIOS by identifying its dominant components. Mathematical formulation is\ndeveloped to guarantee the correct construction of the SISO through searching.\nThe effectiveness of the proposed method is verified with a simulation signal\nand a benchmark study of bearings."
},{
    "category": "cs.CL", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1512.01882v2", 
    "title": "THCHS-30 : A Free Chinese Speech Corpus", 
    "arxiv-id": "1512.01882v2", 
    "author": "Xuewei Zhang", 
    "publish": "2015-12-07T02:07:21Z", 
    "summary": "Speech data is crucially important for speech recognition research. There are\nquite some speech databases that can be purchased at prices that are reasonable\nfor most research institutes. However, for young people who just start research\nactivities or those who just gain initial interest in this direction, the cost\nfor data is still an annoying barrier. We support the `free data' movement in\nspeech recognition: research institutes (particularly supported by public\nfunds) publish their data freely so that new researchers can obtain sufficient\ndata to kick of their career. In this paper, we follow this trend and release a\nfree Chinese speech database THCHS-30 that can be used to build a full- edged\nChinese speech recognition system. We report the baseline system established\nwith this database, including the performance under highly noisy conditions."
},{
    "category": "cs.SD", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1512.02560v1", 
    "title": "Deep Learning for Single and Multi-Session i-Vector Speaker Recognition", 
    "arxiv-id": "1512.02560v1", 
    "author": "Javier Hernando", 
    "publish": "2015-12-08T17:34:49Z", 
    "summary": "The promising performance of Deep Learning (DL) in speech recognition has\nmotivated the use of DL in other speech technology applications such as speaker\nrecognition. Given i-vectors as inputs, the authors proposed an impostor\nselection algorithm and a universal model adaptation process in a hybrid system\nbased on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to\ndiscriminatively model each target speaker. In order to have more insight into\nthe behavior of DL techniques in both single and multi-session speaker\nenrollment tasks, some experiments have been carried out in this paper in both\nscenarios. Additionally, the parameters of the global model, referred to as\nuniversal DBN (UDBN), are normalized before adaptation. UDBN normalization\nfacilitates training DNNs specifically with more than one hidden layer.\nExperiments are performed on the NIST SRE 2006 corpus. It is shown that the\nproposed impostor selection algorithm and UDBN adaptation process enhance the\nperformance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the\nsingle and multi-session tasks, respectively. In both scenarios, the proposed\narchitectures outperform the baseline systems obtaining up to 17 % reduction in\nEER."
},{
    "category": "cs.NI", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1512.05103v1", 
    "title": "Simultaneous Acoustic Localization of Multiple Smartphones with   Euclidean Distance Matrices", 
    "arxiv-id": "1512.05103v1", 
    "author": "Dacfey Dzung", 
    "publish": "2015-12-16T09:46:53Z", 
    "summary": "In this paper, we present an acoustic localization system for multiple\ndevices. In contrast to systems which localise a device relative to one or\nseveral anchor points, we focus on the joint localisation of several devices\nrelative to each other. We present a prototype of our system on off-the-shelf\nsmartphones. No user interaction is required, the phones emit acoustic pulses\naccording to a precomputed schedule. Using the elapsed time between two times\nof arrivals (ETOA) method with sample counting, distances between the devices\nare estimated. These, possibly incomplete, distances are the input to an\nefficient and robust multi-dimensional scaling algorithm returning a position\nfor each phone. We evaluated our system in real-world scenarios, achieving\nerror margins of 15 cm in an office environment."
},{
    "category": "cs.SD", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1512.07370v1", 
    "title": "Musical instrument sound classification with deep convolutional neural   network using feature fusion approach", 
    "arxiv-id": "1512.07370v1", 
    "author": "Taejin Lee", 
    "publish": "2015-12-23T07:09:13Z", 
    "summary": "A new musical instrument classification method using convolutional neural\nnetworks (CNNs) is presented in this paper. Unlike the traditional methods, we\ninvestigated a scheme for classifying musical instruments using the learned\nfeatures from CNNs. To create the learned features from CNNs, we not only used\na conventional spectrogram image, but also proposed multiresolution recurrence\nplots (MRPs) that contain the phase information of a raw input signal.\nConsequently, we fed the characteristic timbre of the particular instrument\ninto a neural network, which cannot be extracted using a phase-blinded\nrepresentations such as a spectrogram. By combining our proposed MRPs and\nspectrogram images with a multi-column network, the performance of our proposed\nclassifier system improves over a system that uses only a spectrogram.\nFurthermore, the proposed classifier also outperforms the baseline result from\ntraditional handcrafted features and classifiers."
},{
    "category": "cs.SD", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1512.08982v1", 
    "title": "Technical Report: a tool for measuring Prosodic Accommodation", 
    "arxiv-id": "1512.08982v1", 
    "author": "Sucheta Ghosh", 
    "publish": "2015-12-30T15:46:30Z", 
    "summary": "Social interaction is a dynamic and joint activity where all participants are\nengaged and coordinate their behaviour in the co-construction of meaning. It is\nobserved that conversational partners adapt their pitch, intensity and timing\nbehaviour to their in- terlocutors. The majority of research has focused on its\nlinear manifestation over the course of an interaction. De Looze et al\nhypothesised that it evolves dynamically with functional social aspects. In the\nwork of De Looze et al, they proposed through their praat based feature\nextraction and matlab based visualisation that one can visualise prosodic\naccommodation at the positive correlation threshold values. and the capture of\nits dynamic manifestation. Here we seek to build a complete system for\nmeasuring prosodic accommodation with matlab. This work uses data collected in\na pilot training scenario where senior pilot and co-pilot are engaged in\nconversation during the flight. Additionally, we use the data from a ship wreck\ntask by two pilots. We also attempt to evaluate this measures of accommodation\nwith ground truth labels given by a trainer of Crew (or sometimes Crisis)\nResource Management (CRM)."
},{
    "category": "cs.SD", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1601.00287v1", 
    "title": "Wavelet Scattering on the Pitch Spiral", 
    "arxiv-id": "1601.00287v1", 
    "author": "St\u00e9phane Mallat", 
    "publish": "2016-01-03T12:30:38Z", 
    "summary": "We present a new representation of harmonic sounds that linearizes the\ndynamics of pitch and spectral envelope, while remaining stable to deformations\nin the time-frequency plane. It is an instance of the scattering transform, a\ngeneric operator which cascades wavelet convolutions and modulus\nnonlinearities. It is derived from the pitch spiral, in that convolutions are\nsuccessively performed in time, log-frequency, and octave index. We give a\nclosed-form approximation of spiral scattering coefficients for a nonstationary\ngeneralization of the harmonic source-filter model."
},{
    "category": "cs.CV", 
    "doi": "10.1088/0957-0233/27/3/035005", 
    "link": "http://arxiv.org/pdf/1601.02220v1", 
    "title": "Joint Object-Material Category Segmentation from Audio-Visual Cues", 
    "arxiv-id": "1601.02220v1", 
    "author": "Philip Torr", 
    "publish": "2016-01-10T14:14:53Z", 
    "summary": "It is not always possible to recognise objects and infer material properties\nfor a scene from visual cues alone, since objects can look visually similar\nwhilst being made of very different materials. In this paper, we therefore\npresent an approach that augments the available dense visual cues with sparse\nauditory cues in order to estimate dense object and material labels. Since\nestimates of object class and material properties are mutually informative, we\noptimise our multi-output labelling jointly using a random-field framework. We\nevaluate our system on a new dataset with paired visual and auditory data that\nwe make publicly available. We demonstrate that this joint estimation of object\nand material labels significantly outperforms the estimation of either category\nin isolation."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1601.05991v3", 
    "title": "Speech vocoding for laboratory phonology", 
    "arxiv-id": "1601.05991v3", 
    "author": "Alexandros Lazaridis", 
    "publish": "2016-01-22T13:22:10Z", 
    "summary": "Using phonological speech vocoding, we propose a platform for exploring\nrelations between phonology and speech processing, and in broader terms, for\nexploring relations between the abstract and physical structures of a speech\nsignal. Our goal is to make a step towards bridging phonology and speech\nprocessing and to contribute to the program of Laboratory Phonology. We show\nthree application examples for laboratory phonology: compositional phonological\nspeech modelling, a comparison of phonological systems and an experimental\nphonological parametric text-to-speech (TTS) system. The featural\nrepresentations of the following three phonological systems are considered in\nthis work: (i) Government Phonology (GP), (ii) the Sound Pattern of English\n(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded\nspeech, we conclude that the latter achieves slightly better results than the\nformer. However, GP - the most compact phonological speech representation -\nperforms comparably to the systems with a higher number of phonological\nfeatures. The parametric TTS based on phonological speech representation, and\ntrained from an unlabelled audiobook in an unsupervised manner, achieves\nintelligibility of 85% of the state-of-the-art parametric speech synthesis. We\nenvision that the presented approach paves the way for researchers in both\nfields to form meaningful hypotheses that are explicitly testable using the\nconcepts developed and exemplified in this paper. On the one hand, laboratory\nphonologists might test the applied concepts of their theoretical models, and\non the other hand, the speech processing community may utilize the concepts\ndeveloped for the theoretical phonological models for improvements of the\ncurrent state-of-the-art applications."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1601.06008v1", 
    "title": "A Robust Frame-based Nonlinear Prediction System for Automatic Speech   Coding", 
    "arxiv-id": "1601.06008v1", 
    "author": "Farbod Razzazi", 
    "publish": "2016-01-22T14:13:53Z", 
    "summary": "In this paper, we propose a neural-based coding scheme in which an artificial\nneural network is exploited to automatically compress and decompress speech\nsignals by a trainable approach. Having a two-stage training phase, the system\ncan be fully specified to each speech frame and have robust performance across\ndifferent speakers and wide range of spoken utterances. Indeed, Frame-based\nnonlinear predictive coding (FNPC) would code a frame in the procedure of\ntraining to predict the frame samples. The motivating objective is to analyze\nthe system behavior in regenerating not only the envelope of spectra, but also\nthe spectra phase. This scheme has been evaluated in time and discrete cosine\ntransform (DCT) domains and the output of predicted phonemes show the\npotentiality of the FNPC to reconstruct complicated signals. The experiments\nwere conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains\nversus the number of neurons in the hidden layer. Experiments approve the FNPC\ncapability as an automatic coding system by which /b/d/g/ phonemes have been\nreproduced with a good accuracy. Evaluations revealed that the performance of\nFNPC system, trained to predict DCT coefficients is more desirable,\nparticularly for frames with the wider distribution of energy, compared to time\nsamples."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1602.02656v1", 
    "title": "LSTM Deep Neural Networks Postfiltering for Improving the Quality of   Synthetic Voices", 
    "arxiv-id": "1602.02656v1", 
    "author": "John Goddard-Close", 
    "publish": "2016-02-08T17:25:22Z", 
    "summary": "Recent developments in speech synthesis have produced systems capable of\noutcome intelligible speech, but now researchers strive to create models that\nmore accurately mimic human voices. One such development is the incorporation\nof multiple linguistic styles in various languages and accents.\n  HMM-based Speech Synthesis is of great interest to many researchers, due to\nits ability to produce sophisticated features with small footprint. Despite\nsuch progress, its quality has not yet reached the level of the predominant\nunit-selection approaches that choose and concatenate recordings of real\nspeech. Recent efforts have been made in the direction of improving these\nsystems.\n  In this paper we present the application of Long-Short Term Memory Deep\nNeural Networks as a Postfiltering step of HMM-based speech synthesis, in order\nto obtain closer spectral characteristics to those of natural speech. The\nresults show how HMM-voices could be improved using this approach."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1602.02950v1", 
    "title": "Spoofing detection under noisy conditions: a preliminary investigation   and an initial database", 
    "arxiv-id": "1602.02950v1", 
    "author": "Haizhou Li", 
    "publish": "2016-02-09T12:00:56Z", 
    "summary": "Spoofing detection for automatic speaker verification (ASV), which is to\ndiscriminate between live speech and attacks, has received increasing\nattentions recently. However, all the previous studies have been done on the\nclean data without significant additive noise. To simulate the real-life\nscenarios, we perform a preliminary investigation of spoofing detection under\nadditive noisy conditions, and also describe an initial database for this task.\nThe noisy database is based on the ASVspoof challenge 2015 database and\ngenerated by artificially adding background noises at different signal-to-noise\nratios (SNRs). Five different additive noises are included. Our preliminary\nresults show that using the model trained from clean data, the system\nperformance degrades significantly in noisy conditions. Phase-based feature is\nmore noise robust than magnitude-based features. And the systems perform\nsignificantly differ under different noise scenarios."
},{
    "category": "cs.MM", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1602.04845v1", 
    "title": "High-Quality, Low-Delay Music Coding in the Opus Codec", 
    "arxiv-id": "1602.04845v1", 
    "author": "Koen Vos", 
    "publish": "2016-02-15T21:30:54Z", 
    "summary": "The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide\nrange of real-time Internet applications by combining a linear prediction coder\nwith a transform coder. We describe the transform coder, with particular\nattention to the psychoacoustic knowledge built into the format. The result\nout-performs existing audio codecs that do not operate under real-time\nconstraints."
},{
    "category": "cs.MM", 
    "doi": "10.1016/j.csl.2016.10.001", 
    "link": "http://arxiv.org/pdf/1602.05311v1", 
    "title": "A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay", 
    "arxiv-id": "1602.05311v1", 
    "author": "Gregory Maxwell", 
    "publish": "2016-02-17T05:50:50Z", 
    "summary": "We propose an audio codec that addresses the low-delay requirements of some\napplications such as network music performance. The codec is based on the\nmodified discrete cosine transform (MDCT) with very short frames and uses\ngain-shape quantization to preserve the spectral envelope. The short frame\nsizes required for low delay typically hinder the performance of transform\ncodecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the\nproposed codec out-performs the ULD codec operating at the same rate. The total\ncomplexity of the codec is small, at only 17 WMOPS for real-time operation at\n48 kHz."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASL.2009.2023186", 
    "link": "http://arxiv.org/pdf/1602.05526v1", 
    "title": "A High-Quality Speech and Audio Codec With Less Than 10 ms Delay", 
    "arxiv-id": "1602.05526v1", 
    "author": "Gregory Maxwell", 
    "publish": "2016-02-17T18:41:16Z", 
    "summary": "With increasing quality requirements for multimedia communications, audio\ncodecs must maintain both high quality and low delay. Typically, audio codecs\noffer either low delay or high quality, but rarely both. We propose a codec\nthat simultaneously addresses both these requirements, with a delay of only 8.7\nms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the\nfrequency domain with time-domain pitch prediction. We demonstrate that the\nproposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C\nand MP3 and has quality comparable to AAC-LD, despite having less than one\nfourth of the algorithmic delay of these codecs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASL.2009.2023186", 
    "link": "http://arxiv.org/pdf/1602.05682v2", 
    "title": "Audio Recording Device Identification Based on Deep Learning", 
    "arxiv-id": "1602.05682v2", 
    "author": "Shaopei Shi", 
    "publish": "2016-02-18T05:49:37Z", 
    "summary": "In this paper we present a research on identification of audio recording\ndevices from background noise, thus providing a method for forensics. The audio\nsignal is the sum of speech signal and noise signal. Usually, people pay more\nattention to speech signal, because it carries the information to deliver. So a\ngreat amount of researches have been dedicated to getting higher\nSignal-Noise-Ratio (SNR). There are many speech enhancement algorithms to\nimprove the quality of the speech, which can be seen as reducing the noise.\nHowever, noises can be regarded as the intrinsic fingerprint traces of an audio\nrecording device. These digital traces can be characterized and identified by\nnew machine learning techniques. Therefore, in our research, we use the noise\nas the intrinsic features. As for the identification, multiple classifiers of\ndeep learning methods are used and compared. The identification result shows\nthat the method of getting feature vector from the noise of each device and\nidentifying them with deep learning techniques is viable, and well-preformed."
},{
    "category": "cs.RO", 
    "doi": "10.1109/TRO.2007.900612", 
    "link": "http://arxiv.org/pdf/1602.06442v1", 
    "title": "Robust Recognition of Simultaneous Speech By a Mobile Robot", 
    "arxiv-id": "1602.06442v1", 
    "author": "Hiroshi G. Okuno", 
    "publish": "2016-02-20T19:36:55Z", 
    "summary": "This paper describes a system that gives a mobile robot the ability to\nperform automatic speech recognition with simultaneous speakers. A microphone\narray is used along with a real-time implementation of Geometric Source\nSeparation and a post-filter that gives a further reduction of interference\nfrom other sources. The post-filter is also used to estimate the reliability of\nspectral features and compute a missing feature mask. The mask is used in a\nmissing feature theory-based speech recognition system to recognize the speech\nfrom simultaneous Japanese speakers in the context of a humanoid robot.\nRecognition rates are presented for three simultaneous speakers located at 2\nmeters from the robot. The system was evaluated on a 200 word vocabulary at\ndifferent azimuths between sources, ranging from 10 to 90 degrees. Compared to\nthe use of the microphone array source separation alone, we demonstrate an\naverage reduction in relative recognition error rate of 24% with the\npost-filter and of 42% when the missing features approach is combined with the\npost-filter. We demonstrate the effectiveness of our multi-source microphone\narray post-filter and the improvement it provides when used in conjunction with\nthe missing features theory."
},{
    "category": "cs.RO", 
    "doi": "10.1109/TRO.2007.900612", 
    "link": "http://arxiv.org/pdf/1602.06652v1", 
    "title": "Auditory System for a Mobile Robot", 
    "arxiv-id": "1602.06652v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-02-22T05:26:40Z", 
    "summary": "In this thesis, we propose an artificial auditory system that gives a robot\nthe ability to locate and track sounds, as well as to separate simultaneous\nsound sources and recognising simultaneous speech. We demonstrate that it is\npossible to implement these capabilities using an array of microphones, without\ntrying to imitate the human auditory system. The sound source localisation and\ntracking algorithm uses a steered beamformer to locate sources, which are then\ntracked using a multi-source particle filter. Separation of simultaneous sound\nsources is achieved using a variant of the Geometric Source Separation (GSS)\nalgorithm, combined with a multi-source post-filter that further reduces noise,\ninterference and reverberation. Speech recognition is performed on separated\nsources, either directly or by using Missing Feature Theory (MFT) to estimate\nthe reliability of the speech features.\n  The results obtained show that it is possible to track up to four\nsimultaneous sound sources, even in noisy and reverberant environments.\nReal-time control of the robot following a sound source is also demonstrated.\nThe sound source separation approach we propose is able to achieve a 13.7 dB\nimprovement in signal-to-noise ratio compared to a single microphone when three\nspeakers are present. In these conditions, the system demonstrates more than\n80% accuracy on digit recognition, higher than most human listeners could\nobtain in our small case study when recognising only one of these sources. All\nthese new capabilities will allow humans to interact more naturally with a\nmobile robot in real life settings."
},{
    "category": "cs.SD", 
    "doi": "10.1109/CISP.2015.7408064", 
    "link": "http://arxiv.org/pdf/1602.07394v1", 
    "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic   Features", 
    "arxiv-id": "1602.07394v1", 
    "author": "Zhenhao Ge", 
    "publish": "2016-02-24T04:33:49Z", 
    "summary": "Researches have shown accent classification can be improved by integrating\nsemantic information into pure acoustic approach. In this work, we combine\nphonetic knowledge, such as vowels, with enhanced acoustic features to build an\nimproved accent classification system. The classifier is based on Gaussian\nMixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual\nLinear Predictive (PLP) features. The features are further optimized by\nPrinciple Component Analysis (PCA) and Hetroscedastic Linear Discriminant\nAnalysis (HLDA). Using 7 major types of accented speech from the Foreign\nAccented English (FAE) corpus, the system achieves classification accuracy 54%\nwith input test data as short as 20 seconds, which is competitive to the state\nof the art in this field."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASL.2006.885935", 
    "link": "http://arxiv.org/pdf/1602.08044v1", 
    "title": "On Adjusting the Learning Rate in Frequency Domain Echo Cancellation   With Double-Talk", 
    "arxiv-id": "1602.08044v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-02-25T19:17:19Z", 
    "summary": "One of the main difficulties in echo cancellation is the fact that the\nlearning rate needs to vary according to conditions such as double-talk and\necho path change. In this paper we propose a new method of varying the learning\nrate of a frequency-domain echo canceller. This method is based on the\nderivation of the optimal learning rate of the NLMS algorithm in the presence\nof noise. The method is evaluated in conjunction with the multidelay block\nfrequency domain (MDF) adaptive filter. We demonstrate that it performs better\nthan current double-talk detection techniques and is simple to implement."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.919235", 
    "link": "http://arxiv.org/pdf/1602.08045v1", 
    "title": "PCA/LDA Approach for Text-Independent Speaker Recognition", 
    "arxiv-id": "1602.08045v1", 
    "author": "Mark J. T. Smith", 
    "publish": "2016-02-25T19:18:06Z", 
    "summary": "Various algorithms for text-independent speaker recognition have been\ndeveloped through the decades, aiming to improve both accuracy and efficiency.\nThis paper presents a novel PCA/LDA-based approach that is faster than\ntraditional statistical model-based methods and achieves competitive results.\nFirst, the performance based on only PCA and only LDA is measured; then a mixed\nmodel, taking advantages of both methods, is introduced. A subset of the TIMIT\ncorpus composed of 200 male speakers, is used for enrollment, validation and\ntesting. The best results achieve 100%; 96% and 95% classification rate at\npopulation level 50; 100 and 200, using 39-dimensional MFCC features with delta\nand double delta. These results are based on 12-second text-independent speech\nfor training and 4-second data for test. These are comparable to the\nconventional MFCC-GMM methods, but require significantly less time to train and\noperate."
},{
    "category": "cs.SY", 
    "doi": "10.1109/LSP.2007.908017", 
    "link": "http://arxiv.org/pdf/1602.08116v1", 
    "title": "Interference-Normalised Least Mean Square Algorithm", 
    "arxiv-id": "1602.08116v1", 
    "author": "Iain B. Collings", 
    "publish": "2016-02-25T21:12:05Z", 
    "summary": "An interference-normalised least mean square (INLMS) algorithm for robust\nadaptive filtering is proposed. The INLMS algorithm extends the\ngradient-adaptive learning rate approach to the case where the signals are\nnon-stationary. In particular, we show that the INLMS algorithm can work even\nfor highly non-stationary interference signals, where previous\ngradient-adaptive learning rate algorithms fail."
},{
    "category": "cs.SD", 
    "doi": "10.1109/CISP.2011.6100685", 
    "link": "http://arxiv.org/pdf/1602.08132v1", 
    "title": "Adaptive Frequency Cepstral Coefficients for Word Mispronunciation   Detection", 
    "arxiv-id": "1602.08132v1", 
    "author": "Mark J. T. Smith", 
    "publish": "2016-02-25T22:17:31Z", 
    "summary": "Systems based on automatic speech recognition (ASR) technology can provide\nimportant functionality in computer assisted language learning applications.\nThis is a young but growing area of research motivated by the large number of\nstudents studying foreign languages. Here we propose a Hidden Markov Model\n(HMM)-based method to detect mispronunciations. Exploiting the specific dialog\nscripting employed in language learning software, HMMs are trained for\ndifferent pronunciations. New adaptive features have been developed and\nobtained through an adaptive warping of the frequency scale prior to computing\nthe cepstral coefficients. The optimization criterion used for the warping\nfunction is to maximize separation of two major groups of pronunciations\n(native and non-native) in terms of classification rate. Experimental results\nshow that the adaptive frequency scale yields a better coefficient\nrepresentation leading to higher classification rates in comparison with\nconventional HMMs using Mel-frequency cepstral coefficients."
},{
    "category": "cs.RO", 
    "doi": "10.1016/j.robot.2006.08.004", 
    "link": "http://arxiv.org/pdf/1602.08139v1", 
    "title": "Robust Localization and Tracking of Simultaneous Moving Sound Sources   Using Beamforming and Particle Filtering", 
    "arxiv-id": "1602.08139v1", 
    "author": "Jean Rouat", 
    "publish": "2016-02-25T22:40:00Z", 
    "summary": "Mobile robots in real-life settings would benefit from being able to localize\nand track sound sources. Such a capability can help localizing a person or an\ninteresting event in the environment, and also provides enhanced processing for\nother capabilities such as speech recognition. To give this capability to a\nrobot, the challenge is not only to localize simultaneous sound sources, but to\ntrack them over time. In this paper we propose a robust sound source\nlocalization and tracking method using an array of eight microphones. The\nmethod is based on a frequency-domain implementation of a steered beamformer\nalong with a particle filter-based tracking algorithm. Results show that a\nmobile robot can localize and track in real-time multiple moving sources of\ndifferent types over a range of 7 meters. These new capabilities allow a mobile\nrobot to interact using more natural means with people in real life settings."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.robot.2006.08.004", 
    "link": "http://arxiv.org/pdf/1602.08185v1", 
    "title": "Extension spectrale d'un signal de parole de la bande t\u00e9l\u00e9phonique   \u00e0 la bande AM", 
    "arxiv-id": "1602.08185v1", 
    "author": "Jean-Marc Valin", 
    "publish": "2016-02-26T03:16:37Z", 
    "summary": "This document proposes a bandwidth extension system producing a wideband\nsignal from a narrowband speech signal. The extension is performed\nindependently for high and low frequencies. High-frequency extension uses the\nexcitation-filter model. Extension of the excitation is performed in the time\ndomain using a non-linear function, while the spectral envelope is extended in\nthe cepstral domain using a multi-layer perceptron. Low-band extension is based\non the sinusoidal model. The amplitude of sinusoids is also estimated using a\nmulti-layer perceptron.\n  The results show that the sound quality after extension is higher than that\nof narrowband speech, with a significant variation across listeners. Some of\nthe techniques, including excitation extension, are of interest in the field of\nspeech coding.\n  -----\n  Le pr\\'esent m\\'emoire propose un syst\\`eme d'extension de la bande\npermettant de produire un signal en bande AM \\`a partir d'un signal de parole\nen bande t\\'el\\'ephonique. L'extension est effectu\\'ee de fa\\c{c}on\nind\\'ependante pour les hautes fr\\'equences et les basses fr\\'equences.\nL'extension des hautes fr\\'equences utilise le mod\\`ele filtre-excitation.\nL'extension de l'excitation est r\\'ealis\\'ee dans le domaine temporel par une\nfonction non lin\\'eaire, alors que l'extension de l'enveloppe spectrale\ns'effectue dans le domaine cepstral par un perceptron multi-couches.\nL'extension de la bande basse utilise le mod\\`ele sinuso\\\"idal. L'amplitude des\nsinuso\\\"ides est aussi estim\\'ee par un perceptron multi-couches.\n  Les r\\'esultats obtenus montrent que la qualit\\'e sonore apr\\`es extension\nest sup\\'erieure \\`a celle de la bande t\\'el\\'ephonique, avec une importante\ndiff\\'erence entre les auditeurs. Certaines techniques d\\'evelopp\\'ees, dont\nl'extension de l'excitation, pr\\'esentent un certain int\\'er\\^et pour le\ndomaine du codage de la parole."
},{
    "category": "cs.RO", 
    "doi": "10.1109/IROS.2003.1248813", 
    "link": "http://arxiv.org/pdf/1602.08213v1", 
    "title": "Robust Sound Source Localization Using a Microphone Array on a Mobile   Robot", 
    "arxiv-id": "1602.08213v1", 
    "author": "Dominic L\u00e9tourneau", 
    "publish": "2016-02-26T06:42:30Z", 
    "summary": "The hearing sense on a mobile robot is important because it is\nomnidirectional and it does not require direct line-of-sight with the sound\nsource. Such capabilities can nicely complement vision to help localize a\nperson or an interesting event in the environment. To do so the robot auditory\nsystem must be able to work in noisy, unknown and diverse environmental\nconditions. In this paper we present a robust sound source localization method\nin three-dimensional space using an array of 8 microphones. The method is based\non time delay of arrival estimation. Results show that a mobile robot can\nlocalize in real time different types of sound sources over a range of 3 meters\nand with a precision of 3 degrees."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2007.366624", 
    "link": "http://arxiv.org/pdf/1602.08609v1", 
    "title": "A New Robust Frequency Domain Echo Canceller With Closed-Loop Learning   Rate Adaptation", 
    "arxiv-id": "1602.08609v1", 
    "author": "Iain B. Collings", 
    "publish": "2016-02-27T16:17:06Z", 
    "summary": "One of the main difficulties in echo cancellation is the fact that the\nlearning rate needs to vary according to conditions such as double-talk and\necho path change. Several methods have been proposed to vary the learning. In\nthis paper we propose a new closed-loop method where the learning rate is\nproportional to a misalignment parameter, which is in turn estimated based on a\ngradient adaptive approach. The method is presented in the context of a\nmultidelay block frequency domain (MDF) echo canceller. We demonstrate that the\nproposed algorithm outperforms current popular double-talk detection techniques\nby up to 6 dB."
},{
    "category": "cs.RO", 
    "doi": "10.1109/ROBOT.2004.1307286", 
    "link": "http://arxiv.org/pdf/1602.08629v1", 
    "title": "Localization of Simultaneous Moving Sound Sources for Mobile Robot Using   a Frequency-Domain Steered Beamformer Approach", 
    "arxiv-id": "1602.08629v1", 
    "author": "Jean Rouat", 
    "publish": "2016-02-27T19:14:48Z", 
    "summary": "Mobile robots in real-life settings would benefit from being able to localize\nsound sources. Such a capability can nicely complement vision to help localize\na person or an interesting event in the environment, and also to provide\nenhanced processing for other capabilities such as speech recognition. In this\npaper we present a robust sound source localization method in three-dimensional\nspace using an array of 8 microphones. The method is based on a\nfrequency-domain implementation of a steered beamformer along with a\nprobabilistic post-processor. Results show that a mobile robot can localize in\nreal time multiple moving sources of different types over a range of 5 meters\nwith a response time of 200 ms."
},{
    "category": "cs.HC", 
    "doi": "10.1109/ROBOT.2004.1307286", 
    "link": "http://arxiv.org/pdf/1602.08750v1", 
    "title": "Filtering Video Noise as Audio with Motion Detection to Form a Musical   Instrument", 
    "arxiv-id": "1602.08750v1", 
    "author": "Carl Thom\u00e9", 
    "publish": "2016-02-28T18:31:31Z", 
    "summary": "Even though they differ in the physical domain, digital video and audio share\nmany characteristics. Both are temporal data streams often stored in buffers\nwith 8-bit values. This paper investigates a method for creating harmonic\nsounds with a video signal as input. A musical instrument is proposed, that\nutilizes video in both a sound synthesis method, and in a controller interface\nfor selecting musical notes at specific velocities. The resulting instrument\nwas informally determined by the author to sound both pleasant and interesting,\nbut hard to control, and therefore suited for synth pad sounds."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ROBOT.2004.1307286", 
    "link": "http://arxiv.org/pdf/1603.00982v4", 
    "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations   using Sequence-to-sequence Autoencoder", 
    "arxiv-id": "1603.00982v4", 
    "author": "Lin-Shan Lee", 
    "publish": "2016-03-03T05:44:51Z", 
    "summary": "The vector representations of fixed dimensionality for words (in text)\noffered by Word2Vec have been shown to be very useful in many application\nscenarios, in particular due to the semantic information they carry. This paper\nproposes a parallel version, the Audio Word2Vec. It offers the vector\nrepresentations of fixed dimensionality for variable-length audio segments.\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with very attractive real\nworld applications such as query-by-example Spoken Term Detection (STD). In\nthis STD application, the proposed approach significantly outperformed the\nconventional Dynamic Time Warping (DTW) based approaches at significantly lower\ncomputation requirements. We propose unsupervised learning of Audio Word2Vec\nfrom audio data without human annotation using Sequence-to-sequence Audoencoder\n(SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM)\nunits: the first RNN (encoder) maps the input audio sequence into a vector\nrepresentation of fixed dimensionality, and the second RNN (decoder) maps the\nrepresentation back to the input audio sequence. The two RNNs are jointly\ntrained by minimizing the reconstruction error. Denoising Sequence-to-sequence\nAutoencoder (DSA) is furthered proposed offering more robust learning."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ROBOT.2004.1307286", 
    "link": "http://arxiv.org/pdf/1603.01770v1", 
    "title": "An Argument-based Creative Assistant for Harmonic Blending", 
    "arxiv-id": "1603.01770v1", 
    "author": "Emilios Cambouropoulos", 
    "publish": "2016-03-06T00:06:09Z", 
    "summary": "Conceptual blending is a powerful tool for computational creativity where,\nfor example, the properties of two harmonic spaces may be combined in a\nconsistent manner to produce a novel harmonic space. However, deciding about\nthe importance of property features in the input spaces and evaluating the\nresults of conceptual blending is a nontrivial task. In the specific case of\nmusical harmony, defining the salient features of chord transitions and\nevaluating invented harmonic spaces requires deep musicological background\nknowledge. In this paper, we propose a creative tool that helps musicologists\nto evaluate and to enhance harmonic innovation. This tool allows a music expert\nto specify arguments over given transition properties. These arguments are then\nconsidered by the system when defining combinations of features in an\nidiom-blending process. A music expert can assess whether the new harmonic\nidiom makes musicological sense and re-adjust the arguments (selection of\nfeatures) to explore alternative blends that can potentially produce better\nharmonic spaces. We conclude with a discussion of future work that would\nfurther automate the harmonisation process."
},{
    "category": "cs.RO", 
    "doi": "10.1109/IROS.2004.1389723", 
    "link": "http://arxiv.org/pdf/1603.02341v1", 
    "title": "Enhanced Robot Audition Based on Microphone Array Source Separation with   Post-Filter", 
    "arxiv-id": "1603.02341v1", 
    "author": "Fran\u00e7ois Michaud", 
    "publish": "2016-03-07T23:30:18Z", 
    "summary": "We propose a system that gives a mobile robot the ability to separate\nsimultaneous sound sources. A microphone array is used along with a real-time\ndedicated implementation of Geometric Source Separation and a post-filter that\ngives us a further reduction of interferences from other sources. We present\nresults and comparisons for separation of multiple non-stationary speech\nsources combined with noise sources. The main advantage of our approach for\nmobile robots resides in the fact that both the frequency-domain Geometric\nSource Separation algorithm and the post-filter are able to adapt rapidly to\nnew sources and non-stationarity. Separation results are presented for three\nsimultaneous interfering speakers in the presence of noise. A reduction of log\nspectral distortion (LSD) and increase of signal-to-noise ratio (SNR) of\napproximately 10 dB and 14 dB are observed."
},{
    "category": "cs.SD", 
    "doi": "10.1109/INDICON.2015.7443805", 
    "link": "http://arxiv.org/pdf/1603.04264v1", 
    "title": "Novel Speech Features for Improved Detection of Spoofing Attacks", 
    "arxiv-id": "1603.04264v1", 
    "author": "Goutam Saha", 
    "publish": "2016-03-14T13:49:18Z", 
    "summary": "Now-a-days, speech-based biometric systems such as automatic speaker\nverification (ASV) are highly prone to spoofing attacks by an imposture. With\nrecent development in various voice conversion (VC) and speech synthesis (SS)\nalgorithms, these spoofing attacks can pose a serious potential threat to the\ncurrent state-of-the-art ASV systems. To impede such attacks and enhance the\nsecurity of the ASV systems, the development of efficient anti-spoofing\nalgorithms is essential that can differentiate synthetic or converted speech\nfrom natural or human speech. In this paper, we propose a set of novel speech\nfeatures for detecting spoofing attacks. The proposed features are computed\nusing alternative frequency-warping technique and formant-specific block\ntransformation of filter bank log energies. We have evaluated existing and\nproposed features against several kinds of synthetic speech data from ASVspoof\n2015 corpora. The results show that the proposed techniques outperform existing\napproaches for various spoofing attack detection task. The techniques\ninvestigated in this paper can also accurately classify natural and synthetic\nspeech as equal error rates (EERs) of 0% have been achieved."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TPAMI.2017.2648793", 
    "link": "http://arxiv.org/pdf/1603.09725v2", 
    "title": "Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion", 
    "arxiv-id": "1603.09725v2", 
    "author": "Radu Horaud", 
    "publish": "2016-03-31T19:15:01Z", 
    "summary": "Speaker diarization consists of assigning speech signals to people engaged in\na dialogue. An audio-visual spatiotemporal diarization model is proposed. The\nmodel is well suited for challenging scenarios that consist of several\nparticipants engaged in multi-party interaction while they move around and turn\ntheir heads towards the other participants rather than facing the cameras and\nthe microphones. Multiple-person visual tracking is combined with multiple\nspeech-source localization in order to tackle the speech-to-person association\nproblem. The latter is solved within a novel audio-visual fusion method on the\nfollowing grounds: binaural spectral features are first extracted from a\nmicrophone pair, then a supervised audio-visual alignment technique maps these\nfeatures onto an image, and finally a semi-supervised clustering method assigns\nbinaural spectral features to visible persons. The main advantage of this\nmethod over previous work is that it processes in a principled way speech\nsignals uttered simultaneously by multiple persons. The diarization itself is\ncast into a latent-variable temporal graphical model that infers speaker\nidentities and speech turns, based on the output of an audio-visual association\nprocess, executed at each time slice, and on the dynamics of the diarization\nvariable itself. The proposed formulation yields an efficient exact inference\nprocedure. A novel dataset, that contains audio-visual training data as well as\na number of scenarios involving several participants engaged in formal and\ninformal dialogue, is introduced. The proposed method is thoroughly tested and\nbenchmarked with respect to several state-of-the art diarization algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2577879", 
    "link": "http://arxiv.org/pdf/1604.00192v1", 
    "title": "Singing Voice Separation and Vocal F0 Estimation based on Mutual   Combination of Robust Principal Component Analysis and Subharmonic Summation", 
    "arxiv-id": "1604.00192v1", 
    "author": "Kazuyoshi Yoshii", 
    "publish": "2016-04-01T10:28:51Z", 
    "summary": "This paper presents a new method of singing voice analysis that performs\nmutually-dependent singing voice separation and vocal fundamental frequency\n(F0) estimation. Vocal F0 estimation is considered to become easier if singing\nvoices can be separated from a music audio signal, and vocal F0 contours are\nuseful for singing voice separation. This calls for an approach that improves\nthe performance of each of these tasks by using the results of the other. The\nproposed method first performs robust principal component analysis (RPCA) for\nroughly extracting singing voices from a target music audio signal. The F0\ncontour of the main melody is then estimated from the separated singing voices\nby finding the optimal temporal path over an F0 saliency spectrogram. Finally,\nthe singing voices are separated again more accurately by combining a\nconventional time-frequency mask given by RPCA with another mask that passes\nonly the harmonic structures of the estimated F0s. Experimental results showed\nthat the proposed method significantly improved the performances of both\nsinging voice separation and vocal F0 estimation. The proposed method also\noutperformed all the other methods of singing voice separation submitted to an\ninternational music analysis competition called MIREX 2014."
},{
    "category": "cs.RO", 
    "doi": "10.1109/ICASSP.2006.1661100", 
    "link": "http://arxiv.org/pdf/1604.01642v1", 
    "title": "Robust 3D Localization and Tracking of Sound Sources Using Beamforming   and Particle Filtering", 
    "arxiv-id": "1604.01642v1", 
    "author": "Jean Rouat", 
    "publish": "2016-02-27T05:07:56Z", 
    "summary": "In this paper we present a new robust sound source localization and tracking\nmethod using an array of eight microphones (US patent pending) . The method\nuses a steered beamformer based on the reliability-weighted phase transform\n(RWPHAT) along with a particle filter-based tracking algorithm. The proposed\nsystem is able to estimate both the direction and the distance of the sources.\nIn a videoconferencing context, the direction was estimated with an accuracy\nbetter than one degree while the distance was accurate within 10% RMS. Tracking\nof up to three simultaneous moving speakers is demonstrated in a noisy\nenvironment."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2006.1661100", 
    "link": "http://arxiv.org/pdf/1604.03276v1", 
    "title": "Noise Robust Speech Recognition Using Multi-Channel Based Channel   Selection And ChannelWeighting", 
    "arxiv-id": "1604.03276v1", 
    "author": "Haizhou Li", 
    "publish": "2016-04-12T07:52:27Z", 
    "summary": "In this paper, we study several microphone channel selection and weighting\nmethods for robust automatic speech recognition (ASR) in noisy conditions. For\nchannel selection, we investigate two methods based on the maximum likelihood\n(ML) criterion and minimum autoencoder reconstruction criterion, respectively.\nFor channel weighting, we produce enhanced log Mel filterbank coefficients as a\nweighted sum of the coefficients of all channels. The weights of the channels\nare estimated by using the ML criterion with constraints. We evaluate the\nproposed methods on the CHiME-3 noisy ASR task. Experiments show that channel\nweighting significantly outperforms channel selection due to its higher\nflexibility. Furthermore, on real test data in which different channels have\ndifferent gains of the target signal, the channel weighting method performs\nequally well or better than the MVDR beamforming, despite the fact that the\nchannel weighting does not make use of the phase delay information which is\nnormally used in beamforming."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1604.04383v3", 
    "title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate   Speech Coding", 
    "arxiv-id": "1604.04383v3", 
    "author": "Philip N. Garner", 
    "publish": "2016-04-15T07:35:00Z", 
    "summary": "Most current very low bit rate (VLBR) speech coding systems use hidden Markov\nmodel (HMM) based speech recognition/synthesis techniques. This allows\ntransmission of information (such as phonemes) segment by segment that\ndecreases the bit rate. However, the encoder based on a phoneme speech\nrecognition may create bursts of segmental errors. Segmental errors are further\npropagated to optional suprasegmental (such as syllable) information coding.\nTogether with the errors of voicing detection in pitch parametrization,\nHMM-based speech coding creates speech discontinuities and unnatural speech\nsound artefacts.\n  In this paper, we propose a novel VLBR speech coding framework based on\nneural networks (NNs) for end-to-end speech analysis and synthesis without\nHMMs. The speech coding framework relies on phonological (sub-phonetic)\nrepresentation of speech, and it is designed as a composition of deep and\nspiking NNs: a bank of phonological analysers at the transmitter, and a\nphonological synthesizer at the receiver, both realised as deep NNs, and a\nspiking NN as an incremental and robust encoder of syllable boundaries for\ncoding of continuous fundamental frequency (F0). A combination of phonological\nfeatures defines much more sound patterns than phonetic features defined by\nHMM-based speech coders, and the finer analysis/synthesis code contributes into\nsmoother encoded speech. Listeners significantly prefer the NN-based approach\ndue to fewer discontinuities and speech artefacts of the encoded speech. A\nsingle forward pass is required during the speech encoding and decoding. The\nproposed VLBR speech coding operates at a bit rate of approximately 360 bits/s."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1604.07160v2", 
    "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic   Event Detection", 
    "arxiv-id": "1604.07160v2", 
    "author": "Luc Van Gool", 
    "publish": "2016-04-25T08:25:03Z", 
    "summary": "We propose a novel method for Acoustic Event Detection (AED). In contrast to\nspeech, sounds coming from acoustic events may be produced by a wide variety of\nsources. Furthermore, distinguishing them often requires analyzing an extended\ntime period due to the lack of a clear sub-word unit. In order to incorporate\nthe long-time frequency structure for AED, we introduce a convolutional neural\nnetwork (CNN) with a large input field. In contrast to previous works, this\nenables to train audio event detection end-to-end. Our architecture is inspired\nby the success of VGGNet and uses small, 3x3 convolutions, but more depth than\nprevious methods in AED. In order to prevent over-fitting and to take full\nadvantage of the modeling capabilities of our network, we further propose a\nnovel data augmentation method to introduce data variation. Experimental\nresults show that our CNN significantly outperforms state of the art methods\nincluding Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%\nabsolute improvement."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1604.08095v1", 
    "title": "Accent Classification with Phonetic Vowel Representation", 
    "arxiv-id": "1604.08095v1", 
    "author": "Aravind Ganapathiraju", 
    "publish": "2016-02-24T02:50:44Z", 
    "summary": "Previous accent classification research focused mainly on detecting accents\nwith pure acoustic information without recognizing accented speech. This work\ncombines phonetic knowledge such as vowels with acoustic information to build\nGuassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)\nfeatures, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With\ninput about 20-second accented speech, this system achieves classification rate\nof 51% on a 7-way classification system focusing on the major types of accents\nin English, which is competitive to the state-of-the-art results in this field."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1604.08723v1", 
    "title": "Music transcription modelling and composition using deep learning", 
    "arxiv-id": "1604.08723v1", 
    "author": "Iryna Korshunova", 
    "publish": "2016-04-29T08:03:00Z", 
    "summary": "We apply deep learning methods, specifically long short-term memory (LSTM)\nnetworks, to music transcription modelling and composition. We build and train\nLSTM networks using approximately 23,000 music transcriptions expressed with a\nhigh-level vocabulary (ABC notation), and use them to generate new\ntranscriptions. Our practical aim is to create music transcription models\nuseful in particular contexts of music composition. We present results from\nthree perspectives: 1) at the population level, comparing descriptive\nstatistics of the set of training transcriptions and generated transcriptions;\n2) at the individual level, examining how a generated transcription reflects\nthe conventions of a music practice in the training transcriptions (Celtic\nfolk); 3) at the application level, using the system for idea generation in\nmusic composition. We make our datasets, software and sound examples open and\navailable: \\url{https://github.com/IraKorshunova/folk-rnn}."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1604.08852v1", 
    "title": "Joint Sound Source Separation and Speaker Recognition", 
    "arxiv-id": "1604.08852v1", 
    "author": "Hugo Van hamme", 
    "publish": "2016-04-29T14:32:03Z", 
    "summary": "Non-negative Matrix Factorization (NMF) has already been applied to learn\nspeaker characterizations from single or non-simultaneous speech for speaker\nrecognition applications. It is also known for its good performance in (blind)\nsource separation for simultaneous speech. This paper explains how NMF can be\nused to jointly solve the two problems in a multichannel speaker recognizer for\nsimultaneous speech. It is shown how state-of-the-art multichannel NMF for\nblind source separation can be easily extended to incorporate speaker\nrecognition. Experiments on the CHiME corpus show that this method outperforms\nthe sequential approach of first applying source separation, followed by\nspeaker recognition that uses state-of-the-art i-vector techniques."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1605.01329v1", 
    "title": "Single Channel Speech Enhancement Using Outlier Detection", 
    "arxiv-id": "1605.01329v1", 
    "author": "Bernard Widrow", 
    "publish": "2016-05-04T16:16:12Z", 
    "summary": "Distortion of the underlying speech is a common problem for single-channel\nspeech enhancement algorithms, and hinders such methods from being used more\nextensively. A dictionary based speech enhancement method that emphasizes\npreserving the underlying speech is proposed. Spectral patches of clean speech\nare sampled and clustered to train a dictionary. Given a noisy speech spectral\npatch, the best matching dictionary entry is selected and used to estimate the\nnoise power at each time-frequency bin. The noise estimation step is formulated\nas an outlier detection problem, where the noise at each bin is assumed present\nonly if it is an outlier to the corresponding bin of the best matching\ndictionary entry. This framework assigns higher priority in removing spectral\nelements that strongly deviate from a typical spoken unit stored in the trained\ndictionary. Even without the aid of a separate noise model, this method can\nachieve significant noise reduction for various non-stationary noises, while\neffectively preserving the underlying speech in more challenging noisy\nenvironments."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1605.01755v1", 
    "title": "DCTNet and PCANet for acoustic signal feature extraction", 
    "arxiv-id": "1605.01755v1", 
    "author": "Loren Nolte", 
    "publish": "2016-04-28T22:21:01Z", 
    "summary": "We introduce the use of DCTNet, an efficient approximation and alternative to\nPCANet, for acoustic signal classification. In PCANet, the eigenfunctions of\nthe local sample covariance matrix (PCA) are used as filterbanks for\nconvolution and feature extraction. When the eigenfunctions are well\napproximated by the Discrete Cosine Transform (DCT) functions, each layer of of\nPCANet and DCTNet is essentially a time-frequency representation. We relate\nDCTNet to spectral feature representation methods, such as the the short time\nFourier transform (STFT), spectrogram and linear frequency spectral\ncoefficients (LFSC). Experimental results on whale vocalization data show that\nDCTNet improves classification rate, demonstrating DCTNet's applicability to\nsignal processing problems such as underwater acoustics."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TASLP.2016.2604566", 
    "link": "http://arxiv.org/pdf/1605.05369v1", 
    "title": "Audio Features Affected by Music Expressiveness", 
    "arxiv-id": "1605.05369v1", 
    "author": "Giuseppe Boccignone", 
    "publish": "2016-05-17T21:10:07Z", 
    "summary": "Within a Music Information Retrieval perspective, the goal of the study\npresented here is to investigate the impact on sound features of the musician's\naffective intention, namely when trying to intentionally convey emotional\ncontents via expressiveness. A preliminary experiment has been performed\ninvolving $10$ tuba players. The recordings have been analysed by extracting a\nvariety of features, which have been subsequently evaluated by combining both\nclassic and machine learning statistical techniques. Results are reported and\ndiscussed."
},{
    "category": "cs.CY", 
    "doi": "10.1109/HealthCom.2015.7454559", 
    "link": "http://arxiv.org/pdf/1605.06238v1", 
    "title": "A Multi-Smartwatch System for Assessing Speech Characteristics of People   with Dysarthria in Group Settings", 
    "arxiv-id": "1605.06238v1", 
    "author": "Leslie Mahler", 
    "publish": "2016-05-20T07:58:07Z", 
    "summary": "Speech-language pathologists (SLPs) frequently use vocal exercises in the\ntreatment of patients with speech disorders. Patients receive treatment in a\nclinical setting and need to practice outside of the clinical setting to\ngeneralize speech goals to functional communication. In this paper, we describe\nthe development of technology that captures mixed speech signals in a group\nsetting and allows the SLP to analyze the speech signals relative to treatment\ngoals. The mixed speech signals are blindly separated into individual signals\nthat are preprocessed before computation of loudness, pitch, shimmer, jitter,\nsemitone standard deviation and sharpness. The proposed method has been\npreviously validated on data obtained from clinical trials of people with\nParkinson disease and healthy controls."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HealthCom.2015.7454559", 
    "link": "http://arxiv.org/pdf/1605.08396v1", 
    "title": "Robust Downbeat Tracking Using an Ensemble of Convolutional Networks", 
    "arxiv-id": "1605.08396v1", 
    "author": "G. Richard", 
    "publish": "2016-05-26T18:27:56Z", 
    "summary": "In this paper, we present a novel state of the art system for automatic\ndownbeat tracking from music signals. The audio signal is first segmented in\nframes which are synchronized at the tatum level of the music. We then extract\ndifferent kind of features based on harmony, melody, rhythm and bass content to\nfeed convolutional neural networks that are adapted to take advantage of each\nfeature characteristics. This ensemble of neural networks is combined to obtain\none downbeat likelihood per tatum. The downbeat sequence is finally decoded\nwith a flexible and efficient temporal model which takes advantage of the\nmetrical continuity of a song. We then perform an evaluation of our system on a\nlarge base of 9 datasets, compare its performance to 4 other published\nalgorithms and obtain a significant increase of 16.8 percent points compared to\nthe second best system, for altogether a moderate cost in test and training.\nThe influence of each step of the method is studied to show its strengths and\nshortcomings."
},{
    "category": "cs.SD", 
    "doi": "10.1109/HealthCom.2015.7454559", 
    "link": "http://arxiv.org/pdf/1606.00298v1", 
    "title": "Automatic tagging using deep convolutional neural networks", 
    "arxiv-id": "1606.00298v1", 
    "author": "Mark Sandler", 
    "publish": "2016-06-01T14:18:08Z", 
    "summary": "We present a content-based automatic music tagging algorithm using fully\nconvolutional neural networks (FCNs). We evaluate different architectures\nconsisting of 2D convolutional layers and subsampling layers only. In the\nexperiments, we measure the AUC-ROC scores of the architectures with different\ncomplexities and input types using the MagnaTagATune dataset, where a 4-layer\narchitecture shows state-of-the-art performance with mel-spectrogram input.\nFurthermore, we evaluated the performances of the architectures with varying\nthe number of layers on a larger dataset (Million Song Dataset), and found that\ndeeper models outperformed the 4-layer architecture. The experiments show that\nmel-spectrogram is an effective time-frequency representation for automatic\ntagging and that more complex models benefit from more training data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.00785v2", 
    "title": "Piano Transcription in the Studio Using an Extensible Alternating   Directions Framework", 
    "arxiv-id": "1606.00785v2", 
    "author": "Mark Sandler", 
    "publish": "2016-06-02T18:15:34Z", 
    "summary": "Given a musical audio recording, the goal of automatic music transcription is\nto determine a score-like representation of the piece underlying the recording.\nDespite significant interest within the research community, several studies\nhave reported on a 'glass ceiling' effect, an apparent limit on the\ntranscription accuracy that current methods seem incapable of overcoming. In\nthis paper, we explore how much this effect can be mitigated by focusing on a\nspecific instrument class and making use of additional information on the\nrecording conditions available in studio or home recording scenarios. In\nparticular, exploiting the availability of single note recordings for the\ninstrument in use we develop a novel signal model employing variable-length\nspectro-temporal patterns as its central building blocks - tailored for pitched\npercussive instruments such as the piano. Temporal dependencies between\nspectral templates are modeled, resembling characteristics of factorial scaled\nhidden Markov models (FS-HMM) and other methods combining Non-Negative Matrix\nFactorization with Markov processes. In contrast to FS-HMMs, our parameter\nestimation is developed in a global, relaxed form within the extensible\nalternating direction method of multipliers (ADMM) framework, which enables the\nsystematic combination of basic regularizers propagating sparsity and local\nstationarity in note activity with more complex regularizers imposing temporal\nsemantics. The proposed method achieves an f-measure of 93-95% for note onsets\non pieces recorded on a Yamaha Disklavier (MAPS DB)."
},{
    "category": "stat.ML", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.01039v2", 
    "title": "Gaussian Processes for Music Audio Modelling and Content Analysis", 
    "arxiv-id": "1606.01039v2", 
    "author": "Dan Stowell", 
    "publish": "2016-06-03T10:45:09Z", 
    "summary": "Real music signals are highly variable, yet they have strong statistical\nstructure. Prior information about the underlying physical mechanisms by which\nsounds are generated and rules by which complex sound structure is constructed\n(notes, chords, a complete musical score), can be naturally unified using\nBayesian modelling techniques. Typically algorithms for Automatic Music\nTranscription independently carry out individual tasks such as multiple-F0\ndetection and beat tracking. The challenge remains to perform joint estimation\nof all parameters. We present a Bayesian approach for modelling music audio,\nand content analysis. The proposed methodology based on Gaussian processes\nseeks joint estimation of multiple music concepts by incorporating into the\nkernel prior information about non-stationary behaviour, dynamics, and rich\nspectral content present in the modelled music signal. We illustrate the\nbenefits of this approach via two tasks: pitch estimation, and inferring\nmissing segments in a polyphonic audio recording."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.02542v1", 
    "title": "Symbolic Music Data Version 1.0", 
    "arxiv-id": "1606.02542v1", 
    "author": "Christian Walder", 
    "publish": "2016-06-08T13:19:01Z", 
    "summary": "In this document, we introduce a new dataset designed for training machine\nlearning models of symbolic music data. Five datasets are provided, one of\nwhich is from a newly collected corpus of 20K midi files. We describe our\npreprocessing and cleaning pipeline, which includes the exclusion of a number\nof files based on scores from a previously developed probabilistic machine\nlearning model. We also define training, testing and validation splits for the\nnew dataset, based on a clustering scheme which we also describe. Some simple\nhistograms are included."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.02816v2", 
    "title": "Audio Content based Geotagging in Multimedia", 
    "arxiv-id": "1606.02816v2", 
    "author": "Bhiksha Raj", 
    "publish": "2016-06-09T04:01:36Z", 
    "summary": "In this paper we propose methods to extract geographically relevant\ninformation in a multimedia recording using its audio. Our method primarily is\nbased on the fact that urban acoustic environment consists of a variety of\nsounds. Hence, location information can be inferred from the composition of\nsound events/classes present in the audio. More specifically, we adopt matrix\nfactorization techniques to obtain semantic content of recording in terms of\ndifferent sound classes. These semantic information are then combined to\nidentify the location of recording."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.04930v1", 
    "title": "Deep Learning for Music", 
    "arxiv-id": "1606.04930v1", 
    "author": "Raymond Wu", 
    "publish": "2016-06-15T19:38:14Z", 
    "summary": "Our goal is to be able to build a generative model from a deep neural network\narchitecture to try to create music that has both harmony and melody and is\npassable as music composed by humans. Previous work in music generation has\nmainly been focused on creating a single melody. More recent work on polyphonic\nmusic modeling, centered around time series probability density estimation, has\nmet some partial success. In particular, there has been a lot of work based off\nof Recurrent Neural Networks combined with Restricted Boltzmann Machines\n(RNN-RBM) and other similar recurrent energy based models. Our approach,\nhowever, is to perform end-to-end learning and generation with deep neural nets\nalone."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.05844v1", 
    "title": "Statistical Parametric Speech Synthesis Using Bottleneck Representation   From Sequence Auto-encoder", 
    "arxiv-id": "1606.05844v1", 
    "author": "Suryakanth V Gangashetty", 
    "publish": "2016-06-19T08:38:26Z", 
    "summary": "In this paper, we describe a statistical parametric speech synthesis approach\nwith unit-level acoustic representation. In conventional deep neural network\nbased speech synthesis, the input text features are repeated for the entire\nduration of phoneme for mapping text and speech parameters. This mapping is\nlearnt at the frame-level which is the de-facto acoustic representation.\nHowever much of this computational requirement can be drastically reduced if\nevery unit can be represented with a fixed-dimensional representation. Using\nrecurrent neural network based auto-encoder, we show that it is indeed possible\nto map units of varying duration to a single vector. We then use this acoustic\nrepresentation at unit-level to synthesize speech using deep neural network\nbased statistical parametric speech synthesis technique. Results show that the\nproposed approach is able to synthesize at the same quality as the conventional\nframe based approach at a highly reduced computational cost."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.06061v2", 
    "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric   Speech Synthesizers for Mobile Devices", 
    "arxiv-id": "1606.06061v2", 
    "author": "Przemys\u0142aw Szczepaniak", 
    "publish": "2016-06-20T10:54:51Z", 
    "summary": "Acoustic models based on long short-term memory recurrent neural networks\n(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and\nshowed significant improvements in naturalness and latency over those based on\nhidden Markov models (HMMs). This paper describes further optimizations of\nLSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,\nmulti-frame inference, and robust inference using an {\\epsilon}-contaminated\nGaussian loss function. Experimental results in subjective listening tests show\nthat these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based\nSPSS in runtime speed while maintaining naturalness. Evaluations between\nLSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also\npresented."
},{
    "category": "cs.SY", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.07729v1", 
    "title": "On Lossless Feedback Delay Networks", 
    "arxiv-id": "1606.07729v1", 
    "author": "Emanuel A. P. Habets", 
    "publish": "2016-06-24T15:51:44Z", 
    "summary": "Lossless Feedback Delay Networks (FDNs) are commonly used as a design\nprototype for artificial reverberation algorithms. The lossless property is\ndependent on the feedback matrix, which connects the output of a set of delays\nto their inputs, and the lengths of the delays. Both, unitary and triangular\nfeedback matrices are known to constitute lossless FDNs, however, the most\ngeneral class of lossless feedback matrices has not been identified. In this\ncontribution, it is shown that the FDN is lossless for any set of delays, if\nall irreducible components of the feedback matrix are diagonally similar to a\nunitary matrix. The necessity of the generalized class of feedback matrices is\ndemonstrated by examples of FDN designs proposed in literature."
},{
    "category": "math.NA", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1606.09178v3", 
    "title": "High-frequency asymptotic compression of dense BEM matrices for general   geometries without ray tracing", 
    "arxiv-id": "1606.09178v3", 
    "author": "Peter Opsomer", 
    "publish": "2016-06-29T16:32:12Z", 
    "summary": "Wave propagation and scattering problems in acoustics, when solved with\nboundary element methods, lead to a discretization matrix that is typically\ndense and large: its size and condition number grow with increasing frequency.\nYet, high frequency scattering problems are intrinsically local in nature.\nAsymptotic methods can be used to reduce the size of the linear system, by\nexplicitly extracting the oscillatory properties from the solution using ray\ntracing or analogous techniques. However, ray tracing becomes expensive or even\nintractable in the presence of (multiple) scattering obstacles with complicated\ngeometries. In this paper, we start from the same discretization that\nconstructs the fully resolved large and dense matrix, and achieve asymptotic\ncompression by explicitly localizing the Green's function instead. This results\nin a large but sparse matrix, with a faster associated matrix-vector product\nand, as numerical experiments indicate, a much improved condition number.\nThough an appropriate localisation of the Green's function also depends on\nasymptotic information unavailable for general geometries, we can construct it\nadaptively in a frequency sweep from small to large frequencies. We show that\nthe approach is robust with respect to non-convex, multiple and even\nnear-trapping domains. Furthermore, in spite of its asymptotic nature, the\nmethod is robust with respect to low-order discretizations such as piecewise\nconstants, linears or cubics, commonly used in applications. On the other hand,\nwe do not decrease the total number of degrees of freedom compared to a\nconventional classical discretization. The combination of the sparsifying\nmodification of the Green's function with other accelerating schemes, such as\nthe fast multipole method, appears possible in principle."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1607.03766v1", 
    "title": "AudioSentibank: Large-scale Semantic Ontology of Acoustic Concepts for   Audio Content Analysis", 
    "arxiv-id": "1607.03766v1", 
    "author": "Andreas Dengel", 
    "publish": "2016-07-13T14:31:54Z", 
    "summary": "Audio carries substantial information about the content of our surroundings.\nThe content has been explored at the semantic level using acoustic concepts,\nbut rarely on concept pairs such as happy crowd and angry crowd. Concept pairs\nconvey unique information and complement other audio and multimedia\napplications. Hence, in this work we explored for the first time the\nclassification's performance of acoustic concepts pairs. For this study, we\nintroduce the AudioSentiBank corpus, which is a large-scale folksology\ncontaining over 1,123 adjective and verb noun pairs. Our contribution consists\non providing the research corpus, the benchmark for classification of acoustic\nconcept pairs, and an analysis on the pairs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1607.04378v1", 
    "title": "DCAR: A Discriminative and Compact Audio Representation to Improve Event   Detection", 
    "arxiv-id": "1607.04378v1", 
    "author": "Gerald Friedland", 
    "publish": "2016-07-15T04:28:14Z", 
    "summary": "This paper presents a novel two-phase method for audio representation,\nDiscriminative and Compact Audio Representation (DCAR), and evaluates its\nperformance at detecting events in consumer-produced videos. In the first phase\nof DCAR, each audio track is modeled using a Gaussian mixture model (GMM) that\nincludes several components to capture the variability within that track. The\nsecond phase takes into account both global structure and local structure. In\nthis phase, the components are rendered more discriminative and compact by\nformulating an optimization problem on Grassmannian manifolds, which we found\nrepresents the structure of audio effectively.\n  Our experiments used the YLI-MED dataset (an open TRECVID-style video corpus\nbased on YFCC100M), which includes ten events. The results show that the\nproposed DCAR representation consistently outperforms state-of-the-art audio\nrepresentations. DCAR's advantage over i-vector, mv-vector, and GMM\nrepresentations is significant for both easier and harder discrimination tasks.\nWe discuss how these performance differences across easy and hard cases follow\nfrom how each type of model leverages (or doesn't leverage) the intrinsic\nstructure of the data. Furthermore, DCAR shows a particularly notable accuracy\nadvantage on events where humans have more difficulty classifying the videos,\ni.e., events with lower mean annotator confidence."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1607.05765v1", 
    "title": "Features and Kernels for Audio Event Recognition", 
    "arxiv-id": "1607.05765v1", 
    "author": "Bhiksha Raj", 
    "publish": "2016-07-19T21:29:03Z", 
    "summary": "One of the most important problems in audio event detection research is\nabsence of benchmark results for comparison with any proposed method. Different\nworks consider different sets of events and datasets which makes it difficult\nto comprehensively analyze any novel method with an existing one. In this paper\nwe propose to establish results for audio event recognition on two recent\npublicly-available datasets. In particular we use Gaussian Mixture model based\nfeature representation and combine them with linear as well as non-linear\nkernel Support Vector Machines."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1608.02272v1", 
    "title": "Incorporation of Speech Duration Information in Score Fusion of Speaker   Recognition Systems", 
    "arxiv-id": "1608.02272v1", 
    "author": "Cenk Demiroglu", 
    "publish": "2016-08-07T21:43:08Z", 
    "summary": "In recent years identity-vector (i-vector) based speaker verification (SV)\nsystems have become very successful. Nevertheless, environmental noise and\nspeech duration variability still have a significant effect on degrading the\nperformance of these systems. In many real-life applications, duration of\nrecordings are very short; as a result, extracted i-vectors cannot reliably\nrepresent the attributes of the speaker. Here, we investigate the effect of\nspeech duration on the performance of three state-of-the-art speaker\nrecognition systems. In addition, using a variety of available score fusion\nmethods, we investigate the effect of score fusion for those speaker\nverification techniques to benefit from the performance difference of different\nmethods under different enrollment and test speech duration conditions. This\ntechnique performed significantly better than the baseline score fusion\nmethods."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1608.03720v1", 
    "title": "Speech Signal Analysis for the Estimation of Heart Rates Under Different   Emotional States", 
    "arxiv-id": "1608.03720v1", 
    "author": "Alex Pappachen James", 
    "publish": "2016-08-12T08:52:23Z", 
    "summary": "A non-invasive method for the monitoring of heart activity can help to reduce\nthe deaths caused by heart disorders such as stroke, arrhythmia and heart\nattack. The human voice can be considered as a biometric data that can be used\nfor estimation of heart rate. In this paper, we propose a method for estimating\nthe heart rate from human speech dynamically using voice signal analysis and by\nthe development of an empirical linear predictor model. The correlation between\nthe voice signal and heart rate are established by classifiers and prediction\nof the heart rates with or without emotions are done using linear models. The\nprediction accuracy was tested using the data collected from 15 subjects, it is\nabout 4050 samples of speech signals and corresponding electrocardiogram\nsamples. The proposed approach can use for early non-invasive detection of\nheart rate changes that can be correlated to an emotional state of the\nindividual and also can be used as a tool for diagnosis of heart conditions in\nreal-time situations."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.03213v1", 
    "title": "Relaxed Binaural LCMV Beamforming", 
    "arxiv-id": "1609.03213v1", 
    "author": "Jesper Jensen", 
    "publish": "2016-09-11T20:36:16Z", 
    "summary": "In this paper we propose a new binaural beamforming technique which can be\nseen as a relaxation of the linearly constrained minimum variance (LCMV)\nframework. The proposed method can achieve simultaneous noise reduction and\nexact binaural cue preservation of the target source, similar to the binaural\nminimum variance distortionless response (BMVDR) method. However, unlike BMVDR,\nthe proposed method is also able to preserve the binaural cues of multiple\ninterferers to a certain predefined accuracy. Specifically, it is able to\ncontrol the trade-off between noise reduction and binaural cue preservation of\nthe interferers by using a separate trade-off parameter per interferer.\nMoreover, we provide a robust way of selecting these trade-off parameters in\nsuch a way that the preservation accuracy for the binaural cues of the\ninterferers is always better than the corresponding ones of the BMVDR. The\nrelaxation of the constraints in the proposed method achieves approximate\nbinaural cue preservation of more interferers than other previously presented\nLCMV-based binaural beamforming methods that use strict equality constraints."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.03499v2", 
    "title": "WaveNet: A Generative Model for Raw Audio", 
    "arxiv-id": "1609.03499v2", 
    "author": "Koray Kavukcuoglu", 
    "publish": "2016-09-12T17:29:40Z", 
    "summary": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.04301v2", 
    "title": "TristouNet: Triplet Loss for Speaker Turn Embedding", 
    "arxiv-id": "1609.04301v2", 
    "author": "Herv\u00e9 Bredin", 
    "publish": "2016-09-14T14:39:36Z", 
    "summary": "TristouNet is a neural network architecture based on Long Short-Term Memory\nrecurrent networks, meant to project speech sequences into a fixed-dimensional\neuclidean space. Thanks to the triplet loss paradigm used for training, the\nresulting sequence embeddings can be compared directly with the euclidean\ndistance, for speaker comparison purposes. Experiments on short (between 500ms\nand 5s) speech turn comparison and speaker change detection show that\nTristouNet brings significant improvements over the current state-of-the-art\ntechniques for both tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.04417v1", 
    "title": "An Adaptive Psychoacoustic Model for Automatic Speech Recognition", 
    "arxiv-id": "1609.04417v1", 
    "author": "Ing Yann Soon", 
    "publish": "2016-09-14T20:02:42Z", 
    "summary": "Compared with automatic speech recognition (ASR), the human auditory system\nis more adept at handling noise-adverse situations, including environmental\nnoise and channel distortion. To mimic this adeptness, auditory models have\nbeen widely incorporated in ASR systems to improve their robustness. This paper\nproposes a novel auditory model which incorporates psychoacoustics and\notoacoustic emissions (OAEs) into ASR. In particular, we successfully implement\nthe frequency-dependent property of psychoacoustic models and effectively\nimprove resulting system performance. We also present a novel double-transform\nspectrum-analysis technique, which can qualitatively predict ASR performance\nfor different noise types. Detailed theoretical analysis is provided to show\nthe effectiveness of the proposed algorithm. Experiments are carried out on the\nAURORA2 database and show that the word recognition rate using our proposed\nfeature extraction method is significantly increased over the baseline. Given\nmodels trained with clean speech, our proposed method achieves up to 85.39%\nword recognition accuracy on noisy data."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.04557v2", 
    "title": "Structured Dropout for Weak Label and Multi-Instance Learning and Its   Application to Score-Informed Source Separation", 
    "arxiv-id": "1609.04557v2", 
    "author": "Mark B. Sandler", 
    "publish": "2016-09-15T09:50:55Z", 
    "summary": "Many success stories involving deep neural networks are instances of\nsupervised learning, where available labels power gradient-based learning\nmethods. Creating such labels, however, can be expensive and thus there is\nincreasing interest in weak labels which only provide coarse information, with\nuncertainty regarding time, location or value. Using such labels often leads to\nconsiderable challenges for the learning process. Current methods for\nweak-label training often employ standard supervised approaches that\nadditionally reassign or prune labels during the learning process. The\ninformation gain, however, is often limited as only the importance of labels\nwhere the network already yields reasonable results is boosted. We propose\ntreating weak-label training as an unsupervised problem and use the labels to\nguide the representation learning to induce structure. To this end, we propose\ntwo autoencoder extensions: class activity penalties and structured dropout. We\ndemonstrate the capabilities of our approach in the context of score-informed\nsource separation of music."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.05104v2", 
    "title": "Intrinsic normalization and extrinsic denormalization of formant data of   vowels", 
    "arxiv-id": "1609.05104v2", 
    "author": "A. G. Ramakrishnan", 
    "publish": "2016-09-16T15:20:32Z", 
    "summary": "Using a known speaker-intrinsic normalization procedure, formant data are\nscaled by the reciprocal of the geometric mean of the first three formant\nfrequencies. This reduces the influence of the talker but results in a\ndistorted vowel space. The proposed speaker-extrinsic procedure re-scales the\nnormalized values by the mean formant values of vowels. When tested on the\nformant data of vowels published by Peterson and Barney, the combined approach\nleads to well separated clusters by reducing the spread due to talkers. The\nproposed procedure performs better than two top-ranked normalization procedures\nbased on the accuracy of vowel classification as the objective measure."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.05152v1", 
    "title": "Style Imitation and Chord Invention in Polyphonic Music with Exponential   Families", 
    "arxiv-id": "1609.05152v1", 
    "author": "Fran\u00e7ois Pachet", 
    "publish": "2016-09-16T17:45:39Z", 
    "summary": "Modeling polyphonic music is a particularly challenging task because of the\nintricate interplay between melody and harmony. A good model should satisfy\nthree requirements: statistical accuracy (capturing faithfully the statistics\nof correlations at various ranges, horizontally and vertically), flexibility\n(coping with arbitrary user constraints), and generalization capacity\n(inventing new material, while staying in the style of the training corpus).\nModels proposed so far fail on at least one of these requirements. We propose a\nstatistical model of polyphonic music, based on the maximum entropy principle.\nThis model is able to learn and reproduce pairwise statistics between\nneighboring note events in a given corpus. The model is also able to invent new\nchords and to harmonize unknown melodies. We evaluate the invention capacity of\nthe model by assessing the amount of cited, re-discovered, and invented chords\non a corpus of Bach chorales. We discuss how the model enables the user to\nspecify and enforce user-defined constraints, which makes it useful for\nstyle-based, interactive music generation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.06404v1", 
    "title": "KU-ISPL Language Recognition System for NIST 2015 i-Vector Machine   Learning Challenge", 
    "arxiv-id": "1609.06404v1", 
    "author": "Hanseok Ko", 
    "publish": "2016-09-21T02:14:23Z", 
    "summary": "In language recognition, the task of rejecting/differentiating closely spaced\nversus acoustically far spaced languages remains a major challenge. For\nconfusable closely spaced languages, the system needs longer input test\nduration material to obtain sufficient information to distinguish between\nlanguages. Alternatively, if languages are distinct and not\nacoustically/linguistically similar to others, duration is not a sufficient\nremedy. The solution proposed here is to explore duration distribution analysis\nfor near/far languages based on the Language Recognition i-Vector Machine\nLearning Challenge 2015 (LRiMLC15) database. Using this knowledge, we propose a\nlikelihood ratio based fusion approach that leveraged both score and duration\ninformation. The experimental results show that the use of duration and score\nfusion improves language recognition performance by 5% relative in LRiMLC15\ncost."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.07245v2", 
    "title": "A New Statistic Feature of the Short-Time Amplitude Spectrum Values for   Human's Unvoiced Pronunciation", 
    "arxiv-id": "1609.07245v2", 
    "author": "Xiaodong Zhuang", 
    "publish": "2016-09-23T07:03:32Z", 
    "summary": "In this paper, a new statistic feature of the discrete short-time amplitude\nspectrum is discovered by experiments for the signals of unvoiced\npronunciation. For the random-varying short-time spectrum, this feature reveals\nthe relationship between the amplitude's average and its standard for every\nfrequency component. On the other hand, the association between the amplitude\ndistributions for different frequency components is also studied. A new model\nrepresenting such association is inspired by the normalized histogram of\namplitude. By mathematical analysis, the new statistic feature discovered is\nproved to be necessary evidence which supports the proposed model, and also can\nbe direct evidence for the widely used hypothesis of \"identical distribution of\namplitude for all frequencies\"."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.07498v1", 
    "title": "Speaker Recognition for Children's Speech", 
    "arxiv-id": "1609.07498v1", 
    "author": "Michael J Carey", 
    "publish": "2016-09-23T20:03:14Z", 
    "summary": "This paper presents results on Speaker Recognition (SR) for children's\nspeech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions\nof the spectrum containing important speaker information for children are\nidentified by conducting SR experiments over 21 frequency bands. As for adults,\nthe spectrum can be split into four regions, with the first (containing primary\nvocal tract resonance information) and third (corresponding to high frequency\nspeech sounds) being most useful for SR. However, the frequencies at which\nthese regions occur are from 11% to 38% higher for children. It is also noted\nthat subband SR rates are lower for younger children. Finally results are\npresented of SR experiments to identify a child in a class (30 children,\nsimilar age) and school (288 children, varying ages). Class performance depends\non age, with accuracy varying from 90% for young children to 99% for older\nchildren. The identification rate achieved for a child in a school is 81%."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.08408v1", 
    "title": "Deep learning for detection of bird vocalisations", 
    "arxiv-id": "1609.08408v1", 
    "author": "Ilyas Potamitis", 
    "publish": "2016-09-25T15:56:06Z", 
    "summary": "This work focuses on reliable detection of bird sound emissions as recorded\nin the open field. Acoustic detection of avian sounds can be used for the\nautomatized monitoring of multiple bird taxa and querying in long-term\nrecordings for species of interest for researchers, conservation practitioners,\nand decision makers. Recordings in the wild can be very noisy due to the\nexposure of the microphones to a large number of audio sources originating from\nall distances and directions, the number and identity of which cannot be known\na-priori. The co-existence of the target vocalizations with abiotic\ninterferences in an unconstrained environment is inefficiently treated by\ncurrent approaches of audio signal enhancement. A technique that would spot\nonly bird vocalization while ignoring other audio sources is of prime\nimportance. These difficulties are tackled in this work, presenting a deep\nautoencoder that maps the audio spectrogram of bird vocalizations to its\ncorresponding binary mask that encircles the spectral blobs of vocalizations\nwhile suppressing other audio sources. The procedure requires minimum human\nattendance, it is very fast during execution, thus suitable to scan massive\nvolumes of data, in order to analyze them, evaluate insights and hypotheses,\nidentify patterns of bird activity that, hopefully, finally lead to design\npolicies on biodiversity issues."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.08433v1", 
    "title": "Local Training for PLDA in Speaker Verification", 
    "arxiv-id": "1609.08433v1", 
    "author": "April Pu", 
    "publish": "2016-09-27T13:37:13Z", 
    "summary": "PLDA is a popular normalization approach for the i-vector model, and it has\ndelivered state-of-the-art performance in speaker verification. However, PLDA\ntraining requires a large amount of labeled development data, which is highly\nexpensive in most cases. A possible approach to mitigate the problem is various\nunsupervised adaptation methods, which use unlabeled data to adapt the PLDA\nscattering matrices to the target domain.\n  In this paper, we present a new `local training' approach that utilizes\ninaccurate but much cheaper local labels to train the PLDA model. These local\nlabels discriminate speakers within a single conversion only, and so are much\neasier to obtain compared to the normal `global labels'. Our experiments show\nthat the proposed approach can deliver significant performance improvement,\nparticularly with limited globally-labeled data."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.08442v1", 
    "title": "Collaborative Learning for Language and Speaker Recognition", 
    "arxiv-id": "1609.08442v1", 
    "author": "Shiyue Zhang", 
    "publish": "2016-09-27T13:48:01Z", 
    "summary": "This paper presents a unified model to perform language and speaker\nrecognition simultaneously and altogether. The model is based on a multi-task\nrecurrent neural network where the output of one task is fed as the input of\nthe other, leading to a collaborative learning framework that can improve both\nlanguage and speaker recognition by borrowing information from each other. Our\nexperiments demonstrated that the multi-task model outperforms the\ntask-specific models on both tasks."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.09743v1", 
    "title": "Rectified binaural ratio: A complex T-distributed feature for robust   sound localization", 
    "arxiv-id": "1609.09743v1", 
    "author": "Florence Forbes", 
    "publish": "2016-09-30T14:15:46Z", 
    "summary": "Most existing methods in binaural sound source localization rely on some kind\nof aggregation of phase-and level-difference cues in the time-frequency plane.\nWhile different ag-gregation schemes exist, they are often heuristic and suffer\nin adverse noise conditions. In this paper, we introduce the rectified binaural\nratio as a new feature for sound source local-ization. We show that for\nGaussian-process point source signals corrupted by stationary Gaussian noise,\nthis ratio follows a complex t-distribution with explicit parameters. This new\nformulation provides a principled and statistically sound way to aggregate\nbinaural features in the presence of noise. We subsequently derive two simple\nand efficient methods for robust relative transfer function and time-delay\nestimation. Experiments on heavily corrupted simulated and speech signals\ndemonstrate the robustness of the proposed scheme."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1609.09744v1", 
    "title": "Phase Unmixing : Multichannel Source Separation with Magnitude   Constraints", 
    "arxiv-id": "1609.09744v1", 
    "author": "Yann Traonmilin", 
    "publish": "2016-09-30T14:17:32Z", 
    "summary": "We consider the problem of estimating the phases of K mixed complex signals\nfrom a multichannel observation, when the mixing matrix and signal magnitudes\nare known. This problem can be cast as a non-convex quadratically constrained\nquadratic program which is known to be NP-hard in general. We propose three\napproaches to tackle it: a heuristic method, an alternate minimization method,\nand a convex relaxation into a semi-definite program. These approaches are\nshowed to outperform the oracle multichannel Wiener filter in under-determined\ninformed source separation tasks, using simulated and speech signals. The\nconvex relaxation approach yields best results, including the potential for\nexact source separation in under-determined settings."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1610.01367v1", 
    "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech   Processing Models", 
    "arxiv-id": "1610.01367v1", 
    "author": "Mohammad Mehdi Homayounpour", 
    "publish": "2016-10-05T11:34:36Z", 
    "summary": "A Pascal challenge entitled monaural multi-talker speech recognition was\ndeveloped, targeting the problem of robust automatic speech recognition against\nspeech like noises which significantly degrades the performance of automatic\nspeech recognition systems. In this challenge, two competing speakers say a\nsimple command simultaneously and the objective is to recognize speech of the\ntarget speaker. Surprisingly during the challenge, a team from IBM research,\ncould achieve a performance better than human listeners on this task. The\nproposed method of the IBM team, consist of an intermediate speech separation\nand then a single-talker speech recognition. This paper reconsiders the task of\nthis challenge based on gain adapted factorial speech processing models. It\ndevelops a joint-token passing algorithm for direct utterance decoding of both\ntarget and masker speakers, simultaneously. Comparing it to the challenge\nwinner, it uses maximum uncertainty during the decoding which cannot be used in\nthe past two-phased method. It provides detailed derivation of inference on\nthese models based on general inference procedures of probabilistic graphical\nmodels. As another improvement, it uses deep neural networks for joint-speaker\nidentification and gain estimation which makes these two steps easier than\nbefore producing competitive results for these steps. The proposed method of\nthis work outperforms past super-human results and even the results were\nachieved recently by Microsoft research, using deep neural networks. It\nachieved 5.5% absolute task performance improvement compared to the first\nsuper-human system and 2.7% absolute task performance improvement compared to\nits recent competitor."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1610.01382v1", 
    "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC   and Random Forest", 
    "arxiv-id": "1610.01382v1", 
    "author": "Sung Wook Baik", 
    "publish": "2016-10-05T12:16:35Z", 
    "summary": "Besides spoken words, speech signals also carry information about speaker\ngender, age, and emotional state which can be used in a variety of speech\nanalysis applications. In this paper, a divide and conquer strategy for\nensemble classification has been proposed to recognize emotions in speech.\nIntrinsic hierarchy in emotions has been utilized to construct an emotions\ntree, which assisted in breaking down the emotion recognition task into smaller\nsub tasks. The proposed framework generates predictions in three phases.\nFirstly, emotions are detected in the input speech signal by classifying it as\nneutral or emotional. If the speech is classified as emotional, then in the\nsecond phase, it is further classified into positive and negative classes.\nFinally, individual positive or negative emotions are identified based on the\noutcomes of the previous stages. Several experiments have been performed on a\nwidely used benchmark dataset. The proposed method was able to achieve improved\nrecognition rates as compared to several other approaches."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1610.03009v1", 
    "title": "Investigation of Synthetic Speech Detection Using Frame- and   Segment-Specific Importance Weighting", 
    "arxiv-id": "1610.03009v1", 
    "author": "Cenk Demiroglu", 
    "publish": "2016-10-10T18:03:29Z", 
    "summary": "Speaker verification systems are vulnerable to spoofing attacks which\npresents a major problem in their real-life deployment. To date, most of the\nproposed synthetic speech detectors (SSDs) have weighted the importance of\ndifferent segments of speech equally. However, different attack methods have\ndifferent strengths and weaknesses and the traces that they leave may be short\nor long term acoustic artifacts. Moreover, those may occur for only particular\nphonemes or sounds. Here, we propose three algorithms that weigh\nlikelihood-ratio scores of individual frames, phonemes, and sound-classes\ndepending on their importance for the SSD. Significant improvement over the\nbaseline system has been obtained for known attack methods that were used in\ntraining the SSDs. However, improvement with unknown attack types was not\nsubstantial. Thus, the type of distortions that were caused by the unknown\nsystems were different and could not be captured better with the proposed SSD\ncompared to the baseline SSD."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1610.03606v1", 
    "title": "Maximum entropy models for generation of expressive music", 
    "arxiv-id": "1610.03606v1", 
    "author": "Fran\u00e7ois Pachet", 
    "publish": "2016-10-12T06:20:22Z", 
    "summary": "In the context of contemporary monophonic music, expression can be seen as\nthe difference between a musical performance and its symbolic representation,\ni.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt)\nmodels can be used to generate musical expression in order to mimic a human\nperformance. As a training corpus, we had a professional pianist play about 150\nmelodies of jazz, pop, and latin jazz. The results show a good predictive\npower, validating the choice of our model. Additionally, we set up a listening\ntest whose results reveal that on average, people significantly prefer the\nmelodies generated by the MaxEnt model than the ones without any expression, or\nwith fully random expression. Furthermore, in some cases, MaxEnt melodies are\nalmost as popular as the human performed ones."
},{
    "category": "stat.ML", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1610.08927v1", 
    "title": "Voice Conversion using Convolutional Neural Networks", 
    "arxiv-id": "1610.08927v1", 
    "author": "Joan Bruna", 
    "publish": "2016-10-27T18:58:06Z", 
    "summary": "The human auditory system is able to distinguish the vocal source of\nthousands of speakers, yet not much is known about what features the auditory\nsystem uses to do this. Fourier Transforms are capable of capturing the pitch\nand harmonic structure of the speaker but this alone proves insufficient at\nidentifying speakers uniquely. The remaining structure, often referred to as\ntimbre, is critical to identifying speakers but we understood little about it.\nIn this paper we use recent advances in neural networks in order to manipulate\nthe voice of one speaker into another by transforming not only the pitch of the\nspeaker, but the timbre. We review generative models built with neural networks\nas well as architectures for creating neural networks that learn analogies. Our\npreliminary results converting voices from one speaker to another are\nencouraging."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.00966v1", 
    "title": "Frame Theory for Signal Processing in Psychoacoustics", 
    "arxiv-id": "1611.00966v1", 
    "author": "Diana Stoeva", 
    "publish": "2016-11-03T12:08:25Z", 
    "summary": "This review chapter aims to strengthen the link between frame theory and\nsignal processing tasks in psychoacoustics. On the one side, the basic concepts\nof frame theory are presented and some proofs are provided to explain those\nconcepts in some detail. The goal is to reveal to hearing scientists how this\nmathematical theory could be relevant for their research. In particular, we\nfocus on frame theory in a filter bank approach, which is probably the most\nrelevant view-point for audio signal processing. On the other side, basic\npsychoacoustic concepts are presented to stimulate mathematicians to apply\ntheir knowledge in this field."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.01783v1", 
    "title": "Domain Adaptation For Formant Estimation Using Deep Learning", 
    "arxiv-id": "1611.01783v1", 
    "author": "Cynthia Clopper", 
    "publish": "2016-11-06T14:00:14Z", 
    "summary": "In this paper we present a domain adaptation technique for formant estimation\nusing a deep network. We first train a deep learning network on a small read\nspeech dataset. We then freeze the parameters of the trained network and use\nseveral different datasets to train an adaptation layer that makes the obtained\nnetwork universal in the sense that it works well for a variety of speakers and\nspeech domains with very different characteristics. We evaluated our adapted\nnetwork on three datasets, each of which has different speaker characteristics\nand speech styles. The performance of our method compares favorably with\nalternative methods for formant estimation."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.02695v1", 
    "title": "Automatic recognition of child speech for robotic applications in noisy   environments", 
    "arxiv-id": "1611.02695v1", 
    "author": "Tony J. Prescott", 
    "publish": "2016-11-08T09:50:30Z", 
    "summary": "Automatic speech recognition (ASR) allows a natural and intuitive interface\nfor robotic educational applications for children. However there are a number\nof challenges to overcome to allow such an interface to operate robustly in\nrealistic settings, including the intrinsic difficulties of recognising child\nspeech and high levels of background noise often present in classrooms. As part\nof the EU EASEL project we have provided several contributions to address these\nchallenges, implementing our own ASR module for use in robotics applications.\nWe used the latest deep neural network algorithms which provide a leap in\nperformance over the traditional GMM approach, and apply data augmentation\nmethods to improve robustness to noise and speaker variation. We provide a\nclose integration between the ASR module and the rest of the dialogue system,\nallowing the ASR to receive in real-time the language models relevant to the\ncurrent section of the dialogue, greatly improving the accuracy. We integrated\nour ASR module into an interactive, multimodal system using a small humanoid\nrobot to help children learn about exercise and energy. The system was\ninstalled at a public museum event as part of a research study where 320\nchildren (aged 3 to 14) interacted with the robot, with our ASR achieving 90%\naccuracy for fluent and near-fluent speech."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.03081v1", 
    "title": "VR 'SPACE OPERA': Mimetic Spectralism in an Immersive Starlight   Audification System", 
    "arxiv-id": "1611.03081v1", 
    "author": "Burak Ulas", 
    "publish": "2016-11-09T09:54:45Z", 
    "summary": "This paper describes a system designed as part of an interactive VR opera,\nwhich immerses a real-time composer and an audience (via a network) in the\nhistorical location of Gobeklitepe, in southern Turkey during an imaginary\nscenario set in the Pre-Pottery Neolithic period (8500-5500 BCE), viewed by\nsome to be the earliest example of a temple, or observatory. In this scene\nmusic is generated, where the harmonic material is determined based on\nobservations of light variation from pulsating stars, that would have\ntheoretically been overhead on the 1st of October 8000 BC at 23:00 and animal\ncalls based on the reliefs in the temple. Based on the observations of the\nstars V465 Per, HD 217860, 16 Lac, BG CVn and KIC 6382916, frequency\ncollections were derived and applied to the generation of musical sound and\nnotation sequences within a custom VR environment using a novel method\nincorporating spectralist techniques. Parameters controlling this 'resynthesis'\ncan be manipulated by the performer using a Leap Motion controller and Oculus\nRift HMD, yielding both sonic and visual results in the environment. The final\nopera is to be viewed via Google Cardboard and delivered over the Internet.\nThis entire process aims to pose questions about real-time composition through\ntime distortion and invoke a sense of wonder and meaningfulness through a\nritualistic experience."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.03477v1", 
    "title": "Song From PI: A Musically Plausible Network for Pop Music Generation", 
    "arxiv-id": "1611.03477v1", 
    "author": "Sanja Fidler", 
    "publish": "2016-11-10T20:35:47Z", 
    "summary": "We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.03533v1", 
    "title": "Landmark-based consonant voicing detection on multilingual corpora", 
    "arxiv-id": "1611.03533v1", 
    "author": "Stefanie Shattuck-Hufnagel", 
    "publish": "2016-11-10T22:11:16Z", 
    "summary": "This paper tests the hypothesis that distinctive feature classifiers anchored\nat phonetic landmarks can be transferred cross-lingually without loss of\naccuracy. Three consonant voicing classifiers were developed: (1) manually\nselected acoustic features anchored at a phonetic landmark, (2) MFCCs (either\naveraged across the segment or anchored at the landmark), and(3) acoustic\nfeatures computed using a convolutional neural network (CNN). All detectors are\ntrained on English data (TIMIT),and tested on English, Turkish, and Spanish\n(performance measured using F1 and accuracy). Experiments demonstrate that\nmanual features outperform all MFCC classifiers, while CNNfeatures outperform\nboth. MFCC-based classifiers suffer an F1reduction of 16% absolute when\ngeneralized from English to other languages. Manual features suffer only a 5%\nF1 reduction,and CNN features actually perform better in Turkish and Span-ish\nthan in the training language, demonstrating that features capable of\nrepresenting long-term spectral dynamics (CNN and landmark-based features) are\nable to generalize cross-lingually with little or no loss of accuracy"
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.07351v1", 
    "title": "MOMOS-MT: Mobile Monophonic System for Music Transcription", 
    "arxiv-id": "1611.07351v1", 
    "author": "Leonard Johard", 
    "publish": "2016-11-22T15:18:31Z", 
    "summary": "Music holds a significant cultural role in social identity and in the\nencouragement of socialization. Technology, by the destruction of physical and\ncultural distance, has lead to many changes in musical themes and the complete\nloss of forms. Yet, it also allows for the preservation and distribution of\nmusic from societies without a history of written sheet music. This paper\npresents early work on a tool for musicians and ethnomusicologists to\ntranscribe sheet music from monophonic voiced pieces for preservation and\ndistribution. Using FFT, the system detects the pitch frequencies, also other\nmethods detect note durations, tempo, time signatures and generates sheet\nmusic. The final system is able to be used in mobile platforms allowing the\nuser to take recordings and produce sheet music in situ to a performance."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.08930v1", 
    "title": "Deep attractor network for single-microphone speaker separation", 
    "arxiv-id": "1611.08930v1", 
    "author": "Nima Mesgarani", 
    "publish": "2016-11-27T22:47:23Z", 
    "summary": "Despite the overwhelming success of deep learning in various speech\nprocessing tasks, the problem of separating simultaneous speakers in a mixture\nremains challenging. Two major difficulties in such systems are the arbitrary\nsource permutation and unknown number of sources in the mixture. We propose a\nnovel deep learning framework for single channel speech separation by creating\nattractor points in high dimensional embedding space of the acoustic signals\nwhich pull together the time-frequency bins corresponding to each source.\nAttractor points in this study are created by finding the centroids of the\nsources in the embedding space, which are subsequently used to determine the\nsimilarity of each bin in the mixture to each source. The network is then\ntrained to minimize the reconstruction error of each source by optimizing the\nembeddings. The proposed model is different from prior works in that it\nimplements an end-to-end training, and it does not depend on the number of\nsources in the mixture. Two strategies are explored in the test time, K-means\nand fixed attractor points, where the latter requires no post-processing and\ncan be implemented in real-time. We evaluated our system on Wall Street Journal\ndataset and show 5.49\\% improvement over the previous state-of-the-art methods."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1611.09526v1", 
    "title": "Learning Filter Banks Using Deep Learning For Acoustic Signals", 
    "arxiv-id": "1611.09526v1", 
    "author": "Samarjit Das", 
    "publish": "2016-11-29T08:46:26Z", 
    "summary": "Designing appropriate features for acoustic event recognition tasks is an\nactive field of research. Expressive features should both improve the\nperformance of the tasks and also be interpret-able. Currently, heuristically\ndesigned features based on the domain knowledge requires tremendous effort in\nhand-crafting, while features extracted through deep network are difficult for\nhuman to interpret. In this work, we explore the experience guided learning\nmethod for designing acoustic features. This is a novel hybrid approach\ncombining both domain knowledge and purely data driven feature designing. Based\non the procedure of log Mel-filter banks, we design a filter bank learning\nlayer. We concatenate this layer with a convolutional neural network (CNN)\nmodel. After training the network, the weight of the filter bank learning layer\nis extracted to facilitate the design of acoustic features. We smooth the\ntrained weight of the learning layer and re-initialize it in filter bank\nlearning layer as audio feature extractor. For the environmental sound\nrecognition task based on the Urban- sound8K dataset, the experience guided\nlearning leads to a 2% accuracy improvement compared with the fixed feature\nextractors (the log Mel-filter bank). The shape of the new filter banks are\nvisualized and explained to prove the effectiveness of the feature design\nprocess."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.00171v1", 
    "title": "A Non Linear Multifractal Study to Illustrate the Evolution of Tagore   Songs Over a Century", 
    "arxiv-id": "1612.00171v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-12-01T08:25:45Z", 
    "summary": "The works of Rabindranath Tagore have been sung by various artistes over\ngenerations spanning over almost 100 years. there are few songs which were\npopular in the early years and have been able to retain their popularity over\nthe years while some others have faded away. In this study we look to find cues\nfor the singing style of these songs which have kept them alive for all these\nyears. For this we took 3 min clip of four Tagore songs which have been sung by\nfive generation of artistes over 100 years and analyze them with the help of\nlatest nonlinear techniques Multifractal Detrended Fluctuation Analysis\n(MFDFA). The multifractal spectral width is a manifestation of the inherent\ncomplexity of the signal and may prove to be an important parameter to identify\nthe singing style of particular generation of singers and how this style varies\nover different generations. The results are discussed in detail."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.00172v1", 
    "title": "A Non Linear Approach towards Automated Emotion Analysis in Hindustani   Music", 
    "arxiv-id": "1612.00172v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-12-01T08:31:23Z", 
    "summary": "In North Indian Classical Music, raga forms the basic structure over which\nindividual improvisations is performed by an artist based on his/her\ncreativity. The Alap is the opening section of a typical Hindustani Music (HM)\nperformance, where the raga is introduced and the paths of its development are\nrevealed using all the notes used in that particular raga and allowed\ntransitions between them with proper distribution over time. In India,\ncorresponding to each raga, several emotional flavors are listed, namely erotic\nlove, pathetic, devotional, comic, horrific, repugnant, heroic, fantastic,\nfurious, peaceful. The detection of emotional cues from Hindustani Classical\nmusic is a demanding task due to the inherent ambiguity present in the\ndifferent ragas, which makes it difficult to identify any particular emotion\nfrom a certain raga. In this study we took the help of a high resolution\nmathematical microscope (MFDFA or Multifractal Detrended Fluctuation Analysis)\nto procure information about the inherent complexities and time series\nfluctuations that constitute an acoustic signal. With the help of this\ntechnique, 3 min alap portion of six conventional ragas of Hindustani classical\nmusic namely, Darbari Kanada, Yaman, Mian ki Malhar, Durga, Jay Jayanti and\nHamswadhani played in three different musical instruments were analyzed. The\nresults are discussed in detail."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.01010v1", 
    "title": "DeepBach: a Steerable Model for Bach chorales generation", 
    "arxiv-id": "1612.01010v1", 
    "author": "Fran\u00e7ois Pachet", 
    "publish": "2016-12-03T19:17:29Z", 
    "summary": "The composition of polyphonic chorale music in the style of J.S Bach has\nrepresented a major challenge in automatic music composition over the last\ndecades. The art of Bach chorales composition involves combining four-part\nharmony with characteristic rhythmic patterns and typical melodic movements to\nproduce musical phrases which begin, evolve and end (cadences) in a harmonious\nway. To our knowledge, no model so far was able to solve all these problems\nsimultaneously using an agnostic machine-learning approach. This paper\nintroduces DeepBach, a statistical model aimed at modeling polyphonic music and\nspecifically four parts, hymn-like pieces. We claim that, after being trained\non the chorale harmonizations by Johann Sebastian Bach, our model is capable of\ngenerating highly convincing chorales in the style of Bach. We evaluate how\nindistinguishable our generated chorales are from existing Bach chorales with a\nlistening test. The results corroborate our claim. A key strength of DeepBach\nis that it is agnostic and flexible. Users can constrain the generation by\nimposing some notes, rhythms or cadences in the generated score. This allows\nusers to reharmonize user-defined melodies. DeepBach's generation is fast,\nmaking it usable for interactive music composition applications. Several\ngeneration examples are provided and discussed from a musical point of view."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.01840v1", 
    "title": "FMA: A Dataset For Music Analysis", 
    "arxiv-id": "1612.01840v1", 
    "author": "Xavier Bresson", 
    "publish": "2016-12-06T14:58:59Z", 
    "summary": "We present a new music dataset that can be used for several music analysis\ntasks. Our major goal is to go beyond the existing limitations of available\nmusic datasets, which are either the small size of datasets with raw audio\ntracks, the availability and legality of the music data, or the lack of\nmeta-data for artists analysis or song ratings for recommender systems.\nExisting datasets such as GTZAN, TagATune, and Million Song suffer from the\nprevious limitations. It is however essential to establish such benchmark\ndatasets to advance the field of music analysis, like the ImageNet dataset\nwhich made possible the large success of deep learning techniques in computer\nvision. In this paper, we introduce the Free Music Archive (FMA) which contains\n77,643 songs and 68 genres spanning 26.9 days of song listening and meta-data\nincluding artist name, song title, music genre, and track counts. For research\npurposes, we define two additional datasets from the original one: a small\ngenre-balanced dataset of 4,000 song data and 10 genres compassing 33.3 hours\nof raw audio and a medium genre-unbalanced dataset of 14,511 data and 20 genres\noffering 5.1 days of track listening, both datasets come with meta-data and\nEchonest audio features. For all datasets, we provide a train-test splitting\nfor future algorithms' comparisons."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.02198v2", 
    "title": "Towards computer-assisted understanding of dynamics in symphonic music", 
    "arxiv-id": "1612.02198v2", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-07T11:18:21Z", 
    "summary": "Many people enjoy classical symphonic music. Its diverse instrumentation\nmakes for a rich listening experience. This diversity adds to the conductor's\nexpressive freedom to shape the sound according to their imagination. As a\nresult, the same piece may sound quite differently from one conductor to\nanother. Differences in interpretation may be noticeable subjectively to\nlisteners, but they are sometimes hard to pinpoint, presumably because of the\nacoustic complexity of the sound. We describe a computational model that\ninterprets dynamics---expressive loudness variations in performances---in terms\nof the musical score, highlighting differences between performances of the same\npiece. We demonstrate experimentally that the model has predictive power, and\ngive examples of conductor ideosyncrasies found by using the model as an\nexplanatory tool. Although the present model is still in active development, it\nmay pave the road for a consumer-oriented companion to interactive classical\nmusic understanding."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.04056v2", 
    "title": "Joint Bayesian Gaussian discriminant analysis for speaker verification", 
    "arxiv-id": "1612.04056v2", 
    "author": "Zhijian Ou", 
    "publish": "2016-12-13T08:13:03Z", 
    "summary": "State-of-the-art i-vector based speaker verification relies on variants of\nProbabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We\nare mainly motivated by the recent work of the joint Bayesian (JB) method,\nwhich is originally proposed for discriminant analysis in face verification. We\napply JB to speaker verification and make three contributions beyond the\noriginal JB. 1) In contrast to the EM iterations with approximated statistics\nin the original JB, the EM iterations with exact statistics are employed and\ngive better performance. 2) We propose to do simultaneous diagonalization (SD)\nof the within-class and between-class covariance matrices to achieve efficient\ntesting, which has broader application scope than the SVD-based efficient\ntesting method in the original JB. 3) We scrutinize similarities and\ndifferences between various Gaussian PLDAs and JB, complementing the previous\nanalysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are\nconducted on NIST SRE10 core condition 5, empirically validating the\nsuperiority of JB with faster convergence rate and 9-13% EER reduction compared\nwith state-of-the-art PLDA."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.04675v1", 
    "title": "Recurrent Deep Stacking Networks for Speech Recognition", 
    "arxiv-id": "1612.04675v1", 
    "author": "Deliang Wang", 
    "publish": "2016-12-14T15:07:51Z", 
    "summary": "This paper presented our work on applying Recurrent Deep Stacking Networks\n(RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we\nalso proposed a more efficient yet comparable substitute to RDSN, Bi- Pass\nStacking Network (BPSN). The main idea of these two models is to add\nphoneme-level information into acoustic models, transforming an acoustic model\nto the combination of an acoustic model and a phoneme-level N-gram model.\nExperiments showed that RDSN and BPsn can substantially improve the\nperformances over conventional DNNs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2593801", 
    "link": "http://arxiv.org/pdf/1612.05065v1", 
    "title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", 
    "arxiv-id": "1612.05065v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-15T14:01:50Z", 
    "summary": "We explore frame-level audio feature learning for chord recognition using\nartificial neural networks. We present the argument that chroma vectors\npotentially hold enough information to model harmonic content of audio for\nchord recognition, but that standard chroma extractors compute too noisy\nfeatures. This leads us to propose a learned chroma feature extractor based on\nartificial neural networks. It is trained to compute chroma features that\nencode harmonic information important for chord recognition, while being robust\nto irrelevant interferences. We achieve this by feeding the network an audio\nspectrum with context instead of a single frame as input. This way, the network\ncan learn to selectively compensate noise and resolve harmonic ambiguities.\n  We compare the resulting features to hand-crafted ones by using a simple\nlinear frame-wise classifier for chord recognition on various data sets. The\nresults show that the learned feature extractor produces superior chroma\nvectors for chord recognition."
},{
    "category": "cs.LG", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.05082v1", 
    "title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", 
    "arxiv-id": "1612.05082v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-15T14:32:20Z", 
    "summary": "Chord recognition systems depend on robust feature extraction pipelines.\nWhile these pipelines are traditionally hand-crafted, recent advances in\nend-to-end machine learning have begun to inspire researchers to explore\ndata-driven methods for such tasks. In this paper, we present a chord\nrecognition system that uses a fully convolutional deep auditory model for\nfeature extraction. The extracted features are processed by a Conditional\nRandom Field that decodes the final chord sequence. Both processing stages are\ntrained automatically and do not require expert knowledge for optimising\nparameters. We show that the learned auditory system extracts musically\ninterpretable features, and that the proposed chord recognition system achieves\nresults on par or better than state-of-the-art algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.05153v1", 
    "title": "On the Potential of Simple Framewise Approaches to Piano Transcription", 
    "arxiv-id": "1612.05153v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-15T17:32:11Z", 
    "summary": "In an attempt at exploring the limitations of simple approaches to the task\nof piano transcription (as usually defined in MIR), we conduct an in-depth\nanalysis of neural network-based framewise transcription. We systematically\ncompare different popular input representations for transcription systems to\ndetermine the ones most suitable for use with neural networks. Exploiting\nrecent advances in training techniques and new regularizers, and taking into\naccount hyper-parameter tuning, we show that it is possible, by simple\nbottom-up frame-wise processing, to obtain a piano transcriber that outperforms\nthe current published state of the art on the publicly available MAPS dataset\n-- without any complex post-processing steps. Thus, we propose this simple\napproach as a new baseline for this dataset, for future transcription research\nto build on and improve."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.05369v1", 
    "title": "Neural networks based EEG-Speech Models", 
    "arxiv-id": "1612.05369v1", 
    "author": "Jun Qin", 
    "publish": "2016-12-16T05:09:14Z", 
    "summary": "In this paper, we describe three neural network (NN) based EEG-Speech (NES)\nmodels that map the unspoken EEG signals to the corresponding phonemes. Instead\nof using conventional feature extraction techniques, the proposed NES models\nrely on graphic learning to project both EEG and speech signals into deep\nrepresentation feature spaces. This NN based linear projection helps to realize\nmultimodal data fusion (i.e., EEG and acoustic signals). It is convenient to\nconstruct the mapping between unspoken EEG signals and phonemes. Specifically,\namong three NES models, two augmented models (i.e., IANES-B and IANES-G)\ninclude spoken EEG signals as either bias or gate information to strengthen the\nfeature learning and translation of unspoken EEG signals. A combined\nunsupervised and supervised training is implemented stepwise to learn the\nmapping for all three NES models. To enhance the computational performance,\nthree way factored NN training technique is applied to IANES-G model. Unlike\nmany existing methods, our augmented NES models incorporate spoken-EEG signals\nthat can efficiently suppress the artifacts in unspoken-EEG signals.\nExperimental results reveal that all three proposed NES models outperform the\nbaseline SVM method, whereas IANES-G demonstrates the best performance on\nspeech recovery and classification task comparatively."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.06287v1", 
    "title": "VAST : The Virtual Acoustic Space Traveler Dataset", 
    "arxiv-id": "1612.06287v1", 
    "author": "Antoine Deleforge", 
    "publish": "2016-12-14T15:40:44Z", 
    "summary": "This paper introduces a new paradigm for sound source lo-calization referred\nto as virtual acoustic space traveling (VAST) and presents a first dataset\ndesigned for this purpose. Existing sound source localization methods are\neither based on an approximate physical model (physics-driven) or on a\nspecific-purpose calibration set (data-driven). With VAST, the idea is to learn\na mapping from audio features to desired audio properties using a massive\ndataset of simulated room impulse responses. This virtual dataset is designed\nto be maximally representative of the potential audio scenes that the\nconsidered system may be evolving in, while remaining reasonably compact. We\nshow that virtually-learned mappings on this dataset generalize to real data,\novercoming some intrinsic limitations of traditional binaural sound\nlocalization methods based on time differences of arrival."
},{
    "category": "cs.CY", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.07608v1", 
    "title": "EchoWear: Smartwatch Technology for Voice and Speech Treatments of   Patients with Parkinson's Disease", 
    "arxiv-id": "1612.07608v1", 
    "author": "Kunal Mankodiya", 
    "publish": "2016-12-21T00:02:08Z", 
    "summary": "About 90 percent of people with Parkinson's disease (PD) experience decreased\nfunctional communication due to the presence of voice and speech disorders\nassociated with dysarthria that can be characterized by monotony of pitch (or\nfundamental frequency), reduced loudness, irregular rate of speech, imprecise\nconsonants, and changes in voice quality. Speech-language pathologists (SLPs)\nwork with patients with PD to improve speech intelligibility using various\nintensive in-clinic speech treatments. SLPs also prescribe home exercises to\nenhance generalization of speech strategies outside of the treatment room. Even\nthough speech therapies are found to be highly effective in improving vocal\nloudness and speech quality, patients with PD find it difficult to follow the\nprescribed exercise regimes outside the clinic and to continue exercises once\nthe treatment is completed. SLPs need techniques to monitor compliance and\naccuracy of their patients exercises at home and in ecologically valid\ncommunication situations. We have designed EchoWear, a smartwatch-based system,\nto remotely monitor speech and voice exercises as prescribed by SLPs. We\nconducted a study of 6 individuals; three with PD and three healthy controls.\nTo assess the performance of EchoWear technology compared with high quality\naudio equipment obtained in a speech laboratory. Our preliminary analysis shows\npromising outcomes for using EchoWear in speech therapies for people with PD.\n  Keywords: Dysarthria; knowledge-based speech processing; Parkinson's disease;\nsmartwatch; speech therapy; wearable system."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.07837v2", 
    "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model", 
    "arxiv-id": "1612.07837v2", 
    "author": "Yoshua Bengio", 
    "publish": "2016-12-22T23:28:47Z", 
    "summary": "In this paper we propose a novel model for unconditional audio generation\nbased on generating one audio sample at a time. We show that our model, which\nprofits from combining memory-less modules, namely autoregressive multilayer\nperceptrons, and stateful recurrent neural networks in a hierarchical structure\nis able to capture underlying sources of variations in the temporal sequences\nover very long time spans, on three datasets of different nature. Human\nevaluation on the generated samples indicate that our model is preferred over\ncompeting models. We also show how each component of the model contributes to\nthe exhibited performance."
},{
    "category": "cs.MM", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1612.08727v1", 
    "title": "Creating A Musical Performance Dataset for Multimodal Music Analysis:   Challenges, Insights, and Applications", 
    "arxiv-id": "1612.08727v1", 
    "author": "Gaurav Sharma", 
    "publish": "2016-12-27T20:27:24Z", 
    "summary": "We introduce a dataset for facilitating audio-visual analysis of musical\nperformances. The dataset comprises a number of simple multi-instrument musical\npieces assembled from coordinated but separately recorded performances of\nindividual tracks. For each piece, we provide the musical score in MIDI format,\nthe audio recordings of the individual tracks, the audio and video recording of\nthe assembled mixture, and ground-truth annotation files including frame-level\nand note-level transcriptions. We anticipate that the dataset will be useful\nfor developing and evaluating multi-modal techniques for music source\nseparation, transcription, score following, and performance analysis. We\ndescribe our methodology for the creation of this dataset, particularly\nhighlighting our approaches for addressing the challenges involved in\nmaintaining synchronization and naturalness. We briefly discuss the research\nquestions that can be investigated with this dataset."
},{
    "category": "cs.CV", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.00495v2", 
    "title": "Vid2speech: Speech Reconstruction from Silent Video", 
    "arxiv-id": "1701.00495v2", 
    "author": "Shmuel Peleg", 
    "publish": "2017-01-02T19:00:22Z", 
    "summary": "Speechreading is a notoriously difficult task for humans to perform. In this\npaper we present an end-to-end model based on a convolutional neural network\n(CNN) for generating an intelligible acoustic speech signal from silent video\nframes of a speaking person. The proposed CNN generates sound features for each\nframe based on its neighboring frames. Waveforms are then synthesized from the\nlearned speech features to produce intelligible speech. We show that by\nleveraging the automatic feature learning capabilities of a CNN, we can obtain\nstate-of-the-art word intelligibility on the GRID dataset, and show promising\nresults for learning out-of-vocabulary (OOV) words."
},{
    "category": "cs.LG", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.03198v1", 
    "title": "Unsupervised Latent Behavior Manifold Learning from Acoustic Features:   audio2behavior", 
    "arxiv-id": "1701.03198v1", 
    "author": "Panayiotis Georgiou", 
    "publish": "2017-01-12T01:02:22Z", 
    "summary": "Behavioral annotation using signal processing and machine learning is highly\ndependent on training data and manual annotations of behavioral labels.\nPrevious studies have shown that speech information encodes significant\nbehavioral information and be used in a variety of automated behavior\nrecognition tasks. However, extracting behavior information from speech is\nstill a difficult task due to the sparseness of training data coupled with the\ncomplex, high-dimensionality of speech, and the complex and multiple\ninformation streams it encodes. In this work we exploit the slow varying\nproperties of human behavior. We hypothesize that nearby segments of speech\nshare the same behavioral context and hence share a similar underlying\nrepresentation in a latent space. Specifically, we propose a Deep Neural\nNetwork (DNN) model to connect behavioral context and derive the behavioral\nmanifold in an unsupervised manner. We evaluate the proposed manifold in the\ncouples therapy domain and also provide examples from publicly available data\n(e.g. stand-up comedy). We further investigate training within the couples'\ntherapy domain and from movie data. The results are extremely encouraging and\npromise improved behavioral quantification in an unsupervised manner and\nwarrants further investigation in a range of applications."
},{
    "category": "cs.MM", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.03274v1", 
    "title": "Investigating the role of musical genre in human perception of music   stretching resistance", 
    "arxiv-id": "1701.03274v1", 
    "author": "Chaokun Wang", 
    "publish": "2017-01-12T09:26:22Z", 
    "summary": "To stretch a music piece to a given length is a common demand in people's\ndaily lives, e.g., in audio-video synchronization and animation production.\nHowever, it is not always guaranteed that the stretched music piece is\nacceptable for general audience since music stretching suffers from people's\nperceptual artefacts. Over-stretching a music piece will make it uncomfortable\nfor human psychoacoustic hearing. The research on music stretching resistance\nattempts to estimate the maximum stretchability of music pieces to further\navoid over-stretch. It has been observed that musical genres can significantly\nimprove the accuracy of automatic estimation of music stretching resistance,\nbut how musical genres are related to music stretching resistance has never\nbeen explained or studied in detail in the literature. In this paper, the\ncharacteristics of music stretching resistance are compared across different\nmusical genres. It is found that music stretching resistance has strong\nintra-genre cohesiveness and inter-genre discrepancies in the experiments.\nMoreover, the ambiguity and the symmetry of music stretching resistance are\nalso observed in the experimental analysis. These findings lead to a new\nmeasurement on the similarity between different musical genres based on their\nmusic stretching resistance. In addition, the analysis of variance (ANOVA) also\nsupports the findings in this paper by verifying the significance of musical\ngenre in shaping music stretching resistance."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.05779v1", 
    "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with   Deep Neural Networks", 
    "arxiv-id": "1701.05779v1", 
    "author": "Hae-Yong Yang", 
    "publish": "2017-01-20T12:48:02Z", 
    "summary": "This work aims to investigate the use of deep neural network to detect\ncommercial hobby drones in real-life environments by analyzing their sound\ndata. The purpose of work is to contribute to a system for detecting drones\nused for malicious purposes, such as for terrorism. Specifically, we present a\nmethod capable of detecting the presence of commercial hobby drones as a binary\nclassification problem based on sound event detection. We recorded the sound\nproduced by a few popular commercial hobby drones, and then augmented this data\nwith diverse environmental sound data to remedy the scarcity of drone sound\ndata in diverse environments. We investigated the effectiveness of\nstate-of-the-art event sound classification methods, i.e., a Gaussian Mixture\nModel (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network\n(RNN), for drone sound detection. Our empirical results, which were obtained\nwith a testing dataset collected on an urban street, confirmed the\neffectiveness of these models for operating in a real environment. In summary,\nour RNN models showed the best detection performance with an F-Score of 0.8009\nwith 240 ms of input audio with a short processing time, indicating their\napplicability to real-time detection systems."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.06078v2", 
    "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics", 
    "arxiv-id": "1701.06078v2", 
    "author": "Kyogu Lee", 
    "publish": "2017-01-21T20:15:08Z", 
    "summary": "Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system."
},{
    "category": "q-bio.NC", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.07138v3", 
    "title": "Learning Mid-Level Auditory Codes from Natural Sound Statistics", 
    "arxiv-id": "1701.07138v3", 
    "author": "Josh H. McDermott", 
    "publish": "2017-01-25T02:00:50Z", 
    "summary": "Interaction with the world requires an organism to transform sensory signals\ninto representations in which behaviorally meaningful properties of the\nenvironment are made explicit. These representations are derived through\ncascades of neuronal processing stages in which neurons at each stage recode\nthe output of preceding stages. Explanations of sensory coding may thus involve\nunderstanding how low-level patterns are combined into more complex structures.\nAlthough models exist in the visual domain to explain how mid-level features\nsuch as junctions and curves might be derived from oriented filters in early\nvisual cortex, little is known about analogous grouping principles for\nmid-level auditory representations. We propose a hierarchical generative model\nof natural sounds that learns combinations of spectrotemporal features from\nnatural stimulus statistics. In the first layer the model forms a sparse\nconvolutional code of spectrograms using a dictionary of learned\nspectrotemporal kernels. To generalize from specific kernel activation\npatterns, the second layer encodes patterns of time-varying magnitude of\nmultiple first layer coefficients. Because second-layer features are sensitive\nto combinations of spectrotemporal features, the representation they support\nencodes more complex acoustic patterns than the first layer. When trained on\ncorpora of speech and environmental sounds, some second-layer units learned to\ngroup spectrotemporal features that occur together in natural sounds. Others\ninstantiate opponency between dissimilar sets of spectrotemporal features. Such\ngroupings might be instantiated by neurons in the auditory cortex, providing a\nhypothesis for mid-level neuronal computation."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.08156v1", 
    "title": "A Comprehensive Survey on Bengali Phoneme Recognition", 
    "arxiv-id": "1701.08156v1", 
    "author": "Marium E Jannat", 
    "publish": "2017-01-27T12:38:47Z", 
    "summary": "Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed."
},{
    "category": "cs.AI", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1701.08343v1", 
    "title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output   HMM for Multiple Voices", 
    "arxiv-id": "1701.08343v1", 
    "author": "Shigeki Sagayama", 
    "publish": "2017-01-29T01:25:57Z", 
    "summary": "In a recent conference paper, we have reported a rhythm transcription method\nbased on a merged-output hidden Markov model (HMM) that explicitly describes\nthe multiple-voice structure of polyphonic music. This model solves a major\nproblem of conventional methods that could not properly describe the nature of\nmultiple voices as in polyrhythmic scores or in the phenomenon of loose\nsynchrony between voices. In this paper we present a complete description of\nthe proposed model and develop an inference technique, which is valid for any\nmerged-output HMMs for which output probabilities depend on past events. We\nalso examine the influence of the architecture and parameters of the method in\nterms of accuracies of rhythm transcription and voice separation and perform\ncomparative evaluations with six other algorithms. Using MIDI recordings of\nclassical piano pieces, we found that the proposed model outperformed other\nmethods by more than 12 points in the accuracy for polyrhythmic performances\nand performed almost as good as the best one for non-polyrhythmic performances.\nThis reveals the state-of-the-art methods of rhythm transcription for the first\ntime in the literature. Publicly available source codes are also provided for\nfuture comparisons."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1702.00178v1", 
    "title": "On the Futility of Learning Complex Frame-Level Language Models for   Chord Recognition", 
    "arxiv-id": "1702.00178v1", 
    "author": "Gerhard Widmer", 
    "publish": "2017-02-01T09:44:44Z", 
    "summary": "Chord recognition systems use temporal models to post-process frame-wise\nchord preditions from acoustic models. Traditionally, first-order models such\nas Hidden Markov Models were used for this task, with recent works suggesting\nto apply Recurrent Neural Networks instead. Due to their ability to learn\nlonger-term dependencies, these models are supposed to learn and to apply\nmusical knowledge, instead of just smoothing the output of the acoustic model.\nIn this paper, we argue that learning complex temporal models at the level of\naudio frames is futile on principle, and that non-Markovian models do not\nperform better than their first-order counterparts. We support our argument\nthrough three experiments on the McGill Billboard dataset. The first two show\n1) that when learning complex temporal models at the frame level, improvements\nin chord sequence modelling are marginal; and 2) that these improvements do not\ntranslate when applied within a full chord recognition system. The third, still\nrather preliminary experiment gives first indications that the use of complex\nsequential models for chord prediction at higher temporal levels might be more\npromising."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1702.00956v2", 
    "title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition   for NIST 2016 Speaker Recognition Evaluation", 
    "arxiv-id": "1702.00956v2", 
    "author": "Hanseok Ko", 
    "publish": "2017-02-03T10:15:29Z", 
    "summary": "Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed\nspeaker recognition system for SRE16 fixed training condition. Data for\nevaluation trials are collected from outside North America, spoken in Tagalog\nand Cantonese while training data only is spoken English. Thus, main issue for\nSRE16 is compensating the discrepancy between different languages. As\ndevelopment dataset which is spoken in Cebuano and Mandarin, we could prepare\nthe evaluation trials through preliminary experiments to compensate the\nlanguage mismatched condition. Our team developed 4 different approaches to\nextract i-vectors and applied state-of-the-art techniques as backend. To\ncompensate language mismatch, we investigated and endeavored unique method such\nas unsupervised language clustering, inter language variability compensation\nand gender/language dependent score normalization."
},{
    "category": "cs.LG", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1702.06286v1", 
    "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event   Detection", 
    "arxiv-id": "1702.06286v1", 
    "author": "Tuomas Virtanen", 
    "publish": "2017-02-21T07:37:59Z", 
    "summary": "Sound events often occur in unstructured environments where they exhibit wide\nvariations in their frequency content and temporal structure. Convolutional\nneural networks (CNN) are able to extract higher level features that are\ninvariant to local spectral and temporal variations. Recurrent neural networks\n(RNNs) are powerful in learning the longer term temporal context in the audio\nsignals. CNNs and RNNs as classifiers have recently shown improved performances\nover established methods in various sound recognition tasks. We combine these\ntwo approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it\non a polyphonic sound event detection task. We compare the performance of the\nproposed CRNN method with CNN, RNN, and other established methods, and observe\na considerable improvement for four different datasets consisting of everyday\nsound events."
},{
    "category": "cs.CL", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1702.07071v1", 
    "title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /\u00e6/,   /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral   Coefficients", 
    "arxiv-id": "1702.07071v1", 
    "author": "Vladimir Vargas-Calder\u00f3n", 
    "publish": "2017-02-23T02:31:03Z", 
    "summary": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study\nmonophthongs pronounced by native English speakers. The objective of this study\nwas to quantitatively measure the extent at which speech recognition methods\ncan distinguish between similar sounding vowels. In particular, the phonemes\n/\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748\nsound files from the corpus were used and subjected to Linear Predictive Coding\n(LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients\n(MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree\nClassifier was used to build a predictive model that learnt the patterns of the\ntwo first formants measured in the data set, as well as the patterns of the 13\ncepstral coefficients. An accuracy of 70\\% was achieved using formants for the\nmentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and\nan accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show\nthat the studied algorithms are far from mimicking the ability of\ndistinguishing subtle differences in sounds like human hearing does."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1702.07713v1", 
    "title": "Multichannel Linear Prediction for Blind Reverberant Audio Source   Separation", 
    "arxiv-id": "1702.07713v1", 
    "author": "Sava\u015fkan Bulek", 
    "publish": "2017-02-24T17:23:01Z", 
    "summary": "A class of methods based on multichannel linear prediction (MCLP) can achieve\neffective blind dereverberation of a source, when the source is observed with a\nmicrophone array. We propose an inventive use of MCLP as a pre-processing step\nfor blind source separation with a microphone array. We show theoretically\nthat, under certain assumptions, such pre-processing reduces the original blind\nreverberant source separation problem to a non-reverberant one, which in turn\ncan be effectively tackled using existing methods. We demonstrate our claims\nusing real recordings obtained with an eight-microphone circular array in\nreverberant environments."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1703.04770v1", 
    "title": "Audio Scene Classification with Deep Recurrent Neural Networks", 
    "arxiv-id": "1703.04770v1", 
    "author": "Alfred Mertins", 
    "publish": "2017-03-14T22:17:49Z", 
    "summary": "We introduce in this work an efficient approach for audio scene\nclassification using deep recurrent neural networks. A scene audio signal is\nfirstly transformed into a sequence of high-level label tree embedding feature\nvectors. The vector sequence is then divided into multiple subsequences on\nwhich a deep GRU-based recurrent neural network is trained for\nsequence-to-label classification. The global predicted label for the entire\nsequence is finally obtained via aggregation of subsequence classification\noutputs. We will show that our approach obtain an F1-score of 97.7% on the\nLITIS Rouen dataset, which is the largest dataset publicly available for the\ntask. Compared to the best previously reported result on the dataset, our\napproach is able to reduce the relative classification error by 35.3%."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1703.04783v1", 
    "title": "Multichannel End-to-end Speech Recognition", 
    "arxiv-id": "1703.04783v1", 
    "author": "John R. Hershey", 
    "publish": "2017-03-14T22:28:51Z", 
    "summary": "The field of speech recognition is in the midst of a paradigm shift:\nend-to-end neural networks are challenging the dominance of hidden Markov\nmodels as a core technology. Using an attention mechanism in a recurrent\nencoder-decoder architecture solves the dynamic time alignment problem,\nallowing joint end-to-end training of the acoustic and language modeling\ncomponents. In this paper we extend the end-to-end framework to encompass\nmicrophone array signal processing for noise suppression and speech enhancement\nwithin the acoustic encoding network. This allows the beamforming components to\nbe optimized jointly within the recognition architecture to improve the\nend-to-end speech recognition objective. Experiments on the noisy speech\nbenchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system\noutperformed the attention-based baseline with input from a conventional\nadaptive beamformer."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/1703.05344v1", 
    "title": "Deducing the severity of psychiatric symptoms from the human voice", 
    "arxiv-id": "1703.05344v1", 
    "author": "Louis-Philippe Morency", 
    "publish": "2017-03-15T18:41:37Z", 
    "summary": "Psychiatric illnesses are often associated with multiple symptoms, whose\nseverity must be graded for accurate diagnosis and treatment. This grading is\nusually done by trained clinicians based on human observations and judgments\nmade within doctor-patient sessions. Current research provides sufficient\nreason to expect that the human voice may carry biomarkers or signatures of\nmany, if not all, these symptoms. Based on this conjecture, we explore the\npossibility of objectively and automatically grading the symptoms of\npsychiatric illnesses with reference to various standard psychiatric rating\nscales. Using acoustic data from several clinician-patient interviews within\nhospital settings, we use non-parametric models to learn and predict the\nrelations between symptom-ratings and voice. In the process, we show that\ndifferent articulatory-phonetic units of speech are able to capture the effects\nof different symptoms differently, and use this to establish a plausible\nmethodology that could be employed for automatically grading psychiatric\nsymptoms for clinical purposes."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/cs/0007006v1", 
    "title": "DISCO: An object-oriented system for music composition and sound design", 
    "arxiv-id": "cs/0007006v1", 
    "author": "Jeff M. Wright", 
    "publish": "2000-07-05T18:20:51Z", 
    "summary": "This paper describes an object-oriented approach to music composition and\nsound design. The approach unifies the processes of music making and instrument\nbuilding by using similar logic, objects, and procedures. The composition\nmodules use an abstract representation of musical data, which can be easily\nmapped onto different synthesis languages or a traditionally notated score. An\nabstract base class is used to derive classes on different time scales. Objects\ncan be related to act across time scales, as well as across an entire piece,\nand relationships between similar objects can replicate traditional music\noperations or introduce new ones. The DISCO (Digital Instrument for\nSonification and Composition) system is an open-ended work in progress."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/cs/0007007v1", 
    "title": "Data sonification and sound visualization", 
    "arxiv-id": "cs/0007007v1", 
    "author": "Elizabeth Wiebel", 
    "publish": "2000-07-05T21:26:48Z", 
    "summary": "This article describes a collaborative project between researchers in the\nMathematics and Computer Science Division at Argonne National Laboratory and\nthe Computer Music Project of the University of Illinois at Urbana-Champaign.\nThe project focuses on the use of sound for the exploration and analysis of\ncomplex data sets in scientific computing. The article addresses digital sound\nsynthesis in the context of DIASS (Digital Instrument for Additive Sound\nSynthesis) and sound visualization in a virtual-reality environment by means of\nM4CAVE. It describes the procedures and preliminary results of some experiments\nin scientific sonification and sound visualization."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/cs/0303025v1", 
    "title": "Algorithmic Clustering of Music", 
    "arxiv-id": "cs/0303025v1", 
    "author": "Ronald de Wolf", 
    "publish": "2003-03-24T16:01:46Z", 
    "summary": "We present a fully automatic method for music classification, based only on\ncompression of strings that represent the music pieces. The method uses no\nbackground knowledge about music whatsoever: it is completely general and can,\nwithout change, be used in different areas like linguistic classification and\ngenomics. It is based on an ideal theory of the information content in\nindividual objects (Kolmogorov complexity), information distance, and a\nuniversal similarity metric. Experiments show that the method distinguishes\nreasonably well between various musical genres and can even cluster pieces by\ncomposer."
},{
    "category": "cs.SD", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/cs/0410027v1", 
    "title": "Detecting User Engagement in Everyday Conversations", 
    "arxiv-id": "cs/0410027v1", 
    "author": "Allison Woodruff", 
    "publish": "2004-10-13T02:28:10Z", 
    "summary": "This paper presents a novel application of speech emotion recognition:\nestimation of the level of conversational engagement between users of a voice\ncommunication system. We begin by using machine learning techniques, such as\nthe support vector machine (SVM), to classify users' emotions as expressed in\nindividual utterances. However, this alone fails to model the temporal and\ninteractive aspects of conversational engagement. We therefore propose the use\nof a multilevel structure based on coupled hidden Markov models (HMM) to\nestimate engagement levels in continuous natural speech. The first level is\ncomprised of SVM-based classifiers that recognize emotional states, which could\nbe (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM\nthen uses these emotional states as input, estimating users' engagement in\nconversation by decoding the internal states of the HMM. We report experimental\nresults obtained by applying our algorithms to the LDC Emotional Prosody and\nCallFriend speech corpora."
},{
    "category": "quant-ph", 
    "doi": "10.1109/MLSP.2016.7738895", 
    "link": "http://arxiv.org/pdf/quant-ph/0309018v1", 
    "title": "Treatment of sound on quantum computers", 
    "arxiv-id": "quant-ph/0309018v1", 
    "author": "Dima Shepelyansky", 
    "publish": "2003-09-01T15:01:27Z", 
    "summary": "We study numerically how a sound signal stored in a quantum computer can be\nrecognized and restored with a minimal number of measurements in presence of\nrandom quantum gate errors. A method developed uses elements of MP3 sound\ncompression and allows to recover human speech and sound of complex quantum\nwavefunctions."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ANZIIS.1994.396943", 
    "link": "http://arxiv.org/pdf/0705.3669v1", 
    "title": "Structural Health Monitoring Using Neural Network Based Vibrational   System Identification", 
    "arxiv-id": "0705.3669v1", 
    "author": "Donald A. Sofge", 
    "publish": "2007-05-24T21:48:18Z", 
    "summary": "Composite fabrication technologies now provide the means for producing\nhigh-strength, low-weight panels, plates, spars and other structural components\nwhich use embedded fiber optic sensors and piezoelectric transducers. These\nmaterials, often referred to as smart structures, make it possible to sense\ninternal characteristics, such as delaminations or structural degradation. In\nthis effort we use neural network based techniques for modeling and analyzing\ndynamic structural information for recognizing structural defects. This yields\nan adaptable system which gives a measure of structural integrity for composite\nstructures."
},{
    "category": "cs.IT", 
    "doi": "10.1109/ANZIIS.1994.396943", 
    "link": "http://arxiv.org/pdf/0707.0514v1", 
    "title": "Phase space methods and psychoacoustic models in lossy transform coding", 
    "arxiv-id": "0707.0514v1", 
    "author": "Matthew Charles Cargo", 
    "publish": "2007-07-03T22:41:11Z", 
    "summary": "I present a method for lossy transform coding of digital audio that uses the\nWeyl symbol calculus for constructing the encoding and decoding transformation.\nThe method establishes a direct connection between a time-frequency\nrepresentation of the signal dependent threshold of masked noise and the\nencode/decode pair. The formalism also offers a time-frequency measure of\nperceptual entropy."
},{
    "category": "cs.NA", 
    "doi": "10.1109/ANZIIS.1994.396943", 
    "link": "http://arxiv.org/pdf/0804.4347v2", 
    "title": "Nonorthogonal Bases and Phase Decomposition: Properties and Applications", 
    "arxiv-id": "0804.4347v2", 
    "author": "Sossio Vergara", 
    "publish": "2008-04-28T08:59:52Z", 
    "summary": "In a previous paper [1] it was discussed the viability of functional analysis\nusing as a basis a couple of generic functions, and hence vectorial\ndecomposition. Here we complete the paradigm exploiting one of the analysis\nmethodologies developed there, but applied to phase coordinates, so needing\nonly one function as a basis. It will be shown that, thanks to the novel\niterative analysis, any function satisfying a rather loose requisite is\nontologically a basis. This in turn generalizes the polar version of the\nFourier theorem to an ample class of nonorthogonal bases. The main advantage of\nthis generalization is that it inherits some of the properties of the original\nFourier theorem. As a result the new transform has a wide range of applications\nand some remarkable consequences. The new tool will be compared with wavelets\nand frames. Examples of analysis and reconstruction of functions using the\ndeveloped algorithms and generic bases will be given. Some of the properties,\nand applications that can promptly benefit from the theory, will be discussed.\nThe implementation of a matched filter for noise suppression will be used as an\nexample of the potential of the theory."
},{
    "category": "physics.soc-ph", 
    "doi": "10.1109/ANZIIS.1994.396943", 
    "link": "http://arxiv.org/pdf/0903.3545v1", 
    "title": "Complexity, time and music", 
    "arxiv-id": "0903.3545v1", 
    "author": "Jean Pierre Boon", 
    "publish": "2009-03-20T15:32:42Z", 
    "summary": "The concept of complexity as considered in terms of its algorithmic\ndefinition proposed by G.J. Chaitin and A.N. Kolmogorov is revisited for the\ndynamical complexity of music. When music pieces are cast in the form of time\nseries of pitch variations, concepts of dynamical systems theory can be used to\ndefine new quantities such as the {\\em dimensionality} as a measure of the {\\em\nglobal temporal dynamics} of a music piece, and the Shanon {\\em entropy} as an\nevaluation of its {\\em local dynamics}. When these quantities are computed\nexplicitly for sequences sampled in the music literature from the 18th to the\n20th century, no indication is found of a systematic increase in complexity\nparalleling historically the evolution of classical western music, but the\nanalysis suggests that the fractional nature of art might have an intrinsic\nvalue of more general significance."
},{
    "category": "physics.data-an", 
    "doi": "10.1088/1367-2630/12/5/053030", 
    "link": "http://arxiv.org/pdf/0911.3842v1", 
    "title": "Musical Genres: Beating to the Rhythms of Different Drums", 
    "arxiv-id": "0911.3842v1", 
    "author": "Luciano da F. Costa", 
    "publish": "2009-11-19T20:29:04Z", 
    "summary": "Online music databases have increased signicantly as a consequence of the\nrapid growth of the Internet and digital audio, requiring the development of\nfaster and more efficient tools for music content analysis. Musical genres are\nwidely used to organize music collections. In this paper, the problem of\nautomatic music genre classification is addressed by exploring rhythm-based\nfeatures obtained from a respective complex network representation. A Markov\nmodel is build in order to analyse the temporal sequence of rhythmic notation\nevents. Feature analysis is performed by using two multivariate statistical\napproaches: principal component analysis(unsupervised) and linear discriminant\nanalysis (supervised). Similarly, two classifiers are applied in order to\nidentify the category of rhythms: parametric Bayesian classifier under gaussian\nhypothesis (supervised), and agglomerative hierarchical clustering\n(unsupervised). Qualitative results obtained by Kappa coefficient and the\nobtained clusters corroborated the effectiveness of the proposed method."
},{
    "category": "cs.SD", 
    "doi": "10.1088/1367-2630/12/5/053030", 
    "link": "http://arxiv.org/pdf/0911.4642v1", 
    "title": "G3 : GENESIS software envrionment update", 
    "arxiv-id": "0911.4642v1", 
    "author": "Olivier Michel Tache", 
    "publish": "2009-11-24T15:07:37Z", 
    "summary": "GENESIS3 is the new version of the GENESIS software environment for musical\ncreation by means of mass-interaction physics network modeling. It was\ndesigned, and developed from scratch, in hindsight of more than 10 years\nworking on and using the previous version. We take the opportunity of this\nbirth to provide in this article (1) an analysis of the peculiarities in\nGENESIS, aiming at highlighting its core ?software paradigm?; and (2) an update\non the features of the new version as compared to the last."
},{
    "category": "math.OC", 
    "doi": "10.1088/1367-2630/12/5/053030", 
    "link": "http://arxiv.org/pdf/1001.3217v1", 
    "title": "Optimal control theory : a method for the design of wind instruments", 
    "arxiv-id": "1001.3217v1", 
    "author": "Georges Le Vey", 
    "publish": "2010-01-19T07:59:54Z", 
    "summary": "It has been asserted previously by the author that optimal control theory can\nbe a valuable framework for theoretical studies about the shape that a wind\ninstrument should have in order to satisfy some optimization criterion, inside\na fairly general class. The purpose of the present work is to develop this new\napproach with a look at a specific criterion to be optimized. In this setting,\nthe Webster horn equation is regarded as a controlled dynamical equation in the\nspace variable. Pressure is the state, the control being made of two parts: one\nvariable part, the inside diameter of the duct and one constant part, the\nweights of the elementary time-harmonic components of the velocity potential.\nThen one looks for a control that optimizes a criterion related to the\ndefinition of an {oscillation regime} as the cooperation of several natural\nmodes of vibration with the excitation, the {playing frequency} being the one\nthat maximizes the total generation of energy, as exposed by A.H. Benade,\nfollowing H. Bouasse. At the same time the relevance of this criterion is\nquestioned with the simulation results."
},{
    "category": "cs.IR", 
    "doi": "10.1088/1367-2630/12/5/053030", 
    "link": "http://arxiv.org/pdf/1004.4464v1", 
    "title": "Audio enabled information extraction system for cricket and hockey   domains", 
    "arxiv-id": "1004.4464v1", 
    "author": "Suresh Reddy. S", 
    "publish": "2010-04-26T10:11:00Z", 
    "summary": "The proposed system aims at the retrieval of the summarized information from\nthe documents collected from web based search engine as per the user query\nrelated to cricket and hockey domain. The system is designed in a manner that\nit takes the voice commands as keywords for search. The parts of speech in the\nquery are extracted using the natural language extractor for English. Based on\nthe keywords the search is categorized into 2 types: - 1.Concept wise -\ninformation retrieved to the query is retrieved based on the keywords and the\nconcept words related to it. The retrieved information is summarized using the\nprobabilistic approach and weighted means algorithm.2.Keyword search - extracts\nthe result relevant to the query from the highly ranked document retrieved from\nthe search by the search engine. The relevant search results are retrieved and\nthen keywords are used for summarizing part. During summarization it follows\nthe weighted and probabilistic approaches in order to identify the data\ncomparable to the keywords extracted. The extracted information is then refined\nrepeatedly through the aggregation process to reduce redundancy. Finally the\nresultant data is submitted to the user in the form of audio output."
},{
    "category": "cs.HC", 
    "doi": "10.1088/1367-2630/12/5/053030", 
    "link": "http://arxiv.org/pdf/1005.4564v1", 
    "title": "A basic gesture and motion format for virtual reality multisensory   applications", 
    "arxiv-id": "1005.4564v1", 
    "author": "Jean-Loup Florens", 
    "publish": "2010-05-25T13:16:29Z", 
    "summary": "The question of encoding movements such as those produced by human gestures\nmay become central in the coming years, given the growing importance of\nmovement data exchanges between heterogeneous systems and applications (musical\napplications, 3D motion control, virtual reality interaction, etc.). For the\npast 20 years, various formats have been proposed for encoding movement,\nespecially gestures. Though, these formats, at different degrees, were designed\nin the context of quite specific applications (character animation, motion\ncapture, musical gesture, biomechanical concerns...). The article introduce a\nnew file format, called GMS (for 'Gesture and Motion Signal'), with the aim of\nbeing more low-level and generic, by defining the minimal features a format\ncarrying movement/gesture information needs, rather than by gathering all the\ninformation generally given by the existing formats. The article argues that,\ngiven its growing presence in virtual reality situations, the \"gesture signal\"\nitself must be encoded, and that a specific format is needed. The proposed\nformat features the inner properties of such signals: dimensionality,\nstructural features, types of variables, and spatial and temporal properties.\nThe article first reviews the various situations with multisensory virtual\nobjects in which gesture controls intervene. The proposed format is then\ndeduced, as a mean to encode such versatile and variable \"gestural and animated\nscene\"."
},{
    "category": "physics.data-an", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1012.0142v1", 
    "title": "Universal patterns in sound amplitudes of songs and music genres", 
    "arxiv-id": "1012.0142v1", 
    "author": "E. K. Lenzi", 
    "publish": "2010-12-01T10:07:42Z", 
    "summary": "We report a statistical analysis over more than eight thousand songs.\nSpecifically, we investigate the probability distribution of the normalized\nsound amplitudes. Our findings seems to suggest a universal form of\ndistribution which presents a good agreement with a one-parameter stretched\nGaussian. We also argue that this parameter can give information on music\ncomplexity, and consequently it goes towards classifying songs as well as music\ngenres. Additionally, we present statistical evidences that correlation aspects\nof the songs are directly related with the non-Gaussian nature of their sound\namplitude distributions."
},{
    "category": "cs.LG", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1103.2832v1", 
    "title": "Autotagging music with conditional restricted Boltzmann machines", 
    "arxiv-id": "1103.2832v1", 
    "author": "Yoshua Bengio", 
    "publish": "2011-03-15T02:39:31Z", 
    "summary": "This paper describes two applications of conditional restricted Boltzmann\nmachines (CRBMs) to the task of autotagging music. The first consists of\ntraining a CRBM to predict tags that a user would apply to a clip of a song\nbased on tags already applied by other users. By learning the relationships\nbetween tags, this model is able to pre-process training data to significantly\nimprove the performance of a support vector machine (SVM) autotagging. The\nsecond is the use of a discriminative RBM, a type of CRBM, to autotag music. By\nsimultaneously exploiting the relationships among tags and between tags and\naudio-based features, this model is able to significantly outperform SVMs,\nlogistic regression, and multi-layer perceptrons. In order to be applied to\nthis problem, the discriminative RBM was generalized to the multi-label setting\nand four different learning algorithms for it were evaluated, the first such\nin-depth analysis of which we are aware."
},{
    "category": "cs.SD", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1106.0760v1", 
    "title": "Simulating the Electroweak Phase Transition: Sonification of Bubble   Nucleation", 
    "arxiv-id": "1106.0760v1", 
    "author": "Deva O'Neil", 
    "publish": "2011-06-03T20:22:03Z", 
    "summary": "As an applicaton of sonification, a simulation of the early universe was\ndeveloped to portray a phase transition that occurred shortly after the Big\nBang. The Standard Model of particle physics postulates that a hypothetical\nparticle, the Higgs boson, is responsible for the breaking of the symmetry\nbetween the electromagnetic force and the weak force. This phase transition may\nhave been responsible for triggering Baryogenesis, the generation of an\nabundance of matter over anti-matter. This hypothesis is known as Electroweak\nBaryogenesis. In this simulation, aspects of bubble nucleation in Standard\nModel Electroweak Baryogenesis were examined and modeled using Mathematica, and\nsonified using SuperCollider3. The resulting simulation, which has been used\nfor pedagogical purposes by one of the authors, suggests interesting\npossibilities for the integration of science and aesthetics as well as auditory\nperception. The sonification component in particular also had the unexpected\nbenefit of being useful in debugging the Mathematica code."
},{
    "category": "cs.IT", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1106.0843v1", 
    "title": "A Novel Adaptive Channel Equalization Method Using Variable Step-Size   Partial Rank Algorithm", 
    "arxiv-id": "1106.0843v1", 
    "author": "Paeiz Azmi", 
    "publish": "2011-06-04T17:58:59Z", 
    "summary": "Recently a framework has been introduced within which a large number of\nclassical and modern adaptive filter algorithms can be viewed as special cases.\nVariable Step-Size (VSS) normalized least mean square (VSSNLMS) and VSS Affine\nProjection Algorithms (VSSAPA) are two particular examples of the adaptive\nalgorithms that can be covered by this generic adaptive filter. In this paper,\nwe introduce a new VSS Partial Rank (VSSPR) adaptive algorithm based on the\ngeneric VSS adaptive filter and use it for channel equalization. The proposed\nalgorithm performs very well in attenuating noise and inter-symbol interference\n(ISI) in comparison with the standard NLMS and the recently introduced AP\nalgorithms."
},{
    "category": "physics.data-an", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1106.2902v1", 
    "title": "Computational approach to multifractal music", 
    "arxiv-id": "1106.2902v1", 
    "author": "Rafa\u0142 Rak", 
    "publish": "2011-06-15T08:22:32Z", 
    "summary": "In this work we perform a fractal analysis of 160 pieces of music belonging\nto six different genres. We show that the majority of the pieces reveal\ncharacteristics that allow us to classify them as physical processes called the\n1/f (pink) noise. However, this is not true for classical music represented\nhere by Frederic Chopin's works and for some jazz pieces that are much more\ncorrelated than the pink noise. We also perform a multifractal (MFDFA) analysis\nof these music pieces. We show that all the pieces reveal multifractal\nproperties. The richest multifractal structures are observed for pop and rock\nmusic. Also the viariably of multifractal features is best visible for popular\nmusic genres. This can suggest that, from the multifractal perspective,\nclassical and jazz music is much more uniform than pieces of the most popular\ngenres of music."
},{
    "category": "cs.SD", 
    "doi": "10.1103/PhysRevE.83.017101", 
    "link": "http://arxiv.org/pdf/1107.4969v1", 
    "title": "An end-to-end machine learning system for harmonic analysis of music", 
    "arxiv-id": "1107.4969v1", 
    "author": "Tijl De Bie", 
    "publish": "2011-07-25T15:09:37Z", 
    "summary": "We present a new system for simultaneous estimation of keys, chords, and bass\nnotes from music audio. It makes use of a novel chromagram representation of\naudio that takes perception of loudness into account. Furthermore, it is fully\nbased on machine learning (instead of expert knowledge), such that it is\npotentially applicable to a wider range of genres as long as training data is\navailable. As compared to other models, the proposed system is fast and memory\nefficient, while achieving state-of-the-art performance."
},{
    "category": "physics.soc-ph", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1112.2316v1", 
    "title": "Complexity-entropy causality plane: a useful approach for distinguishing   songs", 
    "arxiv-id": "1112.2316v1", 
    "author": "E. K. Lenzi", 
    "publish": "2011-12-11T03:38:44Z", 
    "summary": "Nowadays we are often faced with huge databases resulting from the rapid\ngrowth of data storage technologies. This is particularly true when dealing\nwith music databases. In this context, it is essential to have techniques and\ntools able to discriminate properties from these massive sets. In this work, we\nreport on a statistical analysis of more than ten thousand songs aiming to\nobtain a complexity hierarchy. Our approach is based on the estimation of the\npermutation entropy combined with an intensive complexity measure, building up\nthe complexity-entropy causality plane. The results obtained indicate that this\nrepresentation space is very promising to discriminate songs as well as to\nallow a relative quantitative comparison among songs. Additionally, we believe\nthat the here-reported method may be applied in practical situations since it\nis simple, robust and has a fast numerical implementation."
},{
    "category": "cs.HC", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1201.6251v1", 
    "title": "Real-time jam-session support system", 
    "arxiv-id": "1201.6251v1", 
    "author": "Panagiotis Tigas", 
    "publish": "2012-01-27T18:30:11Z", 
    "summary": "We propose a method for the problem of real time chord accompaniment of\nimprovised music. Our implementation can learn an underlying structure of the\nmusical performance and predict next chord. The system uses Hidden Markov Model\nto find the most probable chord sequence for the played melody and then a\nVariable Order Markov Model is used to a) learn the structure (if any) and b)\npredict next chord. We implemented our system in Java and MAX/Msp and compared\nand evaluated using objective (prediction accuracy) and subjective\n(questionnaire) evaluation methods."
},{
    "category": "physics.class-ph", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1203.5101v2", 
    "title": "Entropy-based Tuning of Musical Instruments", 
    "arxiv-id": "1203.5101v2", 
    "author": "Haye Hinrichsen", 
    "publish": "2012-03-22T07:21:53Z", 
    "summary": "The human sense of hearing perceives a combination of sounds 'in tune' if the\ncorresponding harmonic spectra are correlated, meaning that the neuronal\nexcitation pattern in the inner ear exhibits some kind of order. Based on this\nobservation it is suggested that musical instruments such as pianos can be\ntuned by minimizing the Shannon entropy of suitably preprocessed Fourier\nspectra. This method reproduces not only the correct stretch curve but also\nsimilar pitch fluctuations as in the case of high-quality aural tuning."
},{
    "category": "cs.NI", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1205.4361v1", 
    "title": "Transference & Retrieval of Pulse-code modulation Audio over Short   Messaging Service", 
    "arxiv-id": "1205.4361v1", 
    "author": "Saira Beg", 
    "publish": "2012-05-19T21:25:17Z", 
    "summary": "The paper presents the method of transferring PCM (Pulse-Code Modulation)\nbased audio messages through SMS (Short Message Service) over GSM (Global\nSystem for Mobile Communications) network. As SMS is text based service, and\ncould not send voice. Our method enables voice transferring through SMS, by\nconverting PCM audio into characters. Than Huffman coding compression technique\nis applied in order to reduce numbers of characters which will latterly set as\npayload text of SMS. Testing the said method we develop an application using\nJ2me platform"
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1206.6392v1", 
    "title": "Modeling Temporal Dependencies in High-Dimensional Sequences:   Application to Polyphonic Music Generation and Transcription", 
    "arxiv-id": "1206.6392v1", 
    "author": "Pascal Vincent", 
    "publish": "2012-06-27T19:59:59Z", 
    "summary": "We investigate the problem of modeling symbolic sequences of polyphonic music\nin a completely general piano-roll representation. We introduce a probabilistic\nmodel based on distribution estimators conditioned on a recurrent neural\nnetwork that is able to discover temporal dependencies in high-dimensional\nsequences. Our approach outperforms many traditional models of polyphonic music\non a variety of realistic datasets. We show how our musical language model can\nserve as a symbolic prior to improve the accuracy of polyphonic transcription."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.physa.2011.12.009", 
    "link": "http://arxiv.org/pdf/1206.6468v1", 
    "title": "Variational Inference in Non-negative Factorial Hidden Markov Models for   Efficient Audio Source Separation", 
    "arxiv-id": "1206.6468v1", 
    "author": "Maneesh Sahani", 
    "publish": "2012-06-27T19:59:59Z", 
    "summary": "The past decade has seen substantial work on the use of non-negative matrix\nfactorization and its probabilistic counterparts for audio source separation.\nAlthough able to capture audio spectral structure well, these models neglect\nthe non-stationarity and temporal dynamics that are important properties of\naudio. The recently proposed non-negative factorial hidden Markov model\n(N-FHMM) introduces a temporal dimension and improves source separation\nperformance. However, the factorial nature of this model makes the complexity\nof inference exponential in the number of sound sources. Here, we present a\nBayesian variant of the N-FHMM suited to an efficient variational inference\nalgorithm, whose complexity is linear in the number of sound sources. Our\nalgorithm performs comparably to exact inference in the original N-FHMM but is\nsignificantly faster. In typical configurations of the N-FHMM, our method\nachieves around a 30x increase in speed."
},{
    "category": "cs.CV", 
    "doi": "10.5121/csit.2012.2311", 
    "link": "http://arxiv.org/pdf/1208.1880v1", 
    "title": "Stereo Acoustic Perception based on Real Time Video Acquisition for   Navigational Assistance", 
    "arxiv-id": "1208.1880v1", 
    "author": "Rajeshwari Hegde", 
    "publish": "2012-08-09T11:46:10Z", 
    "summary": "A smart navigation system (an Electronic Travel Aid) based on an object\ndetection mechanism has been designed to detect the presence of obstacles that\nimmediately impede the path, by means of real time video processing. The\nalgorithm can be used for any general purpose navigational aid. This paper is\ndiscussed, keeping in mind the navigation of the visually impaired, and is not\nlimited to the same. A video camera feeds images of the surroundings to a Da-\nVinci Digital Media Processor, DM642, which works on the video, frame by frame.\nThe processor carries out image processing techniques whose result contains\ninformation about the object in terms of image pixels. The algorithm aims to\nselect the object which, among all others, poses maximum threat to the\nnavigation. A database containing a total of three sounds is constructed.\nHence, each image translates to a beep, where every beep informs the navigator\nof the obstacles directly in front of him. This paper implements an algorithm\nthat is more efficient as compared to its predecessors."
},{
    "category": "cs.IT", 
    "doi": "10.1109/TSP.2013.2284479", 
    "link": "http://arxiv.org/pdf/1208.5919v1", 
    "title": "The Stationary Phase Approximation, Time-Frequency Decomposition and   Auditory Processing", 
    "arxiv-id": "1208.5919v1", 
    "author": "Bernard Mulgrew", 
    "publish": "2012-08-29T13:58:51Z", 
    "summary": "The principle of stationary phase (PSP) is re-examined in the context of\nlinear time-frequency (TF) decomposition using Gaussian, gammatone and\ngammachirp filters at uniform, logarithmic and cochlear spacings in frequency.\nThis necessitates consideration of the use the PSP on non-asymptotic integrals\nand leads to the introduction of a test for phase rate dominance. Regions of\nthe TF plane that pass the test and don't contain stationary phase points\ncontribute little or nothing to the final output. Analysis values that lie in\nthese regions can thus be set to zero, i.e. sparsity. In regions of the TF\nplane that fail the test or are in the vicinity of stationary phase points,\nsynthesis is performed in the usual way. A new interpretation of the location\nparameters associated with the synthesis filters leads to: (i) a new method for\nlocating stationary phase points in the TF plane; (ii) a test for phase rate\ndominance in that plane. Together this is a TF stationary phase approximation\n(TFSFA) for both analysis and synthesis. The stationary phase regions of\nseveral elementary signals are identified theoretically and examples of\nreconstruction given. An analysis of the TF phase rate characteristics for the\ncase of two simultaneous tones predicts and quantifies a form of simultaneous\nmasking similar to that which characterizes the auditory system."
},{
    "category": "cs.HC", 
    "doi": "10.1109/TSP.2013.2284479", 
    "link": "http://arxiv.org/pdf/1210.2945v1", 
    "title": "The Spatial Real and Virtual Sound Stimuli Optimization for the Auditory   BCI", 
    "arxiv-id": "1210.2945v1", 
    "author": "Tomasz M. Rutkowski", 
    "publish": "2012-10-10T15:00:00Z", 
    "summary": "The paper presents results from a project aiming to create horizontally\ndistributed surround sound sources and virtual sound images as auditory BCI\n(aBCI) stimuli. The purpose is to create evoked brain wave response patterns\ndepending on attended or ignored sound directions. We propose to use a modified\nversion of the vector based amplitude panning (VBAP) approach to achieve the\ngoal. The so created spatial sound stimulus system for the novel oddball aBCI\nparadigm allows us to create a multi-command experimental environment with very\nencouraging results reported in this paper. We also present results showing\nthat a modulation of the sound image depth changes also the subject responses.\nFinally, we also compare the proposed virtual sound approach with the\ntraditional one based on real sound sources generated from the real loudspeaker\ndirections. The so obtained results confirm the hypothesis of the possibility\nto modulate independently the brain responses to spatial types and depths of\nsound sources which allows for the development of the novel multi-command aBCI."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ACSSC.2013.6810587", 
    "link": "http://arxiv.org/pdf/1212.0451v2", 
    "title": "Semi-blind Source Separation via Sparse Representations and Online   Dictionary Learning", 
    "arxiv-id": "1212.0451v2", 
    "author": "Jarvis D. Haupt", 
    "publish": "2012-12-03T17:06:41Z", 
    "summary": "This work examines a semi-blind single-channel source separation problem. Our\nspecific aim is to separate one source whose local structure is approximately\nknown, from another a priori unspecified background source, given only a single\nlinear combination of the two sources. We propose a separation technique based\non local sparse approximations along the lines of recent efforts in sparse\nrepresentations and dictionary learning. A key feature of our procedure is the\nonline learning of dictionaries (using only the data itself) to sparsely model\nthe background source, which facilitates its separation from the\npartially-known source. Our approach is applicable to source separation\nproblems in various application domains; here, we demonstrate the performance\nof our proposed approach via simulation on a stylized audio source separation\ntask."
},{
    "category": "cs.ET", 
    "doi": "10.1109/ACSSC.2013.6810587", 
    "link": "http://arxiv.org/pdf/1302.0785v1", 
    "title": "Beyond Markov Chains, Towards Adaptive Memristor Network-based Music   Generation", 
    "arxiv-id": "1302.0785v1", 
    "author": "Andrew Adamatzky", 
    "publish": "2013-02-04T18:26:03Z", 
    "summary": "We undertook a study of the use of a memristor network for music generation,\nmaking use of the memristor's memory to go beyond the Markov hypothesis. Seed\ntransition matrices are created and populated using memristor equations, and\nwhich are shown to generate musical melodies and change in style over time as a\nresult of feedback into the transition matrix. The spiking properties of simple\nmemristor networks are demonstrated and discussed with reference to\napplications of music making. The limitations of simulating composing memristor\nnetworks in von Neumann hardware is discussed and a hardware solution based on\nphysical memristor properties is presented."
},{
    "category": "cs.LG", 
    "doi": "10.5120/10089-4722", 
    "link": "http://arxiv.org/pdf/1302.1772v1", 
    "title": "An ANN-based Method for Detecting Vocal Fold Pathology", 
    "arxiv-id": "1302.1772v1", 
    "author": "Igor Kheidorov", 
    "publish": "2013-02-07T15:03:24Z", 
    "summary": "There are different algorithms for vocal fold pathology diagnosis. These\nalgorithms usually have three stages which are Feature Extraction, Feature\nReduction and Classification. While the third stage implies a choice of a\nvariety of machine learning methods, the first and second stages play a\ncritical role in performance and accuracy of the classification system. In this\npaper we present initial study of feature extraction and feature reduction in\nthe task of vocal fold pathology diagnosis. A new type of feature vector, based\non wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients\n(MFCCs), is proposed. Also Principal Component Analysis (PCA) is used for\nfeature reduction. An Artificial Neural Network is used as a classifier for\nevaluating the performance of our proposed method."
},{
    "category": "cs.SD", 
    "doi": "10.5120/10089-4722", 
    "link": "http://arxiv.org/pdf/1302.6031v1", 
    "title": "Phoneme discrimination using KS algebra I", 
    "arxiv-id": "1302.6031v1", 
    "author": "Ondrej Such", 
    "publish": "2013-02-25T10:13:09Z", 
    "summary": "In our work we define a new algebra of operators as a substitute for fuzzy\nlogic. Its primary purpose is for construction of binary discriminators for\nphonemes based on spectral content. It is optimized for design of\nnon-parametric computational circuits, and makes uses of 4 operations: $\\min$,\n$\\max$, the difference and generalized additively homogenuous means."
},{
    "category": "cs.SD", 
    "doi": "10.5120/10089-4722", 
    "link": "http://arxiv.org/pdf/1302.6194v1", 
    "title": "Phoneme discrimination using $KS$-algebra II", 
    "arxiv-id": "1302.6194v1", 
    "author": "Lenka Mackovicova", 
    "publish": "2013-02-25T18:56:49Z", 
    "summary": "$KS$-algebra consists of expressions constructed with four kinds operations,\nthe minimum, maximum, difference and additively homogeneous generalized means.\nFive families of $Z$-classifiers are investigated on binary classification\ntasks between English phonemes. It is shown that the classifiers are able to\nreflect well known formant characteristics of vowels, while having very small\nKolmogoroff's complexity."
},{
    "category": "cs.SD", 
    "doi": "10.5120/10089-4722", 
    "link": "http://arxiv.org/pdf/1302.7070v1", 
    "title": "Sound localization using compressive sensing", 
    "arxiv-id": "1302.7070v1", 
    "author": "Paul Wilford", 
    "publish": "2013-02-28T03:43:08Z", 
    "summary": "In a sensor network with remote sensor devices, it is important to have a\nmethod that can accurately localize a sound event with a small amount of data\ntransmitted from the sensors. In this paper, we propose a novel method for\nlocalization of a sound source using compressive sensing. Instead of sampling a\nlarge amount of data at the Nyquist sampling rate in time domain, the acoustic\nsensors take compressive measurements integrated in time. The compressive\nmeasurements can be used to accurately compute the location of a sound source."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICASSP.2013.6637769", 
    "link": "http://arxiv.org/pdf/1303.0663v1", 
    "title": "Denoising Deep Neural Networks Based Voice Activity Detection", 
    "arxiv-id": "1303.0663v1", 
    "author": "Ji Wu", 
    "publish": "2013-03-04T10:17:49Z", 
    "summary": "Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)\nhas been proposed. It is powerful in fusing the advantages of multiple\nfeatures, and achieves the state-of-the-art performance. However, the deep\nlayers of the DBN-based VAD do not show an apparent superiority to the\nshallower layers. In this paper, we propose a denoising-deep-neural-network\n(DDNN) based VAD to address the aforementioned problem. Specifically, we\npre-train a deep neural network in a special unsupervised denoising greedy\nlayer-wise mode, and then fine-tune the whole network in a supervised way by\nthe common back-propagation algorithm. In the pre-training phase, we take the\nnoisy speech signals as the visible layer and try to extract a new feature that\nminimizes the reconstruction cross-entropy loss between the noisy speech\nsignals and its corresponding clean speech signals. Experimental results show\nthat the proposed DDNN-based VAD not only outperforms the DBN-based VAD but\nalso shows an apparent performance improvement of the deep layers over\nshallower layers."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICASSP.2013.6637769", 
    "link": "http://arxiv.org/pdf/1303.3948v1", 
    "title": "An Adaptive Methodology for Ubiquitous ASR System", 
    "arxiv-id": "1303.3948v1", 
    "author": "Vilas Thakare", 
    "publish": "2013-03-16T05:35:39Z", 
    "summary": "Achieving and maintaining the performance of ubiquitous (Automatic Speech\nRecognition) ASR system is a real challenge. The main objective of this work is\nto develop a method that will improve and show the consistency in performance\nof ubiquitous ASR system for real world noisy environment. An adaptive\nmethodology has been developed to achieve an objective with the help of\nimplementing followings, -Cleaning speech signal as much as possible while\npreserving originality / intangibility using various modified filters and\nenhancement techniques. -Extracting features from speech signals using various\nsizes of parameter. -Train the system for ubiquitous environment using\nmulti-environmental adaptation training methods. -Optimize the word recognition\nrate with appropriate variable size of parameters using fuzzy technique. The\nconsistency in performance is tested using standard noise databases as well as\nin real world environment. A good improvement is noticed. This work will be\nhelpful to give discriminative training of ubiquitous ASR system for better\nHuman Computer Interaction (HCI) using Speech User Interface (SUI)."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICASSP.2013.6637769", 
    "link": "http://arxiv.org/pdf/1309.6176v1", 
    "title": "Feature Learning with Gaussian Restricted Boltzmann Machine for Robust   Speech Recognition", 
    "arxiv-id": "1309.6176v1", 
    "author": "Lianhong Cai", 
    "publish": "2013-09-23T13:51:28Z", 
    "summary": "In this paper, we first present a new variant of Gaussian restricted\nBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann\nmachine (MGRBM), with its definition and learning algorithm. Then we propose\nusing a learned GRBM or MGRBM to extract better features for robust speech\nrecognition. Our experiments on Aurora2 show that both GRBM-extracted and\nMGRBM-extracted feature performs much better than Mel-frequency cepstral\ncoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)\nacoustic model, and MGRBM-extracted feature is slightly better."
},{
    "category": "cs.SD", 
    "doi": "10.3233/FI-2014-1118", 
    "link": "http://arxiv.org/pdf/1402.1530v1", 
    "title": "TDOA--based localization in two dimensions: the bifurcation curve", 
    "arxiv-id": "1402.1530v1", 
    "author": "Roberto Notari", 
    "publish": "2014-02-06T23:43:28Z", 
    "summary": "In this paper, we complete the study of the geometry of the TDOA map that\nencodes the noiseless model for the localization of a source from the range\ndifferences between three receivers in a plane, by computing the Cartesian\nequation of the bifurcation curve in terms of the positions of the receivers.\nFrom that equation, we can compute its real asymptotic lines. The present\nmanuscript completes the analysis of [Inverse Problems, Vol. 30, Number 3,\nPages 035004]. Our result is useful to check if a source belongs or is closed\nto the bifurcation curve, where the localization in a noisy scenario is\nambiguous."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TASLP.2014.2357676", 
    "link": "http://arxiv.org/pdf/1402.6926v3", 
    "title": "Sequential Complexity as a Descriptor for Musical Similarity", 
    "arxiv-id": "1402.6926v3", 
    "author": "Simon Dixon", 
    "publish": "2014-02-27T14:51:48Z", 
    "summary": "We propose string compressibility as a descriptor of temporal structure in\naudio, for the purpose of determining musical similarity. Our descriptors are\nbased on computing track-wise compression rates of quantised audio features,\nusing multiple temporal resolutions and quantisation granularities. To verify\nthat our descriptors capture musically relevant information, we incorporate our\ndescriptors into similarity rating prediction and song year prediction tasks.\nWe base our evaluation on a dataset of 15500 track excerpts of Western popular\nmusic, for which we obtain 7800 web-sourced pairwise similarity ratings. To\nassess the agreement among similarity ratings, we perform an evaluation under\ncontrolled conditions, obtaining a rank correlation of 0.33 between intersected\nsets of ratings. Combined with bag-of-features descriptors, we obtain\nperformance gains of 31.1% and 10.9% for similarity rating prediction and song\nyear prediction. For both tasks, analysis of selected descriptors reveals that\nrepresenting features at multiple time scales benefits prediction accuracy."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2014.6854954", 
    "link": "http://arxiv.org/pdf/1404.0400v1", 
    "title": "A Deep Representation for Invariance And Music Classification", 
    "arxiv-id": "1404.0400v1", 
    "author": "Tomaso Poggio", 
    "publish": "2014-04-01T21:15:32Z", 
    "summary": "Representations in the auditory cortex might be based on mechanisms similar\nto the visual ventral stream; modules for building invariance to\ntransformations and multiple layers for compositionality and selectivity. In\nthis paper we propose the use of such computational modules for extracting\ninvariant and discriminative audio representations. Building on a theory of\ninvariance in hierarchical architectures, we propose a novel, mid-level\nrepresentation for acoustical signals, using the empirical distributions of\nprojections on a set of templates and their transformations. Under the\nassumption that, by construction, this dictionary of templates is composed from\nsimilar classes, and samples the orbit of variance-inducing signal\ntransformations (such as shift and scale), the resulting signature is\ntheoretically guaranteed to be unique, invariant to transformations and stable\nto deformations. Modules of projection and pooling can then constitute layers\nof deep networks, for learning composite representations. We present the main\ntheoretical and computational aspects of a framework for unsupervised learning\nof invariant audio representations, empirically evaluated on music genre\nclassification."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22312803/IJCTT-V9P123", 
    "link": "http://arxiv.org/pdf/1404.1468v1", 
    "title": "High Throughput and Less Area AMP Architecture for Audio Signal   Restoration", 
    "arxiv-id": "1404.1468v1", 
    "author": "Rukmani Devi. D", 
    "publish": "2014-04-05T11:51:35Z", 
    "summary": "Audio restoration is effectively achieved by using low complexity algorithm\ncalled AMP. This algorithm has fast convergence and has lower computation\nintensity making it suitable for audio recovery problems. This paper focuses on\nrestoring an audio signal by using VLSI architecture called AMP-M that\nimplements AMP algorithm. This architecture employs MAC unit with fixed bit\nWallace tree multiplier, FFT-MUX and various memory units (RAM) for audio\nrestoration. VLSI and FPGA implementation results shows that reduced area, high\nthroughput, low power is achieved making it suitable for real time audio\nrecovery problems. Prominent examples are Magnetic Resonance Imaging (MRI),\nRadar and Wireless Communications."
},{
    "category": "cs.CR", 
    "doi": "10.14445/22312803/IJCTT-V9P123", 
    "link": "http://arxiv.org/pdf/1406.1213v1", 
    "title": "On Covert Acoustical Mesh Networks in Air", 
    "arxiv-id": "1406.1213v1", 
    "author": "Michael Goetz", 
    "publish": "2014-06-04T21:13:30Z", 
    "summary": "Covert channels can be used to circumvent system and network policies by\nestablishing communications that have not been considered in the design of the\ncomputing system. We construct a covert channel between different computing\nsystems that utilizes audio modulation/demodulation to exchange data between\nthe computing systems over the air medium. The underlying network stack is\nbased on a communication system that was originally designed for robust\nunderwater communication. We adapt the communication system to implement covert\nand stealthy communications by utilizing the ultrasonic frequency range. We\nfurther demonstrate how the scenario of covert acoustical communication over\nthe air medium can be extended to multi-hop communications and even to wireless\nmesh networks. A covert acoustical mesh network can be conceived as a meshed\nbotnet or malnet that is accessible via inaudible audio transmissions.\nDifferent applications of covert acoustical mesh networks are presented,\nincluding the use for remote keylogging over multiple hops. It is shown that\nthe concept of a covert acoustical mesh network renders many conventional\nsecurity concepts useless, as acoustical communications are usually not\nconsidered. Finally, countermeasures against covert acoustical mesh networks\nare discussed, including the use of lowpass filtering in computing systems and\na host-based intrusion detection system for analyzing audio input and output in\norder to detect any irregularities."
},{
    "category": "cs.SD", 
    "doi": "10.14445/22312803/IJCTT-V9P123", 
    "link": "http://arxiv.org/pdf/1406.3915v1", 
    "title": "A Bengali HMM Based Speech Synthesis System", 
    "arxiv-id": "1406.3915v1", 
    "author": "Shyamal Kumar Das Mandal", 
    "publish": "2014-06-16T06:41:54Z", 
    "summary": "The paper presents the capability of an HMM-based TTS system to produce\nBengali speech. In this synthesis method, trajectories of speech parameters are\ngenerated from the trained Hidden Markov Models. A final speech waveform is\nsynthesized from those speech parameters. In our experiments, spectral\nproperties were represented by Mel Cepstrum Coefficients. Both the training and\nsynthesis issues are investigated in this paper using annotated Bengali speech\ndatabase. Experimental evaluation depicts that the developed text-to-speech\nsystem is capable of producing adequately natural speech in terms of\nintelligibility and intonation for Bengali."
},{
    "category": "cs.IR", 
    "doi": "10.1109/LSP.2014.2347582", 
    "link": "http://arxiv.org/pdf/1406.4877v1", 
    "title": "On the Application of Generic Summarization Algorithms to Music", 
    "arxiv-id": "1406.4877v1", 
    "author": "David Martins de Matos", 
    "publish": "2014-06-18T20:10:22Z", 
    "summary": "Several generic summarization algorithms were developed in the past and\nsuccessfully applied in fields such as text and speech summarization. In this\npaper, we review and apply these algorithms to music. To evaluate this\nsummarization's performance, we adopt an extrinsic approach: we compare a Fado\nGenre Classifier's performance using truncated contiguous clips against the\nsummaries extracted with those algorithms on 2 different datasets. We show that\nMaximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)\nall improve classification performance in both datasets used for testing."
},{
    "category": "cs.MS", 
    "doi": "10.1121/1.4901318", 
    "link": "http://arxiv.org/pdf/1408.0854v1", 
    "title": "Semi-Analytical Computation of Acoustic Scattering by Spheroids and   Disks", 
    "arxiv-id": "1408.0854v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2014-08-05T03:18:28Z", 
    "summary": "Analytical solutions to acoustic scattering problems involving nonspherical\nshapes, such as spheroids and disks, have long been known and have many\napplications. However, these solutions require special functions that are not\neasily computable. For this reason, their asymptotic forms are typically used\nsince they are more readily available. We explore these solutions and provide\ncomputational software for calculating their nonasymptotic forms, which are\naccurate over a wide range of frequencies and distances. This software, which\nruns in MATLAB, computes the solutions to acoustic scattering problems\ninvolving spheroids and disks by semi-analytical means, and is freely available\nfrom our webpage."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2405475", 
    "link": "http://arxiv.org/pdf/1408.2700v4", 
    "title": "Co-Localization of Audio Sources in Images Using Binaural Features and   Locally-Linear Regression", 
    "arxiv-id": "1408.2700v4", 
    "author": "Laurent Girin", 
    "publish": "2014-08-12T12:08:13Z", 
    "summary": "This paper addresses the problem of localizing audio sources using binaural\nmeasurements. We propose a supervised formulation that simultaneously localizes\nmultiple sources at different locations. The approach is intrinsically\nefficient because, contrary to prior work, it relies neither on source\nseparation, nor on monaural segregation. The method starts with a training\nstage that establishes a locally-linear Gaussian regression model between the\ndirectional coordinates of all the sources and the auditory features extracted\nfrom binaural measurements. While fixed-length wide-spectrum sounds (white\nnoise) are used for training to reliably estimate the model parameters, we show\nthat the testing (localization) can be extended to variable-length\nsparse-spectrum sounds (such as speech), thus enabling a wide range of\nrealistic applications. Indeed, we demonstrate that the method can be used for\naudio-visual fusion, namely to map speech signals onto images and hence to\nspatially align the audio and visual modalities, thus enabling to discriminate\nbetween speaking and non-speaking faces. We release a novel corpus of real-room\nrecordings that allow quantitative evaluation of the co-localization method in\nthe presence of one or two sound sources. Experiments demonstrate increased\naccuracy and speed relative to several state-of-the-art methods."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2405475", 
    "link": "http://arxiv.org/pdf/1409.7787v1", 
    "title": "Audio Surveillance: a Systematic Review", 
    "arxiv-id": "1409.7787v1", 
    "author": "Vittorio Murino", 
    "publish": "2014-09-27T09:47:16Z", 
    "summary": "Despite surveillance systems are becoming increasingly ubiquitous in our\nliving environment, automated surveillance, currently based on video sensory\nmodality and machine intelligence, lacks most of the time the robustness and\nreliability required in several real applications. To tackle this issue, audio\nsensory devices have been taken into account, both alone or in combination with\nvideo, giving birth, in the last decade, to a considerable amount of research.\nIn this paper audio-based automated surveillance methods are organized into a\ncomprehensive survey: a general taxonomy, inspired by the more widespread video\nsurveillance field, is proposed in order to systematically describe the methods\ncovering background subtraction, event classification, object tracking and\nsituation analysis. For each of these tasks, all the significant works are\nreviewed, detailing their pros and cons and the context for which they have\nbeen proposed. Moreover, a specific section is devoted to audio features,\ndiscussing their expressiveness and their employment in the above described\ntasks. Differently, from other surveys on audio processing and analysis, the\npresent one is specifically targeted to automated surveillance, highlighting\nthe target applications of each described methods and providing the reader\ntables and schemes useful to retrieve the most suited algorithms for a specific\nrequirement."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2405475", 
    "link": "http://arxiv.org/pdf/1501.07496v1", 
    "title": "Implementation of an Automatic Syllabic Division Algorithm from Speech   Files in Portuguese Language", 
    "arxiv-id": "1501.07496v1", 
    "author": "H. M. de Oliveira", 
    "publish": "2015-01-29T16:09:17Z", 
    "summary": "A new algorithm for voice automatic syllabic splitting in the Portuguese\nlanguage is proposed, which is based on the envelope of the speech signal of\nthe input audio file. A computational implementation in MatlabTM is presented\nand made available at the URL\nhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its\nstraightforwardness, the proposed method is very attractive for embedded\nsystems (e.g. i-phones). It can also be used as a screen to assist more\nsophisticated methods. Voice excerpts containing more than one syllable and\nidentified by the same envelope are named as super-syllables and they are\nsubsequently separated. The results indicate which samples corresponds to the\nbeginning and end of each detected syllable. Preliminary tests were performed\nto fifty words at an identification rate circa 70% (further improvements may be\nincorporated to treat particular phonemes). This algorithm is also useful in\nvoice command systems, as a tool in the teaching of Portuguese language or even\nfor patients with speech pathology."
},{
    "category": "cs.PL", 
    "doi": "10.1007/978-3-642-18381-2_8", 
    "link": "http://arxiv.org/pdf/1104.2681v1", 
    "title": "Liquidsoap: a High-Level Programming Language for Multimedia Streaming", 
    "arxiv-id": "1104.2681v1", 
    "author": "Samuel Mimram", 
    "publish": "2011-04-14T07:01:54Z", 
    "summary": "Generating multimedia streams, such as in a netradio, is a task which is\ncomplex and difficult to adapt to every users' needs. We introduce a novel\napproach in order to achieve it, based on a dedicated high-level functional\nprogramming language, called Liquidsoap, for generating, manipulating and\nbroadcasting multimedia streams. Unlike traditional approaches, which are based\non configuration files or static graphical interfaces, it also allows the user\nto build complex and highly customized systems. This language is based on a\nmodel for streams and contains operators and constructions, which make it\nadapted to the generation of streams. The interpreter of the language also\nensures many properties concerning the good execution of the stream generation."
},{
    "category": "physics.data-an", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1109.4653v2", 
    "title": "Can the evolution of music be analyzed in a quantitative manner?", 
    "arxiv-id": "1109.4653v2", 
    "author": "Luciano da Fontoura Costa", 
    "publish": "2011-09-21T20:43:46Z", 
    "summary": "We propose a methodology to study music development by applying multivariate\nstatistics on composers characteristics. Seven representative composers were\nconsidered in terms of eight main musical features. Grades were assigned to\neach characteristic and their correlations were analyzed. A bootstrap method\nwas applied to simulate hundreds of artificial composers influenced by the\nseven representatives chosen. Afterwards we quantify non-numeric relations like\ndialectics, opposition and innovation. Composers differences on style and\ntechnique were represented as geometrical distances in the feature space,\nmaking it possible to quantify, for example, how much Bach and Stockhausen\ndiffer from other composers or how much Beethoven influenced Brahms. In\naddition, we compared the results with a prior investigation on philosophy.\nOpposition, strong on philosophy, was not remarkable on music. Supporting an\nobservation already considered by music theorists, strong influences were\nidentified between composers by the quantification of dialectics, implying\ninheritance and suggesting a stronger master-disciple evolution when compared\nto the philosophy analysis."
},{
    "category": "cs.SD", 
    "doi": "10.1088/1742-5468/2012/08/P08010", 
    "link": "http://arxiv.org/pdf/1207.5560v1", 
    "title": "Evolving Musical Counterpoint: The Chronopoint Musical Evolution System", 
    "arxiv-id": "1207.5560v1", 
    "author": "James Reggia", 
    "publish": "2012-07-23T23:25:36Z", 
    "summary": "Musical counterpoint, a musical technique in which two or more independent\nmelodies are played simultaneously with the goal of creating harmony, has been\naround since the baroque era. However, to our knowledge computational\ngeneration of aesthetically pleasing linear counterpoint based on subjective\nfitness assessment has not been explored by the evolutionary computation\ncommunity (although generation using objective fitness has been attempted in\nquite a few cases). The independence of contrapuntal melodies and the\nsubjective nature of musical aesthetics provide an excellent platform for the\napplication of genetic algorithms. In this paper, a genetic algorithm approach\nto generating contrapuntal melodies is explained, with a description of the\nvarious musical heuristics used and of how variable-length chromosome strings\nare used to avoid generating \"jerky\" rhythms and melodic phrases, as well as\nhow subjectivity is incorporated into the algorithm's fitness measures. Next,\nresults from empirical testing of the algorithm are presented, with a focus on\nhow a user's musical sophistication influences their experience. Lastly,\nfurther musical and compositional applications of the algorithm are discussed\nalong with planned future work on the algorithm."
},{
    "category": "cs.MM", 
    "doi": "10.5121/ijma.2012.4606", 
    "link": "http://arxiv.org/pdf/1301.1894v1", 
    "title": "An Extensive Analysis of Query by Singing/Humming System Through Query   Proportion", 
    "arxiv-id": "1301.1894v1", 
    "author": "Nagappa U. Bhajantri", 
    "publish": "2013-01-09T15:36:40Z", 
    "summary": "Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system\nwith small audio excerpt as query. The rising availability of digital music\nstipulates effective music retrieval methods. Further, MIR systems support\ncontent based searching for music and requires no musical acquaintance. Current\nwork on QBSH focuses mainly on melody features such as pitch, rhythm, note\netc., size of databases, response time, score matching and search algorithms.\nEven though a variety of QBSH techniques are proposed, there is a dearth of\nwork to analyze QBSH through query excerption. Here, we present an analysis\nthat works on QBSH through query excerpt. To substantiate a series of\nexperiments are conducted with the help of Mel-Frequency Cepstral Coefficients\n(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral\nCoefficients (LPCC) to portray the robustness of the knowledge representation.\nProposed experiments attempt to reveal that retrieval performance as well as\nprecision diminishes in the snail phase with the growing database size."
},{
    "category": "cs.CV", 
    "doi": "10.1016/j.sigpro.2013.06", 
    "link": "http://arxiv.org/pdf/1304.0035v1", 
    "title": "Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals", 
    "arxiv-id": "1304.0035v1", 
    "author": "Ivan W. Selesnick", 
    "publish": "2013-03-29T22:00:01Z", 
    "summary": "This paper addresses signal denoising when large-amplitude coefficients form\nclusters (groups). The L1-norm and other separable sparsity models do not\ncapture the tendency of coefficients to cluster (group sparsity). This work\ndevelops an algorithm, called 'overlapping group shrinkage' (OGS), based on the\nminimization of a convex cost function involving a group-sparsity promoting\npenalty function. The groups are fully overlapping so the denoising method is\ntranslation-invariant and blocking artifacts are avoided. Based on the\nprinciple of majorization-minimization (MM), we derive a simple iterative\nminimization algorithm that reduces the cost function monotonically. A\nprocedure for setting the regularization parameter, based on attenuating the\nnoise to a specified level, is also described. The proposed approach is\nillustrated on speech enhancement, wherein the OGS approach is applied in the\nshort-time Fourier transform (STFT) domain. The denoised speech produced by OGS\ndoes not suffer from musical noise."
},{
    "category": "cs.LG", 
    "doi": "10.1016/j.sigpro.2013.06", 
    "link": "http://arxiv.org/pdf/1304.5862v2", 
    "title": "Multi-Label Classifier Chains for Bird Sound", 
    "arxiv-id": "1304.5862v2", 
    "author": "Jed Irvine", 
    "publish": "2013-04-22T07:44:05Z", 
    "summary": "Bird sound data collected with unattended microphones for automatic surveys,\nor mobile devices for citizen science, typically contain multiple\nsimultaneously vocalizing birds of different species. However, few works have\nconsidered the multi-label structure in birdsong. We propose to use an ensemble\nof classifier chains combined with a histogram-of-segments representation for\nmulti-label classification of birdsong. The proposed method is compared with\nbinary relevance and three multi-instance multi-label learning (MIML)\nalgorithms from prior work (which focus more on structure in the sound, and\nless on structure in the label sets). Experiments are conducted on two\nreal-world birdsong datasets, and show that the proposed method usually\noutperforms binary relevance (using the same features and base-classifier), and\nis better in some cases and worse in others compared to the MIML algorithms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2014.2326991", 
    "link": "http://arxiv.org/pdf/1304.6763v2", 
    "title": "Deep Scattering Spectrum", 
    "arxiv-id": "1304.6763v2", 
    "author": "St\u00e9phane Mallat", 
    "publish": "2013-04-24T21:50:03Z", 
    "summary": "A scattering transform defines a locally translation invariant representation\nwhich is stable to time-warping deformations. It extends MFCC representations\nby computing modulation spectrum coefficients of multiple orders, through\ncascades of wavelet convolutions and modulus operators. Second-order scattering\ncoefficients characterize transient phenomena such as attacks and amplitude\nmodulation. A frequency transposition invariant representation is obtained by\napplying a scattering transform along log-frequency. State-the-of-art\nclassification results are obtained for musical genre and phone classification\non GTZAN and TIMIT databases, respectively."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TSP.2014.2326991", 
    "link": "http://arxiv.org/pdf/1305.5078v1", 
    "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments   in Jazz Recordings", 
    "arxiv-id": "1305.5078v1", 
    "author": "Miron B. Kursa", 
    "publish": "2013-05-22T10:43:25Z", 
    "summary": "In this paper, we first apply random ferns for classification of real music\nrecordings of a jazz band. No initial segmentation of audio data is assumed,\ni.e., no onset, offset, nor pitch data are needed. The notion of random ferns\nis described in the paper, to familiarize the reader with this classification\nalgorithm, which was introduced quite recently and applied so far in image\nrecognition tasks. The performance of random ferns is compared with random\nforests for the same data. The results of experiments are presented in the\npaper, and conclusions are drawn."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TSP.2014.2326991", 
    "link": "http://arxiv.org/pdf/1306.2906v1", 
    "title": "Robust Support Vector Machines for Speaker Verification Task", 
    "arxiv-id": "1306.2906v1", 
    "author": "Abderrahmane Amrouche", 
    "publish": "2013-06-12T17:32:02Z", 
    "summary": "An important step in speaker verification is extracting features that best\ncharacterize the speaker voice. This paper investigates a front-end processing\nthat aims at improving the performance of speaker verification based on the\nSVMs classifier, in text independent mode. This approach combines features\nbased on conventional Mel-cepstral Coefficients (MFCCs) and Line Spectral\nFrequencies (LSFs) to constitute robust multivariate feature vectors. To reduce\nthe high dimensionality required for training these feature vectors, we use a\ndimension reduction method called principal component analysis (PCA). In order\nto evaluate the robustness of these systems, different noisy environments have\nbeen used. The obtained results using TIMIT database showed that, using the\nparadigm that combines these spectral cues leads to a significant improvement\nin verification accuracy, especially with PCA reduction for low signal-to-noise\nratio noisy environment."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TSP.2014.2326991", 
    "link": "http://arxiv.org/pdf/1307.0589v1", 
    "title": "The Orchive : Data mining a massive bioacoustic archive", 
    "arxiv-id": "1307.0589v1", 
    "author": "George Tzanetakis", 
    "publish": "2013-07-02T04:59:19Z", 
    "summary": "The Orchive is a large collection of over 20,000 hours of audio recordings\nfrom the OrcaLab research facility located off the northern tip of Vancouver\nIsland. It contains recorded orca vocalizations from the 1980 to the present\ntime and is one of the largest resources of bioacoustic data in the world. We\nhave developed a web-based interface that allows researchers to listen to these\nrecordings, view waveform and spectral representations of the audio, label\nclips with annotations, and view the results of machine learning classifiers\nbased on automatic audio features extraction. In this paper we describe such\nclassifiers that discriminate between background noise, orca calls, and the\nvoice notes that are present in most of the tapes. Furthermore we show\nclassification results for individual calls based on a previously existing orca\ncall catalog. We have also experimentally investigated the scalability of\nclassifiers over the entire Orchive."
},{
    "category": "math.AT", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1307.1201v2", 
    "title": "Topology of Musical Data", 
    "arxiv-id": "1307.1201v2", 
    "author": "William Sethares", 
    "publish": "2013-07-04T05:00:59Z", 
    "summary": "The musical realm is a promising area in which to expect to find nontrivial\ntopological structures. This paper describes several kinds of metrics on\nmusical data, and explores the implications of these metrics in two ways: via\ntechniques of classical topology where the metric space of all-possible musical\ndata can be described explicitly, and via modern data-driven ideas of\npersistent homology which calculates the Betti-number bar-codes of individual\nmusical works. Both analyses are able to recover three well known topological\nstructures in music: the circle of notes (octave-reduced scalar structures),\nthe circle of fifths, and the rhythmic repetition of timelines. Applications to\na variety of musical works (for example, folk music in the form of standard\nMIDI files) are presented, and the bar codes show many interesting features.\nExamples show that individual pieces may span the complete space (in which case\nthe classical and the data-driven analyses agree), or they may span only part\nof the space."
},{
    "category": "cs.CL", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1307.5736v1", 
    "title": "Speaker Independent Continuous Speech to Text Converter for Mobile   Application", 
    "arxiv-id": "1307.5736v1", 
    "author": "M. Sharina", 
    "publish": "2013-07-19T05:27:46Z", 
    "summary": "An efficient speech to text converter for mobile application is presented in\nthis work. The prime motive is to formulate a system which would give optimum\nperformance in terms of complexity, accuracy, delay and memory requirements for\nmobile environment. The speech to text converter consists of two stages namely\nfront-end analysis and pattern recognition. The front end analysis involves\npreprocessing and feature extraction. The traditional voice activity detection\nalgorithms which track only energy cannot successfully identify potential\nspeech from input because the unwanted part of the speech also has some energy\nand appears to be speech. In the proposed system, VAD that calculates energy of\nhigh frequency part separately as zero crossing rate to differentiate noise\nfrom speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as\nfeature extraction method and Generalized Regression Neural Network is used as\nrecognizer. MFCC provides low word error rate and better feature extraction.\nNeural Network improves the accuracy. Thus a small database containing all\npossible syllable pronunciation of the user is sufficient to give recognition\naccuracy closer to 100%. Thus the proposed technique entertains realization of\nreal time speaker independent applications like mobile phones, PDAs etc."
},{
    "category": "cs.LG", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1312.4695v3", 
    "title": "Sparse, complex-valued representations of natural sounds learned with   phase and amplitude continuity priors", 
    "arxiv-id": "1312.4695v3", 
    "author": "Wiktor Mlynarski", 
    "publish": "2013-12-17T09:12:55Z", 
    "summary": "Complex-valued sparse coding is a data representation which employs a\ndictionary of two-dimensional subspaces, while imposing a sparse, factorial\nprior on complex amplitudes. When trained on a dataset of natural image\npatches, it learns phase invariant features which closely resemble receptive\nfields of complex cells in the visual cortex. Features trained on natural\nsounds however, rarely reveal phase invariance and capture other aspects of the\ndata. This observation is a starting point of the present work. As its first\ncontribution, it provides an analysis of natural sound statistics by means of\nlearning sparse, complex representations of short speech intervals. Secondly,\nit proposes priors over the basis function set, which bias them towards\nphase-invariant solutions. In this way, a dictionary of complex basis functions\ncan be learned from the data statistics, while preserving the phase invariance\nproperty. Finally, representations trained on speech sounds with and without\npriors are compared. Prior-based basis functions reveal performance comparable\nto unconstrained sparse coding, while explicitely representing phase as a\ntemporal shift. Such representations can find applications in many perceptual\nand machine learning tasks."
},{
    "category": "cs.CL", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1401.3322v1", 
    "title": "A Subband-Based SVM Front-End for Robust ASR", 
    "arxiv-id": "1401.3322v1", 
    "author": "Matthew Ager", 
    "publish": "2013-12-24T08:45:07Z", 
    "summary": "This work proposes a novel support vector machine (SVM) based robust\nautomatic speech recognition (ASR) front-end that operates on an ensemble of\nthe subband components of high-dimensional acoustic waveforms. The key issues\nof selecting the appropriate SVM kernels for classification in frequency\nsubbands and the combination of individual subband classifiers using ensemble\nmethods are addressed. The proposed front-end is compared with state-of-the-art\nASR front-ends in terms of robustness to additive noise and linear filtering.\nExperiments performed on the TIMIT phoneme classification task demonstrate the\nbenefits of the proposed subband based SVM front-end: it outperforms the\nstandard cepstral front-end in the presence of noise and linear filtering for\nsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposed\nfront-end with a conventional front-end such as MFCC yields further\nimprovements over the individual front ends across the full range of noise\nlevels."
},{
    "category": "cs.CR", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1403.1165v1", 
    "title": "A Taxonomy for Attack Patterns on Information Flows in Component-Based   Operating Systems", 
    "arxiv-id": "1403.1165v1", 
    "author": "J\u00f6rg Keller", 
    "publish": "2014-03-05T15:43:57Z", 
    "summary": "We present a taxonomy and an algebra for attack patterns on component-based\noperating systems. In a multilevel security scenario, where isolation of\npartitions containing data at different security classifications is the primary\nsecurity goal and security breaches are mainly defined as undesired disclosure\nor modification of classified data, strict control of information flows is the\nultimate goal. In order to prevent undesired information flows, we provide a\nclassification of information flow types in a component-based operating system\nand, by this, possible patterns to attack the system. The systematic\nconsideration of informations flows reveals a specific type of operating system\ncovert channel, the covert physical channel, which connects two former isolated\npartitions by emitting physical signals into the computer's environment and\nreceiving them at another interface."
},{
    "category": "cs.SD", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1403.6901v1", 
    "title": "Automatic Segmentation of Broadcast News Audio using Self Similarity   Matrix", 
    "arxiv-id": "1403.6901v1", 
    "author": "Sunil Kumar Kopparapu", 
    "publish": "2014-03-27T01:32:09Z", 
    "summary": "Generally audio news broadcast on radio is com- posed of music, commercials,\nnews from correspondents and recorded statements in addition to the actual news\nread by the newsreader. When news transcripts are available, automatic\nsegmentation of audio news broadcast to time align the audio with the text\ntranscription to build frugal speech corpora is essential. We address the\nproblem of identifying segmentation in the audio news broadcast corresponding\nto the news read by the newsreader so that they can be mapped to the text\ntranscripts. The existing techniques produce sub-optimal solutions when used to\nextract newsreader read segments. In this paper, we propose a new technique\nwhich is able to identify the acoustic change points reliably using an acoustic\nSelf Similarity Matrix (SSM). We describe the two pass technique in detail and\nverify its performance on real audio news broadcast of All India Radio for\ndifferent languages."
},{
    "category": "cs.HC", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1407.4705v1", 
    "title": "Sonification of a Network's Self-Organized Criticality", 
    "arxiv-id": "1407.4705v1", 
    "author": "Tom Fairfax", 
    "publish": "2014-07-17T15:20:26Z", 
    "summary": "Communication networks involve the transmission and reception of large\nvolumes of data. Research indicates that network traffic volumes will continue\nto increase. These traffic volumes will be unprecedented and the behaviour of\nglobal information infrastructures when dealing with these data volumes is\nunknown. It has been shown that complex systems (including computer networks)\nexhibit self-organized criticality under certain conditions. Given the\npossibility in such systems of a sudden and spontaneous system reset the\ndevelopment of techniques to inform system administrators of this behaviour\ncould be beneficial. This article focuses on the combination of two dissimilar\nresearch concepts, namely sonification (a form of auditory display) and\nself-organized criticality (SOC). A system is described that sonifies in real\ntime an information infrastructure's self-organized criticality to alert the\nnetwork administrators of both normal and abnormal network traffic and\noperation."
},{
    "category": "cs.SD", 
    "doi": "10.1080/17459737.2013.850597", 
    "link": "http://arxiv.org/pdf/1407.5514v2", 
    "title": "Raking the Cocktail Party", 
    "arxiv-id": "1407.5514v2", 
    "author": "Martin Vetterli", 
    "publish": "2014-07-21T14:46:41Z", 
    "summary": "We present the concept of an acoustic rake receiver---a microphone beamformer\nthat uses echoes to improve the noise and interference suppression. The rake\nidea is well-known in wireless communications; it involves constructively\ncombining different multipath components that arrive at the receiver antennas.\nUnlike spread-spectrum signals used in wireless communications, speech signals\nare not orthogonal to their shifts. Therefore, we focus on the spatial\nstructure, rather than temporal. Instead of explicitly estimating the channel,\nwe create correspondences between early echoes in time and image sources in\nspace. These multiple sources of the desired and the interfering signal offer\nadditional spatial diversity that we can exploit in the beamformer design.\n  We present several \"intuitive\" and optimal formulations of acoustic rake\nreceivers, and show theoretically and numerically that the rake formulation of\nthe maximum signal-to-interference-and-noise beamformer offers significant\nperformance boosts in terms of noise and interference suppression. Beyond\nsignal-to-noise ratio, we observe gains in terms of the \\emph{perceptual\nevaluation of speech quality} (PESQ) metric for the speech quality. We\naccompany the paper by the complete simulation and processing chain written in\nPython. The code and the sound samples are available online at\n\\url{http://lcav.github.io/AcousticRakeReceiver/}."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1410.2479v2", 
    "title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy   and Reverberant Environments", 
    "arxiv-id": "1410.2479v2", 
    "author": "Walter Kellermann", 
    "publish": "2014-10-09T14:15:42Z", 
    "summary": "We propose a spatial diffuseness feature for deep neural network (DNN)-based\nautomatic speech recognition to improve recognition accuracy in reverberant and\nnoisy environments. The feature is computed in real-time from multiple\nmicrophone signals without requiring knowledge or estimation of the direction\nof arrival, and represents the relative amount of diffuse noise in each time\nand frequency bin. It is shown that using the diffuseness feature as an\nadditional input to a DNN-based acoustic model leads to a reduced word error\nrate for the REVERB challenge corpus, both compared to logmelspec features\nextracted from noisy signals, and features enhanced by spectral subtraction."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1411.5014v1", 
    "title": "Music Data Analysis: A State-of-the-art Survey", 
    "arxiv-id": "1411.5014v1", 
    "author": "Shubhanshu Gupta", 
    "publish": "2014-11-18T14:19:28Z", 
    "summary": "Music accounts for a significant chunk of interest among various online\nactivities. This is reflected by wide array of alternatives offered in music\nrelated web/mobile apps, information portals, featuring millions of artists,\nsongs and events attracting user activity at similar scale. Availability of\nlarge scale structured and unstructured data has attracted similar level of\nattention by data science community. This paper attempts to offer current\nstate-of-the-art in music related analysis. Various approaches involving\nmachine learning, information theory, social network analysis, semantic web and\nlinked open data are represented in the form of taxonomy along with data\nsources and use cases addressed by the research community."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1412.6645v3", 
    "title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models", 
    "arxiv-id": "1412.6645v3", 
    "author": "Emmanuel Dupoux", 
    "publish": "2014-12-20T11:54:41Z", 
    "summary": "We trained a Siamese network with multi-task same/different information on a\nspeech dataset, and found that it was possible to share a network for both\ntasks without a loss in performance. The first task was to discriminate between\ntwo same or different words, and the second was to discriminate between two\nsame or different talkers."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1412.7193v1", 
    "title": "Audio Source Separation Using a Deep Autoencoder", 
    "arxiv-id": "1412.7193v1", 
    "author": "Yung-Hwan Oh", 
    "publish": "2014-12-22T22:38:06Z", 
    "summary": "This paper proposes a novel framework for unsupervised audio source\nseparation using a deep autoencoder. The characteristics of unknown source\nsignals mixed in the mixed input is automatically by properly configured\nautoencoders implemented by a network with many layers, and separated by\nclustering the coefficient vectors in the code layer. By investigating the\nweight vectors to the final target, representation layer, the primitive\ncomponents of the audio signals in the frequency domain are observed. By\nclustering the activation coefficients in the code layer, the previously\nunknown source signals are segregated. The original source sounds are then\nseparated and reconstructed by using code vectors which belong to different\nclusters. The restored sounds are not perfect but yield promising results for\nthe possibility in the success of many practical applications."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1502.00524v2", 
    "title": "Unsupervised Incremental Learning and Prediction of Music Signals", 
    "arxiv-id": "1502.00524v2", 
    "author": "Hendrik Purwins", 
    "publish": "2015-02-02T15:45:38Z", 
    "summary": "A system is presented that segments, clusters and predicts musical audio in\nan unsupervised manner, adjusting the number of (timbre) clusters\ninstantaneously to the audio input. A sequence learning algorithm adapts its\nstructure to a dynamically changing clustering tree. The flow of the system is\nas follows: 1) segmentation by onset detection, 2) timbre representation of\neach segment by Mel frequency cepstrum coefficients, 3) discretization by\nincremental clustering, yielding a tree of different sound classes (e.g.\ninstruments) that can grow or shrink on the fly driven by the instantaneous\nsound events, resulting in a discrete symbol sequence, 4) extraction of\nstatistical regularities of the symbol sequence, using hierarchical N-grams and\nthe newly introduced conceptual Boltzmann machine, and 5) prediction of the\nnext sound event in the sequence. The system's robustness is assessed with\nrespect to complexity and noisiness of the signal. Clustering in isolation\nyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing\nvoice and drums. Onset detection jointly with clustering achieve an ARI of\n81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /\n39.2%."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2015.7178798", 
    "link": "http://arxiv.org/pdf/1502.03163v1", 
    "title": "Gaussian Process Models for HRTF based Sound-Source Localization and   Active-Learning", 
    "arxiv-id": "1502.03163v1", 
    "author": "Ramani Duraiswami", 
    "publish": "2015-02-11T00:55:14Z", 
    "summary": "From a machine learning perspective, the human ability localize sounds can be\nmodeled as a non-parametric and non-linear regression problem between binaural\nspectral features of sound received at the ears (input) and their sound-source\ndirections (output). The input features can be summarized in terms of the\nindividual's head-related transfer functions (HRTFs) which measure the spectral\nresponse between the listener's eardrum and an external point in $3$D. Based on\nthese viewpoints, two related problems are considered: how can one achieve an\noptimal sampling of measurements for training sound-source localization (SSL)\nmodels, and how can SSL models be used to infer the subject's HRTFs in\nlistening tests. First, we develop a class of binaural SSL models based on\nGaussian process regression and solve a \\emph{forward selection} problem that\nfinds a subset of input-output samples that best generalize to all SSL\ndirections. Second, we use an \\emph{active-learning} approach that updates an\nonline SSL model for inferring the subject's SSL errors via headphones and a\ngraphical user interface. Experiments show that only a small fraction of HRTFs\nare required for $5^{\\circ}$ localization accuracy and that the learned HRTFs\nare localized closer to their intended directions than non-individualized\nHRTFs."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2468583", 
    "link": "http://arxiv.org/pdf/1502.04149v4", 
    "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for   Monaural Source Separation", 
    "arxiv-id": "1502.04149v4", 
    "author": "Paris Smaragdis", 
    "publish": "2015-02-13T23:22:16Z", 
    "summary": "Monaural source separation is important for many real world applications. It\nis challenging because, with only a single channel of information available,\nwithout any constraints, an infinite number of solutions are possible. In this\npaper, we explore joint optimization of masking functions and deep recurrent\nneural networks for monaural source separation tasks, including monaural speech\nseparation, monaural singing voice separation, and speech denoising. The joint\noptimization of the deep recurrent neural networks with an extra masking layer\nenforces a reconstruction constraint. Moreover, we explore a discriminative\ncriterion for training neural networks to further enhance the separation\nperformance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT\ndatasets for speech separation, singing voice separation, and speech denoising\ntasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to\nNMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and\n4.32--5.42 dB GSIR gain compared to existing models in the singing voice\nseparation task, and outperform NMF and DNN baselines in the speech denoising\ntask."
},{
    "category": "cs.IT", 
    "doi": "10.1109/TSP.2015.2478751", 
    "link": "http://arxiv.org/pdf/1502.07577v2", 
    "title": "Sampling Sparse Signals on the Sphere: Algorithms and Applications", 
    "arxiv-id": "1502.07577v2", 
    "author": "Yue M. Lu", 
    "publish": "2015-02-26T14:49:38Z", 
    "summary": "We propose a sampling scheme that can perfectly reconstruct a collection of\nspikes on the sphere from samples of their lowpass-filtered observations.\nCentral to our algorithm is a generalization of the annihilating filter method,\na tool widely used in array signal processing and finite-rate-of-innovation\n(FRI) sampling. The proposed algorithm can reconstruct $K$ spikes from\n$(K+\\sqrt{K})^2$ spatial samples. This sampling requirement improves over\npreviously known FRI sampling schemes on the sphere by a factor of four for\nlarge $K$. We showcase the versatility of the proposed algorithm by applying it\nto three different problems: 1) sampling diffusion processes induced by\nlocalized sources on the sphere, 2) shot noise removal, and 3) sound source\nlocalization (SSL) by a spherical microphone array. In particular, we show how\nSSL can be reformulated as a spherical sparse sampling problem."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TSP.2015.2478751", 
    "link": "http://arxiv.org/pdf/1503.00022v1", 
    "title": "Plagiarism Detection in Polyphonic Music using Monaural Signal   Separation", 
    "arxiv-id": "1503.00022v1", 
    "author": "Bhiksha Raj", 
    "publish": "2015-02-27T21:57:16Z", 
    "summary": "Given the large number of new musical tracks released each year, automated\napproaches to plagiarism detection are essential to help us track potential\nviolations of copyright. Most current approaches to plagiarism detection are\nbased on musical similarity measures, which typically ignore the issue of\npolyphony in music. We present a novel feature space for audio derived from\ncompositional modelling techniques, commonly used in signal separation, that\nprovides a mechanism to account for polyphony without incurring an inordinate\namount of computational overhead. We employ this feature representation in\nconjunction with traditional audio feature representations in a classification\nframework which uses an ensemble of distance features to characterize pairs of\nsongs as being plagiarized or not. Our experiments on a database of about 3000\nmusical track pairs show that the new feature space characterization produces\nsignificant improvements over standard baselines."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s00034-016-0310-y", 
    "link": "http://arxiv.org/pdf/1503.02578v2", 
    "title": "Modeling State-Conditional Observation Distribution using Weighted   Stereo Samples for Factorial Speech Processing Models", 
    "arxiv-id": "1503.02578v2", 
    "author": "Mohammad Mehdi Homayounpour", 
    "publish": "2015-03-09T17:40:08Z", 
    "summary": "This paper investigates the effectiveness of factorial speech processing\nmodels in noise-robust automatic speech recognition tasks. For this purpose,\nthe paper proposes an idealistic approach for modeling state-conditional\nobservation distribution of factorial models based on weighted stereo samples.\nThis approach is an extension to previous single pass retraining for ideal\nmodel compensation which is extended here to support multiple audio sources.\nNon-stationary noises can be considered as one of these audio sources with\nmultiple states. Experiments of this paper over the set A of the Aurora 2\ndataset show that recognition performance can be improved by this\nconsideration. The improvement is significant in low signal to noise energy\nconditions, up to 4% absolute word recognition accuracy. In addition to the\npower of the proposed method in accurate representation of state-conditional\nobservation distribution, it has an important advantage over previous methods\nby providing the opportunity to independently select feature spaces for both\nsource and corrupted features. This opens a new window for seeking better\nfeature spaces appropriate for noisy speech, independent from clean speech\nfeatures."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s00034-016-0310-y", 
    "link": "http://arxiv.org/pdf/1503.05471v1", 
    "title": "Shared latent subspace modelling within Gaussian-Binary Restricted   Boltzmann Machines for NIST i-Vector Challenge 2014", 
    "arxiv-id": "1503.05471v1", 
    "author": "Maxim Tkachenko", 
    "publish": "2015-03-18T16:28:18Z", 
    "summary": "This paper presents a novel approach to speaker subspace modelling based on\nGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is\nbased on the idea of shared factors as in the Probabilistic Linear Discriminant\nAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,\nherein the speaker factor is shared over all vectors of the speaker. Then\nMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.\nVarious new scoring techniques for speaker verification using GRBM are\nproposed. The results for NIST i-vector Challenge 2014 dataset are presented."
},{
    "category": "cs.SD", 
    "doi": "10.1007/s00034-016-0310-y", 
    "link": "http://arxiv.org/pdf/1503.05849v1", 
    "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic   Re-Synthesis", 
    "arxiv-id": "1503.05849v1", 
    "author": "Andrew J. R. Simpson", 
    "publish": "2015-03-19T17:24:16Z", 
    "summary": "In the process of recording, storage and transmission of time-domain audio\nsignals, errors may be introduced that are difficult to correct in an\nunsupervised way. Here, we train a convolutional deep neural network to\nre-synthesize input time-domain speech signals at its output layer. We then use\nthis abstract transformation, which we call a deep transform (DT), to perform\nprobabilistic re-synthesis on further speech (of the same speaker) which has\nbeen degraded. Using the convolutive DT, we demonstrate the recovery of speech\naudio that has been subject to extreme degradation. This approach may be useful\nfor correction of errors in communications devices."
},{
    "category": "cs.SD", 
    "doi": "10.1007/s00034-016-0310-y", 
    "link": "http://arxiv.org/pdf/1503.06046v1", 
    "title": "Deep Transform: Cocktail Party Source Separation via Probabilistic   Re-Synthesis", 
    "arxiv-id": "1503.06046v1", 
    "author": "Andrew J. R. Simpson", 
    "publish": "2015-03-20T12:00:44Z", 
    "summary": "In cocktail party listening scenarios, the human brain is able to separate\ncompeting speech signals. However, the signal processing implemented by the\nbrain to perform cocktail party listening is not well understood. Here, we\ntrained two separate convolutive autoencoder deep neural networks (DNN) to\nseparate monaural and binaural mixtures of two concurrent speech streams. We\nthen used these DNNs as convolutive deep transform (CDT) devices to perform\nprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Our\nsimulations demonstrate that very simple neural networks are capable of\nexploiting monaural and binaural information available in a cocktail party\nlistening scenario."
},{
    "category": "cs.IR", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1503.06666v3", 
    "title": "Using Generic Summarization to Improve Music Information Retrieval Tasks", 
    "arxiv-id": "1503.06666v3", 
    "author": "David Martins de Matos", 
    "publish": "2015-03-23T14:48:24Z", 
    "summary": "In order to satisfy processing time constraints, many MIR tasks process only\na segment of the whole music signal. This practice may lead to decreasing\nperformance, since the most important information for the tasks may not be in\nthose processed segments. In this paper, we leverage generic summarization\nalgorithms, previously applied to text and speech summarization, to summarize\nitems in music datasets. These algorithms build summaries, that are both\nconcise and diverse, by selecting appropriate segments from the input signal\nwhich makes them good candidates to summarize music as well. We evaluate the\nsummarization process on binary and multiclass music genre classification\ntasks, by comparing the performance obtained using summarized datasets against\nthe performances obtained using continuous segments (which is the traditional\nmethod used for addressing the previously mentioned time constraints) and full\nsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,\nMMR, and a Support Sets-based Centrality model improve classification\nperformance when compared to selected 30-second baselines. We also show that\nsummarized datasets lead to a classification performance whose difference is\nnot statistically significant from using full songs. Furthermore, we make an\nargument stating the advantages of sharing summarized datasets for future MIR\nresearch."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1503.06962v1", 
    "title": "Probabilistic Binary-Mask Cocktail-Party Source Separation in a   Convolutional Deep Neural Network", 
    "arxiv-id": "1503.06962v1", 
    "author": "Andrew J. R. Simpson", 
    "publish": "2015-03-24T09:34:51Z", 
    "summary": "Separation of competing speech is a key challenge in signal processing and a\nfeat routinely performed by the human auditory brain. A long standing benchmark\nof the spectrogram approach to source separation is known as the ideal binary\nmask. Here, we train a convolutional deep neural network, on a two-speaker\ncocktail party problem, to make probabilistic predictions about binary masks.\nOur results approach ideal binary mask performance, illustrating that\nrelatively simple deep neural networks are capable of robust binary mask\nprediction. We also illustrate the trade-off between prediction statistics and\nseparation quality."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1504.02945v1", 
    "title": "Deep Transform: Cocktail Party Source Separation via Complex Convolution   in a Deep Neural Network", 
    "arxiv-id": "1504.02945v1", 
    "author": "Andrew J. R. Simpson", 
    "publish": "2015-04-12T08:44:56Z", 
    "summary": "Convolutional deep neural networks (DNN) are state of the art in many\nengineering problems but have not yet addressed the issue of how to deal with\ncomplex spectrograms. Here, we use circular statistics to provide a convenient\nprobabilistic estimate of spectrogram phase in a complex convolutional DNN. In\na typical cocktail party source separation scenario, we trained a convolutional\nDNN to re-synthesize the complex spectrograms of two source speech signals\ngiven a complex spectrogram of the monaural mixture - a discriminative deep\ntransform (DT). We then used this complex convolutional DT to obtain\nprobabilistic estimates of the magnitude and phase components of the source\nspectrograms. Our separation results are on a par with equivalent binary-mask\nbased non-complex separation approaches."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1504.04658v1", 
    "title": "Deep Karaoke: Extracting Vocals from Musical Mixtures Using a   Convolutional Deep Neural Network", 
    "arxiv-id": "1504.04658v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2015-04-17T23:07:17Z", 
    "summary": "Identification and extraction of singing voice from within musical mixtures\nis a key challenge in source separation and machine audition. Recently, deep\nneural networks (DNN) have been used to estimate 'ideal' binary masks for\ncarefully controlled cocktail party speech separation problems. However, it is\nnot yet known whether these methods are capable of generalizing to the\ndiscrimination of voice and non-voice in the context of musical mixtures. Here,\nwe trained a convolutional DNN (of around a billion parameters) to provide\nprobabilistic estimates of the ideal binary mask for separation of vocal sounds\nfrom real-world musical mixtures. We contrast our DNN results with more\ntraditional linear methods. Our approach may be useful for automatic removal of\nvocal sounds from musical mixtures for 'karaoke' type applications."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1504.08177v2", 
    "title": "Noise Sensitivity of Teager-Kaiser Energy Operators and Their Ratios", 
    "arxiv-id": "1504.08177v2", 
    "author": "Nirmal B. Chakrabarti", 
    "publish": "2015-04-30T11:49:38Z", 
    "summary": "The Teager-Kaiser energy operator (TKO) belongs to a class of autocorrelators\nand their linear combination that can track the instantaneous energy of a\nnonstationary sinusoidal signal source. TKO-based monocomponent AM-FM\ndemodulation algorithms work under the basic assumption that the operator\noutputs are always positive. In the absence of noise, this is assured for pure\nsinusoidal inputs and the instantaneous property is also guaranteed. Noise\ninvalidates both of these, particularly under small signal conditions.\nPost-detection filtering and thresholding are of use to reestablish these at\nthe cost of some time to acquire. Key questions are: (a) how many samples must\none use and (b) how much noise power at the detector input can one tolerate.\nResults of study of the role of delay and the limits imposed by additive\nGaussian noise are presented along with the computation of the cumulants and\nprobability density functions of the individual quadratic forms and their\nratios."
},{
    "category": "stat.ML", 
    "doi": "10.1109/TASLP.2016.2541299", 
    "link": "http://arxiv.org/pdf/1505.06443v1", 
    "title": "Detecting bird sound in unknown acoustic background using crowdsourced   training data", 
    "arxiv-id": "1505.06443v1", 
    "author": "Kathy Willis", 
    "publish": "2015-05-24T14:58:41Z", 
    "summary": "Biodiversity monitoring using audio recordings is achievable at a truly\nglobal scale via large-scale deployment of inexpensive, unattended recording\nstations or by large-scale crowdsourcing using recording and species\nrecognition on mobile devices. The ability, however, to reliably identify\nvocalising animal species is limited by the fact that acoustic signatures of\ninterest in such recordings are typically embedded in a diverse and complex\nacoustic background. To avoid the problems associated with modelling such\nbackgrounds, we build generative models of bird sounds and use the concept of\nnovelty detection to screen recordings to detect sections of data which are\nlikely bird vocalisations. We present detection results against various\nacoustic environments and different signal-to-noise ratios. We discuss the\nissues related to selecting the cost function and setting detection thresholds\nin such algorithms. Our methods are designed to be scalable and automatically\napplicable to arbitrary selections of species depending on the specific\ngeographic region and time period of deployment."
},{
    "category": "cs.CL", 
    "doi": "10.5121/ijaia.2015.6304", 
    "link": "http://arxiv.org/pdf/1506.05012v1", 
    "title": "Emotion Analysis of Songs Based on Lyrical and Audio Features", 
    "arxiv-id": "1506.05012v1", 
    "author": "Rahul Dubey", 
    "publish": "2015-06-16T16:04:08Z", 
    "summary": "In this paper, a method is proposed to detect the emotion of a song based on\nits lyrical and audio features. Lyrical features are generated by segmentation\nof lyrics during the process of data extraction. ANEW and WordNet knowledge is\nthen incorporated to compute Valence and Arousal values. In addition to this,\nlinguistic association rules are applied to ensure that the issue of ambiguity\nis properly addressed. Audio features are used to supplement the lyrical ones\nand include attributes like energy, tempo, and danceability. These features are\nextracted from The Echo Nest, a widely used music intelligence platform.\nConstruction of training and test sets is done on the basis of social tags\nextracted from the last.fm website. The classification is done by applying\nfeature weighting and stepwise threshold reduction on the k-Nearest Neighbors\nalgorithm to provide fuzziness in the classification."
},{
    "category": "cs.SD", 
    "doi": "10.5121/ijaia.2015.6304", 
    "link": "http://arxiv.org/pdf/1506.06832v1", 
    "title": "Detection and Analysis of Emotion From Speech Signals", 
    "arxiv-id": "1506.06832v1", 
    "author": "Alex Pappachen James", 
    "publish": "2015-06-23T00:28:08Z", 
    "summary": "Recognizing emotion from speech has become one the active research themes in\nspeech processing and in applications based on human-computer interaction. This\npaper conducts an experimental study on recognizing emotions from human speech.\nThe emotions considered for the experiments include neutral, anger, joy and\nsadness. The distinuishability of emotional features in speech were studied\nfirst followed by emotion classification performed on a custom dataset. The\nclassification was performed for different classifiers. One of the main feature\nattribute considered in the prepared dataset was the peak-to-peak distance\nobtained from the graphical representation of the speech signals. After\nperforming the classification tests on a dataset formed from 30 different\nsubjects, it was found that for getting better accuracy, one should consider\nthe data collected from one person rather than considering the data from a\ngroup of people."
},{
    "category": "cs.LG", 
    "doi": "10.5121/ijaia.2015.6304", 
    "link": "http://arxiv.org/pdf/1507.04761v1", 
    "title": "Deep Learning and Music Adversaries", 
    "arxiv-id": "1507.04761v1", 
    "author": "Jan Larsen", 
    "publish": "2015-07-16T20:24:18Z", 
    "summary": "An adversary is essentially an algorithm intent on making a classification\nsystem perform in some particular way given an input, e.g., increase the\nprobability of a false negative. Recent work builds adversaries for deep\nlearning systems applied to image object recognition, which exploits the\nparameters of the system to find the minimal perturbation of the input image\nsuch that the network misclassifies it with high confidence. We adapt this\napproach to construct and deploy an adversary of deep learning systems applied\nto music content analysis. In our case, however, the input to the systems is\nmagnitude spectral frames, which requires special care in order to produce\nvalid input audio signals from network-derived perturbations. For two different\ntrain-test partitionings of two benchmark datasets, and two different deep\narchitectures, we find that this adversary is very effective in defeating the\nresulting systems. We find the convolutional networks are more robust, however,\ncompared with systems based on a majority vote over individually classified\naudio frames. Furthermore, we integrate the adversary into the training of new\ndeep systems, but do not find that this improves their resilience against the\nsame adversary."
},{
    "category": "cs.CV", 
    "doi": "10.1145/2733373.2806293", 
    "link": "http://arxiv.org/pdf/1507.04831v1", 
    "title": "Deep Multimodal Speaker Naming", 
    "arxiv-id": "1507.04831v1", 
    "author": "Wenping Wang", 
    "publish": "2015-07-17T04:13:12Z", 
    "summary": "Automatic speaker naming is the problem of localizing as well as identifying\neach speaking character in a TV/movie/live show video. This is a challenging\nproblem mainly attributes to its multimodal nature, namely face cue alone is\ninsufficient to achieve good performance. Previous multimodal approaches to\nthis problem usually process the data of different modalities individually and\nmerge them using handcrafted heuristics. Such approaches work well for simple\nscenes, but fail to achieve high performance for speakers with large appearance\nvariations. In this paper, we propose a novel convolutional neural networks\n(CNN) based learning framework to automatically learn the fusion function of\nboth face and audio cues. We show that without using face tracking, facial\nlandmark localization or subtitle/transcript, our system with robust multimodal\nfeature extraction is able to achieve state-of-the-art speaker naming\nperformance evaluated on two diverse TV series. The dataset and implementation\nof our algorithm are publicly available online."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2733373.2806293", 
    "link": "http://arxiv.org/pdf/1507.08074v1", 
    "title": "STC Anti-spoofing Systems for the ASVspoof 2015 Challenge", 
    "arxiv-id": "1507.08074v1", 
    "author": "Vadim Shchemelinin", 
    "publish": "2015-07-29T09:22:58Z", 
    "summary": "This paper presents the Speech Technology Center (STC) systems submitted to\nAutomatic Speaker Verification Spoofing and Countermeasures (ASVspoof)\nChallenge 2015. In this work we investigate different acoustic feature spaces\nto determine reliable and robust countermeasures against spoofing attacks. In\naddition to the commonly used front-end MFCC features we explored features\nderived from phase spectrum and features based on applying the multiresolution\nwavelet transform. Similar to state-of-the-art ASV systems, we used the\nstandard TV-JFA approach for probability modelling in spoofing detection\nsystems. Experiments performed on the development and evaluation datasets of\nthe Challenge demonstrate that the use of phase-related and wavelet-based\nfeatures provides a substantial input into the efficiency of the resulting STC\nsystems. In our research we also focused on the comparison of the linear (SVM)\nand nonlinear (DBN) classifiers."
},{
    "category": "stat.ML", 
    "doi": "10.1145/2733373.2806293", 
    "link": "http://arxiv.org/pdf/1508.01774v2", 
    "title": "An End-to-End Neural Network for Polyphonic Piano Music Transcription", 
    "arxiv-id": "1508.01774v2", 
    "author": "Simon Dixon", 
    "publish": "2015-08-07T18:16:32Z", 
    "summary": "We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications."
},{
    "category": "cs.LG", 
    "doi": "10.1145/2733373.2806293", 
    "link": "http://arxiv.org/pdf/1508.04999v3", 
    "title": "A Deep Bag-of-Features Model for Music Auto-Tagging", 
    "arxiv-id": "1508.04999v3", 
    "author": "Kyogu Lee", 
    "publish": "2015-08-20T14:38:56Z", 
    "summary": "Feature learning and deep learning have drawn great attention in recent years\nas a way of transforming input data into more effective representations using\nlearning algorithms. Such interest has grown in the area of music information\nretrieval (MIR) as well, particularly in music audio classification tasks such\nas auto-tagging. In this paper, we present a two-stage learning model to\neffectively predict multiple labels from music audio. The first stage learns to\nproject local spectral patterns of an audio track onto a high-dimensional\nsparse space in an unsupervised manner and summarizes the audio track as a\nbag-of-features. The second stage successively performs the unsupervised\nlearning on the bag-of-features in a layer-by-layer manner to initialize a deep\nneural network and finally fine-tunes it with the tag labels. Through the\nexperiment, we rigorously examine training choices and tuning parameters, and\nshow that the model achieves high performance on Magnatagatune, a popularly\nused dataset in music auto-tagging."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2733373.2806293", 
    "link": "http://arxiv.org/pdf/1509.00533v1", 
    "title": "Enhancement and Recognition of Reverberant and Noisy Speech by Extending   Its Coherence", 
    "arxiv-id": "1509.00533v1", 
    "author": "James Pitton", 
    "publish": "2015-09-02T00:31:40Z", 
    "summary": "Most speech enhancement algorithms make use of the short-time Fourier\ntransform (STFT), which is a simple and flexible time-frequency decomposition\nthat estimates the short-time spectrum of a signal. However, the duration of\nshort STFT frames are inherently limited by the nonstationarity of speech\nsignals. The main contribution of this paper is a demonstration of speech\nenhancement and automatic speech recognition in the presence of reverberation\nand noise by extending the length of analysis windows. We accomplish this\nextension by performing enhancement in the short-time fan-chirp transform\n(STFChT) domain, an overcomplete time-frequency representation that is coherent\nwith speech signals over longer analysis window durations than the STFT. This\nextended coherence is gained by using a linear model of fundamental frequency\nvariation of voiced speech signals. Our approach centers around using a\nsingle-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)\nestimator proposed by Habets, which scales coefficients in a time-frequency\ndomain to suppress noise and reverberation. In the case of multiple\nmicrophones, we preprocess the data with either a minimum variance\ndistortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).\nWe evaluate our algorithm on both speech enhancement and recognition tasks for\nthe REVERB challenge dataset. Compared to the same processing done in the STFT\ndomain, our approach achieves significant improvement in terms of objective\nenhancement metrics (including PESQ---the ITU-T standard measurement for speech\nquality). In terms of automatic speech recognition (ASR) performance as\nmeasured by word error rate (WER), our experiments indicate that the STFT with\na long window is more effective for ASR."
},{
    "category": "cs.SD", 
    "doi": "10.1007/s11045-016-0400-9", 
    "link": "http://arxiv.org/pdf/1509.02380v2", 
    "title": "Source localization and denoising: a perspective from the TDOA space", 
    "arxiv-id": "1509.02380v2", 
    "author": "Stefano Tubaro", 
    "publish": "2015-09-08T14:19:22Z", 
    "summary": "In this manuscript, we formulate the problem of denoising Time Differences of\nArrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA\nmeasurements. The method consists of pre-processing the TDOAs with the purpose\nof reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs\ncomputed at all microphone pairs) is known to form a redundant set, which lies\non a linear subspace in the TDOA space. Noise, however, prevents TDOAs from\nlying exactly on this subspace. We therefore show that TDOA denoising can be\nseen as a projection operation that suppresses the component of the noise that\nis orthogonal to that linear subspace. We then generalize the projection\noperator also to the cases where the set of TDOAs is incomplete. We\nanalytically show that this operator improves the localization accuracy, and we\nfurther confirm that via simulation."
},{
    "category": "cs.LG", 
    "doi": "10.1007/s11045-016-0400-9", 
    "link": "http://arxiv.org/pdf/1509.02409v1", 
    "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", 
    "arxiv-id": "1509.02409v1", 
    "author": "Thomas Hain", 
    "publish": "2015-09-08T15:20:12Z", 
    "summary": "Negative transfer in training of acoustic models for automatic speech\nrecognition has been reported in several contexts such as domain change or\nspeaker characteristics. This paper proposes a novel technique to overcome\nnegative transfer by efficient selection of speech data for acoustic model\ntraining. Here data is chosen on relevance for a specific target. A submodular\nfunction based on likelihood ratios is used to determine how acoustically\nsimilar each training utterance is to a target test set. The approach is\nevaluated on a wide-domain data set, covering speech from radio and TV\nbroadcasts, telephone conversations, meetings, lectures and read speech.\nExperiments demonstrate that the proposed technique both finds relevant data\nand limits negative transfer. Results on a 6--hour test set show a relative\nimprovement of 4% with data selection over using all data in PLP based models,\nand 2% with DNN features."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1509.06279v1", 
    "title": "Sports highlights generation based on acoustic events detection: A rugby   case study", 
    "arxiv-id": "1509.06279v1", 
    "author": "Byeong-Seob Ko", 
    "publish": "2015-09-18T12:47:09Z", 
    "summary": "We approach the challenging problem of generating highlights from sports\nbroadcasts utilizing audio information only. A language-independent,\nmulti-stage classification approach is employed for detection of key acoustic\nevents which then act as a platform for summarization of highlight scenes.\nObjective results and human experience indicate that our system is highly\nefficient."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1511.05520v1", 
    "title": "Automatic Instrument Recognition in Polyphonic Music Using Convolutional   Neural Networks", 
    "arxiv-id": "1511.05520v1", 
    "author": "Tian Wang", 
    "publish": "2015-11-17T19:43:53Z", 
    "summary": "Traditional methods to tackle many music information retrieval tasks\ntypically follow a two-step architecture: feature engineering followed by a\nsimple learning algorithm. In these \"shallow\" architectures, feature\nengineering and learning are typically disjoint and unrelated. Additionally,\nfeature engineering is difficult, and typically depends on extensive domain\nexpertise.\n  In this paper, we present an application of convolutional neural networks for\nthe task of automatic musical instrument identification. In this model, feature\nextraction and learning algorithms are trained together in an end-to-end\nfashion. We show that a convolutional neural network trained on raw audio can\nachieve performance surpassing traditional methods that rely on hand-crafted\nfeatures."
},{
    "category": "cs.LG", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1511.07035v2", 
    "title": "Detecting Road Surface Wetness from Audio: A Deep Learning Approach", 
    "arxiv-id": "1511.07035v2", 
    "author": "Bj\u00f6rn Schuller", 
    "publish": "2015-11-22T17:20:23Z", 
    "summary": "We introduce a recurrent neural network architecture for automated road\nsurface wetness detection from audio of tire-surface interaction. The\nrobustness of our approach is evaluated on 785,826 bins of audio that span an\nextensive range of vehicle speeds, noises from the environment, road surface\ntypes, and pavement conditions including international roughness index (IRI)\nvalues from 25 in/mi to 1400 in/mi. The training and evaluation of the model\nare performed on different roads to minimize the impact of environmental and\nother external factors on the accuracy of the classification. We achieve an\nunweighted average recall (UAR) of 93.2% across all vehicle speeds including 0\nmph. The classifier still works at 0 mph because the discriminating signal is\npresent in the sound of other vehicles driving by."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1512.04243v1", 
    "title": "Trigonometric dictionary based codec for music compression with high   quality recovery", 
    "arxiv-id": "1512.04243v1", 
    "author": "Laura Rebollo-Neira", 
    "publish": "2015-12-14T10:14:53Z", 
    "summary": "A codec for compression of music signals is proposed. The method belongs to\nthe class of transform lossy compression. It is conceived to be applied in the\nhigh quality recovery range though. The transformation, endowing the codec with\nits distinctive feature, relies on the ability to construct high quality sparse\napproximation of music signals. This is achieved by a redundant trigonometric\ndictionary and a dedicated pursuit strategy. The potential of the approach is\nillustrated by comparison with the OGG Vorbis format, on a sample consisting of\nclips of melodic music. The comparison evidences remarkable improvements in\ncompression performance for the identical quality of the decompressed signal."
},{
    "category": "stat.ML", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1512.05073v1", 
    "title": "A Novel Minimum Divergence Approach to Robust Speaker Identification", 
    "arxiv-id": "1512.05073v1", 
    "author": "Debasmita Das", 
    "publish": "2015-12-16T07:29:53Z", 
    "summary": "In this work, a novel solution to the speaker identification problem is\nproposed through minimization of statistical divergences between the\nprobability distribution (g). of feature vectors from the test utterance and\nthe probability distributions of the feature vector corresponding to the\nspeaker classes. This approach is made more robust to the presence of outliers,\nthrough the use of suitably modified versions of the standard divergence\nmeasures. The relevant solutions to the minimum distance methods are referred\nto as the minimum rescaled modified distance estimators (MRMDEs). Three\nmeasures were considered - the likelihood disparity, the Hellinger distance and\nPearson's chi-square distance. The proposed approach is motivated by the\nobservation that, in the case of the likelihood disparity, when the empirical\ndistribution function is used to estimate g, it becomes equivalent to maximum\nlikelihood classification with Gaussian Mixture Models (GMMs) for speaker\nclasses, a highly effective approach used, for example, by Reynolds [22] based\non Mel Frequency Cepstral Coefficients (MFCCs) as features. Significant\nimprovement in classification accuracy is observed under this approach on the\nbenchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, with\nMFCC features, both in isolation and in combination with delta MFCC features.\nMoreover, the ubiquitous principal component transformation, by itself and in\nconjunction with the principle of classifier combination, is found to further\nenhance the performance."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICCE.2015.7066303", 
    "link": "http://arxiv.org/pdf/1512.06222v1", 
    "title": "A new robust adaptive algorithm for underwater acoustic channel   equalization", 
    "arxiv-id": "1512.06222v1", 
    "author": "Suleyman Serdar Kozat", 
    "publish": "2015-12-19T10:27:59Z", 
    "summary": "We introduce a novel family of adaptive robust equalizers for highly\nchallenging underwater acoustic (UWA) channel equalization. Since the\nunderwater environment is highly non-stationary and subjected to impulsive\nnoise, we use adaptive filtering techniques based on a relative logarithmic\ncost function inspired by the competitive methods from the online learning\nliterature. To improve the convergence performance of the conventional linear\nequalization methods, while mitigating the stability issues, we intrinsically\ncombine different norms of the error in the cost function, using logarithmic\nfunctions. Hence, we achieve a comparable convergence performance to least mean\nfourth (LMF) equalizer, while significantly enhancing the stability performance\nin such an adverse communication medium. We demonstrate the performance of our\nalgorithms through highly realistic experiments performed on accurately\nsimulated underwater acoustic channels."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2015.2507862", 
    "link": "http://arxiv.org/pdf/1512.07748v1", 
    "title": "Real-Time Audio-to-Score Alignment of Music Performances Containing   Errors and Arbitrary Repeats and Skips", 
    "arxiv-id": "1512.07748v1", 
    "author": "Shigeki Sagayama", 
    "publish": "2015-12-24T08:21:48Z", 
    "summary": "This paper discusses real-time alignment of audio signals of music\nperformance to the corresponding score (a.k.a. score following) which can\nhandle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)\nin performances. This type of score following is particularly useful in\nautomatic accompaniment for practices and rehearsals, where errors and\nrepeats/skips are often made. Simple extensions of the algorithms previously\nproposed in the literature are not applicable in these situations for scores of\npractical length due to the problem of large computational complexity. To cope\nwith this problem, we present two hidden Markov models of monophonic\nperformance with errors and arbitrary repeats/skips, and derive efficient\nscore-following algorithms with an assumption that the prior probability\ndistributions of score positions before and after repeats/skips are independent\nfrom each other. We confirmed real-time operation of the algorithms with music\nscores of practical length (around 10000 notes) on a modern laptop and their\ntracking ability to the input performance within 0.7 s on average after\nrepeats/skips in clarinet performance data. Further improvements and extension\nfor polyphonic signals are also discussed."
},{
    "category": "cs.CV", 
    "doi": "10.1109/TASLP.2015.2507862", 
    "link": "http://arxiv.org/pdf/1512.08512v2", 
    "title": "Visually Indicated Sounds", 
    "arxiv-id": "1512.08512v2", 
    "author": "William T. Freeman", 
    "publish": "2015-12-28T20:56:50Z", 
    "summary": "Objects make distinctive sounds when they are hit or scratched. These sounds\nreveal aspects of an object's material properties, as well as the actions that\nproduced them. In this paper, we propose the task of predicting what sound an\nobject makes when struck as a way of studying physical interactions within a\nvisual scene. We present an algorithm that synthesizes sound from silent videos\nof people hitting and scratching objects with a drumstick. This algorithm uses\na recurrent neural network to predict sound features from videos and then\nproduces a waveform from these features with an example-based synthesis\nprocedure. We show that the sounds predicted by our model are realistic enough\nto fool participants in a \"real or fake\" psychophysical experiment, and that\nthey convey significant information about material properties and physical\ninteractions."
},{
    "category": "cs.LG", 
    "doi": "10.1109/TASLP.2015.2507862", 
    "link": "http://arxiv.org/pdf/1601.01218v1", 
    "title": "Adaptive and Efficient Nonlinear Channel Equalization for Underwater   Acoustic Communication", 
    "arxiv-id": "1601.01218v1", 
    "author": "Suleyman Serdar Kozat", 
    "publish": "2016-01-06T15:51:54Z", 
    "summary": "We investigate underwater acoustic (UWA) channel equalization and introduce\nhierarchical and adaptive nonlinear channel equalization algorithms that are\nhighly efficient and provide significantly improved bit error rate (BER)\nperformance. Due to the high complexity of nonlinear equalizers and poor\nperformance of linear ones, to equalize highly difficult underwater acoustic\nchannels, we employ piecewise linear equalizers. However, in order to achieve\nthe performance of the best piecewise linear model, we use a tree structure to\nhierarchically partition the space of the received signal. Furthermore, the\nequalization algorithm should be completely adaptive, since due to the highly\nnon-stationary nature of the underwater medium, the optimal MSE equalizer as\nwell as the best piecewise linear equalizer changes in time. To this end, we\nintroduce an adaptive piecewise linear equalization algorithm that not only\nadapts the linear equalizer at each region but also learns the complete\nhierarchical structure with a computational complexity only polynomial in the\nnumber of nodes of the tree. Furthermore, our algorithm is constructed to\ndirectly minimize the final squared error without introducing any ad-hoc\nparameters. We demonstrate the performance of our algorithms through highly\nrealistic experiments performed on accurately simulated underwater acoustic\nchannels."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2560534", 
    "link": "http://arxiv.org/pdf/1601.02828v2", 
    "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model   Adaptation", 
    "arxiv-id": "1601.02828v2", 
    "author": "Steve Renals", 
    "publish": "2016-01-12T12:33:56Z", 
    "summary": "This work presents a broad study on the adaptation of neural network acoustic\nmodels by means of learning hidden unit contributions (LHUC) -- a method that\nlinearly re-combines hidden units in a speaker- or environment-dependent manner\nusing small amounts of unsupervised adaptation data. We also extend LHUC to a\nspeaker adaptive training (SAT) framework that leads to a more adaptable DNN\nacoustic model, working both in a speaker-dependent and a speaker-independent\nmanner, without the requirements to maintain auxiliary speaker-dependent\nfeature extractors or to introduce significant speaker-dependent changes to the\nDNN structure. Through a series of experiments on four different speech\nrecognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4)\ncomprising 270 test speakers, we show that LHUC in both its test-only and SAT\nvariants results in consistent word error rate reductions ranging from 5% to\n23% relative depending on the task and the degree of mismatch between training\nand test data. In addition, we have investigated the effect of the amount of\nadaptation data per speaker, the quality of unsupervised adaptation targets,\nthe complementarity to other adaptation techniques, one-shot adaptation, and an\nextension to adapting DNNs trained in a sequence discriminative manner."
},{
    "category": "q-bio.NC", 
    "doi": "10.1371/journal.pone.0159188", 
    "link": "http://arxiv.org/pdf/1601.06248v1", 
    "title": "Automatic recognition of element classes and boundaries in the birdsong   with variable sequences", 
    "arxiv-id": "1601.06248v1", 
    "author": "Kazuo Okanoya", 
    "publish": "2016-01-23T07:57:56Z", 
    "summary": "Researches on sequential vocalization often require analysis of vocalizations\nin long continuous sounds. In such studies as developmental ones or studies\nacross generations in which days or months of vocalizations must be analyzed,\nmethods for automatic recognition would be strongly desired. Although methods\nfor automatic speech recognition for application purposes have been intensively\nstudied, blindly applying them for biological purposes may not be an optimal\nsolution. This is because, unlike human speech recognition, analysis of\nsequential vocalizations often requires accurate extraction of timing\ninformation. In the present study we propose automated systems suitable for\nrecognizing birdsong, one of the most intensively investigated sequential\nvocalizations, focusing on the three properties of the birdsong. First, a song\nis a sequence of vocal elements, called notes, which can be grouped into\ncategories. Second, temporal structure of birdsong is precisely controlled,\nmeaning that temporal information is important in song analysis. Finally, notes\nare produced according to certain probabilistic rules, which may facilitate the\naccurate song recognition. We divided the procedure of song recognition into\nthree sub-steps: local classification, boundary detection, and global\nsequencing, each of which corresponds to each of the three properties of\nbirdsong. We compared the performances of several different ways to arrange\nthese three steps. As results, we demonstrated a hybrid model of a deep neural\nnetwork and a hidden Markov model is effective in recognizing birdsong with\nvariable note sequences. We propose suitable arrangements of methods according\nto whether accurate boundary detection is needed. Also we designed the new\nmeasure to jointly evaluate the accuracy of note classification and boundary\ndetection. Our methods should be applicable, with small modification and\ntuning, to the songs in other species that hold the three properties of the\nsequential vocalization."
},{
    "category": "cs.SD", 
    "doi": "10.1371/journal.pone.0159188", 
    "link": "http://arxiv.org/pdf/1601.07709v1", 
    "title": "Categorization of Stringed Instruments with Multifractal Detrended   Fluctuation Analysis", 
    "arxiv-id": "1601.07709v1", 
    "author": "Dipak Ghosh", 
    "publish": "2016-01-28T09:50:07Z", 
    "summary": "Categorization is crucial for content description in archiving of music\nsignals. On many occasions, human brain fails to classify the instruments\nproperly just by listening to their sounds which is evident from the human\nresponse data collected during our experiment. Some previous attempts to\ncategorize several musical instruments using various linear analysis methods\nrequired a number of parameters to be determined. In this work, we attempted to\ncategorize a number of string instruments according to their mode of playing\nusing latest-state-of-the-art robust non-linear methods. For this, 30 second\nsound signals of 26 different string instruments from all over the world were\nanalyzed with the help of non linear multifractal analysis (MFDFA) technique.\nThe spectral width obtained from the MFDFA method gives an estimate of the\ncomplexity of the signal. From the variation of spectral width, we observed\ndistinct clustering among the string instruments according to their mode of\nplaying. Also there is an indication that similarity in the structural\nconfiguration of the instruments is playing a major role in the clustering of\ntheir spectral width. The observations and implications are discussed in\ndetail."
},{
    "category": "cs.SD", 
    "doi": "10.1371/journal.pone.0159188", 
    "link": "http://arxiv.org/pdf/1602.00739v1", 
    "title": "Towards a topological fingerprint of music", 
    "arxiv-id": "1602.00739v1", 
    "author": "Barbara Di Fabio", 
    "publish": "2016-02-01T23:00:45Z", 
    "summary": "Can music be represented as a meaningful geometric and topological object? In\nthis paper, we propose a strategy to describe some music features as a\npolyhedral surface obtained by a simplicial interpretation of the\n\\textit{Tonnetz}. The \\textit{Tonnetz} is a graph largely used in computational\nmusicology to describe the harmonic relationships of notes in equal tuning. In\nparticular, we use persistent homology in order to describe the\n\\textit{persistent} properties of music encoded in the aforementioned model.\nBoth the relevance and the characteristics of this approach are discussed by\nanalyzing some paradigmatic compositional styles. Eventually, the task of\nautomatic music style classification is addressed by computing the hierarchical\nclustering of the topological fingerprints associated with some collections of\ncompositions."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TBME.2016.2587382", 
    "link": "http://arxiv.org/pdf/1602.05702v3", 
    "title": "EEG-informed attended speaker extraction from recorded speech mixtures   with application in neuro-steered hearing prostheses", 
    "arxiv-id": "1602.05702v3", 
    "author": "Alexander Bertrand", 
    "publish": "2016-02-18T07:32:00Z", 
    "summary": "OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,\ntwo-speaker acoustic scenario, relying on microphone array recordings from a\nbinaural hearing aid, which are complemented with electroencephalography (EEG)\nrecordings to infer the speaker of interest. METHODS: In this study, we propose\na modular processing flow that first extracts the two speech envelopes from the\nmicrophone recordings, then selects the attended speech envelope based on the\nEEG, and finally uses this envelope to inform a multi-channel speech separation\nand denoising algorithm. RESULTS: Strong suppression of interfering\n(unattended) speech and background noise is achieved, while the attended speech\nis preserved. Furthermore, EEG-based auditory attention detection (AAD) is\nshown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results\nshow that AAD-based speaker extraction from microphone array recordings is\nfeasible and robust, even in noisy acoustic environments, and without access to\nthe clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current\nresearch on AAD always assumes the availability of the clean speech signals,\nwhich limits the applicability in real settings. We have extended this research\nto detect the attended speaker even when only microphone recordings with noisy\nspeech mixtures are available. This is an enabling ingredient for new\nbrain-computer interfaces and effective filtering schemes in neuro-steered\nhearing prostheses. Here, we provide a first proof of concept for EEG-informed\nattended speaker extraction and denoising."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2551865", 
    "link": "http://arxiv.org/pdf/1602.06727v3", 
    "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using   Stacked Bottleneck Features and Minimum Generation Error Training", 
    "arxiv-id": "1602.06727v3", 
    "author": "Simon King", 
    "publish": "2016-02-22T11:11:04Z", 
    "summary": "We propose two novel techniques --- stacking bottleneck features and minimum\ngeneration error training criterion --- to improve the performance of deep\nneural network (DNN)-based speech synthesis. The techniques address the related\nissues of frame-by-frame independence and ignorance of the relationship between\nstatic and dynamic features, within current typical DNN-based synthesis\nframeworks. Stacking bottleneck features, which are an acoustically--informed\nlinguistic representation, provides an efficient way to include more detailed\nlinguistic context at the input. The minimum generation error training\ncriterion minimises overall output trajectory error across an utterance, rather\nthan minimising the error per frame independently, and thus takes into account\nthe interaction between static and dynamic features. The two techniques can be\neasily combined to further improve performance. We present both objective and\nsubjective results that demonstrate the effectiveness of the proposed\ntechniques. The subjective results show that combining the two techniques leads\nto significantly more natural synthetic speech than from conventional DNN or\nlong short-term memory (LSTM) recurrent neural network (RNN) systems."
},{
    "category": "cs.CL", 
    "doi": "10.1109/TASLP.2016.2551865", 
    "link": "http://arxiv.org/pdf/1602.06967v1", 
    "title": "Blind score normalization method for PLDA based speaker recognition", 
    "arxiv-id": "1602.06967v1", 
    "author": "Mikhail Kotov", 
    "publish": "2016-02-22T21:22:49Z", 
    "summary": "Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art\nmethod for modeling $i$-vector space in speaker recognition task. However the\nperformance degradation is observed if enrollment data size differs from one\nspeaker to another. This paper presents a solution to such problem by\nintroducing new PLDA scoring normalization technique. Normalization parameters\nare derived in a blind way, so that, unlike traditional \\textit{ZT-norm}, no\nextra development data is required. Moreover, proposed method has shown to be\noptimal in terms of detection cost function. The experiments conducted on NIST\nSRE 2014 database demonstrate an improved accuracy in a mixed enrollment number\ncondition."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2551865", 
    "link": "http://arxiv.org/pdf/1602.07291v1", 
    "title": "The IBM 2016 Speaker Recognition System", 
    "arxiv-id": "1602.07291v1", 
    "author": "Jason W. Pelecanos", 
    "publish": "2016-02-23T20:39:40Z", 
    "summary": "In this paper we describe the recent advancements made in the IBM i-vector\nspeaker recognition system for conversational speech. In particular, we\nidentify key techniques that contribute to significant improvements in\nperformance of our system, and quantify their contributions. The techniques\ninclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that is\nformulated to alleviate some of the limitations associated with the\nconventional linear discriminant analysis (LDA) that assumes Gaussian\nclass-conditional distributions, 2) the application of speaker- and\nchannel-adapted features, which are derived from an automatic speech\nrecognition (ASR) system, for speaker recognition, and 3) the use of a deep\nneural network (DNN) acoustic model with a large number of output units (~10k\nsenones) to compute the frame-level soft alignments required in the i-vector\nestimation process. We evaluate these techniques on the NIST 2010 speaker\nrecognition evaluation (SRE) extended core conditions involving telephone and\nmicrophone trials. Experimental results indicate that: 1) the NDA is more\neffective (up to 35% relative improvement in terms of EER) than the traditional\nparametric LDA for speaker recognition, 2) when compared to raw acoustic\nfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains in\nspeaker recognition performance, and 3) increasing the number of output units\nin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)\nprovides consistent improvements in performance (for example from 37% to 57%\nrelative EER gains over our baseline GMM i-vector system). To our knowledge,\nresults reported in this paper represent the best performances published to\ndate on the NIST SRE 2010 extended core tasks."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1602.08128v1", 
    "title": "PCA Method for Automated Detection of Mispronounced Words", 
    "arxiv-id": "1602.08128v1", 
    "author": "Mark J. T. Smith", 
    "publish": "2016-02-25T21:48:56Z", 
    "summary": "This paper presents a method for detecting mispronunciations with the aim of\nimproving Computer Assisted Language Learning (CALL) tools used by foreign\nlanguage learners. The algorithm is based on Principle Component Analysis\n(PCA). It is hierarchical with each successive step refining the estimate to\nclassify the test word as being either mispronounced or correct. Preprocessing\nbefore detection, like normalization and time-scale modification, is\nimplemented to guarantee uniformity of the feature vectors input to the\ndetection system. The performance using various features including spectrograms\nand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.\nBest results were obtained using MFCCs, achieving up to 99% accuracy in word\nverification and 93% in native/non-native classification. Compared with Hidden\nMarkov Models (HMMs) which are used pervasively in recognition application,\nthis particular approach is computational efficient and effective when training\ndata is limited."
},{
    "category": "cs.IT", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.01340v1", 
    "title": "OFDM demodulation using virtual time reversal processing in underwater   acoustic communication", 
    "arxiv-id": "1603.01340v1", 
    "author": "Yue Yang", 
    "publish": "2016-03-04T03:57:31Z", 
    "summary": "The extremely long underwater channel delay spread causes severe inter-symbol\ninterference (ISI) for underwater acoustic communications. Passive time\nreversal processing (PTRP) can effectively reduce the channel time dispersion\nin a simple way via convolving the received packet with a time reversed probe\nsignal. However the probe signal itself may introduce extra noise and\ninterference (self-correlation of the probe signal). In this paper, we propose\na virtual time reversal processing (VTRP) for single input single output (SISO)\nOrthogonal Frequency Division Multiplexing (OFDM) systems. It convolves the\nreceived packet with the reversed estimated channel, instead of the probe\nsignal to reduce the interference. Two sparse channel estimation methods,\nmatching pursuit (MP), and basis pursuit de-noising (BPDN), are adopted to\nestimate the channel impulse response (CIR). We compare the performance of VTRP\nwith the PTRP and without any time reversal processing through MATLAB\nsimulations and the pool experiments. The results reveal that VTRP has\noutstanding performance over time-invariant channels."
},{
    "category": "cs.CL", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.03185v2", 
    "title": "Personalized Speech recognition on mobile devices", 
    "arxiv-id": "1603.03185v2", 
    "author": "Carolina Parada", 
    "publish": "2016-03-10T08:51:51Z", 
    "summary": "We describe a large vocabulary speech recognition system that is accurate,\nhas low latency, and yet has a small enough memory and computational footprint\nto run faster than real-time on a Nexus 5 Android smartphone. We employ a\nquantized Long Short-Term Memory (LSTM) acoustic model trained with\nconnectionist temporal classification (CTC) to directly predict phoneme\ntargets, and further reduce its memory footprint using an SVD-based compression\nscheme. Additionally, we minimize our memory footprint by using a single\nlanguage model for both dictation and voice command domains, constructed using\nBayesian interpolation. Finally, in order to properly handle device-specific\ninformation, such as proper names and other context-dependent information, we\ninject vocabulary items into the decoder graph and bias the language model\non-the-fly. Our system achieves 13.5% word error rate on an open-ended\ndictation task, running with a median speed that is seven times faster than\nreal-time."
},{
    "category": "cs.SD", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.04179v2", 
    "title": "Scale-Invariant Reconstruction of Source Images in Blind Source   Separation", 
    "arxiv-id": "1603.04179v2", 
    "author": "Francesco Nesta", 
    "publish": "2016-03-14T09:42:28Z", 
    "summary": "Blind methods often separate or identify signals or signal subspaces up to an\nunknown scaling factor. Sometimes it is necessary to cope with the scaling\nambiguity, which can be done through reconstructing signals as they are\nreceived by sensors, because scales of the sensor responses (images) have known\nphysical interpretations. In this paper, we analyze two approaches that are\nwidely used for computing the sensor responses, each of which provides an\nestimator that is invariant to the unknown scaling. One approach is the\nleast-squares projection, while the other one assumes a regular mixing matrix\nand computes its inverse. We compare the estimators through a theoretical\nstudy, perturbation analysis and simulations, and show some applications."
},{
    "category": "cs.NE", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.05824v1", 
    "title": "Comparing Time and Frequency Domain for Audio Event Recognition Using   Deep Learning", 
    "arxiv-id": "1603.05824v1", 
    "author": "Alfred Mertins", 
    "publish": "2016-03-18T10:38:23Z", 
    "summary": "Recognizing acoustic events is an intricate problem for a machine and an\nemerging field of research. Deep neural networks achieve convincing results and\nare currently the state-of-the-art approach for many tasks. One advantage is\ntheir implicit feature learning, opposite to an explicit feature extraction of\nthe input signal. In this work, we analyzed whether more discriminative\nfeatures can be learned from either the time-domain or the frequency-domain\nrepresentation of the audio signal. For this purpose, we trained multiple deep\nnetworks with different architectures on the Freiburg-106 and ESC-10 datasets.\nOur results show that feature learning from the frequency domain is superior to\nthe time domain. Moreover, additionally using convolution and pooling layers,\nto explore local structures of the audio signal, significantly improves the\nrecognition performance and achieves state-of-the-art results."
},{
    "category": "cs.SI", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.07813v1", 
    "title": "Chatty Maps: Constructing sound maps of urban areas from social media   data", 
    "arxiv-id": "1603.07813v1", 
    "author": "Francesco Aletta", 
    "publish": "2016-03-25T03:33:46Z", 
    "summary": "Urban sound has a huge influence over how we perceive places. Yet, city\nplanning is concerned mainly with noise, simply because annoying sounds come to\nthe attention of city officials in the form of complaints, while general urban\nsounds do not come to the attention as they cannot be easily captured at city\nscale. To capture both unpleasant and pleasant sounds, we applied a new\nmethodology that relies on tagging information of geo-referenced pictures to\nthe cities of London and Barcelona. To begin with, we compiled the first urban\nsound dictionary and compared it to the one produced by collating insights from\nthe literature: ours was experimentally more valid (if correlated with official\nnoise pollution levels) and offered a wider geographic coverage. From picture\ntags, we then studied the relationship between soundscapes and emotions. We\nlearned that streets with music sounds were associated with strong emotions of\njoy or sadness, while those with human sounds were associated with joy or\nsurprise. Finally, we studied the relationship between soundscapes and people's\nperceptions and, in so doing, we were able to map which areas are chaotic,\nmonotonous, calm, and exciting.Those insights promise to inform the creation of\nrestorative experiences in our increasingly urbanized world."
},{
    "category": "cs.CL", 
    "doi": "10.1117/12.884155", 
    "link": "http://arxiv.org/pdf/1603.09509v2", 
    "title": "Learning Multiscale Features Directly From Waveforms", 
    "arxiv-id": "1603.09509v2", 
    "author": "Awni Hannun", 
    "publish": "2016-03-31T09:54:44Z", 
    "summary": "Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2016.7472917", 
    "link": "http://arxiv.org/pdf/1604.00861v1", 
    "title": "Recurrent Neural Networks for Polyphonic Sound Event Detection in Real   Life Recordings", 
    "arxiv-id": "1604.00861v1", 
    "author": "Tuomas Virtanen", 
    "publish": "2016-04-04T13:54:09Z", 
    "summary": "In this paper we present an approach to polyphonic sound event detection in\nreal life recordings based on bi-directional long short term memory (BLSTM)\nrecurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to\nmap acoustic features of a mixture signal consisting of sounds from multiple\nclasses, to binary activity indicators of each event class. Our method is\ntested on a large database of real-life recordings, with 61 classes (e.g.\nmusic, car, speech) from 10 different everyday contexts. The proposed method\noutperforms previous approaches by a large margin, and the results are further\nimproved using data augmentation techniques. Overall, our system reports an\naverage F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a\nrelative improvement over previous state-of-the-art approach of 6.8% and 15.1%\nrespectively."
},{
    "category": "cs.NE", 
    "doi": "10.1109/ICASSP.2016.7472917", 
    "link": "http://arxiv.org/pdf/1604.06338v2", 
    "title": "Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural   Networks", 
    "arxiv-id": "1604.06338v2", 
    "author": "Alfred Mertins", 
    "publish": "2016-04-21T14:51:43Z", 
    "summary": "We present in this paper a simple, yet efficient convolutional neural network\n(CNN) architecture for robust audio event recognition. Opposing to deep CNN\narchitectures with multiple convolutional and pooling layers topped up with\nmultiple fully connected layers, the proposed network consists of only three\nlayers: convolutional, pooling, and softmax layer. Two further features\ndistinguish it from the deep architectures that have been proposed for the\ntask: varying-size convolutional filters at the convolutional layer and 1-max\npooling scheme at the pooling layer. In intuition, the network tends to select\nthe most discriminative features from the whole audio signals for recognition.\nOur proposed CNN not only shows state-of-the-art performance on the standard\ntask of robust audio event recognition but also outperforms other deep\narchitectures up to 4.5% in terms of recognition accuracy, which is equivalent\nto 76.3% relative error reduction."
},{
    "category": "math.AG", 
    "doi": "10.1007/s00332-016-9327-4", 
    "link": "http://arxiv.org/pdf/1604.08076v2", 
    "title": "The algebro-geometric study of range maps", 
    "arxiv-id": "1604.08076v2", 
    "author": "Augusto Sarti", 
    "publish": "2016-04-27T14:08:14Z", 
    "summary": "Localizing a radiant source is a widespread problem to many scientific and\ntechnological research areas. E.g. localization based on range measurements\nstays at the core of technologies like radar, sonar and wireless sensors\nnetworks. In this manuscript we study in depth the model for source\nlocalization based on range measurements obtained from the source signal, from\nthe point of view of algebraic geometry. In the case of three receivers, we\nfind unexpected connections between this problem and the geometry of Kummer's\nand Cayley's surfaces. Our work gives new insights also on the localization\nbased on range differences."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICASSP.2016.7471667", 
    "link": "http://arxiv.org/pdf/1604.08716v1", 
    "title": "Learning Compact Structural Representations for Audio Events Using   Regressor Banks", 
    "arxiv-id": "1604.08716v1", 
    "author": "Alfred Mertins", 
    "publish": "2016-04-29T07:46:59Z", 
    "summary": "We introduce a new learned descriptor for audio signals which is efficient\nfor event representation. The entries of the descriptor are produced by\nevaluating a set of regressors on the input signal. The regressors are\nclass-specific and trained using the random regression forests framework. Given\nan input signal, each regressor estimates the onset and offset positions of the\ntarget event. The estimation confidence scores output by a regressor are then\nused to quantify how the target event aligns with the temporal structure of the\ncorresponding category. Our proposed descriptor has two advantages. First, it\nis compact, i.e. the dimensionality of the descriptor is equal to the number of\nevent classes. Second, we show that even simple linear classification models,\ntrained on our descriptor, yield better accuracies on audio event\nclassification task than not only the nonlinear baselines but also the\nstate-of-the-art results."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICASSP.2016.7471667", 
    "link": "http://arxiv.org/pdf/1605.01635v1", 
    "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis", 
    "arxiv-id": "1605.01635v1", 
    "author": "Sriram Ganapathy", 
    "publish": "2016-05-05T15:57:21Z", 
    "summary": "We present the recent advances along with an error analysis of the IBM\nspeaker recognition system for conversational speech. Some of the key\nadvancements that contribute to our system include: a nearest-neighbor\ndiscriminant analysis (NDA) approach (as opposed to LDA) for intersession\nvariability compensation in the i-vector space, the application of speaker and\nchannel-adapted features derived from an automatic speech recognition (ASR)\nsystem for speaker recognition, and the use of a DNN acoustic model with a very\nlarge number of output units (~10k senones) to compute the frame-level soft\nalignments required in the i-vector estimation process. We evaluate these\ntechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as\nthe 10sec-10sec condition. To our knowledge, results achieved by our system\nrepresent the best performances published to date on these conditions. For\nexample, on the extended tel-tel condition (C5) the system achieves an EER of\n0.59%. To garner further understanding of the remaining errors (on C5), we\nexamine the recordings associated with the low scoring target trials, where\nvarious issues are identified for the problematic recordings/trials.\nInterestingly, it is observed that correcting the pathological recordings not\nonly improves the scores for the target trials but also for the nontarget\ntrials."
},{
    "category": "physics.data-an", 
    "doi": "10.1109/ICASSP.2016.7471667", 
    "link": "http://arxiv.org/pdf/1605.01805v2", 
    "title": "Wave-shape function analysis -- when cepstrum meets time-frequency   analysis", 
    "arxiv-id": "1605.01805v2", 
    "author": "Hau-tieng Wu", 
    "publish": "2016-05-06T03:00:28Z", 
    "summary": "We propose to combine cepstrum and nonlinear time-frequency (TF) analysis to\nstudy mutiple component oscillatory signals with time-varying frequency and\namplitude and with time-varying non-sinusoidal oscillatory pattern. The concept\nof cepstrum is applied to eliminate the wave-shape function influence on the TF\nanalysis, and we propose a new algorithm, named de-shape synchrosqueezing\ntransform (de-shape SST). The mathematical model, adaptive non-harmonic model,\nis introduced and the de-shape SST algorithm is theoretically analyzed. In\naddition to simulated signals, several different physiological, musical and\nbiological signals are analyzed to illustrate the proposed algorithm."
},{
    "category": "cs.SD", 
    "doi": "10.1145/2964284.2964310", 
    "link": "http://arxiv.org/pdf/1605.02401v3", 
    "title": "Audio Event Detection using Weakly Labeled Data", 
    "arxiv-id": "1605.02401v3", 
    "author": "Bhiksha Raj", 
    "publish": "2016-05-09T02:17:12Z", 
    "summary": "Acoustic event detection is essential for content analysis and description of\nmultimedia recordings. The majority of current literature on the topic learns\nthe detectors through fully-supervised techniques employing strongly labeled\ndata. However, the labels available for majority of multimedia data are\ngenerally weak and do not provide sufficient detail for such methods to be\nemployed. In this paper we propose a framework for learning acoustic event\ndetectors using only weakly labeled data. We first show that audio event\ndetection using weak labels can be formulated as an Multiple Instance Learning\nproblem. We then suggest two frameworks for solving multiple-instance learning,\none based on support vector machines, and the other on neural networks. The\nproposed methods can help in removing the time consuming and expensive process\nof manually annotating data to facilitate fully supervised learning. Moreover,\nit can not only detect events in a recording but can also provide temporal\nlocations of events in the recording. This helps in obtaining a complete\ndescription of the recording and is notable since temporal information was\nnever known in the first place in weakly labeled data."
},{
    "category": "cs.HC", 
    "doi": "10.1145/2964284.2964310", 
    "link": "http://arxiv.org/pdf/1605.07733v1", 
    "title": "On model architecture for a children's speech recognition interactive   dialog system", 
    "arxiv-id": "1605.07733v1", 
    "author": "Velin Kralev", 
    "publish": "2016-05-25T04:57:42Z", 
    "summary": "This report presents a general model of the architecture of information\nsystems for the speech recognition of children. It presents a model of the\nspeech data stream and how it works. The result of these studies and presented\nveins architectural model shows that research needs to be focused on\nacoustic-phonetic modeling in order to improve the quality of children's speech\nrecognition and the sustainability of the systems to noise and changes in\ntransmission environment. Another important aspect is the development of more\naccurate algorithms for modeling of spontaneous child speech."
},{
    "category": "cs.CL", 
    "doi": "10.1145/2964284.2964310", 
    "link": "http://arxiv.org/pdf/1605.07735v1", 
    "title": "Design and development a children's speech database", 
    "arxiv-id": "1605.07735v1", 
    "author": "Radoslava Kraleva", 
    "publish": "2016-05-25T05:04:11Z", 
    "summary": "The report presents the process of planning, designing and the development of\na database of spoken children's speech whose native language is Bulgarian. The\nproposed model is designed for children between the age of 4 and 6 without\nspeech disorders, and reflects their specific capabilities. At this age most\nchildren cannot read, there is no sustained concentration, they are emotional,\netc. The aim is to unite all the media information accompanying the recording\nand processing of spoken speech, thereby to facilitate the work of researchers\nin the field of speech recognition. This database will be used for the\ndevelopment of systems for children's speech recognition, children's speech\nsynthesis systems, games which allow voice control, etc. As a result of the\nproposed model a prototype system for speech recognition is presented."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2632307", 
    "link": "http://arxiv.org/pdf/1605.09507v3", 
    "title": "Deep convolutional neural networks for predominant instrument   recognition in polyphonic music", 
    "arxiv-id": "1605.09507v3", 
    "author": "Kyogu Lee", 
    "publish": "2016-05-31T07:11:18Z", 
    "summary": "Identifying musical instruments in polyphonic music recordings is a\nchallenging but important problem in the field of music information retrieval.\nIt enables music search by instrument, helps recognize musical genres, or can\nmake music transcription easier and more accurate. In this paper, we present a\nconvolutional neural network framework for predominant instrument recognition\nin real-world polyphonic music. We train our network from fixed-length music\nexcerpts with a single-labeled predominant instrument and estimate an arbitrary\nnumber of predominant instruments from an audio signal with a variable length.\nTo obtain the audio-excerpt-wise result, we aggregate multiple outputs from\nsliding windows over the test audio. In doing so, we investigated two different\naggregation methods: one takes the average for each instrument and the other\ntakes the instrument-wise sum followed by normalization. In addition, we\nconducted extensive experiments on several important factors that affect the\nperformance, including analysis window size, identification threshold, and\nactivation functions for neural networks to find the optimal set of parameters.\nUsing a dataset of 10k audio excerpts from 11 instruments for evaluation, we\nfound that convolutional neural networks are more robust than conventional\nmethods that exploit spectral features and source separation with support\nvector machines. Experimental results showed that the proposed convolutional\nnetwork architecture obtained an F1 measure of 0.602 for micro and 0.503 for\nmacro, respectively, achieving 19.6% and 16.4% in performance improvement\ncompared with other state-of-the-art algorithms."
},{
    "category": "cs.AI", 
    "doi": "10.1109/TASLP.2016.2632307", 
    "link": "http://arxiv.org/pdf/1606.02096v1", 
    "title": "Towards Playlist Generation Algorithms Using RNNs Trained on   Within-Track Transitions", 
    "arxiv-id": "1606.02096v1", 
    "author": "Mark Sandler", 
    "publish": "2016-06-07T11:07:56Z", 
    "summary": "We introduce a novel playlist generation algorithm that focuses on the\nquality of transitions using a recurrent neural network (RNN). The proposed\nmodel assumes that optimal transitions between tracks can be modelled and\npredicted by internal transitions within music tracks. We introduce modelling\nsequences of high-level music descriptors using RNNs and discuss an experiment\ninvolving different similarity functions, where the sequences are provided by a\nmusical structural analysis algorithm. Qualitative observations show that the\nproposed approach can effectively model transitions of music tracks in\nplaylists."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2632307", 
    "link": "http://arxiv.org/pdf/1606.03044v1", 
    "title": "The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music   Content Analysis Systems", 
    "arxiv-id": "1606.03044v1", 
    "author": "Bob L. Sturm", 
    "publish": "2016-06-09T18:10:31Z", 
    "summary": "Building systems that possess the sensitivity and intelligence to identify\nand describe high-level attributes in music audio signals continues to be an\nelusive goal, but one that surely has broad and deep implications for a wide\nvariety of applications. Hundreds of papers have so far been published toward\nthis goal, and great progress appears to have been made. Some systems produce\nremarkable accuracies at recognising high-level semantic concepts, such as\nmusic style, genre and mood. However, it might be that these numbers do not\nmean what they seem. In this paper, we take a state-of-the-art music content\nanalysis system and investigate what causes it to achieve exceptionally high\nperformance in a benchmark music audio dataset. We dissect the system to\nunderstand its operation, determine its sensitivities and limitations, and\npredict the kinds of knowledge it could and could not possess about music. We\nperform a series of experiments to illuminate what the system has actually\nlearned to do, and to what extent it is performing the intended music listening\ntask. Our results demonstrate how the initial manifestation of music\nintelligence in this state-of-the-art can be deceptive. Our work provides\nconstructive directions toward developing music content analysis systems that\ncan address the music information and creation needs of real-world users."
},{
    "category": "cs.SD", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.03636v1", 
    "title": "BigEAR: Inferring the Ambient and Emotional Correlates from   Smartphone-based Acoustic Big Data", 
    "arxiv-id": "1606.03636v1", 
    "author": "Kunal Mankodiya", 
    "publish": "2016-06-11T22:02:04Z", 
    "summary": "This paper presents a novel BigEAR big data framework that employs\npsychological audio processing chain (PAPC) to process smartphone-based\nacoustic big data collected when the user performs social conversations in\nnaturalistic scenarios. The overarching goal of BigEAR is to identify moods of\nthe wearer from various activities such as laughing, singing, crying, arguing,\nand sighing. These annotations are based on ground truth relevant for\npsychologists who intend to monitor/infer the social context of individuals\ncoping with breast cancer. We pursued a case study on couples coping with\nbreast cancer to know how the conversations affect emotional and social well\nbeing. In the state-of-the-art methods, psychologists and their team have to\nhear the audio recordings for making these inferences by subjective evaluations\nthat not only are time-consuming and costly, but also demand manual data coding\nfor thousands of audio files. The BigEAR framework automates the audio\nanalysis. We computed the accuracy of BigEAR with respect to the ground truth\nobtained from a human rater. Our approach yielded overall average accuracy of\n88.76% on real-world data from couples coping with breast cancer."
},{
    "category": "cs.SD", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.03664v1", 
    "title": "Weakly Supervised Scalable Audio Content Analysis", 
    "arxiv-id": "1606.03664v1", 
    "author": "Bhiksha Raj", 
    "publish": "2016-06-12T04:07:45Z", 
    "summary": "Audio Event Detection is an important task for content analysis of multimedia\ndata. Most of the current works on detection of audio events is driven through\nsupervised learning approaches. We propose a weakly supervised learning\nframework which can make use of the tremendous amount of web multimedia data\nwith significantly reduced annotation effort and expense. Specifically, we use\nseveral multiple instance learning algorithms to show that audio event\ndetection through weak labels is feasible. We also propose a novel scalable\nmultiple instance learning algorithm and show that its competitive with other\nmultiple instance learning algorithms for audio event detection tasks."
},{
    "category": "stat.ML", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.04317v1", 
    "title": "Calibration of Phone Likelihoods in Automatic Speech Recognition", 
    "arxiv-id": "1606.04317v1", 
    "author": "Joost van Doremalen", 
    "publish": "2016-06-14T11:44:31Z", 
    "summary": "In this paper we study the probabilistic properties of the posteriors in a\nspeech recognition system that uses a deep neural network (DNN) for acoustic\nmodeling. We do this by reducing Kaldi's DNN shared pdf-id posteriors to phone\nlikelihoods, and using test set forced alignments to evaluate these using a\ncalibration sensitive metric. Individual frame posteriors are in principle\nwell-calibrated, because the DNN is trained using cross entropy as the\nobjective function, which is a proper scoring rule. When entire phones are\nassessed, we observe that it is best to average the log likelihoods over the\nduration of the phone. Further scaling of the average log likelihoods by the\nlogarithm of the duration slightly improves the calibration, and this\nimprovement is retained when tested on independent test data."
},{
    "category": "cs.LG", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.04750v1", 
    "title": "Multi-Modal Hybrid Deep Neural Network for Speech Enhancement", 
    "arxiv-id": "1606.04750v1", 
    "author": "Rick Siow Mong Goh", 
    "publish": "2016-06-15T13:14:05Z", 
    "summary": "Deep Neural Networks (DNN) have been successful in en- hancing noisy speech\nsignals. Enhancement is achieved by learning a nonlinear mapping function from\nthe features of the corrupted speech signal to that of the reference clean\nspeech signal. The quality of predicted features can be improved by providing\nadditional side channel information that is robust to noise, such as visual\ncues. In this paper we propose a novel deep learning model inspired by insights\nfrom human audio visual perception. In the proposed unified hybrid\narchitecture, features from a Convolution Neural Network (CNN) that processes\nthe visual cues and features from a fully connected DNN that processes the\naudio signal are integrated using a Bidirectional Long Short-Term Memory\n(BiLSTM) network. The parameters of the hybrid model are jointly learned using\nbackpropagation. We compare the quality of enhanced speech from the hybrid\nmodels with those from traditional DNN and BiLSTM models."
},{
    "category": "cs.CL", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.05007v1", 
    "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep   Neural Networks", 
    "arxiv-id": "1606.05007v1", 
    "author": "Beat Pfister", 
    "publish": "2016-06-15T23:45:33Z", 
    "summary": "Phonemic or phonetic sub-word units are the most commonly used atomic\nelements to represent speech signals in modern ASRs. However they are not the\noptimal choice due to several reasons such as: large amount of effort required\nto handcraft a pronunciation dictionary, pronunciation variations, human\nmistakes and under-resourced dialects and languages. Here, we propose a\ndata-driven pronunciation estimation and acoustic modeling method which only\ntakes the orthographic transcription to jointly estimate a set of sub-word\nunits and a reliable dictionary. Experimental results show that the proposed\nmethod which is based on semi-supervised training of a deep neural network\nlargely outperforms phoneme based continuous speech recognition on the TIMIT\ndataset."
},{
    "category": "cs.CE", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.06154v1", 
    "title": "Closed Form Fractional Integration and Differentiation via Real   Exponentially Spaced Pole-Zero Pairs", 
    "arxiv-id": "1606.06154v1", 
    "author": "Harrison Freeman Smith", 
    "publish": "2016-06-07T07:01:50Z", 
    "summary": "We derive closed-form expressions for the poles and zeros of approximate\nfractional integrator/differentiator filters, which correspond to spectral\nroll-off filters having any desired log-log slope to a controllable degree of\naccuracy over any bandwidth. The filters can be described as a uniform\nexponential distribution of poles along the negative-real axis of the s plane,\nwith zeros interleaving them. Arbitrary spectral slopes are obtained by sliding\nthe array of zeros relative to the array of poles, where each array maintains\nperiodic spacing on a log scale. The nature of the slope approximation is close\nto Chebyshev optimal in the interior of the pole-zero array, approaching\nconjectured Chebyshev optimality over all frequencies in the limit as the order\napproaches infinity. Practical designs can arbitrarily approach the\nequal-ripple approximation by enlarging the pole-zero array band beyond the\ndesired frequency band. The spectral roll-off slope can be robustly modulated\nin real time by varying only the zeros controlled by one slope parameter.\nSoftware implementations are provided in matlab and Faust."
},{
    "category": "cs.SD", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.06197v2", 
    "title": "Polymetric Rhythmic Feel for a Cognitive Drum Computer", 
    "arxiv-id": "1606.06197v2", 
    "author": "Oliver Weede", 
    "publish": "2016-06-20T16:27:12Z", 
    "summary": "This paper addresses a question about music cognition: how do we derive\npolymetric structures. A preference rule system is presented which is\nimplemented into a drum computer. The preference rule system allows inferring\nlocal polymetric structures, like two-over-three and three-over-two. By\nanalyzing the micro-timing of West African percussion music a timing pattern\nconsisting of six pulses was discovered. It integrates binary and ternary\nrhythmic feels. The presented drum computer integrates the discovered\nsuperimposed polymetric swing (timing and velocity) appropriate to the rhythmic\nsequence the user inputs. For binary sequences, the amount of binary swing is\nincreased and for ternary sequences, the ternary swing is increased."
},{
    "category": "cs.CL", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.06864v2", 
    "title": "A Curriculum Learning Method for Improved Noise Robustness in Automatic   Speech Recognition", 
    "arxiv-id": "1606.06864v2", 
    "author": "Shih-Chii Liu", 
    "publish": "2016-06-22T09:29:40Z", 
    "summary": "The performance of automatic speech recognition systems under noisy\nenvironments still leaves room for improvement. Speech enhancement or feature\nenhancement techniques for increasing noise robustness of these systems usually\nadd components to the recognition system that need careful optimization. In\nthis work, we propose the use of a relatively simple curriculum training\nstrategy called accordion annealing (ACCAN). It uses a multi-stage training\nschedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are\nfirst added and samples at increasing higher SNR values are gradually added up\nto an SNR value of 50dB. We also use a method called per-epoch noise mixing\n(PEM) that generates noisy training samples online during training and thus\nenables dynamically changing the SNR of our training data. Both the ACCAN and\nthe PEM methods are evaluated on a end-to-end speech recognition pipeline on\nthe Wall Street Journal corpus. ACCAN decreases the average word error rate\n(WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a\nconventional multi-condition training method."
},{
    "category": "cs.NE", 
    "doi": "10.1109/CHASE.2016.46", 
    "link": "http://arxiv.org/pdf/1606.06871v1", 
    "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic   Modeling in Speech Recognition", 
    "arxiv-id": "1606.06871v1", 
    "author": "Hermann Ney", 
    "publish": "2016-06-22T10:00:14Z", 
    "summary": "We present a comprehensive study of deep bidirectional long short-term memory\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\nspeech recognition (ASR). We study the effect of size and depth and train\nmodels of up to 8 layers. We investigate the training aspect and study\ndifferent variants of optimization methods, batching, truncated\nbackpropagation, different regularization techniques such as dropout and $L_2$\nregularization, and different gradient clipping variants.\n  The major part of the experimental analysis was performed on the Quaero\ncorpus. Additional experiments also were performed on the Switchboard corpus.\nOur best LSTM model has a relative improvement in word error rate of over 14\\%\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\nand we show that a pretraining scheme with layer-wise construction helps for\ndeep LSTMs.\n  Finally we compare the training calculation time of many of the presented\nexperiments in relation with recognition performance.\n  All the experiments were done with RETURNN, the RWTH extensible training\nframework for universal recurrent neural networks in combination with RASR, the\nRWTH ASR toolkit."
},{
    "category": "cs.MM", 
    "doi": "10.1145/2964284.2967268", 
    "link": "http://arxiv.org/pdf/1606.07908v2", 
    "title": "Label Tree Embeddings for Acoustic Scene Classification", 
    "arxiv-id": "1606.07908v2", 
    "author": "Alfred Mertins", 
    "publish": "2016-06-25T12:57:44Z", 
    "summary": "We present in this paper an efficient approach for acoustic scene\nclassification by exploring the structure of class labels. Given a set of class\nlabels, a category taxonomy is automatically learned by collectively optimizing\na clustering of the labels into multiple meta-classes in a tree structure. An\nacoustic scene instance is then embedded into a low-dimensional feature\nrepresentation which consists of the likelihoods that it belongs to the\nmeta-classes. We demonstrate state-of-the-art results on two different datasets\nfor the acoustic scene classification task, including the DCASE 2013 and LITIS\nRouen datasets."
},{
    "category": "cs.IT", 
    "doi": "10.1145/2964284.2967268", 
    "link": "http://arxiv.org/pdf/1606.08303v1", 
    "title": "On the Statistical Model of Source Localization based on Range   Difference Measurements", 
    "arxiv-id": "1606.08303v1", 
    "author": "Augusto Sarti", 
    "publish": "2016-06-27T15:01:03Z", 
    "summary": "In this work we study the statistical model of source localization based on\nRange Difference measurements. We investigate the case of planar localization\nof a source using a minimal configuration of three non aligned receivers. Our\nanalysis is based on a previous work of the same authors concerning the\nlocalization in a noiseless scenario. As the set of feasible measurements is a\nsemialgebraic variety, this investigation makes use of techniques from\nAlgebraic Statistics and Information Geometry."
},{
    "category": "cs.MM", 
    "doi": "10.1145/2964284.2967268", 
    "link": "http://arxiv.org/pdf/1606.09047v1", 
    "title": "Minimum-latency Time-frequency Analysis Using Asymmetric Window   Functions", 
    "arxiv-id": "1606.09047v1", 
    "author": "Hau-tieng Wu", 
    "publish": "2016-06-29T11:18:41Z", 
    "summary": "We study the real-time dynamics retrieval from a time series via the\ntime-frequency (TF) analysis with the minimal latency guarantee. While\ndifferent from the well-known intrinsic latency definition in the filter\ndesign, a rigorous definition of intrinsic latency for different time-frequency\nrepresentations (TFR) is provided, including the short time Fourier transform\n(STFT), synchrosqeezing transform (SST) and reassignment method (RM). To\nachieve the minimal latency, a systematic method is proposed to construct an\nasymmetric window from a well-designed symmetric one based on the concept of\nminimum-phase, if the window satisfies some weak conditions. We theoretically\nshow that the TFR determined by SST with the constructed asymmetric window does\nhave a smaller intrinsic latency. Finally, the music onset detection problem is\nstudied to show the strength of the proposed algorithm."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1606.09222v1", 
    "title": "Penambahan emosi menggunakan metode manipulasi prosodi untuk sistem text   to speech bahasa Indonesia", 
    "arxiv-id": "1606.09222v1", 
    "author": "Ary Setijadi Prihatmanto", 
    "publish": "2016-06-29T19:06:48Z", 
    "summary": "Adding an emotions using prosody manipulation method for Indonesian text to\nspeech system. Text To Speech (TTS) is a system that can convert text in one\nlanguage into speech, accordance with the reading of the text in the language\nused. The focus of this research is a natural sounding concept, the make\n\"humanize\" for the pronunciation of voice synthesis system Text To Speech.\nHumans have emotions / intonation that may affect the sound produced. The main\nrequirement for the system used Text To Speech in this research is eSpeak, the\ndatabase MBROLA using id1, Human Speech Corpus database from a website that\nsummarizes the words with the highest frequency (Most Common Words) used in a\ncountry. And there are 3 types of emotional / intonation designed base. There\nis a happy, angry and sad emotion. Method for develop the emotional filter is\nmanipulate the relevant features of prosody (especially pitch and duration\nvalue) using a predetermined rate factor that has been established by analyzing\nthe differences between the standard output Text To Speech and voice recording\nwith emotional prosody / a particular intonation. The test results for the\nperception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for\nangry emotion and 98.75 % for sad emotions. For perception test system carried\nby intelligibility and naturalness test. Intelligibility test for the accuracy\nof sound with the original sentence is 93.3%, and for clarity rate for each\nsentence is 62.8%. For naturalness, accuracy emotional election amounted to\n75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions.\n  -----\n  Text To Speech (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam\nformat suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa\nyang digunakan."
},{
    "category": "cs.AI", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.00087v2", 
    "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough   Estimator of Person-Dependent Audio Emotion Recognition", 
    "arxiv-id": "1607.00087v2", 
    "author": "Ah Chung Tsoi", 
    "publish": "2016-07-01T00:54:10Z", 
    "summary": "As a general means of expression, audio analysis and recognition has\nattracted much attentions for its wide applications in real-life world. Audio\nemotion recognition (AER) attempts to understand emotional states of human with\nthe given utterance signals, and has been studied abroad for its further\ndevelopment on friendly human-machine interfaces. Distinguish from other\nexisting works, the person-dependent patterns of audio emotions are conducted,\nand fractal dimension features are calculated for acoustic feature extraction.\nFurthermore, it is able to efficiently learn intrinsic characteristics of\nauditory emotions, while the utterance features are learned from fractal\ndimensions of each sub-bands. Experimental results show the proposed method is\nable to provide comparative performance for audio emotion recognition."
},{
    "category": "cs.CL", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.00325v2", 
    "title": "Permutation Invariant Training of Deep Models for Speaker-Independent   Multi-talker Speech Separation", 
    "arxiv-id": "1607.00325v2", 
    "author": "Jesper Jensen", 
    "publish": "2016-07-01T17:34:16Z", 
    "summary": "We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common."
},{
    "category": "cs.LG", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.02173v1", 
    "title": "Single-Channel Multi-Speaker Separation using Deep Clustering", 
    "arxiv-id": "1607.02173v1", 
    "author": "John R. Hershey", 
    "publish": "2016-07-07T21:06:48Z", 
    "summary": "Deep clustering is a recently introduced deep learning architecture that uses\ndiscriminatively trained embeddings as the basis for clustering. It was\nrecently applied to spectrogram segmentation, resulting in impressive results\non speaker-independent multi-speaker separation. In this paper we extend the\nbaseline system with an end-to-end signal approximation objective that greatly\nimproves performance on a challenging speech separation. We first significantly\nimprove upon the baseline system performance by incorporating better\nregularization, larger temporal context, and a deeper architecture, culminating\nin an overall improvement in signal to distortion ratio (SDR) of 10.3 dB\ncompared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1\ndB SDR improvement for three-speaker separation. We then extend the model to\nincorporate an enhancement layer to refine the signal estimates, and perform\nend-to-end training through both the clustering and enhancement stages to\nmaximize signal fidelity. We evaluate the results using automatic speech\nrecognition. The new signal approximation objective, combined with end-to-end\ntraining, produces unprecedented performance, reducing the word error rate\n(WER) from 89.1% down to 30.8%. This represents a major advancement towards\nsolving the cocktail party problem."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.02306v2", 
    "title": "CaR-FOREST: Joint Classification-Regression Decision Forests for   Overlapping Audio Event Detection", 
    "arxiv-id": "1607.02306v2", 
    "author": "Alfred Mertins", 
    "publish": "2016-07-08T10:42:43Z", 
    "summary": "This report describes our submissions to Task2 and Task3 of the DCASE 2016\nchallenge. The systems aim at dealing with the detection of overlapping audio\nevents in continuous streams, where the detectors are based on random decision\nforests. The proposed forests are jointly trained for classification and\nregression simultaneously. Initially, the training is classification-oriented\nto encourage the trees to select discriminative features from overlapping\nmixtures to separate positive audio segments from the negative ones. The\nregression phase is then carried out to let the positive audio segments vote\nfor the event onsets and offsets, and therefore model the temporal structure of\naudio events. One random decision forest is specifically trained for each event\ncategory of interest. Experimental results on the development data show that\nour systems significantly outperform the baseline on the Task2 evaluation while\nthey are inferior to the baseline in the Task3 evaluation."
},{
    "category": "cs.LG", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.02444v1", 
    "title": "Explaining Deep Convolutional Neural Networks on Music Classification", 
    "arxiv-id": "1607.02444v1", 
    "author": "Mark Sandler", 
    "publish": "2016-07-08T16:40:30Z", 
    "summary": "Deep convolutional neural networks (CNNs) have been actively adopted in the\nfield of music information retrieval, e.g. genre classification, mood\ndetection, and chord recognition. However, the process of learning and\nprediction is little understood, particularly when it is applied to\nspectrograms. We introduce auralisation of a CNN to understand its underlying\nmechanism, which is based on a deconvolution procedure introduced in [2].\nAuralisation of a CNN is converting the learned convolutional features that are\nobtained from deconvolution into audio signals. In the experiments and\ndiscussions, we explain trained features of a 5-layer CNN based on the\ndeconvolved spectrograms and auralised signals. The pairwise correlations per\nlayers with varying different musical attributes are also investigated to\nunderstand the evolution of the learnt features. It is shown that in the deep\nlayers, the features are learnt to capture textures, the patterns of continuous\ndistributions, rather than shapes of lines."
},{
    "category": "cs.NE", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.02857v1", 
    "title": "Classifying Variable-Length Audio Files with All-Convolutional Networks   and Masked Global Pooling", 
    "arxiv-id": "1607.02857v1", 
    "author": "Alfred Mertins", 
    "publish": "2016-07-11T08:33:48Z", 
    "summary": "We trained a deep all-convolutional neural network with masked global pooling\nto perform single-label classification for acoustic scene classification and\nmulti-label classification for domestic audio tagging in the DCASE-2016\ncontest. Our network achieved an average accuracy of 84.5% on the four-fold\ncross-validation for acoustic scene recognition, compared to the provided\nbaseline of 72.5%, and an average equal error rate of 0.17 for domestic audio\ntagging, compared to the baseline of 0.21. The network therefore improves the\nbaselines by a relative amount of 17% and 19%, respectively. The network only\nconsists of convolutional layers to extract features from the short-time\nFourier transform and one global pooling layer to combine those features. It\nparticularly possesses neither fully-connected layers, besides the\nfully-connected output layer, nor dropout layers."
},{
    "category": "cs.MM", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.03257v1", 
    "title": "City-Identification of Flickr Videos Using Semantic Acoustic Features", 
    "arxiv-id": "1607.03257v1", 
    "author": "Ian Lane", 
    "publish": "2016-07-12T08:30:45Z", 
    "summary": "City-identification of videos aims to determine the likelihood of a video\nbelonging to a set of cities. In this paper, we present an approach using only\naudio, thus we do not use any additional modality such as images, user-tags or\ngeo-tags. In this manner, we show to what extent the city-location of videos\ncorrelates to their acoustic information. Success in this task suggests\nimprovements can be made to complement the other modalities. In particular, we\npresent a method to compute and use semantic acoustic features to perform\ncity-identification and the features show semantic evidence of the\nidentification. The semantic evidence is given by a taxonomy of urban sounds\nand expresses the potential presence of these sounds in the city- soundtracks.\nWe used the MediaEval Placing Task set, which contains Flickr videos labeled by\ncity. In addition, we used the UrbanSound8K set containing audio clips labeled\nby sound- type. Our method improved the state-of-the-art performance and\nprovides a novel semantic approach to this task"
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.03681v2", 
    "title": "Unsupervised Feature Learning Based on Deep Models for Environmental   Audio Tagging", 
    "arxiv-id": "1607.03681v2", 
    "author": "Mark D. Plumbley", 
    "publish": "2016-07-13T11:31:14Z", 
    "summary": "Environmental audio tagging aims to predict only the presence or absence of\ncertain acoustic events in the interested acoustic scene. In this paper we make\ncontributions to audio tagging in two parts, respectively, acoustic modeling\nand feature learning. We propose to use a shrinking deep neural network (DNN)\nframework incorporating unsupervised feature learning to handle the multi-label\nclassification task. For the acoustic modeling, a large set of contextual\nframes of the chunk are fed into the DNN to perform a multi-label\nclassification for the expected tags, considering that only chunk (or\nutterance) level rather than frame-level labels are available. Dropout and\nbackground noise aware training are also adopted to improve the generalization\ncapability of the DNNs. For the unsupervised feature learning, we propose to\nuse a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to\ngenerate new data-driven features from the Mel-Filter Banks (MFBs) features.\nThe new features, which are smoothed against background noise and more compact\nwith contextual information, can further improve the performance of the DNN\nbaseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of\nthe DCASE 2016 audio tagging challenge, our proposed method obtains a\nsignificant equal error rate (EER) reduction from 0.21 to 0.13 on the\ndevelopment set. The proposed aDAE system can get a relative 6.7% EER reduction\ncompared with the strong DNN baseline on the development set. Finally, the\nresults also show that our approach obtains the state-of-the-art performance\nwith 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while\nEER of the first prize of this challenge is 0.17."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.4140.6165", 
    "link": "http://arxiv.org/pdf/1607.03682v3", 
    "title": "Hierarchical learning for DNN-based acoustic scene classification", 
    "arxiv-id": "1607.03682v3", 
    "author": "Mark D. Plumbley", 
    "publish": "2016-07-13T11:31:25Z", 
    "summary": "In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds."
},{
    "category": "cs.SD", 
    "doi": "10.1109/TASLP.2016.2592698", 
    "link": "http://arxiv.org/pdf/1607.04589v1", 
    "title": "Automatic Environmental Sound Recognition: Performance versus   Computational Cost", 
    "arxiv-id": "1607.04589v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2016-07-15T17:29:26Z", 
    "summary": "In the context of the Internet of Things (IoT), sound sensing applications\nare required to run on embedded platforms where notions of product pricing and\nform factor impose hard constraints on the available computing power. Whereas\nAutomatic Environmental Sound Recognition (AESR) algorithms are most often\ndeveloped with limited consideration for computational cost, this article seeks\nwhich AESR algorithm can make the most of a limited amount of computing power\nby comparing the sound classification performance em as a function of its\ncomputational cost. Results suggest that Deep Neural Networks yield the best\nratio of sound classification accuracy across a range of computational costs,\nwhile Gaussian Mixture Models offer a reasonable accuracy at a consistently\nsmall cost, and Support Vector Machines stand between both in terms of\ncompromise between accuracy and computational cost."
},{
    "category": "cs.HC", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1607.04765v1", 
    "title": "Design and implementation of audio communication system for   social-humanoid robot Lumen as an exhibition guide in Electrical Engineering   Days 2015", 
    "arxiv-id": "1607.04765v1", 
    "author": "Ary Setijadi Prihatmanto", 
    "publish": "2016-07-16T16:50:54Z", 
    "summary": "Social Robot Lumen is a humanoid robot created to act like human and be human\nfriend. In this study, Lumen scenario is limited on Lumen as an exhibition\nguide in Electrical Engineering Days 2015, a seminar and exhibition of\nelectrical engineering undergraduate and graduate student of Bandung Institute\nof Technology. To be an exhibition guide, Lumen is equipped by Nao robot, a\nserver, and processing applications. Audio communication system is one of the\nprocessing applications. The purpose of the system is to create verbal\ncommunication that allow Lumen to receive human voice and respond naturally to\nit. To be able to communicate like a human, audio communication system is built\nwith speech recognition module to transform speech data into text, speech\nsynthesizer module to transform text data into speech, and gender\nidentification module to distinguish adult female and male voice. Speech\nrecognition module is implemented using Google Speech Recognition API, speech\nsynthesizer module is implemented using Acapela engine, and gender\nidentification module implemented by utilizing speech signal feature that is\nextracted using Fast Fourier Transform algorithm. Hardware used for\nimplementation are Nao robot, computer, and wireless modem.\n  -----\n  Lumen Robot Sosial Robot merupakan robot humanoid yang diciptakan agar dapat\nbersikap seperti manusia dan menjadi teman bagi manusia. Sistem komunikasi\naudio merupakan salah satu aplikasi pengolah yang bertujuan agar Lumen dapat\nmenerima suara manusia dan meresponnya dengan natural, yaitu seperti cara\nmanusia merespon manusia lainnya. Untuk dapat berkomunikasi seperti manusia,\nsistem komunikasi audio dilengkapi dengan tiga buah modul: speech recognition\nuntuk mengubah data suara menjadi teks, speech synthesizer untuk mengubah data\nteks menjadi suara, dan gender identification untuk membedakan suara wanita dan\npria."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1607.06667v1", 
    "title": "Audio inpainting with similarity graphs", 
    "arxiv-id": "1607.06667v1", 
    "author": "Peter Balazs", 
    "publish": "2016-07-22T13:12:33Z", 
    "summary": "In this contribution, we present a method to compensate for long duration\ndata gaps in audio signals, in particular music. To achieve this task, a\nsimilarity graph is constructed, based on a short-time Fourier analysis of\nreliable signal segments, e.g. the uncorrupted remainder of the music piece,\nand the temporal regions adjacent to the unreliable section of the signal. A\nsuitable candidate segment is then selected through an optimization scheme and\nsmoothly inserted into the gap."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1608.04069v1", 
    "title": "Design of Variable Bandpass Filters Using First Order Allpass   Transformation And Coefficient Decimation", 
    "arxiv-id": "1608.04069v1", 
    "author": "E. M-K. Lai", 
    "publish": "2016-08-14T07:39:51Z", 
    "summary": "In this paper, the design of a computationally efficient variable bandpass\ndigital filter is presented. The center frequency and bandwidth of this filter\ncan be changed online without updating the filter coefficients. The warped\nfilters, obtained by replacing each unit delay of a digital filter with an\nallpass filter, are widely used for various audio processing applications.\nHowever, warped filters fail to provide variable bandwidth bandpass responses\nfor a given center frequency using first order allpass transformation. To\novercome this drawback, our design is accomplished by combining warped filter\nwith the coefficient decimation technique. The design example shows that the\nproposed variable digital filter is simple to design and offers a total gate\ncount reduction of 36% and 65% over the warped filters compared to the designs\npresented in [3] and [1] respectively"
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1608.04363v2", 
    "title": "Deep Convolutional Neural Networks and Data Augmentation for   Environmental Sound Classification", 
    "arxiv-id": "1608.04363v2", 
    "author": "Juan Pablo Bello", 
    "publish": "2016-08-15T18:57:10Z", 
    "summary": "The ability of deep convolutional neural networks (CNN) to learn\ndiscriminative spectro-temporal patterns makes them well suited to\nenvironmental sound classification. However, the relative scarcity of labeled\ndata has impeded the exploitation of this family of high-capacity models. This\nstudy has two primary contributions: first, we propose a deep convolutional\nneural network architecture for environmental sound classification. Second, we\npropose the use of audio data augmentation for overcoming the problem of data\nscarcity and explore the influence of different augmentations on the\nperformance of the proposed CNN architecture. Combined with data augmentation,\nthe proposed model produces state-of-the-art results for environmental sound\nclassification. We show that the improved performance stems from the\ncombination of a deep, high-capacity model and an augmented training set: this\ncombination outperforms both the proposed CNN without augmentation and a\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\ninfluence of each augmentation on the model's classification accuracy for each\nclass, and observe that the accuracy for each class is influenced differently\nby each augmentation, suggesting that the performance of the model could be\nimproved further by applying class-conditional data augmentation."
},{
    "category": "cs.NE", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1608.07373v1", 
    "title": "Applying Topological Persistence in Convolutional Neural Network for   Music Audio Signals", 
    "arxiv-id": "1608.07373v1", 
    "author": "Yi-Hsuan Yang", 
    "publish": "2016-08-26T07:14:37Z", 
    "summary": "Recent years have witnessed an increased interest in the application of\npersistent homology, a topological tool for data analysis, to machine learning\nproblems. Persistent homology is known for its ability to numerically\ncharacterize the shapes of spaces induced by features or functions. On the\nother hand, deep neural networks have been shown effective in various tasks. To\nour best knowledge, however, existing neural network models seldom exploit\nshape information. In this paper, we investigate a way to use persistent\nhomology in the framework of deep neural networks. Specifically, we propose to\nembed the so-called \"persistence landscape,\" a rather new topological summary\nfor data, into a convolutional neural network (CNN) for dealing with audio\nsignals. Our evaluation on automatic music tagging, a multi-label\nclassification task, shows that the resulting persistent convolutional neural\nnetwork (PCNN) model can perform significantly better than state-of-the-art\nmodels in prediction accuracy. We also discuss the intuition behind the design\nof the proposed model, and offer insights into the features that it learns."
},{
    "category": "cs.LG", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.02082v1", 
    "title": "An improved uncertainty decoding scheme with weighted samples for   DNN-HMM hybrid systems", 
    "arxiv-id": "1609.02082v1", 
    "author": "Walter Kellermann", 
    "publish": "2016-08-04T10:11:24Z", 
    "summary": "In this paper, we advance a recently-proposed uncertainty decoding scheme for\nDNN-HMM (deep neural network - hidden Markov model) hybrid systems. This\nnumerical sampling concept averages DNN outputs produced by a finite set of\nfeature samples (drawn from a probabilistic distortion model) to approximate\nthe posterior likelihoods of the context-dependent HMM states. As main\ninnovation, we propose a weighted DNN-output averaging based on a minimum\nclassification error criterion and apply it to a probabilistic distortion model\nfor spatial diffuseness features. The experimental evaluation is performed on\nthe 8-channel REVERB Challenge task using a DNN-HMM hybrid system with\nmultichannel front-end signal enhancement. We show that the recognition\naccuracy of the DNN-HMM hybrid system improves by incorporating uncertainty\ndecoding based on random sampling and that the proposed weighted DNN-output\naveraging further reduces the word error rate scores."
},{
    "category": "cs.NE", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.04243v3", 
    "title": "Convolutional Recurrent Neural Networks for Music Classification", 
    "arxiv-id": "1609.04243v3", 
    "author": "Kyunghyun Cho", 
    "publish": "2016-09-14T12:52:08Z", 
    "summary": "We introduce a convolutional recurrent neural network (CRNN) for music\ntagging. CRNNs take advantage of convolutional neural networks (CNNs) for local\nfeature extraction and recurrent neural networks for temporal summarisation of\nthe extracted features. We compare CRNN with three CNN structures that have\nbeen used for music tagging while controlling the number of parameters with\nrespect to their performance and training time per sample. Overall, we found\nthat CRNNs show a strong performance with respect to the number of parameter\nand training time, indicating the effectiveness of its hybrid structure in\nmusic feature extraction and feature summarisation."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.06026v2", 
    "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", 
    "arxiv-id": "1609.06026v2", 
    "author": "Bhiksha Raj", 
    "publish": "2016-09-20T05:52:06Z", 
    "summary": "Audio event detection in the era of Big Data has the constraint of lacking\nannotations to train robust models that match the scale of class diversity.\nThis is mainly due to the expensive and time-consuming process of manually\nannotating sound events in isolation or as segments within audio recordings. In\nthis paper, we propose an approach for semi-supervised self-training of audio\nevent detectors using unlabeled web data. We started with a small annotated\ndataset and trained sound events detectors. Then, we crawl and collect\nthousands of web videos and extract their soundtrack. The segmented soundtracks\nare run by the detectors and different selection techniques were used to\ndetermine whether a segment should be used for self-training the detectors. The\noriginal detectors were compared to the self-trained detectors and the results\nshowed a performance improvement by the latter when evaluated on the annotated\ntest set."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.07384v2", 
    "title": "Discovering Sound Concepts and Acoustic Relations In Text", 
    "arxiv-id": "1609.07384v2", 
    "author": "Ndapandula Nakashole", 
    "publish": "2016-09-23T14:35:17Z", 
    "summary": "In this paper we describe approaches for discovering acoustic concepts and\nrelations in text. The first major goal is to be able to identify text phrases\nwhich contain a notion of audibility and can be termed as a sound or an\nacoustic concept. We also propose a method to define an acoustic scene through\na set of sound concepts. We use pattern matching and parts of speech tags to\ngenerate sound concepts from large scale text corpora. We use dependency\nparsing and LSTM recurrent neural network to predict a set of sound concepts\nfor a given acoustic scene. These methods are not only helpful in creating an\nacoustic knowledge base but in the future can also directly help acoustic event\nand scene detection research."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.08419v1", 
    "title": "Decision Making Based on Cohort Scores for Speaker Verification", 
    "arxiv-id": "1609.08419v1", 
    "author": "Thomas Fang Zheng", 
    "publish": "2016-09-27T13:29:12Z", 
    "summary": "Decision making is an important component in a speaker verification system.\nFor the conventional GMM-UBM architecture, the decision is usually conducted\nbased on the log likelihood ratio of the test utterance against the GMM of the\nclaimed speaker and the UBM. This single-score decision is simple but tends to\nbe sensitive to the complex variations in speech signals (e.g. text content,\nchannel, speaking style, etc.). In this paper, we propose a decision making\napproach based on multiple scores derived from a set of cohort GMMs (cohort\nscores). Importantly, these cohort scores are not simply averaged as in\nconventional cohort methods; instead, we employ a powerful discriminative model\nas the decision maker. Experimental results show that the proposed method\ndelivers substantial performance improvement over the baseline system,\nespecially when a deep neural network (DNN) is used as the decision maker, and\nthe DNN input involves some statistical features derived from the cohort\nscores."
},{
    "category": "cs.LG", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.08441v1", 
    "title": "Weakly Supervised PLDA Training", 
    "arxiv-id": "1609.08441v1", 
    "author": "Chenghui Zhao", 
    "publish": "2016-09-27T13:46:55Z", 
    "summary": "PLDA is a popular normalization approach for the i-vector model, and it has\ndelivered state-of-the-art performance in speaker verification. However, PLDA\ntraining requires a large amount of labelled development data, which is highly\nexpensive in most cases. We present a cheap PLDA training approach, which\nassumes that speakers in the same session can be easily separated, and speakers\nin different sessions are simply different. This results in `weak labels' which\nare not fully accurate but cheap, leading to a weak PLDA training.\n  Our experimental results on real-life large-scale telephony customer service\nachieves demonstrated that the weak training can offer good performance when\nhuman-labelled data are limited. More interestingly, the weak training can be\nemployed as a discriminative adaptation approach, which is more efficient than\nthe prevailing unsupervised method when human-labelled data are insufficient."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.09430v2", 
    "title": "CNN Architectures for Large-Scale Audio Classification", 
    "arxiv-id": "1609.09430v2", 
    "author": "Kevin Wilson", 
    "publish": "2016-09-29T17:04:50Z", 
    "summary": "Convolutional Neural Networks (CNNs) have proven very effective in image\nclassification and show promise for audio. We use various CNN architectures to\nclassify the soundtracks of a dataset of 70M training videos (5.24 million\nhours) with 30,871 video-level labels. We examine fully connected Deep Neural\nNetworks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We\ninvestigate varying the size of both training set and label vocabulary, finding\nthat analogs of the CNNs used in image classification do well on our audio\nclassification task, and larger training and label sets help up to a point. A\nmodel using embeddings from these classifiers does much better than raw\nfeatures on the Audio Set [5] Acoustic Event Detection (AED) classification\ntask."
},{
    "category": "stat.ML", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1609.09799v2", 
    "title": "Optimal spectral transportation with application to music transcription", 
    "arxiv-id": "1609.09799v2", 
    "author": "Valentin Emiya", 
    "publish": "2016-09-30T16:28:12Z", 
    "summary": "Many spectral unmixing methods rely on the non-negative decomposition of\nspectral data onto a dictionary of spectral templates. In particular,\nstate-of-the-art music transcription systems decompose the spectrogram of the\ninput signal onto a dictionary of representative note spectra. The typical\nmeasures of fit used to quantify the adequacy of the decomposition compare the\ndata and template entries frequency-wise. As such, small displacements of\nenergy from a frequency bin to another as well as variations of timber can\ndisproportionally harm the fit. We address these issues by means of optimal\ntransportation and propose a new measure of fit that treats the frequency\ndistributions of energy holistically as opposed to frequency-wise. Building on\nthe harmonic nature of sound, the new measure is invariant to shifts of energy\nto harmonically-related frequencies, as well as to small and local\ndisplacements of energy. Equipped with this new measure of fit, the dictionary\nof note templates can be considerably simplified to a set of Dirac vectors\nlocated at the target fundamental frequencies (musical pitch values). This in\nturns gives ground to a very fast and simple decomposition algorithm that\nachieves state-of-the-art performance on real musical data."
},{
    "category": "cs.SD", 
    "doi": "10.13140/RG.2.1.3759.7682/1", 
    "link": "http://arxiv.org/pdf/1610.00087v1", 
    "title": "Very Deep Convolutional Neural Networks for Raw Waveforms", 
    "arxiv-id": "1610.00087v1", 
    "author": "Samarjit Das", 
    "publish": "2016-10-01T05:15:15Z", 
    "summary": "Learning acoustic models directly from the raw waveform data with minimal\nprocessing is challenging. Current waveform-based models have generally used\nvery few (~2) convolutional layers, which might be insufficient for building\nhigh-level discriminative features. In this work, we propose very deep\nconvolutional neural networks (CNNs) that directly use time-domain waveforms as\ninputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over\nvery long sequences (e.g., vector of size 32000), necessary for processing\nacoustic waveforms. This is achieved through batch normalization, residual\nlearning, and a careful design of down-sampling in the initial layers. Our\nnetworks are fully convolutional, without the use of fully connected layers and\ndropout, to maximize representation learning. We use a large receptive field in\nthe first convolutional layer to mimic bandpass filters, but very small\nreceptive fields subsequently to control the model capacity. We demonstrate the\nperformance gains with the deeper models. Our evaluation shows that the CNN\nwith 18 weight layers outperform the CNN with 3 weight layers by over 15% in\nabsolute accuracy for an environmental sound recognition task and matches the\nperformance of models using log-mel features."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ins.2016.10.007", 
    "link": "http://arxiv.org/pdf/1610.00468v1", 
    "title": "On the Modeling of Musical Solos as Complex Networks", 
    "arxiv-id": "1610.00468v1", 
    "author": "Stefano Ferretti", 
    "publish": "2016-10-03T09:56:43Z", 
    "summary": "Notes in a musical piece are building blocks employed in non-random ways to\ncreate melodies. It is the \"interaction\" among a limited amount of notes that\nallows constructing the variety of musical compositions that have been written\nin centuries and within different cultures. Networks are a modeling tool that\nis commonly employed to represent a set of entities interacting in some way.\nThus, notes composing a melody can be seen as nodes of a network that are\nconnected whenever these are played in sequence. The outcome of such a process\nresults in a directed graph. By using complex network theory, some main metrics\nof musical graphs can be measured, which characterize the related musical\npieces. In this paper, we define a framework to represent melodies as networks.\nThen, we provide an analysis on a set of guitar solos performed by main\nmusicians. Results of this study indicate that the presented model can have an\nimpact on audio and multimedia applications such as music classification,\nidentification, e-learning, automatic music generation, multimedia\nentertainment."
},{
    "category": "cs.CL", 
    "doi": "10.1016/j.ins.2016.10.007", 
    "link": "http://arxiv.org/pdf/1610.00552v1", 
    "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", 
    "arxiv-id": "1610.00552v1", 
    "author": "Wonyong Sung", 
    "publish": "2016-09-30T10:44:32Z", 
    "summary": "In this paper, a neural network based real-time speech recognition (SR)\nsystem is developed using an FPGA for very low-power operation. The implemented\nsystem employs two recurrent neural networks (RNNs); one is a\nspeech-to-character RNN for acoustic modeling (AM) and the other is for\ncharacter-level language modeling (LM). The system also employs a statistical\nword-level LM to improve the recognition accuracy. The results of the AM, the\ncharacter-level LM, and the word-level LM are combined using a fairly simple\nN-best search algorithm instead of the hidden Markov model (HMM) based network.\nThe RNNs are implemented using massively parallel processing elements (PEs) for\nlow latency and high throughput. The weights are quantized to 6 bits to store\nall of them in the on-chip memory of an FPGA. The proposed algorithm is\nimplemented on a Xilinx XC7Z045, and the system can operate much faster than\nreal-time."
},{
    "category": "cs.SD", 
    "doi": "10.1016/j.ins.2016.10.007", 
    "link": "http://arxiv.org/pdf/1610.02475v1", 
    "title": "A Music-generating System Inspired by the Science of Complex Adaptive   Systems", 
    "arxiv-id": "1610.02475v1", 
    "author": "Liane Gabora", 
    "publish": "2016-10-08T03:42:08Z", 
    "summary": "This paper presents NetWorks (NW), an interactive music generation system\nthat uses a hierarchically clustered scale free network to generate music that\nranges from orderly to chaotic. NW was inspired by the Honing Theory of\ncreativity, according to which human-like creativity hinges on (1) the ability\nto self-organize and maintain dynamics at the 'edge of chaos' using something\nakin to 'psychological entropy', and (2) the capacity to shift between analytic\nand associative processing modes. At the 'edge of chaos', NW generates patterns\nthat exhibit emergent complexity through coherent development at low, mid, and\nhigh levels of musical organization, and often suggests goal seeking behaviour.\nThe architecture consists of four 16-node modules: one each for pitch,\nvelocity, duration, and entry delay. The Core allows users to define how nodes\nare connected, and rules that determine when and how nodes respond to their\ninputs. The Mapping Layer allows users to map node output values to MIDI data\nthat is routed to software instruments in a digital audio workstation. By\nshifting between bottom-up and top-down NW shifts between analytic and\nassociative processing modes."
},{
    "category": "cs.CL", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.03934v1", 
    "title": "A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder", 
    "arxiv-id": "1610.03934v1", 
    "author": "Vallidevi Krishnamurthy", 
    "publish": "2016-10-13T04:10:58Z", 
    "summary": "Speech Translation has always been about giving source text or audio input\nand waiting for system to give translated output in desired form. In this\npaper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice\near-piece translation device. We introduce and survey the recent advances made\nin the field of Speech Engineering, to employ in the ADD, particularly focusing\non the three major processing steps of Recognition, Translation and Synthesis.\nWe tackle the problem of machine understanding of natural language by designing\na recognition unit for source audio to text, a translation unit for source\nlanguage text to target language text, and a synthesis unit for target language\ntext to target language speech. Speech from the surroundings will be recorded\nby the recognition unit present on the ear-piece and translation will start as\nsoon as one sentence is successfully read. This way, we hope to give translated\noutput as and when input is being read. The recognition unit will use Hidden\nMarkov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory\ncells, and the synthesis unit, HMM based speech synthesis system HTS. This\nsystem will initially be built as an English to Tamil translation device."
},{
    "category": "stat.ML", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.03988v1", 
    "title": "Dictionary Update for NMF-based Voice Conversion Using an   Encoder-Decoder Network", 
    "arxiv-id": "1610.03988v1", 
    "author": "Hsin-Min Wang", 
    "publish": "2016-10-13T09:18:53Z", 
    "summary": "In this paper, we propose a dictionary update method for Nonnegative Matrix\nFactorization (NMF) with high dimensional data in a spectral conversion (SC)\ntask. Voice conversion has been widely studied due to its potential\napplications such as personalized speech synthesis and speech enhancement.\nExemplar-based NMF (ENMF) emerges as an effective and probably the simplest\nchoice among all techniques for SC, as long as a source-target parallel speech\ncorpus is given. ENMF-based SC systems usually need a large amount of bases\n(exemplars) to ensure the quality of the converted speech. However, a small and\neffective dictionary is desirable but hard to obtain via dictionary update, in\nparticular when high-dimensional features such as STRAIGHT spectra are used.\nTherefore, we propose a dictionary update framework for NMF by means of an\nencoder-decoder reformulation. Regarding NMF as an encoder-decoder network\nmakes it possible to exploit the whole parallel corpus more effectively and\nefficiently when applied to SC. Our experiments demonstrate significant gains\nof the proposed system with small dictionaries over conventional ENMF-based\nsystems with dictionaries of same or much larger size."
},{
    "category": "stat.ML", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.04019v1", 
    "title": "Voice Conversion from Non-parallel Corpora Using Variational   Auto-encoder", 
    "arxiv-id": "1610.04019v1", 
    "author": "Hsin-Min Wang", 
    "publish": "2016-10-13T10:52:25Z", 
    "summary": "We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora."
},{
    "category": "cs.IT", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.04467v1", 
    "title": "A Geometrical-Statistical approach to outlier removal for TDOA   measuments", 
    "arxiv-id": "1610.04467v1", 
    "author": "Augusto Sarti", 
    "publish": "2016-10-14T14:03:26Z", 
    "summary": "The curse of outlier measurements in estimation problems is a well known\nissue in a variety of fields. Therefore, outlier removal procedures, which\nenables the identification of spurious measurements within a set, have been\ndeveloped for many different scenarios and applications. In this paper, we\npropose a statistically motivated outlier removal algorithm for time\ndifferences of arrival (TDOAs), or equivalently range differences (RD),\nacquired at sensor arrays. The method exploits the TDOA-space formalism and\nworks by only knowing the relative sensor positions. As the proposed method is\ncompletely independent from the application for which measurements are used, it\ncan be reliably used to identify outliers within a set of TDOA/RD measurements\nin different fields (e.g. acoustic source localization, sensor synchronization,\nradar, remote sensing, etc.). The whole theoretical derivation is validated by\nmeans of synthetic simulations and real experiments."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.05945v1", 
    "title": "A multi-task learning model for malware classification with useful file   access pattern from API call sequence", 
    "arxiv-id": "1610.05945v1", 
    "author": "Siu Ming Yiu", 
    "publish": "2016-10-19T10:06:14Z", 
    "summary": "Based on API call sequences, semantic-aware and machine learning (ML) based\nmalware classifiers can be built for malware detection or classification.\nPrevious works concentrate on crafting and extracting various features from\nmalware binaries, disassembled binaries or API calls via static or dynamic\nanalysis and resorting to ML to build classifiers. However, they tend to\ninvolve too much feature engineering and fail to provide interpretability. We\nsolve these two problems with the recent advances in deep learning: 1)\nRNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional\nrepresentation of a malware from its raw API call sequence. 2) Multiple\ndecoders can be trained under different supervisions to give more information,\nother than the class or family label of a malware. Inspired by the works of\ndocument classification and automatic sentence summarization, each API call\nsequence can be regarded as a sentence. In this paper, we make the first\nattempt to build a multi-task malware learning model based on API call\nsequences. The model consists of two decoders, one for malware classification\nand one for $\\emph{file access pattern}$ (FAP) generation given the API call\nsequence of a malware. We base our model on the general seq2seq framework.\nExperiments show that our model can give competitive classification results as\nwell as insightful FAP information."
},{
    "category": "cs.SD", 
    "doi": "10.1109/ICICES.2016.7518940", 
    "link": "http://arxiv.org/pdf/1610.05948v1", 
    "title": "A Bayesian Approach to Estimation of Speaker Normalization Parameters", 
    "arxiv-id": "1610.05948v1", 
    "author": "Rajesh M. Hegde", 
    "publish": "2016-10-19T10:16:46Z", 
    "summary": "In this work, a Bayesian approach to speaker normalization is proposed to\ncompensate for the degradation in performance of a speaker independent speech\nrecognition system. The speaker normalization method proposed herein uses the\ntechnique of vocal tract length normalization (VTLN). The VTLN parameters are\nestimated using a novel Bayesian approach which utilizes the Gibbs sampler, a\nspecial type of Markov Chain Monte Carlo method. Additionally the\nhyperparameters are estimated using maximum likelihood approach. This model is\nused assuming that human vocal tract can be modeled as a tube of uniform cross\nsection. It captures the variation in length of the vocal tract of different\nspeakers more effectively, than the linear model used in literature. The work\nhas also investigated different methods like minimization of Mean Square Error\n(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both\nsingle pass and two pass approaches are then used to build a VTLN based speech\nrecognizer. Experimental results on recognition of vowels and Hindi phrases\nfrom a medium vocabulary indicate that the Bayesian method improves the\nperformance by a considerable margin."
},{
    "category": "stat.ML", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1610.08166v1", 
    "title": "Automatic measurement of vowel duration via structured prediction", 
    "arxiv-id": "1610.08166v1", 
    "author": "Matthew Goldrick", 
    "publish": "2016-10-26T04:50:35Z", 
    "summary": "A key barrier to making phonetic studies scalable and replicable is the need\nto rely on subjective, manual annotation. To help meet this challenge, a\nmachine learning algorithm was developed for automatic measurement of a widely\nused phonetic measure: vowel duration. Manually-annotated data were used to\ntrain a model that takes as input an arbitrary length segment of the acoustic\nsignal containing a single vowel that is preceded and followed by consonants\nand outputs the duration of the vowel. The model is based on the structured\nprediction framework. The input signal and a hypothesized set of a vowel's\nonset and offset are mapped to an abstract vector space by a set of acoustic\nfeature functions. The learning algorithm is trained in this space to minimize\nthe difference in expectations between predicted and manually-measured vowel\ndurations. The trained model can then automatically estimate vowel durations\nwithout phonetic or orthographic transcription. Results comparing the model to\nthree sets of manually annotated data suggest it out-performed the current gold\nstandard for duration measurement, an HMM-based forced aligner (which requires\northographic or phonetic transcription as an input)."
},{
    "category": "cs.CV", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1610.09001v1", 
    "title": "SoundNet: Learning Sound Representations from Unlabeled Video", 
    "arxiv-id": "1610.09001v1", 
    "author": "Antonio Torralba", 
    "publish": "2016-10-27T20:23:39Z", 
    "summary": "We learn rich natural sound representations by capitalizing on large amounts\nof unlabeled sound data collected in the wild. We leverage the natural\nsynchronization between vision and sound to learn an acoustic representation\nusing two-million unlabeled videos. Unlabeled video has the advantage that it\ncan be economically acquired at massive scales, yet contains useful signals\nabout natural sound. We propose a student-teacher training procedure which\ntransfers discriminative visual knowledge from well established visual\nrecognition models into the sound modality using unlabeled video as a bridge.\nOur sound representation yields significant performance improvements over the\nstate-of-the-art results on standard benchmarks for acoustic scene/object\nclassification. Visualizations suggest some high-level semantics automatically\nemerge in the sound network, even though it is trained without ground truth\nlabels."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.00326v2", 
    "title": "Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech   Detection", 
    "arxiv-id": "1611.00326v2", 
    "author": "Jun Qin", 
    "publish": "2016-11-01T18:38:12Z", 
    "summary": "In this letter, we propose enhanced factored three way restricted Boltzmann\nmachines (EFTW-RBMs) for speech detection. The proposed model incorporates\nconditional feature learning by multiplying the dynamical state of the third\nunit, which allows a modulation over the visible-hidden node pairs. Instead of\nstacking previous frames of speech as the third unit in a recursive manner, the\ncorrelation related weighting coefficients are assigned to the contextual\nneighboring frames. Specifically, a threshold function is designed to capture\nthe long-term features and blend the globally stored speech structure. A\nfactored low rank approximation is introduced to reduce the parameters of the\nthree-dimensional interaction tensor, on which non-negative constraint is\nimposed to address the sparsity characteristic. The validations through the\narea-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our\napproach outperforms several existing 1D and 2D (i.e., time and time-frequency\ndomain) speech detection algorithms in various noisy environments."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.00514v1", 
    "title": "The Intelligent Voice 2016 Speaker Recognition System", 
    "arxiv-id": "1611.00514v1", 
    "author": "Nigel Cannings", 
    "publish": "2016-11-02T09:24:10Z", 
    "summary": "This paper presents the Intelligent Voice (IV) system submitted to the NIST\n2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this\nyear was on developing speaker recognition technology which is robust for novel\nlanguages that are much more heterogeneous than those used in the current\nstate-of-the-art, using significantly less training data, that does not contain\nmeta-data from those languages. The system is based on the state-of-the-art\ni-vector/PLDA which is developed on the fixed training condition, and the\nresults are reported on the protocol defined on the development set of the\nchallenge."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.04871v3", 
    "title": "Audio Event and Scene Recognition: A Unified Approach using Strongly and   Weakly Labeled Data", 
    "arxiv-id": "1611.04871v3", 
    "author": "Bhiksha Raj", 
    "publish": "2016-11-12T07:39:50Z", 
    "summary": "In this paper we propose a novel learning framework called Supervised and\nWeakly Supervised Learning where the goal is to learn simultaneously from\nweakly and strongly labeled data. Strongly labeled data can be simply\nunderstood as fully supervised data where all labeled instances are available.\nIn weakly supervised learning only data is weakly labeled which prevents one\nfrom directly applying supervised learning methods. Our proposed framework is\nmotivated by the fact that a small amount of strongly labeled data can give\nconsiderable improvement over only weakly supervised learning. The primary\nproblem domain focus of this paper is acoustic event and scene detection in\naudio recordings. We first propose a naive formulation for leveraging labeled\ndata in both forms. We then propose a more general framework for Supervised and\nWeakly Supervised Learning (SWSL). Based on this general framework, we propose\na graph based approach for SWSL. Our main method is based on manifold\nregularization on graphs in which we show that the unified learning can be\nformulated as a constraint optimization problem which can be solved by\niterative concave-convex procedure (CCCP). Our experiments show that our\nproposed framework can address several concerns of audio content analysis using\nweakly labeled data."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.05416v2", 
    "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level   Encoding", 
    "arxiv-id": "1611.05416v2", 
    "author": "Xiao Zhang", 
    "publish": "2016-11-16T19:42:40Z", 
    "summary": "Creating aesthetically pleasing pieces of art, including music, has been a\nlong-term goal for artificial intelligence research. Despite recent successes\nof long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential\nlearning, LSTM neural networks have not, by themselves, been able to generate\nnatural-sounding music conforming to music theory. To transcend this\ninadequacy, we put forward a novel method for music composition that combines\nthe LSTM with Grammars motivated by music theory. The main tenets of music\ntheory are encoded as grammar argumented (GA) filters on the training data,\nsuch that the machine can be trained to generate music inheriting the\nnaturalness of human-composed pieces from the original dataset while adhering\nto the rules of music theory. Unlike previous approaches, pitches and durations\nare encoded as one semantic entity, which we refer to as note-level encoding.\nThis allows easy implementation of music theory grammars, as well as closer\nemulation of the thinking pattern of a musician. Although the GA rules are\napplied to the training data and never directly to the LSTM music generation,\nour machine still composes music that possess high incidences of diatonic scale\nnotes, small pitch intervals and chords, in deference to music theory."
},{
    "category": "stat.ML", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.06265v1", 
    "title": "Deep Clustering and Conventional Networks for Music Separation: Stronger   Together", 
    "arxiv-id": "1611.06265v1", 
    "author": "Nima Mesgarani", 
    "publish": "2016-11-18T22:33:05Z", 
    "summary": "Deep clustering is the first method to handle general audio separation\nscenarios with multiple sources of the same type and an arbitrary number of\nsources, performing impressively in speaker-independent speech separation\ntasks. However, little is known about its effectiveness in other challenging\nsituations such as music source separation. Contrary to conventional networks\nthat directly estimate the source signals, deep clustering generates an\nembedding for each time-frequency bin, and separates sources by clustering the\nbins in the embedding space. We show that deep clustering outperforms\nconventional networks on a singing voice separation task, in both matched and\nmismatched conditions, even though conventional networks have the advantage of\nend-to-end training for best signal approximation, presumably because its more\nflexible objective engenders better regularization. Since the strengths of deep\nclustering and conventional network architectures appear complementary, we\nexplore combining them in a single hybrid network trained via an approach akin\nto multi-task learning. Remarkably, the combination significantly outperforms\neither of its components."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.06986v1", 
    "title": "Robust end-to-end deep audiovisual speech recognition", 
    "arxiv-id": "1611.06986v1", 
    "author": "Fernando De La Torre", 
    "publish": "2016-11-21T20:08:51Z", 
    "summary": "Speech is one of the most effective ways of communication among humans. Even\nthough audio is the most common way of transmitting speech, very important\ninformation can be found in other modalities, such as vision. Vision is\nparticularly useful when the acoustic signal is corrupted. Multi-modal speech\nrecognition however has not yet found wide-spread use, mostly because the\ntemporal alignment and fusion of the different information sources is\nchallenging.\n  This paper presents an end-to-end audiovisual speech recognizer (AVSR), based\non recurrent neural networks (RNN) with a connectionist temporal classification\n(CTC) loss function. CTC creates sparse \"peaky\" output activations, and we\nanalyze the differences in the alignments of output targets (phonemes or\nvisemes) between audio-only, video-only, and audio-visual feature\nrepresentations. We present the first such experiments on the large vocabulary\nIBM ViaVoice database, which outperform previously published approaches on\nphone accuracy in clean and noisy conditions."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.09482v1", 
    "title": "Fast Wavenet Generation Algorithm", 
    "arxiv-id": "1611.09482v1", 
    "author": "Thomas S. Huang", 
    "publish": "2016-11-29T04:16:44Z", 
    "summary": "This paper presents an efficient implementation of the Wavenet generation\nprocess called Fast Wavenet. Compared to a naive implementation that has\ncomplexity O(2^L) (L denotes the number of layers in the network), our proposed\napproach removes redundant convolution operations by caching previous\ncalculations, thereby reducing the complexity to O(L) time. Timing experiments\nshow significant advantages of our fast implementation over a naive one. While\nthis method is presented for Wavenet, the same scheme can be applied anytime\none wants to perform autoregressive generation or online prediction using a\nmodel with dilated convolution layers. The code for our method is publicly\navailable."
},{
    "category": "stat.ML", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1611.09827v1", 
    "title": "Learning Features of Music from Scratch", 
    "arxiv-id": "1611.09827v1", 
    "author": "Sham Kakade", 
    "publish": "2016-11-29T20:26:00Z", 
    "summary": "We introduce a new large-scale music dataset, MusicNet, to serve as a source\nof supervision and evaluation of machine learning methods for music research.\nMusicNet consists of hundreds of freely-licensed classical music recordings by\n10 composers, written for 11 instruments, together with instrument/note\nannotations resulting in over 1 million temporal labels on 34 hours of chamber\nmusic performances under various studio and microphone conditions.\n  We define a multi-label classification task to predict notes in musical\nrecordings, along with an evaluation protocol. We benchmark several machine\nlearning architectures for this task: i) learning from \"hand-crafted\"\nspectrogram features; ii) end-to-end learning with a neural net; iii)\nend-to-end learning with a convolutional neural net. We show that several\nend-to-end learning proposals outperform approaches based on learning from\nhand-crafted audio features."
},{
    "category": "cs.AI", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.01058v1", 
    "title": "Algorithmic Songwriting with ALYSIA", 
    "arxiv-id": "1612.01058v1", 
    "author": "David Loker", 
    "publish": "2016-12-04T03:36:51Z", 
    "summary": "This paper introduces ALYSIA: Automated LYrical SongwrIting Application.\nALYSIA is based on a machine learning model using Random Forests, and we\ndiscuss its success at pitch and rhythm prediction. Next, we show how ALYSIA\nwas used to create original pop songs that were subsequently recorded and\nproduced. Finally, we discuss our vision for the future of Automated\nSongwriting for both co-creative and autonomous systems."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.01943v1", 
    "title": "Segmental Convolutional Neural Networks for Detection of Cardiac   Abnormality With Noisy Heart Sound Recordings", 
    "arxiv-id": "1612.01943v1", 
    "author": "Ethan J. Li", 
    "publish": "2016-12-06T18:37:30Z", 
    "summary": "Heart diseases constitute a global health burden, and the problem is\nexacerbated by the error-prone nature of listening to and interpreting heart\nsounds. This motivates the development of automated classification to screen\nfor abnormal heart sounds. Existing machine learning-based systems achieve\naccurate classification of heart sound recordings but rely on expert features\nthat have not been thoroughly evaluated on noisy recordings. Here we propose a\nsegmental convolutional neural network architecture that achieves automatic\nfeature learning from noisy heart sound recordings. Our experiments show that\nour best model, trained on noisy recording segments acquired with an existing\nhidden semi-markov model-based approach, attains a classification accuracy of\n87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%\naccuracy of the state-of-the-art statistical classifier trained and evaluated\non the same dataset. Our results indicate the potential of using neural\nnetwork-based methods to increase the accuracy of automated classification of\nheart sound recordings for improved screening of heart diseases."
},{
    "category": "cs.IR", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.02350v2", 
    "title": "An Information-theoretic Approach to Machine-oriented Music   Summarization", 
    "arxiv-id": "1612.02350v2", 
    "author": "Ricardo Ribeiro", 
    "publish": "2016-12-07T18:02:09Z", 
    "summary": "Applying generic media-agnostic summarization to music allows for higher\nefficiency in automatic processing, storage, and communication of datasets\nwhile also alleviating copyright issues. This process has already been proven\nuseful in the context of music genre classification. In this paper, we\ngeneralize conclusions from previous work by evaluating the impact of generic\nsummarization in music from a probabilistic perspective and agnostic relative\nto certain tasks. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy to measure how much\ninformation is lost in the summarization process. Based on this observation, we\nfurther propose a simple yet expressive summarization method that objectively\noutperforms previous methods and is better suited to avoid copyright issues. We\npresent results suggesting that relative entropy is a good predictor of\nsummarization performance in the context of tasks relying on a bag-of-features\nmodel."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.03789v1", 
    "title": "A Unit Selection Methodology for Music Generation Using Deep Neural   Networks", 
    "arxiv-id": "1612.03789v1", 
    "author": "Larry Heck", 
    "publish": "2016-12-12T17:06:19Z", 
    "summary": "Several methods exist for a computer to generate music based on data\nincluding Markov chains, recurrent neural networks, recombinancy, and grammars.\nWe explore the use of unit selection and concatenation as a means of generating\nmusic using a procedure based on ranking, where, we consider a unit to be a\nvariable length number of measures of music. We first examine whether a unit\nselection method, that is restricted to a finite size unit library, can be\nsufficient for encompassing a wide spectrum of music. We do this by developing\na deep autoencoder that encodes a musical input and reconstructs the input by\nselecting from the library. We then describe a generative model that combines a\ndeep structured semantic model (DSSM) with an LSTM to predict the next unit,\nwhere units consist of four, two, and one measures of music. We evaluate the\ngenerative model using objective metrics including mean rank and accuracy and\nwith a subjective listening test in which expert musicians are asked to\ncomplete a forced-choiced ranking task. We compare our model to a note-level\ngenerative baseline that consists of a stacked LSTM trained to predict forward\nby one note."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.04742v2", 
    "title": "Imposing higher-level Structure in Polyphonic Music Generation using   Convolutional Restricted Boltzmann Machines and Constraints", 
    "arxiv-id": "1612.04742v2", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-14T17:33:38Z", 
    "summary": "We introduce a method for imposing higher-level structure on generated,\npolyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a\ngenerative model is combined with gradient descent constraint optimization to\nprovide further control over the generation process. Among other things, this\nallows for the use of a \"template\" piece, from which some structural properties\ncan be extracted, and transferred as constraints to newly generated material.\nThe sampling process is guided with Simulated Annealing in order to avoid local\noptima, and find solutions that both satisfy the constraints, and are\nrelatively stable with respect to the C-RBM. Results show that with this\napproach it is possible to control the higher level self-similarity structure,\nthe meter, as well as tonal properties of the resulting musical piece while\npreserving its local musical coherence."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.04744v1", 
    "title": "Incorporating Language Level Information into Acoustic Models", 
    "arxiv-id": "1612.04744v1", 
    "author": "Deliang Wang", 
    "publish": "2016-12-14T17:40:02Z", 
    "summary": "This paper proposed a class of novel Deep Recurrent Neural Networks which can\nincorporate language-level information into acoustic models. For simplicity, we\nnamed these networks Recurrent Deep Language Networks (RDLNs). Multiple\nvariants of RDLNs were considered, including two kinds of context information,\ntwo methods to process the context, and two methods to incorporate the\nlanguage-level information. RDLNs provided possible methods to fine-tune the\nwhole Automatic Speech Recognition (ASR) system in the acoustic modeling\nprocess."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.05070v1", 
    "title": "Towards End-to-End Audio-Sheet-Music Retrieval", 
    "arxiv-id": "1612.05070v1", 
    "author": "Gerhard Widmer", 
    "publish": "2016-12-15T14:07:51Z", 
    "summary": "This paper demonstrates the feasibility of learning to retrieve short\nsnippets of sheet music (images) when given a short query excerpt of music\n(audio) -- and vice versa --, without any symbolic representation of music or\nscores. This would be highly useful in many content-based musical retrieval\nscenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)\nand learns correlated latent spaces allowing for cross-modality retrieval in\nboth directions. Initial experiments with relatively simple monophonic music\nshow promising results."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1612.07523v1", 
    "title": "Robustness of Voice Conversion Techniques Under Mismatched Conditions", 
    "arxiv-id": "1612.07523v1", 
    "author": "Goutam Saha", 
    "publish": "2016-12-22T10:14:59Z", 
    "summary": "Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions."
},{
    "category": "cs.MM", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1701.00599v2", 
    "title": "AENet: Learning Deep Audio Features for Video Analysis", 
    "arxiv-id": "1701.00599v2", 
    "author": "Luc Van Gool", 
    "publish": "2017-01-03T07:35:54Z", 
    "summary": "We propose a new deep network for audio event recognition, called AENet. In\ncontrast to speech, sounds coming from audio events may be produced by a wide\nvariety of sources. Furthermore, distinguishing them often requires analyzing\nan extended time period due to the lack of clear sub-word units that are\npresent in speech. In order to incorporate this long-time frequency structure\nof audio events, we introduce a convolutional neural network (CNN) operating on\na large temporal input. In contrast to previous works this allows us to train\nan audio event detection system end-to-end. The combination of our network\narchitecture and a novel data augmentation outperforms previous methods for\naudio event detection by 16%. Furthermore, we perform transfer learning and\nshow that our model learnt generic audio features, similar to the way CNNs\nlearn generic features on vision tasks. In video analysis, combining visual\nfeatures and traditional audio features such as MFCC typically only leads to\nmarginal improvements. Instead, combining visual features with our AENet\nfeatures, which can be computed efficiently on a GPU, leads to significant\nperformance improvements on action recognition and video highlight detection.\nIn video highlight detection, our audio features improve the performance by\nmore than 8% over visual features alone."
},{
    "category": "cs.LG", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1701.03360v2", 
    "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant   Speech Recognition", 
    "arxiv-id": "1701.03360v2", 
    "author": "Jungwon Lee", 
    "publish": "2017-01-10T20:03:37Z", 
    "summary": "In this paper, a novel architecture for a deep recurrent neural network,\nresidual LSTM is introduced. A plain LSTM has an internal memory cell that can\nlearn long term dependencies of sequential data. It also provides a temporal\nshortcut path to avoid vanishing or exploding gradients in the temporal domain.\nThe residual LSTM provides an additional spatial shortcut path from lower\nlayers for efficient training of deep networks with multiple LSTM layers.\nCompared with the previous work, highway LSTM, residual LSTM separates a\nspatial shortcut path with temporal one by using output layers, which can help\nto avoid a conflict between spatial and temporal-domain gradient flows.\nFurthermore, residual LSTM reuses the output projection matrix and the output\ngate of LSTM to control the spatial information flow instead of additional gate\nnetworks, which effectively reduces more than 10% of network parameters. An\nexperiment for distant speech recognition on the AMI SDM corpus shows that\n10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in\nWER over 3-layer aselines, respectively. On the contrary, 10-layer residual\nLSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%\nWER reduction over plain and highway LSTM networks, respectively."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1702.02092v1", 
    "title": "Characterisation of speech diversity using self-organising maps", 
    "arxiv-id": "1702.02092v1", 
    "author": "David M. W. Powers", 
    "publish": "2017-01-23T11:18:06Z", 
    "summary": "We report investigations into speaker classification of larger quantities of\nunlabelled speech data using small sets of manually phonemically annotated\nspeech. The Kohonen speech typewriter is a semi-supervised method comprised of\nself-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a\n2D array of cells that learn vector representations of the data based on\nneighbourhoods. In this paper, we report a method to evaluate pronunciation\nusing multilevel SOMs with /hVd/ single syllable utterances for the study of\nvowels, for Australian pronunciation."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1702.03791v1", 
    "title": "DNN Filter Bank Cepstral Coefficients for Spoofing Detection", 
    "arxiv-id": "1702.03791v1", 
    "author": "Jun Guo", 
    "publish": "2017-02-13T14:44:17Z", 
    "summary": "With the development of speech synthesis techniques, automatic speaker\nverification systems face the serious challenge of spoofing attack. In order to\nimprove the reliability of speaker verification systems, we develop a new\nfilter bank based cepstral feature, deep neural network filter bank cepstral\ncoefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The\ndeep neural network filter bank is automatically generated by training a filter\nbank neural network (FBNN) using natural and synthetic speech. By adding\nrestrictions on the training rules, the learned weight matrix of FBNN is\nband-limited and sorted by frequency, similar to the normal filter bank. Unlike\nthe manually designed filter bank, the learned filter bank has different filter\nshapes in different channels, which can capture the differences between natural\nand synthetic speech more effectively. The experimental results on the ASVspoof\n{2015} database show that the Gaussian mixture model maximum-likelihood\n(GMM-ML) classifier trained by the new feature performs better than the\nstate-of-the-art linear frequency cepstral coefficients (LFCC) based\nclassifier, especially on detecting unknown attacks."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1702.07787v1", 
    "title": "Convolutional Gated Recurrent Neural Network Incorporating Spatial   Features for Audio Tagging", 
    "arxiv-id": "1702.07787v1", 
    "author": "Mark D. Plumbley", 
    "publish": "2017-02-24T22:27:29Z", 
    "summary": "Environmental audio tagging is a newly proposed task to predict the presence\nor absence of a specific audio event in a chunk. Deep neural network (DNN)\nbased methods have been successfully adopted for predicting the audio tags in\nthe domestic audio scene. In this paper, we propose to use a convolutional\nneural network (CNN) to extract robust features from mel-filter banks (MFBs),\nspectrograms or even raw waveforms for audio tagging. Gated recurrent unit\n(GRU) based recurrent neural networks (RNNs) are then cascaded to model the\nlong-term temporal structure of the audio signal. To complement the input\ninformation, an auxiliary CNN is designed to learn on the spatial features of\nstereo recordings. We evaluate our proposed methods on Task 4 (audio tagging)\nof the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE\n2016) challenge. Compared with our recent DNN-based method, the proposed\nstructure can reduce the equal error rate (EER) from 0.13 to 0.11 on the\ndevelopment set. The spatial features can further reduce the EER to 0.10. The\nperformance of the end-to-end learning on raw waveforms is also comparable.\nFinally, on the evaluation set, we get the state-of-the-art performance with\n0.12 EER while the performance of the best existing system is 0.15 EER."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1702.07825v2", 
    "title": "Deep Voice: Real-time Neural Text-to-Speech", 
    "arxiv-id": "1702.07825v2", 
    "author": "Mohammad Shoeybi", 
    "publish": "2017-02-25T03:11:04Z", 
    "summary": "We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.01720v1", 
    "title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", 
    "arxiv-id": "1703.01720v1", 
    "author": "Devi Parikh", 
    "publish": "2017-03-06T04:30:12Z", 
    "summary": "Sound and vision are the primary modalities that influence how we perceive\nthe world around us. Thus, it is crucial to incorporate information from these\nmodalities into language to help machines interact better with humans. While\nexisting works have explored incorporating visual cues into language\nembeddings, the task of learning word representations that respect auditory\ngrounding remains under-explored. In this work, we propose a new embedding\nscheme, sound-word2vec that learns language embeddings by grounding them in\nsound -- for example, two seemingly unrelated concepts, leaves and paper are\ncloser in our embedding space as they produce similar rustling sounds. We\ndemonstrate that the proposed embeddings perform better than language-only word\nrepresentations, on two purely textual tasks that require reasoning about aural\ncues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest\nneighbors to highlight the unique dependencies captured by sound-w2v as\ncompared to language-only embeddings."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.01789v1", 
    "title": "Sample-level Deep Convolutional Neural Networks for Music Auto-tagging   Using Raw Waveforms", 
    "arxiv-id": "1703.01789v1", 
    "author": "Juhan Nam", 
    "publish": "2017-03-06T09:49:48Z", 
    "summary": "Recently, the end-to-end approach that learns hierarchical representations\nfrom raw data using deep convolutional neural networks has been successfully\nexplored in the image, text and speech domains. This approach was applied to\nmusical signals as well but has been not fully explored yet. To this end, we\npropose sample-level deep convolutional neural networks which learn\nrepresentations from very small grains of waveforms (e.g. 2 or 3 samples)\nbeyond typical frame-level input representations. This allows the networks to\nhierarchically learn filters that are sensitive to log-scaled frequency, such\nas mel-frequency spectrogram that is widely used in music classification\nsystems. It also helps learning high-level abstraction of music by increasing\nthe depth of layers. We show how deep architectures with sample-level filters\nimprove the accuracy in music auto-tagging and they provide results that are\ncom- parable to previous state-of-the-art performances for the Magnatagatune\ndataset and Million song dataset. In addition, we visualize filters learned in\na sample-level DCNN in each layer to identify hierarchically learned features."
},{
    "category": "cs.NE", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.01793v1", 
    "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained   Convolutional Neural Networks for Music Auto-tagging", 
    "arxiv-id": "1703.01793v1", 
    "author": "Juhan Nam", 
    "publish": "2017-03-06T09:57:25Z", 
    "summary": "Music auto-tagging is often handled in a similar manner to image\nclassification by regarding the 2D audio spectrogram as image data. However,\nmusic auto-tagging is distinguished from image classification in that the tags\nare highly diverse and have different levels of abstractions. Considering this\nissue, we propose a convolutional neural networks (CNN)-based architecture that\nembraces multi-level and multi-scaled features. The architecture is trained in\nthree steps. First, we conduct supervised feature learning to capture local\naudio features using a set of CNNs with different input sizes. Second, we\nextract audio features from each layer of the pre-trained convolutional\nnetworks separately and aggregate them altogether given a long audio clip.\nFinally, we put them into fully-connected networks and make final predictions\nof the tags. Our experiments show that using the combination of multi-level and\nmulti-scale features is highly effective in music auto-tagging and the proposed\nmethod outperforms previous state-of-the-arts on the Magnatagatune dataset and\nthe million song dataset. We further show that the proposed architecture is\nuseful in transfer learning."
},{
    "category": "stat.ML", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.02205v1", 
    "title": "Raw Waveform-based Speech Enhancement by Fully Convolutional Networks", 
    "arxiv-id": "1703.02205v1", 
    "author": "Hisashi Kawai", 
    "publish": "2017-03-07T04:03:27Z", 
    "summary": "This paper proposes a fully convolutional network (FCN) model for raw\nwaveform-based speech enhancement. The proposed system performs speech\nenhancement in an end-to-end (i.e. waveform-in and waveform-out) manner, which\ndiffers from most existing denoising methods that process the magnitude\nspectrum (e.g. log power spectrum (LPS)) only. Because the fully connected\nlayers, which are involved in deep neural networks (DNN) and convolutional\nneural networks (CNN), may not accurately characterize local information of\nspeech signals, especially for high-frequency components, we employed fully\nconvolutional layers to model the waveform. More specifically, FCN only\nconsists convolutional layers and hence the local temporal structures of speech\nsignals can be efficiently and effectively preserved with a relatively small\nnumber of weights. Experimental results show that DNN and CNN based models have\nlimited capability to restore high-frequency components of waveforms, thus\nleading to imperfect intelligibility of enhanced speech. On the other hand, the\nproposed FCN model can not only well recover the waveforms but also outperform\nthe LPS-based DNN baseline in terms of STOI and PESQ. In addition, the number\nof model parameters in FCN is roughly only 0.2% compared with that in DNN and\nCNN."
},{
    "category": "cs.SD", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.02317v1", 
    "title": "Convolutional Recurrent Neural Networks for Bird Audio Detection", 
    "arxiv-id": "1703.02317v1", 
    "author": "Tuomas Virtanen", 
    "publish": "2017-03-07T10:36:30Z", 
    "summary": "Bird sounds possess distinctive spectral structure which may exhibit small\nshifts in spectrum depending on the bird species and environmental conditions.\nIn this paper, we propose using convolutional recurrent neural networks on the\ntask of automated bird audio detection in real-life environments. In the\nproposed method, convolutional layers extract high dimensional, local frequency\nshift invariant features, while recurrent layers capture longer term\ndependencies between the features extracted from short time frames. This method\nachieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data\nand obtains the second place in the Bird Audio Detection challenge."
},{
    "category": "cs.CL", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/1703.05880v1", 
    "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic   Modeling", 
    "arxiv-id": "1703.05880v1", 
    "author": "Dong Yu", 
    "publish": "2017-03-17T03:38:48Z", 
    "summary": "Deep learning models (DLMs) are state-of-the-art techniques in speech\nrecognition. However, training good DLMs can be time consuming especially for\nproduction-size models and corpora. Although several parallel training\nalgorithms have been proposed to improve training efficiency, there is no clear\nguidance on which one to choose for the task in hand due to lack of systematic\nand fair comparison among them. In this paper we aim at filling this gap by\ncomparing four popular parallel training algorithms in speech recognition,\nnamely asynchronous stochastic gradient descent (ASGD), blockwise model-update\nfiltering (BMUF), bulk synchronous parallel (BSP) and elastic averaging\nstochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using\nfeed-forward deep neural networks (DNNs) and convolutional, long short-term\nmemory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the\ntop choice to train acoustic models since it is most stable, scales well with\nnumber of GPUs, can achieve reproducible results, and in many cases even\noutperforms single-GPU SGD. ASGD can be used as a substitute in some cases."
},{
    "category": "cs.IT", 
    "doi": "10.1121/1.4972527", 
    "link": "http://arxiv.org/pdf/0709.2225v1", 
    "title": "Improved Linear Parallel Interference Cancellers", 
    "arxiv-id": "0709.2225v1", 
    "author": "L. B. Milstein", 
    "publish": "2007-09-14T07:56:21Z", 
    "summary": "In this paper, taking the view that a linear parallel interference canceller\n(LPIC) can be seen as a linear matrix filter, we propose new linear matrix\nfilters that can result in improved bit error performance compared to other\nLPICs in the literature. The motivation for the proposed filters arises from\nthe possibility of avoiding the generation of certain interference and noise\nterms in a given stage that would have been present in a conventional LPIC\n(CLPIC). In the proposed filters, we achieve such avoidance of the generation\nof interference and noise terms in a given stage by simply making the diagonal\nelements of a certain matrix in that stage equal to zero. Hence, the proposed\nfilters do not require additional complexity compared to the CLPIC, and they\ncan allow achieving a certain error performance using fewer LPIC stages. We\nalso extend the proposed matrix filter solutions to a multicarrier DS-CDMA\nsystem, where we consider two types of receivers. In one receiver (referred to\nas Type-I receiver), LPIC is performed on each subcarrier first, followed by\nmulticarrier combining (MCC). In the other receiver (called Type-II receiver),\nMCC is performed first, followed by LPIC. We show that in both Type-I and\nType-II receivers, the proposed matrix filters outperform other matrix filters.\nAlso, Type-II receiver performs better than Type-I receiver because of enhanced\naccuracy of the interference estimates achieved due to frequency diversity\noffered by MCC."
},{
    "category": "cs.SD", 
    "doi": "10.1145/1370256.1370262", 
    "link": "http://arxiv.org/pdf/0905.1235v2", 
    "title": "The Modular Audio Recognition Framework (MARF) and its Applications:   Scientific and Software Engineering Notes", 
    "arxiv-id": "0905.1235v2", 
    "author": "for the MARF R&D Group", 
    "publish": "2009-05-08T14:42:03Z", 
    "summary": "MARF is an open-source research platform and a collection of\nvoice/sound/speech/text and natural language processing (NLP) algorithms\nwritten in Java and arranged into a modular and extensible framework\nfacilitating addition of new algorithms. MARF can run distributively over the\nnetwork and may act as a library in applications or be used as a source for\nlearning and extension. A few example applications are provided to show how to\nuse the framework. There is an API reference in the Javadoc format as well as\nthis set of accompanying notes with the detailed description of the\narchitectural design, algorithms, and applications. MARF and its applications\nare released under a BSD-style license and is hosted at SourceForge.net. This\ndocument provides the details and the insight on the internals of MARF and some\nof the mentioned applications."
},{
    "category": "cs.CV", 
    "doi": "10.1007/978-90-481-3662-9_72", 
    "link": "http://arxiv.org/pdf/0905.2459v2", 
    "title": "On Design and Implementation of the Distributed Modular Audio   Recognition Framework: Requirements and Specification Design Document", 
    "arxiv-id": "0905.2459v2", 
    "author": "Serguei A. Mokhov", 
    "publish": "2009-05-15T02:52:28Z", 
    "summary": "We present the requirements and design specification of the open-source\nDistributed Modular Audio Recognition Framework (DMARF), a distributed\nextension of MARF. The distributed version aggregates a number of distributed\ntechnologies (e.g. Java RMI, CORBA, Web Services) in a pluggable and modular\nmodel along with the provision of advanced distributed systems algorithms. We\noutline the associated challenges incurred during the design and implementation\nas well as overall specification of the project and its advantages and\nlimitations."
},{
    "category": "cs.IT", 
    "doi": "10.1007/978-90-481-3662-9_72", 
    "link": "http://arxiv.org/pdf/1005.1684v12", 
    "title": "On Macroscopic Complexity and Perceptual Coding", 
    "arxiv-id": "1005.1684v12", 
    "author": "John Scoville", 
    "publish": "2010-05-10T22:41:10Z", 
    "summary": "The theoretical limits of 'lossy' data compression algorithms are considered.\nThe complexity of an object as seen by a macroscopic observer is the size of\nthe perceptual code which discards all information that can be lost without\naltering the perception of the specified observer. The complexity of this\nmacroscopically observed state is the simplest description of any microstate\ncomprising that macrostate. Inference and pattern recognition based on\nmacrostate rather than microstate complexities will take advantage of the\ncomplexity of the macroscopic observer to ignore irrelevant noise."
},{
    "category": "cs.SD", 
    "doi": "10.1038/srep00521", 
    "link": "http://arxiv.org/pdf/1205.5651v1", 
    "title": "Measuring the evolution of contemporary western popular music", 
    "arxiv-id": "1205.5651v1", 
    "author": "Josep Lluis Arcos", 
    "publish": "2012-05-25T09:54:24Z", 
    "summary": "Popular music is a key cultural expression that has captured listeners'\nattention for ages. Many of the structural regularities underlying musical\ndiscourse are yet to be discovered and, accordingly, their historical evolution\nremains formally unknown. Here we unveil a number of patterns and metrics\ncharacterizing the generic usage of primary musical facets such as pitch,\ntimbre, and loudness in contemporary western popular music. Many of these\npatterns and metrics have been consistently stable for a period of more than\nfifty years, thus pointing towards a great degree of conventionalism.\nNonetheless, we prove important changes or trends related to the restriction of\npitch transitions, the homogenization of the timbral palette, and the growing\nloudness levels. This suggests that our perception of the new would be rooted\non these changing characteristics. Hence, an old tune could perfectly sound\nnovel and fashionable, provided that it consisted of common harmonic\nprogressions, changed the instrumentation, and increased the average loudness."
},{
    "category": "cs.SD", 
    "doi": "10.1038/srep00521", 
    "link": "http://arxiv.org/pdf/1508.01746v2", 
    "title": "Using Deep Learning for Detecting Spoofing Attacks on Speech Signals", 
    "arxiv-id": "1508.01746v2", 
    "author": "Ricardo Violato", 
    "publish": "2015-08-07T16:20:52Z", 
    "summary": "It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks."
},{
    "category": "cs.NE", 
    "doi": "10.1038/srep00521", 
    "link": "http://arxiv.org/pdf/1607.02303v2", 
    "title": "CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label   Tree Embeddings for Audio Scene Recognition", 
    "arxiv-id": "1607.02303v2", 
    "author": "Alfred Mertins", 
    "publish": "2016-07-08T10:39:05Z", 
    "summary": "We describe in this report our audio scene recognition system submitted to\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\ntree is automatically constructed. This category taxonomy is then used in the\nfeature extraction step in which an audio scene instance is represented by a\nlabel tree embedding image. Different convolutional neural networks, which are\ntailored for the task at hand, are finally learned on top of the image features\nfor scene recognition. Our system reaches an overall recognition accuracy of\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\nimprovements of 8.7% and 6.1% on the development and test data, respectively."
},{
    "category": "cs.SD", 
    "doi": "10.1038/srep00521", 
    "link": "http://arxiv.org/pdf/1610.04551v3", 
    "title": "Tonal consonance parameters link microscopic and macroscopic properties   of music exposing a hidden order in melody", 
    "arxiv-id": "1610.04551v3", 
    "author": "Rafael Hurtado", 
    "publish": "2016-10-14T17:42:41Z", 
    "summary": "Consonance is related to the perception of pleasantness arising from a\ncombination of sounds and has been approached quantitatively using mathematical\nrelations, physics, information theory, and psychoacoustics. Tonal consonance\nis present in timbre, musical tuning, harmony, and melody, and it is used for\nconveying sensations, perceptions, and emotions in music. It involves the\nphysical properties of sound waves and is used to study melody and harmony\nthrough musical intervals and chords. From the perspective of complexity, the\nmacroscopic properties of a system with many parts frequently rely on the\nstatistical properties of its constituent elements. Here we show how the tonal\nconsonance parameters for complex tones can be used to study complexity in\nmusic. We apply this formalism to melody, showing that melodic lines in musical\npieces can be described in terms of the physical properties of melodic\nintervals and the existence of an entropy extremalization principle subject to\npsychoacoustic macroscopic constraints with musical meaning. This result\nconnects the human perception of consonance with the complexity of human\ncreativity in music through the physical properties of the musical stimulus."
},{
    "category": "cs.CL", 
    "doi": "10.1038/srep00521", 
    "link": "http://arxiv.org/pdf/1612.01928v1", 
    "title": "Invariant Representations for Noisy Speech Recognition", 
    "arxiv-id": "1612.01928v1", 
    "author": "Yoshua Bengio", 
    "publish": "2016-11-27T22:20:51Z", 
    "summary": "Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training."
},{
    "category": "math-ph", 
    "doi": "10.1088/0266-5611/30/3/035004", 
    "link": "http://arxiv.org/pdf/1402.2642v1", 
    "title": "A comprehensive analysis of the geometry of TDOA maps in localisation   problems", 
    "arxiv-id": "1402.2642v1", 
    "author": "Augusto Sarti", 
    "publish": "2014-02-10T18:08:20Z", 
    "summary": "In this manuscript we consider the well-established problem of TDOA-based\nsource localization and propose a comprehensive analysis of its solutions for\narbitrary sensor measurements and placements. More specifically, we define the\nTDOA map from the physical space of source locations to the space of range\nmeasurements (TDOAs), in the specific case of three receivers in 2D space. We\nthen study the identifiability of the model, giving a complete analytical\ncharacterization of the image of this map and its invertibility. This analysis\nhas been conducted in a completely mathematical fashion, using many different\ntools which make it valid for every sensor configuration. These results are the\nfirst step towards the solution of more general problems involving, for\nexample, a larger number of sensors, uncertainty in their placement, or lack of\nsynchronization."
}]