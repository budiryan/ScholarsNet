[{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9809005v1", 
    "other_authors": "Jim Gray, Goetz Graefe", 
    "title": "The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules   of Thumb", 
    "arxiv-id": "cs/9809005v1", 
    "author": "Goetz Graefe", 
    "publish": "1998-09-02T01:49:32Z", 
    "summary": "Simple economic and performance arguments suggest appropriate lifetimes for\nmain memory pages and suggest optimal page sizes. The fundamental tradeoffs are\nthe prices and bandwidths of RAMs and disks. The analysis indicates that with\ntoday's technology, five minutes is a good lifetime for randomly accessed\npages, one minute is a good lifetime for two-pass sequentially accessed pages,\nand 16 KB is a good size for index pages. These rules-of-thumb change in\npredictable ways as technology ratios change. They also motivate the importance\nof the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9809023v2", 
    "other_authors": "Davood Rafiei, Alberto Mendelzon", 
    "title": "Similarity-Based Queries for Time Series Data", 
    "arxiv-id": "cs/9809023v2", 
    "author": "Alberto Mendelzon", 
    "publish": "1998-09-17T14:41:07Z", 
    "summary": "We study a set of linear transformations on the Fourier series representation\nof a sequence that can be used as the basis for similarity queries on\ntime-series data. We show that our set of transformations is rich enough to\nformulate operations such as moving average and time warping. We present a\nquery processing algorithm that uses the underlying R-tree index of a\nmultidimensional data set to answer similarity queries efficiently. Our\nexperiments show that the performance of this algorithm is competitive to that\nof processing ordinary (exact match) queries using the index, and much faster\nthan sequential scanning. We relate our transformations to the general\nframework for similarity queries of Jagadish et al."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9809033v2", 
    "other_authors": "Davood Rafiei, Alberto Mendelzon", 
    "title": "Efficient Retrieval of Similar Time Sequences Using DFT", 
    "arxiv-id": "cs/9809033v2", 
    "author": "Alberto Mendelzon", 
    "publish": "1998-09-18T21:24:23Z", 
    "summary": "We propose an improvement of the known DFT-based indexing technique for fast\nretrieval of similar time sequences. We use the last few Fourier coefficients\nin the distance computation without storing them in the index since every\ncoefficient at the end is the complex conjugate of a coefficient at the\nbeginning and as strong as its counterpart. We show analytically that this\nobservation can accelerate the search time of the index by more than a factor\nof two. This result was confirmed by our experiments, which were carried out on\nreal stock prices and synthetic data."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9909016v1", 
    "other_authors": "Francis C. Chu, Joseph Y. Halpern, Praveen Seshadri", 
    "title": "Least expected cost query optimization: an exercise in utility", 
    "arxiv-id": "cs/9909016v1", 
    "author": "Praveen Seshadri", 
    "publish": "1999-09-21T21:20:20Z", 
    "summary": "We identify two unreasonable, though standard, assumptions made by database\nquery optimizers that can adversely affect the quality of the chosen evaluation\nplans. One assumption is that it is enough to optimize for the expected\ncase---that is, the case where various parameters (like available memory) take\non their expected value. The other assumption is that the parameters are\nconstant throughout the execution of the query. We present an algorithm based\non the ``System R''-style query optimization algorithm that does not rely on\nthese assumptions. The algorithm we present chooses the plan of the least\nexpected cost instead of the plan of least cost given some fixed value of the\nparameters. In execution environments that exhibit a high degree of\nvariability, our techniques should result in better performance."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9910021v1", 
    "other_authors": "Prasan Roy, S. Seshadri, S. Sudarshan, Siddhesh Bhobe", 
    "title": "Efficient and Extensible Algorithms for Multi Query Optimization", 
    "arxiv-id": "cs/9910021v1", 
    "author": "Siddhesh Bhobe", 
    "publish": "1999-10-25T16:30:20Z", 
    "summary": "Complex queries are becoming commonplace, with the growing use of decision\nsupport systems. These complex queries often have a lot of common\nsub-expressions, either within a single query, or across multiple such queries\nrun as a batch. Multi-query optimization aims at exploiting common\nsub-expressions to reduce evaluation cost. Multi-query optimization has\nhither-to been viewed as impractical, since earlier algorithms were exhaustive,\nand explore a doubly exponential search space.\n  In this paper we demonstrate that multi-query optimization using heuristics\nis practical, and provides significant benefits. We propose three cost-based\nheuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple\nmodifications to the Volcano search strategy, and a greedy heuristic. Our\ngreedy heuristic incorporates novel optimizations that improve efficiency\ngreatly. Our algorithms are designed to be easily added to existing optimizers.\nWe present a performance study comparing the algorithms, using workloads\nconsisting of queries from the TPC-D benchmark. The study shows that our\nalgorithms provide significant benefits over traditional optimization, at a\nvery acceptable overhead in optimization time."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/9912015v1", 
    "other_authors": "Angela Bonifati, Stefano Ceri", 
    "title": "Comparative Analysis of Five XML Query Languages", 
    "arxiv-id": "cs/9912015v1", 
    "author": "Stefano Ceri", 
    "publish": "1999-12-22T15:25:55Z", 
    "summary": "XML is becoming the most relevant new standard for data representation and\nexchange on the WWW. Novel languages for extracting and restructuring the XML\ncontent have been proposed, some in the tradition of database query languages\n(i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query\nlanguage has yet been decided, but the discussion is ongoing within the World\nWide Web Consortium and within many academic institutions and Internet-related\nmajor companies. We present a comparison of five, representative query\nlanguages for XML, highlighting their common features and differences."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0003005v1", 
    "other_authors": "Prasan Roy, Krithi Ramamritham, S. Seshadri, Pradeep Shenoy, S. Sudarshan", 
    "title": "Don't Trash your Intermediate Results, Cache 'em", 
    "arxiv-id": "cs/0003005v1", 
    "author": "S. Sudarshan", 
    "publish": "2000-03-02T08:15:21Z", 
    "summary": "In data warehouse and data mart systems, queries often take a long time to\nexecute due to their complex nature. Query response times can be greatly\nimproved by caching final/intermediate results of previous queries, and using\nthem to answer later queries. In this paper we describe a caching system called\nExchequer which incorporates several novel features including optimization\naware cache maintenance and the use of a cache aware optimizer. In contrast, in\nexisting work, the module that makes cost-benefit decisions is part of the\ncache manager and works independent of the optimizer which essentially\nreconsiders these decisions while finding the best plan for a query. In our\nwork, the optimizer takes the decisions for the cache manager. Furthermore,\nexisting approaches are either restricted to cube (slice/point) queries, or\ncache just the query results. On the other hand, our work is extens ible and in\nfact presents a data-model independent framework and algorithm. Our\nexperimental results attest to the efficacy of our cache management techniques\nand show that over a wide range of parameters (a) Exchequer's query response\ntimes are lower by more than 30% compared to the best performing competitor,\nand (b) Exchequer can deliver the same response time as its competitor with\njust one tenth of the cache size."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0003006v1", 
    "other_authors": "Hoshi Mistry, Prasan Roy, Krithi Ramamritham, S. Sudarshan", 
    "title": "Materialized View Selection and Maintenance Using Multi-Query   Optimization", 
    "arxiv-id": "cs/0003006v1", 
    "author": "S. Sudarshan", 
    "publish": "2000-03-02T08:05:24Z", 
    "summary": "Because the presence of views enhances query performance, materialized views\nare increasingly being supported by commercial database/data warehouse systems.\nWhenever the data warehouse is updated, the materialized views must also be\nupdated. However, whereas the amount of data entering a warehouse, the query\nloads, and the need to obtain up-to-date responses are all increasing, the time\nwindow available for making the warehouse up-to-date is shrinking. These trends\nnecessitate efficient techniques for the maintenance of materialized views.\n  In this paper, we show how to find an efficient plan for maintenance of a\n{\\em set} of views, by exploiting common subexpressions between different view\nmaintenance expressions. These common subexpressions may be materialized\ntemporarily during view maintenance. Our algorithms also choose\nsubexpressions/indices to be materialized permanently (and maintained along\nwith other materialized views), to speed up view maintenance. While there has\nbeen much work on view maintenance in the past, our novel contributions lie in\nexploiting a recently developed framework for multiquery optimization to\nefficiently find good view maintenance plans as above. In addition to faster\nview maintenance, our algorithms can also be used to efficiently select\nmaterialized views to speed up workloads containing queries."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0007044v2", 
    "other_authors": "Avigdor Gal, Jonathan Eckstein", 
    "title": "Managing Periodically Updated Data in Relational Databases: A Stochastic   Modeling Approach", 
    "arxiv-id": "cs/0007044v2", 
    "author": "Jonathan Eckstein", 
    "publish": "2000-07-31T20:15:08Z", 
    "summary": "Recent trends in information management involve the periodic transcription of\ndata onto secondary devices in a networked environment, and the proper\nscheduling of these transcriptions is critical for efficient data management.\nTo assist in the scheduling process, we are interested in modeling the\nreduction of consistency over time between a relation and its replica, termed\nobsolescence of data. The modeling is based on techniques from the field of\nstochastic processes, and provides several stochastic models for content\nevolution in the base relations of a database, taking referential integrity\nconstraints into account. These models are general enough to accommodate most\nof the common scenarios in databases, including batch insertions and life spans\nboth with and without memory. As an initial \"proof of concept\" of the\napplicability of our approach, we validate the insertion portion of our model\nframework via experiments with real data feeds. We also discuss a set of\ntranscription protocols which make use of the proposed stochastic model."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0011024v1", 
    "other_authors": "Sara Cohen, Werner Nutt, Alexander Serebrenik", 
    "title": "Algorithms for Rewriting Aggregate Queries Using Views", 
    "arxiv-id": "cs/0011024v1", 
    "author": "Alexander Serebrenik", 
    "publish": "2000-11-17T12:16:43Z", 
    "summary": "Queries involving aggregation are typical in database applications. One of\nthe main ideas to optimize the execution of an aggregate query is to reuse\nresults of previously answered queries. This leads to the problem of rewriting\naggregate queries using views. Due to a lack of theory, algorithms for this\nproblem were rather ad-hoc. They were sound, but were not proven to be\ncomplete.\n  Recently we have given syntactic characterizations for the equivalence of\naggregate queries and applied them to decide when there exist rewritings.\nHowever, these decision procedures do not lend themselves immediately to an\nimplementation. In this paper, we present practical algorithms for rewriting\nqueries with $\\COUNT$ and $\\SUM$. Our algorithms are sound. They are also\ncomplete for important cases. Our techniques can be used to improve well-known\nprocedures for rewriting non-aggregate queries. These procedures can then be\nadapted to obtain algorithms for rewriting queries with $\\MIN$ and $\\MAX$. The\nalgorithms presented are a basis for realizing optimizers that rewrite queries\nusing views."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0011041v1", 
    "other_authors": "Sara Cohen, Yaron Kanza, Yakov Kogan, Werner Nutt, Yehoshua Sagiv, Alexander Serebrenik", 
    "title": "EquiX---A Search and Query Language for XML", 
    "arxiv-id": "cs/0011041v1", 
    "author": "Alexander Serebrenik", 
    "publish": "2000-11-27T09:15:08Z", 
    "summary": "EquiX is a search language for XML that combines the power of querying with\nthe simplicity of searching. Requirements for such languages are discussed and\nit is shown that EquiX meets the necessary criteria. Both a graphical abstract\nsyntax and a formal concrete syntax are presented for EquiX queries. In\naddition, the semantics is defined and an evaluation algorithm is presented.\nThe evaluation algorithm is polynomial under combined complexity.\n  EquiX combines pattern matching, quantification and logical expressions to\nquery both the data and meta-data of XML documents. The result of a query in\nEquiX is a set of XML documents. A DTD describing the result documents is\nderived automatically from the query."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0103004v1", 
    "other_authors": "Paul M. Aoki, Ian E. Smith, James D. Thornton", 
    "title": "Rapid Application Evolution and Integration Through Document   Metamorphosis", 
    "arxiv-id": "cs/0103004v1", 
    "author": "James D. Thornton", 
    "publish": "2001-03-02T20:00:16Z", 
    "summary": "The Harland document management system implements a data model in which\ndocument (object) structure can be altered by mixin-style multiple inheritance\nat any time. This kind of structural fluidity has long been supported by\nknowledge-base management systems, but its use has primarily been in support of\nreasoning and inference. In this paper, we report our experiences building and\nsupporting several non-trivial applications on top of this data model. Based on\nthese experiences, we argue that structural fluidity is convenient for\ndata-intensive applications other than knowledge-base management. Specifically,\nwe suggest that this flexible data model is a natural fit for the decoupled\nprogramming methodology that arises naturally when using enterprise component\nframeworks."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0106055v1", 
    "other_authors": "Raj P. Gopalan, Tariq Nuruddin, Yudho Giri Sucahyo", 
    "title": "A Seamless Integration of Association Rule Mining with Database Systems", 
    "arxiv-id": "cs/0106055v1", 
    "author": "Yudho Giri Sucahyo", 
    "publish": "2001-06-28T06:02:46Z", 
    "summary": "The need for Knowledge and Data Discovery Management Systems (KDDMS) that\nsupport ad hoc data mining queries has been long recognized. A significant\namount of research has gone into building tightly coupled systems that\nintegrate association rule mining with database systems. In this paper, we\ndescribe a seamless integration scheme for database queries and association\nrule discovery using a common query optimizer for both. Query trees of\nexpressions in an extended algebra are used for internal representation in the\noptimizer. The algebraic representation is flexible enough to deal with\nconstrained association rule queries and other variations of association rule\nspecifications. We propose modularization to simplify the query tree for\ncomplex tasks in data mining. It paves the way for making use of existing\nalgorithms for constructing query plans in the optimization process. How the\nintegration scheme we present will facilitate greater user control over the\ndata mining process is also discussed. The work described in this paper forms\npart of a larger project for fully integrating data mining with database\nmanagement."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0109084v1", 
    "other_authors": "John B. Horrigan", 
    "title": "The Internet and Community Networks: Case Studies of Five U.S. Cities", 
    "arxiv-id": "cs/0109084v1", 
    "author": "John B. Horrigan", 
    "publish": "2001-09-24T20:42:05Z", 
    "summary": "This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,\nand Washington, DC) and explores strategies being employed by community\nactivists and local governments to create and sustain community networking\nprojects. In some cities, community networking initiatives are relatively\nmature, while in others they are in early or intermediate stages. The paper\nlooks at several factors that help explain the evolution of community networks\nin cities:\n  1) Local government support; 2) Federal support 3) Degree of community\nactivism, often reflected by public-private partnerships that help support\ncommunity networks.\n  In addition to these (more or less) measurable elements of local support, the\ncase studies enable description of the different objectives of community\nnetworks in different cities. Several community networking projects aim to\nimprove the delivery of government services (e.g., Portland and Cleveland),\nsome have a job-training focus (e.g., Austin, Washington, DC), others are\noriented very explicitly toward community building (Nashville, DC), and others\ntoward neighborhood entrepreneurship (Portland and Cleveland).\n  The paper ties the case studies together by asking whether community\ntechnology initiatives contribute to social capital in the cities studied."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0110020v1", 
    "other_authors": "N. L. Sarda", 
    "title": "Structuring Business Metadata in Data Warehouse Systems for Effective   Business Support", 
    "arxiv-id": "cs/0110020v1", 
    "author": "N. L. Sarda", 
    "publish": "2001-10-08T06:29:14Z", 
    "summary": "Large organizations today are being served by different types of data\nprocessing and informations systems, ranging from the operational (OLTP)\nsystems, data warehouse systems, to data mining and business intelligence\napplications. It is important to create an integrated repository of what these\nsystems contain and do in order to use them collectively and effectively. The\nrepository contains metadata of source systems, data warehouse, and also the\nbusiness metadata. Decision support and business analysis require extensive and\nin-depth understanding of business entities, tasks, rules and the environment.\nThe purpose of business metadata is to provide this understanding. Realizing\nthe importance of metadata, many standardization efforts has been initiated to\ndefine metadata models. In trying to define an integrated metadata and\ninformation systems for a banking application, we discover some important\nlimitations or inadequacies of the business metadata proposals. They relate to\nproviding an integrated and flexible inter-operability and navigation between\nmetadata and data, and to the important issue of systematically handling\ntemporal characteristics and evolution of the metadata itself.\n  In this paper, we study the issue of structuring business metadata so that it\ncan provide a context for business management and decision support when\nintegrated with data warehousing. We define temporal object-oriented business\nmetadata model, and relate it both to the technical metadata and the data\nwarehouse. We also define ways of accessing and navigating metadata in\nconjunction with data."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0110044v1", 
    "other_authors": "Sara Cohen, Yaron Kanza, Yakov Kogan, Werner Nutt, Yehoshua Sagiv, Alexander Serebrenik", 
    "title": "EquiX--A Search and Query Language for XML", 
    "arxiv-id": "cs/0110044v1", 
    "author": "Alexander Serebrenik", 
    "publish": "2001-10-22T08:03:50Z", 
    "summary": "EquiX is a search language for XML that combines the power of querying with\nthe simplicity of searching. Requirements for such languages are discussed and\nit is shown that EquiX meets the necessary criteria. Both a graph-based\nabstract syntax and a formal concrete syntax are presented for EquiX queries.\nIn addition, the semantics is defined and an evaluation algorithm is presented.\nThe evaluation algorithm is polynomial under combined complexity.\n  EquiX combines pattern matching, quantification and logical expressions to\nquery both the data and meta-data of XML documents. The result of a query in\nEquiX is a set of XML documents. A DTD describing the result documents is\nderived automatically from the query."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0110052v1", 
    "other_authors": "N. L. Sarda, Ankur Jain", 
    "title": "Mragyati : A System for Keyword-based Searching in Databases", 
    "arxiv-id": "cs/0110052v1", 
    "author": "Ankur Jain", 
    "publish": "2001-10-25T08:55:57Z", 
    "summary": "The web, through many search engine sites, has popularized the keyword-based\nsearch paradigm, where a user can specify a string of keywords and expect to\nretrieve relevant documents, possibly ranked by their relevance to the query.\nSince a lot of information is stored in databases (and not as HTML documents),\nit is important to provide a similar search paradigm for databases, where users\ncan query a database without knowing the database schema and database query\nlanguages such as SQL. In this paper, we propose such a database search system,\nwhich accepts a free-form query as a collection of keywords, translates it into\nqueries on the database using the database metadata, and presents query results\nin a well-structured and browsable form. Th eysytem maps keywords onto the\ndatabase schema and uses inter-relationships (i.e., data semantics) among the\nreferred tables to generate meaningful query results. We also describe our\nprototype for database search, called Mragyati. Th eapproach proposed here is\nscalable, as it does not build an in-memory graph of the entire database for\nsearching for relationships among the objects selected by the user's query."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0111004v1", 
    "other_authors": "D. E. R. Quock, F. H. Munson, K. J. Eder, S. L. Dean", 
    "title": "The Relational Database Aspects of Argonne's ATLAS Control System", 
    "arxiv-id": "cs/0111004v1", 
    "author": "S. L. Dean", 
    "publish": "2001-11-01T22:07:01Z", 
    "summary": "The Relational Database Aspects of Argonnes ATLAS Control System Argonnes\nATLAS (Argonne Tandem Linac Accelerator System) control system comprises two\nseparate database concepts. The first is the distributed real-time database\nstructure provided by the commercial product Vsystem [1]. The second is a more\nstatic relational database archiving system designed by ATLAS personnel using\nOracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS\nfacility has presented a unique opportunity to construct a control system\nrelational database that is capable of storing and retrieving complete archived\ntune-up configurations for the entire accelerator. This capability has been a\nmajor factor in allowing the facility to adhere to a rigorous operating\nschedule. Most recently, a Web-based operator interface to the control systems\nOracle Rdb database has been installed. This paper explains the history of the\nATLAS database systems, how they interact with each other, the design of the\nnew Web-based operator interface, and future plans."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0111006v1", 
    "other_authors": "Robert Soliday", 
    "title": "Proliferation of SDDS Support for Various Platforms and Languages", 
    "arxiv-id": "cs/0111006v1", 
    "author": "Robert Soliday", 
    "publish": "2001-11-05T18:14:16Z", 
    "summary": "Since Self-Describing Data Sets (SDDS) were first introduced, the source code\nhas been ported to many different operating systems and various languages. SDDS\nis now available in C, Tcl, Java, Fortran, and Python. All of these versions\nare supported on Solaris, Linux, and Windows. The C version of SDDS is also\nsupported on VxWorks. With the recent addition of the Java port, SDDS can now\nbe deployed on virtually any operating system. Due to this proliferation, SDDS\nfiles serve to link not only a collection of C programs, but programs and\nscripts in many languages on different operating systems. The platform\nindependent binary feature of SDDS also facilitates portability among operating\nsystems. This paper presents an overview of various benefits of SDDS platform\ninteroperability."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-34109-0_10", 
    "link": "http://arxiv.org/pdf/cs/0202035v1", 
    "other_authors": "Satyanarayana R Valluri, Soujanya Vadapalli, Kamalakar Karlapalem", 
    "title": "Sprinkling Selections over Join DAGs for Efficient Query Optimization", 
    "arxiv-id": "cs/0202035v1", 
    "author": "Kamalakar Karlapalem", 
    "publish": "2002-02-21T05:23:51Z", 
    "summary": "In optimizing queries, solutions based on AND/OR DAG can generate all\npossible join orderings and select placements before searching for optimal\nquery execution strategy. But as the number of joins and selection conditions\nincrease, the space and time complexity to generate optimal query plan\nincreases exponentially. In this paper, we use join graph for a relational\ndatabase schema to either pre-compute all possible join orderings that can be\nexecuted and store it as a join DAG or, extract joins in the queries to\nincrementally build a history join DAG as and when the queries are executed.\nThe select conditions in the queries are appropriately placed in the retrieved\njoin DAG (or, history join DAG) to generate optimal query execution strategy.\nWe experimentally evaluate our query optimization technique on TPC-D/H query\nsets to show their effectiveness over AND/OR DAG query optimization strategy.\nFinally, we illustrate how our technique can be used for efficient multiple\nquery optimization and selection of materialized views in data warehousing\nenvironments."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0202037v2", 
    "other_authors": "Jan Van den Bussche, Stijn Vansummeren, Gottfried Vossen", 
    "title": "Towards practical meta-querying", 
    "arxiv-id": "cs/0202037v2", 
    "author": "Gottfried Vossen", 
    "publish": "2002-02-25T19:35:12Z", 
    "summary": "We describe a meta-querying system for databases containing queries in\naddition to ordinary data. In the context of such databases, a meta-query is a\nquery about queries. Representing stored queries in XML, and using the standard\nXML manipulation language XSLT as a sublanguage, we show that just a few\nfeatures need to be added to SQL to turn it into a fully-fledged meta-query\nlanguage. The good news is that these features can be directly supported by\nextensible database technology."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0204010v1", 
    "other_authors": "Jan Chomicki, Jerzy Marcinkowski", 
    "title": "On the Computational Complexity of Consistent Query Answers", 
    "arxiv-id": "cs/0204010v1", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2002-04-05T22:24:33Z", 
    "summary": "We consider here the problem of obtaining reliable, consistent information\nfrom inconsistent databases -- databases that do not have to satisfy given\nintegrity constraints. We use the notion of consistent query answer -- a query\nanswer which is true in every (minimal) repair of the database. We provide a\ncomplete classification of the computational complexity of consistent answers\nto first-order queries w.r.t. functional dependencies and denial constraints.\nWe show how the complexity depends on the {\\em type} of the constraints\nconsidered, their {\\em number}, and the {\\em size} of the query. We obtain\nseveral new PTIME cases, using new algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0205060v1", 
    "other_authors": "Christoph Koch", 
    "title": "Optimizing Queries Using a Meta-level Database", 
    "arxiv-id": "cs/0205060v1", 
    "author": "Christoph Koch", 
    "publish": "2002-05-23T14:15:55Z", 
    "summary": "Graph simulation (using graph schemata or data guides) has been successfully\nproposed as a technique for adding structure to semistructured data. Design\npatterns for description (such as meta-classes and homomorphisms between schema\nlayers), which are prominent in the object-oriented programming community,\nconstitute a generalization of this graph simulation approach.\n  In this paper, we show description applicable to a wide range of data models\nthat have some notion of object (-identity), and propose to turn it into a data\nmodel primitive much like, say, inheritance. We argue that such an extension\nfills a practical need in contemporary data management. Then, we present\nalgebraic techniques for query optimization (using the notions of described and\ndescription queries). Finally, in the semistructured setting, we discuss the\npruning of regular path queries (with nested conditions) using description\nmeta-data. In this context, our notion of meta-data extends graph schemata and\ndata guides by meta-level values, allowing to boost query performance and to\nreduce the redundancy of data."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0207093v1", 
    "other_authors": "Jan Chomicki", 
    "title": "Preference Queries", 
    "arxiv-id": "cs/0207093v1", 
    "author": "Jan Chomicki", 
    "publish": "2002-07-27T00:46:50Z", 
    "summary": "The handling of user preferences is becoming an increasingly important issue\nin present-day information systems. Among others, preferences are used for\ninformation filtering and extraction to reduce the volume of data presented to\nthe user. They are also used to keep track of user profiles and formulate\npolicies to improve and automate decision making.\n  We propose here a simple, logical framework for formulating preferences as\npreference formulas. The framework does not impose any restrictions on the\npreference relations and allows arbitrary operation and predicate signatures in\npreference formulas. It also makes the composition of preference relations\nstraightforward. We propose a simple, natural embedding of preference formulas\ninto relational algebra (and SQL) through a single winnow operator\nparameterized by a preference formula. The embedding makes possible the\nformulation of complex preference queries, e.g., involving aggregation, by\npiggybacking on existing SQL constructs. It also leads in a natural way to the\ndefinition of further, preference-related concepts like ranking. Finally, we\npresent general algebraic laws governing the winnow operator and its\ninteraction with other relational algebra operators. The preconditions on the\napplicability of the laws are captured by logical formulas. The laws provide a\nformal foundation for the algebraic optimization of preference queries. We\ndemonstrate the usefulness of our approach through numerous examples."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0207094v1", 
    "other_authors": "Marcelo Arenas, Leopoldo Bertossi, Jan Chomicki", 
    "title": "Answer Sets for Consistent Query Answering in Inconsistent Databases", 
    "arxiv-id": "cs/0207094v1", 
    "author": "Jan Chomicki", 
    "publish": "2002-07-26T21:18:50Z", 
    "summary": "A relational database is inconsistent if it does not satisfy a given set of\nintegrity constraints. Nevertheless, it is likely that most of the data in it\nis consistent with the constraints. In this paper we apply logic programming\nbased on answer sets to the problem of retrieving consistent information from a\npossibly inconsistent database. Since consistent information persists from the\noriginal database to every of its minimal repairs, the approach is based on a\nspecification of database repairs using disjunctive logic programs with\nexceptions, whose answer set semantics can be represented and computed by\nsystems that implement stable model semantics. These programs allow us to\ndeclare persistence by defaults and repairing changes by exceptions. We\nconcentrate mainly on logic programs for binary integrity constraints, among\nwhich we find most of the integrity constraints found in practice."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0211020v2", 
    "other_authors": "Georg Gottlob, Christoph Koch", 
    "title": "Monadic Datalog and the Expressive Power of Languages for Web   Information Extraction", 
    "arxiv-id": "cs/0211020v2", 
    "author": "Christoph Koch", 
    "publish": "2002-11-15T20:01:54Z", 
    "summary": "Research on information extraction from Web pages (wrapping) has seen much\nactivity recently (particularly systems implementations), but little work has\nbeen done on formally studying the expressiveness of the formalisms proposed or\non the theoretical foundations of wrapping. In this paper, we first study\nmonadic datalog over trees as a wrapping language. We show that this simple\nlanguage is equivalent to monadic second order logic (MSO) in its ability to\nspecify wrappers. We believe that MSO has the right expressiveness required for\nWeb information extraction and propose MSO as a yardstick for evaluating and\ncomparing wrappers. Along the way, several other results on the complexity of\nquery evaluation and query containment for monadic datalog over trees are\nestablished, and a simple normal form for this language is presented. Using the\nabove results, we subsequently study the kernel fragment Elog$^-$ of the Elog\nwrapping language used in the Lixto system (a visual wrapper generator).\nCuriously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,\nprograms in this language can be entirely visually specified."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0212004v1", 
    "other_authors": "Jan Chomicki, Jerzy Marcinkowski", 
    "title": "Minimal-Change Integrity Maintenance Using Tuple Deletions", 
    "arxiv-id": "cs/0212004v1", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2002-12-05T16:23:35Z", 
    "summary": "We address the problem of minimal-change integrity maintenance in the context\nof integrity constraints in relational databases. We assume that\nintegrity-restoration actions are limited to tuple deletions. We identify two\nbasic computational issues: repair checking (is a database instance a repair of\na given database?) and consistent query answers (is a tuple an answer to a\ngiven query in every repair of a given database?). We study the computational\ncomplexity of both problems, delineating the boundary between the tractable and\nthe intractable. We consider denial constraints, general functional and\ninclusion dependencies, as well as key and foreign key constraints. Our results\nshed light on the computational feasibility of minimal-change integrity\nmaintenance. The tractable cases should lead to practical implementations. The\nintractability results highlight the inherent limitations of any integrity\nenforcement mechanism, e.g., triggers or referential constraint actions, as a\nway of performing minimal-change integrity maintenance."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0212017v1", 
    "other_authors": "Jan Chomicki, Sofie Haesevoets, Bart Kuijpers, Peter Revesz", 
    "title": "Classes of Spatiotemporal Objects and Their Closure Properties", 
    "arxiv-id": "cs/0212017v1", 
    "author": "Peter Revesz", 
    "publish": "2002-12-09T17:29:12Z", 
    "summary": "We present a data model for spatio-temporal databases. In this model\nspatio-temporal data is represented as a finite union of objects described by\nmeans of a spatial reference object, a temporal object and a geometric\ntransformation function that determines the change or movement of the reference\nobject in time.\n  We define a number of practically relevant classes of spatio-temporal\nobjects, and give complete results concerning closure under Boolean set\noperators for these classes. Since only few classes are closed under all set\noperators, we suggest an extension of the model, which leads to better closure\nproperties, and therefore increased practical applicability. We also discuss a\nnormal form for this extended data model."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0212051v1", 
    "other_authors": "Asuman Dogac, Gokce Laleci, Yildiray Kabak, Ibrahim Cingil", 
    "title": "ExploitingWeb Service Semantics: Taxonomies vs. Ontologies", 
    "arxiv-id": "cs/0212051v1", 
    "author": "Ibrahim Cingil", 
    "publish": "2002-12-23T10:26:36Z", 
    "summary": "Comprehensive semantic descriptions of Web services are essential to exploit\nthem in their full potential, that is, discovering them dynamically, and\nenabling automated service negotiation, composition and monitoring. The\nsemantic mechanisms currently available in service registries which are based\non taxonomies fail to provide the means to achieve this. Although the terms\ntaxonomy and ontology are sometimes used interchangably there is a critical\ndifference. A taxonomy indicates only class/subclass relationship whereas an\nontology describes a domain completely. The essential mechanisms that ontology\nlanguages provide include their formal specification (which allows them to be\nqueried) and their ability to define properties of classes. Through properties\nvery accurate descriptions of services can be defined and services can be\nrelated to other services or resources. In this paper, we discuss the\nadvantages of describing service semantics through ontology languages and\ndescribe how to relate the semantics defined with the services advertised in\nservice registries like UDDI and ebXML."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0212052v1", 
    "other_authors": "Asuman Dogac, Ibrahim Cingil, Gokce Laleci, Yildiray Kabak", 
    "title": "Improving the Functionality of UDDI Registries through Web Service   Semantics", 
    "arxiv-id": "cs/0212052v1", 
    "author": "Yildiray Kabak", 
    "publish": "2002-12-25T15:15:16Z", 
    "summary": "In this paper we describe a framework for exploiting the semantics of Web\nservices through UDDI registries. As a part of this framework, we extend the\nDAML-S upper ontology to describe the functionality we find essential for\ne-businesses. This functionality includes relating the services with electronic\ncatalogs, describing the complementary services and finding services according\nto the properties of products or services. Once the semantics is defined, there\nis a need for a mechanism in the service registry to relate it with the service\nadvertised. The ontology model developed is general enough to be used with any\nservice registry. However when it comes to relating the semantics with services\nadvertised, the capabilities provided by the registry effects how this is\nachieved. We demonstrate how to integrate the described service semantics to\nUDDI registries."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0301009v1", 
    "other_authors": "Qingguo Zheng", 
    "title": "A Script Language for Data Integration in Database", 
    "arxiv-id": "cs/0301009v1", 
    "author": "Qingguo Zheng", 
    "publish": "2003-01-13T05:34:39Z", 
    "summary": "A Script Language in this paper is designed to transform the original data\ninto the target data by the computing formula. The Script Language can be\ntranslated into the corresponding SQL Language, and the computation is finally\nimplemented by the first type of dynamic SQL. The Script Language has the\noperations of insert, update, delete, union, intersect, and minus for the table\nin the database.The Script Language is edited by a text file and you can easily\nmodify the computing formula in the text file to deal with the situations when\nthe computing formula have been changed. So you only need modify the text of\nthe script language, but needn't change the programs that have complied."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0301017v1", 
    "other_authors": "Millist W. Vincent, Jixue Liu", 
    "title": "Completeness and Decidability Properties for Functional Dependencies in   XML", 
    "arxiv-id": "cs/0301017v1", 
    "author": "Jixue Liu", 
    "publish": "2003-01-20T00:13:18Z", 
    "summary": "XML is of great importance in information storage and retrieval because of\nits recent emergence as a standard for data representation and interchange on\nthe Internet. However XML provides little semantic content and as a result\nseveral papers have addressed the topic of how to improve the semantic\nexpressiveness of XML. Among the most important of these approaches has been\nthat of defining integrity constraints in XML. In a companion paper we defined\nstrong functional dependencies in XML(XFDs). We also presented a set of axioms\nfor reasoning about the implication of XFDs and showed that the axiom system is\nsound for arbitrary XFDs. In this paper we prove that the axioms are also\ncomplete for unary XFDs (XFDs with a single path on the l.h.s.). The second\ncontribution of the paper is to prove that the implication problem for unary\nXFDs is decidable and to provide a linear time algorithm for it."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0305038v1", 
    "other_authors": "Nancy Hartline Bercich", 
    "title": "The Evolution of the Computerized Database", 
    "arxiv-id": "cs/0305038v1", 
    "author": "Nancy Hartline Bercich", 
    "publish": "2003-05-21T15:59:56Z", 
    "summary": "Databases, collections of related data, are as old as the written word. A\ndatabase can be anything from a homemaker's metal recipe file to a\nsophisticated data warehouse. Yet today, when we think of a database we\ninvariably think of computerized data and their DBMSs (database management\nsystems). How did we go from organizing our data in a simple metal filing box\nor cabinet to storing our data in a sophisticated computerized database? How\ndid the computerized database evolve?\n  This paper defines what we mean by a database. It traces the evolution of the\ndatabase, from its start as a non-computerized set of related data, to the, now\nstandard, computerized RDBMS (relational database management system). Early\ncomputerized storage methods are reviewed including both the ISAM (Indexed\nSequential Access Method) and VSAM (Virtual Storage Access Method) storage\nmethods. Early database models are explored including the network and\nhierarchical database models. Eventually, the relational, object-relational and\nobject-oriented databases models are discussed. An appendix of diagrams,\nincluding hierarchical occurrence tree, network schema, ER (entity\nrelationship) and UML (unified modeling language) diagrams, is included to\nsupport the text.\n  This paper concludes with an exploration of current and future trends in DBMS\ndevelopment. It discusses the factors affecting these trends. It delves into\nthe relationship between DBMSs and the increasingly popular object-oriented\ndevelopment methodologies. Finally, it speculates on the future of the DBMS."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306006v2", 
    "other_authors": "A. Amorim, J. Lima, C. Oliveira, L. Pedro, N. Barros", 
    "title": "Experience with the Open Source based implementation for ATLAS   Conditions Data Management System", 
    "arxiv-id": "cs/0306006v2", 
    "author": "N. Barros", 
    "publish": "2003-05-30T23:08:44Z", 
    "summary": "Conditions Data in high energy physics experiments is frequently seen as\nevery data needed for reconstruction besides the event data itself. This\nincludes all sorts of slowly evolving data like detector alignment, calibration\nand robustness, and data from detector control system. Also, every Conditions\nData Object is associated with a time interval of validity and a version.\nBesides that, quite often is useful to tag collections of Conditions Data\nObjects altogether. These issues have already been investigated and a data\nmodel has been proposed and used for different implementations based in\ncommercial DBMSs, both at CERN and for the BaBar experiment. The special case\nof the ATLAS complex trigger that requires online access to calibration and\nalignment data poses new challenges that have to be met using a flexible and\ncustomizable solution more in the line of Open Source components. Motivated by\nthe ATLAS challenges we have developed an alternative implementation, based in\nan Open Source RDBMS. Several issues were investigated land will be described\nin this paper:\n  -The best way to map the conditions data model into the relational database\nconcept considering what are foreseen as the most frequent queries.\n  -The clustering model best suited to address the scalability problem.\n-Extensive tests were performed and will be described.\n  The very promising results from these tests are attracting the attention from\nthe HEP community and driving further developments."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306013v1", 
    "other_authors": "Julius Hrivnac", 
    "title": "Transparent Persistence with Java Data Objects", 
    "arxiv-id": "cs/0306013v1", 
    "author": "Julius Hrivnac", 
    "publish": "2003-06-02T09:38:47Z", 
    "summary": "Flexible and performant Persistency Service is a necessary component of any\nHEP Software Framework. The building of a modular, non-intrusive and performant\npersistency component have been shown to be very difficult task. In the past,\nit was very often necessary to sacrifice modularity to achieve acceptable\nperformance. This resulted in the strong dependency of the overall Frameworks\non their Persistency subsystems.\n  Recent development in software technology has made possible to build a\nPersistency Service which can be transparently used from other Frameworks. Such\nService doesn't force a strong architectural constraints on the overall\nFramework Architecture, while satisfying high performance requirements. Java\nData Object standard (JDO) has been already implemented for almost all major\ndatabases. It provides truly transparent persistency for any Java object (both\ninternal and external). Objects in other languages can be handled via\ntransparent proxies. Being only a thin layer on top of a used database, JDO\ndoesn't introduce any significant performance degradation. Also Aspect-Oriented\nProgramming (AOP) makes possible to treat persistency as an orthogonal Aspect\nof the Application Framework, without polluting it with persistence-specific\nconcepts.\n  All these techniques have been developed primarily (or only) for the Java\nenvironment. It is, however, possible to interface them transparently to\nFrameworks built in other languages, like for example C++.\n  Fully functional prototypes of flexible and non-intrusive persistency modules\nhave been build for several other packages, as for example FreeHEP AIDA and LCG\nPool AttributeSet (package Indicium)."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306019v1", 
    "other_authors": "I. Sourikova, D. Morrison", 
    "title": "Relational databases for data management in PHENIX", 
    "arxiv-id": "cs/0306019v1", 
    "author": "D. Morrison", 
    "publish": "2003-06-04T17:38:23Z", 
    "summary": "PHENIX is one of the two large experiments at the Relativistic Heavy Ion\nCollider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly\n100TB of experimental data per year. In addition, large volumes of simulated\ndata are produced at multiple off-site computing centers. For any file catalog\nto play a central role in data management it has to face problems associated\nwith the need for distributed access and updates. To be used effectively by the\nhundreds of PHENIX collaborators in 12 countries the catalog must satisfy the\nfollowing requirements: 1) contain up-to-date data, 2) provide fast and\nreliable access to the data, 3) have write permissions for the sites that store\nportions of data. We present an analysis of several available Relational\nDatabase Management Systems (RDBMS) to support a catalog meeting the above\nrequirements and discuss the PHENIX experience with building and using the\ndistributed file catalog."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306020v2", 
    "other_authors": "Adeyemi Adesanya, Tofigh Azemoon, Jacek Becla, Andrew Hanushevsky, Adil Hasan, Wilko Kroeger, Artem Trunov, Daniel Wang, Igor Gaponenko, Simon Patton, David Quarrie", 
    "title": "On the Verge of One Petabyte - the Story Behind the BaBar Database   System", 
    "arxiv-id": "cs/0306020v2", 
    "author": "David Quarrie", 
    "publish": "2003-06-04T19:22:52Z", 
    "summary": "The BaBar database has pioneered the use of a commercial ODBMS within the HEP\ncommunity. The unique object-oriented architecture of Objectivity/DB has made\nit possible to manage over 700 terabytes of production data generated since\nMay'99, making the BaBar database the world's largest known database. The\nongoing development includes new features, addressing the ever-increasing\nluminosity of the detector as well as other changing physics requirements.\nSignificant efforts are focused on reducing space requirements and operational\ncosts. The paper discusses our experience with developing a large scale\ndatabase system, emphasizing universal aspects which may be applied to any\nlarge scale system, independently of underlying technology used."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306034v1", 
    "other_authors": "William Tanenbaum", 
    "title": "A ROOT/IO Based Software Framework for CMS", 
    "arxiv-id": "cs/0306034v1", 
    "author": "William Tanenbaum", 
    "publish": "2003-06-07T15:09:16Z", 
    "summary": "The implementation of persistency in the Compact Muon Solenoid (CMS) Software\nFramework uses the core I/O functionality of ROOT. We will discuss the current\nROOT/IO implementation, its evolution from the prior Objectivity/DB\nimplementation, and the plans and ongoing work for the conversion to \"POOL\",\nprovided by the LHC Computing Grid (LCG) persistency project."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306056v1", 
    "other_authors": "D. Chamont, C. Charlot", 
    "title": "Twelve Ways to Build CMS Crossings from ROOT Files", 
    "arxiv-id": "cs/0306056v1", 
    "author": "C. Charlot", 
    "publish": "2003-06-12T17:24:16Z", 
    "summary": "The simulation of CMS raw data requires the random selection of one hundred\nand fifty pileup events from a very large set of files, to be superimposed in\nmemory to the signal event. The use of ROOT I/O for that purpose is quite\nunusual: the events are not read sequentially but pseudo-randomly, they are not\nprocessed one by one in memory but by bunches, and they do not contain orthodox\nROOT objects but many foreign objects and templates. In this context, we have\ncompared the performance of ROOT containers versus the STL vectors, and the use\nof trees versus a direct storage of containers. The strategy with best\nperformances is by far the one using clones within trees, but it stays hard to\ntune and very dependant on the exact use-case. The use of STL vectors could\nbring more easily similar performances in a future ROOT release."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.is.2004.04.001", 
    "link": "http://arxiv.org/pdf/cs/0306065v1", 
    "other_authors": "C. Cioffi, S. Eckmann, M. Girone, J. Hrivnac, D. Malon, H. Schmuecker, A. Vaniachine, J. Wojcieszuk, Z. Xie", 
    "title": "POOL File Catalog, Collection and Metadata Components", 
    "arxiv-id": "cs/0306065v1", 
    "author": "Z. Xie", 
    "publish": "2003-06-13T16:21:53Z", 
    "summary": "The POOL project is the common persistency framework for the LHC experiments\nto store petabytes of experiment data and metadata in a distributed and grid\nenabled way. POOL is a hybrid event store consisting of a data streaming layer\nand a relational layer. This paper describes the design of file catalog,\ncollection and metadata components which are not part of the data streaming\nlayer of POOL and outlines how POOL aims to provide transparent and efficient\ndata access for a wide range of environments and use cases - ranging from a\nlarge production site down to a single disconnected laptops. The file catalog\nis the central POOL component translating logical data references to physical\ndata files in a grid environment. POOL collections with their associated\nmetadata provide an abstract way of accessing experiment data via their logical\ngrouping into sets of related data objects."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0306066v1", 
    "other_authors": "Venicio Duic, Massimo Lamanna", 
    "title": "The COMPASS Event Store in 2002", 
    "arxiv-id": "cs/0306066v1", 
    "author": "Massimo Lamanna", 
    "publish": "2003-06-13T14:33:53Z", 
    "summary": "COMPASS, the fixed-target experiment at CERN studying the structure of the\nnucleon and spectroscopy, collected over 260 TB during summer 2002 run. All\nthese data, together with reconstructed events information, were put from the\nbeginning in a database infrastructure based on Objectivity/DB and on the\nhierarchical storage manager CASTOR. The experience in the usage of the\ndatabase is reviewed and the evolution of the system outlined."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0306077v1", 
    "other_authors": "Lars Hagge, Jens Kreutzkamp, Kathrin Lappe", 
    "title": "The TESLA Requirements Database", 
    "arxiv-id": "cs/0306077v1", 
    "author": "Kathrin Lappe", 
    "publish": "2003-06-13T22:10:36Z", 
    "summary": "In preparation for the planned linear collider TESLA, DESY is designing the\nrequired buildings and facilities. The accelerator and infrastructure\ncomponents have to be allocated to buildings, and their required areas for\ninstallation, operation and maintenance have to be determined.\nInterdisciplinary working groups specify the project from different viewpoints\nand need to develop a common vision as a precondition for an optimal solution.\nA commercial requirements database is used as a collaborative tool, enabling\nconcurrent requirements specification by independent working groups. The\nrequirements database ensures long term storage and availability of the\nemerging knowledge, and it offers a central platform for communication which is\navailable for all project members. It is successfully operating since summer\n2002 and has since then become an important tool for the design team."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0306079v1", 
    "other_authors": "Jochen Buerger, Lars Hagge, Jens Kreutzkamp, Kathrin Lappe, Andrea Robben", 
    "title": "Integrated Information Management for TESLA", 
    "arxiv-id": "cs/0306079v1", 
    "author": "Andrea Robben", 
    "publish": "2003-06-13T23:04:10Z", 
    "summary": "Next-generation projects in High Energy Physics will reach again a new\ndimension of complexity. Information management has to ensure an efficient and\neconomic information flow within the collaborations, offering world-wide\nup-to-date information access to the collaborators as one condition for\nsuccessful projects. DESY introduces several information systems in preparation\nfor the planned linear collider TESLA: a Requirements Management System (RMS)\nis in production for the TESLA planning group, a Product Data Management System\n(PDMS) is in production since the beginning of 2002 and is supporting the\ncavity preparation and the general engineering of accelerator components. A\npilot Asset Management System (AMS) is in production for supporting the\nmanagement and maintenance of the technical infrastructure, and a Facility\nManagement System (FMS) with a Geographic Information System (GIS) is currently\nbeing introduced to support civil engineering. Efforts have been started to\nintegrate the systems with the goal that users can retrieve information through\na single point of access. The paper gives an introduction to information\nmanagement and the activities at DESY."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0306081v1", 
    "other_authors": "M. Barczyc, D. Burckhart-Chromek, M. Caprini, J. Da Silva Conceicao, M. Dobson, J. Flammer, R. Jones, A. Kazarov, S. Kolos, D. Liko, L. Mapelli, I. Soloviev, R. Hart NIKHEF, A. Amorim, D. Klose, J. Lima, L. Lucio, L. Pedro, H. Wolters, E. Badescu NIPNE, I. Alexandrov, V. Kotov, M. Mineev JINR, Yu. Ryabov PNPI", 
    "title": "An on-line Integrated Bookkeeping: electronic run log book and Meta-Data   Repository for ATLAS", 
    "arxiv-id": "cs/0306081v1", 
    "author": "Yu. Ryabov PNPI", 
    "publish": "2003-06-14T00:42:30Z", 
    "summary": "In the context of the ATLAS experiment there is growing evidence of the\nimportance of different kinds of Meta-data including all the important details\nof the detector and data acquisition that are vital for the analysis of the\nacquired data. The Online BookKeeper (OBK) is a component of ATLAS online\nsoftware that stores all information collected while running the experiment,\nincluding the Meta-data associated with the event acquisition, triggering and\nstorage. The facilities for acquisition of control data within the on-line\nsoftware framework, together with a full functional Web interface, make the OBK\na powerful tool containing all information needed for event analysis, including\nan electronic log book.\n  In this paper we explain how OBK plays a role as one of the main collectors\nand managers of Meta-data produced on-line, and we'll also focus on the Web\nfacilities already available. The usage of the web interface as an electronic\nrun logbook is also explained, together with the future extensions.\n  We describe the technology used in OBK development and how we arrived at the\npresent level explaining the previous experience with various DBMS\ntechnologies. The extensive performance evaluations that have been performed\nand the usage in the production environment of the ATLAS test beams are also\nanalysed."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0307015v2", 
    "other_authors": "Maurice HT Ling, Chi Wai So", 
    "title": "Architecture of an Open-Sourced, Extensible Data Warehouse Builder:   InterBase 6 Data Warehouse Builder (IB-DWB)", 
    "arxiv-id": "cs/0307015v2", 
    "author": "Chi Wai So", 
    "publish": "2003-07-07T14:07:59Z", 
    "summary": "We report the development of an open-sourced data warehouse builder,\nInterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open\nEdition Database Server. InterBase 6 is used for its low maintenance and small\nfootprint. IB-DWB is designed modularly and consists of 5 main components, Data\nPlug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query\nSupporter, bounded together by a Kernel. It is also an extensible system, made\npossible by the Data Plug Platform and the Discoverer Platform. Currently,\nextensions are only possible via dynamic linked-libraries (DLLs).\nMulti-Dimensional Cube Builder represents a basal mean of data aggregation. The\narchitectural philosophy of IB-DWB centers around providing a base platform\nthat is extensible, which is functionally supported by expansion modules.\nIB-DWB is currently being hosted by sourceforge.net (Project Unix Name:\nib-dwb), licensed under GNU General Public License, Version 2."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0307073v1", 
    "other_authors": "Richard Wheeldon, Mark Levene, Kevin Keenoy", 
    "title": "Search and Navigation in Relational Databases", 
    "arxiv-id": "cs/0307073v1", 
    "author": "Kevin Keenoy", 
    "publish": "2003-07-31T11:18:51Z", 
    "summary": "We present a new application for keyword search within relational databases,\nwhich uses a novel algorithm to solve the join discovery problem by finding\nMemex-like trails through the graph of foreign key dependencies. It differs\nfrom previous efforts in the algorithms used, in the presentation mechanism and\nin the use of primary-key only database queries at query-time to maintain a\nfast response for users. We present examples using the DBLP data set."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0309011v1", 
    "other_authors": "Agust S. Egilsson, Hakon Gudbjartsson", 
    "title": "Indexing of Tables Referencing Complex Structures", 
    "arxiv-id": "cs/0309011v1", 
    "author": "Hakon Gudbjartsson", 
    "publish": "2003-09-08T19:57:46Z", 
    "summary": "We introduce indexing of tables referencing complex structures such as\ndigraphs and spatial objects, appearing in genetics and other data intensive\nanalysis. The indexing is achieved by extracting dimension schemas from the\nreferenced structures. The schemas and their dimensionality are determined by\nproper coloring algorithms and the duality between all such schemas and all\nsuch possible proper colorings is established. This duality, in turn, provides\nus with an extensive library of solutions when addressing indexing questions.\nIt is illustrated how to use the schemas, in connection with additional\nrelational database technologies, to optimize queries conditioned on the\nstructural information being referenced. Comparisons using bitmap indexing in\nthe Oracle 9.2i database, on the one hand, and multidimensional clustering in\nDB2 8.1.2, on the other hand, are used to illustrate the applicability of the\nindexing to different technology settings. Finally, we illustrate how the\nindexing can be used to extract low dimensional schemas from a binary interval\ntree in order to resolve efficiently interval and stabbing queries."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0310006v1", 
    "other_authors": "Serge Abiteboul, Rakesh Agrawal, Phil Bernstein, Mike Carey, Stefano Ceri, Bruce Croft, David DeWitt, Mike Franklin, Hector Garcia Molina, Dieter Gawlick, Jim Gray, Laura Haas, Alon Halevy, Joe Hellerstein, Yannis Ioannidis, Martin Kersten, Michael Pazzani, Mike Lesk, David Maier, Jeff Naughton, Hans Schek, Timos Sellis, Avi Silberschatz, Mike Stonebraker, Rick Snodgrass, Jeff Ullman, Gerhard Weikum, Jennifer Widom, Stan Zdonik", 
    "title": "The Lowell Database Research Self Assessment", 
    "arxiv-id": "cs/0310006v1", 
    "author": "Stan Zdonik", 
    "publish": "2003-10-06T05:42:49Z", 
    "summary": "A group of senior database researchers gathers every few years to assess the\nstate of database research and to point out problem areas that deserve\nadditional focus. This report summarizes the discussion and conclusions of the\nsixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that\ninformation management continues to be a critical component of most complex\nsoftware systems. It recommends that database researchers increase focus on:\nintegration of text, data, code, and streams; fusion of information from\nheterogeneous data sources; reasoning about uncertain data; unsupervised data\nmining for interesting correlations; information privacy; and self-adaptation\nand repair."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0310012v1", 
    "other_authors": "Georg Gottlob, Christoph Koch", 
    "title": "A Formal Comparison of Visual Web Wrapper Generators", 
    "arxiv-id": "cs/0310012v1", 
    "author": "Christoph Koch", 
    "publish": "2003-10-08T00:18:31Z", 
    "summary": "We study the core fragment of the Elog wrapping language used in the Lixto\nsystem (a visual wrapper generator) and formally compare Elog to other wrapping\nlanguages proposed in the literature."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0310028v1", 
    "other_authors": "Anoop Jain, Parag Sarda, Jayant R. Haritsa", 
    "title": "Providing Diversity in K-Nearest Neighbor Query Results", 
    "arxiv-id": "cs/0310028v1", 
    "author": "Jayant R. Haritsa", 
    "publish": "2003-10-15T16:07:56Z", 
    "summary": "Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN)\nqueries return the K closest answers according to given distance metric in the\ndatabase with respect to Q. In this scenario, it is possible that a majority of\nthe answers may be very similar to some other, especially when the data has\nclusters. For a variety of applications, such homogeneous result sets may not\nadd value to the user. In this paper, we consider the problem of providing\ndiversity in the results of KNN queries, that is, to produce the closest result\nset such that each answer is sufficiently different from the rest. We first\npropose a user-tunable definition of diversity, and then present an algorithm,\ncalled MOTLEY, for producing a diverse result set as per this definition.\nThrough a detailed experimental evaluation on real and synthetic data, we show\nthat MOTLEY can produce diverse result sets by reading only a small fraction of\nthe tuples in the database. Further, it imposes no additional overhead on the\nevaluation of traditional KNN queries, thereby providing a seamless interface\nbetween diversity and distance."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0310035v1", 
    "other_authors": "Abhijit Kadlag, Amol Wanjari, Juliana Freire, Jayant R. Haritsa", 
    "title": "Supporting Exploratory Queries in Database Centric Web Applications", 
    "arxiv-id": "cs/0310035v1", 
    "author": "Jayant R. Haritsa", 
    "publish": "2003-10-17T10:02:11Z", 
    "summary": "Users of database-centric Web applications, especially in the e-commerce\ndomain, often resort to exploratory ``trial-and-error'' queries since the\nunderlying data space is huge and unfamiliar, and there are several\nalternatives for search attributes in this space. For example, scouting for\ncheap airfares typically involves posing multiple queries, varying flight\ntimes, dates, and airport locations. Exploratory queries are problematic from\nthe perspective of both the user and the server. For the database server, it\nresults in a drastic reduction in effective throughput since much of the\nprocessing is duplicated in each successive query. For the client, it results\nin a marked increase in response times, especially when accessing the service\nthrough wireless channels.\n  In this paper, we investigate the design of automated techniques to minimize\nthe need for repetitive exploratory queries. Specifically, we present SAUNA, a\nserver-side query relaxation algorithm that, given the user's initial range\nquery and a desired cardinality for the answer set, produces a relaxed query\nthat is expected to contain the required number of answers. The algorithm\nincorporates a range-query-specific distance metric that is weighted to produce\nrelaxed queries of a desired shape (e.g. aspect ratio preserving), and utilizes\nmulti-dimensional histograms for query size estimation. A detailed performance\nevaluation of SAUNA over a variety of multi-dimensional data sets indicates\nthat its relaxed queries can significantly reduce the costs associated with\nexploratory query processing."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0310038v1", 
    "other_authors": "Shipra Agrawal, Vijay Krishnan, Jayant Haritsa", 
    "title": "On Addressing Efficiency Concerns in Privacy Preserving Data Mining", 
    "arxiv-id": "cs/0310038v1", 
    "author": "Jayant Haritsa", 
    "publish": "2003-10-17T16:55:08Z", 
    "summary": "Data mining services require accurate input data for their results to be\nmeaningful, but privacy concerns may influence users to provide spurious\ninformation. To encourage users to provide correct inputs, we recently proposed\na data distortion scheme for association rule mining that simultaneously\nprovides both privacy to the user and accuracy in the mining results. However,\nmining the distorted database can be orders of magnitude more time-consuming as\ncompared to mining the original database. In this paper, we address this issue\nand demonstrate that by (a) generalizing the distortion process to perform\nsymbol-specific distortion, (b) appropriately choosing the distortion\nparameters, and (c) applying a variety of optimizations in the reconstruction\nprocess, runtime efficiencies that are well within an order of magnitude of\nundistorted mining can be achieved."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0311038v1", 
    "other_authors": "Wolfgang May", 
    "title": "XPath-Logic and XPathLog: A Logic-Programming Style XML Data   Manipulation Language", 
    "arxiv-id": "cs/0311038v1", 
    "author": "Wolfgang May", 
    "publish": "2003-11-25T09:42:59Z", 
    "summary": "We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a\nclear, declarative language for querying and manipulating XML whose\nperspectives are especially in XML data integration. In our characterization,\nthe formal semantics is defined wrt. an edge-labeled graph-based model which\ncovers the XML data model. We give a complete, logic-based characterization of\nXML data and the main language concept for XML, XPath. XPath-Logic extends the\nXPath language with variable bindings and embeds it into first-order logic.\nXPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style,\nrule-based language for querying and manipulating XML data. The model-theoretic\nsemantics of XPath-Logic serves as the base of XPathLog as a logic-programming\nlanguage, whereas also an equivalent answer-set semantics for evaluating\nXPathLog queries is given. In contrast to other approaches, the XPath syntax\nand semantics is also used for a declarative specification how the database\nshould be updated: when used in rule heads, XPath filters are interpreted as\nspecifications of elements and properties which should be added to the\ndatabase."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0312042v1", 
    "other_authors": "Sergio Flesca, Sergio Greco", 
    "title": "Declarative Semantics for Active Rules", 
    "arxiv-id": "cs/0312042v1", 
    "author": "Sergio Greco", 
    "publish": "2003-12-18T17:43:43Z", 
    "summary": "In this paper we analyze declarative deterministic and non-deterministic\nsemantics for active rules. In particular we consider several (partial) stable\nmodel semantics, previously defined for deductive rules, such as well-founded,\nmax deterministic, unique total stable model, total stable model, and maximal\nstable model semantics. The semantics of an active program AP is given by first\nrewriting it into a deductive program P, then computing a model M defining the\ndeclarative semantics of P and, finally, applying `consistent' updates\ncontained in M to the source database. The framework we propose permits a\nnatural integration of deductive and active rules and can also be applied to\nqueries with function symbols or to queries over infinite databases."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0312043v1", 
    "other_authors": "Laks V. S. Lakshmanan, Fereidoon Sadri", 
    "title": "On A Theory of Probabilistic Deductive Databases", 
    "arxiv-id": "cs/0312043v1", 
    "author": "Fereidoon Sadri", 
    "publish": "2003-12-18T20:08:57Z", 
    "summary": "We propose a framework for modeling uncertainty where both belief and doubt\ncan be given independent, first-class status. We adopt probability theory as\nthe mathematical formalism for manipulating uncertainty. An agent can express\nthe uncertainty in her knowledge about a piece of information in the form of a\nconfidence level, consisting of a pair of intervals of probability, one for\neach of her belief and doubt. The space of confidence levels naturally leads to\nthe notion of a trilattice, similar in spirit to Fitting's bilattices.\nIntuitively, thep oints in such a trilattice can be ordered according to truth,\ninformation, or precision. We develop a framework for probabilistic deductive\ndatabases by associating confidence levels with the facts and rules of a\nclassical deductive database. While the trilattice structure offers a variety\nof choices for defining the semantics of probabilistic deductive databases, our\nchoice of semantics is based on the truth-ordering, which we find to be closest\nto the classical framework for deductive databases. In addition to proposing a\ndeclarative semantics based on valuations and an equivalent semantics based on\nfixpoint theory, we also propose a proof procedure and prove it sound and\ncomplete. We show that while classical Datalog query programs have a polynomial\ntime data complexity, certain query programs in the probabilistic deductive\ndatabase framework do not even terminate on some input databases. We identify a\nlarge natural class of query programs of practical interest in our framework,\nand show that programs in this class possess polynomial time data complexity,\ni.e., not only do they terminate on every input database, they are guaranteed\nto do so in a number of steps polynomial in the input database size."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0401014v1", 
    "other_authors": "Vadim Tropashko", 
    "title": "Nested Intervals with Farey Fractions", 
    "arxiv-id": "cs/0401014v1", 
    "author": "Vadim Tropashko", 
    "publish": "2004-01-18T03:09:04Z", 
    "summary": "Relational Databases are universally conceived as an advance over their\npredecessors Network and Hierarchical models. Superior in every querying\nrespect, they turned out to be surprisingly incomplete when modeling transitive\ndependencies. Almost every couple of months a question how to model a tree in\nthe database surfaces at comp.database.theory newsgroup. This article completes\na series of articles exploring Nested Intervals Model. Previous articles\nintroduced tree encoding with Binary Rational Numbers. However, binary encoding\ngrows exponentially, both in breadth and in depth. In this article, we'll\nleverage Farey fractions in order to overcome this problem. We'll also\ndemonstrate that our implementation scales to a tree with 1M nodes."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0402003v1", 
    "other_authors": "Jan Chomicki", 
    "title": "Semantic Optimization of Preference Queries", 
    "arxiv-id": "cs/0402003v1", 
    "author": "Jan Chomicki", 
    "publish": "2004-02-02T01:42:35Z", 
    "summary": "The notion of preference is becoming more and more ubiquitous in present-day\ninformation systems. Preferences are primarily used to filter and personalize\nthe information reaching the users of such systems. In database systems,\npreferences are usually captured as preference relations that are used to build\npreference queries. In our approach, preference queries are relational algebra\nor SQL queries that contain occurrences of the winnow operator (\"find the most\npreferred tuples in a given relation\").\n  We present here a number of semantic optimization techniques applicable to\npreference queries. The techniques make use of integrity constraints, and make\nit possible to remove redundant occurrences of the winnow operator and to apply\na more efficient algorithm for the computation of winnow. We also study the\npropagation of integrity constraints in the result of the winnow. We have\nidentified necessary and sufficient conditions for the applicability of our\ntechniques, and formulated those conditions as constraint satisfiability\nproblems."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0402051v2", 
    "other_authors": "Vadim Tropashko", 
    "title": "Nested Intervals Tree Encoding with Continued Fractions", 
    "arxiv-id": "cs/0402051v2", 
    "author": "Vadim Tropashko", 
    "publish": "2004-02-20T19:45:56Z", 
    "summary": "We introduce a new variation of Tree Encoding with Nested Intervals, find\nconnections with Materialized Path, and suggest a method for moving parts of\nthe hierarchy."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0403014v2", 
    "other_authors": "Girish Motwani, Sandhya G. Nair", 
    "title": "Search Efficiency in Indexing Structures for Similarity Searching", 
    "arxiv-id": "cs/0403014v2", 
    "author": "Sandhya G. Nair", 
    "publish": "2004-03-11T06:30:30Z", 
    "summary": "Similarity searching finds application in a wide variety of domains including\nmultilingual databases, computational biology, pattern recognition and text\nretrieval. Similarity is measured in terms of a distance function, edit\ndistance, in general metric spaces, which is expensive to compute. Indexing\ntechniques can be used reduce the number of distance computations. We present\nan analysis of various existing similarity indexing structures for the same.\nThe performance obtained using the index structures studied was found to be\nunsatisfactory . We propose an indexing technique that combines the features of\nclustering with M tree(MTB) and the results indicate that this gives better\nperformance."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0403017v1", 
    "other_authors": "Maria A. Nieto-Santisteban, William O'Mullane, Jim Gray, Nolan Li, Tamas Budavari, Alexander S. Szalay, Aniruddha R. Thakar", 
    "title": "Extending the SDSS Batch Query System to the National Virtual   Observatory Grid", 
    "arxiv-id": "cs/0403017v1", 
    "author": "Aniruddha R. Thakar", 
    "publish": "2004-03-12T09:42:04Z", 
    "summary": "The Sloan Digital Sky Survey science database is approaching 2TB. While the\nvast majority of queries normally execute in seconds or minutes, this\ninteractive execution time can be disproportionately increased by a small\nfraction of queries that take hours or days to run; either because they require\nnon-index scans of the largest tables or because they request very large result\nsets. In response to this, we added a multi-queue job submission and tracking\nsystem. The transfer of very large result sets from queries over the network is\nanother serious problem. Statistics suggested that much of this data transfer\nis unnecessary; users would prefer to store results locally in order to allow\nfurther cross matching and filtering. To allow local analysis, we implemented a\nsystem that gives users their own personal database (MyDB) at the portal site.\nUsers may transfer data to their MyDB, and then perform further analysis before\nextracting it to their own machine.\n  We intend to extend the MyDB and asynchronous query ideas to multiple NVO\nnodes. This implies development, in a distributed manner, of several features,\nwhich have been demonstrated for a single node in the SDSS Batch Query System\n(CasJobs). The generalization of asynchronous queries necessitates some form of\nMyDB storage as well as workflow tracking services on each node and\ncoordination strategies among nodes."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0403018v1", 
    "other_authors": "Jim Gray, Alexander S. Szalay", 
    "title": "The World Wide Telescope: An Archetype for Online Science", 
    "arxiv-id": "cs/0403018v1", 
    "author": "Alexander S. Szalay", 
    "publish": "2004-03-12T09:57:43Z", 
    "summary": "Most scientific data will never be directly examined by scientists; rather it\nwill be put into online databases where it will be analyzed and summarized by\ncomputer programs. Scientists increasingly see their instruments through online\nscientific archives and analysis tools, rather than examining the raw data.\nToday this analysis is primarily driven by scientists asking queries, but\nscientific archives are becoming active databases that self-organize and\nrecognize interesting and anomalous facts as data arrives. In some fields, data\nfrom many different archives can be cross-correlated to produce new insights.\nAstronomy presents an excellent example of these trends; and, federating\nAstronomy archives presents interesting challenges for computer scientists."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0403020v1", 
    "other_authors": "Aniruddha R. Thakar, Alexander S. Szalay, Peter Z. Kunszt, Jim Gray", 
    "title": "The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte   Astronomical Archive from Object to Relational DBMS", 
    "arxiv-id": "cs/0403020v1", 
    "author": "Jim Gray", 
    "publish": "2004-03-12T10:20:23Z", 
    "summary": "The Sloan Digital Sky Survey Science Archive is the first in a series of\nmulti-Terabyte digital archives in Astronomy and other data-intensive sciences.\nTo facilitate data mining in the SDSS archive, we adapted a commercial database\nengine and built specialized tools on top of it. Originally we chose an\nobject-oriented database management system due to its data organization\ncapabilities, platform independence, query performance and conceptual fit to\nthe data. However, after using the object database for the first couple of\nyears of the project, it soon began to fall short in terms of its query support\nand data mining performance. This was as much due to the inability of the\ndatabase vendor to respond our demands for features and bug fixes as it was due\nto their failure to keep up with the rapid improvements in hardware\nperformance, particularly faster RAID disk systems. In the end, we were forced\nto abandon the object database and migrate our data to a relational database.\nWe describe below the technical issues that we faced with the object database\nand how and why we migrated to relational technology."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0404003v1", 
    "other_authors": "Elisa Bertino, Barbara Catania, Roberta Gori", 
    "title": "Enhancing the expressive power of the U-Datalog language", 
    "arxiv-id": "cs/0404003v1", 
    "author": "Roberta Gori", 
    "publish": "2004-04-02T02:03:32Z", 
    "summary": "U-Datalog has been developed with the aim of providing a set-oriented logical\nupdate language, guaranteeing update parallelism in the context of a\nDatalog-like language. In U-Datalog, updates are expressed by introducing\nconstraints (+p(X), to denote insertion, and [minus sign]p(X), to denote\ndeletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP\nprogram. In this framework, a set of updates (constraints) is satisfiable if it\ndoes not represent an inconsistent theory, that is, it does not require the\ninsertion and the deletion of the same fact. This approach resembles a very\nsimple form of negation. However, on the other hand, U-Datalog does not provide\nany mechanism to explicitly deal with negative information, resulting in a\nlanguage with limited expressive power. In this paper, we provide a semantics,\nbased on stratification, handling the use of negated atoms in U-Datalog\nprograms, and we show which problems arise in defining a compositional\nsemantics."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0405076v1", 
    "other_authors": "Chiaki Sakama, Katsumi Inoue", 
    "title": "An Abductive Framework For Computing Knowledge Base Updates", 
    "arxiv-id": "cs/0405076v1", 
    "author": "Katsumi Inoue", 
    "publish": "2004-05-22T10:30:35Z", 
    "summary": "This paper introduces an abductive framework for updating knowledge bases\nrepresented by extended disjunctive programs. We first provide a simple\ntransformation from abductive programs to update programs which are logic\nprograms specifying changes on abductive hypotheses. Then, extended abduction,\nwhich was introduced by the same authors as a generalization of traditional\nabduction, is computed by the answer sets of update programs. Next, different\ntypes of updates, view updates and theory updates are characterized by\nabductive programs and computed by update programs. The task of consistency\nrestoration is also realized as special cases of these updates. Each update\nproblem is comparatively assessed from the computational complexity viewpoint.\nThe result of this paper provides a uniform framework for different types of\nknowledge base updates, and each update is computed using existing procedures\nof logic programming."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0406004v1", 
    "other_authors": "Muhammad Nadeem, Syed Ata Hussain Jaffri", 
    "title": "Application of Business Intelligence In Banks (Pakistan)", 
    "arxiv-id": "cs/0406004v1", 
    "author": "Syed Ata Hussain Jaffri", 
    "publish": "2004-06-02T12:55:04Z", 
    "summary": "The financial services industry is rapidly changing. Factors such as\nglobalization, deregulation, mergers and acquisitions, competition from\nnon-financial institutions, and technological innovation, have forced companies\nto re-think their business.Many large companies have been using Business\nIntelligence (BI) computer software for some years to help them gain\ncompetitive advantage. With the introduction of cheaper and more generalized\nproducts to the market place BI is now in the reach of smaller and medium sized\ncompanies. Business Intelligence is also known as knowledge management,\nmanagement information systems (MIS), Executive information systems (EIS) and\nOn-line analytical Processing (OLAP)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0406016v1", 
    "other_authors": "Christoph Koch, Stefanie Scherzinger, Nicole Schweikardt, Bernhard Stegmaier", 
    "title": "Schema-based Scheduling of Event Processors and Buffer Minimization for   Queries on Structured Data Streams", 
    "arxiv-id": "cs/0406016v1", 
    "author": "Bernhard Stegmaier", 
    "publish": "2004-06-07T20:45:28Z", 
    "summary": "We introduce an extension of the XQuery language, FluX, that supports\nevent-based query processing and the conscious handling of main memory buffers.\nPurely event-based queries of this language can be executed on streaming XML\ndata in a very direct way. We then develop an algorithm that allows to\nefficiently rewrite XQueries into the event-based FluX language. This algorithm\nuses order constraints from a DTD to schedule event handlers and to thus\nminimize the amount of buffering required for evaluating a query. We discuss\nthe various technical aspects of query optimization and query evaluation within\nour framework. This is complemented with an experimental evaluation of our\napproach."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0406029v1", 
    "other_authors": "Satyanarayana R Valluri, Kamalakar Karlapalem", 
    "title": "Subset Queries in Relational Databases", 
    "arxiv-id": "cs/0406029v1", 
    "author": "Kamalakar Karlapalem", 
    "publish": "2004-06-17T09:11:49Z", 
    "summary": "In this paper, we motivated the need for relational database systems to\nsupport subset query processing. We defined new operators in relational\nalgebra, and new constructs in SQL for expressing subset queries. We also\nillustrated the applicability of subset queries through different examples\nexpressed using extended SQL statements and relational algebra expressions. Our\naim is to show the utility of subset queries for next generation applications."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0408030v1", 
    "other_authors": "Jim Gray", 
    "title": "The Revolution In Database System Architecture", 
    "arxiv-id": "cs/0408030v1", 
    "author": "Jim Gray", 
    "publish": "2004-08-14T02:31:50Z", 
    "summary": "Database system architectures are undergoing revolutionary changes.\nAlgorithms and data are being unified by integrating programming languages with\nthe database system. This gives an extensible object-relational system where\nnon-procedural relational operators manipulate object sets. Coupled with this,\neach DBMS is now a web service. This has huge implications for how we structure\napplications. DBMSs are now object containers. Queues are the first objects to\nbe added. These queues are the basis for transaction processing and workflow\napplica-tions. Future workflow systems are likely to be built on this core.\nData cubes and online analytic processing are now baked into most DBMSs. Beyond\nthat, DBMSs have a framework for data mining and machine learning algorithms.\nDecision trees, Bayes nets, clustering, and time series analysis are built in;\nnew algorithms can be added. Text, temporal, and spatial data access methods,\nalong with their probabilistic reasoning have been added to database systems.\nAllowing approximate and probabilistic answers is essential for many\napplications. Many believe that XML and xQuery will be the main data structure\nand access pattern. Database systems must accommodate that perspective.These\nchanges mandate a much more dynamic query optimization strategy. Intelligence\nis moving to the periphery of the network. Each disk and each sensor will be a\ncompetent database machine. Relational algebra is a convenient way to program\nthese systems. Database systems are now expected to be self-managing,\nself-healing, and always-up."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0408031v1", 
    "other_authors": "Jim Gray, Alexander S. Szalay, Aniruddha R. Thakar, Gyorgy Fekete, William O'Mullane, Maria A. Nieto-Santisteban, Gerd Heber, Arnold H. Rots", 
    "title": "There Goes the Neighborhood: Relational Algebra for Spatial Data Search", 
    "arxiv-id": "cs/0408031v1", 
    "author": "Arnold H. Rots", 
    "publish": "2004-08-14T02:40:59Z", 
    "summary": "We explored ways of doing spatial search within a relational database: (1)\nhierarchical triangular mesh (a tessellation of the sphere), (2) a zoned\nbucketing system, and (3) representing areas as disjunctive-normal form\nconstraints. Each of these approaches has merits. They all allow efficient\npoint-in-region queries. A relational representation for regions allows Boolean\noperations among them and allows quick tests for point-in-region,\nregions-containing-point, and region-overlap. The speed of these algorithms is\nmuch improved by a zone and multi-scale zone-pyramid scheme. The approach has\nthe virtue that the zone mechanism works well on B-Trees native to all SQL\nsystems and integrates naturally with current query optimizers - rather than\nrequiring a new spatial access method and concomitant query optimizer\nextensions. Over the last 5 years, we have used these techniques extensively in\nour work on SkyServer.sdss.org, and SkyQuery.net."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0408051v1", 
    "other_authors": "Zhimao Guo, Min Li, Xiaoling Wang, Aoying Zhou", 
    "title": "Scalable XSLT Evaluation", 
    "arxiv-id": "cs/0408051v1", 
    "author": "Aoying Zhou", 
    "publish": "2004-08-22T03:19:19Z", 
    "summary": "XSLT is an increasingly popular language for processing XML data. It is\nwidely supported by application platform software. However, little optimization\neffort has been made inside the current XSLT processing engines. Evaluating a\nvery simple XSLT program on a large XML document with a simple schema may\nresult in extensive usage of memory. In this paper, we present a novel notion\nof \\emph{Streaming Processing Model} (\\emph{SPM}) to evaluate a subset of XSLT\nprograms on XML documents, especially large ones. With SPM, an XSLT processor\ncan transform an XML source document to other formats without extra memory\nbuffers required. Therefore, our approach can not only tackle large source\ndocuments, but also produce large results. We demonstrate with a performance\nstudy the advantages of the SPM approach. Experimental results clearly confirm\nthat SPM improves XSLT evaluation typically 2 to 10 times better than the\nexisting approaches. Moreover, the SPM approach also features high scalability."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0409020v1", 
    "other_authors": "Haibin Wang, Yuanchun He, Rajshekhar Sunderraman", 
    "title": "A Generalized Disjunctive Paraconsistent Data Model for Negative and   Disjunctive Information", 
    "arxiv-id": "cs/0409020v1", 
    "author": "Rajshekhar Sunderraman", 
    "publish": "2004-09-11T11:02:30Z", 
    "summary": "This paper presents a generalization of the disjunctive paraconsistent\nrelational data model in which disjunctive positive and negative information\ncan be represented explicitly and manipulated. There are situations where the\nclosed world assumption to infer negative facts is not valid or undesirable and\nthere is a need to represent and reason with negation explicitly. We consider\nexplicit disjunctive negation in the context of disjunctive databases as there\nis an interesting interplay between these two types of information. Generalized\ndisjunctive paraconsistent relation is introduced as the main structure in this\nmodel. The relational algebra is appropriately generalized to work on\ngeneralized disjunctive paraconsistent relations and their correctness is\nestablished."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0410001v2", 
    "other_authors": "C. S. Jensen, H. Lahrmann, S. Pakalnis, J. Runge", 
    "title": "The Infati Data", 
    "arxiv-id": "cs/0410001v2", 
    "author": "J. Runge", 
    "publish": "2004-10-01T16:55:38Z", 
    "summary": "The ability to perform meaningful empirical studies is of essence in research\nin spatio-temporal query processing. Such studies are often necessary to gain\ndetailed insight into the functional and performance characteristics of\nproposals for new query processing techniques.\n  We present a collection of spatio-temporal data, collected during an\nintelligent speed adaptation project, termed INFATI, in which some two dozen\ncars equipped with GPS receivers and logging equipment took part. We describe\nhow the data was collected and how it was \"modified\" to afford the drivers some\ndegree of anonymity.\n  We also present the road network in which the cars were moving during data\ncollection.\n  The GPS data is publicly available for non-commercial purposes. It is our\nhope that this resource will help the spatio-temporal research community in its\nefforts to develop new and better query processing techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0410038v1", 
    "other_authors": "Floris Geerts", 
    "title": "Frequent Knot Discovery", 
    "arxiv-id": "cs/0410038v1", 
    "author": "Floris Geerts", 
    "publish": "2004-10-16T13:14:54Z", 
    "summary": "We explore the possibility of applying the framework of frequent pattern\nmining to a class of continuous objects appearing in nature, namely knots. We\nintroduce the frequent knot mining problem and present a solution. The key\nobservation is that a database consisting of knots can be transformed into a\ntransactional database. This observation is based on the Prime Decomposition\nTheorem of knots."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0410053v1", 
    "other_authors": "Haibin Wang, Hao Tian, Rajshekhar Sunderraman", 
    "title": "An Extended Generalized Disjunctive Paraconsistent Data Model for   Disjunctive Information", 
    "arxiv-id": "cs/0410053v1", 
    "author": "Rajshekhar Sunderraman", 
    "publish": "2004-10-20T08:50:22Z", 
    "summary": "This paper presents an extension of generalized disjunctive paraconsistent\nrelational data model in which pure disjunctive positive and negative\ninformation as well as mixed disjunctive positive and negative information can\nbe represented explicitly and manipulated. We consider explicit mixed\ndisjunctive information in the context of disjunctive databases as there is an\ninteresting interplay between these two types of information. Extended\ngeneralized disjunctive paraconsistent relation is introduced as the main\nstructure in this model. The relational algebra is appropriately generalized to\nwork on extended generalized disjunctive paraconsistent relations and their\ncorrectness is established."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0410054v1", 
    "other_authors": "Rajshekhar Sunderraman, Haibin Wang", 
    "title": "Paraconsistent Intuitionistic Fuzzy Relational Data Model", 
    "arxiv-id": "cs/0410054v1", 
    "author": "Haibin Wang", 
    "publish": "2004-10-20T11:40:50Z", 
    "summary": "In this paper, we present a generalization of the relational data model based\non paraconsistent intuitionistic fuzzy sets. Our data model is capable of\nmanipulating incomplete as well as inconsistent information. Fuzzy relation or\nintuitionistic fuzzy relation can only handle incomplete information.\nAssociated with each relation are two membership functions one is called\ntruth-membership function $T$ which keeps track of the extent to which we\nbelieve the tuple is in the relation, another is called false-membership\nfunction which keeps track of the extent to which we believe that it is not in\nthe relation. A paraconsistent intuitionistic fuzzy relation is inconsistent if\nthere exists one tuple $a$ such that $T(a) + F(a) > 1$. In order to handle\ninconsistent situation, we propose an operator called split to transform\ninconsistent paraconsistent intuitionistic fuzzy relations into\npseudo-consistent paraconsistent intuitionistic fuzzy relations and do the\nset-theoretic and relation-theoretic operations on them and finally use another\noperator called combine to transform the result back to paraconsistent\nintuitionistic fuzzy relation. For this model, we define algebraic operators\nthat are generalisations of the usual operators such as union, selection, join\non fuzzy relations. Our data model can underlie any database and knowledge-base\nmanagement system that deals with incomplete and inconsistent information."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0410070v1", 
    "other_authors": "Giovanni Gasparri", 
    "title": "Using image partitions in 4th Dimension", 
    "arxiv-id": "cs/0410070v1", 
    "author": "Giovanni Gasparri", 
    "publish": "2004-10-26T17:00:40Z", 
    "summary": "I have plotted an image by using mathematical functions in the Database \"4th\nDimension\". I'm going to show an alternative method to: detect which sector has\nbeen clicked; highlight it and combine it with other sectors already\nhighlighted; store the graph information in an efficient way; load and splat\nimage layers to reconstruct the stored graph."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0501029v1", 
    "other_authors": "Francesco Buccafurri, Filippo Furfaro, Domenico Sacca'", 
    "title": "Estimating Range Queries using Aggregate Data with Integrity   Constraints: a Probabilistic Approach", 
    "arxiv-id": "cs/0501029v1", 
    "author": "Domenico Sacca'", 
    "publish": "2005-01-14T19:45:10Z", 
    "summary": "The problem of recovering (count and sum) range queries over multidimensional\ndata only on the basis of aggregate information on such data is addressed. This\nproblem can be formalized as follows. Suppose that a transformation T producing\na summary from a multidimensional data set is used. Now, given a data set D, a\nsummary S=T(D) and a range query r on D, the problem consists of studying r by\nmodelling it as a random variable defined over the sample space of all the data\nsets D' such that T(D) = S. The study of such a random variable, done by the\ndefinition of its probability distribution and the computation of its mean\nvalue and variance, represents a well-founded, theoretical probabilistic\napproach for estimating the query only on the basis of the available\ninformation (that is the summary S) without assumptions on original data."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0501053v3", 
    "other_authors": "Vadim Tropashko", 
    "title": "Relational Algebra as non-Distributive Lattice", 
    "arxiv-id": "cs/0501053v3", 
    "author": "Vadim Tropashko", 
    "publish": "2005-01-21T20:07:32Z", 
    "summary": "We reduce the set of classic relational algebra operators to two binary\noperations: natural join and generalized union. We further demonstrate that\nthis set of operators is relationally complete and honors lattice axioms."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0503012v2", 
    "other_authors": "Floris Geerts, Sofie Haesevoets, Bart Kuijpers", 
    "title": "First-order Complete and Computationally Complete Query Languages for   Spatio-Temporal Databases", 
    "arxiv-id": "cs/0503012v2", 
    "author": "Bart Kuijpers", 
    "publish": "2005-03-04T15:34:45Z", 
    "summary": "We address a fundamental question concerning spatio-temporal database\nsystems: ``What are exactly spatio-temporal queries?'' We define\nspatio-temporal queries to be computable mappings that are also generic,\nmeaning that the result of a query may only depend to a limited extent on the\nactual internal representation of the spatio-temporal data. Genericity is\ndefined as invariance under groups of geometric transformations that preserve\ncertain characteristics of spatio-temporal data (e.g., collinearity, distance,\nvelocity, acceleration, ...). These groups depend on the notions that are\nrelevant in particular spatio-temporal database applications.\n  These transformations also have the distinctive property that they respect\nthe monotone and unidirectional nature of time.\n  We investigate different genericity classes with respect to the constraint\ndatabase model for spatio-temporal databases and we identify sound and complete\nlanguages for the first-order and the computable queries in these genericity\nclasses. We distinguish between genericity determined by time-invariant\ntransformations, genericity notions concerning physical quantities and\ngenericity determined by time-dependent transformations."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0503022v2", 
    "other_authors": "Daniel Pfeifer, Peter C. Lockemann", 
    "title": "Theory and Practice of Transactional Method Caching", 
    "arxiv-id": "cs/0503022v2", 
    "author": "Peter C. Lockemann", 
    "publish": "2005-03-09T17:53:15Z", 
    "summary": "Nowadays, tiered architectures are widely accepted for constructing large\nscale information systems. In this context application servers often form the\nbottleneck for a system's efficiency. An application server exposes an object\noriented interface consisting of set of methods which are accessed by\npotentially remote clients. The idea of method caching is to store results of\nread-only method invocations with respect to the application server's interface\non the client side. If the client invokes the same method with the same\narguments again, the corresponding result can be taken from the cache without\ncontacting the server. It has been shown that this approach can considerably\nimprove a real world system's efficiency.\n  This paper extends the concept of method caching by addressing the case where\nclients wrap related method invocations in ACID transactions. Demarcating\nsequences of method calls in this way is supported by many important\napplication server standards. In this context the paper presents an\narchitecture, a theory and an efficient protocol for maintaining full\ntransactional consistency and in particular serializability when using a method\ncache on the client side. In order to create a protocol for scheduling cached\nmethod results, the paper extends a classical transaction formalism. Based on\nthis extension, a recovery protocol and an optimistic serializability protocol\nare derived. The latter one differs from traditional transactional cache\nprotocols in many essential ways. An efficiency experiment validates the\napproach: Using the cache a system's performance and scalability are\nconsiderably improved."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0505038v1", 
    "other_authors": "Albrecht Schmidt, Christian S. Jensen", 
    "title": "Efficient Management of Short-Lived Data", 
    "arxiv-id": "cs/0505038v1", 
    "author": "Christian S. Jensen", 
    "publish": "2005-05-16T14:42:01Z", 
    "summary": "Motivated by the increasing prominence of loosely-coupled systems, such as\nmobile and sensor networks, which are characterised by intermittent\nconnectivity and volatile data, we study the tagging of data with so-called\nexpiration times. More specifically, when data are inserted into a database,\nthey may be tagged with time values indicating when they expire, i.e., when\nthey are regarded as stale or invalid and thus are no longer considered part of\nthe database. In a number of applications, expiration times are known and can\nbe assigned at insertion time. We present data structures and algorithms for\nonline management of data tagged with expiration times. The algorithms are\nbased on fully functional, persistent treaps, which are a combination of binary\nsearch trees with respect to a primary attribute and heaps with respect to a\nsecondary attribute. The primary attribute implements primary keys, and the\nsecondary attribute stores expiration times in a minimum heap, thus keeping a\npriority queue of tuples to expire. A detailed and comprehensive experimental\nstudy demonstrates the well-behavedness and scalability of the approach as well\nas its efficiency with respect to a number of competitors."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0505059v1", 
    "other_authors": "Sergio Flesca, Filippo Furfaro, Francesco Parisi", 
    "title": "Consistent query answers on numerical databases under aggregate   constraints", 
    "arxiv-id": "cs/0505059v1", 
    "author": "Francesco Parisi", 
    "publish": "2005-05-23T14:19:24Z", 
    "summary": "The problem of extracting consistent information from relational databases\nviolating integrity constraints on numerical data is addressed. In particular,\naggregate constraints defined as linear inequalities on aggregate-sum queries\non input data are considered. The notion of repair as consistent set of updates\nat attribute-value level is exploited, and the characterization of several\ncomplexity issues related to repairing data and computing consistent query\nanswers is provided."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0505074v1", 
    "other_authors": "Stijn Dekeyser, Jan Hidders, Jan Paredaens, Roel Vercammen", 
    "title": "Instance-Independent View Serializability for Semistructured Databases", 
    "arxiv-id": "cs/0505074v1", 
    "author": "Roel Vercammen", 
    "publish": "2005-05-26T09:51:46Z", 
    "summary": "Semistructured databases require tailor-made concurrency control mechanisms\nsince traditional solutions for the relational model have been shown to be\ninadequate. Such mechanisms need to take full advantage of the hierarchical\nstructure of semistructured data, for instance allowing concurrent updates of\nsubtrees of, or even individual elements in, XML documents. We present an\napproach for concurrency control which is document-independent in the sense\nthat two schedules of semistructured transactions are considered equivalent if\nthey are equivalent on all possible documents. We prove that it is decidable in\npolynomial time whether two given schedules in this framework are equivalent.\nThis also solves the view serializability for semistructured schedules\npolynomially in the size of the schedule and exponentially in the number of\ntransactions."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0506002v1", 
    "other_authors": "Angela Bonifati, Elaine Qing Chang, Terence Ho, Laks V. S. Lakshmanan", 
    "title": "HepToX: Heterogeneous Peer to Peer XML Databases", 
    "arxiv-id": "cs/0506002v1", 
    "author": "Laks V. S. Lakshmanan", 
    "publish": "2005-06-01T01:30:09Z", 
    "summary": "We study a collection of heterogeneous XML databases maintaining similar and\nrelated information, exchanging data via a peer to peer overlay network. In\nthis setting, a mediated global schema is unrealistic. Yet, users/applications\nwish to query the databases via one peer using its schema. We have recently\ndeveloped HepToX, a P2P Heterogeneous XML database system. A key idea is that\nwhenever a peer enters the system, it establishes an acquaintance with a small\nnumber of peer databases, possibly with different schema. The peer\nadministrator provides correspondences between the local schema and the\nacquaintance schema using an informal and intuitive notation of arrows and\nboxes. We develop a novel algorithm that infers a set of precise mapping rules\nbetween the schemas from these visual annotations. We pin down a semantics of\nquery translation given such mapping rules, and present a novel query\ntranslation algorithm for a simple but expressive fragment of XQuery, that\nemploys the mapping rules in either direction. We show the translation\nalgorithm is correct. Finally, we demonstrate the utility and scalability of\nour ideas and algorithms with a detailed set of experiments on top of the\nEmulab, a large scale P2P network emulation testbed."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0506026v1", 
    "other_authors": "Rada Chirkova, Michael R. Genesereth", 
    "title": "Database Reformulation with Integrity Constraints (extended abstract)", 
    "arxiv-id": "cs/0506026v1", 
    "author": "Michael R. Genesereth", 
    "publish": "2005-06-08T22:03:49Z", 
    "summary": "In this paper we study the problem of reducing the evaluation costs of\nqueries on finite databases in presence of integrity constraints, by designing\nand materializing views. Given a database schema, a set of queries defined on\nthe schema, a set of integrity constraints, and a storage limit, to find a\nsolution to this problem means to find a set of views that satisfies the\nstorage limit, provides equivalent rewritings of the queries under the\nconstraints (this requirement is weaker than equivalence in the absence of\nconstraints), and reduces the total costs of evaluating the queries. This\nproblem, database reformulation, is important for many applications, including\ndata warehousing and query optimization. We give complexity results and\nalgorithms for database reformulation in presence of constraints, for\nconjunctive queries, views, and rewritings and for several types of\nconstraints, including functional and inclusion dependencies. To obtain better\ncomplexity results, we introduce an unchase technique, which reduces the\nproblem of query equivalence under constraints to equivalence in the absence of\nconstraints without increasing query size."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0506063v1", 
    "other_authors": "Slawomir Staworko, Jan Chomicki", 
    "title": "Priority-Based Conflict Resolution in Inconsistent Relational Databases", 
    "arxiv-id": "cs/0506063v1", 
    "author": "Jan Chomicki", 
    "publish": "2005-06-14T23:10:47Z", 
    "summary": "We study here the impact of priorities on conflict resolution in inconsistent\nrelational databases. We extend the framework of repairs and consistent query\nanswers. We propose a set of postulates that an extended framework should\nsatisfy and consider two instantiations of the framework: (locally preferred)\nl-repairs and (globally preferred) g-repairs. We study the relationships\nbetween them and the impact each notion of repair has on the computational\ncomplexity of repair checking and consistent query answers."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0508120v1", 
    "other_authors": "Gennady P. Berman, Vyacheslav N. Gorshkov, Edward P. MacKerrow, Xidi Wang", 
    "title": "Iterative Algorithm for Finding Frequent Patterns in Transactional   Databases", 
    "arxiv-id": "cs/0508120v1", 
    "author": "Xidi Wang", 
    "publish": "2005-08-26T21:05:44Z", 
    "summary": "A high-performance algorithm for searching for frequent patterns (FPs) in\ntransactional databases is presented. The search for FPs is carried out by\nusing an iterative sieve algorithm by computing the set of enclosed cycles. In\neach inner cycle of level FPs composed of elements are generated. The assigned\nnumber of enclosed cycles (the parameter of the problem) defines the maximum\nlength of the desired FPs. The efficiency of the algorithm is produced by (i)\nthe extremely simple logical searching scheme, (ii) the avoidance of recursive\nprocedures, and (iii) the usage of only one-dimensional arrays of integers."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0509088v1", 
    "other_authors": "Babajide Afolabi, Odile Thiery", 
    "title": "Business intelligence systems and user's parameters: an application to a   documents' database", 
    "arxiv-id": "cs/0509088v1", 
    "author": "Odile Thiery", 
    "publish": "2005-09-28T08:38:15Z", 
    "summary": "This article presents earlier results of our research works in the area of\nmodeling Business Intelligence Systems. The basic idea of this research area is\npresented first. We then show the necessity of including certain users'\nparameters in Information systems that are used in Business Intelligence\nsystems in order to integrate a better response from such systems. We\nidentified two main types of attributes that can be missing from a base and we\nshowed why they needed to be included. A user model that is based on a\ncognitive user evolution is presented. This model when used together with a\ngood definition of the information needs of the user (decision maker) will\naccelerate his decision making process."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0511106v1", 
    "other_authors": "Sergiu Theodor Chelcea, Alzennyr Da Silva, Yves Lechevallier, Doru Tanasa, Brigitte Trousse", 
    "title": "Benefits of InterSite Pre-Processing and Clustering Methods in   E-Commerce Domain", 
    "arxiv-id": "cs/0511106v1", 
    "author": "Brigitte Trousse", 
    "publish": "2005-11-30T16:12:38Z", 
    "summary": "This paper presents our preprocessing and clustering analysis on the\nclickstream dataset proposed for the ECMLPKDD 2005 Discovery Challenge. The\nmain contributions of this article are double. First, after presenting the\nclickstream dataset, we show how we build a rich data warehouse based an\nadvanced preprocesing. We take into account the intersite aspects in the given\necommerce domain, which offers an interesting data structuration. A preliminary\nstatistical analysis based on time period clickstreams is given, emphasing the\nimportance of intersite user visits in such a context. Secondly, we describe\nour crossed-clustering method which is applied on data generated from our data\nwarehouse. Our preliminary results are interesting and promising illustrating\nthe benefits of our WUM methods, even if more investigations are needed on the\nsame dataset."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0602039v1", 
    "other_authors": "Andrei Arion, Angela Bonifati, Ioana Manolescu, Andrea Pugliese", 
    "title": "Path Summaries and Path Partitioning in Modern XML Databases", 
    "arxiv-id": "cs/0602039v1", 
    "author": "Andrea Pugliese", 
    "publish": "2006-02-10T12:21:42Z", 
    "summary": "We study the applicability of XML path summaries in the context of\ncurrent-day XML databases. We find that summaries provide an excellent basis\nfor optimizing data access methods, which furthermore mixes very well with\npath-partitioned stores. We provide practical algorithms for building and\nexploiting summaries, and prove its benefits through extensive experiments."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0603044v2", 
    "other_authors": "Marshall Spight, Vadim Tropashko", 
    "title": "First Steps in Relational Lattice", 
    "arxiv-id": "cs/0603044v2", 
    "author": "Vadim Tropashko", 
    "publish": "2006-03-10T18:11:49Z", 
    "summary": "Relational lattice reduces the set of six classic relational algebra\noperators to two binary lattice operations: natural join and inner union. We\ngive an introduction to this theory with emphasis on formal algebraic laws. New\nresults include Spight distributivity criteria and its applications to query\ntransformations."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0604076v1", 
    "other_authors": "Loreto Bravo, Leopoldo Bertossi", 
    "title": "Semantically Correct Query Answers in the Presence of Null Values", 
    "arxiv-id": "cs/0604076v1", 
    "author": "Leopoldo Bertossi", 
    "publish": "2006-04-19T22:33:33Z", 
    "summary": "For several reasons a database may not satisfy a given set of integrity\nconstraints(ICs), but most likely most of the information in it is still\nconsistent with those ICs; and could be retrieved when queries are answered.\nConsistent answers to queries wrt a set of ICs have been characterized as\nanswers that can be obtained from every possible minimally repaired consistent\nversion of the original database. In this paper we consider databases that\ncontain null values and are also repaired, if necessary, using null values. For\nthis purpose, we propose first a precise semantics for IC satisfaction in a\ndatabase with null values that is compatible with the way null values are\ntreated in commercial database management systems. Next, a precise notion of\nrepair is introduced that privileges the introduction of null values when\nrepairing foreign key constraints, in such a way that these new values do not\ncreate an infinite cycle of new inconsistencies. Finally, we analyze how to\nspecify this kind of repairs of a database that contains null values using\ndisjunctive logic programs with stable model semantics."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0605124v1", 
    "other_authors": "Jorge Perez, Marcelo Arenas, Claudio Gutierrez", 
    "title": "Semantics and Complexity of SPARQL", 
    "arxiv-id": "cs/0605124v1", 
    "author": "Claudio Gutierrez", 
    "publish": "2006-05-26T16:41:15Z", 
    "summary": "SPARQL is the W3C candidate recommendation query language for RDF. In this\npaper we address systematically the formal study of SPARQL, concentrating in\nits graph pattern facility. We consider for this study a fragment without\nliterals and a simple version of filters which encompasses all the main issues\nyet is simple to formalize. We provide a compositional semantics, prove there\nare normal forms, prove complexity bounds, among others that the evaluation of\nSPARQL patterns is PSPACE-complete, compare our semantics to an alternative\noperational semantics, give simple and natural conditions when both semantics\ncoincide and discuss optimizations procedures."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0606075v2", 
    "other_authors": "Lyublena Antova, Christoph Koch, Dan Olteanu", 
    "title": "10^(10^6) Worlds and Beyond: Efficient Representation and Processing of   Incomplete Information", 
    "arxiv-id": "cs/0606075v2", 
    "author": "Dan Olteanu", 
    "publish": "2006-06-16T14:24:34Z", 
    "summary": "Current systems and formalisms for representing incomplete information\ngenerally suffer from at least one of two weaknesses. Either they are not\nstrong enough for representing results of simple queries, or the handling and\nprocessing of the data, e.g. for query evaluation, is intractable.\n  In this paper, we present a decomposition-based approach to addressing this\nproblem. We introduce world-set decompositions (WSDs), a space-efficient\nformalism for representing any finite set of possible worlds over relational\ndatabases. WSDs are therefore a strong representation system for any relational\nquery language. We study the problem of efficiently evaluating relational\nalgebra queries on sets of worlds represented by WSDs. We also evaluate our\ntechnique experimentally in a large census data scenario and show that it is\nboth scalable and efficient."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0609144v1", 
    "other_authors": "E. Jimenez-Ruiz, R. Berlanga, I. Sanz, R. McClatchey, R. Danger, D. Manset, J. Paraire, A. Rios", 
    "title": "The Management and Integration of Biomedical Knowledge: Application in   the Health-e-Child Project (Position Paper)", 
    "arxiv-id": "cs/0609144v1", 
    "author": "A. Rios", 
    "publish": "2006-09-26T15:21:40Z", 
    "summary": "The Health-e-Child project aims to develop an integrated healthcare platform\nfor European paediatrics. In order to achieve a comprehensive view of childrens\nhealth, a complex integration of biomedical data, information, and knowledge is\nnecessary. Ontologies will be used to formally define this domain knowledge and\nwill form the basis for the medical knowledge management system. This paper\nintroduces an innovative methodology for the vertical integration of biomedical\nknowledge. This approach will be largely clinician-centered and will enable the\ndefinition of ontology fragments, connections between them (semantic bridges)\nand enriched ontology fragments (views). The strategy for the specification and\ncapture of fragments, bridges and views is outlined with preliminary examples\ndemonstrated in the collection of biomedical information from hospital\ndatabases, biomedical ontologies, and biomedical public databases."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0610020v2", 
    "other_authors": "William F. Gilreath", 
    "title": "XString: XML as a String", 
    "arxiv-id": "cs/0610020v2", 
    "author": "William F. Gilreath", 
    "publish": "2006-10-04T23:21:11Z", 
    "summary": "Extensible markup language (XML) is a technology that has been much hyped, so\nthat XML has become an industry buzzword. Behind the hype is a powerful\ntechnology for data representation in a platform independent manner. As a text\ndocument, however, XML suffers from being too bloated, and requires an XML\nparser to access and manipulate it. XString is an encoding method for XML, in\nessence, a markup language's markup language. XString gives the benefit of\ncompressing XML, and allows for easy manipulation and processing of XML source\nas a very long string."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0611031v2", 
    "other_authors": "Scot Anderson, Peter Revesz", 
    "title": "Efficient Threshold Aggregation of Moving Objects", 
    "arxiv-id": "cs/0611031v2", 
    "author": "Peter Revesz", 
    "publish": "2006-11-07T18:23:34Z", 
    "summary": "Calculating aggregation operators of moving point objects, using time as a\ncontinuous variable, presents unique problems when querying for congestion in a\nmoving and changing (or dynamic) query space. We present a set of congestion\nquery operators, based on a threshold value, that estimate the following 5\naggregation operations in d-dimensions. 1) We call the count of point objects\nthat intersect the dynamic query space during the query time interval, the\nCountRange. 2) We call the Maximum (or Minimum) congestion in the dynamic query\nspace at any time during the query time interval, the MaxCount (or MinCount).\n3) We call the sum of time that the dynamic query space is congested, the\nThresholdSum. 4) We call the number of times that the dynamic query space is\ncongested, the ThresholdCount. And 5) we call the average length of time of all\nthe time intervals when the dynamic query space is congested, the\nThresholdAverage. These operators rely on a novel approach to transforming the\nproblem of selection based on position to a problem of selection based on a\nthreshold. These operators can be used to predict concentrations of migrating\nbirds that may carry disease such as Bird Flu and hence the information may be\nused to predict high risk areas. On a smaller scale, those operators are also\napplicable to maintaining safety in airplane operations. We present the theory\nof our estimation operators and provide algorithms for exact operators. The\nimplementations of those operators, and experiments, which include data from\nmore than 7500 queries, indicate that our estimation operators produce fast,\nefficient results with error under 5%."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0611094v1", 
    "other_authors": "Ravindra Guravannavar, S Sudarshan, Ajit A Diwan, Ch. Sobhan Babu", 
    "title": "Reducing Order Enforcement Cost in Complex Query Plans", 
    "arxiv-id": "cs/0611094v1", 
    "author": "Ch. Sobhan Babu", 
    "publish": "2006-11-20T10:12:46Z", 
    "summary": "Algorithms that exploit sort orders are widely used to implement joins,\ngrouping, duplicate elimination and other set operations. Query optimizers\ntraditionally deal with sort orders by using the notion of interesting orders.\nThe number of interesting orders is unfortunately factorial in the number of\nparticipating attributes. Optimizer implementations use heuristics to prune the\nnumber of interesting orders, but the quality of the heuristics is unclear.\nIncreasingly complex decision support queries and increasing use of covering\nindices, which provide multiple alternative sort orders for relations, motivate\nus to better address the problem of optimization with interesting orders.\n  We show that even a simplified version of optimization with sort orders is\nNP-hard and provide principled heuristics for choosing interesting orders. We\nhave implemented the proposed techniques in a Volcano-style cost-based\noptimizer, and our performance study shows significant improvements in\nestimated cost. We also executed our plans on a widely used commercial database\nsystem, and on PostgreSQL, and found that actual execution times for our plans\nwere significantly better than for plans generated by those systems in several\ncases."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612102v2", 
    "other_authors": "Nilesh Dalvi, Dan Suciu", 
    "title": "The Dichotomy of Conjunctive Queries on Probabilistic Structures", 
    "arxiv-id": "cs/0612102v2", 
    "author": "Dan Suciu", 
    "publish": "2006-12-20T21:11:05Z", 
    "summary": "We show that for every conjunctive query, the complexity of evaluating it on\na probabilistic database is either \\PTIME or #\\P-complete, and we give an\nalgorithm for deciding whether a given conjunctive query is \\PTIME or\n#\\P-complete. The dichotomy property is a fundamental result on query\nevaluation on probabilistic databases and it gives a complete classification of\nthe complexity of conjunctive queries."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612103v1", 
    "other_authors": "Vibhor Rastogi, Dan Suciu, Sungho Hong", 
    "title": "The Boundary Between Privacy and Utility in Data Anonymization", 
    "arxiv-id": "cs/0612103v1", 
    "author": "Sungho Hong", 
    "publish": "2006-12-21T00:21:45Z", 
    "summary": "We consider the privacy problem in data publishing: given a relation I\ncontaining sensitive information 'anonymize' it to obtain a view V such that,\non one hand attackers cannot learn any sensitive information from V, and on the\nother hand legitimate users can use V to compute useful statistics on I. These\nare conflicting goals. We use a definition of privacy that is derived from\nexisting ones in the literature, which relates the a priori probability of a\ngiven tuple t, Pr(t), with the a posteriori probability, Pr(t | V), and propose\na novel and quite practical definition for utility. Our main result is the\nfollowing. Denoting n the size of I and m the size of the domain from which I\nwas drawn (i.e. n < m) then: when the a priori probability is Pr(t) =\nOmega(n/sqrt(m)) for some t, there exists no useful anonymization algorithm,\nwhile when Pr(t) = O(n/m) for all tuples t, then we give a concrete\nanonymization algorithm that is both private and useful. Our algorithm is quite\ndifferent from the k-anonymization algorithm studied intensively in the\nliterature, and is based on random deletions and insertions to I."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612110v1", 
    "other_authors": "James Hamilton", 
    "title": "Architecture for Modular Data Centers", 
    "arxiv-id": "cs/0612110v1", 
    "author": "James Hamilton", 
    "publish": "2006-12-21T19:36:38Z", 
    "summary": "Several factors are driving high-scale deployments of large data centers\nbuilt upon commodity components. These commodity clusters are far cheaper than\nmainframe systems of the past but they bring serious heat and power density\nissues. Also the high failure rate of the individual components drives\nsignificant administrative costs. This proposal outlines an architecture for\ndata center design based upon 20'x8'x8' modules that substantially changes how\nthese systems are acquired, administered, and then later recycled."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612111v1", 
    "other_authors": "Russell Sears, Catharine van Ingen", 
    "title": "Fragmentation in Large Object Repositories", 
    "arxiv-id": "cs/0612111v1", 
    "author": "Catharine van Ingen", 
    "publish": "2006-12-21T21:51:15Z", 
    "summary": "Fragmentation leads to unpredictable and degraded application performance.\nWhile these problems have been studied in detail for desktop filesystem\nworkloads, this study examines newer systems such as scalable object stores and\nmultimedia repositories. Such systems use a get/put interface to store objects.\nIn principle, databases and filesystems can support such applications\nefficiently, allowing system designers to focus on complexity, deployment cost\nand manageability. Although theoretical work proves that certain storage\npolicies behave optimally for some workloads, these policies often behave\npoorly in practice. Most storage benchmarks focus on short-term behavior or do\nnot measure fragmentation. We compare SQL Server to NTFS and find that\nfragmentation dominates performance when object sizes exceed 256KB-1MB. NTFS\nhandles fragmentation better than SQL Server. Although the performance curves\nwill vary with other systems and workloads, we expect the same interactions\nbetween fragmentation and free space to apply. It is well-known that\nfragmentation is related to the percentage free space. We found that the ratio\nof free space to object size also impacts performance. Surprisingly, in both\nsystems, storing objects of a single size causes fragmentation, and changing\nthe size of write requests affects fragmentation. These problems could be\naddressed with simple changes to the filesystem and database interfaces. It is\nour hope that an improved understanding of fragmentation will lead to\npredictable storage systems that require less maintenance after deployment."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612112v1", 
    "other_authors": "Boris Baryshnikov, Cipri Clinciu, Conor Cunningham, Leo Giakoumakis, Slava Oks, Stefano Stefani", 
    "title": "Managing Query Compilation Memory Consumption to Improve DBMS Throughput", 
    "arxiv-id": "cs/0612112v1", 
    "author": "Stefano Stefani", 
    "publish": "2006-12-21T21:51:55Z", 
    "summary": "While there are known performance trade-offs between database page buffer\npool and query execution memory allocation policies, little has been written on\nthe impact of query compilation memory use on overall throughput of the\ndatabase management system (DBMS). We present a new aspect of the query\noptimization problem and offer a solution implemented in Microsoft SQL Server\n2005. The solution provides stable throughput for a range of workloads even\nwhen memory requests outstrip the ability of the hardware to service those\nrequests."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612113v1", 
    "other_authors": "Paul Greenfield, Alan Fekete, Julian Jang, Dean Kuo, Surya Nepal", 
    "title": "Isolation Support for Service-based Applications: A Position Paper", 
    "arxiv-id": "cs/0612113v1", 
    "author": "Surya Nepal", 
    "publish": "2006-12-21T22:08:17Z", 
    "summary": "In this paper, we propose an approach to providing the benefits of isolation\nin service-oriented applications where it is not feasible to hold traditional\nlocks for ACID transactions. Our technique, called \"Promises\", provides an\nuniform view for clients which covers a wide range of implementation techniques\non the service side, all allowing the client to check a condition and then\nlater rely on that condition still holding."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612114v1", 
    "other_authors": "Alexander B\u00f6hm, Carl-Christian Kanne, Guido Moerkotte", 
    "title": "Demaq: A Foundation for Declarative XML Message Processing", 
    "arxiv-id": "cs/0612114v1", 
    "author": "Guido Moerkotte", 
    "publish": "2006-12-21T22:29:07Z", 
    "summary": "This paper gives an overview of Demaq, an XML message processing system\noperating on the foundation of transactional XML message queues. We focus on\nthe syntax and semantics of its fully declarative, rule-based application\nlanguage and demonstrate our message-based programming paradigm in the context\nof a case study. Further, we discuss optimization opportunities for executing\nDemaq programs."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612115v1", 
    "other_authors": "Roger S. Barga, Jonathan Goldstein, Mohamed Ali, Mingsheng Hong", 
    "title": "Consistent Streaming Through Time: A Vision for Event Stream Processing", 
    "arxiv-id": "cs/0612115v1", 
    "author": "Mingsheng Hong", 
    "publish": "2006-12-21T22:40:33Z", 
    "summary": "Event processing will play an increasingly important role in constructing\nenterprise applications that can immediately react to business critical events.\nVarious technologies have been proposed in recent years, such as event\nprocessing, data streams and asynchronous messaging (e.g. pub/sub). We believe\nthese technologies share a common processing model and differ only in target\nworkload, including query language features and consistency requirements. We\nargue that integrating these technologies is the next step in a natural\nprogression. In this paper, we present an overview and discuss the foundations\nof CEDR, an event streaming system that embraces a temporal stream model to\nunify and further enrich query language features, handle imperfections in event\ndelivery and define correctness guarantees. We describe specific contributions\nmade so far and outline next steps in developing the CEDR system."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612127v1", 
    "other_authors": "Mohamed Y. Eltabakh, Mourad Ouzzani, Walid G. Aref", 
    "title": "bdbms -- A Database Management System for Biological Data", 
    "arxiv-id": "cs/0612127v1", 
    "author": "Walid G. Aref", 
    "publish": "2006-12-22T20:32:00Z", 
    "summary": "Biologists are increasingly using databases for storing and managing their\ndata. Biological databases typically consist of a mixture of raw data,\nmetadata, sequences, annotations, and related data obtained from various\nsources. Current database technology lacks several functionalities that are\nneeded by biological databases. In this paper, we introduce bdbms, an\nextensible prototype database management system for supporting biological data.\nbdbms extends the functionalities of current DBMSs to include: (1) Annotation\nand provenance management including storage, indexing, manipulation, and\nquerying of annotation and provenance as first class objects in bdbms, (2)\nLocal dependency tracking to track the dependencies and derivations among data\nitems, (3) Update authorization to support data curation via content-based\nauthorization, in contrast to identity-based authorization, and (4) New access\nmethods and their supporting operators that support pattern matching on various\ntypes of compressed biological data types. This paper presents the design of\nbdbms along with the techniques proposed to support these functionalities\nincluding an extension to SQL. We also outline some open issues in building\nbdbms."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612128v1", 
    "other_authors": "Daniel Gyllstrom, Eugene Wu, Hee-Jin Chae, Yanlei Diao, Patrick Stahlberg, Gordon Anderson", 
    "title": "SASE: Complex Event Processing over Streams", 
    "arxiv-id": "cs/0612128v1", 
    "author": "Gordon Anderson", 
    "publish": "2006-12-22T20:38:58Z", 
    "summary": "RFID technology is gaining adoption on an increasing scale for tracking and\nmonitoring purposes. Wide deployments of RFID devices will soon generate an\nunprecedented volume of data. Emerging applications require the RFID data to be\nfiltered and correlated for complex pattern detection and transformed to events\nthat provide meaningful, actionable information to end applications. In this\nwork, we design and develop SASE, a com-plex event processing system that\nperforms such data-information transformation over real-time streams. We design\na complex event language for specifying application logic for such\ntransformation, devise new query processing techniques to effi-ciently\nimplement the language, and develop a comprehensive system that collects,\ncleans, and processes RFID data for deliv-ery of relevant, timely information\nas well as storing necessary data for future querying. We demonstrate an\ninitial prototype of SASE through a real-world retail management scenario."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612129v1", 
    "other_authors": "Bishwaranjan Bhattacharjee, Vuk Ercegovac, Joseph Glider, Richard Golding, Guy Lohman, Volke Markl, Hamid Pirahesh, Jun Rao, Robert Rees, Frederick Reiss, Eugene Shekita, Garret Swart", 
    "title": "Impliance: A Next Generation Information Management Appliance", 
    "arxiv-id": "cs/0612129v1", 
    "author": "Garret Swart", 
    "publish": "2006-12-22T20:49:29Z", 
    "summary": "ably successful in building a large market and adapting to the changes of the\nlast three decades, its impact on the broader market of information management\nis surprisingly limited. If we were to design an information management system\nfrom scratch, based upon today's requirements and hardware capabilities, would\nit look anything like today's database systems?\" In this paper, we introduce\nImpliance, a next-generation information management system consisting of\nhardware and software components integrated to form an easy-to-administer\nappliance that can store, retrieve, and analyze all types of structured,\nsemi-structured, and unstructured information. We first summarize the trends\nthat will shape information management for the foreseeable future. Those trends\nimply three major requirements for Impliance: (1) to be able to store, manage,\nand uniformly query all data, not just structured records; (2) to be able to\nscale out as the volume of this data grows; and (3) to be simple and robust in\noperation. We then describe four key ideas that are uniquely combined in\nImpliance to address these requirements, namely the ideas of: (a) integrating\nsoftware and off-the-shelf hardware into a generic information appliance; (b)\nautomatically discovering, organizing, and managing all data - unstructured as\nwell as structured - in a uniform way; (c) achieving scale-out by exploiting\nsimple, massive parallel processing, and (d) virtualizing compute and storage\nresources to unify, simplify, and streamline the management of Impliance.\nImpliance is an ambitious, long-term effort to define simpler, more robust, and\nmore scalable information systems for tomorrow's enterprises."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0612137v1", 
    "other_authors": "Eric Robinson, David DeWitt", 
    "title": "Turning Cluster Management into Data Management: A System Overview", 
    "arxiv-id": "cs/0612137v1", 
    "author": "David DeWitt", 
    "publish": "2006-12-27T22:21:57Z", 
    "summary": "This paper introduces the CondorJ2 cluster management system. Traditionally,\ncluster management systems such as Condor employ a process-oriented approach\nwith little or no use of modern database system technology. In contrast,\nCondorJ2 employs a data-centric, 3-tier web-application architecture for all\nsystem functions (e.g., job submission, monitoring and scheduling; node\nconfiguration, monitoring and management, etc.) except for job execution.\nEmploying a data-oriented approach allows the core challenge (i.e., managing\nand coordinating a large set of distributed computing resources) to be\ntransformed from a relatively low-level systems problem into a more abstract,\nhigher-level data management problem. Preliminary results suggest that\nCondorJ2's use of standard 3-tier software represents a significant step\nforward to the design and implementation of large clusters (1,000 to 10,000\nnodes)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0701155v1", 
    "other_authors": "Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao, Frank Pellow, Hamid Pirahesh", 
    "title": "Data Cube: A Relational Aggregation Operator Generalizing Group-By,   Cross-Tab, and Sub-Totals", 
    "arxiv-id": "cs/0701155v1", 
    "author": "Hamid Pirahesh", 
    "publish": "2007-01-25T22:39:37Z", 
    "summary": "Data analysis applications typically aggregate data across many dimensions\nlooking for anomalies or unusual patterns. The SQL aggregate functions and the\nGROUP BY operator produce zero-dimensional or one-dimensional aggregates.\nApplications need the N-dimensional generalization of these operators. This\npaper defines that operator, called the data cube or simply cube. The cube\noperator generalizes the histogram, cross-tabulation, roll-up, drill-down, and\nsub-total constructs found in most report writers. The novelty is that cubes\nare relations. Consequently, the cube operator can be imbedded in more complex\nnon-procedural data analysis programs. The cube operator treats each of the N\naggregation attributes as a dimension of N-space. The aggregate of a particular\nset of attribute values is a point in this space. The set of points forms an\nN-dimensional cube. Super-aggregates are computed by aggregating the N-cube to\nlower dimensional spaces. This paper (1) explains the cube and roll-up\noperators, (2) shows how they fit in SQL, (3) explains how users can define new\naggregate functions for cubes, and (4) discusses efficient techniques to\ncompute the cube. Many of these features are being added to the SQL Standard."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0701156v1", 
    "other_authors": "Jim Gray", 
    "title": "Data Management: Past, Present, and Future", 
    "arxiv-id": "cs/0701156v1", 
    "author": "Jim Gray", 
    "publish": "2007-01-25T22:42:28Z", 
    "summary": "Soon most information will be available at your fingertips, anytime,\nanywhere. Rapid advances in storage, communications, and processing allow us\nmove all information into Cyberspace. Software to define, search, and visualize\nonline information is also a key to creating and accessing online information.\nThis article traces the evolution of data management systems and outlines\ncurrent trends. Data management systems began by automating traditional tasks:\nrecording transactions in business, science, and commerce. This data consisted\nprimarily of numbers and character strings. Today these systems provide the\ninfrastructure for much of our society, allowing fast, reliable, secure, and\nautomatic access to data distributed throughout the world. Increasingly these\nsystems automatically design and manage access to the data. The next steps are\nto automate access to richer forms of data: images, sound, video, maps, and\nother media. A second major challenge is automatically summarizing and\nabstracting data in anticipation of user requests. These multi-media databases\nand tools to access them will be a cornerstone of our move to Cyberspace."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0701157v1", 
    "other_authors": "Hal Berenson, Phil Bernstein, Jim Gray, Jim Melton, Elizabeth O'Neil, Patrick O'Neil", 
    "title": "A Critique of ANSI SQL Isolation Levels", 
    "arxiv-id": "cs/0701157v1", 
    "author": "Patrick O'Neil", 
    "publish": "2007-01-25T22:53:48Z", 
    "summary": "ANSI SQL-92 defines Isolation Levels in terms of phenomena: Dirty Reads,\nNon-Repeatable Reads, and Phantoms. This paper shows that these phenomena and\nthe ANSI SQL definitions fail to characterize several popular isolation levels,\nincluding the standard locking implementations of the levels. Investigating the\nambiguities of the phenomena leads to clearer definitions; in addition new\nphenomena that better characterize isolation types are introduced. An important\nmultiversion isolation type, Snapshot Isolation, is defined."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0701158v1", 
    "other_authors": "Jim Gray", 
    "title": "Queues Are Databases", 
    "arxiv-id": "cs/0701158v1", 
    "author": "Jim Gray", 
    "publish": "2007-01-25T22:56:45Z", 
    "summary": "Message-oriented-middleware (MOM) has become an small industry. MOM offers\nqueued transaction processing as an advance over pure client-server transaction\nprocessing. This note makes four points: Queued transaction processing is less\ngeneral than direct transaction processing. Queued systems are built on top of\ndirect systems. You cannot build a direct system atop a queued system. It is\ndifficult to build direct, conversational, or distributed transactions atop a\nqueued system. Queues are interesting databases with interesting concurrency\ncontrol. It is best to build these mechanisms into a standard database system\nso other applications can use these interesting features. Queue systems need\nDBMS functionality. Queues need security, configuration, performance\nmonitoring, recovery, and reorganization utilities. Database systems already\nhave these features. A full-function MOM system duplicates these database\nfeatures. Queue managers are simple TP-monitors managing server pools driven by\nqueues. Database systems are encompassing many server pool features as they\nevolve to TP-lite systems."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0701168v1", 
    "other_authors": "Russell Sears, Catharine Van Ingen, Jim Gray", 
    "title": "To BLOB or Not To BLOB: Large Object Storage in a Database or a   Filesystem?", 
    "arxiv-id": "cs/0701168v1", 
    "author": "Jim Gray", 
    "publish": "2007-01-26T00:54:04Z", 
    "summary": "Application designers often face the question of whether to store large\nobjects in a filesystem or in a database. Often this decision is made for\napplication design simplicity. Sometimes, performance measurements are also\nused. This paper looks at the question of fragmentation - one of the\noperational issues that can affect the performance and/or manageability of the\nsystem as deployed long term. As expected from the common wisdom, objects\nsmaller than 256KB are best stored in a database while objects larger than 1M\nare best stored in the filesystem. Between 256KB and 1MB, the read:write ratio\nand rate of object overwrite or replacement are important factors. We used the\nnotion of \"storage age\" or number of object overwrites as way of normalizing\nwall clock time. Storage age allows our results or similar such results to be\napplied across a number of read:write ratios and object replacement rates."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TNS.2004.832645", 
    "link": "http://arxiv.org/pdf/cs/0702075v1", 
    "other_authors": "Maurice HT Ling", 
    "title": "Firebird Database Backup by Serialized Database Table Dump", 
    "arxiv-id": "cs/0702075v1", 
    "author": "Maurice HT Ling", 
    "publish": "2007-02-13T11:13:29Z", 
    "summary": "This paper presents a simple data dump and load utility for Firebird\ndatabases which mimics mysqldump in MySQL. This utility, fb_dump and fb_load,\nfor dumping and loading respectively, retrieves each database table using\nkinterbasdb and serializes the data using marshal module. This utility has two\nadvantages over the standard Firebird database backup utility, gbak. Firstly,\nit is able to backup and restore single database tables which might help to\nrecover corrupted databases. Secondly, the output is in text-coded format (from\nmarshal module) making it more resilient than a compressed text backup, as in\nthe case of using gbak."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/cs/0702143v1", 
    "other_authors": "Owen Kaser, Daniel Lemire", 
    "title": "Attribute Value Reordering For Efficient Hybrid OLAP", 
    "arxiv-id": "cs/0702143v1", 
    "author": "Daniel Lemire", 
    "publish": "2007-02-24T03:11:09Z", 
    "summary": "The normalization of a data cube is the ordering of the attribute values. For\nlarge multidimensional arrays where dense and sparse chunks are stored\ndifferently, proper normalization can lead to improved storage efficiency. We\nshow that it is NP-hard to compute an optimal normalization even for 1x3\nchunks, although we find an exact algorithm for 1x2 chunks. When dimensions are\nnearly statistically independent, we show that dimension-wise attribute\nfrequency sorting is an optimal normalization and takes time O(d n log(n)) for\ndata cubes of size n^d. When dimensions are not independent, we propose and\nevaluate several heuristics. The hybrid OLAP (HOLAP) storage mechanism is\nalready 19%-30% more efficient than ROLAP, but normalization can improve it\nfurther by 9%-13% for a total gain of 29%-44% over ROLAP."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/cs/0702144v2", 
    "other_authors": "Daniel Lemire, Anna Maclachlan", 
    "title": "Slope One Predictors for Online Rating-Based Collaborative Filtering", 
    "arxiv-id": "cs/0702144v2", 
    "author": "Anna Maclachlan", 
    "publish": "2007-02-24T03:16:27Z", 
    "summary": "Rating-based collaborative filtering is the process of predicting how a user\nwould rate a given item from other user ratings. We propose three related slope\none schemes with predictors of the form f(x) = x + b, which precompute the\naverage difference between the ratings of one item and another for users who\nrated both. Slope one algorithms are easy to implement, efficient to query,\nreasonably accurate, and they support both online queries and dynamic updates,\nwhich makes them good candidates for real-world systems. The basic slope one\nscheme is suggested as a new reference scheme for collaborative filtering. By\nfactoring in items that a user liked separately from items that a user\ndisliked, we achieve results competitive with slower memory-based schemes over\nthe standard benchmark EachMovie and Movielens data sets while better\nfulfilling the desiderata of CF applications."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/cs/0703103v3", 
    "other_authors": "Jia Tao, Shashi Gadia, Tsz Shing Cheng", 
    "title": "Concept of a Value in Multilevel Security Databases", 
    "arxiv-id": "cs/0703103v3", 
    "author": "Tsz Shing Cheng", 
    "publish": "2007-03-22T03:31:27Z", 
    "summary": "This paper has been withdrawn."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/cs/0703113v1", 
    "other_authors": "Kamel Aouiche, Jerome Darmont, Omar Boussaid, Fadila Bentayeb", 
    "title": "Automatic Selection of Bitmap Join Indexes in Data Warehouses", 
    "arxiv-id": "cs/0703113v1", 
    "author": "Fadila Bentayeb", 
    "publish": "2007-03-23T04:25:36Z", 
    "summary": "The queries defined on data warehouses are complex and use several join\noperations that induce an expensive computational cost. This cost becomes even\nmore prohibitive when queries access very large volumes of data. To improve\nresponse time, data warehouse administrators generally use indexing techniques\nsuch as star join indexes or bitmap join indexes. This task is nevertheless\ncomplex and fastidious. Our solution lies in the field of data warehouse\nauto-administration. In this framework, we propose an automatic index selection\nstrategy. We exploit a data mining technique ; more precisely frequent itemset\nmining, in order to determine a set of candidate indexes from a given workload.\nThen, we propose several cost models allowing to create an index configuration\ncomposed by the indexes providing the best profit. These models evaluate the\ncost of accessing data using bitmap join indexes, and the cost of updating and\nstoring these indexes."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/cs/0703114v1", 
    "other_authors": "Kamel Aouiche, Pierre-Emmanuel Jouve, Jerome Darmont", 
    "title": "Clustering-Based Materialized View Selection in Data Warehouses", 
    "arxiv-id": "cs/0703114v1", 
    "author": "Jerome Darmont", 
    "publish": "2007-03-23T04:31:35Z", 
    "summary": "Materialized view selection is a non-trivial task. Hence, its complexity must\nbe reduced. A judicious choice of views must be cost-driven and influenced by\nthe workload experienced by the system. In this paper, we propose a framework\nfor materialized view selection that exploits a data mining technique\n(clustering), in order to determine clusters of similar queries. We also\npropose a view merging algorithm that builds a set of candidate views, as well\nas a greedy process for selecting a set of views to materialize. This selection\nis based on cost models that evaluate the cost of accessing data using views\nand the cost of storing these views. To validate our strategy, we executed a\nworkload of decision-support queries on a test data warehouse, with and without\nusing our strategy. Our experimental results demonstrate its efficiency, even\nwhen storage space is limited."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0704.3500v1", 
    "other_authors": "Zhen He, J\u00e9r\u00f4me Darmont", 
    "title": "Une plate-forme dynamique pour l'\u00e9valuation des performances des bases   de donn\u00e9es \u00e0 objets", 
    "arxiv-id": "0704.3500v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-04-26T09:10:41Z", 
    "summary": "In object-oriented or object-relational databases such as multimedia\ndatabases or most XML databases, access patterns are not static, i.e.,\napplications do not always access the same objects in the same order\nrepeatedly. However, this has been the way these databases and associated\noptimisation techniques such as clustering have been evaluated up to now. This\npaper opens up research regarding this issue by proposing a dynamic object\nevaluation framework (DOEF). DOEF accomplishes access pattern change by\ndefining configurable styles of change. It is a preliminary prototype that has\nbeen designed to be open and fully extensible. Though originally designed for\nthe object-oriented model, it can also be used within the object-relational\nmodel with few adaptations. Furthermore, new access pattern change models can\nbe added too. To illustrate the capabilities of DOEF, we conducted two\ndifferent sets of experiments. In the first set of experiments, we used DOEF to\ncompare the performances of four state of the art dynamic clustering\nalgorithms. The results show that DOEF is effective at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern.\nThey also led us to conclude that dynamic clustering algorithms can cope with\nmoderate levels of access pattern change, but that performance rapidly degrades\nto be worse than no clustering when vigorous styles of access pattern change\nare applied. In the second set of experiments, we used DOEF to compare the\nperformance of two different object stores: Platypus and SHORE. The use of DOEF\nexposed the poor swapping performance of Platypus."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0704.3501v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Fadila Bentayeb, Omar Boussa\u00efd", 
    "title": "Conception d'un banc d'essais d\u00e9cisionnel", 
    "arxiv-id": "0704.3501v1", 
    "author": "Omar Boussa\u00efd", 
    "publish": "2007-04-26T09:13:04Z", 
    "summary": "We present in this paper a new benchmark for evaluating the performances of\ndata warehouses. Benchmarking is useful either to system users for comparing\nthe performances of different systems, or to system engineers for testing the\neffect of various design choices. While the TPC (Transaction Processing\nPerformance Council) standard benchmarks address the first point, they are not\ntuneable enough to address the second one. Our Data Warehouse Engineering\nBenchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses\nand workloads. DWEB is fully parameterized. However, two levels of\nparameterization keep it easy to tune. Since DWEB mainly meets engineering\nbenchmarking needs, it is complimentary to the TPC standard benchmarks, and not\na competitor. Finally, DWEB is implemented as a Java free software that can be\ninterfaced with most existing relational database management systems."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0704.3520v1", 
    "other_authors": "Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "Vers l'auto-administration des entrep\u00f4ts de donn\u00e9es", 
    "arxiv-id": "0704.3520v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-04-26T11:47:35Z", 
    "summary": "With the wide development of databases in general and data warehouses in\nparticular, it is important to reduce the tasks that a database administrator\nmust perform manually. The idea of using data mining techniques to extract\nuseful knowledge for administration from the data themselves has existed for\nsome years. However, little research has been achieved. The aim of this study\nis to search for a way of extracting useful knowledge from stored data to\nautomatically apply performance optimization techniques, and more particularly\nindexing techniques. We have designed a tool that extracts frequent itemsets\nfrom a given workload to compute an index configuration that helps optimizing\ndata access time. The experiments we performed showed that the index\nconfigurations generated by our tool allowed performance gains of 15% to 25% on\na test database and a test data warehouse."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.0281v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Christophe Fromantin, St\u00e9phane R\u00e9gnier, Le Gruenwald, Michel Schneider", 
    "title": "Dynamic Clustering in Object-Oriented Databases: An Advocacy for   Simplicity", 
    "arxiv-id": "0705.0281v1", 
    "author": "Michel Schneider", 
    "publish": "2007-05-02T12:50:39Z", 
    "summary": "We present in this paper three dynamic clustering techniques for\nObject-Oriented Databases (OODBs). The first two, Dynamic, Statistical &\nTunable Clustering (DSTC) and StatClust, exploit both comprehensive usage\nstatistics and the inter-object reference graph. They are quite elaborate.\nHowever, they are also complex to implement and induce a high overhead. The\nthird clustering technique, called Detection & Reclustering of Objects (DRO),\nis based on the same principles, but is much simpler to implement. These three\nclustering algorithm have been implemented in the Texas persistent object store\nand compared in terms of clustering efficiency (i.e., overall performance\nincrease) and overhead using the Object Clustering Benchmark (OCB). The results\nobtained showed that DRO induced a lighter overhead while still achieving\nbetter overall performance."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.0450v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Michel Schneider", 
    "title": "VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the   Performances of OODBs", 
    "arxiv-id": "0705.0450v1", 
    "author": "Michel Schneider", 
    "publish": "2007-05-03T12:50:04Z", 
    "summary": "Performance of object-oriented database systems (OODBs) is still an issue to\nboth designers and users nowadays. The aim of this paper is to propose a\ngeneric discrete-event random simulation model, called VOODB, in order to\nevaluate the performances of OODBs in general, and the performances of\noptimization methods like clustering in particular. Such optimization methods\nundoubtedly improve the performances of OODBs. Yet, they also always induce\nsome kind of overhead for the system. Therefore, it is important to evaluate\ntheir exact impact on the overall performances. VOODB has been designed as a\ngeneric discrete-event random simulation model by putting to use a modelling\napproach, and has been validated by simulating the behavior of the O2 OODB and\nthe Texas persistent object store. Since our final objective is to compare\nobject clustering algorithms, some experiments have also been conducted on the\nDSTC clustering technique, which is implemented in Texas. To validate VOODB,\nperformance results obtained by simulation for a given experiment have been\ncompared to the results obtained by benchmarking the real systems in the same\nconditions. Benchmarking and simulation performance evaluations have been\nobserved to be consistent, so it appears that simulation can be a reliable\napproach to evaluate the performances of OODBs."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.0453v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Bertrand Petit, Michel Schneider", 
    "title": "OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented   Database Systems", 
    "arxiv-id": "0705.0453v1", 
    "author": "Michel Schneider", 
    "publish": "2007-05-03T12:54:30Z", 
    "summary": "We present in this paper a generic object-oriented benchmark (the Object\nClustering Benchmark) that has been designed to evaluate the performances of\nclustering policies in object-oriented databases. OCB is generic because its\nsample database may be customized to fit the databases introduced by the main\nexisting benchmarks (e.g., OO1). OCB's current form is clustering-oriented\nbecause of its clustering-oriented workload, but it can be easily adapted to\nother purposes. Lastly, OCB's code is compact and easily portable. OCB has been\nimplemented in a real system (Texas, running on a Sun workstation), in order to\ntest a specific clustering policy called DSTC. A few results concerning this\ntest are presented."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.0454v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Amar Attoui, Michel Gourgand", 
    "title": "Performance Evaluation for Clustering Algorithms in Object-Oriented   Database Systems", 
    "arxiv-id": "0705.0454v1", 
    "author": "Michel Gourgand", 
    "publish": "2007-05-03T13:02:06Z", 
    "summary": "It is widely acknowledged that good object clustering is critical to the\nperformance of object-oriented databases. However, object clustering always\ninvolves some kind of overhead for the system. The aim of this paper is to\npropose a modelling methodology in order to evaluate the performances of\ndifferent clustering policies. This methodology has been used to compare the\nperformances of three clustering algorithms found in the literature (Cactis, CK\nand ORION) that we considered representative of the current research in the\nfield of object clustering. The actual performance evaluation was performed\nusing simulation. Simulation experiments we performed showed that the Cactis\nalgorithm is better than the ORION algorithm and that the CK algorithm totally\noutperforms both other algorithms in terms of response time and clustering\noverhead."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.1453v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Fadila Bentayeb, Omar Boussa\u00efd", 
    "title": "DWEB: A Data Warehouse Engineering Benchmark", 
    "arxiv-id": "0705.1453v1", 
    "author": "Omar Boussa\u00efd", 
    "publish": "2007-05-10T12:23:35Z", 
    "summary": "Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed. This is usually\ndone with the help of benchmarks, which can either help system users comparing\nthe performances of different systems, or help system engineers testing the\neffect of various design choices. While the TPC standard decision support\nbenchmarks address the first point, they are not tuneable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc\nsynthetic data warehouses and workloads. DWEB is fully parameterized to fulfill\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. Finally, DWEB is implemented as a Java free software\nthat can be interfaced with most existing relational database management\nsystems. A sample usage of DWEB is also provided in this paper."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.1454v1", 
    "other_authors": "Zhen He, J\u00e9r\u00f4me Darmont", 
    "title": "DOEF: A Dynamic Object Evaluation Framework", 
    "arxiv-id": "0705.1454v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-05-10T12:24:27Z", 
    "summary": "In object-oriented or object-relational databases such as multimedia\ndatabases or most XML databases, access patterns are not static, i.e.,\napplications do not always access the same objects in the same order\nrepeatedly. However, this has been the way these databases and associated\noptimisation techniques like clustering have been evaluated up to now. This\npaper opens up research regarding this issue by proposing a dynamic object\nevaluation framework (DOEF) that accomplishes access pattern change by defining\nconfigurable styles of change. This preliminary prototype has been designed to\nbe open and fully extensible. To illustrate the capabilities of DOEF, we used\nit to compare the performances of four state of the art dynamic clustering\nalgorithms. The results show that DOEF is indeed effective at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.1455v1", 
    "other_authors": "Fadila Bentayeb, J\u00e9r\u00f4me Darmont", 
    "title": "Decision tree modeling with relational views", 
    "arxiv-id": "0705.1455v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-05-10T12:25:57Z", 
    "summary": "Data mining is a useful decision support technique that can be used to\ndiscover production rules in warehouses or corporate data. Data mining research\nhas made much effort to apply various mining algorithms efficiently on large\ndatabases. However, a serious problem in their practical application is the\nlong processing time of such algorithms. Nowadays, one of the key challenges is\nto integrate data mining methods within the framework of traditional database\nsystems. Indeed, such implementations can take advantage of the efficiency\nprovided by SQL engines. In this paper, we propose an integrating approach for\ndecision trees within a classical database system. In other words, we try to\ndiscover knowledge from relational databases, in the form of production rules,\nvia a procedure embedding SQL queries. The obtained decision tree is defined by\nsuccessive, related relational views. Each view corresponds to a given\npopulation in the underlying decision tree. We selected the classical Induction\nDecision Tree (ID3) algorithm to build the decision tree. To prove that our\nimplementation of ID3 works properly, we successfully compared the output of\nour procedure with the output of an existing and validated data mining\nsoftware, SIPINA. Furthermore, since our approach is tuneable, it can be\ngeneralized to any other similar decision tree-based method."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.1456v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Omar Boussa\u00efd, Fadila Bentayeb", 
    "title": "Warehousing Web Data", 
    "arxiv-id": "0705.1456v1", 
    "author": "Fadila Bentayeb", 
    "publish": "2007-05-10T12:28:52Z", 
    "summary": "In a data warehousing process, mastering the data preparation phase allows\nsubstantial gains in terms of time and performance when performing\nmultidimensional analysis or using data mining algorithms. Furthermore, a data\nwarehouse can require external data. The web is a prevalent data source in this\ncontext. In this paper, we propose a modeling process for integrating diverse\nand heterogeneous (so-called multiform) data into a unified format.\nFurthermore, the very schema definition provides first-rate metadata in our\ndata warehousing context. At the conceptual level, a complex object is\nrepresented in UML. Our logical model is an XML schema that can be described\nwith a DTD or the XML-Schema language. Eventually, we have designed a Java\nprototype that transforms our multiform input data into XML documents\nrepresenting our physical model. Then, the XML documents we obtain are mapped\ninto a relational database we view as an ODS (Operational Data Storage), whose\ncontent will have to be re-modeled in a multidimensional way to allow its\nstorage in a star schema-based warehouse and, later, its analysis."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.1457v1", 
    "other_authors": "Sami Miniaoui, J\u00e9r\u00f4me Darmont, Omar Boussa\u00efd", 
    "title": "Web data modeling for integration in data warehouses", 
    "arxiv-id": "0705.1457v1", 
    "author": "Omar Boussa\u00efd", 
    "publish": "2007-05-10T12:30:19Z", 
    "summary": "In a data warehousing process, the data preparation phase is crucial.\nMastering this phase allows substantial gains in terms of time and performance\nwhen performing a multidimensional analysis or using data mining algorithms.\nFurthermore, a data warehouse can require external data. The web is a prevalent\ndata source in this context, but the data broadcasted on this medium are very\nheterogeneous. We propose in this paper a UML conceptual model for a complex\nobject representing a superclass of any useful data source (databases, plain\ntexts, HTML and XML documents, images, sounds, video clips...). The translation\ninto a logical model is achieved with XML, which helps integrating all these\ndiverse, heterogeneous data into a unified format, and whose schema definition\nprovides first-rate metadata in our data warehousing context. Moreover, we\nbenefit from XML's flexibility, extensibility and from the richness of the\nsemi-structured data model, but we are still able to later map XML documents\ninto a database if more structuring is needed."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.2787v1", 
    "other_authors": "David J. Martin, Daniel Kifer, Ashwin Machanavajjhala, Johannes Gehrke, Joseph Y. Halpern", 
    "title": "Worst-Case Background Knowledge for Privacy-Preserving Data Publishing", 
    "arxiv-id": "0705.2787v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2007-05-19T00:12:24Z", 
    "summary": "Recent work has shown the necessity of considering an attacker's background\nknowledge when reasoning about privacy in data publishing. However, in\npractice, the data publisher does not know what background knowledge the\nattacker possesses. Thus, it is important to consider the worst-case. In this\npaper, we initiate a formal study of worst-case background knowledge. We\npropose a language that can express any background knowledge about the data. We\nprovide a polynomial time algorithm to measure the amount of disclosure of\nsensitive information in the worst case, given that the attacker has at most a\nspecified number of pieces of information in this language. We also provide a\nmethod to efficiently sanitize the data so that the amount of disclosure in the\nworst case is less than a specified threshold."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0705.4442v2", 
    "other_authors": "Dan Olteanu, Christoph Koch, Lyublena Antova", 
    "title": "World-set Decompositions: Expressiveness and Efficient Algorithms", 
    "arxiv-id": "0705.4442v2", 
    "author": "Lyublena Antova", 
    "publish": "2007-05-30T17:56:06Z", 
    "summary": "Uncertain information is commonplace in real-world data management scenarios.\nThe ability to represent large sets of possible instances (worlds) while\nsupporting efficient storage and processing is an important challenge in this\ncontext. The recent formalism of world-set decompositions (WSDs) provides a\nspace-efficient representation for uncertain data that also supports scalable\nprocessing. WSDs are complete for finite world-sets in that they can represent\nany finite set of possible worlds. For possibly infinite world-sets, we show\nthat a natural generalization of WSDs precisely captures the expressive power\nof c-tables. We then show that several important decision problems are\nefficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we\ngive a polynomial-time algorithm for factorizing WSDs, i.e. an efficient\nalgorithm for minimizing such representations."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.0745v1", 
    "other_authors": "K. Munir, M. Odeh, R. McClatchey, S. Khan, I. Habib", 
    "title": "Semantic Information Retrieval from Distributed Heterogeneous Data   Sources", 
    "arxiv-id": "0707.0745v1", 
    "author": "I. Habib", 
    "publish": "2007-07-05T09:40:19Z", 
    "summary": "Information retrieval from distributed heterogeneous data sources remains a\nchallenging issue. As the number of data sources increases more intelligent\nretrieval techniques, focusing on information content and semantics, are\nrequired. Currently ontologies are being widely used for managing semantic\nknowledge, especially in the field of bioinformatics. In this paper we describe\nan ontology assisted system that allows users to query distributed\nheterogeneous data sources by hiding details like location, information\nstructure, access pattern and semantic structure of the data. Our goal is to\nprovide an integrated view on biomedical information sources for the\nHealth-e-Child project with the aim to overcome the lack of sufficient\nsemantic-based reformulation techniques for querying distributed data sources.\nIn particular, this paper examines the problem of query reformulation across\nbiomedical data sources, based on merged ontologies and the underlying\nheterogeneous descriptions of the respective data sources."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.0763v1", 
    "other_authors": "Ashiq Anjum, Peter Bloodsworth, Andrew Branson, Tamas Hauer, Richard McClatchey, Kamran Munir, Dmitry Rogulin, Jetendr Shamdasani", 
    "title": "The Requirements for Ontologies in Medical Data Integration: A Case   Study", 
    "arxiv-id": "0707.0763v1", 
    "author": "Jetendr Shamdasani", 
    "publish": "2007-07-05T11:21:39Z", 
    "summary": "Evidence-based medicine is critically dependent on three sources of\ninformation: a medical knowledge base, the patients medical record and\nknowledge of available resources, including where appropriate, clinical\nprotocols. Patient data is often scattered in a variety of databases and may,\nin a distributed model, be held across several disparate repositories.\nConsequently addressing the needs of an evidence-based medicine community\npresents issues of biomedical data integration, clinical interpretation and\nknowledge management. This paper outlines how the Health-e-Child project has\napproached the challenge of requirements specification for (bio-) medical data\nintegration, from the level of cellular data, through disease to that of\npatient and population. The approach is illuminated through the requirements\nelicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three\ndiseases being studied in the EC-funded Health-e-Child project."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.1288v1", 
    "other_authors": "Riadh Ben Messaoud, Kamel Aouiche, C\u00e9cile Favre", 
    "title": "Espaces de repr\u00e9sentation multidimensionnels d\u00e9di\u00e9s \u00e0 la   visualisation", 
    "arxiv-id": "0707.1288v1", 
    "author": "C\u00e9cile Favre", 
    "publish": "2007-07-09T15:52:02Z", 
    "summary": "In decision-support systems, the visual component is important for On Line\nAnalysis Processing (OLAP). In this paper, we propose a new approach that faces\nthe visualization problem due to data sparsity. We use the results of a\nMultiple Correspondence Analysis (MCA) to reduce the negative effect of\nsparsity by organizing differently data cube cells. Our approach does not\nreduce sparsity, however it tries to build relevant representation spaces where\nfacts are efficiently gathered. In order to evaluate our approach, we propose\nan homogeneity criterion based on geometric neighborhood of cells. The obtained\nexperimental results have shown the efficiency of our method."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.1304v1", 
    "other_authors": "Hadj Mahboubi, Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "Un index de jointure pour les entrep\u00f4ts de donn\u00e9es XML", 
    "arxiv-id": "0707.1304v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-07-09T16:58:14Z", 
    "summary": "XML data warehouses form an interesting basis for decision-support\napplications that exploit heterogeneous data from multiple sources. However,\nXML-native database systems currently bear limited performances and it is\nnecessary to research ways to optimize them. In this paper, we propose a new\nindex that is specifically adapted to the multidimensional architecture of XML\nwarehouses and eliminates join operations, while preserving the information\ncontained in the original warehouse. A theoretical study and experimental\nresults demonstrate the efficiency of our index, even when queries are complex."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.1306v1", 
    "other_authors": "Nora Maiz, Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "S\u00e9lection simultan\u00e9e d'index et de vues mat\u00e9rialis\u00e9es", 
    "arxiv-id": "0707.1306v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-07-09T17:23:31Z", 
    "summary": "Indices and materialized views are physical structures that accelerate data\naccess in data warehouses. However, these data structures generate some\nmaintenance overhead. They also share the same storage space. The existing\nstudies about index and materialized view selection consider these structures\nseparately. In this paper, we adopt the opposite stance and couple index and\nmaterialized view selection to take into account the interactions between them\nand achieve an efficient storage space sharing. We develop cost models that\nevaluate the respective benefit of indexing and view materialization. These\ncost models are then exploited by a greedy algorithm to select a relevant\nconfiguration of indices and materialized views. Experimental results show that\nour strategy performs better than the independent selection of indices and\nmaterialized views."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.1534v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Omar Boussaid, Jean-Christian Ralaivao, Kamel Aouiche", 
    "title": "An Architecture Framework for Complex Data Warehouses", 
    "arxiv-id": "0707.1534v1", 
    "author": "Kamel Aouiche", 
    "publish": "2007-07-10T22:01:40Z", 
    "summary": "Nowadays, many decision support applications need to exploit data that are\nnot only numerical or symbolic, but also multimedia, multistructure,\nmultisource, multimodal, and/or multiversion. We term such data complex data.\nManaging and analyzing complex data involves a lot of different issues\nregarding their structure, storage and processing, and metadata are a key\nelement in all these processes. Such problems have been addressed by classical\ndata warehousing (i.e., applied to \"simple\" data). However, data warehousing\napproaches need to be adapted for complex data. In this paper, we first propose\na precise, though open, definition of complex data. Then we present a general\narchitecture framework for warehousing complex data. This architecture heavily\nrelies on metadata and domain-related knowledge, and rests on the XML language,\nwhich helps storing data, metadata and domain-specific knowledge altogether,\nand facilitates communication between the various warehousing processes."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.1548v1", 
    "other_authors": "Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "Data Mining-based Materialized View and Index Selection in Data   Warehouses", 
    "arxiv-id": "0707.1548v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2007-07-11T02:45:10Z", 
    "summary": "Materialized views and indexes are physical structures for accelerating data\naccess that are casually used in data warehouses. However, these data\nstructures generate some maintenance overhead. They also share the same storage\nspace. Most existing studies about materialized view and index selection\nconsider these structures separately. In this paper, we adopt the opposite\nstance and couple materialized view and index selection to take view-index\ninteractions into account and achieve efficient storage space sharing.\nCandidate materialized views and indexes are selected through a data mining\nprocess. We also exploit cost models that evaluate the respective benefit of\nindexing and view materialization, and help select a relevant configuration of\nindexes and materialized views among the candidates. Experimental results show\nthat our strategy performs better than an independent selection of materialized\nviews and indexes."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0707.4304v1", 
    "other_authors": "Leticia Gomez, Sofie Haesevoets, Bart Kuijpers, Alejandro Vaisman", 
    "title": "Spatial Aggregation: Data Model and Implementation", 
    "arxiv-id": "0707.4304v1", 
    "author": "Alejandro Vaisman", 
    "publish": "2007-07-29T21:46:50Z", 
    "summary": "Data aggregation in Geographic Information Systems (GIS) is only marginally\npresent in commercial systems nowadays, mostly through ad-hoc solutions. In\nthis paper, we first present a formal model for representing spatial data. This\nmodel integrates geographic data and information contained in data warehouses\nexternal to the GIS. We define the notion of geometric aggregation, a general\nframework for aggregate queries in a GIS setting. We also identify the class of\nsummable queries, which can be efficiently evaluated by precomputing the\noverlay of two or more of the thematic layers involved in the query. We also\nsketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS\nand OLAP features. In addition, we introduce Piet, an implementation of our\nproposal, that makes use of overlay precomputation for answering spatial\nqueries (aggregate or not). Our experimental evaluation showed that for a\ncertain class of geometric queries with or without aggregation, overlay\nprecomputation outperforms R-tree-based techniques. Finally, as a particular\napplication of our proposal, we study topological queries."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0708.0361v7", 
    "other_authors": "Evgeniy Grigoriev", 
    "title": "Why the relational data model can be considered as a formal basis for   group operations in object-oriented systems", 
    "arxiv-id": "0708.0361v7", 
    "author": "Evgeniy Grigoriev", 
    "publish": "2007-08-02T15:24:29Z", 
    "summary": "Relational data model defines a specification of a type \"relation\". However,\nits simplicity does not mean that the system implementing this model must\noperate with structures having the same simplicity. We consider two principles\nallowing create a system which combines object-oriented paradigm (OOP) and\nrelational data model (RDM) in one framework. The first principle -- \"complex\ndata in encapsulated domains\" -- is well known from The Third Manifesto by Date\nand Darwen. The second principle --\"data complexity in names\"-- is the basis\nfor a system where data are described as complex objects and uniquely\nrepresented as a set of relations. Names of these relations and names of their\nattributes are combinations of names entered in specifications of the complex\nobjects. Below, we consider the main properties of such a system."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0708.2076v1", 
    "other_authors": "Loreto Bravo, James Cheney, Irini Fundulaki", 
    "title": "Repairing Inconsistent XML Write-Access Control Policies", 
    "arxiv-id": "0708.2076v1", 
    "author": "Irini Fundulaki", 
    "publish": "2007-08-15T18:31:48Z", 
    "summary": "XML access control policies involving updates may contain security flaws,\nhere called inconsistencies, in which a forbidden operation may be simulated by\nperforming a sequence of allowed operations. This paper investigates the\nproblem of deciding whether a policy is consistent, and if not, how its\ninconsistencies can be repaired. We consider policies expressed in terms of\nannotated DTDs defining which operations are allowed or denied for the XML\ntrees that are instances of the DTD. We show that consistency is decidable in\nPTIME for such policies and that consistent partial policies can be extended to\nunique \"least-privilege\" consistent total policies. We also consider repair\nproblems based on deleting privileges to restore consistency, show that finding\nminimal repairs is NP-complete, and give heuristics for finding repairs."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2005.09.005", 
    "link": "http://arxiv.org/pdf/0708.2717v1", 
    "other_authors": "Leticia Gomez, Bart Kuijpers, Alejandro Vaisman", 
    "title": "Aggregation Languages for Moving Object and Places of Interest Data", 
    "arxiv-id": "0708.2717v1", 
    "author": "Alejandro Vaisman", 
    "publish": "2007-08-20T20:08:53Z", 
    "summary": "We address aggregate queries over GIS data and moving object data, where\nnon-spatial data are stored in a data warehouse. We propose a formal data model\nand query language to express complex aggregate queries. Next, we study the\ncompression of trajectory data, produced by moving objects, using the notions\nof stops and moves. We show that stops and moves are expressible in our query\nlanguage and we consider a fragment of this language, consisting of regular\nexpressions to talk about temporally ordered sequences of stops and moves. This\nfragment can be used to efficiently express data mining and pattern matching\ntasks over trajectory data."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0709.1166v1", 
    "other_authors": "Daniel Lemire, Martin Brooks, Yuhong Yan", 
    "title": "An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation", 
    "arxiv-id": "0709.1166v1", 
    "author": "Yuhong Yan", 
    "publish": "2007-09-07T21:18:57Z", 
    "summary": "Monotonicity is a simple yet significant qualitative characteristic. We\nconsider the problem of segmenting a sequence in up to K segments. We want\nsegments to be as monotonic as possible and to alternate signs. We propose a\nquality metric for this problem using the l_inf norm, and we present an optimal\nlinear time algorithm based on novel formalism. Moreover, given a\nprecomputation in time O(n log n) consisting of a labeling of all extrema, we\ncompute any optimal segmentation in constant time. We compare experimentally\nits performance to two piecewise linear segmentation heuristics (top-down and\nbottom-up). We show that our algorithm is faster and more accurate.\nApplications include pattern recognition and qualitative modeling."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0710.2156v3", 
    "other_authors": "Kamel Aouiche, Daniel Lemire, Robert Godin", 
    "title": "Collaborative OLAP with Tag Clouds: Web 2.0 OLAP Formalism and   Experimental Evaluation", 
    "arxiv-id": "0710.2156v3", 
    "author": "Robert Godin", 
    "publish": "2007-10-11T19:48:10Z", 
    "summary": "Increasingly, business projects are ephemeral. New Business Intelligence\ntools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds\nare a popular community-driven visualization technique. Hence, we investigate\ntag-cloud views with support for OLAP operations such as roll-ups, slices,\ndices, clustering, and drill-downs. As a case study, we implemented an\napplication where users can upload data and immediately navigate through its ad\nhoc dimensions. To support social networking, views can be easily shared and\nembedded in other Web sites. Algorithmically, our tag-cloud views are\napproximate range top-k queries over spontaneous data cubes. We present\nexperimental evidence that iceberg cuboids provide adequate online\napproximations. We benchmark several browser-oblivious tag-cloud layout\noptimizations."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0710.2604v1", 
    "other_authors": "Raymond Chi-Wing Wong, Ada Wai-chee Fu, Jian Pei, Yip Sing Ho, Tai Wong, Yubao Liu", 
    "title": "Efficient Skyline Querying with Variable User Preferences on Nominal   Attributes", 
    "arxiv-id": "0710.2604v1", 
    "author": "Yubao Liu", 
    "publish": "2007-10-13T11:47:14Z", 
    "summary": "Current skyline evaluation techniques assume a fixed ordering on the\nattributes. However, dynamic preferences on nominal attributes are more\nrealistic in known applications. In order to generate online response for any\nsuch preference issued by a user, we propose two methods of different\ncharacteristics. The first one is a semi-materialization method and the second\nis an adaptive SFS method. Finally, we conduct experiments to show the\nefficiency of our proposed algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0710.5333v1", 
    "other_authors": "Haibin Wang, Rajshekhar Sunderraman, Florentin Smarandache, Andre Rogatko", 
    "title": "Neutrosophic Relational Data Model", 
    "arxiv-id": "0710.5333v1", 
    "author": "Andre Rogatko", 
    "publish": "2007-10-29T19:46:58Z", 
    "summary": "In this paper, we present a generalization of the relational data model based\non interval neutrosophic set. Our data model is capable of manipulating\nincomplete as well as inconsistent information. Fuzzy relation or\nintuitionistic fuzzy relation can only handle incomplete information.\nAssociated with each relation are two membership functions one is called\ntruth-membership function T which keeps track of the extent to which we believe\nthe tuple is in the relation, another is called falsity-membership function F\nwhich keeps track of the extent to which we believe that it is not in the\nrelation. A neutrosophic relation is inconsistent if there exists one tuple a\nsuch that T(a) + F(a) > 1 . In order to handle inconsistent situation, we\npropose an operator called \"split\" to transform inconsistent neutrosophic\nrelations into pseudo-consistent neutrosophic relations and do the\nset-theoretic and relation-theoretic operations on them and finally use another\noperator called \"combine\" to transform the result back to neutrosophic\nrelation. For this data model, we define algebraic operators that are\ngeneralizations of the usual operators such as intersection, union, selection,\njoin on fuzzy relations. Our data model can underlie any database and\nknowledge-base management system that deals with incomplete and inconsistent\ninformation."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0711.3375v1", 
    "other_authors": "Loredana Afanasiev, Torsten Grust, Maarten Marx, Jan Rittinger, Jens Teubner", 
    "title": "An Inflationary Fixed Point Operator in XQuery", 
    "arxiv-id": "0711.3375v1", 
    "author": "Jens Teubner", 
    "publish": "2007-11-21T13:22:15Z", 
    "summary": "We introduce a controlled form of recursion in XQuery, inflationary fixed\npoints, familiar in the context of relational databases. This imposes\nrestrictions on the expressible types of recursion, but we show that\ninflationary fixed points nevertheless are sufficiently versatile to capture a\nwide range of interesting use cases, including the semantics of Regular XPath\nand its core transitive closure construct.\n  While the optimization of general user-defined recursive functions in XQuery\nappears elusive, we will describe how inflationary fixed points can be\nefficiently evaluated, provided that the recursive XQuery expressions exhibit a\ndistributivity property. We show how distributivity can be assessed both,\nsyntactically and algebraically, and provide experimental evidence that XQuery\nprocessors can substantially benefit during inflationary fixed point\nevaluation."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0801.0131v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Two-Level Concept-Oriented Data Model", 
    "arxiv-id": "0801.0131v1", 
    "author": "Alexandr Savinov", 
    "publish": "2007-12-30T14:29:17Z", 
    "summary": "In this paper we describe a new approach to data modelling called the\nconcept-oriented model (CoM). This model is based on the formalism of nested\nordered sets which uses inclusion relation to produce hierarchical structure of\nsets and ordering relation to produce multi-dimensional structure among its\nelements. Nested ordered set is defined as an ordered set where an each element\ncan be itself an ordered set. Ordering relation in CoM is used to define data\nsemantics and operations with data such as projection and de-projection. This\ndata model can be applied to very different problems and the paper describes\nsome its uses such grouping with aggregation and multi-dimensional analysis."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0801.0139v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Principles of the Concept-Oriented Data Model", 
    "arxiv-id": "0801.0139v1", 
    "author": "Alexandr Savinov", 
    "publish": "2007-12-30T15:04:25Z", 
    "summary": "In the paper a new approach to data representation and manipulation is\ndescribed, which is called the concept-oriented data model (CODM). It is\nsupposed that items represent data units, which are stored in concepts. A\nconcept is a combination of superconcepts, which determine the concept's\ndimensionality or properties. An item is a combination of superitems taken by\none from all the superconcepts. An item stores a combination of references to\nits superitems. The references implement inclusion relation or attribute-value\nrelation among items. A concept-oriented database is defined by its concept\nstructure called syntax or schema and its item structure called semantics. The\nmodel defines formal transformations of syntax and semantics including the\ncanonical semantics where all concepts are merged and the data semantics is\nrepresented by one set of items. The concept-oriented data model treats\nrelations as subconcepts where items are instances of the relations.\nMulti-valued attributes are defined via subconcepts as a view on the database\nsemantics rather than as a built-in mechanism. The model includes\nconcept-oriented query language, which is based on collection manipulations. It\nalso has such mechanisms as aggregation and inference based on semantics\npropagation through the database schema."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0802.0137v3", 
    "other_authors": "Pierre Sutra, Marc Shapiro", 
    "title": "Fault-Tolerant Partial Replication in Large-Scale Database Systems", 
    "arxiv-id": "0802.0137v3", 
    "author": "Marc Shapiro", 
    "publish": "2008-02-01T14:47:24Z", 
    "summary": "We investigate a decentralised approach to committing transactions in a\nreplicated database, under partial replication. Previous protocols either\nre-execute transactions entirely and/or compute a total order of transactions.\nIn contrast, ours applies update values, and orders only conflicting\ntransactions. It results that transactions execute faster, and distributed\ndatabases commit in small committees. Both effects contribute to preserve\nscalability as the number of databases and transactions increase. Our algorithm\nensures serializability, and is live and safe in spite of faults."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0804.3171v2", 
    "other_authors": "Prashanth Alluvada", 
    "title": "Optimization Approach for Detecting the Critical Data on a Database", 
    "arxiv-id": "0804.3171v2", 
    "author": "Prashanth Alluvada", 
    "publish": "2008-04-20T03:23:38Z", 
    "summary": "Through purposeful introduction of malicious transactions (tracking\ntransactions) into randomly select nodes of a (database) graph, soiled and\nclean segments are identified. Soiled and clean measures corresponding those\nsegments are then computed. These measures are used to repose the problem of\ncritical database elements detection as an optimization problem over the graph.\nThis method is universally applicable over a large class of graphs (including\ndirected, weighted, disconnected, cyclic) that occur in several contexts of\ndatabases. A generalization argument is presented which extends the critical\ndata problem to abstract settings."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0805.3339v3", 
    "other_authors": "Kamel Aouiche, Daniel Lemire, Owen Kaser", 
    "title": "Tri de la table de faits et compression des index bitmaps avec   alignement sur les mots", 
    "arxiv-id": "0805.3339v3", 
    "author": "Owen Kaser", 
    "publish": "2008-05-21T19:50:46Z", 
    "summary": "Bitmap indexes are frequently used to index multidimensional data. They rely\nmostly on sequential input/output. Bitmaps can be compressed to reduce\ninput/output costs and minimize CPU usage. The most efficient compression\ntechniques are based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. This type of compression accelerates logical operations\n(AND, OR) over the bitmaps. However, run-length encoding is sensitive to the\norder of the facts. Thus, we propose to sort the fact tables. We review\nlexicographic, Gray-code, and block-wise sorting. We found that a lexicographic\nsort improves compression--sometimes generating indexes twice as small--and\nmake indexes several times faster. While sorting takes time, this is partially\noffset by the fact that it is faster to index a sorted table. Column order is\nsignificant: it is generally preferable to put the columns having more distinct\nvalues at the beginning. A block-wise sort is much less efficient than a full\nsort. Moreover, we found that Gray-code sorting is not better than\nlexicographic sorting when using word-aligned compression."
},{
    "category": "cs.DB", 
    "doi": "10.1080/00207160701694153", 
    "link": "http://arxiv.org/pdf/0808.2083v3", 
    "other_authors": "Owen Kaser, Daniel Lemire, Kamel Aouiche", 
    "title": "Histogram-Aware Sorting for Enhanced Word-Aligned Compression in Bitmap   Indexes", 
    "arxiv-id": "0808.2083v3", 
    "author": "Kamel Aouiche", 
    "publish": "2008-08-15T03:14:55Z", 
    "summary": "Bitmap indexes must be compressed to reduce input/output costs and minimize\nCPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use\ntechniques based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. These techniques are sensitive to the order of the rows: a\nsimple lexicographical sort can divide the index size by 9 and make indexes\nseveral times faster. We investigate reordering heuristics based on computed\nattribute-value histograms. Simply permuting the columns of the table based on\nthese histograms can increase the sorting efficiency by 40%."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.0116v1", 
    "other_authors": "David J. Martin, Johannes Gehrke, Joseph Y. Halpern", 
    "title": "Toward Expressive and Scalable Sponsored Search Auctions", 
    "arxiv-id": "0809.0116v1", 
    "author": "Joseph Y. Halpern", 
    "publish": "2008-08-31T12:24:38Z", 
    "summary": "Internet search results are a growing and highly profitable advertising\nplatform. Search providers auction advertising slots to advertisers on their\nsearch result pages. Due to the high volume of searches and the users' low\ntolerance for search result latency, it is imperative to resolve these auctions\nfast. Current approaches restrict the expressiveness of bids in order to\nachieve fast winner determination, which is the problem of allocating slots to\nadvertisers so as to maximize the expected revenue given the advertisers' bids.\nThe goal of our work is to permit more expressive bidding, thus allowing\nadvertisers to achieve complex advertising goals, while still providing fast\nand scalable techniques for winner determination."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.1551v2", 
    "other_authors": "Slawomir Staworko, Jan Chomicki", 
    "title": "Consistent Query Answers in the Presence of Universal Constraints", 
    "arxiv-id": "0809.1551v2", 
    "author": "Jan Chomicki", 
    "publish": "2008-09-09T15:04:52Z", 
    "summary": "The framework of consistent query answers and repairs has been introduced to\nalleviate the impact of inconsistent data on the answers to a query. A repair\nis a minimally different consistent instance and an answer is consistent if it\nis present in every repair. In this article we study the complexity of\nconsistent query answers and repair checking in the presence of universal\nconstraints.\n  We propose an extended version of the conflict hypergraph which allows to\ncapture all repairs w.r.t. a set of universal constraints. We show that repair\nchecking is in PTIME for the class of full tuple-generating dependencies and\ndenial constraints, and we present a polynomial repair algorithm. This\nalgorithm is sound, i.e. always produces a repair, but also complete, i.e.\nevery repair can be constructed. Next, we present a polynomial-time algorithm\ncomputing consistent answers to ground quantifier-free queries in the presence\nof denial constraints, join dependencies, and acyclic full-tuple generating\ndependencies. Finally, we show that extending the class of constraints leads to\nintractability. For arbitrary full tuple-generating dependencies consistent\nquery answering becomes coNP-complete. For arbitrary universal constraints\nconsistent query answering is \\Pi_2^p-complete and repair checking\ncoNP-complete."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.1963v1", 
    "other_authors": "Hadj Mahboubi, Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "Materialized View Selection by Query Clustering in XML Data Warehouses", 
    "arxiv-id": "0809.1963v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-11T11:59:00Z", 
    "summary": "XML data warehouses form an interesting basis for decision-support\napplications that exploit complex data. However, native XML database management\nsystems currently bear limited performances and it is necessary to design\nstrategies to optimize them. In this paper, we propose an automatic strategy\nfor the selection of XML materialized views that exploits a data mining\ntechnique, more precisely the clustering of the query workload. To validate our\nstrategy, we implemented an XML warehouse modeled along the XCube\nspecifications. We executed a workload of XQuery decision-support queries on\nthis warehouse, with and without using our strategy. Our experimental results\ndemonstrate its efficiency, even when queries are complex."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.1965v1", 
    "other_authors": "St\u00e9phane Azefack, Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "Dynamic index selection in data warehouses", 
    "arxiv-id": "0809.1965v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-11T12:01:34Z", 
    "summary": "Analytical queries defined on data warehouses are complex and use several\njoin operations that are very costly, especially when run on very large data\nvolumes. To improve response times, data warehouse administrators casually use\nindexing techniques. This task is nevertheless complex and fastidious. In this\npaper, we present an automatic, dynamic index selection method for data\nwarehouses that is based on incremental frequent itemset mining from a given\nquery workload. The main advantage of this approach is that it helps update the\nset of selected indexes when workload evolves instead of recreating it from\nscratch. Preliminary experimental results illustrate the efficiency of this\napproach, both in terms of performance enhancement and overhead."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.1971v1", 
    "other_authors": "Jean-Christian Ralaivao, J\u00e9r\u00f4me Darmont", 
    "title": "Knowledge and Metadata Integration for Warehousing Complex Data", 
    "arxiv-id": "0809.1971v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-11T12:20:00Z", 
    "summary": "With the ever-growing availability of so-called complex data, especially on\nthe Web, decision-support systems such as data warehouses must store and\nprocess data that are not only numerical or symbolic. Warehousing and analyzing\nsuch data requires the joint exploitation of metadata and domain-related\nknowledge, which must thereby be integrated. In this paper, we survey the types\nof knowledge and metadata that are needed for managing complex data, discuss\nthe issue of knowledge and metadata integration, and propose a CWM-compliant\nintegration solution that we incorporate into an XML complex data warehousing\nframework we previously designed."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.1981v1", 
    "other_authors": "Hadj Mahboubi, Kamel Aouiche, J\u00e9r\u00f4me Darmont", 
    "title": "A Join Index for XML Data Warehouses", 
    "arxiv-id": "0809.1981v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-11T12:44:10Z", 
    "summary": "XML data warehouses form an interesting basis for decision-support\napplications that exploit complex data. However, native-XML database management\nsystems (DBMSs) currently bear limited performances and it is necessary to\nresearch for ways to optimize them. In this paper, we propose a new join index\nthat is specifically adapted to the multidimensional architecture of XML\nwarehouses. It eliminates join operations while preserving the information\ncontained in the original warehouse. A theoretical study and experimental\nresults demonstrate the efficiency of our join index. They also show that\nnative XML DBMSs can compete with XML-compatible, relational DBMSs when\nwarehousing and analyzing XML data."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.2686v1", 
    "other_authors": "Omar Boussa\u00efd, Fadila Bentayeb, J\u00e9r\u00f4me Darmont", 
    "title": "An MAS-Based ETL Approach for Complex Data", 
    "arxiv-id": "0809.2686v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-16T11:42:42Z", 
    "summary": "In a data warehousing process, the phase of data integration is crucial. Many\nmethods for data integration have been published in the literature. However,\nwith the development of the Internet, the availability of various types of data\n(images, texts, sounds, videos, databases...) has increased, and structuring\nsuch data is a difficult task. We name these data, which may be structured or\nunstructured, \"complex data\". In this paper, we propose a new approach for\ncomplex data integration, based on a Multi-Agent System (MAS), in association\nto a data warehousing approach. Our objective is to take advantage of the MAS\nto perform the integration phase for complex data. We indeed consider the\ndifferent tasks of the data integration process as services offered by agents.\nTo validate this approach, we have actually developed an MAS for complex data\nintegration."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.2687v1", 
    "other_authors": "Kamel Aouiche, J\u00e9r\u00f4me Darmont, Le Gruenwald", 
    "title": "Frequent itemsets mining for database auto-administration", 
    "arxiv-id": "0809.2687v1", 
    "author": "Le Gruenwald", 
    "publish": "2008-09-16T11:44:39Z", 
    "summary": "With the wide development of databases in general and data warehouses in\nparticular, it is important to reduce the tasks that a database administrator\nmust perform manually. The aim of auto-administrative systems is to\nadministrate and adapt themselves automatically without loss (or even with a\ngain) in performance. The idea of using data mining techniques to extract\nuseful knowledge for administration from the data themselves has existed for\nsome years. However, little research has been achieved. This idea nevertheless\nremains a very promising approach, notably in the field of data warehousing,\nwhere queries are very heterogeneous and cannot be interpreted easily. The aim\nof this study is to search for a way of extracting useful knowledge from stored\ndata themselves to automatically apply performance optimization techniques, and\nmore particularly indexing techniques. We have designed a tool that extracts\nfrequent itemsets from a given workload to compute an index configuration that\nhelps optimizing data access time. The experiments we performed showed that the\nindex configurations generated by our tool allowed performance gains of 15% to\n25% on a test database and a test data warehouse."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.2688v1", 
    "other_authors": "J\u00e9r\u00f4me Darmont, Emerson Olivier", 
    "title": "A Complex Data Warehouse for Personalized, Anticipative Medicine", 
    "arxiv-id": "0809.2688v1", 
    "author": "Emerson Olivier", 
    "publish": "2008-09-16T11:47:58Z", 
    "summary": "With the growing use of new technologies, healthcare is nowadays undergoing\nsignificant changes. Information-based medicine has to exploit medical\ndecision-support systems and requires the analysis of various, heterogeneous\ndata, such as patient records, medical images, biological analysis results,\netc. In this paper, we present the design of the complex data warehouse\nrelating to high-level athletes. It is original in two ways. First, it is aimed\nat storing complex medical data. Second, it is designed to allow innovative and\nquite different kinds of analyses to support: (1) personalized and anticipative\nmedicine (in opposition to curative medicine) for well-identified patients; (2)\nbroad-band statistical studies over a given population of patients.\nFurthermore, the system includes data relating to several medical fields. It is\nalso designed to be evolutionary to take into account future advances in\nmedical research."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0809.2691v1", 
    "other_authors": "Marouane Hachicha, Hadj Mahboubi, J\u00e9r\u00f4me Darmont", 
    "title": "Expressing OLAP operators with the TAX XML algebra", 
    "arxiv-id": "0809.2691v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-09-16T12:12:15Z", 
    "summary": "With the rise of XML as a standard for representing business data, XML data\nwarehouses appear as suitable solutions for Web-based decision-support\napplications. In this context, it is necessary to allow OLAP analyses over XML\ndata cubes (XOLAP). Thus, XQuery extensions are needed. To help define a formal\nframework and allow much-needed performance optimizations on analytical queries\nexpressed in XQuery, having an algebra at one's disposal is desirable. However,\nXOLAP approaches and algebras from the literature still largely rely on the\nrelational model and/or only feature a small number of OLAP operators. In\nopposition, we propose in this paper to express a broad set of OLAP operators\nwith the TAX XML algebra."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0810.4809v1", 
    "other_authors": "T. Grust, M. Mayr, J. Rittinger", 
    "title": "XQuery Join Graph Isolation", 
    "arxiv-id": "0810.4809v1", 
    "author": "J. Rittinger", 
    "publish": "2008-10-27T13:41:48Z", 
    "summary": "A purely relational account of the true XQuery semantics can turn any\nrelational database system into an XQuery processor. Compiling nested\nexpressions of the fully compositional XQuery language, however, yields odd\nalgebraic plan shapes featuring scattered distributions of join operators that\ncurrently overwhelm commercial SQL query optimizers.\n  This work rewrites such plans before submission to the relational database\nback-end. Once cast into the shape of join graphs, we have found off-the-shelf\nrelational query optimizers--the B-tree indexing subsystem and join tree\nplanner, in particular--to cope and even be autonomously capable of\n\"reinventing\" advanced processing strategies that have originally been devised\nspecifically for the XQuery domain, e.g., XPath step reordering, axis reversal,\nand path stitching. Performance assessments provide evidence that relational\nquery engines are among the most versatile and efficient XQuery processors\nreadily available today."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0811.0741v1", 
    "other_authors": "Hadj Mahboubi, J\u00e9r\u00f4me Darmont", 
    "title": "Data Mining-based Fragmentation of XML Data Warehouses", 
    "arxiv-id": "0811.0741v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2008-11-05T15:00:32Z", 
    "summary": "With the multiplication of XML data sources, many XML data warehouse models\nhave been proposed to handle data heterogeneity and complexity in a way\nrelational data warehouses fail to achieve. However, XML-native database\nsystems currently suffer from limited performances, both in terms of manageable\ndata volume and response time. Fragmentation helps address both these issues.\nDerived horizontal fragmentation is typically used in relational data\nwarehouses and can definitely be adapted to the XML context. However, the\nnumber of fragments produced by classical algorithms is difficult to control.\nIn this paper, we propose the use of a k-means-based fragmentation approach\nthat allows to master the number of fragments through its $k$ parameter. We\nexperimentally compare its efficiency to classical derived horizontal\nfragmentation algorithms adapted to XML data warehouses and show its\nsuperiority."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0811.2117v1", 
    "other_authors": "Cristian Molinaro, Jan Chomicki, Jerzy Marcinkowski", 
    "title": "Disjunctive Databases for Representing Repairs", 
    "arxiv-id": "0811.2117v1", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2008-11-13T14:12:57Z", 
    "summary": "This paper addresses the problem of representing the set of repairs of a\npossibly inconsistent database by means of a disjunctive database.\nSpecifically, the class of denial constraints is considered. We show that,\ngiven a database and a set of denial constraints, there exists a (unique)\ndisjunctive database, called canonical, which represents the repairs of the\ndatabase w.r.t. the constraints and is contained in any other disjunctive\ndatabase with the same set of minimal models. We propose an algorithm for\ncomputing the canonical disjunctive database. Finally, we study the size of the\ncanonical disjunctive database in the presence of functional dependencies for\nboth repairs and cardinality-based repairs."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0811.2250v2", 
    "other_authors": "Xi Zhang, Jan Chomicki", 
    "title": "Semantics and Evaluation of Top-k Queries in Probabilistic Databases", 
    "arxiv-id": "0811.2250v2", 
    "author": "Jan Chomicki", 
    "publish": "2008-11-14T01:47:14Z", 
    "summary": "We study here fundamental issues involved in top-k query evaluation in\nprobabilistic databases. We consider simple probabilistic databases in which\nprobabilities are associated with individual tuples, and general probabilistic\ndatabases in which, additionally, exclusivity relationships between tuples can\nbe represented. In contrast to other recent research in this area, we do not\nlimit ourselves to injective scoring functions. We formulate three intuitive\npostulates that the semantics of top-k queries in probabilistic databases\nshould satisfy, and introduce a new semantics, Global-Topk, that satisfies\nthose postulates to a large degree. We also show how to evaluate queries under\nthe Global-Topk semantics. For simple databases we design dynamic-programming\nbased algorithms, and for general databases we show polynomial-time reductions\nto the simple cases. For example, we demonstrate that for a fixed k the time\ncomplexity of top-k query evaluation is as low as linear, under the assumption\nthat probabilistic databases are simple and scoring functions are injective."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0811.3691v1", 
    "other_authors": "Leticia Gomez, Bart Kuijpers, Alejandro Vaisman", 
    "title": "Temporal Support of Regular Expressions in Sequential Pattern Mining", 
    "arxiv-id": "0811.3691v1", 
    "author": "Alejandro Vaisman", 
    "publish": "2008-11-22T15:22:40Z", 
    "summary": "Classic algorithms for sequential pattern discovery, return all frequent\nsequences present in a database, but, in general, only a few ones are\ninteresting for the user. Languages based on regular expressions (RE) have been\nproposed to restrict frequent sequences to the ones that satisfy user-specified\nconstraints. Although the support of a sequence is computed as the number of\ndata-sequences satisfying a pattern with respect to the total number of\ndata-sequences in the database, once regular expressions come into play, new\napproaches to the concept of support are needed. For example, users may be\ninterested in computing the support of the RE as a whole, in addition to the\none of a particular pattern. Also, when the items are frequently updated, the\ntraditional way of counting support in sequential pattern mining may lead to\nincorrect (or, at least incomplete), conclusions. The problem gets more\ninvolved if we are interested in categorical sequential patterns. In light of\nthe above, in this paper we propose to revise the classic notion of support in\nsequential pattern mining, introducing the concept of temporal support of\nregular expressions, intuitively defined as the number of sequences satisfying\na target pattern, out of the total number of sequences that could have possibly\nmatched such pattern, where the pattern is defined as a RE over complex items\n(i.e., not only item identifiers, but also attributes and functions)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.2049v1", 
    "other_authors": "Jian Li, Amol Deshpande", 
    "title": "Consensus Answers for Queries over Probabilistic Databases", 
    "arxiv-id": "0812.2049v1", 
    "author": "Amol Deshpande", 
    "publish": "2008-12-10T23:20:17Z", 
    "summary": "We address the problem of finding a \"best\" deterministic query answer to a\nquery over a probabilistic database. For this purpose, we propose the notion of\na consensus world (or a consensus answer) which is a deterministic world\n(answer) that minimizes the expected distance to the possible worlds (answers).\nThis problem can be seen as a generalization of the well-studied inconsistent\ninformation aggregation problems (e.g. rank aggregation) to probabilistic\ndatabases. We consider this problem for various types of queries including SPJ\nqueries, \\Topk queries, group-by aggregate queries, and clustering. For\ndifferent distance metrics, we obtain polynomial time optimal or approximation\nalgorithms for computing the consensus answers (or prove NP-hardness). Most of\nour results are for a general probabilistic database model, called {\\em and/xor\ntree model}, which significantly generalizes previous probabilistic database\nmodels like x-tuples and block-independent disjoint models, and is of\nindependent interest."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.2195v3", 
    "other_authors": "Rada Chirkova, Michael Genesereth", 
    "title": "Equivalence of SQL Queries in Presence of Embedded Dependencies", 
    "arxiv-id": "0812.2195v3", 
    "author": "Michael Genesereth", 
    "publish": "2008-12-11T17:25:55Z", 
    "summary": "We consider the problem of finding equivalent minimal-size reformulations of\nSQL queries in presence of embedded dependencies [1]. Our focus is on\nselect-project-join (SPJ) queries with equality comparisons, also known as safe\nconjunctive (CQ) queries, possibly with grouping and aggregation. For SPJ\nqueries, the semantics of the SQL standard treat query answers as multisets\n(a.k.a. bags), whereas the stored relations may be treated either as sets,\nwhich is called bag-set semantics for query evaluation, or as bags, which is\ncalled bag semantics. (Under set semantics, both query answers and stored\nrelations are treated as sets.)\n  In the context of the above Query-Reformulation Problem, we develop a\ncomprehensive framework for equivalence of CQ queries under bag and bag-set\nsemantics in presence of embedded dependencies, and make a number of conceptual\nand technical contributions. Specifically, we develop equivalence tests for CQ\nqueries in presence of arbitrary sets of embedded dependencies under bag and\nbag-set semantics, under the condition that chase [9] under set semantics\n(set-chase) on the inputs terminates. We also present equivalence tests for\naggregate CQ queries in presence of embedded dependencies. We use our\nequivalence tests to develop sound and complete (whenever set-chase on the\ninputs terminates) algorithms for solving instances of the Query-Reformulation\nProblem with CQ queries under each of bag and bag-set semantics, as well as for\ninstances of the problem with aggregate queries."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.2874v1", 
    "other_authors": "Andrew Branson, Tamas Hauer, Richard McClatchey, Dmitry Rogulin, Jetendr Shamdasani", 
    "title": "A Data Model for Integrating Heterogeneous Medical Data in the   Health-e-Child Project", 
    "arxiv-id": "0812.2874v1", 
    "author": "Jetendr Shamdasani", 
    "publish": "2008-12-15T18:25:51Z", 
    "summary": "There has been much research activity in recent times about providing the\ndata infrastructures needed for the provision of personalised healthcare. In\nparticular the requirement of integrating multiple, potentially distributed,\nheterogeneous data sources in the medical domain for the use of clinicians has\nset challenging goals for the healthgrid community. The approach advocated in\nthis paper surrounds the provision of an Integrated Data Model plus links\nto/from ontologies to homogenize biomedical (from genomic, through cellular,\ndisease, patient and population-related) data in the context of the EC\nFramework 6 Health-e-Child project. Clinical requirements are identified, the\ndesign approach in constructing the model is detailed and the integrated model\ndescribed in the context of examples taken from that project. Pointers are\ngiven to future work relating the model to medical ontologies and challenges to\nthe use of fully integrated models and ontologies are identified."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.2879v1", 
    "other_authors": "Kamran Munir, Mohammed Odeh, Richard McClatchey", 
    "title": "Ontology Assisted Query Reformulation Using Semantic and Assertion   Capabilities of OWL-DL Ontologies", 
    "arxiv-id": "0812.2879v1", 
    "author": "Richard McClatchey", 
    "publish": "2008-12-15T18:34:44Z", 
    "summary": "End users of recent biomedical information systems are often unaware of the\nstorage structure and access mechanisms of the underlying data sources and can\nrequire simplified mechanisms for writing domain specific complex queries. This\nresearch aims to assist users and their applications in formulating queries\nwithout requiring complete knowledge of the information structure of underlying\ndata sources. To achieve this, query reformulation techniques and algorithms\nhave been developed that can interpret ontology-based search criteria and\nassociated domain knowledge in order to reformulate a relational query. These\nquery reformulation algorithms exploit the semantic relationships and assertion\ncapabilities of OWL-DL based domain ontologies for query reformulation. In this\npaper, this approach is applied to the integrated database schema of the EU\nfunded Health-e-Child (HeC) project with the aim of providing ontology assisted\nquery reformulation techniques to simplify the global access that is needed to\nmillions of medical records across the UK and Europe."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.3715v1", 
    "other_authors": "Aur\u00e9lie Bissay, Philippe Pernelle, Arnaud Lefebvre, Abdelaziz Bouras", 
    "title": "Business processes integration and performance indicators in a PLM", 
    "arxiv-id": "0812.3715v1", 
    "author": "Abdelaziz Bouras", 
    "publish": "2008-12-19T07:52:46Z", 
    "summary": "In an economic environment more and more competitive, the effective\nmanagement of information and knowledge is a strategic issue for industrial\nenterprises. In the global marketplace, companies must use reactive strategies\nand reduce their products development cycle. In this context, the PLM (Product\nLifecycle Management) is considered as a key component of the information\nsystem. The aim of this paper is to present an approach to integrate Business\nProcesses in a PLM system. This approach is implemented in automotive sector\nwith second-tier subcontractor"
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.3788v2", 
    "other_authors": "Michael Schmidt, Michael Meier, Georg Lausen", 
    "title": "Foundations of SPARQL Query Optimization", 
    "arxiv-id": "0812.3788v2", 
    "author": "Georg Lausen", 
    "publish": "2008-12-19T13:51:57Z", 
    "summary": "The SPARQL query language is a recent W3C standard for processing RDF data, a\nformat that has been developed to encode information in a machine-readable way.\nWe investigate the foundations of SPARQL query optimization and (a) provide\nnovel complexity results for the SPARQL evaluation problem, showing that the\nmain source of complexity is operator OPTIONAL alone; (b) propose a\ncomprehensive set of algebraic query rewriting rules; (c) present a framework\nfor constraint-based SPARQL optimization based upon the well-known chase\nprocedure for Conjunctive Query minimization. In this line, we develop two\nnovel termination conditions for the chase. They subsume the strongest\nconditions known so far and do not increase the complexity of the recognition\nproblem, thus making a larger class of both Conjunctive and SPARQL queries\namenable to constraint-based optimization. Our results are of immediate\npractical interest and might empower any SPARQL query optimizer."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0812.4986v1", 
    "other_authors": "Albrecht Schmidt", 
    "title": "An Array Algebra", 
    "arxiv-id": "0812.4986v1", 
    "author": "Albrecht Schmidt", 
    "publish": "2008-12-29T23:14:00Z", 
    "summary": "This is a proposal of an algebra which aims at distributed array processing.\nThe focus lies on re-arranging and distributing array data, which may be\nmulti-dimensional. The context of the work is scientific processing; thus, the\ncore science operations are assumed to be taken care of in external libraries\nor languages. A main design driver is the desire to carry over some of the\nstrategies of the relational algebra into the array domain."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2008.4497432", 
    "link": "http://arxiv.org/pdf/0901.2224v4", 
    "other_authors": "Alexandr Savinov", 
    "title": "Concept-Oriented Model and Query Language", 
    "arxiv-id": "0901.2224v4", 
    "author": "Alexandr Savinov", 
    "publish": "2009-01-15T10:41:28Z", 
    "summary": "We describe a new approach to data modeling, called the concept-oriented\nmodel (COM), and a novel concept-oriented query language (COQL). The model is\nbased on three principles: duality principle postulates that any element is a\ncouple consisting of one identity and one entity, inclusion principle\npostulates that any element has a super-element, and order principle assumes\nthat any element has a number of greater elements within a partially ordered\nset. Concept-oriented query language is based on a new data modeling construct,\ncalled concept, inclusion relation between concepts, and concept partial\nordering in which greater concepts are represented by their field types. It is\ndemonstrated how COM and COQL can be used to solve three general data modeling\ntasks: logical navigation, multidimensional analysis and inference. Logical\nnavigation is based on two operations of projection and de-projection.\nMultidimensional analysis uses product operation for producing a cube from\nlevel concepts chosen along the chosen dimension paths. Inference is defined as\na two-step procedure where input constraints are first propagated downwards\nusing de-projection and then the constrained result is propagated upwards using\nprojection."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2009.08.006", 
    "link": "http://arxiv.org/pdf/0901.3751v7", 
    "other_authors": "Daniel Lemire, Owen Kaser, Kamel Aouiche", 
    "title": "Sorting improves word-aligned bitmap indexes", 
    "arxiv-id": "0901.3751v7", 
    "author": "Kamel Aouiche", 
    "publish": "2009-01-23T19:01:06Z", 
    "summary": "Bitmap indexes must be compressed to reduce input/output costs and minimize\nCPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use\ntechniques based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. These techniques are sensitive to the order of the rows: a\nsimple lexicographical sort can divide the index size by 9 and make indexes\nseveral times faster. We investigate row-reordering heuristics. Simply\npermuting the columns of the table can increase the sorting efficiency by 40%.\nSecondary contributions include efficient algorithms to construct and aggregate\nbitmaps. The effect of word length is also reviewed by constructing 16-bit,\n32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are\nslightly faster than 32-bit indexes despite being nearly twice as large."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2009.08.006", 
    "link": "http://arxiv.org/pdf/0901.3984v2", 
    "other_authors": "Michael Meier, Michael Schmidt, Georg Lausen", 
    "title": "Stop the Chase", 
    "arxiv-id": "0901.3984v2", 
    "author": "Georg Lausen", 
    "publish": "2009-01-26T12:09:38Z", 
    "summary": "The chase procedure, an algorithm proposed 25+ years ago to fix constraint\nviolations in database instances, has been successfully applied in a variety of\ncontexts, such as query optimization, data exchange, and data integration. Its\npracticability, however, is limited by the fact that - for an arbitrary set of\nconstraints - it might not terminate; even worse, chase termination is an\nundecidable problem in general. In response, the database community has\nproposed sufficient restrictions on top of the constraints that guarantee chase\ntermination on any database instance. In this paper, we propose a novel\nsufficient termination condition, called inductive restriction, which strictly\ngeneralizes previous conditions, but can be checked as efficiently.\nFurthermore, we motivate and study the problem of data-dependent chase\ntermination and, as a key result, present sufficient termination conditions\nw.r.t. fixed instances. They are strictly more general than inductive\nrestriction and might guarantee termination although the chase does not\nterminate in the general case."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2009.08.006", 
    "link": "http://arxiv.org/pdf/0902.2504v1", 
    "other_authors": "Richard Molyneux", 
    "title": "Hyperset Approach to Semi-structured Databases and the Experimental   Implementation of the Query Language Delta", 
    "arxiv-id": "0902.2504v1", 
    "author": "Richard Molyneux", 
    "publish": "2009-02-15T18:53:19Z", 
    "summary": "This thesis presents practical suggestions towards the implementation of the\nhyperset approach to semi-structured databases and the associated query\nlanguage Delta. This work can be characterised as part of a top-down approach\nto semi-structured databases, from theory to practice. The main original part\nof this work consisted in implementation of the hyperset Delta query language\nto semi-structured databases, including worked example queries. In fact, the\ngoal was to demonstrate the practical details of this approach and language.\nThe required development of an extended, practical version of the language\nbased on the existing theoretical version, and the corresponding operational\nsemantics. Here we present detailed description of the most essential steps of\nthe implementation. Another crucial problem for this approach was to\ndemonstrate how to deal in reality with the concept of the equality relation\nbetween (hyper)sets, which is computationally realised by the bisimulation\nrelation. In fact, this expensive procedure, especially in the case of\ndistributed semi-structured data, required some additional theoretical\nconsiderations and practical suggestions for efficient implementation. To this\nend the 'local/global' strategy for computing the bisimulation relation over\ndistributed semi-structured data was developed and its efficiency was\nexperimentally confirmed."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2009.08.006", 
    "link": "http://arxiv.org/pdf/0902.4535v1", 
    "other_authors": "Simona Angela Apostol, Cosmin Catu, Corina Vernic", 
    "title": "Electronical Health Record's Systems. Interoperability", 
    "arxiv-id": "0902.4535v1", 
    "author": "Corina Vernic", 
    "publish": "2009-02-26T09:06:29Z", 
    "summary": "Understanding the importance that the electronic medical health records\nsystem has, with its various structural types and grades, has led to the\nelaboration of a series of standards and quality control methods, meant to\ncontrol its functioning. In time, the electronic health records system has\nevolved along with the medical data change of structure. Romania has not yet\nmanaged to fully clarify this concept, various definitions still being\nencountered, such as \"Patient's electronic chart\", \"Electronic health file\". A\nslow change from functional interoperability (OSI level 6) to semantic\ninteroperability (level 7) is being aimed at the moment. This current article\nwill try to present the main electronic files models, from a functional\ninteroperability system's possibility to be created perspective."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2009.08.006", 
    "link": "http://arxiv.org/pdf/0905.1755v1", 
    "other_authors": "Raymond Chi-Wing Wong, Ada Wai-Chee Fu, Ke Wang, Yabo Xu, Philip S. Yu", 
    "title": "Can the Utility of Anonymized Data be used for Privacy Breaches?", 
    "arxiv-id": "0905.1755v1", 
    "author": "Philip S. Yu", 
    "publish": "2009-05-12T03:36:26Z", 
    "summary": "Group based anonymization is the most widely studied approach for privacy\npreserving data publishing. This includes k-anonymity, l-diversity, and\nt-closeness, to name a few. The goal of this paper is to raise a fundamental\nissue on the privacy exposure of the current group based approach. This has\nbeen overlooked in the past. The group based anonymization approach basically\nhides each individual record behind a group to preserve data privacy. If not\nproperly anonymized, patterns can actually be derived from the published data\nand be used by the adversary to breach individual privacy. For example, from\nthe medical records released, if patterns such as people from certain countries\nrarely suffer from some disease can be derived, then the information can be\nused to imply linkage of other people in an anonymized group with this disease\nwith higher likelihood. We call the derived patterns from the published data\nthe foreground knowledge. This is in contrast to the background knowledge that\nthe adversary may obtain from other channels as studied in some previous work.\nFinally, we show by experiments that the attack is realistic in the privacy\nbenchmark dataset under the traditional group based anonymization approach."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-01344-7_5", 
    "link": "http://arxiv.org/pdf/0905.2657v2", 
    "other_authors": "Kamel Aouiche, Daniel Lemire, Robert Godin", 
    "title": "Web 2.0 OLAP: From Data Cubes to Tag Clouds", 
    "arxiv-id": "0905.2657v2", 
    "author": "Robert Godin", 
    "publish": "2009-05-16T05:57:06Z", 
    "summary": "Increasingly, business projects are ephemeral. New Business Intelligence\ntools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds\nare a popular community-driven visualization technique. Hence, we investigate\ntag-cloud views with support for OLAP operations such as roll-ups, slices,\ndices, clustering, and drill-downs. As a case study, we implemented an\napplication where users can upload data and immediately navigate through its ad\nhoc dimensions. To support social networking, views can be easily shared and\nembedded in other Web sites. Algorithmically, our tag-cloud views are\napproximate range top-k queries over spontaneous data cubes. We present\nexperimental evidence that iceberg cuboids provide adequate online\napproximations. We benchmark several browser-oblivious tag-cloud layout\noptimizations."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-01344-7_5", 
    "link": "http://arxiv.org/pdf/0905.4605v1", 
    "other_authors": "Ovidiu Crista", 
    "title": "Techniques for Securing Data Exchange between a Database Server and a   Client Program", 
    "arxiv-id": "0905.4605v1", 
    "author": "Ovidiu Crista", 
    "publish": "2009-05-28T10:29:20Z", 
    "summary": "The goal of the presented work is to illustrate a method by which the data\nexchange between a standalone computer software and a shared database server\ncan be protected of unauthorized interceptation of the traffic in Internet\nnetwork, a transport network for data managed by those two systems,\ninterceptation by which an attacker could gain illegetimate access to the\ndatabase, threatening this way the data integrity and compromising the\ndatabase."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-03555-5_8", 
    "link": "http://arxiv.org/pdf/0905.4761v1", 
    "other_authors": "Gregory Leighton, Denilson Barbosa", 
    "title": "Optimizing XML Compression", 
    "arxiv-id": "0905.4761v1", 
    "author": "Denilson Barbosa", 
    "publish": "2009-05-28T22:36:57Z", 
    "summary": "The eXtensible Markup Language (XML) provides a powerful and flexible means\nof encoding and exchanging data. As it turns out, its main advantage as an\nencoding format (namely, its requirement that all open and close markup tags\nare present and properly balanced) yield also one of its main disadvantages:\nverbosity. XML-conscious compression techniques seek to overcome this drawback.\nMany of these techniques first separate XML structure from the document\ncontent, and then compress each independently. Further compression gains can be\nrealized by identifying and compressing together document content that is\nhighly similar, thereby amortizing the storage costs of auxiliary information\nrequired by the chosen compression algorithm. Additionally, the proper choice\nof compression algorithm is an important factor not only for the achievable\ncompression gain, but also for access performance. Hence, choosing a\ncompression configuration that optimizes compression gain requires one to\ndetermine (1) a partitioning strategy for document content, and (2) the best\navailable compression algorithm to apply to each set within this partition. In\nthis paper, we show that finding an optimal compression configuration with\nrespect to compression gain is an NP-hard optimization problem. This problem\nremains intractable even if one considers a single compression algorithm for\nall content. We also describe an approximation algorithm for selecting a\npartitioning strategy for document content based on the branch-and-bound\nparadigm."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0906.0252v1", 
    "other_authors": "Jeong-Hoon Lee, Kyu-Young Whang, Hyo-Sang Lim, Byung-Suk Lee, Jun-Seok Heo", 
    "title": "Progressive Processing of Continuous Range Queries in Hierarchical   Wireless Sensor Networks", 
    "arxiv-id": "0906.0252v1", 
    "author": "Jun-Seok Heo", 
    "publish": "2009-06-01T11:04:04Z", 
    "summary": "In this paper, we study the problem of processing continuous range queries in\na hierarchical wireless sensor network. Contrasted with the traditional\napproach of building networks in a \"flat\" structure using sensor devices of the\nsame capability, the hierarchical approach deploys devices of higher capability\nin a higher tier, i.e., a tier closer to the server. While query processing in\nflat sensor networks has been widely studied, the study on query processing in\nhierarchical sensor networks has been inadequate. In wireless sensor networks,\nthe main costs that should be considered are the energy for sending data and\nthe storage for storing queries. There is a trade-off between these two costs.\nBased on this, we first propose a progressive processing method that\neffectively processes a large number of continuous range queries in\nhierarchical sensor networks. The proposed method uses the query merging\ntechnique proposed by Xiang et al. as the basis and additionally considers the\ntrade-off between the two costs. More specifically, it works toward reducing\nthe storage cost at lower-tier nodes by merging more queries, and toward\nreducing the energy cost at higher-tier nodes by merging fewer queries (thereby\nreducing \"false alarms\"). We then present how to build a hierarchical sensor\nnetwork that is optimal with respect to the weighted sum of the two costs. It\nallows for a cost-based systematic control of the trade-off based on the\nrelative importance between the storage and energy in a given network\nenvironment and application. Experimental results show that the proposed method\nachieves a near-optimal control between the storage and energy and reduces the\ncost by 0.989~84.995 times compared with the cost achieved using the flat\n(i.e., non-hierarchical) setup as in the work by Xiang et al."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0906.4327v1", 
    "other_authors": "Jigyasa Bisaria, Namita Shrivastava, K. R. Pardasani", 
    "title": "A Rough Sets Partitioning Model for Mining Sequential Patterns with Time   Constraint", 
    "arxiv-id": "0906.4327v1", 
    "author": "K. R. Pardasani", 
    "publish": "2009-06-23T18:29:06Z", 
    "summary": "Now a days, data mining and knowledge discovery methods are applied to a\nvariety of enterprise and engineering disciplines to uncover interesting\npatterns from databases. The study of Sequential patterns is an important data\nmining problem due to its wide applications to real world time dependent\ndatabases. Sequential patterns are inter-event patterns ordered over a\ntime-period associated with specific objects under study. Analysis and\ndiscovery of frequent sequential patterns over a predetermined time-period are\ninteresting data mining results, and can aid in decision support in many\nenterprise applications. The problem of sequential pattern mining poses\ncomputational challenges as a long frequent sequence contains enormous number\nof frequent subsequences. Also useful results depend on the right choice of\nevent window. In this paper, we have studied the problem of sequential pattern\nmining through two perspectives, one the computational aspect of the problem\nand the other is incorporation and adjustability of time constraint. We have\nused Indiscernibility relation from theory of rough sets to partition the\nsearch space of sequential patterns and have proposed a novel algorithm that\nallows previsualization of patterns and allows adjustment of time constraint\nprior to execution of mining task. The algorithm Rough Set Partitioning is at\nleast ten times faster than the naive time constraint based sequential pattern\nmining algorithm GSP. Besides this an additional knowledge of time interval of\nsequential patterns is also determined with the method."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0906.4927v1", 
    "other_authors": "Lijun Chang, Jeffrey Xu Yu, Lu Qin", 
    "title": "Fast Probabilistic Ranking under x-Relation Model", 
    "arxiv-id": "0906.4927v1", 
    "author": "Lu Qin", 
    "publish": "2009-06-26T13:24:57Z", 
    "summary": "The probabilistic top-k queries based on the interplay of score and\nprobability, under the possible worlds semantic, become an important research\nissue that considers both score and uncertainty on the same basis. In the\nliterature, many different probabilistic top-k queries are proposed. Almost all\nof them need to compute the probability of a tuple t_i to be ranked at the j-th\nposition across the entire set of possible worlds. The cost of such computing\nis the dominant cost and is known as O(kn^2), where n is the size of dataset.\nIn this paper, we propose a new novel algorithm that computes such probability\nin O(kn)."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0907.3183v2", 
    "other_authors": "Nedyalko Borisov, Shivnath Babu, Sandeep Uttamchandani, Ramani Routray, Aameek Singh", 
    "title": "Why Did My Query Slow Down?", 
    "arxiv-id": "0907.3183v2", 
    "author": "Aameek Singh", 
    "publish": "2009-07-18T06:37:37Z", 
    "summary": "Many enterprise environments have databases running on network-attached\nserver-storage infrastructure (referred to as Storage Area Networks or SANs).\nBoth the database and the SAN are complex systems that need their own separate\nadministrative teams. This paper puts forth the vision of an innovative\nmanagement framework to simplify administrative tasks that require an in-depth\nunderstanding of both the database and the SAN. As a concrete instance, we\nconsider the task of diagnosing the slowdown in performance of a database query\nthat is executed multiple times (e.g., in a periodic report-generation\nsetting). This task is very challenging because the space of possible causes\nincludes problems specific to the database, problems specific to the SAN, and\nproblems that arise due to interactions between the two systems. In addition,\nthe monitoring data available from these systems can be noisy.\n  We describe the design of DIADS which is an integrated diagnosis tool for\ndatabase and SAN administrators. DIADS generates and uses a powerful\nabstraction called Annotated Plan Graphs (APGs) that ties together the\nexecution path of queries in the database and the SAN. Using an innovative\nworkflow that combines domain-specific knowledge with machine-learning\ntechniques, DIADS was applied successfully to diagnose query slowdowns caused\nby complex combinations of events across a PostgreSQL database and a production\nSAN."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0908.0464v3", 
    "other_authors": "Slawomir Staworko, Jan Chomicki, Jerzy Marcinkowski", 
    "title": "Prioritized Repairing and Consistent Query Answering in Relational   Databases", 
    "arxiv-id": "0908.0464v3", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2009-08-04T15:10:25Z", 
    "summary": "A consistent query answer in an inconsistent database is an answer obtained\nin every (minimal) repair. The repairs are obtained by resolving all conflicts\nin all possible ways. Often, however, the user is able to provide a preference\non how conflicts should be resolved. We investigate here the framework of\npreferred consistent query answers, in which user preferences are used to\nnarrow down the set of repairs to a set of preferred repairs. We axiomatize\ndesirable properties of preferred repairs. We present three different families\nof preferred repairs and study their mutual relationships. Finally, we\ninvestigate the complexity of preferred repairing and computing preferred\nconsistent query answers."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0908.3957v1", 
    "other_authors": "Hadj Mahboubi, J\u00e9r\u00f4me Darmont", 
    "title": "Enhancing XML Data Warehouse Query Performance by Fragmentation", 
    "arxiv-id": "0908.3957v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2009-08-27T09:12:08Z", 
    "summary": "XML data warehouses form an interesting basis for decision-support\napplications that exploit heterogeneous data from multiple sources. However,\nXML-native database systems currently suffer from limited performances in terms\nof manageable data volume and response time for complex analytical queries.\nFragmenting and distributing XML data warehouses (e.g., on data grids) allow to\naddress both these issues. In this paper, we work on XML warehouse\nfragmentation. In relational data warehouses, several studies recommend the use\nof derived horizontal fragmentation. Hence, we propose to adapt it to the XML\ncontext. We particularly focus on the initial horizontal fragmentation of\ndimensions' XML documents and exploit two alternative algorithms. We\nexperimentally validate our proposal and compare these alternatives with\nrespect to a unified XML warehouse model we advocate for."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0910.3372v4", 
    "other_authors": "Marcelo Arenas, Jorge Perez, Juan Reutter, Cristian Riveros", 
    "title": "Composition and Inversion of Schema Mappings", 
    "arxiv-id": "0910.3372v4", 
    "author": "Cristian Riveros", 
    "publish": "2009-10-18T13:46:36Z", 
    "summary": "In the recent years, a lot of attention has been paid to the development of\nsolid foundations for the composition and inversion of schema mappings. In this\npaper, we review the proposals for the semantics of these crucial operators.\nFor each of these proposals, we concentrate on the three following problems:\nthe definition of the semantics of the operator, the language needed to express\nthe operator, and the algorithmic issues associated to the problem of computing\nthe operator. It should be pointed out that we primarily consider the\nformalization of schema mappings introduced in the work on data exchange. In\nparticular, when studying the problem of computing the composition and inverse\nof a schema mapping, we will be mostly interested in computing these operators\nfor mappings specified by source-to-target tuple-generating dependencies."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0911.0508v1", 
    "other_authors": "Ravindra Guravannavar", 
    "title": "Optimization and Evaluation of Nested Queries and Procedures", 
    "arxiv-id": "0911.0508v1", 
    "author": "Ravindra Guravannavar", 
    "publish": "2009-11-03T06:25:23Z", 
    "summary": "Many database applications perform complex data retrieval and update tasks.\nNested queries, and queries that invoke user-defined functions, which are\nwritten using a mix of procedural and SQL constructs, are often used in such\napplications. A straight-forward evaluation of such queries involves repeated\nexecution of parameterized sub-queries or blocks containing queries and\nprocedural code.\n  An important problem that arises while optimizing nested queries as well as\nqueries with joins, aggregates and set operations is the problem of finding an\noptimal sort order from a factorial number of possible sort orders. We show\nthat even a special case of this problem is NP-Hard, and present practical\nheuristics that are effective and easy to incorporate in existing query\noptimizers.\n  We also consider iterative execution of queries and updates inside complex\nprocedural blocks such as user-defined functions and stored procedures.\nParameter batching is an important means of improving performance as it enables\nset-orientated processing. The key challenge to parameter batching lies in\nrewriting a given procedure/function to process a batch of parameter values. We\npropose a solution, based on program analysis and rewrite rules, to automate\nthe generation of batched forms of procedures and replace iterative database\ncalls within imperative loops with a single call to the batched form.\n  We present experimental results for the proposed techniques, and the results\nshow significant gains in performance."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0911.3600v1", 
    "other_authors": "P. De Meo, G. Quattrone, G. Terracina, D. Ursino", 
    "title": "\"Almost automatic\" and semantic integration of XML Schemas at various   \"severity\" levels", 
    "arxiv-id": "0911.3600v1", 
    "author": "D. Ursino", 
    "publish": "2009-11-18T16:59:38Z", 
    "summary": "This paper presents a novel approach for the integration of a set of XML\nSchemas. The proposed approach is specialized for XML, is almost automatic,\nsemantic and \"light\". As a further, original, peculiarity, it is parametric\nw.r.t. a \"severity\" level against which the integration task is performed. The\npaper describes the approach in all details, illustrates various theoretical\nresults, presents the experiments we have performed for testing it and,\nfinally, compares it with various related approaches already proposed in the\nliterature."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0911.4329v2", 
    "other_authors": "Ki-Hoon Lee, Kyu-Young Whang, Wook-Shin Han, Min-Soo Kim", 
    "title": "Structural Consistency: Enabling XML Keyword Search to Eliminate   Spurious Results Consistently", 
    "arxiv-id": "0911.4329v2", 
    "author": "Min-Soo Kim", 
    "publish": "2009-11-23T06:45:37Z", 
    "summary": "XML keyword search is a user-friendly way to query XML data using only\nkeywords. In XML keyword search, to achieve high precision without sacrificing\nrecall, it is important to remove spurious results not intended by the user.\nEfforts to eliminate spurious results have enjoyed some success by using the\nconcepts of LCA or its variants, SLCA and MLCA. However, existing methods still\ncould find many spurious results. The fundamental cause for the occurrence of\nspurious results is that the existing methods try to eliminate spurious results\nlocally without global examination of all the query results and, accordingly,\nsome spurious results are not consistently eliminated. In this paper, we\npropose a novel keyword search method that removes spurious results\nconsistently by exploiting the new concept of structural consistency."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.0579v1", 
    "other_authors": "Mohammad Ghulam Ali", 
    "title": "A Multidatabase System as 4-Tiered Client-Server Distributed   Heterogeneous Database System", 
    "arxiv-id": "0912.0579v1", 
    "author": "Mohammad Ghulam Ali", 
    "publish": "2009-12-03T05:34:56Z", 
    "summary": "In this paper, we describe a multidatabase system as 4tiered Client-Server\nDBMS architectures. We discuss their functional components and provide an\noverview of their performance characteristics. The first component of this\nproposed system is a web based interface or Graphical User Interface, which\nresides on top of the Client Application Program, the second component of the\nsystem is a client Application program running in an application server, which\nresides on top of the Global Database Management System, the third component of\nthe system is a Global Database Management System and global schema of the\nmultidatabase system server, which resides on top of the distributed\nheterogeneous local component database system servers, and the fourth component\nis remote heterogeneous local component database system servers. Transaction\nsubmitted from client interface to a multidatabase system server through an\napplication server will be decomposed into a set of sub queries and will be\nexecuted at various remote heterogeneous local component database servers and\nalso in case of information retrieval all sub queries will be composed and will\nget back results to the end users."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.0603v1", 
    "other_authors": "Mohammad Ghulam Ali", 
    "title": "Object Oriented Approach for Integration of Heterogeneous Databases in a   Multidatabase System and Local Schemas Modifications Propagation", 
    "arxiv-id": "0912.0603v1", 
    "author": "Mohammad Ghulam Ali", 
    "publish": "2009-12-03T09:01:39Z", 
    "summary": "One of the challenging problems in the multidatabase systems is to find the\nmost viable solution to the problem of interoperability of distributed\nheterogeneous autonomous local component databases. This has resulted in the\ncreation of a global schema over set of these local component database schemas\nto provide a uniform representation of local schemas. The aim of this paper is\nto use object oriented approach to integrate schemas of distributed\nheterogeneous autonomous local component database schemas into a global schema.\nThe resulting global schema provides a uniform interface and high level of\nlocation transparency for retrieval of data from the local component databases.\nA set of integration operators are defined to integrate local schemas based on\nthe semantic relevance of their classes and to provide a model independent\nrepresentation of virtual classes of the global schema. The schematic\nrepresentation and heterogeneity is also taken into account in the integration\nprocess. Justifications about Object Oriented Modal are also discussed. Bottom\nup local schema modifications propagation in Global schema is also considered\nto maintain Global schema as local schemas are autonomous and evolve over time.\nAn example illustrates the applicability of the integration operator defined."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.1016v1", 
    "other_authors": "Ayeesha Dsousa, Shalini Bhatia", 
    "title": "Refactoring of a Database", 
    "arxiv-id": "0912.1016v1", 
    "author": "Shalini Bhatia", 
    "publish": "2009-12-05T13:21:57Z", 
    "summary": "The technique of database refactoring is all about applying disciplined and\ncontrolled techniques to change an existing database schema. The problem is to\nsuccessfully create a Database Refactoring Framework for databases. This paper\nconcentrates on the feasibility of adapting this concept to work as a generic\ntemplate. To retain the constraints regardless of the modifications to the\nmetadata, the paper proposes a MetaData Manipulation Tool to facilitate change.\nThe tool adopts a Template Design Pattern to make it database independent. The\npaper presents a drawback of using java for constraint extraction and proposes\nan alternative."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.1110v1", 
    "other_authors": "Serge Boucher, Boris Verhaegen, Esteban Zim\u00e1nyi", 
    "title": "XML Multidimensional Modelling and Querying", 
    "arxiv-id": "0912.1110v1", 
    "author": "Esteban Zim\u00e1nyi", 
    "publish": "2009-12-06T15:26:43Z", 
    "summary": "As XML becomes ubiquitous and XML storage and processing becomes more\nefficient, the range of use cases for these technologies widens daily. One\npromising area is the integration of XML and data warehouses, where an\nXML-native database stores multidimensional data and processes OLAP queries\nwritten in the XQuery interrogation language. This paper explores issues\narising in the implementation of such a data warehouse. We first compare\napproaches for multidimensional data modelling in XML, then describe how\ntypical OLAP queries on these models can be expressed in XQuery. We then show\nhow, regardless of the model, the grouping features of XQuery 1.1 improve\nperformance and readability of these queries. Finally, we evaluate the\nperformance of query evaluation in each modelling choice using the eXist\ndatabase, which we extended with a grouping clause implementation."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.3924v1", 
    "other_authors": "M. Ramaswami, R. Bhaskaran", 
    "title": "A Study on Feature Selection Techniques in Educational Data Mining", 
    "arxiv-id": "0912.3924v1", 
    "author": "R. Bhaskaran", 
    "publish": "2009-12-19T18:51:40Z", 
    "summary": "Educational data mining (EDM) is a new growing research area and the essence\nof data mining concepts are used in the educational field for the purpose of\nextracting useful information on the behaviors of students in the learning\nprocess. In this EDM, feature selection is to be made for the generation of\nsubset of candidate variables. As the feature selection influences the\npredictive accuracy of any performance model, it is essential to study\nelaborately the effectiveness of student performance model in connection with\nfeature selection techniques. In this connection, the present study is devoted\nnot only to investigate the most relevant subset features with minimum\ncardinality for achieving high predictive performance by adopting various\nfiltered feature selection techniques in data mining but also to evaluate the\ngoodness of subsets with different cardinalities and the quality of six\nfiltered feature selection algorithms in terms of F-measure value and Receiver\nOperating Characteristics (ROC) value, generated by the NaiveBayes algorithm as\nbase-line classifier method. The comparative study carried out by us on six\nfilter feature section algorithms reveals the best method, as well as optimal\ndimensionality of the feature subset. Benchmarking of filter feature selection\nmethod is subsequently carried out by deploying different classifier models.\nThe result of the present study effectively supports the well known fact of\nincrease in the predictive accuracy with the existence of minimum number of\nfeatures. The expected outcomes show a reduction in computational time and\nconstructional cost in both training and classification phases of the student\nperformance model."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/0912.5426v1", 
    "other_authors": "Xiaokui Xiao, Ke Yi, Yufei Tao", 
    "title": "The Hardness and Approximation Algorithms for L-Diversity", 
    "arxiv-id": "0912.5426v1", 
    "author": "Yufei Tao", 
    "publish": "2009-12-30T08:31:10Z", 
    "summary": "The existing solutions to privacy preserving publication can be classified\ninto the theoretical and heuristic categories. The former guarantees provably\nlow information loss, whereas the latter incurs gigantic loss in the worst\ncase, but is shown empirically to perform well on many real inputs. While\nnumerous heuristic algorithms have been developed to satisfy advanced privacy\nprinciples such as l-diversity, t-closeness, etc., the theoretical category is\ncurrently limited to k-anonymity which is the earliest principle known to have\nsevere vulnerability to privacy attacks. Motivated by this, we present the\nfirst theoretical study on l-diversity, a popular principle that is widely\nadopted in the literature. First, we show that optimal l-diverse generalization\nis NP-hard even when there are only 3 distinct sensitive values in the\nmicrodata. Then, an (l*d)-approximation algorithm is developed, where d is the\ndimensionality of the underlying dataset. This is the first known algorithm\nwith a non-trivial bound on information loss. Extensive experiments with real\ndatasets validate the effectiveness and efficiency of proposed solution."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.1276v1", 
    "other_authors": "Nizar Idoudi, Nada louati, Claude Duvallet, Bruno Sadeg, Rafik Bouaziz, Faiez Gargouri", 
    "title": "A framework to model real-time databases", 
    "arxiv-id": "1001.1276v1", 
    "author": "Faiez Gargouri", 
    "publish": "2010-01-08T13:53:15Z", 
    "summary": "Real-time databases deal with time-constrained data and time-constrained\ntransactions. The design of this kind of databases requires the introduction of\nnew concepts to support both data structures and the dynamic behaviour of the\ndatabase. In this paper, we give an overview about different aspects of\nreal-time databases and we clarify requirements of their modelling. Then, we\npresent a framework for real-time database design and describe its fundamental\noperations. A case study demonstrates the validity of the structural model and\nillustrates SQL queries and Java code generated from the classes of the model"
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.2275v1", 
    "other_authors": "Mohammad Nadimi Shahraki, Norwati Mustapha, Md Nasir B Sulaiman, Ali B Mamat", 
    "title": "Efficient Candidacy Reduction For Frequent Pattern Mining", 
    "arxiv-id": "1001.2275v1", 
    "author": "Ali B Mamat", 
    "publish": "2010-01-13T19:19:06Z", 
    "summary": "Certainly, nowadays knowledge discovery or extracting knowledge from large\namount of data is a desirable task in competitive businesses. Data mining is a\nmain step in knowledge discovery process. Meanwhile frequent patterns play\ncentral role in data mining tasks such as clustering, classification, and\nassociation analysis. Identifying all frequent patterns is the most time\nconsuming process due to a massive number of candidate patterns. For the past\ndecade there have been an increasing number of efficient algorithms to mine the\nfrequent patterns. However reducing the number of candidate patterns and\ncomparisons for support counting are still two problems in this field which\nhave made the frequent pattern mining one of the active research themes in data\nmining. A reasonable solution is identifying a small candidate pattern set from\nwhich can generate all frequent patterns. In this paper, a method is proposed\nbased on a new candidate set called candidate head set or H which forms a small\nset of candidate patterns. The experimental results verify the accuracy of the\nproposed method and reduction of the number of candidate patterns and\ncomparisons."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.2625v2", 
    "other_authors": "Arnab Bhattacharya, Abhishek Bhowmick, Ambuj K. Singh", 
    "title": "Finding top-k similar pairs of objects annotated with terms from an   ontology", 
    "arxiv-id": "1001.2625v2", 
    "author": "Ambuj K. Singh", 
    "publish": "2010-01-15T07:01:37Z", 
    "summary": "With the growing focus on semantic searches and interpretations, an\nincreasing number of standardized vocabularies and ontologies are being\ndesigned and used to describe data. We investigate the querying of objects\ndescribed by a tree-structured ontology. Specifically, we consider the case of\nfinding the top-k best pairs of objects that have been annotated with terms\nfrom such an ontology when the object descriptions are available only at\nruntime. We consider three distance measures. The first one defines the object\ndistance as the minimum pairwise distance between the sets of terms describing\nthem, and the second one defines the distance as the average pairwise term\ndistance. The third and most useful distance measure, earth mover's distance,\nfinds the best way of matching the terms and computes the distance\ncorresponding to this best matching. We develop lower bounds that can be\naggregated progressively and utilize them to speed up the search for top-k\nobject pairs when the earth mover's distance is used. For the minimum pairwise\ndistance, we devise an algorithm that runs in O(D + Tk log k) time, where D is\nthe total information size and T is the total number of terms in the ontology.\nWe also develop a novel best-first search strategy for the average pairwise\ndistance that utilizes lower bounds generated in an ordered manner. Experiments\non real and synthetic datasets demonstrate the practicality and scalability of\nour algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.3488v1", 
    "other_authors": "Pratima Gautam, Neelu Khare, K. R. Pardasani", 
    "title": "A Model for Mining Multilevel Fuzzy Association Rule in Database", 
    "arxiv-id": "1001.3488v1", 
    "author": "K. R. Pardasani", 
    "publish": "2010-01-20T07:49:15Z", 
    "summary": "The problem of developing models and algorithms for multilevel association\nmining pose for new challenges for mathematics and computer science. These\nproblems become more challenging, when some form of uncertainty like fuzziness\nis present in data or relationships in data. This paper proposes a multilevel\nfuzzy association rule mining models for extracting knowledge implicit in\ntransactions database with different support at each level. The proposed\nalgorithm adopts a top-down progressively deepening approach to derive large\nitemsets. This approach incorporates fuzzy boundaries instead of sharp boundary\nintervals. An example is also given to demonstrate that the proposed mining\nalgorithm can derive the multiple-level association rules under different\nsupports in a simple and effective manner."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.3494v1", 
    "other_authors": "Mohammad-Reza Feizi-Derakhshi, Hasan Asil, Amir Asil", 
    "title": "Proposing a New Method for Query Processing Adaption in DataBase", 
    "arxiv-id": "1001.3494v1", 
    "author": "Amir Asil", 
    "publish": "2010-01-20T08:02:20Z", 
    "summary": "This paper proposes a multi agent system by compiling two technologies, query\nprocessing optimization and agents which contains features of personalized\nqueries and adaption with changing of requirements. This system uses a new\nalgorithm based on modeling of users' long-term requirements and also GA to\ngather users' query data. Experimented Result shows more adaption capability\nfor presented algorithm in comparison with classic algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.3720v1", 
    "other_authors": "Yi-Reun Kim, Kyu-Young Whang, Il-Yeol Song", 
    "title": "Page-Differential Logging: An Efficient and DBMS-independent Approach   for Storing Data into Flash Memory", 
    "arxiv-id": "1001.3720v1", 
    "author": "Il-Yeol Song", 
    "publish": "2010-01-21T05:18:08Z", 
    "summary": "Flash memory is widely used as the secondary storage in lightweight computing\ndevices due to its outstanding advantages over magnetic disks. Flash memory has\nmany access characteristics different from those of magnetic disks, and how to\ntake advantage of them is becoming an important research issue. There are two\nexisting approaches to storing data into flash memory: page-based and\nlog-based. The former has good performance for read operations, but poor\nperformance for write operations. In contrast, the latter has good performance\nfor write operations when updates are light, but poor performance for read\noperations. In this paper, we propose a new method of storing data, called\npage-differential logging, for flash-based storage systems that solves the\ndrawbacks of the two methods. The primary characteristics of our method are:\n(1) writing only the difference (which we define as the page-differential)\nbetween the original page in flash memory and the up-to-date page in memory;\n(2) computing and writing the page-differential only once at the time the page\nneeds to be reflected into flash memory. The former contrasts with existing\npage-based methods that write the whole page including both changed and\nunchanged parts of data or from log-based ones that keep track of the history\nof all the changes in a page. Our method allows existing disk-based DBMSs to be\nreused as flash-based DBMSs just by modifying the flash memory driver, i.e., it\nis DBMS-independent. Experimental results show that the proposed method\nimproves the I/O performance by 1.2 ~ 6.1 times over existing methods for the\nTPC-C data of approximately 1 Gbytes."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1001.4880v1", 
    "other_authors": "Benjamin Nguyen, Spyros Zoupanos", 
    "title": "The WebContent XML Store", 
    "arxiv-id": "1001.4880v1", 
    "author": "Spyros Zoupanos", 
    "publish": "2010-01-27T09:33:54Z", 
    "summary": "In this article, we describe the XML storage system used in the WebContent\nproject. We begin by advocating the use of an XML database in order to store\nWebContent documents, and we present two different ways of storing and querying\nthese documents : the use of a centralized XML database and the use of a P2P\nXML database."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.0139v1", 
    "other_authors": "P. S Hiremath, Siddu P. Algur", 
    "title": "Extraction of Flat and Nested Data Records from Web Pages", 
    "arxiv-id": "1002.0139v1", 
    "author": "Siddu P. Algur", 
    "publish": "2010-01-31T16:39:26Z", 
    "summary": "This paper studies the problem of identification and extraction of flat and\nnested data records from a given web page. With the explosive growth of\ninformation sources available on the World Wide Web, it has become increasingly\ndifficult to identify the relevant pieces of information, since web pages are\noften cluttered with irrelevant content like advertisements, navigation-panels,\ncopyright notices etc., surrounding the main content of the web page. Hence, it\nis useful to mine such data regions and data records in order to extract\ninformation from such web pages to provide value-added services. Currently\navailable automatic techniques to mine data regions and data records from web\npages are still unsatisfactory because of their poor performance. In this paper\na novel method to identify and extract the flat and nested data records from\nthe web pages automatically is proposed. It comprises of two steps : (1)\nIdentification and Extraction of the data regions based on visual clues\ninformation. (2) Identification and extraction of flat and nested data records\nfrom the data region of a web page automatically. For step1, a novel and more\neffective method is proposed, which finds the data regions formed by all types\nof tags using visual clues. For step2, a more effective and efficient method\nnamely, Visual Clue based Extraction of web Data (VCED), is proposed, which\nextracts each record from the data region and identifies it whether it is a\nflat or nested data record based on visual clue information the area covered by\nand the number of data items present in each record. Our experimental results\nshow that the proposed technique is effective and better than existing\ntechniques."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.0971v1", 
    "other_authors": "Benjamin Nguyen, Fran\u00e7ois-Xavier Dudouet, Dario Colazzo, Antoine Vion, Ioana Manolescu, Pierre Senellart", 
    "title": "The WebStand Project", 
    "arxiv-id": "1002.0971v1", 
    "author": "Pierre Senellart", 
    "publish": "2010-02-04T12:09:15Z", 
    "summary": "In this paper we present the state of advancement of the French ANR WebStand\nproject. The objective of this project is to construct a customizable XML based\nwarehouse platform to acquire, transform, analyze, store, query and export data\nfrom the web, in particular mailing lists, with the final intension of using\nthis data to perform sociological studies focused on social groups of World\nWide Web, with a specific emphasis on the temporal aspects of this data. We are\ncurrently using this system to analyze the standardization process of the W3C,\nthrough its social network of standard setters."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.1143v2", 
    "other_authors": "Nadeem Mahmood, Aqil Burney, Kamran Ahsan", 
    "title": "A Logical Temporal Relational Data Model", 
    "arxiv-id": "1002.1143v2", 
    "author": "Kamran Ahsan", 
    "publish": "2010-02-05T08:22:24Z", 
    "summary": "Time is one of the most difficult aspects to handle in real world\napplications such as database systems. Relational database management systems\nproposed by Codd offer very little built-in query language support for temporal\ndata management. The model itself incorporates neither the concept of time nor\nany theory of temporal semantics. Many temporal extensions of the relational\nmodel have been proposed and some of them are also implemented. This paper\noffers a brief introduction to temporal database research. We propose a\nconceptual model for handling time varying attributes in the relational\ndatabase model with minimal temporal attributes."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.1150v1", 
    "other_authors": "Mahdi Esmaeili, Fazekas Gabor", 
    "title": "Finding Sequential Patterns from Large Sequence Data", 
    "arxiv-id": "1002.1150v1", 
    "author": "Fazekas Gabor", 
    "publish": "2010-02-05T08:43:45Z", 
    "summary": "Data mining is the task of discovering interesting patterns from large\namounts of data. There are many data mining tasks, such as classification,\nclustering, association rule mining, and sequential pattern mining. Sequential\npattern mining finds sets of data items that occur together frequently in some\nsequences. Sequential pattern mining, which extracts frequent subsequences from\na sequence database, has attracted a great deal of interest during the recent\ndata mining research because it is the basis of many applications, such as: web\nuser analysis, stock trend prediction, DNA sequence analysis, finding language\nor linguistic patterns from natural language texts, and using the history of\nsymptoms to predict certain kind of disease. The diversity of the applications\nmay not be possible to apply a single sequential pattern model to all these\nproblems. Each application may require a unique model and solution. A number of\nresearch projects were established in recent years to develop meaningful\nsequential pattern models and efficient algorithms for mining these patterns.\nIn this paper, we theoretically provided a brief overview three types of\nsequential patterns model."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.1159v1", 
    "other_authors": "Yuval Cohen", 
    "title": "Mining The Successful Binary Combinations: Methodology and A Simple Case   Study", 
    "arxiv-id": "1002.1159v1", 
    "author": "Yuval Cohen", 
    "publish": "2010-02-05T09:07:54Z", 
    "summary": "The importance of finding the characteristics leading to either a success or\na failure is one of the driving forces of data mining. The various application\nareas of finding success/failure factors cover vast variety of areas such as\ncredit risk evaluation and granting loans, micro array analysis, health factors\nand health risk factors, and parameter combination leading to a product\nsuccess. This paper presents a new approach for making inferences about\ndichotomous data. The objective is to determine rules that lead to a certain\nresult. The method consists of four phases: in the first phase, the data is\nprocessed into a binary format of a truth table, in the second phase; rules are\nfound by utilizing an algorithm that minimizes Boolean functions. In the third\nphase the rules are checked and filtered. In the fourth phase, simple rules\nthat involve one to two features are revealed."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.1185v1", 
    "other_authors": "Kanak Saxena, Rahul Shukla", 
    "title": "Significant Interval and Frequent Pattern Discovery in Web Log Data", 
    "arxiv-id": "1002.1185v1", 
    "author": "Rahul Shukla", 
    "publish": "2010-02-05T10:30:51Z", 
    "summary": "There is a considerable body of work on sequence mining of Web Log Data. We\nare using One Pass frequent Episode discovery (or FED) algorithm, takes a\ndifferent approach than the traditional apriori class of pattern detection\nalgorithms. In this approach significant intervals for each Website are\ncomputed first (independently) and these interval used for detecting frequent\npatterns/Episode and then the Analysis is performed on Significant Intervals\nand frequent patterns That can be used to forecast the user's behavior using\nprevious trends and this can be also used for advertising purpose. This type of\napplications predicts the Website interest. In this approach, time-series data\nare folded over a periodicity (day, week, etc.) Which are used to form the\nInterval? Significant intervals are discovered from these time points that\nsatisfy the criteria of minimum confidence and maximum interval length\nspecified by the user."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1002.4315v2", 
    "other_authors": "Sourav Dutta, Arnab Bhattacharya", 
    "title": "Mining Statistically Significant Substrings Based on the Chi-Square   Measure", 
    "arxiv-id": "1002.4315v2", 
    "author": "Arnab Bhattacharya", 
    "publish": "2010-02-23T12:45:31Z", 
    "summary": "Given the vast reservoirs of data stored worldwide, efficient mining of data\nfrom a large information store has emerged as a great challenge. Many databases\nlike that of intrusion detection systems, web-click records, player statistics,\ntexts, proteins etc., store strings or sequences. Searching for an unusual\npattern within such long strings of data has emerged as a requirement for\ndiverse applications. Given a string, the problem then is to identify the\nsubstrings that differs the most from the expected or normal behavior, i.e.,\nthe substrings that are statistically significant. In other words, these\nsubstrings are less likely to occur due to chance alone and may point to some\ninteresting information or phenomenon that warrants further exploration. To\nthis end, we use the chi-square measure. We propose two heuristics for\nretrieving the top-k substrings with the largest chi-square measure. We show\nthat the algorithms outperform other competing algorithms in the runtime, while\nmaintaining a high approximation ratio of more than 0.96."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.1179v1", 
    "other_authors": "Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini, Moshe Y. Vardi", 
    "title": "View Synthesis from Schema Mappings", 
    "arxiv-id": "1003.1179v1", 
    "author": "Moshe Y. Vardi", 
    "publish": "2010-03-05T03:24:13Z", 
    "summary": "In data management, and in particular in data integration, data exchange,\nquery optimization, and data privacy, the notion of view plays a central role.\nIn several contexts, such as data integration, data mashups, and data\nwarehousing, the need arises of designing views starting from a set of known\ncorrespondences between queries over different schemas. In this paper we deal\nwith the issue of automating such a design process. We call this novel problem\n\"view synthesis from schema mappings\": given a set of schema mappings, each\nrelating a query over a source schema to a query over a target schema,\nautomatically synthesize for each source a view over the target schema in such\na way that for each mapping, the query over the source is a rewriting of the\nquery over the target wrt the synthesized views. We study view synthesis from\nschema mappings both in the relational setting, where queries and views are\n(unions of) conjunctive queries, and in the semistructured data setting, where\nqueries and views are (two-way) regular path queries, as well as unions of\nconjunctions thereof. We provide techniques and complexity upper bounds for\neach of these cases."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.1500v1", 
    "other_authors": "M . V. Vijaya Saradhi, B. R. Sastry, P. Satish", 
    "title": "Hierarchical Approach for Online Mining--Emphasis towards Software   Metrics", 
    "arxiv-id": "1003.1500v1", 
    "author": "P. Satish", 
    "publish": "2010-03-07T17:44:18Z", 
    "summary": "Several multi-pass algorithms have been proposed for Association Rule Mining\nfrom static repositories. However, such algorithms are incapable of online\nprocessing of transaction streams. In this paper we introduce an efficient\nsingle-pass algorithm for mining association rules, given a hierarchical\nclassification amongest items. Processing efficiency is achieved by utilizing\ntwo optimizations, hierarchy aware counting and transaction reduction, which\nbecome possible in the context of hierarchical classification. This paper\nconsiders the problem of integrating constraints that are Boolean expression\nover the presence or absence of items into the association discovery algorithm.\nThis paper present three integrated algorithms for mining association rules\nwith item constraints and discuss their tradeoffs. It is concluded that the\nvariation of complexity depends on the measure of DIT (Depth of Inheritance\nTree) and NOC (Number of Children) in the context of Hierarchical\nClassification."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.1816v1", 
    "other_authors": "Sabyasachi Pattanaik, Partha Pratim Ghosh", 
    "title": "Role of Data Mining in E-Payment systems", 
    "arxiv-id": "1003.1816v1", 
    "author": "Partha Pratim Ghosh", 
    "publish": "2010-03-09T07:31:37Z", 
    "summary": "Data Mining deals extracting hidden knowledge, unexpected pattern and new\nrules from large database. Various customized data mining tools have been\ndeveloped for domain specific applications such as Biomedicine, DNA analysis\nand telecommunication. Trends in data mining include further efforts towards\nthe exploration of new application areas and methods for handling complex data\ntypes, algorithm scalability, constraint based data mining and visualization\nmethods. In this paper we will present domain specific Secure Multiparty\ncomputation technique and applications. Data mining has matured as a field of\nbasic and applied research in computer science in general. In this paper, we\nsurvey some of the recent approaches and architectures where data mining has\nbeen applied in the fields of e-payment systems. In this paper we limit our\ndiscussion to data mining in the context of e-payment systems. We also mention\na few directions for further work in this domain, based on the survey."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.3370v1", 
    "other_authors": "Yeb Havinga, Willem Dijkstra, Ander de Keijzer", 
    "title": "Adding HL7 version 3 data types to PostgreSQL", 
    "arxiv-id": "1003.3370v1", 
    "author": "Ander de Keijzer", 
    "publish": "2010-03-17T14:01:19Z", 
    "summary": "The HL7 standard is widely used to exchange medical information\nelectronically. As a part of the standard, HL7 defines scalar communication\ndata types like physical quantity, point in time and concept descriptor but\nalso complex types such as interval types, collection types and probabilistic\ntypes. Typical HL7 applications will store their communications in a database,\nresulting in a translation from HL7 concepts and types into database types.\nSince the data types were not designed to be implemented in a relational\ndatabase server, this transition is cumbersome and fraught with programmer\nerror. The purpose of this paper is two fold. First we analyze the HL7 version\n3 data type definitions and define a number of conditions that must be met, for\nthe data type to be suitable for implementation in a relational database. As a\nresult of this analysis we describe a number of possible improvements in the\nHL7 specification. Second we describe an implementation in the PostgreSQL\ndatabase server and show that the database server can effectively execute\nscientific calculations with units of measure, supports a large number of\noperations on time points and intervals, and can perform operations that are\nakin to a medical terminology server. Experiments on synthetic data show that\nthe user defined types perform better than an implementation that uses only\nstandard data types from the database server."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4068v1", 
    "other_authors": "Pratima Gautam, K. R. Pardasani", 
    "title": "A Novel Approach For Discovery Multi Level Fuzzy Association Rule Mining", 
    "arxiv-id": "1003.4068v1", 
    "author": "K. R. Pardasani", 
    "publish": "2010-03-22T05:38:24Z", 
    "summary": "Finding multilevel association rules in transaction databases is most\ncommonly seen in is widely used in data mining. In this paper, we present a\nmodel of mining multilevel association rules which satisfies the different\nminimum support at each level, we have employed fuzzy set concepts, multi-level\ntaxonomy and different minimum supports to find fuzzy multilevel association\nrules in a given transaction data set. Apriori property is used in model to\nprune the item sets. The proposed model adopts a topdown progressively\ndeepening approach to derive large itemsets. This approach incorporates fuzzy\nboundaries instead of sharp boundary intervals. An example is also given to\ndemonstrate and support that the proposed mining algorithm can derive the\nmultiple-level association rules under different supports in a simple and\neffective manner."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4076v1", 
    "other_authors": "M. S. Danessh, C. Balasubramanian, K. Duraiswamy", 
    "title": "Similarity Data Item Set Approach: An Encoded Temporal Data Base   Technique", 
    "arxiv-id": "1003.4076v1", 
    "author": "K. Duraiswamy", 
    "publish": "2010-03-22T06:23:56Z", 
    "summary": "Data mining has been widely recognized as a powerful tool to explore added\nvalue from large-scale databases. Finding frequent item sets in databases is a\ncrucial in data mining process of extracting association rules. Many algorithms\nwere developed to find the frequent item sets. This paper presents a summary\nand a comparative study of the available FP-growth algorithm variations\nproduced for mining frequent item sets showing their capabilities and\nefficiency in terms of time and memory consumption on association rule mining\nby taking application of specific information into account. It proposes pattern\ngrowth mining paradigm based FP-tree growth algorithm, which employs a tree\nstructure to compress the database. The performance study shows that the anti-\nFP-growth method is efficient and scalable for mining both long and short\nfrequent patterns and is about an order of magnitude faster than the Apriority\nalgorithm and also faster than some recently reported new frequent-pattern\nmining."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4353v2", 
    "other_authors": "Sebastian Maneth, Kim Nguyen", 
    "title": "XPath Whole Query Optimization", 
    "arxiv-id": "1003.4353v2", 
    "author": "Kim Nguyen", 
    "publish": "2010-03-23T09:11:11Z", 
    "summary": "Previous work reports about SXSI, a fast XPath engine which executes tree\nautomata over compressed XML indexes. Here, reasons are investigated why SXSI\nis so fast. It is shown that tree automata can be used as a general framework\nfor fine grained XML query optimization. We define the \"relevant nodes\" of a\nquery as those nodes that a minimal automaton must touch in order to answer the\nquery. This notion allows to skip many subtrees during execution, and, with the\nhelp of particular tree indexes, even allows to skip internal nodes of the\ntree. We efficiently approximate runs over relevant nodes by means of\non-the-fly removal of alternation and non-determinism of (alternating) tree\nautomata. We also introduce many implementation techniques which allows us to\nefficiently evaluate tree automata, even in the absence of special indexes.\nThrough extensive experiments, we demonstrate the impact of the different\noptimization techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4827v1", 
    "other_authors": "Jos\u00e9 Martinez, Carmelo Malta", 
    "title": "Tuple-based abstract data types: full parallelism", 
    "arxiv-id": "1003.4827v1", 
    "author": "Carmelo Malta", 
    "publish": "2010-03-25T09:23:18Z", 
    "summary": "Commutativity has the same inherent limitations as compatibility. Then, it is\nworth conceiving simple concurrency control techniques. We propose a restricted\nform of commutativity which increases parallelism without incurring a higher\noverhead than compatibility. Advantages of our proposition are: (1)\ncommutativity of operations is determined at compile-time, (2) run-time\nchecking is as efficient as for compatibility, (3) neither commutativity\nrelations, (4) nor inverse operations, need to be specified, and (5) log space\nutilization is reduced."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4828v1", 
    "other_authors": "Carmelo Malta, Jos\u00e9 Martinez", 
    "title": "A framework for designing concurrent and recoverable abstract data types   based on commutativity", 
    "arxiv-id": "1003.4828v1", 
    "author": "Jos\u00e9 Martinez", 
    "publish": "2010-03-25T09:33:35Z", 
    "summary": "In this paper, we try to focus the reader's interest on the problems that\ntransactional systems have to resolve for taking advantage of commutativity in\na serializable and recoverable way. Our framework is, (as others), based on the\nuse of conditional commutativity on abstract date types. We present new\nfeatures that have not been found in the literature hitherto, that both\nincrease concurrency and simplify recovery."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.1832", 
    "link": "http://arxiv.org/pdf/1003.4830v1", 
    "other_authors": "Carmelo Malta, Jos\u00e9 Martinez", 
    "title": "Limits of Commutativity on Abstract Data Types", 
    "arxiv-id": "1003.4830v1", 
    "author": "Jos\u00e9 Martinez", 
    "publish": "2010-03-25T09:52:39Z", 
    "summary": "We present some formal properties of (symmetrical) commutativity, the major\ncriterion used in transactional systems, which allow us to fully understand its\nadvantages and disadvantages. The main result is that commutativity is subject\nto the same limitation as compatibility for arbitrary objects. However,\ncommutativity has also a number of attracting properties, one of which is\nrelated to recovery and, to our knowledge, has not been exploited in the\nliterature. Advantages and disadvantages are illustrated on abstract data types\nof interest. We also show how limits of commutativity have been circumvented,\nwhich gives guidelines for doing so (or not!)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.1993.344057", 
    "link": "http://arxiv.org/pdf/1003.4836v1", 
    "other_authors": "Carmelo Malta, Jos\u00e9 Martinez", 
    "title": "Automating Fine Concurrency Control in Object-Oriented Databases", 
    "arxiv-id": "1003.4836v1", 
    "author": "Jos\u00e9 Martinez", 
    "publish": "2010-03-25T09:55:48Z", 
    "summary": "Several propositions were done to provide adapted concurrency control to\nobject-oriented databases. However, most of these proposals miss the fact that\nconsidering solely read and write access modes on instances may lead to less\nparallelism than in relational databases! This paper cope with that issue, and\nadvantages are numerous: (1) commutativity of methods is determined a priori\nand automatically by the compiler, without measurable overhead, (2) run-time\nchecking of commutativity is as efficient as for compatibility, (3) inverse\noperations need not be specified for recovery, (4) this scheme does not\npreclude more sophisticated approaches, and, last but not least, (5) relational\nand object-oriented concurrency control schemes with read and write access\nmodes are subsumed under this proposition."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1003.5056v1", 
    "other_authors": "Sebastien Nedjar, Alain Casali, Rosine Cicchetti, Lotfi Lakhal", 
    "title": "Cubes convexes", 
    "arxiv-id": "1003.5056v1", 
    "author": "Lotfi Lakhal", 
    "publish": "2010-03-26T07:46:33Z", 
    "summary": "In various approaches, data cubes are pre-computed in order to answer\nefficiently OLAP queries. The notion of data cube has been declined in various\nways: iceberg cubes, range cubes or differential cubes. In this paper, we\nintroduce the concept of convex cube which captures all the tuples of a\ndatacube satisfying a constraint combination. It can be represented in a very\ncompact way in order to optimize both computation time and required storage\nspace. The convex cube is not an additional structure appended to the list of\ncube variants but we propose it as a unifying structure that we use to\ncharacterize, in a simple, sound and homogeneous way, the other quoted types of\ncubes. Finally, we introduce the concept of emerging cube which captures the\nsignificant trend inversions. characterizations."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1003.5080v1", 
    "other_authors": "Xiaokui Xiao, Yufei Tao, Nick Koudas", 
    "title": "Transparent Anonymization: Thwarting Adversaries Who Know the Algorithm", 
    "arxiv-id": "1003.5080v1", 
    "author": "Nick Koudas", 
    "publish": "2010-03-26T08:19:02Z", 
    "summary": "Numerous generalization techniques have been proposed for privacy preserving\ndata publishing. Most existing techniques, however, implicitly assume that the\nadversary knows little about the anonymization algorithm adopted by the data\npublisher. Consequently, they cannot guard against privacy attacks that exploit\nvarious characteristics of the anonymization mechanism. This paper provides a\npractical solution to the above problem. First, we propose an analytical model\nfor evaluating disclosure risks, when an adversary knows everything in the\nanonymization process, except the sensitive values. Based on this model, we\ndevelop a privacy principle, transparent l-diversity, which ensures privacy\nprotection against such powerful adversaries. We identify three algorithms that\nachieve transparent l-diversity, and verify their effectiveness and efficiency\nthrough extensive experiments with real data."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.0048v1", 
    "other_authors": "Sudipto Das, Omer Egecioglu, Amr El Abbadi", 
    "title": "Anonimos: An LP based Approach for Anonymizing Weighted Social Network   Graphs", 
    "arxiv-id": "1004.0048v1", 
    "author": "Amr El Abbadi", 
    "publish": "2010-04-01T03:39:33Z", 
    "summary": "The increasing popularity of social networks has initiated a fertile research\narea in information extraction and data mining. Anonymization of these social\ngraphs is important to facilitate publishing these data sets for analysis by\nexternal entities. Prior work has concentrated mostly on node identity\nanonymization and structural anonymization. But with the growing interest in\nanalyzing social networks as a weighted network, edge weight anonymization is\nalso gaining importance. We present An\\'onimos, a Linear Programming based\ntechnique for anonymization of edge weights that preserves linear properties of\ngraphs. Such properties form the foundation of many important graph-theoretic\nalgorithms such as shortest paths problem, k-nearest neighbors, minimum cost\nspanning tree, and maximizing information spread. As a proof of concept, we\napply An\\'onimos to the shortest paths problem and its extensions, prove the\ncorrectness, analyze complexity, and experimentally evaluate it using real\nsocial network data sets. Our experiments demonstrate that An\\'onimos\nanonymizes the weights, improves k-anonymity of the weights, and also scrambles\nthe relative ordering of the edges sorted by weights, thereby providing robust\nand effective anonymization of the sensitive edge-weights. Additionally, we\ndemonstrate the composability of different models generated using An\\'onimos, a\nproperty that allows a single anonymized graph to preserve multiple linear\nproperties."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.1249v6", 
    "other_authors": "Karl Schnaitter, Neoklis Polyzotis", 
    "title": "Semi-Automatic Index Tuning: Keeping DBAs in the Loop", 
    "arxiv-id": "1004.1249v6", 
    "author": "Neoklis Polyzotis", 
    "publish": "2010-04-08T06:10:11Z", 
    "summary": "To obtain good system performance, a DBA must choose a set of indices that is\nappropriate for the workload. The system can aid in this challenging task by\nproviding recommendations for the index configuration. We propose a new index\nrecommendation technique, termed semi-automatic tuning, that keeps the DBA \"in\nthe loop\" by generating recommendations that use feedback about the DBA's\npreferences. The technique also works online, which avoids the limitations of\ncommercial tools that require the workload to be known in advance. The\nfoundation of our approach is the Work Function Algorithm, which can solve a\nwide variety of online optimization problems with strong competitive\nguarantees. We present an experimental analysis that validates the benefits of\nsemi-automatic tuning in a wide variety of conditions."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.1614v1", 
    "other_authors": "Anish Das Sarma, Alpa Jain, Philip Bohannon", 
    "title": "PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines", 
    "arxiv-id": "1004.1614v1", 
    "author": "Philip Bohannon", 
    "publish": "2010-04-09T17:33:37Z", 
    "summary": "Complex information extraction (IE) pipelines assembled by plumbing together\noff-the-shelf operators, specially customized operators, and operators re-used\nfrom other text processing pipelines are becoming an integral component of most\ntext processing frameworks. A critical task faced by the IE pipeline user is to\nrun a post-mortem analysis on the output. Due to the diverse nature of\nextraction operators (often implemented by independent groups), it is time\nconsuming and error-prone to describe operator semantics formally or\noperationally to a provenance system. We introduce the first system that helps\nIE users analyze pipeline semantics and infer provenance interactively while\ndebugging. This allows the effort to be proportional to the need, and to focus\non the portions of the pipeline under the greatest suspicion. We present a\ngeneric debugger for running post-execution analysis of any IE pipeline\nconsisting of arbitrary types of operators. We propose an effective provenance\nmodel for IE pipelines which captures a variety of operator types, ranging from\nthose for which full or no specifications are available. We present a suite of\nalgorithms to effectively build provenance and facilitate debugging. Finally,\nwe present an extensive experimental study on large-scale real-world\nextractions from an index of ~500 million Web documents."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.1677v1", 
    "other_authors": "J. Arokia Renjit, K. L. Shunmuganathan", 
    "title": "Mining The Data From Distributed Database Using An Improved Mining   Algorithm", 
    "arxiv-id": "1004.1677v1", 
    "author": "K. L. Shunmuganathan", 
    "publish": "2010-04-10T03:46:01Z", 
    "summary": "Association rule mining is an active data mining research area and most ARM\nalgorithms cater to a centralized environment. Centralized data mining to\ndiscover useful patterns in distributed databases isn't always feasible because\nmerging data sets from different sites incurs huge network communication costs.\nIn this paper, an Improved algorithm based on good performance level for data\nmining is being proposed. In local sites, it runs the application based on the\nimproved LMatrix algorithm, which is used to calculate local support counts.\nLocal Site also finds a centre site to manage every message exchanged to obtain\nall globally frequent item sets. It also reduces the time of scan of partition\ndatabase by using LMatrix which increases the performance of the algorithm.\nTherefore, the research is to develop a distributed algorithm for\ngeographically distributed data sets that reduces communication costs, superior\nrunning efficiency, and stronger scalability than direct application of a\nsequential algorithm in distributed databases."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.1747v1", 
    "other_authors": "Samidha Dwivedi Sharma, Dr. R. S. Kasana", 
    "title": "Mobile Database System: Role of Mobility on the Query Processing", 
    "arxiv-id": "1004.1747v1", 
    "author": "Dr. R. S. Kasana", 
    "publish": "2010-04-10T22:23:27Z", 
    "summary": "The rapidly expanding technology of mobile communication will give mobile\nusers capability of accessing information from anywhere and any time. The\nwireless technology has made it possible to achieve continuous connectivity in\nmobile environment. When the query is specified as continuous, the requesting\nmobile user can obtain continuously changing result. In order to provide\naccurate and timely outcome to requesting mobile user, the locations of moving\nobject has to be closely monitored. The objective of paper is to discuss the\nproblem related to the role of personal and terminal mobility and query\nprocessing in the mobile environment."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.3272v1", 
    "other_authors": "Nattapon Pannurat, Nittaya Kerdprasop, Kittisak Kerdprasop", 
    "title": "Database Reverse Engineering based on Association Rule Mining", 
    "arxiv-id": "1004.3272v1", 
    "author": "Kittisak Kerdprasop", 
    "publish": "2010-04-19T18:18:43Z", 
    "summary": "Maintaining a legacy database is a difficult task especially when system\ndocumentation is poor written or even missing. Database reverse engineering is\nan attempt to recover high-level conceptual design from the existing database\ninstances. In this paper, we propose a technique to discover conceptual schema\nusing the association mining technique. The discovered schema corresponds to\nthe normalization at the third normal form, which is a common practice in many\nbusiness organizations. Our algorithm also includes the rule filtering\nheuristic to solve the problem of exponential growth of discovered rules\ninherited with the association mining technique."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.3565v1", 
    "other_authors": "P. Velvadivu, K. Duraisamy", 
    "title": "An Optimized Weighted Association Rule Mining On Dynamic Content", 
    "arxiv-id": "1004.3565v1", 
    "author": "K. Duraisamy", 
    "publish": "2010-04-20T20:29:12Z", 
    "summary": "Association rule mining aims to explore large transaction databases for\nassociation rules. Classical Association Rule Mining (ARM) model assumes that\nall items have the same significance without taking their weight into account.\nIt also ignores the difference between the transactions and importance of each\nand every itemsets. But, the Weighted Association Rule Mining (WARM) does not\nwork on databases with only binary attributes. It makes use of the importance\nof each itemset and transaction. WARM requires each item to be given weight to\nreflect their importance to the user. The weights may correspond to special\npromotions on some products, or the profitability of different items. This\nresearch work first focused on a weight assignment based on a directed graph\nwhere nodes denote items and links represent association rules. A generalized\nversion of HITS is applied to the graph to rank the items, where all nodes and\nlinks are allowed to have weights. This research then uses enhanced HITS\nalgorithm by developing an online eigenvector calculation method that can\ncompute the results of mutual reinforcement voting in case of frequent updates.\nFor Example in Share Market Shares price may go down or up. So we need to\ncarefully watch the market and our association rule mining has to produce the\nitems that have undergone frequent changes. These are done by estimating the\nupper bound of perturbation and postponing of the updates whenever possible.\nNext we prove that enhanced algorithm is more efficient than the original HITS\nunder the context of dynamic data."
},{
    "category": "cs.DB", 
    "doi": "10.3166/isi.11.6.11-31", 
    "link": "http://arxiv.org/pdf/1004.4022v1", 
    "other_authors": "Paul Lesov", 
    "title": "Database Security: A Historical Perspective", 
    "arxiv-id": "1004.4022v1", 
    "author": "Paul Lesov", 
    "publish": "2010-04-22T22:22:02Z", 
    "summary": "The importance of security in database research has greatly increased over\nthe years as most of critical functionality of the business and military\nenterprises became digitized. Database is an integral part of any information\nsystem and they often hold sensitive data. The security of the data depends on\nphysical security, OS security and DBMS security. Database security can be\ncompromised by obtaining sensitive data, changing data or degrading\navailability of the database. Over the last 30 years the information technology\nenvironment have gone through many changes of evolution and the database\nresearch community have tried to stay a step ahead of the upcoming threats to\nthe database security. The database research community has thoughts about these\nissues long before they were address by the implementations. This paper will\nexamine the different topics pertaining to database security and see the\nadaption of the research to the changing environment. Some short term database\nresearch trends will be ascertained at the conclusion."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.3120", 
    "link": "http://arxiv.org/pdf/1004.4718v1", 
    "other_authors": "Woong-Kee Loh, Yang-Sae Moon, Jun-Gyu Kang", 
    "title": "A Data Cleansing Method for Clustering Large-scale Transaction Databases", 
    "arxiv-id": "1004.4718v1", 
    "author": "Jun-Gyu Kang", 
    "publish": "2010-04-27T05:56:24Z", 
    "summary": "In this paper, we emphasize the need for data cleansing when clustering\nlarge-scale transaction databases and propose a new data cleansing method that\nimproves clustering quality and performance. We evaluate our data cleansing\nmethod through a series of experiments. As a result, the clustering quality and\nperformance were significantly improved by up to 165% and 330%, respectively."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.3120", 
    "link": "http://arxiv.org/pdf/1006.0575v1", 
    "other_authors": "Bogdan Butnaru, Benjamin Nguyen, Georges Gardarin, Laurent Yeh", 
    "title": "XQ2P: Efficient XQuery P2P Time Series Processing", 
    "arxiv-id": "1006.0575v1", 
    "author": "Laurent Yeh", 
    "publish": "2010-06-03T07:59:15Z", 
    "summary": "In this demonstration, we propose a model for the management of XML time\nseries (TS), using the new XQuery 1.1 window operator. We argue that\ncentralized computation is slow, and demonstrate XQ2P, our prototype of\nefficient XQuery P2P TS computation in the context of financial analysis of\nlarge data sets (>1M values)."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E93.D.3120", 
    "link": "http://arxiv.org/pdf/1006.0576v1", 
    "other_authors": "Georges Gardarin, Benjamin Nguyen, Laurent Yeh, Karine Zeitouni, Bogdan Butnaru, Iulian Sandu-Popa", 
    "title": "Gestion efficace de s\u00e9ries temporelles en P2P: Application \u00e0   l'analyse technique et l'\u00e9tude des objets mobiles", 
    "arxiv-id": "1006.0576v1", 
    "author": "Iulian Sandu-Popa", 
    "publish": "2010-06-03T07:59:40Z", 
    "summary": "In this paper, we propose a simple generic model to manage time series. A\ntime series is composed of a calendar with a typed value for each calendar\nentry. Although the model could support any kind of XML typed values, in this\npaper we focus on real numbers, which are the usual application. We define\nbasic vector space operations (plus, minus, scale), and also relational-like\nand application oriented operators to manage time series. We show the interest\nof this generic model on two applications: (i) a stock investment helper; (ii)\nan ecological transport management system. Stock investment requires\nwindow-based operations while trip management requires complex queries. The\nmodel has been implemented and tested in PHP, Java, and XQuery. We show\nbenchmark results illustrating that the computing of 5000 series of over\n100.000 entries in length - common requirements for both applications - is\ndifficult on classical centralized PCs. In order to serve a community of users\nsharing time series, we propose a P2P implementation of time series by dividing\nthem in segments and providing optimized algorithms for operator expression\ncomputation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.0876v1", 
    "other_authors": "Mohamed Salah Gouider, Amine Farhat", 
    "title": "Building a Data Warehouse for National Social Security Fund of the   Republic of Tunisia", 
    "arxiv-id": "1006.0876v1", 
    "author": "Amine Farhat", 
    "publish": "2010-06-04T12:03:32Z", 
    "summary": "The amounts of data available to decision makers are increasingly important,\ngiven the network availability, low cost storage and diversity of applications.\nTo maximize the potential of these data within the National Social Security\nFund (NSSF) in Tunisia, we have built a data warehouse as a multidimensional\ndatabase, cleaned, homogenized, historicized and consolidated. We used Oracle\nWarehouse Builder to extract, transform and load the source data into the Data\nWarehouse, by applying the KDD process. We have implemented the Data Warehouse\nas an Oracle OLAP. The knowledge extraction has been performed using the Oracle\nDiscoverer tool. This allowed users to take maximum advantage of knowledge as a\nregular report or as ad hoc queries. We started by implementing the main topic\nfor this public institution, accounting for the movements of insured persons.\nThe great success that has followed the completion of this work has encouraged\nthe NSSF to complete the achievement of other topics of interest within the\nNSSF. We suggest in the near future to use Multidimensional Data Mining to\nextract hidden knowledge and that are not predictable by the OLAP."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.1309v1", 
    "other_authors": "S. M. Joshi, S. Sanyal, S. Banerjee, S. Srikumar", 
    "title": "Using Grid Files for a Relational Database Management System", 
    "arxiv-id": "1006.1309v1", 
    "author": "S. Srikumar", 
    "publish": "2010-06-07T17:47:30Z", 
    "summary": "This paper describes our experience with using Grid files as the main storage\norganization for a relational database management system. We primarily focus on\nthe following two aspects. (i) Strategies for implementing grid files\nefficiently. (ii) Methods for efficiency evaluating queries posed to a database\norganized using grid files."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.1663v1", 
    "other_authors": "Spits Warnars", 
    "title": "Tata Kelola Database Perguruan Tinggi Yang Optimal Dengan Data Warehouse", 
    "arxiv-id": "1006.1663v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-08T20:57:02Z", 
    "summary": "The emergence of new higher education institutions has created the\ncompetition in higher education market, and data warehouse can be used as an\neffective technology tools for increasing competitiveness in the higher\neducation market. Data warehouse produce reliable reports for the institution's\nhigh-level management in short time for faster and better decision making, not\nonly on increasing the admission number of students, but also on the\npossibility to find extraordinary, unconventional funds for the institution.\nEfficiency comparison was based on length and amount of processed records,\ntotal processed byte, amount of processed tables, time to run query and\nproduced record on OLTP database and data warehouse. Efficiency percentages was\nmeasured by the formula for percentage increasing and the average efficiency\npercentage of 461.801,04% shows that using data warehouse is more powerful and\nefficient rather than using OLTP database. Data warehouse was modeled based on\nhypercube which is created by limited high demand reports which usually used by\nhigh level management. In every table of fact and dimension fields will be\ninserted which represent the loading constructive merge where the ETL\n(Extraction, Transformation and Loading) process is run based on the old and\nnew files."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.1695v1", 
    "other_authors": "Spits Warnars", 
    "title": "Attribute Oriented Induction with simple select SQL statement", 
    "arxiv-id": "1006.1695v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-09T03:19:36Z", 
    "summary": "Searching learning or rules in relational database for data mining purposes\nwith characteristic or classification/discriminant rule in attribute oriented\ninduction technique can be quicker, easy, and simple with simple SQL statement.\nWith just only one simple SQL statement, characteristic and classification rule\ncan be created simultaneously. Collaboration SQL statement with any other\napplication software will increase the ability for creating t-weight as\nmeasurement the typicality of each record in the characteristic rule and\nd-weight as measurement the discriminating behavior of the learned\nclassification/discriminant rule, particularly for further generalization in\ncharacteristic rule. Handling concept hierarchy into tables based on concept\ntree will influence for the successful simple SQL statement and by knowing the\nright standard knowledge to transform each of concept tree in concept hierarchy\ninto one table as transforming concept hierarchy into table, the simple SQL\nstatement can be run properly."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.1699v1", 
    "other_authors": "Spits Warnars", 
    "title": "Multidimensional Datawarehouse with Combination Formula", 
    "arxiv-id": "1006.1699v1", 
    "author": "Spits Warnars", 
    "publish": "2010-06-09T03:43:16Z", 
    "summary": "Multidimensional in data warehouse is a compulsion and become the most\nimportant for information delivery, without multidimensional Multidimensional\nin data warehouse is a compulsion and become the most important for information\ndelivery, without multidimensional datawarehouse is incomplete.\nMultidimensional give ability to analyze business measurement in many different\nways. Multidimensional is also synonymous with online analytical processing\n(OLAP). By using some concepts in datawarehouse like slice-dice,drill down and\nroll up will increase the ability of multidimensional datawarehouse. The\nresearch question and the discussing for this paper are how much deepest the\nmultidimensional ability from each fact table in datawarehouse. By using the\nstatistic combination formula we try to explore the combination that can be\nyielded from each dimension in hypercubes, the entire of dimensi combination,\nminimum combination and maximum combination."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.2077v1", 
    "other_authors": "H. L. H Spits Warnars", 
    "title": "Multidimensi Pada Data Warehouse Dengan Menggunakan Rumus Kombinasi", 
    "arxiv-id": "1006.2077v1", 
    "author": "H. L. H Spits Warnars", 
    "publish": "2010-06-10T16:16:37Z", 
    "summary": "Multidimensional in data warehouse is a compulsion and become the most\nimportant for information delivery, without multidimensional data warehouse is\nincomplete. Multidimensional give the able to analyze business measurement in\nmany different ways. Multidimensional is also synonymous with online analytical\nprocessing (OLAP)."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2207", 
    "link": "http://arxiv.org/pdf/1006.2088v1", 
    "other_authors": "Spits Warnars H. L. H", 
    "title": "Classification rule with simple select SQL statement", 
    "arxiv-id": "1006.2088v1", 
    "author": "Spits Warnars H. L. H", 
    "publish": "2010-06-10T17:20:14Z", 
    "summary": "A simple sql statement can be used to search learning or rule in relational\ndatabase for data mining purposes particularly for classification rule. With\njust only one simple sql statement, characteristic and classification rule can\nbe created simultaneously. Collaboration sql statement with any other\napplication software will increase the ability for creating t-weight as\nmeasurement the typicality of each record in the characteristic rule and\nd-weight as measurement the discriminating behavior of the learned\nclassification/discriminant rule, specifically for further generalization in\ncharacteristic rule. Handling concept hierarchy into tables based on concept\ntree will influence for the successful simple sql statement and by knowing the\nright standard knowledge to transform each of concept tree in concept hierarchy\ninto one table as to transform concept hierarchy into table, the simple sql\nstatement can be run properly."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2013.01.001", 
    "link": "http://arxiv.org/pdf/1006.3726v4", 
    "other_authors": "Hazel Webb, Daniel Lemire, Owen Kaser", 
    "title": "Diamond Dicing", 
    "arxiv-id": "1006.3726v4", 
    "author": "Owen Kaser", 
    "publish": "2010-06-18T15:38:04Z", 
    "summary": "In OLAP, analysts often select an interesting sample of the data. For\nexample, an analyst might focus on products bringing revenues of at least 100\n000 dollars, or on shops having sales greater than 400 000 dollars. However,\ncurrent systems do not allow the application of both of these thresholds\nsimultaneously, selecting products and shops satisfying both thresholds. For\nsuch purposes, we introduce the diamond cube operator, filling a gap among\nexisting data warehouse operations.\n  Because of the interaction between dimensions the computation of diamond\ncubes is challenging. We compare and test various algorithms on large data sets\nof more than 100 million facts. We find that while it is possible to implement\ndiamonds in SQL, it is inefficient. Indeed, our custom implementation can be a\nhundred times faster than popular database engines (including a row-store and a\ncolumn-store)."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.datak.2013.01.001", 
    "link": "http://arxiv.org/pdf/1006.4833v1", 
    "other_authors": "Graham Kirby, Evangelos Zirintsis, Alan Dearle, Ron Morrison", 
    "title": "A Generic Storage API", 
    "arxiv-id": "1006.4833v1", 
    "author": "Ron Morrison", 
    "publish": "2010-06-24T16:47:28Z", 
    "summary": "We present a generic API suitable for provision of highly generic storage\nfacilities that can be tailored to produce various individually customised\nstorage infrastructures. The paper identifies a candidate set of minimal\nstorage system building blocks, which are sufficiently simple to avoid\nencapsulating policy where it cannot be customised by applications, and\ncomposable to build highly flexible storage architectures. Four main generic\ncomponents are defined: the store, the namer, the caster and the interpreter.\nIt is hypothesised that these are sufficiently general that they could act as\nbuilding blocks for any information storage and retrieval system. The essential\ncharacteristics of each are defined by an interface, which may be implemented\nby multiple implementing classes."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.917", 
    "link": "http://arxiv.org/pdf/1006.5273v1", 
    "other_authors": "Myeong-Seon Gil, Yang-Sae Moon, Bum-Soo Kim", 
    "title": "Linear Detrending Subsequence Matching in Time-Series Databases", 
    "arxiv-id": "1006.5273v1", 
    "author": "Bum-Soo Kim", 
    "publish": "2010-06-28T06:37:32Z", 
    "summary": "Each time-series has its own linear trend, the directionality of a\ntimeseries, and removing the linear trend is crucial to get the more intuitive\nmatching results. Supporting the linear detrending in subsequence matching is a\nchallenging problem due to a huge number of possible subsequences. In this\npaper we define this problem the linear detrending subsequence matching and\npropose its efficient index-based solution. To this end, we first present a\nnotion of LD-windows (LD means linear detrending), which is obtained as\nfollows: we eliminate the linear trend from a subsequence rather than each\nwindow itself and obtain LD-windows by dividing the subsequence into windows.\nUsing the LD-windows we then present a lower bounding theorem for the\nindex-based matching solution and formally prove its correctness. Based on the\nlower bounding theorem, we next propose the index building and subsequence\nmatching algorithms for linear detrending subsequence matching.We finally show\nthe superiority of our index-based solution through extensive experiments."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.917", 
    "link": "http://arxiv.org/pdf/1006.5794v1", 
    "other_authors": "Evangelos Zirintsis, Graham Kirby, Alan Dearle, Ron Morrison", 
    "title": "Report on the XBase Project", 
    "arxiv-id": "1006.5794v1", 
    "author": "Ron Morrison", 
    "publish": "2010-06-30T07:57:10Z", 
    "summary": "This project addressed the conceptual fundamentals of data storage,\ninvestigating techniques for provision of highly generic storage facilities\nthat can be tailored to produce various individually customised storage\ninfrastructures, compliant to the needs of particular applications. This\nrequires the separation of mechanism and policy wherever possible. Aspirations\ninclude: actors, whether users or individual processes, should be able to bind\nto, update and manipulate data and programs transparently with respect to their\nrespective locations; programs should be expressed independently of the storage\nand network technology involved in their execution; storage facilities should\nbe structure-neutral so that actors can impose multiple interpretations over\ninformation, simultaneously and safely; information should not be discarded so\nthat arbitrary historical views are supported; raw stored information should be\nopen to all; where security restrictions on its use are required this should be\nachieved using cryptographic techniques. The key advances of the research were:\n1) the identification of a candidate set of minimal storage system building\nblocks, which are sufficiently simple to avoid encapsulating policy where it\ncannot be customised by applications, and composable to build highly flexible\nstorage architectures 2) insight into the nature of append-only storage\ncomponents, and the issues arising from their application to common storage\nuse-cases."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.917", 
    "link": "http://arxiv.org/pdf/1009.0255v1", 
    "other_authors": "Flavio Rizzolo, Iluju Kiringa, Rachel Pottinger, Kwok Wong", 
    "title": "The Conceptual Integration Modeling Framework: Abstracting from the   Multidimensional Model", 
    "arxiv-id": "1009.0255v1", 
    "author": "Kwok Wong", 
    "publish": "2010-09-01T19:27:40Z", 
    "summary": "Data warehouses are overwhelmingly built through a bottom-up process, which\nstarts with the identification of sources, continues with the extraction and\ntransformation of data from these sources, and then loads the data into a set\nof data marts according to desired multidimensional relational schemas. End\nuser business intelligence tools are added on top of the materialized\nmultidimensional schemas to drive decision making in an organization.\nUnfortunately, this bottom-up approach is costly both in terms of the skilled\nusers needed and the sheer size of the warehouses. This paper proposes a\ntop-down framework in which data warehousing is driven by a conceptual model.\nThe framework offers both design time and run time environments. At design\ntime, a business user first uses the conceptual modeling language as a\nmultidimensional object model to specify what business information is needed;\nthen she maps the conceptual model to a pre-existing logical multidimensional\nrepresentation. At run time, a system will transform the user conceptual model\ntogether with the mappings into views over the logical multidimensional\nrepresentation. We focus on how the user can conceptually abstract from an\nexisting data warehouse, and on how this conceptual model can be mapped to the\nlogical multidimensional representation. We also give an indication of what\nquery language is used over the conceptual model. Finally, we argue that our\nframework is a step along the way to allowing automatic generation of the data\nwarehouse."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.917", 
    "link": "http://arxiv.org/pdf/1009.0368v1", 
    "other_authors": "Sandeep Singh Rawat, Lakshmi Rajamani", 
    "title": "Discovering potential user browsing behaviors using custom-built apriori   algorithm", 
    "arxiv-id": "1009.0368v1", 
    "author": "Lakshmi Rajamani", 
    "publish": "2010-09-02T10:00:51Z", 
    "summary": "Most of the organizations put information on the web because they want it to\nbe seen by the world. Their goal is to have visitors come to the site, feel\ncomfortable and stay a while and try to know completely about the running\norganization. As educational system increasingly requires data mining, the\nopportunity arises to mine the resulting large amounts of student information\nfor hidden useful information (patterns like rule, clustering, and\nclassification, etc). The education domain offers ground for many interesting\nand challenging data mining applications like astronomy, chemistry,\nengineering, climate studies, geology, oceanography, ecology, physics, biology,\nhealth sciences and computer science. Collecting the interesting patterns using\nthe required interestingness measures, which help us in discovering the\nsophisticated patterns that are ultimately used for developing the site. We\nstudy the application of data mining to educational log data collected from\nGuru Nanak Institute of Technology, Ibrahimpatnam, India. We have proposed a\ncustom-built apriori algorithm to find the effective pattern analysis. Finally,\nanalyzing web logs for usage and access trends can not only provide important\ninformation to web site developers and administrators, but also help in\ncreating adaptive web sites."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2414", 
    "link": "http://arxiv.org/pdf/1009.0384v1", 
    "other_authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", 
    "title": "Clustering high dimensional data using subspace and projected clustering   algorithms", 
    "arxiv-id": "1009.0384v1", 
    "author": "Abdullah Embong", 
    "publish": "2010-09-02T10:47:11Z", 
    "summary": "Problem statement: Clustering has a number of techniques that have been\ndeveloped in statistics, pattern recognition, data mining, and other fields.\nSubspace clustering enumerates clusters of objects in all subspaces of a\ndataset. It tends to produce many over lapping clusters. Approach: Subspace\nclustering and projected clustering are research areas for clustering in high\ndimensional spaces. In this research we experiment three clustering oriented\nalgorithms, PROCLUS, P3C and STATPC. Results: In general, PROCLUS performs\nbetter in terms of time of calculation and produced the least number of\nun-clustered data while STATPC outperforms PROCLUS and P3C in the accuracy of\nboth cluster points and relevant attributes found. Conclusions/Recommendations:\nIn this study, we analyze in detail the properties of different data clustering\nmethod."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2414", 
    "link": "http://arxiv.org/pdf/1009.0397v1", 
    "other_authors": "wided oueslati, jalel akaichi", 
    "title": "Mobile Information Collectors' Trajectory Data Warehouse Design", 
    "arxiv-id": "1009.0397v1", 
    "author": "jalel akaichi", 
    "publish": "2010-09-02T11:41:29Z", 
    "summary": "To analyze complex phenomena which involve moving objects, Trajectory Data\nWarehouse (TDW) seems to be an answer for many recent decision problems related\nto various professions (physicians, commercial representatives, transporters,\necologists ...) concerned with mobility. This work aims to make trajectories as\na first class concept in the trajectory data conceptual model and to design a\nTDW, in which data resulting from mobile information collectors' trajectory are\ngathered. These data will be analyzed, according to trajectory characteristics,\nfor decision making purposes, such as new products commercialization, new\ncommerce implementation, etc."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2414", 
    "link": "http://arxiv.org/pdf/1009.0827v1", 
    "other_authors": "Hamed khataeimaragheh, Hassan Rashidi", 
    "title": "A Novel Watermarking Scheme for Detecting and Recovering Distortions in   Database Tables", 
    "arxiv-id": "1009.0827v1", 
    "author": "Hassan Rashidi", 
    "publish": "2010-09-04T11:29:55Z", 
    "summary": "In this paper a novel fragile watermarking scheme is proposed to detect,\nlocalize and recover malicious modifications in relational databases. In the\nproposed scheme, all tuples in the database are first securely divided into\ngroups. Then watermarks are embedded and verified group-by-group independently.\nBy using the embedded watermark, we are able to detect and localize the\nmodification made to the database and even we recover the true data from the\ndatabase modified locations. Our experimental results show that this scheme is\nso qualified; i.e. distortion detection and true data recovery both are\nperformed successfully."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1009.0929v1", 
    "other_authors": "Hao-En Chueh", 
    "title": "Mining Target-Oriented Sequential Patterns with Time-Intervals", 
    "arxiv-id": "1009.0929v1", 
    "author": "Hao-En Chueh", 
    "publish": "2010-09-05T16:48:59Z", 
    "summary": "A target-oriented sequential pattern is a sequential pattern with a concerned\nitemset in the end of pattern. A time-interval sequential pattern is a\nsequential pattern with time-intervals between every pair of successive\nitemsets. In this paper we present an algorithm to discover target-oriented\nsequential pattern with time-intervals. To this end, the original sequences are\nreversed so that the last itemsets can be arranged in front of the sequences.\nThe contrasts between reversed sequences and the concerned itemset are then\nused to exclude the irrelevant sequences. Clustering analysis is used with\ntypical sequential pattern mining algorithm to extract the sequential patterns\nwith time-intervals between successive itemsets. Finally, the discovered\ntime-interval sequential patterns are reversed again to the original order for\nsearching the target patterns."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1009.0971v1", 
    "other_authors": "B. Kiran Kumar, A. Bhaskar", 
    "title": "ETP-Mine: An Efficient Method for Mining Transitional Patterns", 
    "arxiv-id": "1009.0971v1", 
    "author": "A. Bhaskar", 
    "publish": "2010-09-06T05:46:18Z", 
    "summary": "A Transaction database contains a set of transactions along with items and\ntheir associated timestamps. Transitional patterns are the patterns which\nspecify the dynamic behavior of frequent patterns in a transaction database. To\ndiscover transitional patterns and their significant milestones, first we have\nto extract all frequent patterns and their supports using any frequent pattern\ngeneration algorithm. These frequent patterns are used in the generation of\ntransitional patterns. The existing algorithm (TP-Mine) generates frequent\npatterns, some of which cannot be used in generation of transitional patterns.\nIn this paper, we propose a modification to the existing algorithm, which\nprunes the candidate items to be used in the generation of frequent patterns.\nThis method drastically reduces the number of frequent patterns which are used\nin discovering transitional patterns. Extensive simulation test is done to\nevaluate the proposed method."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1009.2270v1", 
    "other_authors": "L. Caroprese, M. Truszczynski", 
    "title": "Active Integrity Constraints and Revision Programming", 
    "arxiv-id": "1009.2270v1", 
    "author": "M. Truszczynski", 
    "publish": "2010-09-12T22:14:04Z", 
    "summary": "We study active integrity constraints and revision programming, two\nformalisms designed to describe integrity constraints on databases and to\nspecify policies on preferred ways to enforce them. Unlike other more commonly\naccepted approaches, these two formalisms attempt to provide a declarative\nsolution to the problem. However, the original semantics of founded repairs for\nactive integrity constraints and justified revisions for revision programs\ndiffer. Our main goal is to establish a comprehensive framework of semantics\nfor active integrity constraints, to find a parallel framework for revision\nprograms, and to relate the two. By doing so, we demonstrate that the two\nformalisms proposed independently of each other and based on different\nintuitions when viewed within a broader semantic framework turn out to be\nnotational variants of each other. That lends support to the adequacy of the\nsemantics we develop for each of the formalisms as the foundation for a\ndeclarative approach to the problem of database update and repair. In the paper\nwe also study computational properties of the semantics we consider and\nestablish results concerned with the concept of the minimality of change and\nthe invariance under the shifting transformation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1009.2764v2", 
    "other_authors": "Karl Malbrain", 
    "title": "A Blink Tree latch method and protocol to support synchronous node   deletion", 
    "arxiv-id": "1009.2764v2", 
    "author": "Karl Malbrain", 
    "publish": "2010-09-14T20:15:14Z", 
    "summary": "A Blink Tree latch method and protocol supports synchronous node deletion in\na high concurrency environment. Full source code is available."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1009.5149v1", 
    "other_authors": "Eya ben Ahmed, Mohamed Salah Gouider", 
    "title": "Towards an incremental maintenance of cyclic association rules", 
    "arxiv-id": "1009.5149v1", 
    "author": "Mohamed Salah Gouider", 
    "publish": "2010-09-27T03:10:28Z", 
    "summary": "Recently, the cyclic association rules have been introduced in order to\ndiscover rules from items characterized by their regular variation over time.\nIn real life situations, temporal databases are often appended or updated.\nRescanning the whole database every time is highly expensive while existing\nincremental mining techniques can efficiently solve such a problem. In this\npaper, we propose an incremental algorithm for cyclic association rules\nmaintenance. The carried out experiments of our proposal stress on its\nefficiency and performance."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1010.0122v1", 
    "other_authors": "Michael Hartung, Anika Gro\u00df, Erhard Rahm", 
    "title": "Rule-based Generation of Diff Evolution Mappings between Ontology   Versions", 
    "arxiv-id": "1010.0122v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-10-01T11:05:18Z", 
    "summary": "Ontologies such as taxonomies, product catalogs or web directories are\nheavily used and hence evolve frequently to meet new requirements or to better\nreflect the current instance data of a domain. To effectively manage the\nevolution of ontologies it is essential to identify the difference (Diff)\nbetween two ontology versions. We propose a novel approach to determine an\nexpressive and invertible diff evolution mapping between given versions of an\nontology. Our approach utilizes the result of a match operation to determine an\nevolution mapping consisting of a set of basic change operations\n(insert/update/delete). To semantically enrich the evolution mapping we adopt a\nrule-based approach to transform the basic change operations into a smaller set\nof more complex change operations, such as merge, split, or changes of entire\nsubgraphs. The proposed algorithm is customizable in different ways to meet the\nrequirements of diverse ontologies and application scenarios. We evaluate the\nproposed approach by determining and analyzing evolution mappings for\nreal-world life science ontologies and web directories."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1010.0924v1", 
    "other_authors": "Daniele Riboni, Linda Pareschi, Claudio Bettini", 
    "title": "Preserving Privacy in Sequential Data Release against Background   Knowledge Attacks", 
    "arxiv-id": "1010.0924v1", 
    "author": "Claudio Bettini", 
    "publish": "2010-10-05T15:51:50Z", 
    "summary": "A large amount of transaction data containing associations between\nindividuals and sensitive information flows everyday into data stores. Examples\ninclude web queries, credit card transactions, medical exam records, transit\ndatabase records. The serial release of these data to partner institutions or\ndata analysis centers is a common situation. In this paper we show that, in\nmost domains, correlations among sensitive values associated to the same\nindividuals in different releases can be easily mined, and used to violate\nusers' privacy by adversaries observing multiple data releases. We provide a\nformal model for privacy attacks based on this sequential background knowledge,\nas well as on background knowledge on the probability distribution of sensitive\nvalues over different individuals. We show how sequential background knowledge\ncan be actually obtained by an adversary, and used to identify with high\nconfidence the sensitive values associated with an individual. A defense\nalgorithm based on Jensen-Shannon divergence is proposed, and extensive\nexperiments show the superiority of the proposed technique with respect to\nother applicable solutions. To the best of our knowledge, this is the first\nwork that systematically investigates the role of sequential background\nknowledge in serial release of transaction data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1010.2148v1", 
    "other_authors": "Angela Bonifati, Giansalvatore Mecca, Domenica Sileo, Gianvito Summa", 
    "title": "Ontological Matchmaking in Recommender Systems", 
    "arxiv-id": "1010.2148v1", 
    "author": "Gianvito Summa", 
    "publish": "2010-10-11T16:22:43Z", 
    "summary": "The electronic marketplace offers great potential for the recommendation of\nsupplies. In the so called recommender systems, it is crucial to apply\nmatchmaking strategies that faithfully satisfy the predicates specified in the\ndemand, and take into account as much as possible the user preferences. We\nfocus on real-life ontology-driven matchmaking scenarios and identify a number\nof challenges, being inspired by such scenarios. A key challenge is that of\npresenting the results to the users in an understandable and clear-cut fashion\nin order to facilitate the analysis of the results. Indeed, such scenarios\nevoke the opportunity to rank and group the results according to specific\ncriteria. A further challenge consists of presenting the results to the user in\nan asynchronous fashion, i.e. the 'push' mode, along with the 'pull' mode, in\nwhich the user explicitly issues a query, and displays the results. Moreover,\nan important issue to consider in real-life cases is the possibility of\nsubmitting a query to multiple providers, and collecting the various results.\nWe have designed and implemented an ontology-based matchmaking system that\nsuitably addresses the above challenges. We have conducted a comprehensive\nexperimental study, in order to investigate the usability of the system, the\nperformance and the effectiveness of the matchmaking strategies with real\nontological datasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1010.3615v1", 
    "other_authors": "St\u00e9phane Martin, Pascal Urso, St\u00e9phane Weiss", 
    "title": "Scalable XML Collaborative Editing with Undo short paper", 
    "arxiv-id": "1010.3615v1", 
    "author": "St\u00e9phane Weiss", 
    "publish": "2010-10-18T14:38:31Z", 
    "summary": "Commutative Replicated Data-Type (CRDT) is a new class of algorithms that\nensures scalable consistency of replicated data. It has been successfully\napplied to collaborative editing of texts without complex concurrency control.\nIn this paper, we present a CRDT to edit XML data. Compared to existing\napproaches for XML collaborative editing, our approach is more scalable and\nhandles all the XML editing aspects : elements, contents, attributes and undo.\nIndeed, undo is recognized as an important feature for collaborative editing\nthat allows to overcome system complexity through error recovery or\ncollaborative conflict resolution."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2010.2410", 
    "link": "http://arxiv.org/pdf/1010.4850v2", 
    "other_authors": "S\u00e9bastien Nedjar, Fabien Pesci, Lotfi Lakhal, Rosine Cicchetti", 
    "title": "Treillis des concepts skylines : Analyse multidimensionnelle des   skylines fond\u00e9e sur les ensembles en accord", 
    "arxiv-id": "1010.4850v2", 
    "author": "Rosine Cicchetti", 
    "publish": "2010-10-23T06:01:31Z", 
    "summary": "The skyline concept has been introduced in order to exhibit the best objects\naccording to all the criterion combinations and makes it possible to analyse\nthe relationships between skyline objects. Like the data cube, the skycube is\nso voluminous that reduction approaches are really necessary. In this paper, we\ndefine an approach which partially materializes the skycube. The underlying\nidea is to discard from the representation the skycuboids which can be computed\nagain the most easily. To meet this reduction objective, we characterize a\nformal framework: the agree concept lattice. From this structure, we derive the\nskyline concept lattice which is one of its constrained instances. The strong\npoints of our approach are: (i) it is attribute oriented; (ii) it provides a\nboundary for the number of lattice nodes; (iii) it facilitates the navigation\nwithin the Skycuboids."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1011.0328v1", 
    "other_authors": "Soumadip Ghosh, Sushanta Biswas, Debasree Sarkar, Partha Pratim Sarkar", 
    "title": "Mining Frequent Itemsets Using Genetic Algorithm", 
    "arxiv-id": "1011.0328v1", 
    "author": "Partha Pratim Sarkar", 
    "publish": "2010-11-01T14:14:10Z", 
    "summary": "In general frequent itemsets are generated from large data sets by applying\nassociation rule mining algorithms like Apriori, Partition, Pincer-Search,\nIncremental, Border algorithm etc., which take too much computer time to\ncompute all the frequent itemsets. By using Genetic Algorithm (GA) we can\nimprove the scenario. The major advantage of using GA in the discovery of\nfrequent itemsets is that they perform global search and its time complexity is\nless compared to other algorithms as the genetic algorithm is based on the\ngreedy approach. The main aim of this paper is to find all the frequent\nitemsets from given data sets using genetic algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1011.2511v1", 
    "other_authors": "Graham Cormode", 
    "title": "Individual Privacy vs Population Privacy: Learning to Attack   Anonymization", 
    "arxiv-id": "1011.2511v1", 
    "author": "Graham Cormode", 
    "publish": "2010-11-10T21:39:24Z", 
    "summary": "Over the last decade there have been great strides made in developing\ntechniques to compute functions privately. In particular, Differential Privacy\ngives strong promises about conclusions that can be drawn about an individual.\nIn contrast, various syntactic methods for providing privacy (criteria such as\nkanonymity and l-diversity) have been criticized for still allowing private\ninformation of an individual to be inferred. In this report, we consider the\nability of an attacker to use data meeting privacy definitions to build an\naccurate classifier. We demonstrate that even under Differential Privacy, such\nclassifiers can be used to accurately infer \"private\" attributes in realistic\ndata. We compare this to similar approaches for inferencebased attacks on other\nforms of anonymized data. We place these attacks on the same scale, and observe\nthat the accuracy of inference of private attributes for Differentially Private\ndata and l-diverse data can be quite similar."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1101.1110v1", 
    "other_authors": "Yael Amsterdamer, Daniel Deutch, Val Tannen", 
    "title": "Provenance for Aggregate Queries", 
    "arxiv-id": "1101.1110v1", 
    "author": "Val Tannen", 
    "publish": "2011-01-05T22:20:29Z", 
    "summary": "We study in this paper provenance information for queries with aggregation.\nProvenance information was studied in the context of various query languages\nthat do not allow for aggregation, and recent work has suggested to capture\nprovenance by annotating the different database tuples with elements of a\ncommutative semiring and propagating the annotations through query evaluation.\nWe show that aggregate queries pose novel challenges rendering this approach\ninapplicable. Consequently, we propose a new approach, where we annotate with\nprovenance information not just tuples but also the individual values within\ntuples, using provenance to describe the values computation. We realize this\napproach in a concrete construction, first for \"simple\" queries where the\naggregation operator is the last one applied, and then for arbitrary (positive)\nrelational algebra queries with aggregation; the latter queries are shown to be\nmore challenging in this context. Finally, we use aggregation to encode queries\nwith difference, and study the semantics obtained for such queries on\nprovenance annotated databases."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1101.2613v2", 
    "other_authors": "Thomas Bernecker, Tobias Emrich, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz, Andreas Zuefle", 
    "title": "A Novel Probabilistic Pruning Approach to Speed Up Similarity Queries in   Uncertain Databases", 
    "arxiv-id": "1101.2613v2", 
    "author": "Andreas Zuefle", 
    "publish": "2011-01-13T17:32:01Z", 
    "summary": "In this paper, we propose a novel, effective and efficient probabilistic\npruning criterion for probabilistic similarity queries on uncertain data. Our\napproach supports a general uncertainty model using continuous probabilistic\ndensity functions to describe the (possibly correlated) uncertain attributes of\nobjects. In a nutshell, the problem to be solved is to compute the PDF of the\nrandom variable denoted by the probabilistic domination count: Given an\nuncertain database object B, an uncertain reference object R and a set D of\nuncertain database objects in a multi-dimensional space, the probabilistic\ndomination count denotes the number of uncertain objects in D that are closer\nto R than B. This domination count can be used to answer a wide range of\nprobabilistic similarity queries. Specifically, we propose a novel geometric\npruning filter and introduce an iterative filter-refinement strategy for\nconservatively and progressively estimating the probabilistic domination count\nin an efficient way while keeping correctness according to the possible world\nsemantics. In an experimental evaluation, we show that our proposed technique\nallows to acquire tight probability bounds for the probabilistic domination\ncount quickly, even for large uncertain databases."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1101.4270v1", 
    "other_authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", 
    "title": "A Comparative Agglomerative Hierarchical Clustering Method to Cluster   Implemented Course", 
    "arxiv-id": "1101.4270v1", 
    "author": "Abdullah Embong", 
    "publish": "2011-01-22T08:00:33Z", 
    "summary": "There are many clustering methods, such as hierarchical clustering method.\nMost of the approaches to the clustering of variables encountered in the\nliterature are of hierarchical type. The great majority of hierarchical\napproaches to the clustering of variables are of agglomerative nature. The\nagglomerative hierarchical approach to clustering starts with each observation\nas its own cluster and then continually groups the observations into\nincreasingly larger groups. Higher Learning Institution (HLI) provides training\nto introduce final-year students to the real working environment. In this\nresearch will use Euclidean single linkage and complete linkage. MATLAB and HCE\n3.5 software will used to train data and cluster course implemented during\nindustrial training. This study indicates that different method will create a\ndifferent number of clusters."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1101.5668v1", 
    "other_authors": "L. K. Joshila Grace, V. Maheswari, Dhinaharan Nagamalai", 
    "title": "Analysis of Web Logs and Web User in Web Mining", 
    "arxiv-id": "1101.5668v1", 
    "author": "Dhinaharan Nagamalai", 
    "publish": "2011-01-29T05:09:20Z", 
    "summary": "Log files contain information about User Name, IP Address, Time Stamp, Access\nRequest, number of Bytes Transferred, Result Status, URL that Referred and User\nAgent. The log files are maintained by the web servers. By analysing these log\nfiles gives a neat idea about the user. This paper gives a detailed discussion\nabout these log files, their formats, their creation, access procedures, their\nuses, various algorithms used and the additional parameters that can be used in\nthe log files which in turn gives way to an effective mining. It also provides\nthe idea of creating an extended log file and learning the user behaviour."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijaia.2010.1411", 
    "link": "http://arxiv.org/pdf/1102.0115v1", 
    "other_authors": "Oksana Grabova, J\u00e9r\u00f4me Darmont, Jean-Hugues Chauchat, Iryna Zolotaryova", 
    "title": "Business Intelligence for Small and Middle-Sized Entreprises", 
    "arxiv-id": "1102.0115v1", 
    "author": "Iryna Zolotaryova", 
    "publish": "2011-02-01T10:11:15Z", 
    "summary": "Data warehouses are the core of decision support sys- tems, which nowadays\nare used by all kind of enter- prises in the entire world. Although many\nstudies have been conducted on the need of decision support systems (DSSs) for\nsmall businesses, most of them adopt ex- isting solutions and approaches, which\nare appropriate for large-scaled enterprises, but are inadequate for small and\nmiddle-sized enterprises. Small enterprises require cheap, lightweight\narchitec- tures and tools (hardware and software) providing on- line data\nanalysis. In order to ensure these features, we review web-based business\nintelligence approaches. For real-time analysis, the traditional OLAP\narchitecture is cumbersome and storage-costly; therefore, we also re- view\nin-memory processing. Consequently, this paper discusses the existing approa-\nches and tools working in main memory and/or with web interfaces (including\nfreeware tools), relevant for small and middle-sized enterprises in decision\nmaking."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-18206-8_14", 
    "link": "http://arxiv.org/pdf/1102.0372v1", 
    "other_authors": "Hadj Mahboubi, J\u00e9r\u00f4me Darmont", 
    "title": "XWeB: the XML Warehouse Benchmark", 
    "arxiv-id": "1102.0372v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2011-02-02T07:11:13Z", 
    "summary": "With the emergence of XML as a standard for representing business data, new\ndecision support applications are being developed. These XML data warehouses\naim at supporting On-Line Analytical Processing (OLAP) operations that\nmanipulate irregular XML data. To ensure feasibility of these new tools,\nimportant performance issues must be addressed. Performance is customarily\nassessed with the help of benchmarks. However, decision support benchmarks do\nnot currently support XML features. In this paper, we introduce the XML\nWarehouse Benchmark (XWeB), which aims at filling this gap. XWeB derives from\nthe relational decision support benchmark TPC-H. It is mainly composed of a\ntest data warehouse that is based on a unified reference model for XML\nwarehouses and that features XML-specific structures, and its associate XQuery\ndecision support workload. XWeB's usage is illustrated by experiments on\nseveral XML database management systems."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-18206-8_14", 
    "link": "http://arxiv.org/pdf/1102.0952v1", 
    "other_authors": "Marouane Hachicha, J\u00e9r\u00f4me Darmont", 
    "title": "Pattern tree-based XOLAP rollup operator for XML complex hierarchies", 
    "arxiv-id": "1102.0952v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2011-02-04T15:59:12Z", 
    "summary": "With the rise of XML as a standard for representing business data, XML data\nwarehousing appears as a suitable solution for decision-support applications.\nIn this context, it is necessary to allow OLAP analyses on XML data cubes.\nThus, XQuery extensions are needed. To define a formal framework and allow\nmuch-needed performance optimizations on analytical queries expressed in\nXQuery, defining an algebra is desirable. However, XML-OLAP (XOLAP) algebras\nfrom the literature still largely rely on the relational model. Hence, we\npropose in this paper a rollup operator based on a pattern tree in order to\nhandle multidimensional XML data expressed within complex hierarchies."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-18206-8_14", 
    "link": "http://arxiv.org/pdf/1102.4429v1", 
    "other_authors": "Wided Oueslati, Jalel Akaichi", 
    "title": "A Trajectory UML profile For Modeling Trajectory Data: A Mobile Hospital   Use Case", 
    "arxiv-id": "1102.4429v1", 
    "author": "Jalel Akaichi", 
    "publish": "2011-02-22T08:15:51Z", 
    "summary": "A large amount of data resulting from trajectories of moving objects\nactivities are collected thanks to localization based services and some\nassociated automated processes. Trajectories data can be used either for\ntransactional and analysis purposes in various domains (heath care, commerce,\nenvironment, etc.). For this reason, modeling trajectory data at the conceptual\nlevel is an important stair leading to global vision and successful\nimplementations. However, current modeling tools fail to fulfill specific\nmoving objects activities requirements. In this paper, we propose a new profile\nbased on UML in order to enhance the conceptual modeling of trajectory data\nrelated to mobile objects by new stereotypes and icons. As illustration, we\npresent a mobile hospital use case."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-18206-8_14", 
    "link": "http://arxiv.org/pdf/1102.5190v1", 
    "other_authors": "Jalal Laassiri, Said Elhajji, Mohamed Bouhdadi, Ghizlane Orhanou, Youssef Balouki", 
    "title": "Specifying Data Bases Management Systems by Using RM-ODP Engineering   Language", 
    "arxiv-id": "1102.5190v1", 
    "author": "Youssef Balouki", 
    "publish": "2011-02-25T08:39:16Z", 
    "summary": "Distributed systems can be very large and complex. The various considerations\nthat influence their design can result in a substantial specification, which\nrequires a structured framework that has to be managed successfully. The\npurpose of the RMODP is to define such a framework. The Reference Model for\nOpen Distributed Processing (RM-ODP) provides a framework within which support\nof distribution, inter-working and portability can be integrated. It defines:\nan object model, architectural concepts and architecture for the development of\nODP systems in terms of five viewpoints. Which include an information\nviewpoint. Since the usage of Data bases management systems (DBMS) in complex\nnetworks is increasing considerably, we are interested, in our work, in giving\nDBMS specifications through the use of the three schemas (static, dynamic,\ninvariant). The present paper is organized as follows. After a literature\nreview, we will describe then the subset of concepts considered in this work\nnamed the database management system (DBMS) object model. In the third section,\nwe will be interested in the engineering language and DMBS structure by\ndescribing essentially DBMS objects. Finally, we will present DBMS engineering\nspecifications and makes the connection between models and their instances.\nThis introduces the basic form of the semantic approach we have described here."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0083v1", 
    "other_authors": "Hao-En Chueh", 
    "title": "Mining Target-Oriented Fuzzy Correlation Rules to Optimize Telecom   Service Management", 
    "arxiv-id": "1103.0083v1", 
    "author": "Hao-En Chueh", 
    "publish": "2011-03-01T05:17:30Z", 
    "summary": "To optimize telecom service management, it is necessary that information\nabout telecom services is highly related to the most popular telecom service.\nTo this end, we propose an algorithm for mining target-oriented fuzzy\ncorrelation rules. In this paper, we show that by using the fuzzy statistics\nanalysis and the data mining technology, the target-oriented fuzzy correlation\nrules can be obtained from a given database. We conduct an experiment by using\na sample database from a telecom service provider in Taiwan. Our work can be\nused to assist the telecom service provider in providing the appropriate\nservices to the customers for better customer relationship management."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0172v2", 
    "other_authors": "Thomas Bernecker, Tobias Emrich, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz, Shiming Zhang, Andreas Z\u00fcfle", 
    "title": "Inverse Queries For Multidimensional Spaces", 
    "arxiv-id": "1103.0172v2", 
    "author": "Andreas Z\u00fcfle", 
    "publish": "2011-03-01T14:07:04Z", 
    "summary": "Traditional spatial queries return, for a given query object $q$, all\ndatabase objects that satisfy a given predicate, such as epsilon range and\n$k$-nearest neighbors. This paper defines and studies {\\em inverse} spatial\nqueries, which, given a subset of database objects $Q$ and a query predicate,\nreturn all objects which, if used as query objects with the predicate, contain\n$Q$ in their result. We first show a straightforward solution for answering\ninverse spatial queries for any query predicate. Then, we propose a\nfilter-and-refinement framework that can be used to improve efficiency. We show\nhow to apply this framework on a variety of inverse queries, using appropriate\nspace pruning strategies. In particular, we propose solutions for inverse\nepsilon range queries, inverse $k$-nearest neighbor queries, and inverse\nskyline queries. Our experiments show that our framework is significantly more\nefficient than naive approaches."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0633v1", 
    "other_authors": "Y. V. Dongare, P. S. Dhabe, S. V. Deshmukh", 
    "title": "RDBNorma: - A semi-automated tool for relational database schema   normalization up to third normal form", 
    "arxiv-id": "1103.0633v1", 
    "author": "S. V. Deshmukh", 
    "publish": "2011-03-03T09:44:14Z", 
    "summary": "In this paper a tool called RDBNorma is proposed, that uses a novel approach\nto represent a relational database schema and its functional dependencies in\ncomputer memory using only one linked list and used for semi-automating the\nprocess of relational database schema normalization up to third normal form.\nThis paper addresses all the issues of representing a relational schema along\nwith its functional dependencies using one linked list along with the\nalgorithms to convert a relation into second and third normal form by using\nabove representation. We have compared performance of RDBNorma with existing\ntool called Micro using standard relational schemas collected from various\nresources. It is observed that proposed tool is at least 2.89 times faster than\nthe Micro and requires around half of the space than Micro to represent a\nrelation. Comparison is done by entering all the attributes and functional\ndependencies holds on a relation in the same order and implementing both the\ntools in same language and on same machine."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0686v1", 
    "other_authors": "Mohamed Mkaouar, Rafik Bouaziz, Mohamed Moalla", 
    "title": "Querying and Manipulating Temporal Databases", 
    "arxiv-id": "1103.0686v1", 
    "author": "Mohamed Moalla", 
    "publish": "2011-03-03T14:09:24Z", 
    "summary": "Many works have focused, for over twenty five years, on the integration of\nthe time dimension in databases (DB). However, the standard SQL3 does not yet\nallow easy definition, manipulation and querying of temporal DBs. In this\npaper, we study how we can simplify querying and manipulating temporal facts in\nSQL3, using a model that integrates time in a native manner. To do this, we\npropose new keywords and syntax to define different temporal versions for many\nrelational operators and functions used in SQL. It then becomes possible to\nperform various queries and updates appropriate to temporal facts. We\nillustrate the use of these proposals on many examples from a real application."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0825v1", 
    "other_authors": "Graham Cormode, Magda Procopiuc, Divesh Srivastava, Thanh T. L. Tran", 
    "title": "Differentially Private Publication of Sparse Data", 
    "arxiv-id": "1103.0825v1", 
    "author": "Thanh T. L. Tran", 
    "publish": "2011-03-04T05:02:47Z", 
    "summary": "The problem of privately releasing data is to provide a version of a dataset\nwithout revealing sensitive information about the individuals who contribute to\nthe data. The model of differential privacy allows such private release while\nproviding strong guarantees on the output. A basic mechanism achieves\ndifferential privacy by adding noise to the frequency counts in the contingency\ntables (or, a subset of the count data cube) derived from the dataset. However,\nwhen the dataset is sparse in its underlying space, as is the case for most\nmulti-attribute relations, then the effect of adding noise is to vastly\nincrease the size of the published data: it implicitly creates a huge number of\ndummy data points to mask the true data, making it almost impossible to work\nwith.\n  We present techniques to overcome this roadblock and allow efficient private\nrelease of sparse data, while maintaining the guarantees of differential\nprivacy. Our approach is to release a compact summary of the noisy data.\nGenerating the noisy data and then summarizing it would still be very costly,\nso we show how to shortcut this step, and instead directly generate the summary\nfrom the input data, without materializing the vast intermediate noisy data. We\ninstantiate this outline for a variety of sampling and filtering methods, and\nshow how to use the resulting summary for approximate, private, query\nanswering. Our experimental study shows that this is an effective, practical\nsolution, with comparable and occasionally improved utility over the costly\nmaterialization approach."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.0921v1", 
    "other_authors": "Hela Limam, Jalel Akaichi", 
    "title": "Managing and Querying Web Services Communities: A Survey", 
    "arxiv-id": "1103.0921v1", 
    "author": "Jalel Akaichi", 
    "publish": "2011-03-03T10:45:11Z", 
    "summary": "With the advance of Web Services technologies and the emergence of Web\nServices into the information space, tremendous opportunities for empowering\nusers and organizations appear in various application domains including\nelectronic commerce, travel, intelligence information gathering and analysis,\nhealth care, digital government, etc. However, the technology to organize,\nsearch, integrate these Web Services has not kept pace with the rapid growth of\nthe available information space. The number of Web Services to be integrated\nmay be large and continuously changing. To ease and improve the process of Web\nservices discovery in an open environment like the Internet, it is suggested to\ngather similar Web services into groups known as communities. Although Web\nservices are intensively investigated, the community management issues have not\nbeen addressed yet In this paper we draw an overview of several Web services\nCommunities' management approaches based on some currently existing communities\nplatforms and frameworks. We also discuss different approaches for querying and\nselecting Web services under the umbrella of Web services communities'. We\ncompare the current approaches among each others with respect to some key\nrequirements."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.1255v1", 
    "other_authors": "Antoine Zimmermann, Nuno Lopes, Axel Polleres, Umberto Straccia", 
    "title": "A General Framework for Representing, Reasoning and Querying with   Annotated Semantic Web Data", 
    "arxiv-id": "1103.1255v1", 
    "author": "Umberto Straccia", 
    "publish": "2011-03-07T11:43:58Z", 
    "summary": "We describe a generic framework for representing and reasoning with annotated\nSemantic Web data, a task becoming more important with the recent increased\namount of inconsistent and non-reliable meta-data on the web. We formalise the\nannotated language, the corresponding deductive system and address the query\nanswering problem. Previous contributions on specific RDF annotation domains\nare encompassed by our unified reasoning formalism as we show by instantiating\nit on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we\nprovide a generic method for combining multiple annotation domains allowing to\nrepresent, e.g. temporally-annotated fuzzy RDF. Furthermore, we address the\ndevelopment of a query language -- AnQL -- that is inspired by SPARQL,\nincluding several features of SPARQL 1.1 (subqueries, aggregates, assignment,\nsolution modifiers) along with the formal definitions of their semantics."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.1367v1", 
    "other_authors": "Chao Li, Gerome Miklau", 
    "title": "Efficient Batch Query Answering Under Differential Privacy", 
    "arxiv-id": "1103.1367v1", 
    "author": "Gerome Miklau", 
    "publish": "2011-03-07T20:22:38Z", 
    "summary": "Differential privacy is a rigorous privacy condition achieved by randomizing\nquery answers. This paper develops efficient algorithms for answering multiple\nqueries under differential privacy with low error. We pursue this goal by\nadvancing a recent approach called the matrix mechanism, which generalizes\nstandard differentially private mechanisms. This new mechanism works by first\nanswering a different set of queries (a strategy) and then inferring the\nanswers to the desired workload of queries. Although a few strategies are known\nto work well on specific workloads, finding the strategy which minimizes error\non an arbitrary workload is intractable. We prove a new lower bound on the\noptimal error of this mechanism, and we propose an efficient algorithm that\napproaches this bound for a wide range of workloads."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.2406v1", 
    "other_authors": "Nilesh Dalvi, Ravi Kumar, Mohamed Soliman", 
    "title": "Automatic Wrappers for Large Scale Web Extraction", 
    "arxiv-id": "1103.2406v1", 
    "author": "Mohamed Soliman", 
    "publish": "2011-03-12T01:05:38Z", 
    "summary": "We present a generic framework to make wrapper induction algorithms tolerant\nto noise in the training data. This enables us to learn wrappers in a\ncompletely unsupervised manner from automatically and cheaply obtained noisy\ntraining data, e.g., using dictionaries and regular expressions. By removing\nthe site-level supervision that wrapper-based techniques require, we are able\nto perform information extraction at web-scale, with accuracy unattained with\nexisting unsupervised extraction techniques. Our system is used in production\nat Yahoo! and powers live applications."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.2410v1", 
    "other_authors": "Vibhor Rastogi, Nilesh Dalvi, Minos Garofalakis", 
    "title": "Large-Scale Collective Entity Matching", 
    "arxiv-id": "1103.2410v1", 
    "author": "Minos Garofalakis", 
    "publish": "2011-03-12T01:09:30Z", 
    "summary": "There have been several recent advancements in Machine Learning community on\nthe Entity Matching (EM) problem. However, their lack of scalability has\nprevented them from being applied in practical settings on large real-life\ndatasets. Towards this end, we propose a principled framework to scale any\ngeneric EM algorithm. Our technique consists of running multiple instances of\nthe EM algorithm on small neighborhoods of the data and passing messages across\nneighborhoods to construct a global solution. We prove formal properties of our\nframework and experimentally demonstrate the effectiveness of our approach in\nscaling EM algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.3103v1", 
    "other_authors": "Mohamed Yakout, Ahmed K. Elmagarmid, Jennifer Neville, Mourad Ouzzani, Ihab F. Ilyas", 
    "title": "Guided Data Repair", 
    "arxiv-id": "1103.3103v1", 
    "author": "Ihab F. Ilyas", 
    "publish": "2011-03-16T05:51:51Z", 
    "summary": "In this paper we present GDR, a Guided Data Repair framework that\nincorporates user feedback in the cleaning process to enhance and accelerate\nexisting automatic repair techniques while minimizing user involvement. GDR\nconsults the user on the updates that are most likely to be beneficial in\nimproving data quality. GDR also uses machine learning methods to identify and\napply the correct updates directly to the database without the actual\ninvolvement of the user on these specific updates. To rank potential updates\nfor consultation by the user, we first group these repairs and quantify the\nutility of each group using the decision-theory concept of value of information\n(VOI). We then apply active learning to order updates within a group based on\ntheir ability to improve the learned model. User feedback is used to repair the\ndatabase and to adaptively refine the training set for the model. We\nempirically evaluate GDR on a real-world dataset and show significant\nimprovement in data quality using our user guided repairing process. We also,\nassess the trade-off between the user efforts and the resulting data quality."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.3107v2", 
    "other_authors": "Mehmet Levent Koc, Christopher R\u00e9", 
    "title": "Incrementally Maintaining Classification using an RDBMS", 
    "arxiv-id": "1103.3107v2", 
    "author": "Christopher R\u00e9", 
    "publish": "2011-03-16T05:57:58Z", 
    "summary": "The proliferation of imprecise data has motivated both researchers and the\ndatabase industry to push statistical techniques into relational database\nmanagement systems (RDBMSs). We study algorithms to maintain model-based views\nfor a popular statistical technique, classification, inside an RDBMS in the\npresence of updates to the training examples. We make three technical\ncontributions: (1) An algorithm that incrementally maintains classification\ninside an RDBMS. (2) An analysis of the above algorithm that shows that our\nalgorithm is optimal among all deterministic algorithms (and asymptotically\nwithin a factor of 2 of a nondeterministic optimal). (3) An index structure\nbased on the technical ideas that underlie the above algorithm which allows us\nto store only a fraction of the entities in memory. We apply our techniques to\ntext processing, and we demonstrate that our algorithms provide several orders\nof magnitude improvement over non-incremental approaches to classification on a\nvariety of data sets: such as the Cora, UCI Machine Learning Repository data\nsets, Citeseer, and DBLife."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.3753v2", 
    "other_authors": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "title": "On the Scalability of Multidimensional Databases", 
    "arxiv-id": "1103.3753v2", 
    "author": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "publish": "2011-03-19T06:12:56Z", 
    "summary": "It is commonly accepted in the practice of on-line analytical processing of\ndatabases that the multidimensional database organization is less scalable than\nthe relational one. It is easy to see that the size of the multidimensional\norganization may increase very quickly. For example, if we introduce one\nadditional dimension, then the total number of possible cells will be at least\ndoubled. However, this reasoning does not takethe fact into account that the\nmultidimensional organization can be compressed. There are compression\ntechniques, which can remove all or at least a part of the empty cells from the\nmultidimensional organization, while maintaining a good retrieval performance.\nRelational databases often use B-tree indices to speed up the access to given\nrows of tables. It can be proven, under some reasonable assumptions, that the\ntotal size of the table and the B-tree index is bigger than a compressed\nmultidimensional representation. This implies that the compressed array results\nin a smaller database and faster access at the same time. This paper compares\nseveral compression techniques and shows when we should and should not apply\ncompressed arrays instead of relational tables."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.3857v2", 
    "other_authors": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "title": "Difference Sequence Compression of Multidimensional Databases", 
    "arxiv-id": "1103.3857v2", 
    "author": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "publish": "2011-03-20T15:35:03Z", 
    "summary": "The multidimensional databases often use compression techniques in order to\ndecrease the size of the database. This paper introduces a new method called\ndifference sequence compression. Under some conditions, this new technique is\nable to create a smaller size multidimensional database than others like single\ncount header compression, logical position compression or base-offset\ncompression. Keywords: compression, multidimensional database, On-line\nAnalytical Processing, OLAP."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2011.3106", 
    "link": "http://arxiv.org/pdf/1103.3863v3", 
    "other_authors": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "title": "Multidimensional or Relational? / How to Organize an On-line Analytical   Processing Database", 
    "arxiv-id": "1103.3863v3", 
    "author": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "publish": "2011-03-20T17:02:03Z", 
    "summary": "In the past few years, the number of OLAP applications increased quickly.\nThese applications use two significantly different DB structures:\nmultidimensional (MD) and table-based. One can show that the traditional model\nof relational databases cannot make difference between these two structures.\nAnother model is necessary to make the differences visible. One of these is the\nspeed of the system. It can be proven that the multidimensional DB organization\nresults in shorter response times. And it is crucial, since a manager may\nbecome impatient, if he or she has to wait say more than 20 seconds for the\nnext screen. On the other hand, we have to pay for the speed with a bigger DB\nsize. Why does the size of MD databases grow so quickly? The reason is the\nsparsity of data: The MD matrix contains many empty cells. Efficient handling\nof sparse matrices is indispensable in an OLAP application. One way to handle\nsparsity is to take the structure closer to the table-based one. Thus the DB\nsize decreases, while the application gets slower. Therefore, other methods are\nneeded. This paper deals with the comparison of the two DB structures and the\nlimits of their usage. The new results of the paper: (1) It gives a\nconstructive proof that all relations can be represented in MD arrays. (2) It\nalso shows when the MD array representation is quicker than the table-based\none. (3) The MD representation results in smaller DB size under some\nconditions. One such sufficient condition is proved in the paper. (4) A\nvariation of the single count header compression scheme is described with an\nalgorithm, which creates the compressed array from the ordered table without\nmaterializing the uncompressed array. (5) The speed of the two different\ndatabase organizations is tested with experiments, as well. The tests are done\non benchmark as well as real life data. The experiments support the theoretical\nresults."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.4168v2", 
    "other_authors": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "title": "Caching in Multidimensional Databases", 
    "arxiv-id": "1103.4168v2", 
    "author": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "publish": "2011-03-21T22:07:08Z", 
    "summary": "One utilisation of multidimensional databases is the field of On-line\nAnalytical Processing (OLAP). The applications in this area are designed to\nmake the analysis of shared multidimensional information fast [9]. On one hand,\nspeed can be achieved by specially devised data structures and algorithms. On\nthe other hand, the analytical process is cyclic. In other words, the user of\nthe OLAP application runs his or her queries one after the other. The output of\nthe last query may be there (at least partly) in one of the previous results.\nTherefore caching also plays an important role in the operation of these\nsystems. However, caching itself may not be enough to ensure acceptable\nperformance. Size does matter: The more memory is available, the more we gain\nby loading and keeping information in there. Oftentimes, the cache size is\nfixed. This limits the performance of the multidimensional database, as well,\nunless we compress the data in order to move a greater proportion of them into\nthe memory. Caching combined with proper compression methods promise further\nperformance improvements. In this paper, we investigate how caching influences\nthe speed of OLAP systems. Different physical representations (multidimensional\nand table) are evaluated. For the thorough comparison, models are proposed. We\ndraw conclusions based on these models, and the conclusions are verified with\nempirical data."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.4169v3", 
    "other_authors": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "title": "Difference-Huffman Coding of Multidimensional Databases", 
    "arxiv-id": "1103.4169v3", 
    "author": "Istv\u00e1n Sz\u00e9pk\u00fati", 
    "publish": "2011-03-21T22:12:10Z", 
    "summary": "A new compression method called difference-Huffman coding (DHC) is introduced\nin this paper. It is verified empirically that DHC results in a smaller\nmultidimensional physical representation than those for other previously\npublished techniques (single count header compression, logical position\ncompression, base-offset compression and difference sequence compression). The\narticle examines how caching influences the expected retrieval time of the\nmultidimensional and table representations of relations. A model is proposed\nfor this, which is then verified with empirical data. Conclusions are drawn,\nbased on the model and the experiment, about when one physical representation\noutperforms another in terms of retrieval time. Over the tested range of\navailable memory, the performance for the multidimensional representation was\nalways much quicker than for the table representation."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.4410v1", 
    "other_authors": "Zhao Cao, Charles Sutton, Yanlei Diao, Prashant Shenoy", 
    "title": "Distributed Inference and Query Processing for RFID Tracking and   Monitoring", 
    "arxiv-id": "1103.4410v1", 
    "author": "Prashant Shenoy", 
    "publish": "2011-03-22T23:23:20Z", 
    "summary": "In this paper, we present the design of a scalable, distributed stream\nprocessing system for RFID tracking and monitoring. Since RFID data lacks\ncontainment and location information that is key to query processing, we\npropose to combine location and containment inference with stream query\nprocessing in a single architecture, with inference as an enabling mechanism\nfor high-level query processing. We further consider challenges in\ninstantiating such a system in large distributed settings and design techniques\nfor distributed inference and query processing. Our experimental results, using\nboth real-world data and large synthetic traces, demonstrate the accuracy,\nefficiency, and scalability of our proposed techniques."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.4916v2", 
    "other_authors": "B. G. Kodge, P. S. Hiremath", 
    "title": "Detection of Spatial Changes using Spatial Data Mining", 
    "arxiv-id": "1103.4916v2", 
    "author": "P. S. Hiremath", 
    "publish": "2011-03-25T07:10:50Z", 
    "summary": "The Change detection based on analysis and samples are analyzed. Land\nuse/cover change detection based on SDM is discussed."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.4979v3", 
    "other_authors": "K. Viswanathan Iyer", 
    "title": "An Introduction to Functional dependency in Relational Databases", 
    "arxiv-id": "1103.4979v3", 
    "author": "K. Viswanathan Iyer", 
    "publish": "2011-03-25T14:24:18Z", 
    "summary": "This write-up is the suggested lecture notes for a second level course on\nadvanced topics in database systems for master's students of Computer Science\nwith a theoretical focus. A prerequisite in algorithms and an exposure to\ndatabase systems are required. Additional reading may require exposure to\nmathematical logic. The starting point for these notes are from M.Y.Vardi's\nsurvey listed herein as a reference - some of the proofs are presented as such\n. This select rewrite on functional dependency is intended to provide a few\nclarifications even though radically new design approaches are now being\nproposed."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.5170v3", 
    "other_authors": "Graham Cormode, Magda Procopiuc, Entong Shen, Divesh Srivastava, Ting Yu", 
    "title": "Differentially Private Spatial Decompositions", 
    "arxiv-id": "1103.5170v3", 
    "author": "Ting Yu", 
    "publish": "2011-03-26T22:47:47Z", 
    "summary": "Differential privacy has recently emerged as the de facto standard for\nprivate data release. This makes it possible to provide strong theoretical\nguarantees on the privacy and utility of released data. While it is well-known\nhow to release data based on counts and simple functions under this guarantee,\nit remains to provide general purpose techniques to release different kinds of\ndata. In this paper, we focus on spatial data such as locations and more\ngenerally any data that can be indexed by a tree structure. Directly applying\nexisting differential privacy methods to this type of data simply generates\nnoise. Instead, we introduce a new class of \"private spatial decompositions\":\nthese adapt standard spatial indexing methods such as quadtrees and kd-trees to\nprovide a private description of the data distribution. Equipping such\nstructures with differential privacy requires several steps to ensure that they\nprovide meaningful privacy guarantees. Various primitives, such as choosing\nsplitting points and describing the distribution of points within a region,\nmust be done privately, and the guarantees of the different building blocks\ncomposed to provide an overall guarantee. Consequently, we expose the design\nspace for private spatial decompositions, and analyze some key examples. Our\nexperimental study demonstrates that it is possible to build such\ndecompositions efficiently, and use them to answer a variety of queries\nprivately with high accuracy."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1103.5795v1", 
    "other_authors": "M. Shahriar Hossain, Rafal A. Angryk", 
    "title": "Heuristic Algorithm for Interpretation of Non-Atomic Categorical   Attributes in Similarity-based Fuzzy Databases - Scalability Evaluation", 
    "arxiv-id": "1103.5795v1", 
    "author": "Rafal A. Angryk", 
    "publish": "2011-03-29T23:46:06Z", 
    "summary": "In this work we are analyzing scalability of the heuristic algorithm we used\nin the past to discover knowledge from multi-valued symbolic attributes in\nfuzzy databases. The non-atomic descriptors, characterizing a single attribute\nof a database record, are commonly used in fuzzy databases to reflect\nuncertainty about the recorded observation. In this paper, we present\nimplementation details and scalability tests of the algorithm, which we\ndeveloped to precisely interpret such non-atomic values and to transfer (i.e.\ndefuzzify) the fuzzy tuples to the forms acceptable for many regular (i.e.\natomic values based) data mining algorithms. Important advantages of our\napproach are: (1) its linear scalability, and (2) its unique capability of\nincorporating background knowledge, implicitly stored in the fuzzy database\nmodels in the form of fuzzy similarity hierarchy, into the\ninterpretation/defuzzification process."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.0049v1", 
    "other_authors": "Anup Patel, Niveeta Sharma, Magdalini Eirinaki", 
    "title": "Negative Database for Data Security", 
    "arxiv-id": "1105.0049v1", 
    "author": "Magdalini Eirinaki", 
    "publish": "2011-04-30T05:19:42Z", 
    "summary": "Data Security is a major issue in any web-based application. There have been\napproaches to handle intruders in any system, however, these approaches are not\nfully trustable; evidently data is not totally protected. Real world databases\nhave information that needs to be securely stored. The approach of generating\nnegative database could help solve such problem. A Negative Database can be\ndefined as a database that contains huge amount of data consisting of\ncounterfeit data along with the real data. Intruders may be able to get access\nto such databases, but, as they try to extract information, they will retrieve\ndata sets that would include both the actual and the negative data. In this\npaper we present our approach towards implementing the concept of negative\ndatabase to help prevent data theft from malicious users and provide efficient\ndata retrieval for all valid users."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.0350v1", 
    "other_authors": "C. Ramya, G. Kavitha, Dr. K. S. Shreedhara", 
    "title": "Preprocessing: A Prerequisite for Discovering Patterns in Web Usage   Mining Process", 
    "arxiv-id": "1105.0350v1", 
    "author": "Dr. K. S. Shreedhara", 
    "publish": "2011-05-02T15:07:34Z", 
    "summary": "Web log data is usually diverse and voluminous. This data must be assembled\ninto a consistent, integrated and comprehensive view, in order to be used for\npattern discovery. Without properly cleaning, transforming and structuring the\ndata prior to the analysis, one cannot expect to find meaningful patterns. As\nin most data mining applications, data preprocessing involves removing and\nfiltering redundant and irrelevant data, removing noise, transforming and\nresolving any inconsistencies. In this paper, a complete preprocessing\nmethodology having merging, data cleaning, user/session identification and data\nformatting and summarization activities to improve the quality of data by\nreducing the quantity of data has been proposed. To validate the efficiency of\nthe proposed preprocessing methodology, several experiments are conducted and\nthe results show that the proposed methodology reduces the size of Web access\nlog files down to 73-82% of the initial size and offers richer logs that are\nstructured for further stages of Web Usage Mining (WUM). So preprocessing of\nraw data in this WUM process is the central theme of this paper."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.1930v1", 
    "other_authors": "Anisoara Nica, Fabian Suchanek, Aparna Varde", 
    "title": "Emerging multidisciplinary research across database management systems", 
    "arxiv-id": "1105.1930v1", 
    "author": "Aparna Varde", 
    "publish": "2011-05-10T12:36:17Z", 
    "summary": "The database community is exploring more and more multidisciplinary avenues:\nData semantics overlaps with ontology management; reasoning tasks venture into\nthe domain of artificial intelligence; and data stream management and\ninformation retrieval shake hands, e.g., when processing Web click-streams.\nThese new research avenues become evident, for example, in the topics that\ndoctoral students choose for their dissertations. This paper surveys the\nemerging multidisciplinary research by doctoral students in database systems\nand related areas. It is based on the PIKM 2010, which is the 3rd Ph.D.\nworkshop at the International Conference on Information and Knowledge\nManagement (CIKM). The topics addressed include ontology development, data\nstreams, natural language processing, medical databases, green energy, cloud\ncomputing, and exploratory search. In addition to core ideas from the workshop,\nwe list some open research questions in these multidisciplinary areas."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.1950v1", 
    "other_authors": "Mahnoosh Kholghi, Mohammadreza Keyvanpour", 
    "title": "An analytical framework for data stream mining techniques based on   challenges and requirements", 
    "arxiv-id": "1105.1950v1", 
    "author": "Mohammadreza Keyvanpour", 
    "publish": "2011-05-10T13:58:10Z", 
    "summary": "A growing number of applications that generate massive streams of data need\nintelligent data processing and online analysis. Real-time surveillance\nsystems, telecommunication systems, sensor networks and other dynamic\nenvironments are such examples. The imminent need for turning such data into\nuseful information and knowledge augments the development of systems,\nalgorithms and frameworks that address streaming challenges. The storage,\nquerying and mining of such data sets are highly computationally challenging\ntasks. Mining data streams is concerned with extracting knowledge structures\nrepresented in models and patterns in non stopping streams of information.\nGenerally, two main challenges are designing fast mining methods for data\nstreams and need to promptly detect changing concepts and data distribution\nbecause of highly dynamic nature of data streams. The goal of this article is\nto analyze and classify the application of diverse data mining techniques in\ndifferent challenges of data stream mining. In this paper, we present the\ntheoretical foundations of data stream analysis and propose an analytical\nframework for data stream mining techniques."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.2255v1", 
    "other_authors": "Yael Amsterdamer, Daniel Deutch, Val Tannen", 
    "title": "On the Limitations of Provenance for Queries With Difference", 
    "arxiv-id": "1105.2255v1", 
    "author": "Val Tannen", 
    "publish": "2011-05-11T17:22:32Z", 
    "summary": "The annotation of the results of database transformations was shown to be\nvery effective for various applications. Until recently, most works in this\ncontext focused on positive query languages. The provenance semirings is a\nparticular approach that was proven effective for these languages, and it was\nshown that when propagating provenance with semirings, the expected equivalence\naxioms of the corresponding query languages are satisfied. There have been\nseveral attempts to extend the framework to account for relational algebra\nqueries with difference. We show here that these suggestions fail to satisfy\nsome expected equivalence axioms (that in particular hold for queries on\n\"standard\" set and bag databases). Interestingly, we show that this is not a\npitfall of these particular attempts, but rather every such attempt is bound to\nfail in satisfying these axioms, for some semirings. Finally, we show\nparticular semirings for which an extension for supporting difference is\n(im)possible."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.4251v1", 
    "other_authors": "Hoa Nguyen, Ariel Fuxman, Stelios Paparizos, Juliana Freire, Rakesh Agrawal", 
    "title": "Synthesizing Products for Online Catalogs", 
    "arxiv-id": "1105.4251v1", 
    "author": "Rakesh Agrawal", 
    "publish": "2011-05-21T12:06:39Z", 
    "summary": "A high-quality, comprehensive product catalog is essential to the success of\nProduct Search engines and shopping sites such as Yahoo! Shopping, Google\nProduct Search or Bing Shopping. But keeping catalogs up-to-date becomes a\nchallenging task, calling for the need of automated techniques. In this paper,\nwe introduce the problem of product synthesis, a key component of catalog\ncreation and maintenance. Given a set of offers advertised by merchants, the\ngoal is to identify new products and add them to the catalog together with\ntheir (structured) attributes. A fundamental challenge is the scale of the\nproblem: a Product Search engine receives data from thousands of merchants and\nmillions of products; the product taxonomy contains thousands of categories,\nwhere each category comes in a different schema; and merchants use\nrepresentations for products that are different from the ones used in the\ncatalog of the Product Search engine.\n  We propose a system that provides an end-to-end solution to the product\nsynthesis problem, and includes components for extraction, and addresses issues\ninvolved in data extraction from offers, schema reconciliation, and data\nfusion. We developed a novel and scalable technique for schema matching which\nleverages knowledge about previously-known instance-level associations between\noffers and products; and it is trained using automatically created training\nsets (no manually-labeled data is needed). We present an experimental\nevaluation of our system using data from Bing Shopping for more than 800K\noffers, a thousand merchants, and 400 categories. The evaluation confirms that\nour approach is able to automatically generate a large number of accurate\nproduct specifications, and that our schema reconciliation component\noutperforms state-of-the-art schema matching techniques in terms of precision\nand recall."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.4253v1", 
    "other_authors": "David Lomet, Kostas Tzoumas, Michael Zwilling", 
    "title": "Implementing Performance Competitive Logical Recovery", 
    "arxiv-id": "1105.4253v1", 
    "author": "Michael Zwilling", 
    "publish": "2011-05-21T12:08:23Z", 
    "summary": "New hardware platforms, e.g. cloud, multi-core, etc., have led to a\nreconsideration of database system architecture. Our Deuteronomy project\nseparates transactional functionality from data management functionality,\nenabling a flexible response to exploiting new platforms. This separation\nrequires, however, that recovery is described logically. In this paper, we\nextend current recovery methods to work in this logical setting. While this is\nstraightforward in principle, performance is an issue. We show how ARIES style\nrecovery optimizations can work for logical recovery where page information is\nnot captured on the log. In side-by-side performance experiments using a common\nlog, we compare logical recovery with a state-of-the art ARIES style recovery\nimplementation and show that logical redo performance can be competitive."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.4395v1", 
    "other_authors": "Wolfgang Gatterbauer, Alexandra Meliou, Dan Suciu", 
    "title": "Default-all is dangerous!", 
    "arxiv-id": "1105.4395v1", 
    "author": "Dan Suciu", 
    "publish": "2011-05-23T03:19:25Z", 
    "summary": "We show that the default-all propagation scheme for database annotations is\ndangerous. Dangerous here means that it can propagate annotations to the query\noutput which are semantically irrelevant to the query the user asked. This is\nthe result of considering all relationally equivalent queries and returning the\nunion of their where-provenance in an attempt to define a propagation scheme\nthat is insensitive to query rewriting. We propose an alternative\nquery-rewrite-insensitive (QRI) where-provenance called minimum propagation. It\nis analogous to the minimum witness basis for why-provenance, straight-forward\nto evaluate, and returns all relevant and only relevant annotations."
},{
    "category": "cs.DB", 
    "doi": "10.3311/pp.ee.2007-3-4.06", 
    "link": "http://arxiv.org/pdf/1105.5951v1", 
    "other_authors": "Muhammad Tayyab Shahzad, Muhammad Rizwan", 
    "title": "Performance of Short-Commit in Extreme Database Environment", 
    "arxiv-id": "1105.5951v1", 
    "author": "Muhammad Rizwan", 
    "publish": "2011-05-30T11:58:42Z", 
    "summary": "Atomic commit protocols are used where data integrity is more important than\ndata availability. Two-Phase commit (2PC) is a standard commit protocol for\ncommercial database management systems. To reduce certain drawbacks in 2PC\nprotocol people have suggested different variance of this protocol.\nShort-Commit protocol is developed with an objective to achieve low cost\ntransaction commitment cost with non-blocking capability. In this paper we have\nbriefly explained short-commit protocol executing pattern. Experimental\nanalysis and results are presented to support the claim that short-commit can\nwork efficiently in extreme database environment."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1105.6001v4", 
    "other_authors": "Antonio Badia, Daniel Lemire", 
    "title": "A Call to Arms: Revisiting Database Design", 
    "arxiv-id": "1105.6001v4", 
    "author": "Daniel Lemire", 
    "publish": "2011-05-30T14:07:10Z", 
    "summary": "Good database design is crucial to obtain a sound, consistent database, and -\nin turn - good database design methodologies are the best way to achieve the\nright design. These methodologies are taught to most Computer Science\nundergraduates, as part of any Introduction to Database class. They can be\nconsidered part of the \"canon\", and indeed, the overall approach to database\ndesign has been unchanged for years. Moreover, none of the major database\nresearch assessments identify database design as a strategic research\ndirection.\n  Should we conclude that database design is a solved problem?\n  Our thesis is that database design remains a critical unsolved problem.\nHence, it should be the subject of more research. Our starting point is the\nobservation that traditional database design is not used in practice - and if\nit were used it would result in designs that are not well adapted to current\nenvironments. In short, database design has failed to keep up with the times.\nIn this paper, we put forth arguments to support our viewpoint, analyze the\nroot causes of this situation and suggest some avenues of research."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.0186v3", 
    "other_authors": "Yuan Hong, Jaideep Vaidya, Haibing Lu, Mingrui Wu", 
    "title": "Differentially Private Search Log Sanitization with Optimal Output   Utility", 
    "arxiv-id": "1108.0186v3", 
    "author": "Mingrui Wu", 
    "publish": "2011-07-31T15:55:48Z", 
    "summary": "Web search logs contain extremely sensitive data, as evidenced by the recent\nAOL incident. However, storing and analyzing search logs can be very useful for\nmany purposes (i.e. investigating human behavior). Thus, an important research\nquestion is how to privately sanitize search logs. Several search log\nanonymization techniques have been proposed with concrete privacy models.\nHowever, in all of these solutions, the output utility of the techniques is\nonly evaluated rather than being maximized in any fashion. Indeed, for\neffective search log anonymization, it is desirable to derive the optimal\n(maximum utility) output while meeting the privacy standard. In this paper, we\npropose utility-maximizing sanitization based on the rigorous privacy standard\nof differential privacy, in the context of search logs. Specifically, we\nutilize optimization models to maximize the output utility of the sanitization\nfor different applications, while ensuring that the production process\nsatisfies differential privacy. An added benefit is that our novel\nrandomization strategy ensures that the schema of the output is identical to\nthat of the input. A comprehensive evaluation on real search logs validates the\napproach and demonstrates its robustness and scalability."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.0729v1", 
    "other_authors": "Eduardo Cunha de Almeida", 
    "title": "Estudo de Viabilidade de uma Plataforma de Baixo Custo para Data   Warehouse", 
    "arxiv-id": "1108.0729v1", 
    "author": "Eduardo Cunha de Almeida", 
    "publish": "2011-08-03T02:41:02Z", 
    "summary": "Often corporations need tools to improve their decision making in a\ncompetitive market. In general, these tools are based on data warehouse\nplatforms to mange and analyze large amounts of data. However, several of these\ncorporations do not have enough resources to buy such platforms because of the\nhigh cost. This work is dedicated to a feasibility study of a low cost platform\nto data warehouse. We consider as a low cost platform the use of open source\nsoftware like the PostgreSQL database system and the GNU/Linux operational\nsystem. We verify the feasibility of this platform by executing two benchmarks\nthat simulate a data warehouse workload. The workload reproduces a multi-user\nenvironment with the execution of complex queries, which executes:\naggregations, nested sub queries, multi joins, in-line views and more.\nConsidering the results we were able to highlight some problems on the\nPostgreSQL database system, and discuss improvements in the context of data\nwarehouse."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.0831v1", 
    "other_authors": "Pablo Bisceglia, Leticia Gomez, Alejandro Vaisman", 
    "title": "Towards Spatio-Temporal SOLAP", 
    "arxiv-id": "1108.0831v1", 
    "author": "Alejandro Vaisman", 
    "publish": "2011-08-03T12:36:23Z", 
    "summary": "The integration of Geographic Information Systems (GIS) and On-Line\nAnalytical Processing (OLAP), denoted SOLAP, is aimed at exploring and\nanalyzing spatial data. In real-world SOLAP applications, spatial and\nnon-spatial data are subject to changes. In this paper we present a temporal\nquery language for SOLAP, called TPiet-QL, supporting so-called discrete\nchanges (for example, in land use or cadastral applications there are\nsituations where parcels are merged or split). TPiet-QL allows expressing\nintegrated GIS-OLAP queries in an scenario where spatial objects change across\ntime."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.1925v1", 
    "other_authors": "Eric Peukert, Julian Eberius, Erhard Rahm", 
    "title": "Rule-based Construction of Matching Processes", 
    "arxiv-id": "1108.1925v1", 
    "author": "Erhard Rahm", 
    "publish": "2011-08-09T13:40:29Z", 
    "summary": "Mapping complex metadata structures is crucial in a number of domains such as\ndata integration, ontology alignment or model management. To speed up that\nprocess automatic matching systems were developed to compute mapping\nsuggestions that can be corrected by a user. However, constructing and tuning\nmatch strategies still requires a high manual effort by matching experts as\nwell as correct mappings to evaluate generated mappings. We therefore propose a\nself-configuring schema matching system that is able to automatically adapt to\nthe given mapping problem at hand. Our approach is based on analyzing the input\nschemas as well as intermediate matching results. A variety of matching rules\nuse the analysis results to automatically construct and adapt an underlying\nmatching process for a given match task. We comprehensively evaluate our\napproach on different mapping problems from the schema, ontology and model\nmanagement domains. The evaluation shows that our system is able to robustly\nreturn good quality mappings across different mapping problems and domains."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.4596v1", 
    "other_authors": "Benjamin Nguyen, Antoine Vion, Fran\u00e7ois-Xavier Dudouet, Dario Colazzo, Ioana Manolescu, Pierre Senellart", 
    "title": "XML content warehousing: Improving sociological studies of mailing lists   and web data", 
    "arxiv-id": "1108.4596v1", 
    "author": "Pierre Senellart", 
    "publish": "2011-08-23T13:31:56Z", 
    "summary": "In this paper, we present the guidelines for an XML-based approach for the\nsociological study of Web data such as the analysis of mailing lists or\ndatabases available online. The use of an XML warehouse is a flexible solution\nfor storing and processing this kind of data. We propose an implemented\nsolution and show possible applications with our case study of profiles of\nexperts involved in W3C standard-setting activity. We illustrate the\nsociological use of semi-structured databases by presenting our XML Schema for\nmailing-list warehousing. An XML Schema allows many adjunctions or crossings of\ndata sources, without modifying existing data sets, while allowing possible\nstructural evolution. We also show that the existence of hidden data implies\nincreased complexity for traditional SQL users. XML content warehousing allows\naltogether exhaustive warehousing and recursive queries through contents, with\nfar less dependence on the initial storage. We finally present the possibility\nof exporting the data stored in the warehouse to commonly-used advanced\nsoftware devoted to sociological analysis."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.5253v1", 
    "other_authors": "Bay Vo, Bac Le", 
    "title": "A Frequent Closed Itemsets Lattice-based Approach for Mining Minimal   Non-Redundant Association Rules", 
    "arxiv-id": "1108.5253v1", 
    "author": "Bac Le", 
    "publish": "2011-08-26T08:07:22Z", 
    "summary": "There are many algorithms developed for improvement the time of mining\nfrequent itemsets (FI) or frequent closed itemsets (FCI). However, the\nalgorithms which deal with the time of generating association rules were not\nput in deep research. In reality, in case of a database containing many FI/FCI\n(from ten thousands up to millions), the time of generating association rules\nis much larger than that of mining FI/FCI. Therefore, this paper presents an\napplication of frequent closed itemsets lattice (FCIL) for mining minimal\nnon-redundant association rules (MNAR) to reduce a lot of time for generating\nrules. Firstly, we use CHARM-L for building FCIL. After that, based on FCIL, an\nalgorithm for fast generating MNAR will be proposed. Experimental results show\nthat the proposed algorithm is much faster than frequent itemsets lattice-based\nalgorithm in the mining time."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.5451v1", 
    "other_authors": "Andreas Behrend", 
    "title": "A Uniform Fixpoint Approach to the Implementation of Inference Methods   for Deductive Databases", 
    "arxiv-id": "1108.5451v1", 
    "author": "Andreas Behrend", 
    "publish": "2011-08-27T14:09:08Z", 
    "summary": "Within the research area of deductive databases three different database\ntasks have been deeply investigated: query evaluation, update propagation and\nview updating. Over the last thirty years various inference mechanisms have\nbeen proposed for realizing these main functionalities of a rule-based system.\nHowever, these inference mechanisms have been rarely used in commercial DB\nsystems until now. One important reason for this is the lack of a uniform\napproach well-suited for implementation in an SQL-based system. In this paper,\nwe present such a uniform approach in form of a new version of the soft\nconsequence operator. Additionally, we present improved transformation-based\napproaches to query optimization and update propagation and view updating which\nare all using this operator as underlying evaluation mechanism."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.5592v1", 
    "other_authors": "Abhishek Taneja, R. K. Chauhan", 
    "title": "A Performance Study of Data Mining Techniques: Multiple Linear   Regression vs. Factor Analysis", 
    "arxiv-id": "1108.5592v1", 
    "author": "R. K. Chauhan", 
    "publish": "2011-08-26T07:08:13Z", 
    "summary": "The growing volume of data usually creates an interesting challenge for the\nneed of data analysis tools that discover regularities in these data. Data\nmining has emerged as disciplines that contribute tools for data analysis,\ndiscovery of hidden knowledge, and autonomous decision making in many\napplication domains. The purpose of this study is to compare the performance of\ntwo data mining techniques viz., factor analysis and multiple linear regression\nfor different sample sizes on three unique sets of data. The performance of the\ntwo data mining techniques is compared on following parameters like mean square\nerror (MSE), R-square, R-Square adjusted, condition number, root mean square\nerror(RMSE), number of variables included in the prediction model, modified\ncoefficient of efficiency, F-value, and test of normality. These parameters\nhave been computed using various data mining tools like SPSS, XLstat, Stata,\nand MS-Excel. It is seen that for all the given dataset, factor analysis\noutperform multiple linear regression. But the absolute value of prediction\naccuracy varied between the three datasets indicating that the data\ndistribution and data characteristics play a major role in choosing the correct\nprediction technique."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.5619v1", 
    "other_authors": "Karanjit Singh, Shuchita Bhasin", 
    "title": "Modification of GTD from Flat File Format to OLAP for Data Mining", 
    "arxiv-id": "1108.5619v1", 
    "author": "Shuchita Bhasin", 
    "publish": "2011-08-26T07:06:03Z", 
    "summary": "This document is part of original research work by the authors in a bid to\nexplore new fields for applying Data Mining Techniques. The sample data is part\nof a large data set from University of Maryland (UMD) and outlines how more\nmeaningful patterns can be discovered by preprocessing the data in the form of\nOLAP cubes."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2070736.2070750", 
    "link": "http://arxiv.org/pdf/1108.6328v5", 
    "other_authors": "Olaf Hartig, Johann-Christoph Freytag", 
    "title": "Foundations of Traversal Based Query Execution over Linked Data   (Extended Version)", 
    "arxiv-id": "1108.6328v5", 
    "author": "Johann-Christoph Freytag", 
    "publish": "2011-08-31T19:22:48Z", 
    "summary": "Query execution over the Web of Linked Data has attracted much attention\nrecently. A particularly interesting approach is link traversal based query\nexecution which proposes to integrate the traversal of data links into the\nconstruction of query results. Hence -in contrast to traditional query\nexecution paradigms- this approach does not assume a fixed set of relevant data\nsources beforehand; instead, it discovers data on the fly and, thus, enables\napplications to tap the full potential of the Web.\n  While several authors study possibilities to implement the idea of link\ntraversal based query execution and to optimize query execution in this\ncontext, no work exists that discusses the theoretical foundations of the\napproach in general. Our paper fills this gap.\n  We introduce a well-defined semantics for queries that may be executed using\nthe link traversal based approach. Based on this semantics we formally analyze\nproperties of such queries. In particular, we study the computability of\nqueries as well as the implications of querying a potentially infinite Web of\nLinked Data. Our results show that query computation in general is not\nguaranteed to terminate and that for any given query it is undecidable whether\nthe execution terminates. Furthermore, we define an abstract execution model\nthat captures the integration of link traversal into the query execution\nprocess. Based on this model we prove the soundness and completeness of link\ntraversal based query execution and analyze an existing implementation\napproach.."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1999320.1999337", 
    "link": "http://arxiv.org/pdf/1110.0207v1", 
    "other_authors": "Alain Tamayo, Carlos Granell, Joaqu\u00edn Huerta", 
    "title": "Analysing complexity of XML Schemas in geospatial web services", 
    "arxiv-id": "1110.0207v1", 
    "author": "Joaqu\u00edn Huerta", 
    "publish": "2011-10-02T17:49:47Z", 
    "summary": "XML Schema is the language used to define the structure of messages exchanged\nbetween OGC-based web service clients and providers. The size of these schemas\nhas been growing with time, reaching a state that makes its understanding and\neffective application a hard task. A first step to cope with this situation is\nto provide different ways to measure the complexity of the schemas. In this\nregard, we present in this paper an analysis of the complexity of XML schemas\nin OGC web services. We use a group of metrics found in the literature and\nintroduce new metrics to measure size and/or complexity of these schemas. The\nuse of adequate metrics allows us to quantify the complexity, quality and other\nproperties of the schemas, which can be very useful in different scenarios."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1999320.1999336", 
    "link": "http://arxiv.org/pdf/1110.0209v1", 
    "other_authors": "Alain Tamayo, Carlos Granell, Joaqu\u00edn Huerta", 
    "title": "Dealing with large schema sets in mobile SOS-based applications", 
    "arxiv-id": "1110.0209v1", 
    "author": "Joaqu\u00edn Huerta", 
    "publish": "2011-10-02T18:08:34Z", 
    "summary": "Although the adoption of OGC Web Services for server, desktop and web\napplications has been successful, its penetration in mobile devices has been\nslow. One of the main reasons is the performance problems associated with XML\nprocessing as it consumes a lot of memory and processing time, which are scarce\nresources in a mobile device. In this paper we propose an algorithm to generate\nefficient code for XML data binding for mobile SOS-based applications. The\nalgorithm take advantage of the fact that individual implementations use only\nsome portions of the standards' schemas, which allows the simplification of\nlarge XML schema sets in an application-specific manner by using a subset of\nXML instance files conforming to these schemas."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1999320.1999336", 
    "link": "http://arxiv.org/pdf/1110.1700v1", 
    "other_authors": "Shirin Mohammadi, Ali A. Safaei, Fatemeh Abdi, Mostafa S. Haghjoo", 
    "title": "Adaptive Data Stream Management System Using Learning Automata", 
    "arxiv-id": "1110.1700v1", 
    "author": "Mostafa S. Haghjoo", 
    "publish": "2011-10-08T06:05:02Z", 
    "summary": "In many modern applications, data are received as infinite, rapid,\nunpredictable and time- variant data elements that are known as data streams.\nSystems which are able to process data streams with such properties are called\nData Stream Management Systems (DSMS). Due to the unpredictable and time-\nvariant properties of data streams as well as system, adaptivity of the DSMS is\na major requirement for each DSMS. Accordingly, determining parameters which\nare effective on the most important performance metric of a DSMS (i.e.,\nresponse time) and analysing them will affect on designing an adaptive DSMS. In\nthis paper, effective parameters on response time of DSMS are studied and\nanalysed and a solution is proposed for DSMSs' adaptivity. The proposed\nadaptive DSMS architecture includes a learning unit that frequently evaluates\nsystem to adjust the optimal value for each of tuneable effective. Learning\nAutomata is used as the learning mechanism of the learning unit to adjust the\nvalue of tuneable effective parameters. So, when system faces some changes, the\nlearning unit increases performance by tuning each of tuneable effective\nparameters to its optimum value. Evaluation results illustrate that after a\nwhile, parameters reach their optimum value and then DSMS's adaptivity will be\nimproved considerably."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1966895.1966897", 
    "link": "http://arxiv.org/pdf/1110.1729v1", 
    "other_authors": "L\u00e1szl\u00f3 Dobos, Alexander Szalay, Jos\u00e9 Blakeley, Tam\u00e1s Budav\u00e1ri, Istv\u00e1n Csabai, Dragan Tomic, Milos Milovanovic, Marko Tintor, Andrija Jovanovic", 
    "title": "Array Requirements for Scientific Applications and an Implementation for   Microsoft SQL Server", 
    "arxiv-id": "1110.1729v1", 
    "author": "Andrija Jovanovic", 
    "publish": "2011-10-08T12:16:48Z", 
    "summary": "This paper outlines certain scenarios from the fields of astrophysics and\nfluid dynamics simulations which require high performance data warehouses that\nsupport array data type. A common feature of all these use cases is that\nsubsetting and preprocessing the data on the server side (as far as possible\ninside the database server process) is necessary to avoid the client-server\noverhead and to minimize IO utilization. Analyzing and summarizing the\nrequirements of the various fields help software engineers to come up with a\ncomprehensive design of an array extension to relational database systems that\ncovers a wide range of scientific applications. We also present a working\nimplementation of an array data type for Microsoft SQL Server 2008 to support\nlarge-scale scientific applications. We introduce the design of the array type,\nresults from a performance evaluation, and discuss the lessons learned from\nthis implementation. The library can be downloaded from our website at\nhttp://voservices.net/sqlarray/"
},{
    "category": "cs.DB", 
    "doi": "10.1145/1966895.1966897", 
    "link": "http://arxiv.org/pdf/1110.2704v1", 
    "other_authors": "Huu Hoa Nguyen, Nouria Harbi, J\u00e9r\u00f4me Darmont", 
    "title": "An Efficient Fuzzy Clustering-Based Approach for Intrusion Detection", 
    "arxiv-id": "1110.2704v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2011-10-12T17:06:12Z", 
    "summary": "The need to increase accuracy in detecting sophisticated cyber attacks poses\na great challenge not only to the research community but also to corporations.\nSo far, many approaches have been proposed to cope with this threat. Among\nthem, data mining has brought on remarkable contributions to the intrusion\ndetection problem. However, the generalization ability of data mining-based\nmethods remains limited, and hence detecting sophisticated attacks remains a\ntough task. In this thread, we present a novel method based on both clustering\nand classification for developing an efficient intrusion detection system\n(IDS). The key idea is to take useful information exploited from fuzzy\nclustering into account for the process of building an IDS. To this aim, we\nfirst present cornerstones to construct additional cluster features for a\ntraining set. Then, we come up with an algorithm to generate an IDS based on\nsuch cluster features and the original input features. Finally, we\nexperimentally prove that our method outperforms several well-known methods."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1966895.1966897", 
    "link": "http://arxiv.org/pdf/1110.2890v1", 
    "other_authors": "Rui Zhou, Chengfei Liu, Jianxin Li, Jeffrey Xu Yu", 
    "title": "ELCA Evaluation for Keyword Search on Probabilistic XML Data", 
    "arxiv-id": "1110.2890v1", 
    "author": "Jeffrey Xu Yu", 
    "publish": "2011-10-13T10:45:33Z", 
    "summary": "As probabilistic data management is becoming one of the main research focuses\nand keyword search is turning into a more popular query means, it is natural to\nthink how to support keyword queries on probabilistic XML data. With regards to\nkeyword query on deterministic XML documents, ELCA (Exclusive Lowest Common\nAncestor) semantics allows more relevant fragments rooted at the ELCAs to\nappear as results and is more popular compared with other keyword query result\nsemantics (such as SLCAs).\n  In this paper, we investigate how to evaluate ELCA results for keyword\nqueries on probabilistic XML documents. After defining probabilistic ELCA\nsemantics in terms of possible world semantics, we propose an approach to\ncompute ELCA probabilities without generating possible worlds. Then we develop\nan efficient stack-based algorithm that can find all probabilistic ELCA results\nand their ELCA probabilities for a given keyword query on a probabilistic XML\ndocument. Finally, we experimentally evaluate the proposed ELCA algorithm and\ncompare it with its SLCA counterpart in aspects of result effectiveness, time\nand space efficiency, and scalability."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1966895.1966897", 
    "link": "http://arxiv.org/pdf/1110.3158v1", 
    "other_authors": "Rashed Salem, J\u00e9r\u00f4me Darmont, Omar Boussa\u00efd", 
    "title": "Efficient Incremental Breadth-Depth XML Event Mining", 
    "arxiv-id": "1110.3158v1", 
    "author": "Omar Boussa\u00efd", 
    "publish": "2011-10-14T09:25:52Z", 
    "summary": "Many applications log a large amount of events continuously. Extracting\ninteresting knowledge from logged events is an emerging active research area in\ndata mining. In this context, we propose an approach for mining frequent events\nand association rules from logged events in XML format. This approach is\ncomposed of two-main phases: I) constructing a novel tree structure called\nFrequency XML-based Tree (FXT), which contains the frequency of events to be\nmined; II) querying the constructed FXT using XQuery to discover frequent\nitemsets and association rules. The FXT is constructed with a single-pass over\nlogged data. We implement the proposed algorithm and study various performance\nissues. The performance study shows that the algorithm is efficient, for both\nconstructing the FXT and discovering association rules."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1966895.1966897", 
    "link": "http://arxiv.org/pdf/1110.3569v1", 
    "other_authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", 
    "title": "Dimension Reduction of Health Data Clustering", 
    "arxiv-id": "1110.3569v1", 
    "author": "Abdullah Embong", 
    "publish": "2011-10-17T03:40:07Z", 
    "summary": "The current data tends to be more complex than conventional data and need\ndimension reduction. Dimension reduction is important in cluster analysis and\ncreates a smaller data in volume and has the same analytical results as the\noriginal representation. A clustering process needs data reduction to obtain an\nefficient processing time while clustering and mitigate curse of\ndimensionality. This paper proposes a model for extracting multidimensional\ndata clustering of health database. We implemented four dimension reduction\ntechniques such as Singular Value Decomposition (SVD), Principal Component\nAnalysis (PCA), Self Organizing Map (SOM) and FastICA. The results show that\ndimension reductions significantly reduce dimension and shorten processing time\nand also increased performance of cluster in several health datasets."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.3879v1", 
    "other_authors": "Akihiro Inokuchi, Hiroaki Ikuta, Takashi Washio", 
    "title": "GTRACE-RS: Efficient Graph Sequence Mining using Reverse Search", 
    "arxiv-id": "1110.3879v1", 
    "author": "Takashi Washio", 
    "publish": "2011-10-18T05:48:03Z", 
    "summary": "The mining of frequent subgraphs from labeled graph data has been studied\nextensively. Furthermore, much attention has recently been paid to frequent\npattern mining from graph sequences. A method, called GTRACE, has been proposed\nto mine frequent patterns from graph sequences under the assumption that\nchanges in graphs are gradual. Although GTRACE mines the frequent patterns\nefficiently, it still needs substantial computation time to mine the patterns\nfrom graph sequences containing large graphs and long sequences. In this paper,\nwe propose a new version of GTRACE that enables efficient mining of frequent\npatterns based on the principle of a reverse search. The underlying concept of\nthe reverse search is a general scheme for designing efficient algorithms for\nhard enumeration problems. Our performance study shows that the proposed method\nis efficient and scalable for mining both long and large graph sequence\npatterns and is several orders of magnitude faster than the original GTRACE."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6647v1", 
    "other_authors": "Andrew Pavlo, Evan P. C. Jones, Stanley Zdonik", 
    "title": "On Predictive Modeling for Optimizing Transaction Execution in Parallel   OLTP Systems", 
    "arxiv-id": "1110.6647v1", 
    "author": "Stanley Zdonik", 
    "publish": "2011-10-30T20:21:05Z", 
    "summary": "A new emerging class of parallel database management systems (DBMS) is\ndesigned to take advantage of the partitionable workloads of on-line\ntransaction processing (OLTP) applications. Transactions in these systems are\noptimized to execute to completion on a single node in a shared-nothing cluster\nwithout needing to coordinate with other nodes or use expensive concurrency\ncontrol measures. But some OLTP applications cannot be partitioned such that\nall of their transactions execute within a single-partition in this manner.\nThese distributed transactions access data not stored within their local\npartitions and subsequently require more heavy-weight concurrency control\nprotocols. Further difficulties arise when the transaction's execution\nproperties, such as the number of partitions it may need to access or whether\nit will abort, are not known beforehand. The DBMS could mitigate these\nperformance issues if it is provided with additional information about\ntransactions. Thus, in this paper we present a Markov model-based approach for\nautomatically selecting which optimizations a DBMS could use, namely (1) more\nefficient concurrency control schemes, (2) intelligent scheduling, (3) reduced\nundo logging, and (4) speculative execution. To evaluate our techniques, we\nimplemented our models and integrated them into a parallel, main-memory OLTP\nDBMS to show that we can improve the performance of applications with diverse\nworkloads."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6648v1", 
    "other_authors": "Fran\u00e7ois Goasdou\u00e9, Konstantinos Karanasos, Julien Leblay, Ioana Manolescu", 
    "title": "View Selection in Semantic Web Databases", 
    "arxiv-id": "1110.6648v1", 
    "author": "Ioana Manolescu", 
    "publish": "2011-10-30T20:21:16Z", 
    "summary": "We consider the setting of a Semantic Web database, containing both explicit\ndata encoded in RDF triples, and implicit data, implied by the RDF semantics.\nBased on a query workload, we address the problem of selecting a set of views\nto be materialized in the database, minimizing a combination of query\nprocessing, view storage, and view maintenance costs. Starting from an existing\nrelational view selection method, we devise new algorithms for recommending\nview sets, and show that they scale significantly beyond the existing\nrelational ones when adapted to the RDF context. To account for implicit\ntriples in query answers, we propose a novel RDF query reformulation algorithm\nand an innovative way of incorporating it into view selection in order to avoid\na combinatorial explosion in the complexity of the selection process. The\ninterest of our techniques is demonstrated through a set of experiments."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6649v1", 
    "other_authors": "Jeffrey Jestes, Ke Yi, Feifei Li", 
    "title": "Building Wavelet Histograms on Large Data in MapReduce", 
    "arxiv-id": "1110.6649v1", 
    "author": "Feifei Li", 
    "publish": "2011-10-30T20:21:30Z", 
    "summary": "MapReduce is becoming the de facto framework for storing and processing\nmassive data, due to its excellent scalability, reliability, and elasticity. In\nmany MapReduce applications, obtaining a compact accurate summary of data is\nessential. Among various data summarization tools, histograms have proven to be\nparticularly important and useful for summarizing data, and the wavelet\nhistogram is one of the most widely used histograms. In this paper, we\ninvestigate the problem of building wavelet histograms efficiently on large\ndatasets in MapReduce. We measure the efficiency of the algorithms by both\nend-to-end running time and communication cost. We demonstrate straightforward\nadaptations of existing exact and approximate methods for building wavelet\nhistograms to MapReduce clusters are highly inefficient. To that end, we design\nnew algorithms for computing exact and approximate wavelet histograms and\ndiscuss their implementation in MapReduce. We illustrate our techniques in\nHadoop, and compare to baseline solutions with extensive experiments performed\nin a heterogeneous Hadoop cluster of 16 nodes, using large real and synthetic\ndatasets, up to hundreds of gigabytes. The results suggest significant (often\norders of magnitude) performance improvement achieved by our new algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6650v1", 
    "other_authors": "Di Yang, Elke A. Rundensteiner, Matthew O. Ward", 
    "title": "Summarization and Matching of Density-Based Clusters in Streaming   Environments", 
    "arxiv-id": "1110.6650v1", 
    "author": "Matthew O. Ward", 
    "publish": "2011-10-30T20:21:40Z", 
    "summary": "Density-based cluster mining is known to serve a broad range of applications\nranging from stock trade analysis to moving object monitoring. Although methods\nfor efficient extraction of density-based clusters have been studied in the\nliterature, the problem of summarizing and matching of such clusters with\narbitrary shapes and complex cluster structures remains unsolved. Therefore,\nthe goal of our work is to extend the state-of-art of density-based cluster\nmining in streams from cluster extraction only to now also support analysis and\nmanagement of the extracted clusters. Our work solves three major technical\nchallenges. First, we propose a novel multi-resolution cluster summarization\nmethod, called Skeletal Grid Summarization (SGS), which captures the key\nfeatures of density-based clusters, covering both their external shape and\ninternal cluster structures. Second, in order to summarize the extracted\nclusters in real-time, we present an integrated computation strategy C-SGS,\nwhich piggybacks the generation of cluster summarizations within the online\nclustering process. Lastly, we design a mechanism to efficiently execute\ncluster matching queries, which identify similar clusters for given cluster of\nanalyst's interest from clusters extracted earlier in the stream history. Our\nexperimental study using real streaming data shows the clear superiority of our\nproposed methods in both efficiency and effectiveness for cluster summarization\nand cluster matching queries to other potential alternatives."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6651v1", 
    "other_authors": "Thanh Nguyen, Viviane Moreira, Huong Nguyen, Hoa Nguyen, Juliana Freire", 
    "title": "Multilingual Schema Matching for Wikipedia Infoboxes", 
    "arxiv-id": "1110.6651v1", 
    "author": "Juliana Freire", 
    "publish": "2011-10-30T20:21:47Z", 
    "summary": "Recent research has taken advantage of Wikipedia's multilingualism as a\nresource for cross-language information retrieval and machine translation, as\nwell as proposed techniques for enriching its cross-language structure. The\navailability of documents in multiple languages also opens up new opportunities\nfor querying structured Wikipedia content, and in particular, to enable answers\nthat straddle different languages. As a step towards supporting such queries,\nin this paper, we propose a method for identifying mappings between attributes\nfrom infoboxes that come from pages in different languages. Our approach finds\nmappings in a completely automated fashion. Because it does not require\ntraining data, it is scalable: not only can it be used to find mappings between\nmany language pairs, but it is also effective for languages that are\nunder-represented and lack sufficient training samples. Another important\nbenefit of our approach is that it does not depend on syntactic similarity\nbetween attribute names, and thus, it can be applied to language pairs that\nhave distinct morphologies. We have performed an extensive experimental\nevaluation using a corpus consisting of pages in Portuguese, Vietnamese, and\nEnglish. The results show that not only does our approach obtain high precision\nand recall, but it also outperforms state-of-the-art techniques. We also\npresent a case study which demonstrates that the multilingual mappings we\nderive lead to substantial improvements in answer quality and coverage for\nstructured queries over Wikipedia content."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1110.6652v1", 
    "other_authors": "Guimei Liu, Haojun Zhang, Limsoon Wong", 
    "title": "Controlling False Positives in Association Rule Mining", 
    "arxiv-id": "1110.6652v1", 
    "author": "Limsoon Wong", 
    "publish": "2011-10-30T20:22:00Z", 
    "summary": "Association rule mining is an important problem in the data mining area. It\nenumerates and tests a large number of rules on a dataset and outputs rules\nthat satisfy user-specified constraints. Due to the large number of rules being\ntested, rules that do not represent real systematic effect in the data can\nsatisfy the given constraints purely by random chance. Hence association rule\nmining often suffers from a high risk of false positive errors. There is a lack\nof comprehensive study on controlling false positives in association rule\nmining. In this paper, we adopt three multiple testing correction\napproaches---the direct adjustment approach, the permutation-based approach and\nthe holdout approach---to control false positives in association rule mining,\nand conduct extensive experiments to study their performance. Our results show\nthat (1) Numerous spurious rules are generated if no correction is made. (2)\nThe three approaches can control false positives effectively. Among the three\napproaches, the permutation-based approach has the highest power of detecting\nreal association rules, but it is very computationally expensive. We employ\nseveral techniques to reduce its cost effectively."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1112.1117v2", 
    "other_authors": "Mohammad Khabbaz, Smriti Bhagat, Laks V. S. Lakshmanan", 
    "title": "Finding Heavy Paths in Graphs: A Rank Join Approach", 
    "arxiv-id": "1112.1117v2", 
    "author": "Laks V. S. Lakshmanan", 
    "publish": "2011-12-05T23:12:31Z", 
    "summary": "Graphs have been commonly used to model many applications. A natural problem\nwhich abstracts applications such as itinerary planning, playlist\nrecommendation, and flow analysis in information networks is that of finding\nthe heaviest path(s) in a graph. More precisely, we can model these\napplications as a graph with non-negative edge weights, along with a monotone\nfunction such as sum, which aggregates edge weights into a path weight,\ncapturing some notion of quality. We are then interested in finding the top-k\nheaviest simple paths, i.e., the $k$ simple (cycle-free) paths with the\ngreatest weight, whose length equals a given parameter $\\ell$. We call this the\n\\emph{Heavy Path Problem} (HPP). It is easy to show that the problem is\nNP-Hard.\n  In this work, we develop a practical approach to solve the Heavy Path problem\nby leveraging a strong connection with the well-known Rank Join paradigm. We\nfirst present an algorithm by adapting the Rank Join algorithm. We identify its\nlimitations and develop a new exact algorithm called HeavyPath and a scalable\nheuristic algorithm. We conduct a comprehensive set of experiments on three\nreal data sets and show that HeavyPath outperforms the baseline algorithms\nsignificantly, with respect to both $\\ell$ and $k$. Further, our heuristic\nalgorithm scales to longer lengths, finding paths that are empirically within\n50% of the optimum solution or better under various settings, and takes only a\nfraction of the running time compared to the exact algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1112.1668v1", 
    "other_authors": "Casey Bennett, Thomas Doub", 
    "title": "Data Mining and Electronic Health Records: Selecting Optimal Clinical   Treatments in Practice", 
    "arxiv-id": "1112.1668v1", 
    "author": "Thomas Doub", 
    "publish": "2011-12-07T19:44:19Z", 
    "summary": "Electronic health records (EHR's) are only a first step in capturing and\nutilizing health-related data - the problem is turning that data into useful\ninformation. Models produced via data mining and predictive analysis profile\ninherited risks and environmental/behavioral factors associated with patient\ndisorders, which can be utilized to generate predictions about treatment\noutcomes. This can form the backbone of clinical decision support systems\ndriven by live data based on the actual population. The advantage of such an\napproach based on the actual population is that it is \"adaptive\". Here, we\nevaluate the predictive capacity of a clinical EHR of a large mental healthcare\nprovider (~75,000 distinct clients a year) to provide decision support\ninformation in a real-world clinical setting. Initial research has achieved a\n70% success rate in predicting treatment outcomes using these methods."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1112.2020v1", 
    "other_authors": "Rui Chen, Benjamin C. M. Fung, Bipin C. Desai", 
    "title": "Differentially Private Trajectory Data Publication", 
    "arxiv-id": "1112.2020v1", 
    "author": "Bipin C. Desai", 
    "publish": "2011-12-09T05:19:57Z", 
    "summary": "With the increasing prevalence of location-aware devices, trajectory data has\nbeen generated and collected in various application domains. Trajectory data\ncarries rich information that is useful for many data analysis tasks. Yet,\nimproper publishing and use of trajectory data could jeopardize individual\nprivacy. However, it has been shown that existing privacy-preserving trajectory\ndata publishing methods derived from partition-based privacy models, for\nexample k-anonymity, are unable to provide sufficient privacy protection.\n  In this paper, motivated by the data publishing scenario at the Societe de\ntransport de Montreal (STM), the public transit agency in Montreal area, we\nstudy the problem of publishing trajectory data under the rigorous differential\nprivacy model. We propose an efficient data-dependent yet differentially\nprivate sanitization algorithm, which is applicable to different types of\ntrajectory data. The efficiency of our approach comes from adaptively narrowing\ndown the output domain by building a noisy prefix tree based on the underlying\ndata. Moreover, as a post-processing step, we make use of the inherent\nconstraints of a prefix tree to conduct constrained inferences, which lead to\nbetter utility. This is the first paper to introduce a practical solution for\npublishing large volume of trajectory data under differential privacy. We\nexamine the utility of sanitized data in terms of count queries and frequent\nsequential pattern mining. Extensive experiments on real-life trajectory data\nfrom the STM demonstrate that our approach maintains high utility and is\nscalable to large trajectory datasets."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E95.D.1947", 
    "link": "http://arxiv.org/pdf/1112.2137v1", 
    "other_authors": "S. P. Syed Ibrahim, K. R. Chandran", 
    "title": "Compact Weighted Class Association Rule Mining using Information Gain", 
    "arxiv-id": "1112.2137v1", 
    "author": "K. R. Chandran", 
    "publish": "2011-12-09T16:22:00Z", 
    "summary": "Weighted association rule mining reflects semantic significance of item by\nconsidering its weight. Classification constructs the classifier and predicts\nthe new data instance. This paper proposes compact weighted class association\nrule mining method, which applies weighted association rule mining in the\nclassification and constructs an efficient weighted associative classifier.\nThis proposed associative classification algorithm chooses one non class\ninformative attribute from dataset and all the weighted class association rules\nare generated based on that attribute. The weight of the item is considered as\none of the parameter in generating the weighted class association rules. This\nproposed algorithm calculates the weight using the HITS model. Experimental\nresults show that the proposed system generates less number of high quality\nrules which improves the classification accuracy."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3404", 
    "link": "http://arxiv.org/pdf/1112.2155v1", 
    "other_authors": "Ali Karami, Ahmad Baraani-Dastjerdi", 
    "title": "A Concurrency Control Method Based on Commitment Ordering in Mobile   Databases", 
    "arxiv-id": "1112.2155v1", 
    "author": "Ahmad Baraani-Dastjerdi", 
    "publish": "2011-12-09T17:27:51Z", 
    "summary": "Disconnection of mobile clients from server, in an unclear time and for an\nunknown duration, due to mobility of mobile clients, is the most important\nchallenges for concurrency control in mobile database with client-server model.\nApplying pessimistic common classic methods of concurrency control (like 2pl)\nin mobile database leads to long duration blocking and increasing waiting time\nof transactions. Because of high rate of aborting transactions, optimistic\nmethods aren`t appropriate in mobile database. In this article, OPCOT\nconcurrency control algorithm is introduced based on optimistic concurrency\ncontrol method. Reducing communications between mobile client and server,\ndecreasing blocking rate and deadlock of transactions, and increasing\nconcurrency degree are the most important motivation of using optimistic method\nas the basis method of OPCOT algorithm. To reduce abortion rate of\ntransactions, in execution time of transactions` operators a timestamp is\nassigned to them. In other to checking commitment ordering property of\nscheduler, the assigned timestamp is used in server on time of commitment. In\nthis article, serializability of OPCOT algorithm scheduler has been proved by\nusing serializability graph. Results of evaluating simulation show that OPCOT\nalgorithm decreases abortion rate and waiting time of transactions in compare\nto 2pl and optimistic algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3404", 
    "link": "http://arxiv.org/pdf/1112.2336v1", 
    "other_authors": "Nasrin Mazaheri Soudani, Ahmad Baraani-Dastgerdi", 
    "title": "The Spatial Nearest Neighbor Skyline Queries", 
    "arxiv-id": "1112.2336v1", 
    "author": "Ahmad Baraani-Dastgerdi", 
    "publish": "2011-12-11T08:43:54Z", 
    "summary": "User preference queries are very important in spatial databases. With the\nhelp of these queries, one can found best location among points saved in\ndatabase. In many situation users evaluate quality of a location with its\ndistance from its nearest neighbor among a special set of points. There has\nbeen less attention about evaluating a location with its distance to nearest\nneighbors in spatial user preference queries. This problem has application in\nmany domains such as service recommendation systems and investment planning.\nRelated works in this field are based on top-k queries. The problem with top-k\nqueries is that user must set weights for attributes and a function for\naggregating them. This is hard for him in most cases. In this paper a new type\nof user preference queries called spatial nearest neighbor skyline queries will\nbe introduced in which user has some sets of points as query parameters. For\neach point in database attributes are its distances to the nearest neighbors\nfrom each set of query points. By separating this query as a subset of dynamic\nskyline queries N2S2 algorithm is provided for computing it. This algorithm has\ngood performance compared with the general branch and bound algorithm for\nskyline queries."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3404", 
    "link": "http://arxiv.org/pdf/1112.2401v1", 
    "other_authors": "Jihen Drira Rekik, Leila Baccouche, Henda Ben Ghezala", 
    "title": "A Real-Time Database QoS-aware Service Selection Protocol for MANET", 
    "arxiv-id": "1112.2401v1", 
    "author": "Henda Ben Ghezala", 
    "publish": "2011-12-11T21:00:16Z", 
    "summary": "The real-time database service selection depends typically to the system\nstability in order to handle the time-constrained transactions within their\ndeadline. However, applying the real-time database system in the mobile ad hoc\nnetworks requires considering the mobile nodes limited capacities. In this\npaper, we propose cross-layer service selection which combines performance\nmetrics measured in the real-time database system to those used by the routing\nprotocol in order to make the best selection decision. It ensures both\ntimeliness and energy efficiency by avoiding low-power and busy service\nprovider node. A multicast packet is used in order to reduce the transmission\ncost and network load when sending the same packet to multiple service\nproviders. In this paper, we evaluate the performance of our proposed protocol.\nSimulation results, using the Network Simulator NS2, improve that the protocol\ndecreases the deadline miss ratio of packets, increases the service\navailability and reduces the service response time."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3404", 
    "link": "http://arxiv.org/pdf/1112.2610v1", 
    "other_authors": "Konstantinos Karanasos, Asterios Katsifodimos, Ioana Manolescu, Spyros Zoupanos", 
    "title": "The ViP2P Platform: XML Views in P2P", 
    "arxiv-id": "1112.2610v1", 
    "author": "Spyros Zoupanos", 
    "publish": "2011-12-12T16:29:25Z", 
    "summary": "The growing volumes of XML data sources on the Web or produced by\nenterprises, organizations etc. raise many performance challenges for data\nmanagement applications. In this work, we are concerned with the distributed,\npeer-to-peer management of large corpora of XML documents, based on distributed\nhash table (or DHT, in short) overlay networks. We present ViP2P (standing for\nViews in Peer-to-Peer), a distributed platform for sharing XML documents based\non a structured P2P network infrastructure (DHT). At the core of ViP2P stand\ndistributed materialized XML views, defined by arbitrary XML queries, filled in\nwith data published anywhere in the network, and exploited to efficiently\nanswer queries issued by any network peer. ViP2P allows user queries to be\nevaluated over XML documents published by peers in two modes. First, a\nlong-running subscription mode, when a query can be registered in the system\nand receive answers incrementally when and if published data matches the query.\nSecond, queries can also be asked in an ad-hoc, snapshot mode, where results\nare required immediately and must be computed based on the results of other\nlong-running, subscription queries. ViP2P innovates over other similar\nDHT-based XML sharing platforms by using a very expressive structured XML query\nlanguage. This expressivity leads to a very flexible distribution of XML\ncontent in the ViP2P network, and to efficient snapshot query execution. ViP2P\nhas been tested in real deployments of hundreds of computers. We present the\nplatform architecture, its internal algorithms, and demonstrate its efficiency\nand scalability through a set of experiments. Our experimental results outgrow\nby orders of magnitude similar competitor systems in terms of data volumes,\nnetwork size and data dissemination throughput."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1112.2663v1", 
    "other_authors": "Dr. Sankar Rajagopal", 
    "title": "Customer Data Clustering using Data Mining Technique", 
    "arxiv-id": "1112.2663v1", 
    "author": "Dr. Sankar Rajagopal", 
    "publish": "2011-12-09T20:52:14Z", 
    "summary": "Classification and patterns extraction from customer data is very important\nfor business support and decision making. Timely identification of newly\nemerging trends is very important in business process. Large companies are\nhaving huge volume of data but starving for knowledge. To overcome the\norganization current issue, the new breed of technique is required that has\nintelligence and capability to solve the knowledge scarcity and the technique\nis called Data mining. The objectives of this paper are to identify the\nhigh-profit, high-value and low-risk customers by one of the data mining\ntechnique - customer clustering. In the first phase, cleansing the data and\ndeveloped the patterns via demographic clustering algorithm using IBM I-Miner.\nIn the second phase, profiling the data, develop the clusters and identify the\nhigh-value low-risk customers. This cluster typically represents the 10-20\npercent of customers which yields 80% of the revenue."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1112.3018v1", 
    "other_authors": "Marco Tereso, Jorge Bernardino", 
    "title": "Open Source CRM Systems for SMEs", 
    "arxiv-id": "1112.3018v1", 
    "author": "Jorge Bernardino", 
    "publish": "2011-12-13T20:19:12Z", 
    "summary": "Customer Relationship Management (CRM) systems are very common in large\ncompanies. However, CRM systems are not very common in Small and Medium\nEnterprises (SMEs). Most SMEs do not implement CRM systems due to several\nreasons, such as lack of knowledge about CRM or lack of financial resources to\nimplement CRM systems. SMEs have to start implementing Information Systems (IS)\ntechnology into their business operations in order to improve business values\nand gain more competitive advantage over rivals. CRM system has the potential\nto help improve the business value and competitive capabilities of SMEs. Given\nthe high fixed costs of normal activity of companies, we intend to promote free\nand viable solutions for small and medium businesses. In this paper, we explain\nthe reasons why SMEs do not implement CRM system and the benefits of using open\nsource CRM system in SMEs. We also describe the functionalities of top open\nsource CRM systems, examining the applicability of these tools in fitting the\nneeds of SMEs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1112.3134v1", 
    "other_authors": "Mohammad-Reza Feizi-Derakhshi, Azade Roohany", 
    "title": "Proposing Cluster_Similarity Method in Order to Find as Much Better   Similarities in Databases", 
    "arxiv-id": "1112.3134v1", 
    "author": "Azade Roohany", 
    "publish": "2011-12-14T07:12:31Z", 
    "summary": "Different ways of entering data into databases result in duplicate records\nthat cause increasing of databases' size. This is a fact that we cannot ignore\nit easily. There are several methods that are used for this purpose. In this\npaper, we have tried to increase the accuracy of operations by using cluster\nsimilarity instead of direct similarity of fields. So that clustering is done\non fields of database and according to accomplished clustering on fields,\nsimilarity degree of records is obtained. In this method by using present\ninformation in database, more logical similarity is obtained for deficient\ninformation that in general, the method of cluster similarity could improve\noperations 24% compared with previous methods."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1112.5957v2", 
    "other_authors": "Eya Ben Ahmed, Ahlem Nabli, Fa\u00efez Gargouri", 
    "title": "Usage Des Mesures Pour La G\u00e9n\u00e9ration Des R\u00e8gles d'Associations   Cycliques", 
    "arxiv-id": "1112.5957v2", 
    "author": "Fa\u00efez Gargouri", 
    "publish": "2011-12-27T13:15:47Z", 
    "summary": "The online analytical processing (OLAP) does not provide any explanation of\ncorrelations discovered between data. Thus, the coupling of OLAP and data\nmining, especially association rules, is considered as an efficient solution to\nthis problem. In this context, we mainly focus on a particular class of\nassociation rules which is the cyclic association rules. These rules aimed to\ndiscover patterns that display regular variation over user-defined intervals.\nGenerally,the generated patterns do not take an advantage from the\nspecificities of the multidimensional context namely, the consideration of the\nmeasures and their aggregations. In this paper, we introduce a novel method for\nextracting cyclic association rules from measures, and we redefine the\nevaluation metrics of association rules quality inspired of the temporal\nsummarizability of measures concept through the integration of appropriate\naggregation functions. To prove the usefulness of our approach, we conduct an\nempirical study on a real data warehouse."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0226v1", 
    "other_authors": "Ning Zhang, Junichi Tatemura, Jignesh M. Patel, Hakan Hac\u0131g\u00fcm\u00fc\u015f", 
    "title": "Towards Cost-Effective Storage Provisioning for DBMSs", 
    "arxiv-id": "1201.0226v1", 
    "author": "Hakan Hac\u0131g\u00fcm\u00fc\u015f", 
    "publish": "2011-12-31T05:32:48Z", 
    "summary": "Data center operators face a bewildering set of choices when considering how\nto provision resources on machines with complex I/O subsystems. Modern I/O\nsubsystems often have a rich mix of fast, high performing, but expensive SSDs\nsitting alongside with cheaper but relatively slower (for random accesses)\ntraditional hard disk drives. The data center operators need to determine how\nto provision the I/O resources for specific workloads so as to abide by\nexisting Service Level Agreements (SLAs), while minimizing the total operating\ncost (TOC) of running the workload, where the TOC includes the amortized\nhardware costs and the run time energy costs. The focus of this paper is on\nintroducing this new problem of TOC-based storage allocation, cast in a\nframework that is compatible with traditional DBMS query optimization and query\nprocessing architecture. We also present a heuristic-based solution to this\nproblem, called DOT. We have implemented DOT in PostgreSQL, and experiments\nusing TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various\nsettings."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0227v1", 
    "other_authors": "Hongchan Roh, Sanghyun Park, Sungho Kim, Mincheol Shin, Sang-Won Lee", 
    "title": "B+-tree Index Optimization by Exploiting Internal Parallelism of   Flash-based Solid State Drives", 
    "arxiv-id": "1201.0227v1", 
    "author": "Sang-Won Lee", 
    "publish": "2011-12-31T05:33:08Z", 
    "summary": "Previous research addressed the potential problems of the hard-disk oriented\ndesign of DBMSs of flashSSDs. In this paper, we focus on exploiting potential\nbenefits of flashSSDs. First, we examine the internal parallelism issues of\nflashSSDs by conducting benchmarks to various flashSSDs. Then, we suggest\nalgorithm-design principles in order to best benefit from the internal\nparallelism. We present a new I/O request concept, called psync I/O that can\nexploit the internal parallelism of flashSSDs in a single process. Based on\nthese ideas, we introduce B+-tree optimization methods in order to utilize\ninternal parallelism. By integrating the results of these methods, we present a\nB+-tree variant, PIO B-tree. We confirmed that each optimization method\nsubstantially enhances the index performance. Consequently, PIO B-tree enhanced\nB+-tree's insert performance by a factor of up to 16.3, while improving\npoint-search performance by a factor of 1.2. The range search of PIO B-tree was\nup to 5 times faster than that of the B+-tree. Moreover, PIO B-tree\noutperformed other flash-aware indexes in various synthetic workloads. We also\nconfirmed that PIO B-tree outperforms B+-tree in index traces collected inside\nthe Postgresql DBMS with TPC-C benchmark."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0228v1", 
    "other_authors": "Per-\u00c5ke Larson, Spyros Blanas, Cristian Diaconu, Craig Freedman, Jignesh M. Patel, Mike Zwilling", 
    "title": "High-Performance Concurrency Control Mechanisms for Main-Memory   Databases", 
    "arxiv-id": "1201.0228v1", 
    "author": "Mike Zwilling", 
    "publish": "2011-12-31T05:34:34Z", 
    "summary": "A database system optimized for in-memory storage can support much higher\ntransaction rates than current systems. However, standard concurrency control\nmethods used today do not scale to the high transaction rates achievable by\nsuch systems. In this paper we introduce two efficient concurrency control\nmethods specifically designed for main-memory databases. Both use\nmultiversioning to isolate read-only transactions from updates but differ in\nhow atomicity is ensured: one is optimistic and one is pessimistic. To avoid\nexpensive context switching, transactions never block during normal processing\nbut they may have to wait before commit to ensure correct serialization\nordering. We also implemented a main-memory optimized version of single-version\nlocking. Experimental results show that while single-version locking works well\nwhen transactions are short and contention is low performance degrades under\nmore demanding conditions. The multiversion schemes have higher overhead but\nare much less sensitive to hotspots and the presence of long-running\ntransactions."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0229v1", 
    "other_authors": "Shuai Ma, Yang Cao, Wenfei Fan, Jinpeng Huai, Tianyu Wo", 
    "title": "Capturing Topology in Graph Pattern Matching", 
    "arxiv-id": "1201.0229v1", 
    "author": "Tianyu Wo", 
    "publish": "2011-12-31T05:34:57Z", 
    "summary": "Graph pattern matching is often defined in terms of subgraph isomorphism, an\nNP-complete problem. To lower its complexity, various extensions of graph\nsimulation have been considered instead. These extensions allow pattern\nmatching to be conducted in cubic-time. However, they fall short of capturing\nthe topology of data graphs, i.e., graphs may have a structure drastically\ndifferent from pattern graphs they match, and the matches found are often too\nlarge to understand and analyze. To rectify these problems, this paper proposes\na notion of strong simulation, a revision of graph simulation, for graph\npattern matching. (1) We identify a set of criteria for preserving the topology\nof graphs matched. We show that strong simulation preserves the topology of\ndata graphs and finds a bounded number of matches. (2) We show that strong\nsimulation retains the same complexity as earlier extensions of simulation, by\nproviding a cubic-time algorithm for computing strong simulation. (3) We\npresent the locality property of strong simulation, which allows us to\neffectively conduct pattern matching on distributed graphs. (4) We\nexperimentally verify the effectiveness and efficiency of these algorithms,\nusing real-life data and synthetic data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0230v1", 
    "other_authors": "Mateusz Pawlik, Nikolaus Augsten", 
    "title": "RTED: A Robust Algorithm for the Tree Edit Distance", 
    "arxiv-id": "1201.0230v1", 
    "author": "Nikolaus Augsten", 
    "publish": "2011-12-31T05:35:26Z", 
    "summary": "We consider the classical tree edit distance between ordered labeled trees,\nwhich is defined as the minimum-cost sequence of node edit operations that\ntransform one tree into another. The state-of-the-art solutions for the tree\nedit distance are not satisfactory. The main competitors in the field either\nhave optimal worst-case complexity, but the worst case happens frequently, or\nthey are very efficient for some tree shapes, but degenerate for others. This\nleads to unpredictable and often infeasible runtimes. There is no obvious way\nto choose between the algorithms. In this paper we present RTED, a robust tree\nedit distance algorithm. The asymptotic complexity of RTED is smaller or equal\nto the complexity of the best competitors for any input instance, i.e., RTED is\nboth efficient and worst-case optimal. We introduce the class of LRH\n(Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit\ndistance algorithms presented in literature. We prove that RTED outperforms all\npreviously proposed LRH algorithms in terms of runtime complexity. In our\nexperiments on synthetic and real world data we empirically evaluate our\nsolution and compare it to the state-of-the-art."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0231v1", 
    "other_authors": "Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, Julia Stoyanovich, Val Tannen", 
    "title": "Putting Lipstick on Pig: Enabling Database-style Workflow Provenance", 
    "arxiv-id": "1201.0231v1", 
    "author": "Val Tannen", 
    "publish": "2011-12-31T05:35:52Z", 
    "summary": "Workflow provenance typically assumes that each module is a \"black-box\", so\nthat each output depends on all inputs (coarse-grained dependencies).\nFurthermore, it does not model the internal state of a module, which can change\nbetween repeated executions. In practice, however, an output may depend on only\na small subset of the inputs (fine-grained dependencies) as well as on the\ninternal state of the module. We present a novel provenance framework that\nmarries database-style and workflow-style provenance, by using Pig Latin to\nexpose the functionality of modules, thus capturing internal state and\nfine-grained dependencies. A critical ingredient in our solution is the use of\na novel form of provenance graph that models module invocations and yields a\ncompact representation of fine-grained workflow provenance. It also enables a\nnumber of novel graph transformation operations, allowing to choose the desired\nlevel of granularity in provenance querying (ZoomIn and ZoomOut), and\nsupporting \"what-if\" workflow analytic queries. We implemented our approach in\nthe Lipstick system and developed a benchmark in support of a systematic\nperformance evaluation. Our results demonstrate the feasibility of tracking and\nquerying fine-grained workflow provenance."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0232v1", 
    "other_authors": "Jun Gao, Ruoming Jin, Jiashuai Zhou, Jeffrey Xu Yu, Xiao Jiang, Tengjiao Wang", 
    "title": "Relational Approach for Shortest Path Discovery over Large Graphs", 
    "arxiv-id": "1201.0232v1", 
    "author": "Tengjiao Wang", 
    "publish": "2011-12-31T05:36:12Z", 
    "summary": "With the rapid growth of large graphs, we cannot assume that graphs can still\nbe fully loaded into memory, thus the disk-based graph operation is inevitable.\nIn this paper, we take the shortest path discovery as an example to investigate\nthe technique issues when leveraging existing infrastructure of relational\ndatabase (RDB) in the graph data management. Based on the observation that a\nvariety of graph search queries can be implemented by iterative operations\nincluding selecting frontier nodes from visited nodes, making expansion from\nthe selected frontier nodes, and merging the expanded nodes into the visited\nones, we introduce a relational FEM framework with three corresponding\noperators to implement graph search tasks in the RDB context. We show new\nfeatures such as window function and merge statement introduced by recent SQL\nstandards can not only simplify the expression but also improve the performance\nof the FEM framework. In addition, we propose two optimization strategies\nspecific to shortest path discovery inside the FEM framework. First, we take a\nbi-directional set Dijkstra's algorithm in the path finding. The bi-directional\nstrategy can reduce the search space, and set Dijkstra's algorithm finds the\nshortest path in a set-at-a-time fashion. Second, we introduce an index named\nSegTable to preserve the local shortest segments, and exploit SegTable to\nfurther improve the performance. The final extensive experimental results\nillustrate our relational approach with the optimization strategies achieves\nhigh scalability and performance."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0233v1", 
    "other_authors": "Marina Barsky, Sangkyum Kim, Tim Weninger, Jiawei Han", 
    "title": "Mining Flipping Correlations from Large Datasets with Taxonomies", 
    "arxiv-id": "1201.0233v1", 
    "author": "Jiawei Han", 
    "publish": "2011-12-31T05:36:29Z", 
    "summary": "In this paper we introduce a new type of pattern -- a flipping correlation\npattern. The flipping patterns are obtained from contrasting the correlations\nbetween items at different levels of abstraction. They represent surprising\ncorrelations, both positive and negative, which are specific for a given\nabstraction level, and which \"flip\" from positive to negative and vice versa\nwhen items are generalized to a higher level of abstraction. We design an\nefficient algorithm for finding flipping correlations, the Flipper algorithm,\nwhich outperforms naive pattern mining methods by several orders of magnitude.\nWe apply Flipper to real-life datasets and show that the discovered patterns\nare non-redundant, surprising and actionable. Flipper finds strong contrasting\ncorrelations in itemsets with low-to-medium support, while existing techniques\ncannot handle the pattern discovery in this frequency range."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.0234v1", 
    "other_authors": "Arnd Christian K\u00f6nig, Bolin Ding, Surajit Chaudhuri, Vivek Narasayya", 
    "title": "A Statistical Approach Towards Robust Progress Estimation", 
    "arxiv-id": "1201.0234v1", 
    "author": "Vivek Narasayya", 
    "publish": "2011-12-31T05:36:46Z", 
    "summary": "The need for accurate SQL progress estimation in the context of decision\nsupport administration has led to a number of techniques proposed for this\ntask. Unfortunately, no single one of these progress estimators behaves\nrobustly across the variety of SQL queries encountered in practice, meaning\nthat each technique performs poorly for a significant fraction of queries. This\npaper proposes a novel estimator selection framework that uses a statistical\nmodel to characterize the sets of conditions under which certain estimators\noutperform others, leading to a significant increase in estimation robustness.\nThe generality of this framework also enables us to add a number of novel\n\"special purpose\" estimators which increase accuracy further. Most importantly,\nthe resulting model generalizes well to queries very different from the ones\nused to train it. We validate our findings using a large number of industrial\nreal-life and benchmark workloads."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.2766v1", 
    "other_authors": "Spyros Sioutas, Peter Triantafillou, George Papaloukopoulos, Evangelos Sakkopoulos, Kostas Tsichlas, Yannis Manolopoulos", 
    "title": "ART : Sub-Logarithmic Decentralized Range Query Processing with   Probabilistic Guarantees", 
    "arxiv-id": "1201.2766v1", 
    "author": "Yannis Manolopoulos", 
    "publish": "2012-01-13T08:54:37Z", 
    "summary": "We focus on range query processing on large-scale, typically distributed\ninfrastructures, such as clouds of thousands of nodes of shared-datacenters, of\np2p distributed overlays, etc. In such distributed environments, efficient\nrange query processing is the key for managing the distributed data sets per\nse, and for monitoring the infrastructure's resources. We wish to develop an\narchitecture that can support range queries in such large-scale decentralized\nenvironments and can scale in terms of the number of nodes as well as in terms\nof the data items stored. Of course, in the last few years there have been a\nnumber of solutions (mostly from researchers in the p2p domain) for designing\nsuch large-scale systems. However, these are inadequate for our purposes, since\nat the envisaged scales the classic logarithmic complexity (for point queries)\nis still too expensive while for range queries it is even more disappointing.\nIn this paper we go one step further and achieve a sub-logarithmic complexity.\nWe contribute the ART, which outperforms the most popular decentralized\nstructures, including Chord (and some of its successors), BATON (and its\nsuccessor) and Skip-Graphs. We contribute theoretical analysis, backed up by\ndetailed experimental results, showing that the communication cost of query and\nupdate operations is $O(\\log_{b}^2 \\log N)$ hops, where the base $b$ is a\ndouble-exponentially power of two and $N$ is the total number of nodes.\nMoreover, ART is a fully dynamic and fault-tolerant structure, which supports\nthe join/leave node operations in $O(\\log \\log N)$ expected w.h.p number of\nhops. Our experimental performance studies include a detailed performance\ncomparison which showcases the improved performance, scalability, and\nrobustness of ART."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.3458v1", 
    "other_authors": "Di Wu, Yiping Ke, Jeffrey Xu Yu, Zheng Liu", 
    "title": "Detecting Priming News Events", 
    "arxiv-id": "1201.3458v1", 
    "author": "Zheng Liu", 
    "publish": "2012-01-17T08:59:57Z", 
    "summary": "We study a problem of detecting priming events based on a time series index\nand an evolving document stream. We define a priming event as an event which\ntriggers abnormal movements of the time series index, i.e., the Iraq war with\nrespect to the president approval index of President Bush. Existing solutions\neither focus on organizing coherent keywords from a document stream into events\nor identifying correlated movements between keyword frequency trajectories and\nthe time series index. In this paper, we tackle the problem in two major steps.\n(1) We identify the elements that form a priming event. The element identified\nis called influential topic which consists of a set of coherent keywords. And\nwe extract them by looking at the correlation between keyword trajectories and\nthe interested time series index at a global level. (2) We extract priming\nevents by detecting and organizing the bursty influential topics at a micro\nlevel. We evaluate our algorithms on a real-world dataset and the result\nconfirms that our method is able to discover the priming events effectively."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6112v1", 
    "other_authors": "Seyed Aliakbar Mousavi, Muhammad Rafie Hj Arshad, Hasimah Hj Mohamed, Saleh Ali Alomari", 
    "title": "An Efficient Method for Mining Event-Related Potential Patterns", 
    "arxiv-id": "1201.6112v1", 
    "author": "Saleh Ali Alomari", 
    "publish": "2012-01-30T06:31:06Z", 
    "summary": "In the present paper, we propose a Neuroelectromagnetic Ontology Framework\n(NOF) for mining Event-related Potentials (ERP) patterns as well as the\nprocess. The aim for this research is to develop an infrastructure for mining,\nanalysis and sharing the ERP domain ontologies. The outcome of this research is\na Neuroelectromagnetic knowledge-based system. The framework has 5 stages: 1)\nData pre-processing and preparation; 2) Data mining application; 3) Rule\nComparison and Evaluation; 4) Association rules Post-processing 5) Domain\nOntologies. In 5th stage a new set of hidden rules can be discovered base on\ncomparing association rules by domain ontologies and expert rules."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6563v1", 
    "other_authors": "Yizhou Sun, Charu C. Aggarwal, Jiawei Han", 
    "title": "Relation Strength-Aware Clustering of Heterogeneous Information Networks   with Incomplete Attributes", 
    "arxiv-id": "1201.6563v1", 
    "author": "Jiawei Han", 
    "publish": "2012-01-31T15:08:41Z", 
    "summary": "With the rapid development of online social media, online shopping sites and\ncyber-physical systems, heterogeneous information networks have become\nincreasingly popular and content-rich over time. In many cases, such networks\ncontain multiple types of objects and links, as well as different kinds of\nattributes. The clustering of these objects can provide useful insights in many\napplications. However, the clustering of such networks can be challenging since\n(a) the attribute values of objects are often incomplete, which implies that an\nobject may carry only partial attributes or even no attributes to correctly\nlabel itself; and (b) the links of different types may carry different kinds of\nsemantic meanings, and it is a difficult task to determine the nature of their\nrelative importance in helping the clustering for a given purpose. In this\npaper, we address these challenges by proposing a model-based clustering\nalgorithm. We design a probabilistic model which clusters the objects of\ndifferent types into a common hidden space, by using a user-specified set of\nattributes, as well as the links from different relations. The strengths of\ndifferent types of links are automatically learned, and are determined by the\ngiven purpose of clustering. An iterative algorithm is designed for solving the\nclustering problem, in which the strengths of different types of links and the\nquality of clustering results mutually enhance each other. Our experimental\nresults on real and synthetic data sets demonstrate the effectiveness and\nefficiency of the algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6564v1", 
    "other_authors": "Lingkun Wu, Xiaokui Xiao, Dingxiong Deng, Gao Cong, Andy Diwen Zhu, Shuigeng Zhou", 
    "title": "Shortest Path and Distance Queries on Road Networks: An Experimental   Evaluation", 
    "arxiv-id": "1201.6564v1", 
    "author": "Shuigeng Zhou", 
    "publish": "2012-01-31T15:08:53Z", 
    "summary": "Computing the shortest path between two given locations in a road network is\nan important problem that finds applications in various map services and\ncommercial navigation products. The state-of-the-art solutions for the problem\ncan be divided into two categories: spatial-coherence-based methods and\nvertex-importance-based approaches. The two categories of techniques, however,\nhave not been compared systematically under the same experimental framework, as\nthey were developed from two independent lines of research that do not refer to\neach other. This renders it difficult for a practitioner to decide which\ntechnique should be adopted for a specific application. Furthermore, the\nexperimental evaluation of the existing techniques, as presented in previous\nwork, falls short in several aspects. Some methods were tested only on small\nroad networks with up to one hundred thousand vertices; some approaches were\nevaluated using distance queries (instead of shortest path queries), namely,\nqueries that ask only for the length of the shortest path; a state-of-the-art\ntechnique was examined based on a faulty implementation that led to incorrect\nquery results. To address the above issues, this paper presents a comprehensive\ncomparison of the most advanced spatial-coherence-based and\nvertex-importance-based approaches. Using a variety of real road networks with\nup to twenty million vertices, we evaluated each technique in terms of its\npreprocessing time, space consumption, and query efficiency (for both shortest\npath and distance queries). Our experimental results reveal the characteristics\nof different techniques, based on which we provide guidelines on selecting\nappropriate methods for various scenarios."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6565v1", 
    "other_authors": "D\u00f3ra Erd\u00f6s, Vatche Ishakian, Andrei Lapets, Evimaria Terzi, Azer Bestavros", 
    "title": "The Filter-Placement Problem and its Application to Minimizing   Information Multiplicity", 
    "arxiv-id": "1201.6565v1", 
    "author": "Azer Bestavros", 
    "publish": "2012-01-31T15:09:10Z", 
    "summary": "In many information networks, data items -- such as updates in social\nnetworks, news flowing through interconnected RSS feeds and blogs, measurements\nin sensor networks, route updates in ad-hoc networks -- propagate in an\nuncoordinated manner: nodes often relay information they receive to neighbors,\nindependent of whether or not these neighbors received the same information\nfrom other sources. This uncoordinated data dissemination may result in\nsignificant, yet unnecessary communication and processing overheads, ultimately\nreducing the utility of information networks. To alleviate the negative impacts\nof this information multiplicity phenomenon, we propose that a subset of nodes\n(selected at key positions in the network) carry out additional information\nfiltering functionality. Thus, nodes are responsible for the removal (or\nsignificant reduction) of the redundant data items relayed through them. We\nrefer to such nodes as filters. We formally define the Filter Placement problem\nas a combinatorial optimization problem, and study its computational complexity\nfor different types of graphs. We also present polynomial-time approximation\nalgorithms and scalable heuristics for the problem. Our experimental results,\nwhich we obtained through extensive simulations on synthetic and real-world\ninformation flow networks, suggest that in many settings a relatively small\nnumber of filters are fairly effective in removing a large fraction of\nredundant information."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6566v1", 
    "other_authors": "Yasuhiro Fujiwara, Makoto Nakatsuji, Makoto Onizuka, Masaru Kitsuregawa", 
    "title": "Fast and Exact Top-k Search for Random Walk with Restart", 
    "arxiv-id": "1201.6566v1", 
    "author": "Masaru Kitsuregawa", 
    "publish": "2012-01-31T15:09:50Z", 
    "summary": "Graphs are fundamental data structures and have been employed for centuries\nto model real-world systems and phenomena. Random walk with restart (RWR)\nprovides a good proximity score between two nodes in a graph, and it has been\nsuccessfully used in many applications such as automatic image captioning,\nrecommender systems, and link prediction. The goal of this work is to find\nnodes that have top-k highest proximities for a given node. Previous approaches\nto this problem find nodes efficiently at the expense of exactness. The main\nmotivation of this paper is to answer, in the affirmative, the question, `Is it\npossible to improve the search time without sacrificing the exactness?'. Our\nsolution, {it K-dash}, is based on two ideas: (1) It computes the proximity of\na selected node efficiently by sparse matrices, and (2) It skips unnecessary\nproximity computations when searching for the top-k nodes. Theoretical analyses\nshow that K-dash guarantees result exactness. We perform comprehensive\nexperiments to verify the efficiency of K-dash. The results show that K-dash\ncan find top-k nodes significantly faster than the previous approaches while it\nguarantees exactness."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6567v1", 
    "other_authors": "Bahman Bahmani, Ravi Kumar, Sergei Vassilvitskii", 
    "title": "Densest Subgraph in Streaming and MapReduce", 
    "arxiv-id": "1201.6567v1", 
    "author": "Sergei Vassilvitskii", 
    "publish": "2012-01-31T15:10:03Z", 
    "summary": "The problem of finding locally dense components of a graph is an important\nprimitive in data analysis, with wide-ranging applications from community\nmining to spam detection and the discovery of biological network modules. In\nthis paper we present new algorithms for finding the densest subgraph in the\nstreaming model. For any epsilon>0, our algorithms make O((log n)/log\n(1+epsilon)) passes over the input and find a subgraph whose density is\nguaranteed to be within a factor 2(1+epsilon) of the optimum. Our algorithms\nare also easily parallelizable and we illustrate this by realizing them in the\nMapReduce model. In addition we perform extensive experimental evaluation on\nmassive real-world graphs showing the performance and scalability of our\nalgorithms in practice."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6568v1", 
    "other_authors": "Arlei Silva, Wagner Meira Jr., Mohammed J. Zaki", 
    "title": "Mining Attribute-structure Correlated Patterns in Large Attributed   Graphs", 
    "arxiv-id": "1201.6568v1", 
    "author": "Mohammed J. Zaki", 
    "publish": "2012-01-31T15:10:13Z", 
    "summary": "In this work, we study the correlation between attribute sets and the\noccurrence of dense subgraphs in large attributed graphs, a task we call\nstructural correlation pattern mining. A structural correlation pattern is a\ndense subgraph induced by a particular attribute set. Existing methods are not\nable to extract relevant knowledge regarding how vertex attributes interact\nwith dense subgraphs. Structural correlation pattern mining combines aspects of\nfrequent itemset and quasi-clique mining problems. We propose statistical\nsignificance measures that compare the structural correlation of attribute sets\nagainst their expected values using null models. Moreover, we evaluate the\ninterestingness of structural correlation patterns in terms of size and\ndensity. An efficient algorithm that combines search and pruning strategies in\nthe identification of the most relevant structural correlation patterns is\npresented. We apply our method for the analysis of three real-world attributed\ngraphs: a collaboration, a music, and a citation network, verifying that it\nprovides valuable knowledge in a feasible time."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1201.6569v1", 
    "other_authors": "Robert Fink, Larisa Han, Dan Olteanu", 
    "title": "Aggregation in Probabilistic Databases via Knowledge Compilation", 
    "arxiv-id": "1201.6569v1", 
    "author": "Dan Olteanu", 
    "publish": "2012-01-31T15:10:34Z", 
    "summary": "This paper presents a query evaluation technique for positive relational\nalgebra queries with aggregates on a representation system for probabilistic\ndata based on the algebraic structures of semiring and semimodule. The core of\nour evaluation technique is a procedure that compiles semimodule and semiring\nexpressions into so-called decomposition trees, for which the computation of\nthe probability distribution can be done in time linear in the product of the\nsizes of the probability distributions represented by its nodes. We give\nsyntactic characterisations of tractable queries with aggregates by exploiting\nthe connection between query tractability and polynomial-time decomposition\ntrees. A prototype of the technique is incorporated in the probabilistic\ndatabase engine SPROUT. We report on performance experiments with custom\ndatasets and TPC-H data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0024v1", 
    "other_authors": "Babak Bagheri Hariri, Diego Calvanese, Giuseppe De Giacomo, Alin Deutsch, Marco Montali", 
    "title": "Verification of Relational Data-Centric Dynamic Systems with External   Services", 
    "arxiv-id": "1203.0024v1", 
    "author": "Marco Montali", 
    "publish": "2012-02-29T21:41:50Z", 
    "summary": "Data-centric dynamic systems are systems where both the process controlling\nthe dynamics and the manipulation of data are equally central. In this paper we\nstudy verification of (first-order) mu-calculus variants over relational\ndata-centric dynamic systems, where data are represented by a full-fledged\nrelational database, and the process is described in terms of atomic actions\nthat evolve the database. The execution of such actions may involve calls to\nexternal services, providing fresh data inserted into the system. As a result\nsuch systems are typically infinite-state. We show that verification is\nundecidable in general, and we isolate notable cases, where decidability is\nachieved. Specifically we start by considering service calls that return values\ndeterministically (depending only on passed parameters). We show that in a\nmu-calculus variant that preserves knowledge of objects appeared along a run we\nget decidability under the assumption that the fresh data introduced along a\nrun are bounded, though they might not be bounded in the overall system. In\nfact we tie such a result to a notion related to weak acyclicity studied in\ndata exchange. Then, we move to nondeterministic services where the assumption\nof data bounded run would result in a bound on the service calls that can be\ninvoked during the execution and hence would be too restrictive. So we\ninvestigate decidability under the assumption that knowledge of objects is\npreserved only if they are continuously present. We show that if infinitely\nmany values occur in a run but do not accumulate in the same state, then we get\nagain decidability. We give syntactic conditions to avoid this accumulation\nthrough the novel notion of \"generate-recall acyclicity\", which takes into\nconsideration that every service call activation generates new values that\ncannot be accumulated indefinitely."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0055v1", 
    "other_authors": "Felix Halim, Stratos Idreos, Panagiotis Karras, Roland H. C. Yap", 
    "title": "Stochastic Database Cracking: Towards Robust Adaptive Indexing in   Main-Memory Column-Stores", 
    "arxiv-id": "1203.0055v1", 
    "author": "Roland H. C. Yap", 
    "publish": "2012-03-01T00:16:29Z", 
    "summary": "Modern business applications and scientific databases call for inherently\ndynamic data storage environments. Such environments are characterized by two\nchallenging features: (a) they have little idle system time to devote on\nphysical design; and (b) there is little, if any, a priori workload knowledge,\nwhile the query and data workload keeps changing dynamically. In such\nenvironments, traditional approaches to index building and maintenance cannot\napply. Database cracking has been proposed as a solution that allows on-the-fly\nphysical data reorganization, as a collateral effect of query processing.\nCracking aims to continuously and automatically adapt indexes to the workload\nat hand, without human intervention. Indexes are built incrementally,\nadaptively, and on demand. Nevertheless, as we show, existing adaptive indexing\nmethods fail to deliver workload-robustness; they perform much better with\nrandom workloads than with others. This frailty derives from the inelasticity\nwith which these approaches interpret each query as a hint on how data should\nbe stored. Current cracking schemes blindly reorganize the data within each\nquery's range, even if that results into successive expensive operations with\nminimal indexing benefit. In this paper, we introduce stochastic cracking, a\nsignificantly more resilient approach to adaptive indexing. Stochastic cracking\nalso uses each query as a hint on how to reorganize data, but not blindly so;\nit gains resilience and avoids performance bottlenecks by deliberately applying\ncertain arbitrary choices in its decision-making. Thereby, we bring adaptive\nindexing forward to a mature formulation that confers the workload-robustness\nprevious approaches lacked. Our extensive experimental study verifies that\nstochastic cracking maintains the desired properties of original database\ncracking while at the same time it performs well with diverse realistic\nworkloads."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0056v1", 
    "other_authors": "Georgios Giannikis, Gustavo Alonso, Donald Kossmann", 
    "title": "SharedDB: Killing One Thousand Queries With One Stone", 
    "arxiv-id": "1203.0056v1", 
    "author": "Donald Kossmann", 
    "publish": "2012-03-01T00:17:13Z", 
    "summary": "Traditional database systems are built around the query-at-a-time model. This\napproach tries to optimize performance in a best-effort way. Unfortunately,\nbest effort is not good enough for many modern applications. These applications\nrequire response time guarantees in high load situations. This paper describes\nthe design of a new database architecture that is based on batching queries and\nshared computation across possibly hundreds of concurrent queries and updates.\nPerformance experiments with the TPC-W benchmark show that the performance of\nour implementation, SharedDB, is indeed robust across a wide range of dynamic\nworkloads."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0057v1", 
    "other_authors": "Joachim Selke, Christoph Lofi, Wolf-Tilo Balke", 
    "title": "Pushing the Boundaries of Crowd-enabled Databases with Query-driven   Schema Expansion", 
    "arxiv-id": "1203.0057v1", 
    "author": "Wolf-Tilo Balke", 
    "publish": "2012-03-01T00:17:22Z", 
    "summary": "By incorporating human workers into the query execution process crowd-enabled\ndatabases facilitate intelligent, social capabilities like completing missing\ndata at query time or performing cognitive operators. But despite all their\nflexibility, crowd-enabled databases still maintain rigid schemas. In this\npaper, we extend crowd-enabled databases by flexible query-driven schema\nexpansion, allowing the addition of new attributes to the database at query\ntime. However, the number of crowd-sourced mini-tasks to fill in missing values\nmay often be prohibitively large and the resulting data quality is doubtful.\nInstead of simple crowd-sourcing to obtain all values individually, we leverage\nthe user-generated data found in the Social Web: By exploiting user ratings we\nbuild perceptual spaces, i.e., highly-compressed representations of opinions,\nimpressions, and perceptions of large numbers of users. Using few training\nsamples obtained by expert crowd sourcing, we then can extract all missing data\nautomatically from the perceptual space with high quality and at low costs.\nExtensive experiments show that our approach can boost both performance and\nquality of crowd-enabled databases, while also providing the flexibility to\nexpand schemas in a query-driven fashion."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0059v1", 
    "other_authors": "Prasang Upadhyaya, Magdalena Balazinska, Dan Suciu", 
    "title": "How to Price Shared Optimizations in the Cloud", 
    "arxiv-id": "1203.0059v1", 
    "author": "Dan Suciu", 
    "publish": "2012-03-01T00:17:40Z", 
    "summary": "Data-management-as-a-service systems are increasingly being used in\ncollaborative settings, where multiple users access common datasets. Cloud\nproviders have the choice to implement various optimizations, such as indexing\nor materialized views, to accelerate queries over these datasets. Each\noptimization carries a cost and may benefit multiple users. This creates a\nmajor challenge: how to select which optimizations to perform and how to share\ntheir cost among users. The problem is especially challenging when users are\nselfish and will only report their true values for different optimizations if\ndoing so maximizes their utility. In this paper, we present a new approach for\nselecting and pricing shared optimizations by using Mechanism Design. We first\nshow how to apply the Shapley Value Mechanism to the simple case of selecting\nand pricing additive optimizations, assuming an offline game where all users\naccess the service for the same time-period. Second, we extend the approach to\nonline scenarios where users come and go. Finally, we consider the case of\nsubstitutive optimizations. We show analytically that our mechanisms induce\ntruth- fulness and recover the optimization costs. We also show experimentally\nthat our mechanisms yield higher utility than the state-of-the-art approach\nbased on regret accumulation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0060v1", 
    "other_authors": "Albert Angel, Nick Koudas, Nikos Sarkas, Divesh Srivastava", 
    "title": "Dense Subgraph Maintenance under Streaming Edge Weight Updates for   Real-time Story Identification", 
    "arxiv-id": "1203.0060v1", 
    "author": "Divesh Srivastava", 
    "publish": "2012-03-01T00:17:48Z", 
    "summary": "Recent years have witnessed an unprecedented proliferation of social media.\nPeople around the globe author, every day, millions of blog posts, social\nnetwork status updates, etc. This rich stream of information can be used to\nidentify, on an ongoing basis, emerging stories, and events that capture\npopular attention. Stories can be identified via groups of tightly-coupled\nreal-world entities, namely the people, locations, products, etc., that are\ninvolved in the story. The sheer scale, and rapid evolution of the data\ninvolved necessitate highly efficient techniques for identifying important\nstories at every point of time. The main challenge in real-time story\nidentification is the maintenance of dense subgraphs (corresponding to groups\nof tightly-coupled entities) under streaming edge weight updates (resulting\nfrom a stream of user-generated content). This is the first work to study the\nefficient maintenance of dense subgraphs under such streaming edge weight\nupdates. For a wide range of definitions of density, we derive theoretical\nresults regarding the magnitude of change that a single edge weight update can\ncause. Based on these, we propose a novel algorithm, DYNDENS, which outperforms\nadaptations of existing techniques to this setting, and yields meaningful\nresults. Our approach is validated by a thorough experimental evaluation on\nlarge-scale real and synthetic datasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0061v1", 
    "other_authors": "Iman Elghandour, Ashraf Aboulnaga", 
    "title": "ReStore: Reusing Results of MapReduce Jobs", 
    "arxiv-id": "1203.0061v1", 
    "author": "Ashraf Aboulnaga", 
    "publish": "2012-03-01T00:17:58Z", 
    "summary": "Analyzing large scale data has emerged as an important activity for many\norganizations in the past few years. This large scale data analysis is\nfacilitated by the MapReduce programming and execution model and its\nimplementations, most notably Hadoop. Users of MapReduce often have analysis\ntasks that are too complex to express as individual MapReduce jobs. Instead,\nthey use high-level query languages such as Pig, Hive, or Jaql to express their\ncomplex tasks. The compilers of these languages translate queries into\nworkflows of MapReduce jobs. Each job in these workflows reads its input from\nthe distributed file system used by the MapReduce system and produces output\nthat is stored in this distributed file system and read as input by the next\njob in the workflow. The current practice is to delete these intermediate\nresults from the distributed file system at the end of executing the workflow.\nOne way to improve the performance of workflows of MapReduce jobs is to keep\nthese intermediate results and reuse them for future workflows submitted to the\nsystem. In this paper, we present ReStore, a system that manages the storage\nand reuse of such intermediate results. ReStore can reuse the output of whole\nMapReduce jobs that are part of a workflow, and it can also create additional\nreuse opportunities by materializing and storing the output of query execution\noperators that are executed within a MapReduce job. We have implemented ReStore\nas an extension to the Pig dataflow system on top of Hadoop, and we\nexperimentally demonstrate significant speedups on queries from the PigMix\nbenchmark."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0077v1", 
    "other_authors": "Vince Barany, Balder ten Cate, Martin Otto", 
    "title": "Queries with Guarded Negation (full version)", 
    "arxiv-id": "1203.0077v1", 
    "author": "Martin Otto", 
    "publish": "2012-03-01T02:34:00Z", 
    "summary": "A well-established and fundamental insight in database theory is that\nnegation (also known as complementation) tends to make queries difficult to\nprocess and difficult to reason about. Many basic problems are decidable and\nadmit practical algorithms in the case of unions of conjunctive queries, but\nbecome difficult or even undecidable when queries are allowed to contain\nnegation. Inspired by recent results in finite model theory, we consider a\nrestricted form of negation, guarded negation. We introduce a fragment of SQL,\ncalled GN-SQL, as well as a fragment of Datalog with stratified negation,\ncalled GN-Datalog, that allow only guarded negation, and we show that these\nquery languages are computationally well behaved, in terms of testing query\ncontainment, query evaluation, open-world query answering, and boundedness.\nGN-SQL and GN-Datalog subsume a number of well known query languages and\nconstraint languages, such as unions of conjunctive queries, monadic Datalog,\nand frontier-guarded tgds. In addition, an analysis of standard benchmark\nworkloads shows that most usage of negation in SQL in practice is guarded\nnegation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.0617v2", 
    "other_authors": "Yonghui Xiao, Li Xiong", 
    "title": "Bayesian inference under differential privacy", 
    "arxiv-id": "1203.0617v2", 
    "author": "Li Xiong", 
    "publish": "2012-03-03T07:19:56Z", 
    "summary": "Bayesian inference is an important technique throughout statistics. The\nessence of Beyesian inference is to derive the posterior belief updated from\nprior belief by the learned information, which is a set of differentially\nprivate answers under differential privacy. Although Bayesian inference can be\nused in a variety of applications, it becomes theoretically hard to solve when\nthe number of differentially private answers is large. To facilitate Bayesian\ninference under differential privacy, this paper proposes a systematic\nmechanism. The key step of the mechanism is the implementation of Bayesian\nupdating with the best linear unbiased estimator derived by Gauss-Markov\ntheorem. In addition, we also apply the proposed inference mechanism into an\nonline queryanswering system, the novelty of which is that the utility for\nusers is guaranteed by Bayesian inference in the form of credible interval and\nconfidence level. Theoretical and experimental analysis are shown to\ndemonstrate the efficiency and effectiveness of both inference mechanism and\nonline query-answering system."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3401", 
    "link": "http://arxiv.org/pdf/1203.1569v2", 
    "other_authors": "Olaf Hartig", 
    "title": "SPARQL for a Web of Linked Data: Semantics and Computability (Extended   Version)", 
    "arxiv-id": "1203.1569v2", 
    "author": "Olaf Hartig", 
    "publish": "2012-03-07T18:53:35Z", 
    "summary": "The World Wide Web currently evolves into a Web of Linked Data where content\nproviders publish and link data as they have done with hypertext for the last\n20 years. While the declarative query language SPARQL is the de facto for\nquerying a-priory defined sets of data from the Web, no language exists for\nquerying the Web of Linked Data itself. However, it seems natural to ask\nwhether SPARQL is also suitable for such a purpose.\n  In this paper we formally investigate the applicability of SPARQL as a query\nlanguage for Linked Data on the Web. In particular, we study two query models:\n1) a full-Web semantics where the scope of a query is the complete set of\nLinked Data on the Web and 2) a family of reachability-based semantics which\nrestrict the scope to data that is reachable by traversing certain data links.\nFor both models we discuss properties such as monotonicity and computability as\nwell as the implications of querying a Web that is infinitely large due to data\ngenerating servers."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.1878v1", 
    "other_authors": "Saptarsi Goswami, Samiran Ghosh, Amlan Chakrabarti", 
    "title": "Outlier detection from ETL Execution trace", 
    "arxiv-id": "1203.1878v1", 
    "author": "Amlan Chakrabarti", 
    "publish": "2012-03-08T18:30:25Z", 
    "summary": "Extract, Transform, Load (ETL) is an integral part of Data Warehousing (DW)\nimplementation. The commercial tools that are used for this purpose captures\nlot of execution trace in form of various log files with plethora of\ninformation. However there has been hardly any initiative where any proactive\nanalyses have been done on the ETL logs to improve their efficiency. In this\npaper we utilize outlier detection technique to find the processes varying most\nfrom the group in terms of execution trace. As our experiment was carried on\nactual production processes, any outlier we would consider as a signal rather\nthan a noise. To identify the input parameters for the outlier detection\nalgorithm we employ a survey among developer community with varied mix of\nexperience and expertise. We use simple text parsing to extract these features\nfrom the logs, as shortlisted from the survey. Subsequently we applied outlier\ndetection technique (Clustering based) on the logs. By this process we reduced\nour domain of detailed analysis from 500 logs to 44 logs (8 Percentage). Among\nthe 5 outlier cluster, 2 of them are genuine concern, while the other 3 figure\nout because of the huge number of rows involved."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.2574v2", 
    "other_authors": "Xixuan Feng, Arun Kumar, Ben Recht, Christopher R\u00e9", 
    "title": "Towards a Unified Architecture for in-RDBMS Analytics", 
    "arxiv-id": "1203.2574v2", 
    "author": "Christopher R\u00e9", 
    "publish": "2012-03-12T18:07:58Z", 
    "summary": "The increasing use of statistical data analysis in enterprise applications\nhas created an arms race among database vendors to offer ever more\nsophisticated in-database analytics. One challenge in this race is that each\nnew statistical technique must be implemented from scratch in the RDBMS, which\nleads to a lengthy and complex development process. We argue that the root\ncause for this overhead is the lack of a unified architecture for in-database\nanalytics. Our main contribution in this work is to take a step towards such a\nunified architecture. A key benefit of our unified architecture is that\nperformance optimizations for analytics techniques can be studied generically\ninstead of an ad hoc, per-technique fashion. In particular, our technical\ncontributions are theoretical and empirical studies of two key factors that we\nfound impact performance: the order data is stored, and parallelization of\ncomputations on a single-node multicore RDBMS. We demonstrate the feasibility\nof our architecture by integrating several popular analytics techniques into\ntwo commercial and one open-source RDBMS. Our architecture requires changes to\nonly a few dozen lines of code to integrate a new statistical technique. We\nthen compare our approach with the native analytics tools offered by the\ncommercial RDBMSes on various analytics tasks, and validate that our approach\nachieves competitive or higher performance, while still achieving the same\nquality."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.4157v1", 
    "other_authors": "Saptarsi Goswami, Amlan Chakrabarti", 
    "title": "Quartile Clustering: A quartile based technique for Generating   Meaningful Clusters", 
    "arxiv-id": "1203.4157v1", 
    "author": "Amlan Chakrabarti", 
    "publish": "2012-03-19T16:38:05Z", 
    "summary": "Clustering is one of the main tasks in exploratory data analysis and\ndescriptive statistics where the main objective is partitioning observations in\ngroups. Clustering has a broad range of application in varied domains like\nclimate, business, information retrieval, biology, psychology, to name a few. A\nvariety of methods and algorithms have been developed for clustering tasks in\nthe last few decades. We observe that most of these algorithms define a cluster\nin terms of value of the attributes, density, distance etc. However these\ndefinitions fail to attach a clear meaning/semantics to the generated clusters.\nWe argue that clusters having understandable and distinct semantics defined in\nterms of quartiles/halves are more appealing to business analysts than the\nclusters defined by data boundaries or prototypes. On the samepremise, we\npropose our new algorithm named as quartile clustering technique. Through a\nseries of experiments we establish efficacy of this algorithm. We demonstrate\nthat the quartile clustering technique adds clear meaning to each of the\nclusters compared to K-means. We use DB Index to measure goodness of the\nclusters and show our method is comparable to EM (Expectation Maximization),\nPAM (Partition around Medoid) and K Means. We have explored its capability in\ndetecting outlier and the benefit of added semantics. We discuss some of the\nlimitations in its present form and also provide a rough direction in\naddressing the issue of merging the generated clusters."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.4163v1", 
    "other_authors": "Saptarsi Goswami, Samiran Ghosh, Amlan Chakrabarti", 
    "title": "Outlier Detection Techniques for SQL and ETL Tuning", 
    "arxiv-id": "1203.4163v1", 
    "author": "Amlan Chakrabarti", 
    "publish": "2012-03-19T16:54:39Z", 
    "summary": "RDBMS is the heart for both OLTP and OLAP types of applications. For both\ntypes of applications thousands of queries expressed in terms of SQL are\nexecuted on daily basis. All the commercial DBMS engines capture various\nattributes in system tables about these executed queries. These queries need to\nconform to best practices and need to be tuned to ensure optimal performance.\nWhile we use checklists, often tools to enforce the same, a black box technique\non the queries for profiling, outlier detection is not employed for a summary\nlevel understanding. This is the motivation of the paper, as this not only\npoints out to inefficiencies built in the system, but also has the potential to\npoint evolving best practices and inappropriate usage. Certainly this can\nreduce latency in information flow and optimal utilization of hardware and\nsoftware capacity. In this paper we start with formulating the problem. We\nexplore four outlier detection techniques. We apply these techniques over rich\ncorpora of production queries and analyze the results. We also explore benefit\nof an ensemble approach. We conclude with future courses of action. The same\nphilosophy we have used for optimization of extraction, transform, load (ETL)\njobs in one of our previous work. We give a brief introduction of the same in\nsection four."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.4380v3", 
    "other_authors": "Natalia Vanetik", 
    "title": "Analyzing closed frequent itemsets with convex polytopes", 
    "arxiv-id": "1203.4380v3", 
    "author": "Natalia Vanetik", 
    "publish": "2012-03-20T10:41:29Z", 
    "summary": "Computing frequent itemsets in transactional databases is a vital but\ncomputationally expensive task. Measuring the difference of two datasets is\noften done by computing their respective frequent itemsets despite high\ncomputational cost. This paper proposes a linear programming-based approach to\nthis problem and shows that there exists a distance measure for transactional\ndatabase that relies on closed frequent itemsets but does not require their\ngeneration."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.4732v1", 
    "other_authors": "Paola Bonizzoni, Peter J. Cameron, Gianluca Della Vedova, Alberto Leporati, Giancarlo Mauri", 
    "title": "A Unifying Framework to Characterize the Power of a Language to Express   Relations", 
    "arxiv-id": "1203.4732v1", 
    "author": "Giancarlo Mauri", 
    "publish": "2012-03-21T13:34:38Z", 
    "summary": "In this extended abstract we provide a unifying framework that can be used to\ncharacterize and compare the expressive power of query languages for different\ndata base models. The framework is based upon the new idea of valid partition,\nthat is a partition of the elements of a given data base, where each class of\nthe partition is composed by elements that cannot be separated (distinguished)\naccording to some level of information contained in the data base. We describe\ntwo applications of this new framework, first by deriving a new syntactic\ncharacterization of the expressive power of relational algebra which is\nequivalent to the one given by Paredaens, and subsequently by studying the\nexpressive power of a simple graph-based data model."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6400v1", 
    "other_authors": "Nodira Khoussainova, Magdalena Balazinska, Dan Suciu", 
    "title": "PerfXplain: Debugging MapReduce Job Performance", 
    "arxiv-id": "1203.6400v1", 
    "author": "Dan Suciu", 
    "publish": "2012-03-29T00:04:35Z", 
    "summary": "While users today have access to many tools that assist in performing large\nscale data analysis tasks, understanding the performance characteristics of\ntheir parallel computations, such as MapReduce jobs, remains difficult. We\npresent PerfXplain, a system that enables users to ask questions about the\nrelative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain\nprovides a new query language for articulating performance queries and an\nalgorithm for generating explanations from a log of past MapReduce job\nexecutions. We formally define the notion of an explanation together with three\nmetrics, relevance, precision, and generality, that measure explanation\nquality. We present the explanation-generation algorithm based on techniques\nrelated to decision-tree building. We evaluate the approach on a log of past\nexecutions on Amazon EC2, and show that our approach can generate quality\nexplanations, outperforming two naive explanation-generation methods."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6401v1", 
    "other_authors": "Francesco Gullo, Andrea Tagarelli", 
    "title": "Uncertain Centroid based Partitional Clustering of Uncertain Data", 
    "arxiv-id": "1203.6401v1", 
    "author": "Andrea Tagarelli", 
    "publish": "2012-03-29T00:05:40Z", 
    "summary": "Clustering uncertain data has emerged as a challenging task in uncertain data\nmanagement and mining. Thanks to a computational complexity advantage over\nother clustering paradigms, partitional clustering has been particularly\nstudied and a number of algorithms have been developed. While existing\nproposals differ mainly in the notions of cluster centroid and clustering\nobjective function, little attention has been given to an analysis of their\ncharacteristics and limits. In this work, we theoretically investigate major\nexisting methods of partitional clustering, and alternatively propose a\nwell-founded approach to clustering uncertain data based on a novel notion of\ncluster centroid. A cluster centroid is seen as an uncertain object defined in\nterms of a random variable whose realizations are derived based on all\ndeterministic representations of the objects to be clustered. As demonstrated\ntheoretically and experimentally, this allows for better representing a cluster\nof uncertain objects, thus supporting a consistently improved clustering\nperformance while maintaining comparable efficiency with existing partitional\nclustering algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6402v1", 
    "other_authors": "Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, Sergei Vassilvitskii", 
    "title": "Scalable K-Means++", 
    "arxiv-id": "1203.6402v1", 
    "author": "Sergei Vassilvitskii", 
    "publish": "2012-03-29T00:06:07Z", 
    "summary": "Over half a century old and showing no signs of aging, k-means remains one of\nthe most popular data processing algorithms. As is well-known, a proper\ninitialization of k-means is crucial for obtaining a good final solution. The\nrecently proposed k-means++ initialization algorithm achieves this, obtaining\nan initial set of centers that is provably close to the optimum solution. A\nmajor downside of the k-means++ is its inherent sequential nature, which limits\nits applicability to massive data: one must make k passes over the data to find\na good initial set of centers. In this work we show how to drastically reduce\nthe number of passes needed to obtain, in parallel, a good initialization. This\nis unlike prevailing efforts on parallelizing k-means that have mostly focused\non the post-initialization phases of k-means. We prove that our proposed\ninitialization algorithm k-means|| obtains a nearly optimal solution after a\nlogarithmic number of passes, and then show that in practice a constant number\nof passes suffices. Experimental evaluation on real-world large-scale data\ndemonstrates that k-means|| outperforms k-means++ in both sequential and\nparallel settings."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6403v1", 
    "other_authors": "Michael Benedikt, Pierre Bourhis, Clemens Ley", 
    "title": "Querying Schemas With Access Restrictions", 
    "arxiv-id": "1203.6403v1", 
    "author": "Clemens Ley", 
    "publish": "2012-03-29T00:06:16Z", 
    "summary": "We study verification of systems whose transitions consist of accesses to a\nWeb-based data-source. An access is a lookup on a relation within a relational\ndatabase, fixing values for a set of positions in the relation. For example, a\ntransition can represent access to a Web form, where the user is restricted to\nfilling in values for a particular set of fields. We look at verifying\nproperties of a schema describing the possible accesses of such a system. We\npresent a language where one can describe the properties of an access path, and\nalso specify additional restrictions on accesses that are enforced by the\nschema. Our main property language, AccLTL, is based on a first-order extension\nof linear-time temporal logic, interpreting access paths as sequences of\nrelational structures. We also present a lower-level automaton model,\nAautomata, which AccLTL specifications can compile into. We show that AccLTL\nand A-automata can express static analysis problems related to \"querying with\nlimited access patterns\" that have been studied in the database literature in\nthe past, such as whether an access is relevant to answering a query, and\nwhether two queries are equivalent in the accessible data they can return. We\nprove decidability and complexity results for several restrictions and variants\nof AccLTL, and explain which properties of paths can be expressed in each\nrestriction."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6404v1", 
    "other_authors": "Goetz Graefe, Harumi Kuno", 
    "title": "Definition, Detection, and Recovery of Single-Page Failures, a Fourth   Class of Database Failures", 
    "arxiv-id": "1203.6404v1", 
    "author": "Harumi Kuno", 
    "publish": "2012-03-29T00:06:48Z", 
    "summary": "The three traditional failure classes are system, media, and transaction\nfailures. Sometimes, however, modern storage exhibits failures that differ from\nall of those. In order to capture and describe such cases, single-page failures\nare introduced as a fourth failure class. This class encompasses all failures\nto read a data page correctly and with plausible contents despite all\ncorrection attempts in lower system levels. Efficient recovery seems to require\na new data structure called the page recovery index. Its transactional\nmaintenance can be accomplished writing the same number of log records as\ntoday's efficient implementations of logging and recovery. Detection and\nrecovery of a single-page failure can be sufficiently fast that the affected\ndata access is merely delayed, without the need to abort the transaction."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6405v1", 
    "other_authors": "Goetz Graefe, Felix Halim, Stratos Idreos, Harumi Kuno, Stefan Manegold", 
    "title": "Concurrency Control for Adaptive Indexing", 
    "arxiv-id": "1203.6405v1", 
    "author": "Stefan Manegold", 
    "publish": "2012-03-29T00:07:00Z", 
    "summary": "Adaptive indexing initializes and optimizes indexes incrementally, as a side\neffect of query processing. The goal is to achieve the benefits of indexes\nwhile hiding or minimizing the costs of index creation. However,\nindex-optimizing side effects seem to turn read-only queries into update\ntransactions that might, for example, create lock contention. This paper\nstudies concurrency control in the context of adaptive indexing. We show that\nthe design and implementation of adaptive indexing rigorously separates index\nstructures from index contents; this relaxes the constraints and requirements\nduring adaptive indexing compared to those of traditional index updates. Our\ndesign adapts to the fact that an adaptive index is refined continuously, and\nexploits any concurrency opportunities in a dynamic way. A detailed\nexperimental analysis demonstrates that (a) adaptive indexing maintains its\nadaptive properties even when running concurrent queries, (b) adaptive indexing\ncan exploit the opportunity for parallelism due to concurrent queries, (c) the\nnumber of concurrency conflicts and any concurrency administration overheads\nfollow an adaptive behavior, decreasing as the workload evolves and adapting to\nthe workload needs."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6406v1", 
    "other_authors": "Nilesh Dalvi, Ashwin Machanavajjhala, Bo Pang", 
    "title": "An Analysis of Structured Data on the Web", 
    "arxiv-id": "1203.6406v1", 
    "author": "Bo Pang", 
    "publish": "2012-03-29T00:07:19Z", 
    "summary": "In this paper, we analyze the nature and distribution of structured data on\nthe Web. Web-scale information extraction, or the problem of creating\nstructured tables using extraction from the entire web, is gathering lots of\nresearch interest. We perform a study to understand and quantify the value of\nWeb-scale extraction, and how structured information is distributed amongst top\naggregator websites and tail sites for various interesting domains. We believe\nthis is the first study of its kind, and gives us new insights for information\nextraction over the Web."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1203.6454v1", 
    "other_authors": "Mohammed Adam Ibrahim Fakharaldien, Jasni Mohamed Zain, Norrozila Sulaiman", 
    "title": "XRecursive: An Efficient Method to Store and Query XML Documents", 
    "arxiv-id": "1203.6454v1", 
    "author": "Norrozila Sulaiman", 
    "publish": "2012-03-29T07:43:18Z", 
    "summary": "Storing XML documents in a relational database is a promising solution\nbecause relational databases are mature and scale very well and they have the\nadvantages that in a relational database XML data and structured data can\ncoexist making it possible to build application that involve both kinds of data\nwith little extra effort . In this paper, we propose an algorithm schema named\nXRecursive that translates XML documents to relational database according to\nthe proposed storing structure. The steps and algorithm are given in details to\ndescribe how to use the storing structure to storage and query XML documents in\nrelational database. Then we report our experimental results on a real database\nto show the performance of our method in some features."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0103v2", 
    "other_authors": "Florin Rusu, Yu Cheng", 
    "title": "A Survey on Array Storage, Query Languages, and Systems", 
    "arxiv-id": "1302.0103v2", 
    "author": "Yu Cheng", 
    "publish": "2013-02-01T07:51:58Z", 
    "summary": "Since scientific investigation is one of the most important providers of\nmassive amounts of ordered data, there is a renewed interest in array data\nprocessing in the context of Big Data. To the best of our knowledge, a unified\nresource that summarizes and analyzes array processing research over its long\nexistence is currently missing. In this survey, we provide a guide for past,\npresent, and future research in array processing. The survey is organized along\nthree main topics. Array storage discusses all the aspects related to array\npartitioning into chunks. The identification of a reduced set of array\noperators to form the foundation for an array query language is analyzed across\nmultiple such proposals. Lastly, we survey real systems for array processing.\nThe result is a thorough survey on array data storage and processing that\nshould be consulted by anyone interested in this research topic, independent of\nexperience level. The survey is not complete though. We greatly appreciate\npointers towards any work we might have forgotten to mention."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0309v2", 
    "other_authors": "Peter Bailis, Aaron Davidson, Alan Fekete, Ali Ghodsi, Joseph M. Hellerstein, Ion Stoica", 
    "title": "Highly Available Transactions: Virtues and Limitations (Extended   Version)", 
    "arxiv-id": "1302.0309v2", 
    "author": "Ion Stoica", 
    "publish": "2013-02-01T22:29:28Z", 
    "summary": "To minimize network latency and remain online during server failures and\nnetwork partitions, many modern distributed data storage systems eschew\ntransactional functionality, which provides strong semantic guarantees for\ngroups of multiple operations over multiple data items. In this work, we\nconsider the problem of providing Highly Available Transactions (HATs):\ntransactional guarantees that do not suffer unavailability during system\npartitions or incur high network latency. We introduce a taxonomy of highly\navailable systems and analyze existing ACID isolation and distributed data\nconsistency guarantees to identify which can and cannot be achieved in HAT\nsystems. This unifies the literature on weak transactional isolation, replica\nconsistency, and highly available systems. We analytically and experimentally\nquantify the availability and performance benefits of HATs--often two to three\norders of magnitude over wide-area networks--and discuss their necessary\nsemantic compromises."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0337v1", 
    "other_authors": "Leon Andretti Abdillah", 
    "title": "Perancangan basisdata sistem informasi penggajian", 
    "arxiv-id": "1302.0337v1", 
    "author": "Leon Andretti Abdillah", 
    "publish": "2013-02-02T03:36:37Z", 
    "summary": "The purpose of this research is to design database scheme of information\nsystem at XYZ University. By using database design methods (conceptual scheme,\nlogical scheme, & physical scheme) the writer designs payroll information\nsystem. The physical scheme is compatible with Borland Delphi Database Engine\nScheme to support the implementation of the I.S. After 3 (three) steps we get 7\n(seven) tables, dan 6 (six) forms. By using this shemce, the system can produce\nseveral reports quickly, accurately, efficiently, and effectively."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0351v2", 
    "other_authors": "Gaurav Saxena, Ruchi Narula, Manish Mishra", 
    "title": "New Dimension Value Introduction for In-Memory What-If Analysis", 
    "arxiv-id": "1302.0351v2", 
    "author": "Manish Mishra", 
    "publish": "2013-02-02T06:51:20Z", 
    "summary": "OLAP systems operate on historical data and provide answers to analysts\nqueries. Recent in-memory implementations provide significant performance\nimprovement for real time ad-hoc analysis. Philosophy and techniques of what-if\nanalysis on data warehouse and in-memory data store based OLAP systems have\nbeen covered in great detail before but exploration of new dimension value\n(attribute) introduction has been limited in the context of what-if analysis.\nWe extend the approach of Andrey Balmin et al of using select modify operator\non data graph to introduce new values for dimensions and measures in a\nread-only in-memory data store as scenarios. Our system constructs scenarios\nwithout materializing the rows and stores the row information as queries. The\nrows associated with the scenarios are constructed as and when required by an\nad-hoc query."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0914v5", 
    "other_authors": "Hung Q. Ngo, Dung T. Nguyen, Christopher R\u00e9, Atri Rudra", 
    "title": "Beyond Worst-Case Analysis for Joins with Minesweeper", 
    "arxiv-id": "1302.0914v5", 
    "author": "Atri Rudra", 
    "publish": "2013-02-05T01:24:12Z", 
    "summary": "We describe a new algorithm, Minesweeper, that is able to satisfy stronger\nruntime guarantees than previous join algorithms (colloquially, `beyond\nworst-case guarantees') for data in indexed search trees. Our first\ncontribution is developing a framework to measure this stronger notion of\ncomplexity, which we call {\\it certificate complexity}, that extends notions of\nBarbay et al. and Demaine et al.; a certificate is a set of propositional\nformulae that certifies that the output is correct. This notion captures a\nnatural class of join algorithms. In addition, the certificate allows us to\ndefine a strictly stronger notion of runtime complexity than traditional\nworst-case guarantees. Our second contribution is to develop a dichotomy\ntheorem for the certificate-based notion of complexity. Roughly, we show that\nMinesweeper evaluates $\\beta$-acyclic queries in time linear in the certificate\nplus the output size, while for any $\\beta$-cyclic query there is some instance\nthat takes superlinear time in the certificate (and for which the output is no\nlarger than the certificate size). We also extend our certificate-complexity\nanalysis to queries with bounded treewidth and the triangle query."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.0971v1", 
    "other_authors": "Leon Andretti Abdillah", 
    "title": "Validasi data dengan menggunakan objek lookup pada borland delphi 7.0", 
    "arxiv-id": "1302.0971v1", 
    "author": "Leon Andretti Abdillah", 
    "publish": "2013-02-05T09:32:54Z", 
    "summary": "Developing an application with some tables must concern the validation of\ninput (specially in Table Child). In order to maximize the accuracy and data\ninput validation. Its called lookup (took data from other dataset). There are\ntwo ways to look up data from Table Parent: 1) Using Objects (DBLookupComboBox\nand DBookupListBox), or 2) Arranging the properties of data types fields (shown\nby using DBGrid). In this article is using Borland Delphi software (Inprise\nproduct). The method is offered using 5 (five) practise steps: 1) Relational\nDatabase Scheme, 2) Form Design, 3) Object DatabasesRelationships Scheme, 4)\nProperties and Field Type Arrangement, and 5) Procedures. The result of this\npaper are: 1) The relationship that using lookup objects are valid, and 2)\nDelphi Lookup Objects can be used for 1-1, 1-N, and M-N relationship."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.1638v1", 
    "other_authors": "Jnanamurthy H. K.", 
    "title": "Discovery of Maximal Frequent Item Sets using Subset Creation", 
    "arxiv-id": "1302.1638v1", 
    "author": "Jnanamurthy H. K.", 
    "publish": "2013-02-07T04:29:39Z", 
    "summary": "Data mining is the practice to search large amount of data to discover data\npatterns. Data mining uses mathematical algorithms to group the data and\nevaluate the future events. Association rule is a research area in the field of\nknowledge discovery. Many data mining researchers had improved upon the quality\nof association rule for business development by incorporating influential\nfactors like utility, number of items sold and for the mining of association\ndata patterns. In this paper, we propose an efficient algorithm to find maximal\nfrequent itemset first. Most of the association rule algorithms used to find\nminimal frequent item first, then with the help of minimal frequent itemsets\nderive the maximal frequent itemsets, these methods consume more time to find\nmaximal frequent itemsets. To overcome this problem, we propose a new approach\nto find maximal frequent itemset directly using the concepts of subsets. The\nproposed method is found to be efficient in finding maximal frequent itemsets."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.1923v1", 
    "other_authors": "Jixue Liu, Chengfei Liu, Theo Haerder, Jeffery Xu Yu", 
    "title": "Update XML Views", 
    "arxiv-id": "1302.1923v1", 
    "author": "Jeffery Xu Yu", 
    "publish": "2013-02-08T01:22:54Z", 
    "summary": "View update is the problem of translating an update to a view to some updates\nto the source data of the view. In this paper, we show the factors determining\nXML view update translation, propose a translation procedure, and propose\ntranslated updates to the source document for different types of views. We\nfurther show that the translated updates are precise. The proposed solution\nmakes it possible for users who do not have access privileges to the source\ndata to update the source data via a view."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.2966v1", 
    "other_authors": "Sherif Sakr, Anna Liu, Ayman G. Fayoumi", 
    "title": "The Family of MapReduce and Large Scale Data Processing Systems", 
    "arxiv-id": "1302.2966v1", 
    "author": "Ayman G. Fayoumi", 
    "publish": "2013-02-13T01:35:03Z", 
    "summary": "In the last two decades, the continuous increase of computational power has\nproduced an overwhelming flow of data which has called for a paradigm shift in\nthe computing architecture and large scale data processing mechanisms.\nMapReduce is a simple and powerful programming model that enables easy\ndevelopment of scalable parallel applications to process vast amounts of data\non large clusters of commodity machines. It isolates the application from the\ndetails of running a distributed program such as issues on data distribution,\nscheduling and fault tolerance. However, the original implementation of the\nMapReduce framework had some limitations that have been tackled by many\nresearch efforts in several followup works after its introduction. This article\nprovides a comprehensive survey for a family of approaches and mechanisms of\nlarge scale data processing mechanisms that have been implemented based on the\noriginal idea of the MapReduce framework and are currently gaining a lot of\nmomentum in both research and industrial communities. We also cover a set of\nintroduced systems that have been implemented to provide declarative\nprogramming interfaces on top of the MapReduce framework. In addition, we\nreview several large scale data processing systems that resemble some of the\nideas of the MapReduce framework for different purposes and application\nscenarios. Finally, we discuss some of the future research directions for\nimplementing the next generation of MapReduce-like solutions."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1302.6556v3", 
    "other_authors": "Theodoros Rekatsinas, Amol Deshpande, Ashwin Machanavajjhala", 
    "title": "On Sharing Private Data with Multiple Non-Colluding Adversaries", 
    "arxiv-id": "1302.6556v3", 
    "author": "Ashwin Machanavajjhala", 
    "publish": "2013-02-26T19:49:55Z", 
    "summary": "We present SPARSI, a theoretical framework for partitioning sensitive data\nacross multiple non-colluding adversaries. Most work in privacy-aware data\nsharing has considered disclosing summaries where the aggregate information\nabout the data is preserved, but sensitive user information is protected.\nNonetheless, there are applications, including online advertising, cloud\ncomputing and crowdsourcing markets, where detailed and fine-grained user-data\nmust be disclosed. We consider a new data sharing paradigm and introduce the\nproblem of privacy-aware data partitioning, where a sensitive dataset must be\npartitioned among k untrusted parties (adversaries). The goal is to maximize\nthe utility derived by partitioning and distributing the dataset, while\nminimizing the amount of sensitive information disclosed. The data should be\ndistributed so that an adversary, without colluding with other adversaries,\ncannot draw additional inferences about the private information, by linking\ntogether multiple pieces of information released to her. The assumption of no\ncollusion is both reasonable and necessary in the above application domains\nthat require release of private user information. SPARSI enables us to formally\ndefine privacy-aware data partitioning using the notion of sensitive properties\nfor modeling private information and a hypergraph representation for describing\nthe interdependencies between data entries and private information. We show\nthat solving privacy-aware partitioning is, in general, NP-hard, but for\nspecific information disclosure functions, good approximate solutions can be\nfound using relaxation techniques. Finally, we present a local search algorithm\napplicable to generic information disclosure functions. We apply SPARSI\ntogether with the proposed algorithms on data from a real advertising scenario\nand show that we can partition data with no disclosure to any single\nadvertiser."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.0866v1", 
    "other_authors": "David W. LeJeune Jr", 
    "title": "Adaptive Partitioning and its Applicability to a Highly Scalable and   Available Geo-Spatial Indexing Solution", 
    "arxiv-id": "1303.0866v1", 
    "author": "David W. LeJeune Jr", 
    "publish": "2013-03-04T21:32:18Z", 
    "summary": "Satellite Tracking of People (STOP) tracks thousands of GPS-enabled devices\n24 hours a day and 365 days a year. With locations captured for each device\nevery minute, STOP servers receive tens of millions of points each day. In\naddition to cataloging these points in real-time, STOP must also respond to\nquestions from customers such as, \"What devices of mine were at this location\ntwo months ago?\" They often then broaden their question to one such as, \"Which\nof my devices have ever been at this location?\" The processing requirements\nnecessary to answer these questions while continuing to process inbound data in\nreal-time is non-trivial.\n  To meet this demand, STOP developed Adaptive Partitioning to provide a\ncost-effective and highly available hardware platform for the geographical and\ntime-spatial indexing capabilities necessary for responding to customer data\nrequests while continuing to catalog inbound data in real-time."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.1951v1", 
    "other_authors": "Dr. Mohammed Otair", 
    "title": "Approximate k-nearest neighbour based spatial clustering using k-d tree", 
    "arxiv-id": "1303.1951v1", 
    "author": "Dr. Mohammed Otair", 
    "publish": "2013-03-08T11:13:02Z", 
    "summary": "Different spatial objects that vary in their characteristics, such as\nmolecular biology and geography, are presented in spatial areas. Methods to\norganize, manage, and maintain those objects in a structured manner are\nrequired. Data mining raised different techniques to overcome these\nrequirements. There are many major tasks of data mining, but the mostly used\ntask is clustering. Data set within the same cluster share common features that\ngive each cluster its characteristics. In this paper, an implementation of\nApproximate kNN-based spatial clustering algorithm using the K-d tree is\nproposed. The major contribution achieved by this research is the use of the\nk-d tree data structure for spatial clustering, and comparing its performance\nto the brute-force approach. The results of the work performed in this paper\nrevealed better performance using the k-d tree, compared to the traditional\nbrute-force approach."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.2310v1", 
    "other_authors": "Xiaohui Li, Vaida Ceikute, Christian S. Jensen, Kian-Lee Tan", 
    "title": "Trajectory Based Optimal Segment Computation in Road Network Databases", 
    "arxiv-id": "1303.2310v1", 
    "author": "Kian-Lee Tan", 
    "publish": "2013-03-10T11:49:41Z", 
    "summary": "Finding a location for a new facility such that the facility attracts the\nmaximal number of customers is a challenging problem. Existing studies either\nmodel customers as static sites and thus do not consider customer movement, or\nthey focus on theoretical aspects and do not provide solutions that are shown\nempirically to be scalable. Given a road network, a set of existing facilities,\nand a collection of customer route traversals, an optimal segment query returns\nthe optimal road network segment(s) for a new facility. We propose a practical\nframework for computing this query, where each route traversal is assigned a\nscore that is distributed among the road segments covered by the route\naccording to a score distribution model. The query returns the road segment(s)\nwith the highest score. To achieve low latency, it is essential to prune the\nvery large search space. We propose two algorithms that adopt different\napproaches to computing the query. Algorithm AUG uses graph augmentation, and\nITE uses iterative road-network partitioning. Empirical studies with real data\nsets demonstrate that the algorithms are capable of offering high performance\nin realistic settings."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.2595v1", 
    "other_authors": "Norbert Paul, Patrick Erik Bradley, Martin Breunig", 
    "title": "Integrating Space, Time, Version and Scale Using Alexandrov Topologies", 
    "arxiv-id": "1303.2595v1", 
    "author": "Martin Breunig", 
    "publish": "2013-03-11T17:46:13Z", 
    "summary": "This article introduces a novel approach to spatial database design. Instead\nof extending the canonical Solid-Face-Edge-Vertex schema by, say, \"hypersolids\"\nthese classes are generalised to a common type SpatialEntity, and the\nindividual BoundedBy relations between two consecutive classes are generalised\nto one BoundedBy relation on SpatialEntity instances. Then the pair\n(SpatialEntity, BoundedBy) is a so-called incidence graph.\n  The novelty about this approach uses the observation that an incidence graph\nrepresents a topological space of SpatialEntity instances because the\nBoundedBy-relation defines a so-called Alexandrov topology for them turning\nthem into a topological space. So spatial data becomes part of mathematical\ntopology and topology can be immediately applied to spatial data. For example,\ncontinuous functions between two instances of spatial data allow the consistent\nmodelling of generalisation. Further, it is also possible to establish a formal\ntopological definition of spatial data dimension, and every topological data\nmodel of arbitrary dimension gets a simple uniform data model. This model\ncovers space-time, and the version history of a spatial model can be\nrepresented by an Alexandrov topology, too. By integrating space, time,\nversion, and scale into one single schema, topological queries across those\naspects are enabled through topological constructions. In fact, the topological\nconstructions cover a relationally complete query language for spaces and can\nbe redefined to operate accordingly on their graph representations.\n  With these observations a relational database schema for a spatial data model\nof dimension 6 and more is developed. The schema seamlessly integrates 4D\nspace-time, levels of detail and version history, and it can be easily expanded\nto also contain non-spatial information or be linked to other data sources."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.3233v1", 
    "other_authors": "Sergio Flesca, Filippo Furfaro, Francesco Parisi", 
    "title": "Consistency Checking and Querying in Probabilistic Databases under   Integrity Constraints", 
    "arxiv-id": "1303.3233v1", 
    "author": "Francesco Parisi", 
    "publish": "2013-03-13T17:53:15Z", 
    "summary": "We address the issue of incorporating a particular yet expressive form of\nintegrity constraints (namely, denial constraints) into probabilistic\ndatabases. To this aim, we move away from the common way of giving semantics to\nprobabilistic databases, which relies on considering a unique interpretation of\nthe data, and address two fundamental problems: consistency checking and query\nevaluation. The former consists in verifying whether there is an interpretation\nwhich conforms to both the marginal probabilities of the tuples and the\nintegrity constraints. The latter is the problem of answering queries under a\n\"cautious\" paradigm, taking into account all interpretations of the data in\naccordance with the constraints. In this setting, we investigate the complexity\nof the above-mentioned problems, and identify several tractable cases of\npractical relevance."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.4277v4", 
    "other_authors": "Iovka Boneva, Radu Ciucanu, Slawek Staworko", 
    "title": "Simple Schemas for Unordered XML", 
    "arxiv-id": "1303.4277v4", 
    "author": "Slawek Staworko", 
    "publish": "2013-03-18T15:03:43Z", 
    "summary": "We consider unordered XML, where the relative order among siblings is\nignored, and propose two simple yet practical schema formalisms: disjunctive\nmultiplicity schemas (DMS), and its restriction, disjunction-free multiplicity\nschemas (MS). We investigate their computational properties and characterize\nthe complexity of the following static analysis problems: schema\nsatisfiability, membership of a tree to the language of a schema, schema\ncontainment, twig query satisfiability, implication, and containment in the\npresence of schema. Our research indicates that the proposed formalisms retain\nmuch of the expressiveness of DTDs without an increase in computational\ncomplexity."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.4471v1", 
    "other_authors": "Oliver Kennedy, Lukasz Ziarek", 
    "title": "BarQL: Collaborating Through Change", 
    "arxiv-id": "1303.4471v1", 
    "author": "Lukasz Ziarek", 
    "publish": "2013-03-19T02:15:24Z", 
    "summary": "Applications such as Google Docs, Office 365, and Dropbox show a growing\ntrend towards incorporating multi-user live collaboration functionality into\nweb applications. These collaborative applications share a need to efficiently\nexpress shared state, and a common strategy for doing so is a shared log\nabstraction. Extensive research efforts on log abstractions by the database,\nprogramming languages, and distributed systems communities have identified a\nvariety of optimization techniques based on the algebraic properties of updates\n(i.e., pairwise commutativity, subsumption, and idempotence). Although these\ntechniques have been applied to specific applications and use-cases, to the\nbest of our knowledge, no attempt has been made to create a general framework\nfor such optimizations in the context of a non-trivial update language. In this\npaper, we introduce mutation languages, a low-level framework for reasoning\nabout the algebraic properties of state updates, or mutations. We define BarQL,\na general purpose state-update language, and show how mutation languages allow\nus to reason about the algebraic properties of updates expressed in BarQ L ."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.4869v1", 
    "other_authors": "Raik Niemann, Nikolaos Korfiatis, Roberto Zicari, Richard G\u00f6bel", 
    "title": "Does query performance optimization lead to energy efficiency? A   comparative analysis of energy efficiency of database operations under   different workload scenarios", 
    "arxiv-id": "1303.4869v1", 
    "author": "Richard G\u00f6bel", 
    "publish": "2013-03-20T08:22:11Z", 
    "summary": "With the continuous increase of online services as well as energy costs,\nenergy consumption becomes a significant cost factor for the evaluation of data\ncenter operations. A significant contributor to that is the performance of\ndatabase servers which are found to constitute the backbone of online services.\nFrom a software approach, while a set of novel data management technologies\nappear in the market e.g. key-value based or in-memory databases, classic\nrelational database management systems (RDBMS) are still widely used. In\naddition from a hardware perspective, the majority of database servers is still\nusing standard magnetic hard drives (HDDs) instead of solid state drives (SSDs)\ndue to lower cost of storage per gigabyte, disregarding the performance boost\nthat might be given due to high cost.\n  In this study we focus on a software based assessment of the energy\nconsumption of a database server by running three different and complete\ndatabase workloads namely TCP-H, Star Schema Benchmark -SSB as well a modified\nbenchmark we have derived for this study called W22. We profile the energy\ndistribution among the ost important server components and by using different\nresource allocation we assess the energy consumption of a typical open source\nRDBMS (PostgreSQL) on a standard server in relation with its performance\n(measured by query time).\n  Results confirm the well-known fact that even for complete workloads,\noptimization of the RDBMS results to lower energy consumption."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1303.6682v1", 
    "other_authors": "Gosta Grahne, Adrian Onet", 
    "title": "Anatomy of the chase", 
    "arxiv-id": "1303.6682v1", 
    "author": "Adrian Onet", 
    "publish": "2013-03-26T22:10:15Z", 
    "summary": "A lot of research activity has recently taken place around the chase\nprocedure, due to its usefulness in data integration, data exchange, query\noptimization, peer data exchange and data correspondence, to mention a few. As\nthe chase has been investigated and further developed by a number of research\ngroups and authors, many variants of the chase have emerged and associated\nresults obtained. Due to the heterogeneous nature of the area it is frequently\ndifficult to verify the scope of each result. In this paper we take closer look\nat recent developments, and provide additional results. Our analysis allows us\ncreate a taxonomy of the chase variations and the properties they satisfy.\n  Two of the most central problems regarding the chase is termination, and\ndiscovery of restricted classes of sets of dependencies that guarantee\ntermination of the chase. The search for the restricted classes has been\nmotivated by a fairly recent result that shows that it is undecidable to\ndetermine whether the chase with a given dependency set will terminate on a\ngiven instance. There is a small dissonance here, since the quest has been for\nclasses of sets of dependencies guaranteeing termination of the chase on all\ninstances, even though the latter problem was not known to be undecidable. We\nresolve the dissonance in this paper by showing that determining whether the\nchase with a given set of dependencies terminates on all instances is\ncoRE-complete. Our reduction also gives us the aforementioned\ninstance-dependent RE-completeness result as a byproduct. For one of the\nrestricted classes, the stratified sets dependencies, we provide new complexity\nresults for the problem of testing whether a given set of dependencies belongs\nto it. These results rectify some previous claims that have occurred in the\nliterature."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.0769v1", 
    "other_authors": "Yasunori Ishihara, Nobutaka Suzuki, Kenji Hashimoto, Shogo Shimizu, Toru Fujiwara", 
    "title": "XPath Satisfiability with Parent Axes or Qualifiers Is Tractable under   Many of Real-World DTDs", 
    "arxiv-id": "1308.0769v1", 
    "author": "Toru Fujiwara", 
    "publish": "2013-08-04T02:20:05Z", 
    "summary": "This paper aims at finding a subclass of DTDs that covers many of the\nreal-world DTDs while offering a polynomial-time complexity for deciding the\nXPath satisfiability problem. In our previous work, we proposed RW-DTDs, which\ncover most of the real-world DTDs (26 out of 27 real-world DTDs and 1406 out of\n1407 DTD rules). However, under RW-DTDs, XPath satisfiability with only child,\ndescendant-or-self, and sibling axes is tractable. In this paper, we propose\nMRW-DTDs, which are slightly smaller than RW-DTDs but have tractability on\nXPath satisfiability with parent axes or qualifiers. MRW-DTDs are a proper\nsuperclass of duplicate-free DTDs proposed by Montazerian et al., and cover 24\nout of the 27 real-world DTDs and 1403 out of the 1407 DTD rules. Under\nMRW-DTDs, we show that XPath satisfiability problems with (1) child, parent,\nand sibling axes, and (2) child and sibling axes and qualifiers are both\ntractable, which are known to be intractable under RW-DTDs."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.1440v1", 
    "other_authors": "L\u00e1szl\u00f3 Dobos, Alexander S. Szalay, Tam\u00e1s Budav\u00e1ri, Istv\u00e1n Csabai, Nolan Li", 
    "title": "Graywulf: A platform for federated scientific databases and services", 
    "arxiv-id": "1308.1440v1", 
    "author": "Nolan Li", 
    "publish": "2013-08-06T23:02:12Z", 
    "summary": "Many fields of science rely on relational database management systems to\nanalyze, publish and share data. Since RDBMS are originally designed for, and\ntheir development directions are primarily driven by, business use cases they\noften lack features very important for scientific applications. Horizontal\nscalability is probably the most important missing feature which makes it\nchallenging to adapt traditional relational database systems to the ever\ngrowing data sizes. Due to the limited support of array data types and metadata\nmanagement, successful application of RDBMS in science usually requires the\ndevelopment of custom extensions. While some of these extensions are specific\nto the field of science, the majority of them could easily be generalized and\nreused in other disciplines. With the Graywulf project we intend to target\nseveral goals. We are building a generic platform that offers reusable\ncomponents for efficient storage, transformation, statistical analysis and\npresentation of scientific data stored in Microsoft SQL Server. Graywulf also\naddresses the distributed computational issues arising from current RDBMS\ntechnologies. The current version supports load balancing of simple queries and\nparallel execution of partitioned queries over a set of mirrored databases.\nUniform user access to the data is provided through a web based query interface\nand a data surface for software clients. Queries are formulated in a slightly\nmodified syntax of SQL that offers a transparent view of the distributed data.\nThe software library consists of several components that can be reused to\ndevelop complex scientific data warehouses: a system registry, administration\ntools to manage entire database server clusters, a sophisticated workflow\nexecution framework, and a SQL parser library."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.1471v1", 
    "other_authors": "R. Arokia Paul Rajan, F. Sagayaraj Francis", 
    "title": "Application of Inventory Management Principles for Efficient Data   Placement in Storage Networks", 
    "arxiv-id": "1308.1471v1", 
    "author": "F. Sagayaraj Francis", 
    "publish": "2013-08-07T04:12:50Z", 
    "summary": "The principles and strategies found in material management are comparable and\nanalogue with the data management. This paper concentrates on the conversion of\nproduct inventory management principles into data inventory management\nprinciples. Efforts were made to enumerate various impacting parameters that\nwould be appropriate to consider if any data inventory model could be plotted."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.2310v1", 
    "other_authors": "Rakesh Duggirala, P. Narayana", 
    "title": "Mining Positive and Negative Association Rules Using CoherentApproach", 
    "arxiv-id": "1308.2310v1", 
    "author": "P. Narayana", 
    "publish": "2013-08-10T13:21:20Z", 
    "summary": "In the data mining field, association rules are discovered having domain\nknowledge specified as a minimum support threshold. The accuracy in setting up\nthis threshold directly influences the number and the quality of association\nrules discovered. Typically, before association rules are mined, a user needs\nto determine a support threshold in order to obtain only the frequent item\nsets. Having users to determine a support threshold attracts a number of\nissues. We propose an association rule mining framework that does not require a\nper-set support threshold. Often, the number of association rules, even though\nlarge in number, misses some interesting rules and the rules quality\nnecessitates further analysis. As a result, decision making using these rules\ncould lead to risky actions."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.3679v1", 
    "other_authors": "Pinaki Mitra, Girish Sundaram, Sreedish PS", 
    "title": "Just In Time Indexing", 
    "arxiv-id": "1308.3679v1", 
    "author": "Sreedish PS", 
    "publish": "2013-08-16T17:49:24Z", 
    "summary": "One of the major challenges being faced by Database managers today is to\nmanage the performance of complex SQL queries which are dynamic in nature.\nSince it is not possible to tune each and every query because of its dynamic\nnature, there is a definite possibility that these queries may cause serious\ndatabase performance issues if left alone. Conventional indexes are useful only\nfor those queries which are frequently executed or those columns which are\nfrequently joined in SQL queries. This proposal is regarding a method, a query\noptimizer for optimizing database queries in a database management system. Just\nIn Time(JIT) indexes are On Demand, temporary indexes created on the fly based\non current needs so that they would be able to satisfy any kind of queries. JIT\nindexes are created only when the configured threshold values for resource\nconsumption are exceeded for a query. JIT indexes will be stored in a temporary\nbasis and will get replaced by new JIT indexes in course of time. The proposal\nis substantiated with the help of experimental programs and with various test\ncases. The idea of parallel programming is also brought into picture as it can\nbe effectively used in a multiprocessor system. Multiple threads are employed\nwhile one set of threads proceed in the conventional way and the other set of\nthreads proceed with the proposed way. A live switch over is made when a\nsuitable stage is reached and from then onwards the proposed method will only\ncome into picture."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICECTECH.2011.5942112", 
    "link": "http://arxiv.org/pdf/1308.4027v2", 
    "other_authors": "Rada Chirkova", 
    "title": "Combined-Semantics Equivalence Is Decidable for a Practical Class of   Conjunctive Queries", 
    "arxiv-id": "1308.4027v2", 
    "author": "Rada Chirkova", 
    "publish": "2013-08-19T14:06:11Z", 
    "summary": "In this paper, we focus on the problem of determining whether two conjunctive\n(\"CQ\") queries posed on relational data are combined-semantics equivalent [9].\nWe continue the tradition of [2,5,9] of studying this problem using the tool of\ncontainment between queries. We introduce a syntactic necessary and sufficient\ncondition for equivalence of queries belonging to a large natural language of\n\"explicit-wave\" combined-semantics CQ queries; this language encompasses (but\nis not limited to) all set, bag, and bag-set queries, and appears to cover all\ncombined-semantics CQ queries that are expressible in SQL. Our result solves in\nthe positive the decidability problem of determining combined-semantics\nequivalence for pairs of explicit-wave CQ queries. That is, for an arbitrary\npair of combined-semantics CQ queries, it is decidable (i) to determine whether\neach of the queries is explicit wave, and (ii) to determine, in case both\nqueries are explicit wave, whether or not they are combined-semantics\nequivalent, by using our syntactic criterion. (The problem of determining\nequivalence for general combined-semantics CQ queries remains open. Even so,\nour syntactic sufficient containment condition could still be used to determine\nthat two general CQ queries are combined-semantics equivalent.) Our equivalence\ntest, as well as our general sufficient condition for containment of\ncombined-semantics CQ queries, reduce correctly to the special cases reported\nin [2,5] for set, bag, and bag-set semantics. Our containment and equivalence\nconditions also properly generalize the results of [9], provided that the\nlatter are restricted to the language of (combined-semantics) CQ queries."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.4048v1", 
    "other_authors": "M. Laxmaiah, A. Govardhan", 
    "title": "Gcube Indexing", 
    "arxiv-id": "1308.4048v1", 
    "author": "A. Govardhan", 
    "publish": "2013-08-19T15:12:18Z", 
    "summary": "Spatial Online Analytical Processing System involves the non-categorical\nattribute information also whereas standard Online Analytical Processing System\ndeals with only categorical attributes. Providing spatial information to the\ndata warehouse (DW); two major challenges faced are;1.Defining and Aggregation\nof Spatial/Continues values and 2.Representation, indexing, updating and\nefficient query processing. In this paper, we present GCUBE(Geographical Cube)\nstorage and indexing procedure to aggregate the spatial information/Continuous\nvalues. We employed the proposed approach storing and indexing using synthetic\nand real data sets and evaluated its build, update and Query time. It is\nobserved that the proposed procedure offers significant performance advantage."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.5585v1", 
    "other_authors": "Bogdan Cautis, Alin Deutsch, Ioana Ileana, Nicola Onose", 
    "title": "Rewriting XPath Queries using View Intersections: Tractability versus   Completeness", 
    "arxiv-id": "1308.5585v1", 
    "author": "Nicola Onose", 
    "publish": "2013-08-26T13:45:05Z", 
    "summary": "The standard approach for optimization of XPath queries by rewriting using\nviews techniques consists in navigating inside a view's output, thus allowing\nthe usage of only one view in the rewritten query. Algorithms for richer\nclasses of XPath rewritings, using intersection or joins on node identifiers,\nhave been proposed, but they either lack completeness guarantees, or require\nadditional information about the data. We identify the tightest restrictions\nunder which an XPath can be rewritten in polynomial time using an intersection\nof views and propose an algorithm that works for any documents or type of\nidentifiers. As a side-effect, we analyze the complexity of the related problem\nof deciding if an XPath with intersection can be equivalently rewritten as one\nwithout intersection or union. We extend our formal study of the view-based\nrewriting problem for XPath by describing also (i) algorithms for more complex\nrewrite plans, with no limitations on the number of intersection and navigation\nsteps inside view outputs they employ, and (ii) adaptations of our techniques\nto deal with XML documents without persistent node Ids, in the presence of XML\nkeys. Complementing our computational complexity study, we describe a\nproof-of-concept implementation of our techniques and possible choices that may\nspeed up execution in practice, regarding how rewrite plans are built, tested\nand executed. We also give a thorough experimental evaluation of these\ntechniques, focusing on scalability and the running time improvements achieved\nby the execution of view-based plans."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.5703v2", 
    "other_authors": "Marcelo Arenas, Gonzalo I. Diaz, Achille Fokoue, Anastasios Kementsietsidis, Kavitha Srinivas", 
    "title": "A Principled Approach to Bridging the Gap between Graph Data and their   Schemas", 
    "arxiv-id": "1308.5703v2", 
    "author": "Kavitha Srinivas", 
    "publish": "2013-08-26T21:26:00Z", 
    "summary": "Although RDF graphs have schema information associated with them, in practice\nit is very common to find cases in which data do not fully conform to their\nschema. A prominent example of this is DBpedia, which is RDF data extracted\nfrom Wikipedia, a publicly editable source of information. In such situations,\nit becomes interesting to study the structural properties of the actual data,\nbecause the schema gives an incomplete description of the organization of a\ndataset. In this paper we have approached the study of the structuredness of an\nRDF graph in a principled way: we propose a framework for specifying\nstructuredness functions, which gauge the degree to which an RDF graph conforms\nto a schema. In particular, we first define a formal language for specifying\nstructuredness functions with expressions we call rules. This language allows a\nuser or a database administrator to state a rule to which an RDF graph may\nfully or partially conform. Then we consider the issue of discovering a\nrefinement of a sort (type) by partitioning the dataset into subsets whose\nstructuredness is over a specified threshold. In particular, we prove that the\nnatural decision problem associated to this refinement problem is NP-complete,\nand we provide a natural translation of this problem into Integer Linear\nProgramming (ILP). Finally, we test this ILP solution with two real world\ndatasets, DBpedia Persons and WordNet Nouns, and 4 different and intuitive\nrules, which gauge the structuredness in different ways. The rules give\nmeaningful refinements of the datasets, showing that our language can be a\npowerful tool for understanding the structure of RDF data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.5933v1", 
    "other_authors": "Ala'a Atallah Al-Mughrabi, Hussein Owaied", 
    "title": "Framework Model for Database Replication within the Availability Zones", 
    "arxiv-id": "1308.5933v1", 
    "author": "Hussein Owaied", 
    "publish": "2013-08-27T17:49:50Z", 
    "summary": "This paper presents a proposed model for database replication model in\nprivate cloud availability regions, which is an enhancement of the SQL Server\nAlwaysOn Layers of Protection Model presents by Microsoft in 2012. The\nenhancement concentrates in the database replication for private cloud\navailability regions through the use of primary and secondary servers. The\nprocesses of proposed model during the client send Write/Read Request to the\nserver, in synchronous and semi synchronous replication level has been\ndescribed in details also the processes of proposed model when the client send\nWrite/Read Request to the Primary Server presented in details. All the types of\nautomatic failover situations are presented in this thesis. Using the proposed\nmodels will increase the performance because each one of the secondary servers\nwill open for Read / Write and allow the clients to connect to the nearby\nsecondary and less loading on each server. Keywords: Availability Regions,\nCloud Computing, Database Replication, SQL Server AlwaysOn, Synchronization."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.6682v1", 
    "other_authors": "Marouane Hachicha, Chantola Kit, J\u00e9r\u00f4me Darmont", 
    "title": "A Novel Query-Based Approach for Addressing Summarizability Issues in   XOLAP", 
    "arxiv-id": "1308.6682v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2013-08-30T09:01:25Z", 
    "summary": "The business intelligence and decision-support systems used in many\napplication domains casually rely on data warehouses, which are\ndecision-oriented data repositories modeled as multidimensional (MD)\nstructures. MD structures help navigate data through hierarchical levels of\ndetail. In many real-world situations, hierarchies in MD models are complex,\nwhich causes data aggregation issues, collectively known as the summarizability\nproblem. This problem leads to incorrect analyses and critically affects\ndecision making. To enforce summarizability, existing approaches alter either\nMD models or data, and must be applied a priori, on a case-by-case basis, by an\nexpert. To alter neither models nor data, a few query-time approaches have been\nproposed recently, but they only detect summarizability issues without solving\nthem. Thus, we propose in this paper a novel approach that automatically\ndetects and processes summarizability issues at query time, without requiring\nany particular expertise from the user. Moreover, while most existing\napproaches are based on the relational model, our approach focus on an XML MD\nmodel, since XML data is customarily used to represent business data and its\nformat better copes with complex hierarchies than the relational model.\nFinally, our experiments show that our method is likely to scale better than a\nreference approach for addressing the summarizability problem in the MD\ncontext."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.6683v1", 
    "other_authors": "Chantola Kit, Marouane Hachicha, J\u00e9r\u00f4me Darmont", 
    "title": "Benchmarking Summarizability Processing in XML Warehouses with Complex   Hierarchies", 
    "arxiv-id": "1308.6683v1", 
    "author": "J\u00e9r\u00f4me Darmont", 
    "publish": "2013-08-30T09:02:02Z", 
    "summary": "Business Intelligence plays an important role in decision making. Based on\ndata warehouses and Online Analytical Processing, a business intelligence tool\ncan be used to analyze complex data. Still, summarizability issues in data\nwarehouses cause ineffective analyses that may become critical problems to\nbusinesses. To settle this issue, many researchers have studied and proposed\nvarious solutions, both in relational and XML data warehouses. However, they\nfind difficulty in evaluating the performance of their proposals since the\navailable benchmarks lack complex hierarchies. In order to contribute to\nsummarizability analysis, this paper proposes an extension to the XML warehouse\nbenchmark (XWeB) with complex hierarchies. The benchmark enables us to generate\nXML data warehouses with scalable complex hierarchies as well as\nsummarizability processing. We experimentally demonstrated that complex\nhierarchies can definitely be included into a benchmark dataset, and that our\nbenchmark is able to compare two alternative approaches dealing with\nsummarizability issues."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1308.6701v1", 
    "other_authors": "Adriana Olteanu, Grigore Stamatescu, Anca Daniela Ionita, Valentin Sgarciu", 
    "title": "Enhanced Data Integration for LabVIEW Laboratory Systems", 
    "arxiv-id": "1308.6701v1", 
    "author": "Valentin Sgarciu", 
    "publish": "2013-08-30T10:25:51Z", 
    "summary": "Integrating data is a basic concern in many accredited laboratories that\nperform a large variety of measurements. However, the present working style in\nengineering faculties does not focus much on this aspect. To deal with this\nchallenge, we developed an educational platform that allows characterization of\nacquisition ensembles, generation of Web pages for lessons, as well as\ntransformation of measured data and storage in a common format. As generally we\nhad to develop individual parsers for each instrument, we also added the\npossibility to integrate the LabVIEW workbench, often used for rapid\ndevelopment of applications in electrical engineering and automatic control.\nThis paper describes how we configure the platform for specific equipment, i.e.\nhow we model it, how we create the learning material and how we integrate the\nresults in a central database. It also introduces a case study for collecting\ndata from a thermocouple-based acquisition system based on LabVIEW, used by\nstudents for a laboratory of measurement technologies and transducers."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1402.1257v2", 
    "other_authors": "Nishant Vadnere, R. G. Mehta, D. P. Rana, N. J. Mistry, M. M. Raghuwanshi", 
    "title": "Incremental classification using Feature Tree", 
    "arxiv-id": "1402.1257v2", 
    "author": "M. M. Raghuwanshi", 
    "publish": "2014-02-06T06:17:18Z", 
    "summary": "In recent years, stream data have become an immensely growing area of\nresearch for the database, computer science and data mining communities. Stream\ndata is an ordered sequence of instances. In many applications of data stream\nmining data can be read only once or a small number of times using limited\ncomputing and storage capabilities. Some of the issues occurred in classifying\nstream data that have significant impact in algorithm development are size of\ndatabase, online streaming, high dimensionality and concept drift. The concept\ndrift occurs when the properties of the historical data and target variable\nchange over time abruptly in such a case that the predictions will become\ninaccurate as time passes. In this paper the framework of incremental\nclassification is proposed to solve the issues for the classification of stream\ndata. The Trie structure based incremental feature tree, Trie structure based\nincremental FP (Frequent Pattern) growth tree and tree based incremental\nclassification algorithm are introduced in the proposed framework."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3406", 
    "link": "http://arxiv.org/pdf/1402.1258v1", 
    "other_authors": "Mohit Kumar Gupta, Vishal Verma, Megha Singh Verma", 
    "title": "In-Memory Database Systems - A Paradigm Shift", 
    "arxiv-id": "1402.1258v1", 
    "author": "Megha Singh Verma", 
    "publish": "2014-02-06T06:17:51Z", 
    "summary": "In today world, organizations like Google, Yahoo, Amazon, Facebook etc. are\nfacing drastic increase in data. This leads to the problem of capturing,\nstoring, managing and analyzing terabytes or petabytes of data, stored in\nmultiple formats, from different internal and external sources. Moreover, new\napplications scenarios like weather forecasting, trading, artificial\nintelligence etc. need huge data processing in real time. These requirements\nexceed the processing capacity of traditional on-disk database management\nsystems to manage this data and to give speedy real time results. Therefore,\ndata management needs new solutions for coping with the challenges of data\nvolumes and processing data in real-time. An in-memory database system (IMDS)\nis a latest breed of database management system which is becoming answer to\nabove challenges with other supporting technologies. IMDS is capable to process\nmassive data distinctly faster. This paper explores IMDS approach and its\nassociated design issues and challenges. It also investigates some famous\ncommercial and open-source IMDS solutions available in the market."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V7P140", 
    "link": "http://arxiv.org/pdf/1402.1327v1", 
    "other_authors": "Mr. Rushirajsinh L. Zala, Mr. Brijesh B. Mehta, Mr. Mahipalsinh R. Zala", 
    "title": "A Survey on Spatial Co-location Patterns Discovery from Spatial Datasets", 
    "arxiv-id": "1402.1327v1", 
    "author": "Mr. Mahipalsinh R. Zala", 
    "publish": "2014-02-06T11:37:45Z", 
    "summary": "Spatial data mining or Knowledge discovery in spatial database is the\nextraction of implicit knowledge, spatial relations and spatial patterns that\nare not explicitly stored in databases. Co-location patterns discovery is the\nprocess of finding the subsets of features that are frequently located together\nin the same geographic area. In this paper, we discuss the different approaches\nlike Rule based approach, Join-less approach, Partial Join approach and\nConstraint neighborhood based approach for finding co-location patterns."
},{
    "category": "cs.DB", 
    "doi": "10.4236/ijcns.2013.612054", 
    "link": "http://arxiv.org/pdf/1402.1469v1", 
    "other_authors": "Evgeniy Pluzhnik, Evgeny Nikulchev", 
    "title": "Use of Dynamical Systems Modeling to Hybrid Cloud Database", 
    "arxiv-id": "1402.1469v1", 
    "author": "Evgeny Nikulchev", 
    "publish": "2014-02-06T19:59:14Z", 
    "summary": "In the article, an experiment is aimed at clarifying the transfer efficiency\nof the database in the cloud infrastructure. The system was added to the\ncontrol unit, which has guided the database search in the local part or in the\ncloud. It is shown that the time data acquisition remains unchanged as a result\nof modification. Suggestions have been made about the use of the theory of\ndynamic systems to hybrid cloud database. The present work is aimed at\nattracting the attention of spe-cialists in the field of cloud database to the\napparatus control theory. The experiment presented in this article allows the\nuse of the description of the known methods for solving important practical\nproblems."
},{
    "category": "cs.DB", 
    "doi": "10.4236/ijcns.2013.612054", 
    "link": "http://arxiv.org/pdf/1402.1546v1", 
    "other_authors": "Renchu Song, Weiwei Sun, Baihua Zheng, Yu Zheng", 
    "title": "PRESS: A Novel Framework of Trajectory Compression in Road Networks", 
    "arxiv-id": "1402.1546v1", 
    "author": "Yu Zheng", 
    "publish": "2014-02-07T03:29:08Z", 
    "summary": "Location data becomes more and more important. In this paper, we focus on the\ntrajectory data, and propose a new framework, namely PRESS (Paralleled\nRoad-Network-Based Trajectory Compression), to effectively compress trajectory\ndata under road network constraints. Different from existing work, PRESS\nproposes a novel representation for trajectories to separate the spatial\nrepresentation of a trajectory from the temporal representation, and proposes a\nHybrid Spatial Compression (HSC) algorithm and error Bounded Temporal\nCompression (BTC) algorithm to compress the spatial and temporal information of\ntrajectories respectively. PRESS also supports common spatial-temporal queries\nwithout fully decompressing the data. Through an extensive experimental study\non real trajectory dataset, PRESS significantly outperforms existing approaches\nin terms of saving storage cost of trajectory data with bounded errors."
},{
    "category": "cs.DB", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.1814v1", 
    "other_authors": "Prof. Paresh Tanna, Dr. Yogesh Ghodasara", 
    "title": "Foundation for Frequent Pattern Mining Algorithms Implementation", 
    "arxiv-id": "1402.1814v1", 
    "author": "Dr. Yogesh Ghodasara", 
    "publish": "2014-02-08T03:25:57Z", 
    "summary": "As with the development of the IT technologies, the amount of accumulated\ndata is also increasing. Thus the role of data mining comes into picture.\nAssociation rule mining becomes one of the significant responsibilities of\ndescriptive technique which can be defined as discovering meaningful patterns\nfrom large collection of data. The frequent pattern mining algorithms determine\nthe frequent patterns from a database. Mining frequent itemset is very\nfundamental part of association rule mining. Many algorithms have been proposed\nfrom last many decades including majors are Apriori, Direct Hashing and\nPruning, FP-Growth, ECLAT etc. The aim of this study is to analyze the existing\ntechniques for mining frequent patterns and evaluate the performance of them by\ncomparing Apriori and DHP algorithms in terms of candidate generation, database\nand transaction pruning. This creates a foundation to develop newer algorithm\nfor frequent pattern mining."
},{
    "category": "cs.DB", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2237v4", 
    "other_authors": "Peter Bailis, Alan Fekete, Michael J. Franklin, Ali Ghodsi, Joseph M. Hellerstein, Ion Stoica", 
    "title": "Coordination Avoidance in Database Systems (Extended Version)", 
    "arxiv-id": "1402.2237v4", 
    "author": "Ion Stoica", 
    "publish": "2014-02-10T19:01:33Z", 
    "summary": "Minimizing coordination, or blocking communication between concurrently\nexecuting operations, is key to maximizing scalability, availability, and high\nperformance in database systems. However, uninhibited coordination-free\nexecution can compromise application correctness, or consistency. When is\ncoordination necessary for correctness? The classic use of serializable\ntransactions is sufficient to maintain correctness but is not necessary for all\napplications, sacrificing potential scalability. In this paper, we develop a\nformal framework, invariant confluence, that determines whether an application\nrequires coordination for correct execution. By operating on application-level\ninvariants over database states (e.g., integrity constraints), invariant\nconfluence analysis provides a necessary and sufficient condition for safe,\ncoordination-free execution. When programmers specify their application\ninvariants, this analysis allows databases to coordinate only when anomalies\nthat might violate invariants are possible. We analyze the invariant confluence\nof common invariants and operations from real-world database systems (i.e.,\nintegrity constraints) and applications and show that many are invariant\nconfluent and therefore achievable without coordination. We apply these results\nto a proof-of-concept coordination-avoiding database prototype and demonstrate\nsizable performance gains compared to serializable execution, notably a 25-fold\nimprovement over prior TPC-C New-Order performance on a 200 server cluster."
},{
    "category": "cs.DB", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2394v1", 
    "other_authors": "Reynold S. Xin, Daniel Crankshaw, Ankur Dave, Joseph E. Gonzalez, Michael J. Franklin, Ion Stoica", 
    "title": "GraphX: Unifying Data-Parallel and Graph-Parallel Analytics", 
    "arxiv-id": "1402.2394v1", 
    "author": "Ion Stoica", 
    "publish": "2014-02-11T08:23:49Z", 
    "summary": "From social networks to language modeling, the growing scale and importance\nof graph data has driven the development of numerous new graph-parallel systems\n(e.g., Pregel, GraphLab). By restricting the computation that can be expressed\nand introducing new techniques to partition and distribute the graph, these\nsystems can efficiently execute iterative graph algorithms orders of magnitude\nfaster than more general data-parallel systems. However, the same restrictions\nthat enable the performance gains also make it difficult to express many of the\nimportant stages in a typical graph-analytics pipeline: constructing the graph,\nmodifying its structure, or expressing computation that spans multiple graphs.\nAs a consequence, existing graph analytics pipelines compose graph-parallel and\ndata-parallel systems using external storage systems, leading to extensive data\nmovement and complicated programming model.\n  To address these challenges we introduce GraphX, a distributed graph\ncomputation framework that unifies graph-parallel and data-parallel\ncomputation. GraphX provides a small, core set of graph-parallel operators\nexpressive enough to implement the Pregel and PowerGraph abstractions, yet\nsimple enough to be cast in relational algebra. GraphX uses a collection of\nquery optimization techniques such as automatic join rewrites to efficiently\nimplement these graph-parallel operators. We evaluate GraphX on real-world\ngraphs and workloads and demonstrate that GraphX achieves comparable\nperformance as specialized graph computation systems, while outperforming them\nin end-to-end graph pipelines. Moreover, GraphX achieves a balance between\nexpressiveness, performance, and ease of use."
},{
    "category": "cs.DB", 
    "doi": "10.14445/2231-2803", 
    "link": "http://arxiv.org/pdf/1402.2487v1", 
    "other_authors": "Partha Ghosh, Soumya Sen", 
    "title": "Materialized View Replacement using Markovs Analysis", 
    "arxiv-id": "1402.2487v1", 
    "author": "Soumya Sen", 
    "publish": "2014-02-11T13:46:30Z", 
    "summary": "Materialized view is used in large data centric applications to expedite\nquery processing. The efficiency of materialized view depends on degree of\nresult found against the queries over the existing materialized views.\nMaterialized views are constructed following different methodologies. Thus the\nefficacy of the materialized views depends on the methodology based on which\nthese are formed. Construction of materialized views are often time consuming\nand moreover after a certain time the performance of the materialized views\ndegrade when the nature of queries change. In this situation either new\nmaterialized views could be constructed from scratch or the existing views\ncould be upgraded. Fresh construction of materialized views has higher time\ncomplexity hence the modification of the existing views is a better\nsolution.Modification process of materialized view is classified under\nmaterialized view maintenance scheme. Materialized view maintenance is a\ncontinuous process and the system could be tuned to ensure a constant rate of\nperformance. If a materialized view construction process is not supported by\nmaterialized view maintenance scheme that system would suffer from performance\ndegradation. In this paper a new materialized view maintenance scheme is\nproposed using markovs analysis to ensure consistent performance. Markovs\nanalysis is chosen here to predict steady state probability over initial\nprobability."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2014.03.09", 
    "link": "http://arxiv.org/pdf/1402.2892v1", 
    "other_authors": "Thabet Slimani, Amor Lazzez", 
    "title": "Efficient Analysis of Pattern and Association Rule Mining Approaches", 
    "arxiv-id": "1402.2892v1", 
    "author": "Amor Lazzez", 
    "publish": "2014-02-12T16:45:54Z", 
    "summary": "The process of data mining produces various patterns from a given data\nsource. The most recognized data mining tasks are the process of discovering\nfrequent itemsets, frequent sequential patterns, frequent sequential rules and\nfrequent association rules. Numerous efficient algorithms have been proposed to\ndo the above processes. Frequent pattern mining has been a focused topic in\ndata mining research with a good number of references in literature and for\nthat reason an important progress has been made, varying from performant\nalgorithms for frequent itemset mining in transaction databases to complex\nalgorithms, such as sequential pattern mining, structured pattern mining,\ncorrelation mining. Association Rule mining (ARM) is one of the utmost current\ndata mining techniques designed to group objects together from large databases\naiming to extract the interesting correlation and relation among huge amount of\ndata. In this article, we provide a brief review and analysis of the current\nstatus of frequent pattern mining and discuss some promising research\ndirections. Additionally, this paper includes a comparative study between the\nperformance of the described approaches."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2014.03.09", 
    "link": "http://arxiv.org/pdf/1402.3329v1", 
    "other_authors": "Justin Hsu, Marco Gaboardi, Andreas Haeberlen, Sanjeev Khanna, Arjun Narayan, Benjamin C. Pierce, Aaron Roth", 
    "title": "Differential Privacy: An Economic Method for Choosing Epsilon", 
    "arxiv-id": "1402.3329v1", 
    "author": "Aaron Roth", 
    "publish": "2014-02-13T22:47:13Z", 
    "summary": "Differential privacy is becoming a gold standard for privacy research; it\noffers a guaranteed bound on loss of privacy due to release of query results,\neven under worst-case assumptions. The theory of differential privacy is an\nactive research area, and there are now differentially private algorithms for a\nwide range of interesting problems.\n  However, the question of when differential privacy works in practice has\nreceived relatively little attention. In particular, there is still no rigorous\nmethod for choosing the key parameter $\\epsilon$, which controls the crucial\ntradeoff between the strength of the privacy guarantee and the accuracy of the\npublished results.\n  In this paper, we examine the role that these parameters play in concrete\napplications, identifying the key questions that must be addressed when\nchoosing specific values. This choice requires balancing the interests of two\ndifferent parties: the data analyst and the prospective participant, who must\ndecide whether to allow their data to be included in the analysis. We propose a\nsimple model that expresses this balance as formulas over a handful of\nparameters, and we use our model to choose $\\epsilon$ on a series of simple\nstatistical studies. We also explore a surprising insight: in some\ncircumstances, a differentially private study can be more accurate than a\nnon-private study for the same cost, under our model. Finally, we discuss the\nsimplifying assumptions in our model and outline a research agenda for possible\nrefinements."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TKDE.2014.2334302", 
    "link": "http://arxiv.org/pdf/1402.5781v1", 
    "other_authors": "Karthik Ramachandra, Mahendra Chavan, Ravindra Guravannavar, S Sudarshan", 
    "title": "Program Transformations for Asynchronous and Batched Query Submission", 
    "arxiv-id": "1402.5781v1", 
    "author": "S Sudarshan", 
    "publish": "2014-02-24T10:39:44Z", 
    "summary": "The performance of database/Web-service backed applications can be\nsignificantly improved by asynchronous submission of queries/requests well\nahead of the point where the results are needed, so that results are likely to\nhave been fetched already when they are actually needed. However, manually\nwriting applications to exploit asynchronous query submission is tedious and\nerror-prone. In this paper we address the issue of automatically transforming a\nprogram written assuming synchronous query submission, to one that exploits\nasynchronous query submission. Our program transformation method is based on\ndata flow analysis and is framed as a set of transformation rules. Our rules\ncan handle query executions within loops, unlike some of the earlier work in\nthis area. We also present a novel approach that, at runtime, can combine\nmultiple asynchronous requests into batches, thereby achieving the benefits of\nbatching in addition to that of asynchronous submission. We have built a tool\nthat implements our transformation techniques on Java programs that use JDBC\ncalls; our tool can be extended to handle Web service calls. We have carried\nout a detailed experimental study on several real-life applications, which\nshows the effectiveness of the proposed rewrite techniques, both in terms of\ntheir applicability and the performance gains achieved."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1402.6407v10", 
    "other_authors": "Samy Chambi, Daniel Lemire, Owen Kaser, Robert Godin", 
    "title": "Better bitmap performance with Roaring bitmaps", 
    "arxiv-id": "1402.6407v10", 
    "author": "Robert Godin", 
    "publish": "2014-02-26T04:38:22Z", 
    "summary": "Bitmap indexes are commonly used in databases and search engines. By\nexploiting bit-level parallelism, they can significantly accelerate queries.\nHowever, they can use much memory, and thus we might prefer compressed bitmap\nindexes. Following Oracle's lead, bitmaps are often compressed using run-length\nencoding (RLE). Building on prior work, we introduce the Roaring compressed\nbitmap format: it uses packed arrays for compression instead of RLE. We compare\nit to two high-performance RLE-based bitmap encoding techniques: WAH (Word\nAligned Hybrid compression scheme) and Concise (Compressed `n' Composable\nInteger Set). On synthetic and real data, we find that Roaring bitmaps (1)\noften compress significantly better (e.g., 2 times) and (2) are faster than the\ncompressed alternatives (up to 900 times faster for intersections). Our results\nchallenge the view that RLE-based bitmap compression is best."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1402.7063v1", 
    "other_authors": "Nikolaos Nodarakis, Spyros Sioutas, Dimitrios Tsoumakos, Giannis Tzimas, Evaggelia Pitoura", 
    "title": "Rapid AkNN Query Processing for Fast Classification of Multidimensional   Data in the Cloud", 
    "arxiv-id": "1402.7063v1", 
    "author": "Evaggelia Pitoura", 
    "publish": "2014-02-27T20:46:09Z", 
    "summary": "A $k$-nearest neighbor ($k$NN) query determines the $k$ nearest points, using\ndistance metrics, from a specific location. An all $k$-nearest neighbor\n(A$k$NN) query constitutes a variation of a $k$NN query and retrieves the $k$\nnearest points for each point inside a database. Their main usage resonates in\nspatial databases and they consist the backbone of many location-based\napplications and not only (i.e. $k$NN joins in databases, classification in\ndata mining). So, it is very crucial to develop methods that answer them\nefficiently. In this work, we propose a novel method for classifying\nmultidimensional data using an A$k$NN algorithm in the MapReduce framework. Our\napproach exploits space decomposition techniques for processing the\nclassification procedure in a parallel and distributed manner. To our\nknowledge, we are the first to study the classification of multidimensional\nobjects under this perspective. Through an extensive experimental evaluation we\nprove that our solution is efficient and scalable in processing the given\nqueries. We investigate many different perspectives that can affect the total\ncomputational cost, such as different dataset distributions, number of\ndimensions, growth of $k$ value and granularity of space decomposition and\nprove that our system is efficient, robust and scalable."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1402.7190v1", 
    "other_authors": "S kumarasawamy, Srikanth P L, Manjula S H, K R Venugopal, L M Patnaik", 
    "title": "Two Stage Prediction Process with Gradient Descent Methods Aligning with   the Data Privacy Preservation", 
    "arxiv-id": "1402.7190v1", 
    "author": "L M Patnaik", 
    "publish": "2014-02-28T10:36:16Z", 
    "summary": "Privacy preservation emphasize on authorization of data, which signifies that\ndata should be accessed only by authorized users. Ensuring the privacy of data\nis considered as one of the challenging task in data management. The\ngeneralization of data with varying concept hierarchies seems to be interesting\nsolution. This paper proposes two stage prediction processes on privacy\npreserved data. The privacy is preserved using generalization and betraying\nother communicating parties by disguising generalized data which adds another\nlevel of privacy. The generalization with betraying is performed in first stage\nto define the knowledge or hypothesis and which is further optimized using\ngradient descent method in second stage prediction for accurate prediction of\ndata. The experiment carried with both batch and stochastic gradient methods\nand it is shown that bulk operation performed by batch takes long time and more\niterations than stochastic to give more accurate solution."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1007.3275v1", 
    "other_authors": "Amel Benabbou, Safia Nait Bahloul, Youssef Amghar", 
    "title": "An Algorithmic Structuration of a Type System for an Orthogonal   Object/Relational Model", 
    "arxiv-id": "1007.3275v1", 
    "author": "Youssef Amghar", 
    "publish": "2010-07-19T20:32:00Z", 
    "summary": "Date and Darwen have proposed a theory of types, the latter forms the basis\nof a detailed presentation of a panoply of simple and complex types. However,\nthis proposal has not been structured in a formal system. Specifically, Date\nand Darwen haven't indicated the formalism of the type system that corresponds\nto the type theory established. In this paper, we propose a pseudo-algorithmic\nand grammatical description of a system of types for Date and Darwen's model.\nOur type system is supposed take into account null values; for such intention,\nwe introduce a particular type noted #, which expresses one or more occurrences\nof incomplete information in a database. Our algebraic grammar describes in\ndetail the complete specification of an inheritance model and the subryping\nrelation induced, thus the different definitions of related concepts."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1007.3781v1", 
    "other_authors": "Alexandra Meliou, Carlos Guestrin, Joseph M. Hellerstein", 
    "title": "Multiresolution Cube Estimators for Sensor Network Aggregate Queries", 
    "arxiv-id": "1007.3781v1", 
    "author": "Joseph M. Hellerstein", 
    "publish": "2010-07-22T01:37:46Z", 
    "summary": "In this work we present in-network techniques to improve the efficiency of\nspatial aggregate queries. Such queries are very common in a sensornet setting,\ndemanding more targeted techniques for their handling. Our approach constructs\nand maintains multi-resolution cube hierarchies inside the network, which can\nbe constructed in a distributed fashion. In case of failures, recovery can also\nbe performed with in-network decisions. In this paper we demonstrate how\nin-network cube hierarchies can be used to summarize sensor data, and how they\ncan be exploited to improve the efficiency of spatial aggregate queries. We\nshow that query plans over our cube summaries can be computed in polynomial\ntime, and we present a PTIME algorithm that selects the minimum number of data\nrequests that can compute the answer to a spatial query. We further extend our\nalgorithm to handle optimization over multiple queries, which can also be done\nin polynomial time. We discuss enriching cube hierarchies with extra summary\ninformation, and present an algorithm for distributed cube construction.\nFinally we investigate node and area failures, and algorithms to recover query\nresults."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.0553v3", 
    "other_authors": "Michael Benedikt, Georg Gottlob, Pierre Senellart", 
    "title": "Determining Relevance of Accesses at Runtime (Extended Version)", 
    "arxiv-id": "1104.0553v3", 
    "author": "Pierre Senellart", 
    "publish": "2011-04-04T12:56:47Z", 
    "summary": "Consider the situation where a query is to be answered using Web sources that\nrestrict the accesses that can be made on backend relational data by requiring\nsome attributes to be given as input of the service. The accesses provide\nlookups on the collection of attributes values that match the binding. They can\ndiffer in whether or not they require arguments to be generated from prior\naccesses. Prior work has focused on the question of whether a query can be\nanswered using a set of data sources, and in developing static access plans\n(e.g., Datalog programs) that implement query answering. We are interested in\ndynamic aspects of the query answering problem: given partial information about\nthe data, which accesses could provide relevant data for answering a given\nquery? We consider immediate and long-term notions of \"relevant accesses\", and\nascertain the complexity of query relevance, for both conjunctive queries and\narbitrary positive queries. In the process, we relate dynamic relevance of an\naccess to query containment under access limitations and characterize the\ncomplexity of this problem; we produce several complexity results about\ncontainment that are of interest by themselves."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.1311v1", 
    "other_authors": "Gowri Shankar Ramaswamy, F Sagayaraj Francis", 
    "title": "Latent table discovery by semantic relationship extraction between   unrelated sets of entity sets of structured data sources", 
    "arxiv-id": "1104.1311v1", 
    "author": "F Sagayaraj Francis", 
    "publish": "2011-04-07T12:11:53Z", 
    "summary": "Querying is one of the basic functionality expected from a database system.\nQuery efficiency is adversely affected by increase in the number of\nparticipating tables. Also, querying based on syntax largely limits the gamut\nof queries a database system can process. Syntactic queries rely on the\ndatabase table structure, which is a cause of concern for large organisations\ndue to incompatibility between heterogeneous systems that store data\ndistributed across geographic locations. Solution to these problems is answered\nto some extent by moving towards semantic technology by making data and the\ndatabase meaningful. In doing so, relationship between sets of entity sets will\nnot be limited only to syntactic constraints but would also permit semantic\nconnections nonetheless such relationships may be tacit, intangible and\ninvisible. The goal of this work is to extract such hidden relationships\nbetween unrelated sets of entity sets and store them in a tangible form. A few\nsample cases are provided to vindicate that the proposed work improves querying\nsignificantly."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.2079v1", 
    "other_authors": "V\u00e9ronique Benzaken, Giuseppe Castagna, Dario Colazzo, Kim Nguyen", 
    "title": "Optimizing XML querying using type-based document projection", 
    "arxiv-id": "1104.2079v1", 
    "author": "Kim Nguyen", 
    "publish": "2011-04-11T22:32:31Z", 
    "summary": "XML data projection (or pruning) is a natural optimization for main memory\nquery engines: given a query Q over a document D, the subtrees of D that are\nnot necessary to evaluate Q are pruned, thus producing a smaller document D';\nthe query Q is then executed on D', hence avoiding to allocate and process\nnodes that will never be reached by Q. In this article, we propose a new\napproach, based on types, that greatly improves current solutions. Besides\nproviding comparable or greater precision and far lesser pruning overhead, our\nsolution ---unlike current approaches--- takes into account backward axes,\npredicates, and can be applied to multiple queries rather than just to single\nones. A side contribution is a new type system for XPath able to handle\nbackward axes. The soundness of our approach is formally proved. Furthermore,\nwe prove that the approach is also complete (i.e., yields the best possible\ntype-driven pruning) for a relevant class of queries and Schemas. We further\nvalidate our approach using the XMark and XPathMark benchmarks and show that\npruning not only improves the main memory query engine's performances (as\nexpected) but also those of state of the art native XML databases."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.2284v1", 
    "other_authors": "C. Ramya, K S Shreedhara, G Kavitha", 
    "title": "Preprocessing: A Prerequisite for Discovering Patterns in WUM Process", 
    "arxiv-id": "1104.2284v1", 
    "author": "G Kavitha", 
    "publish": "2011-04-12T17:42:57Z", 
    "summary": "Web log data is usually diverse and voluminous. This data must be assembled\ninto a consistent, integrated and comprehensive view, in order to be used for\npattern discovery. Without properly cleaning, transforming and structuring the\ndata prior to the analysis, one cannot expect to find meaningful patterns. As\nin most data mining applications, data preprocessing involves removing and\nfiltering redundant and irrelevant data, removing noise, transforming and\nresolving any inconsistencies. In this paper, a complete preprocessing\nmethodology having merging, data cleaning, user/session identification and data\nformatting and summarization activities to improve the quality of data by\nreducing the quantity of data has been proposed. To validate the efficiency of\nthe proposed preprocessing methodology, several experiments are conducted and\nthe results show that the proposed methodology reduces the size of Web access\nlog files down to 73-82% of the initial size and offers richer logs that are\nstructured for further stages of Web Usage Mining (WUM). So preprocessing of\nraw data in this WUM process is the central theme of this paper."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.2721v1", 
    "other_authors": "Alaa H. AL-Hamami, Soukaena H. Hashem", 
    "title": "Optimal Cell Towers Distribution by using Spatial Mining and Geographic   Information System", 
    "arxiv-id": "1104.2721v1", 
    "author": "Soukaena H. Hashem", 
    "publish": "2011-04-14T11:01:56Z", 
    "summary": "The appearance of wireless communication is dramatically changing our life.\nMobile telecommunications emerged as a technological marvel allowing for access\nto personal and other services, devices, computation and communication, in any\nplace and at any time through effortless plug and play. Setting up wireless\nmobile networks often requires: Frequency Assignment, Communication Protocol\nselection, Routing schemes selection, and cells towers distributions. This\nresearch aims to optimize the cells towers distribution by using spatial mining\nwith Geographic Information System (GIS) as a tool. The distribution\noptimization could be done by applying the Digital Elevation Model (DEM) on the\nimage of the area which must be covered with two levels of hierarchy. The\nresearch will apply the spatial association rules technique on the second level\nto select the best square in the cell for placing the antenna. From that the\nproposal will try to minimize the number of installed towers, makes tower's\nlocation feasible, and provides full area coverage."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.2756v1", 
    "other_authors": "Tanzima Hashem, Lars Kulik, Rui Zhang", 
    "title": "Privacy Preserving Moving KNN Queries", 
    "arxiv-id": "1104.2756v1", 
    "author": "Rui Zhang", 
    "publish": "2011-04-14T13:28:08Z", 
    "summary": "We present a novel approach that protects trajectory privacy of users who\naccess location-based services through a moving k nearest neighbor (MkNN)\nquery. An MkNN query continuously returns the k nearest data objects for a\nmoving user (query point). Simply updating a user's imprecise location such as\na region instead of the exact position to a location-based service provider\n(LSP) cannot ensure privacy of the user for an MkNN query: continuous\ndisclosure of regions enables the LSP to follow a user's trajectory. We\nidentify the problem of trajectory privacy that arises from the overlap of\nconsecutive regions while requesting an MkNN query and provide the first\nsolution to this problem. Our approach allows a user to specify the confidence\nlevel that represents a bound of how much more the user may need to travel than\nthe actual kth nearest data object. By hiding a user's required confidence\nlevel and the required number of nearest data objects from an LSP, we develop a\ntechnique to prevent the LSP from tracking the user's trajectory for MkNN\nqueries. We propose an efficient algorithm for the LSP to find k nearest data\nobjects for a region with a user's specified confidence level, which is an\nessential component to evaluate an MkNN query in a privacy preserving manner;\nthis algorithm is at least two times faster than the state-of-the-art\nalgorithm. Extensive experimental studies validate the effectiveness of our\ntrajectory privacy protection technique and the efficiency of our algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.3214v1", 
    "other_authors": "Debabrata Dash, Neoklis Polyzotis, Anastasia Ailamaki", 
    "title": "CoPhy: A Scalable, Portable, and Interactive Index Advisor for Large   Workloads", 
    "arxiv-id": "1104.3214v1", 
    "author": "Anastasia Ailamaki", 
    "publish": "2011-04-16T08:51:27Z", 
    "summary": "Index tuning, i.e., selecting the indexes appropriate for a workload, is a\ncrucial problem in database system tuning. In this paper, we solve index tuning\nfor large problem instances that are common in practice, e.g., thousands of\nqueries in the workload, thousands of candidate indexes and several hard and\nsoft constraints. Our work is the first to reveal that the index tuning problem\nhas a well structured space of solutions, and this space can be explored\nefficiently with well known techniques from linear optimization. Experimental\nresults demonstrate that our approach outperforms state-of-the-art commercial\nand research techniques by a significant margin (up to an order of magnitude)."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.3216v1", 
    "other_authors": "Feng Niu, Christopher R\u00e9, AnHai Doan, Jude Shavlik", 
    "title": "Tuffy: Scaling up Statistical Inference in Markov Logic Networks using   an RDBMS", 
    "arxiv-id": "1104.3216v1", 
    "author": "Jude Shavlik", 
    "publish": "2011-04-16T08:52:25Z", 
    "summary": "Markov Logic Networks (MLNs) have emerged as a powerful framework that\ncombines statistical and logical reasoning; they have been applied to many data\nintensive problems including information extraction, entity resolution, and\ntext mining. Current implementations of MLNs do not scale to large real-world\ndata sets, which is preventing their wide-spread adoption. We present Tuffy\nthat achieves scalability via three novel contributions: (1) a bottom-up\napproach to grounding that allows us to leverage the full power of the\nrelational optimizer, (2) a novel hybrid architecture that allows us to perform\nAI-style local search efficiently using an RDBMS, and (3) a theoretical insight\nthat shows when one can (exponentially) improve the efficiency of stochastic\nlocal search. We leverage (3) to build novel partitioning, loading, and\nparallel algorithms. We show that our approach outperforms state-of-the-art\nimplementations in both quality and speed on several publicly available\ndatasets."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.4384v1", 
    "other_authors": "Nitin Gupta", 
    "title": "EMBANKS: Towards Disk Based Algorithms For Keyword-Search In Structured   Databases", 
    "arxiv-id": "1104.4384v1", 
    "author": "Nitin Gupta", 
    "publish": "2011-04-22T03:49:09Z", 
    "summary": "In recent years, there has been a lot of interest in the field of keyword\nquerying relational databases. A variety of systems such as DBXplorer [ACD02],\nDiscover [HP02] and ObjectRank [BHP04] have been proposed. Another such system\nis BANKS, which enables data and schema browsing together with keyword-based\nsearch for relational databases. It models tuples as nodes in a graph,\nconnected by links induced by foreign key and other relationships. The size of\nthe database graph that BANKS uses is proportional to the sum of the number of\nnodes and edges in the graph. Systems such as SPIN, which search on Personal\nInformation Networks and use BANKS as the backend, maintain a lot of\ninformation about the users' data. Since these systems run on the user\nworkstation which have other demands of memory, such a heavy use of memory is\nunreasonable and if possible, should be avoided. In order to alleviate this\nproblem, we introduce EMBANKS (acronym for External Memory BANKS), a framework\nfor an optimized disk-based BANKS system. The complexity of this framework\nposes many questions, some of which we try to answer in this thesis. We\ndemonstrate that the cluster representation proposed in EMBANKS enables\nin-memory processing of very large database graphs. We also present detailed\nexperiments that show that EMBANKS can significantly reduce database load time\nand query execution times when compared to the original BANKS algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1002/spe.2325", 
    "link": "http://arxiv.org/pdf/1104.4899v1", 
    "other_authors": "Zoran Majkic", 
    "title": "Data Base Mappings and Theory of Sketches", 
    "arxiv-id": "1104.4899v1", 
    "author": "Zoran Majkic", 
    "publish": "2011-04-26T12:25:56Z", 
    "summary": "In this paper we will present the two basic operations for database schemas\nused in database mapping systems (separation and Data Federation), and we will\nexplain why the functorial semantics for database mappings needed a new base\ncategory instead of usual Set category. Successively, it is presented a\ndefinition of the graph G for a schema database mapping system, and the\ndefinition of its sketch category Sch(G). Based on this framework we presented\nfunctorial semantics for database mapping systems with the new base category\nDB."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijwest.2011.2204", 
    "link": "http://arxiv.org/pdf/1104.5139v1", 
    "other_authors": "Hela Limam, Jalel Akaichi", 
    "title": "Web services synchronization health care application", 
    "arxiv-id": "1104.5139v1", 
    "author": "Jalel Akaichi", 
    "publish": "2011-04-27T13:43:12Z", 
    "summary": "With the advance of Web Services technologies and the emergence of Web\nServices into the information space, tremendous opportunities for empowering\nusers and organizations appear in various application domains including\nelectronic commerce, travel, intelligence information gathering and analysis,\nhealth care, digital government, etc. In fact, Web services appear to be s\nsolution for integrating distributed, autonomous and heterogeneous information\nsources. However, as Web services evolve in a dynamic environment which is the\nInternet many changes can occur and affect them. A Web service is affected when\none or more of its associated information sources is affected by schema\nchanges. Changes can alter the information sources contents but also their\nschemas which may render Web services partially or totally undefined. In this\npaper, we propose a solution for integrating information sources into Web\nservices. Then we tackle the Web service synchronization problem by\nsubstituting the affected information sources. Our work is illustrated with a\nhealthcare case study."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2012.02.07", 
    "link": "http://arxiv.org/pdf/1204.0176v1", 
    "other_authors": "M. Rizwan Jameel Qureshi, Mehboob Sharif, Nayyar Iqbal", 
    "title": "Using Fuzzy Logic to Evaluate Normalization Completeness for An Improved   Database Design", 
    "arxiv-id": "1204.0176v1", 
    "author": "Nayyar Iqbal", 
    "publish": "2012-04-01T08:17:47Z", 
    "summary": "A new approach, to measure normalization completeness for conceptual model,\nis introduced using quantitative fuzzy functionality in this paper. We measure\nthe normalization completeness of the conceptual model in two steps. In the\nfirst step, different normalization techniques are analyzed up to Boyce Codd\nNormal Form (BCNF) to find the current normal form of the relation. In the\nsecond step, fuzzy membership values are used to scale the normal form between\n0 and 1. Case studies to explain schema transformation rules and measurements.\nNormalization completeness is measured by considering completeness attributes,\npreventing attributes of the functional dependencies and total number of\nattributes such as if the functional dependency is non-preventing then the\nattributes of that functional dependency are completeness attributes. The\nattributes of functional dependency which prevent to go to the next normal form\nare called preventing attributes."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2012.02.07", 
    "link": "http://arxiv.org/pdf/1204.0864v1", 
    "other_authors": "Phan Nhat Hai, Pascal Poncelet, Maguelonne Teisseire", 
    "title": "GeT_Move: An Efficient and Unifying Spatio-Temporal Pattern Mining   Algorithm for Moving Objects", 
    "arxiv-id": "1204.0864v1", 
    "author": "Maguelonne Teisseire", 
    "publish": "2012-04-04T05:07:47Z", 
    "summary": "Recent improvements in positioning technology has led to a much wider\navailability of massive moving object data. A crucial task is to find the\nmoving objects that travel together. Usually, these object sets are called\nspatio-temporal patterns. Due to the emergence of many different kinds of\nspatio-temporal patterns in recent years, different approaches have been\nproposed to extract them. However, each approach only focuses on mining a\nspecific kind of pattern. In addition to being a painstaking task due to the\nlarge number of algorithms used to mine and manage patterns, it is also time\nconsuming. Moreover, we have to execute these algorithms again whenever new\ndata are added to the existing database. To address these issues, we first\nredefine spatio-temporal patterns in the itemset context. Secondly, we propose\na unifying approach, named GeT_Move, which uses a frequent closed itemset-based\nspatio-temporal pattern-mining algorithm to mine and manage different\nspatio-temporal patterns. GeT_Move is implemented in two versions which are\nGeT_Move and Incremental GeT_Move. To optimize the efficiency and to free the\nparameters setting, we also propose a Parameter Free Incremental GeT_Move\nalgorithm. Comprehensive experiments are performed on real datasets as well as\nlarge synthetic datasets to demonstrate the effectiveness and efficiency of our\napproaches."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2012.02.07", 
    "link": "http://arxiv.org/pdf/1204.2731v1", 
    "other_authors": "Anika Gross, Michael Hartung, Andreas Thor, Erhard Rahm", 
    "title": "How do Ontology Mappings Change in the Life Sciences?", 
    "arxiv-id": "1204.2731v1", 
    "author": "Erhard Rahm", 
    "publish": "2012-04-12T14:05:37Z", 
    "summary": "Mappings between related ontologies are increasingly used to support data\nintegration and analysis tasks. Changes in the ontologies also require the\nadaptation of ontology mappings. So far the evolution of ontology mappings has\nreceived little attention albeit ontologies change continuously especially in\nthe life sciences. We therefore analyze how mappings between popular life\nscience ontologies evolve for different match algorithms. We also evaluate\nwhich semantic ontology changes primarily affect the mappings. We further\ninvestigate alternatives to predict or estimate the degree of future mapping\nchanges based on previous ontology and mapping transitions."
},{
    "category": "cs.DB", 
    "doi": "10.5815/ijitcs.2012.02.07", 
    "link": "http://arxiv.org/pdf/1204.3223v1", 
    "other_authors": "Oussama Tlili, Minyar Sassi, Habib Ounelli", 
    "title": "Intelligent Database Flexible Querying System by Approximate Query   Processing", 
    "arxiv-id": "1204.3223v1", 
    "author": "Habib Ounelli", 
    "publish": "2012-04-14T22:30:15Z", 
    "summary": "Database flexible querying is an alternative to the classic one for users.\nThe use of Formal Concepts Analysis (FCA) makes it possible to make approximate\nanswers that those turned over by a classic DataBase Management System (DBMS).\nSome applications do not need exact answers. However, flexible querying can be\nexpensive in response time. This time is more significant when the flexible\nquerying require the calculation of aggregate functions (\"Sum\", \"Avg\", \"Count\",\n\"Var\" etc.). In this paper, we propose an approach which tries to solve this\nproblem by using Approximate Query Processing (AQP)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.3432v3", 
    "other_authors": "T. Gogacz, J. Marcinkowski", 
    "title": "Converging to the Chase - a Tool for Finite Controllability", 
    "arxiv-id": "1204.3432v3", 
    "author": "J. Marcinkowski", 
    "publish": "2012-04-16T10:13:43Z", 
    "summary": "We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined\nin the cited paper as an element of the Datalog\\pm project, has the finite\ncontrollability property. In order to do that, we develop a technique, which we\nbelieve can have further applications, of approximating Chase(D, T), for a\ndatabase instance D and some sets of tuple generating dependencies T, by an\ninfinite sequence of finite structures, all of them being models of T."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.4948v2", 
    "other_authors": "Jakub Michaliszyn, Anca Muscholl, S\u0142awek Staworko, Piotr Wieczorek, Zhilin Wu", 
    "title": "On Injective Embeddings of Tree Patterns", 
    "arxiv-id": "1204.4948v2", 
    "author": "Zhilin Wu", 
    "publish": "2012-04-22T23:47:28Z", 
    "summary": "We study three different kinds of embeddings of tree patterns:\nweakly-injective, ancestor-preserving, and lca-preserving. While each of them\nis often referred to as injective embedding, they form a proper hierarchy and\ntheir computational properties vary (from P to NP-complete). We present a\nthorough study of the complexity of the model checking problem i.e., is there\nan embedding of a given tree pattern in a given tree, and we investigate the\nimpact of various restrictions imposed on the tree pattern: bound on the degree\nof a node, bound on the height, and type of allowed labels and edges."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.5648v1", 
    "other_authors": "Ibrahim El Bitar, Fatima Zahra Belouadha, Ounsa Roudies", 
    "title": "Taxonomy and synthesis of Web services querying languages", 
    "arxiv-id": "1204.5648v1", 
    "author": "Ounsa Roudies", 
    "publish": "2012-04-25T13:01:23Z", 
    "summary": "Most works on Web services has focused on discovery, composition and\nselection processes of these kinds of services. Other few works were interested\nin how to represent Web services search queries. However, these queries cannot\nbe processed by ensuring a high level of performance without being adequately\nrepresented first. To this end, different query languages were designed. Even\nso, in the absence of a standard, these languages are quite various. Their\ndiversity makes it difficult choosing the most suitable language. In fact, this\nlanguage should be able to cover all types of preferences or requirements of\nclients such as their functional, nonfunctional,temporal or even specific\nconstraints as is the case of geographical or spatial constraints and meet\ntheir needs and preferences helping to provide them the best answer. It must\nalso be mutually simple and imposes no restrictions or at least not too many\nconstraints in terms of prior knowledge to use and also provide a formal or\nsemi-formal queries presentation to support their automatic post-processing. A\ncomparative study is eventually established to allow to reveal the advantages\nand limitations of various existing languages in this context. It is a\nsynthesis of this category of languages discussing their performance level and\ntheir capability to respond to various needs related to the Web services\nresearch and discovery case. The criterions identified at this stage may, in\nour opinion, constitute then the main pre-requisite that a language should\nsatisfy to be called perfect or to be a future standard."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.6076v1", 
    "other_authors": "Kyriakos Mouratidis, Man Lung Yiu", 
    "title": "Shortest Path Computation with No Information Leakage", 
    "arxiv-id": "1204.6076v1", 
    "author": "Man Lung Yiu", 
    "publish": "2012-04-26T23:25:02Z", 
    "summary": "Shortest path computation is one of the most common queries in location-based\nservices (LBSs). Although particularly useful, such queries raise serious\nprivacy concerns. Exposing to a (potentially untrusted) LBS the client's\nposition and her destination may reveal personal information, such as social\nhabits, health condition, shopping preferences, lifestyle choices, etc. The\nonly existing method for privacy-preserving shortest path computation follows\nthe obfuscation paradigm; it prevents the LBS from inferring the source and\ndestination of the query with a probability higher than a threshold. This\nimplies, however, that the LBS still deduces some information (albeit not\nexact) about the client's location and her destination. In this paper we aim at\nstrong privacy, where the adversary learns nothing about the shortest path\nquery. We achieve this via established private information retrieval\ntechniques, which we treat as black-box building blocks. Experiments on real,\nlarge-scale road networks assess the practicality of our schemes."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.6077v1", 
    "other_authors": "Ahmed Metwally, Christos Faloutsos", 
    "title": "V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity   Joins of Multisets and Vectors", 
    "arxiv-id": "1204.6077v1", 
    "author": "Christos Faloutsos", 
    "publish": "2012-04-26T23:25:14Z", 
    "summary": "This work proposes V-SMART-Join, a scalable MapReduce-based framework for\ndiscovering all pairs of similar entities. The V-SMART-Join framework is\napplicable to sets, multisets, and vectors. V-SMART-Join is motivated by the\nobserved skew in the underlying distributions of Internet traffic, and is a\nfamily of 2-stage algorithms, where the first stage computes and joins the\npartial results, and the second stage computes the similarity exactly for all\ncandidate pairs. The V-SMART-Join algorithms are very efficient and scalable in\nthe number of entities, as well as their cardinalities. They were up to 30\ntimes faster than the state of the art algorithm, VCL, when compared on a real\ndataset of a small size. We also established the scalability of the proposed\nalgorithms by running them on a dataset of a realistic size, on which VCL never\nsucceeded to finish. Experiments were run using real datasets of IPs and\ncookies, where each IP is represented as a multiset of cookies, and the goal is\nto discover similar IPs to identify Internet proxies."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.6079v1", 
    "other_authors": "Rishabh Singh, Sumit Gulwani", 
    "title": "Learning Semantic String Transformations from Examples", 
    "arxiv-id": "1204.6079v1", 
    "author": "Sumit Gulwani", 
    "publish": "2012-04-26T23:25:35Z", 
    "summary": "We address the problem of performing semantic transformations on strings,\nwhich may represent a variety of data types (or their combination) such as a\ncolumn in a relational table, time, date, currency, etc. Unlike syntactic\ntransformations, which are based on regular expressions and which interpret a\nstring as a sequence of characters, semantic transformations additionally\nrequire exploiting the semantics of the data type represented by the string,\nwhich may be encoded as a database of relational tables. Manually performing\nsuch transformations on a large collection of strings is error prone and\ncumbersome, while programmatic solutions are beyond the skill-set of end-users.\nWe present a programming by example technology that allows end-users to\nautomate such repetitive tasks. We describe an expressive transformation\nlanguage for semantic manipulation that combines table lookup operations and\nsyntactic manipulations. We then present a synthesis algorithm that can learn\nall transformations in the language that are consistent with the user-provided\nset of input-output examples. We have implemented this technology as an add-in\nfor the Microsoft Excel Spreadsheet system and have evaluated it successfully\nover several benchmarks picked from various Excel help-forums."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.6080v1", 
    "other_authors": "Changbin Liu, Lu Ren, Boon Thau Loo, Yun Mao, Prithwish Basu", 
    "title": "Cologne: A Declarative Distributed Constraint Optimization Platform", 
    "arxiv-id": "1204.6080v1", 
    "author": "Prithwish Basu", 
    "publish": "2012-04-26T23:25:43Z", 
    "summary": "This paper presents Cologne, a declarative optimization platform that enables\nconstraint optimization problems (COPs) to be declaratively specified and\nincrementally executed in distributed systems. Cologne integrates a declarative\nnetworking engine with an off-the-shelf constraint solver. We have developed\nthe Colog language that combines distributed Datalog used in declarative\nnetworking with language constructs for specifying goals and constraints used\nin COPs. Cologne uses novel query processing strategies for processing Colog\nprograms, by combining the use of bottom-up distributed Datalog evaluation with\ntop-down goal-oriented constraint solving. Using case studies based on cloud\nand wireless network optimizations, we demonstrate that Cologne (1) can\nflexibly support a wide range of policy-based optimizations in distributed\nsystems, (2) results in orders of magnitude less code compared to imperative\nimplementations, and (3) is highly efficient with low overhead and fast\nconvergence times."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1204.6081v1", 
    "other_authors": "Yi Zhang, Jun Yang", 
    "title": "Optimizing I/O for Big Array Analytics", 
    "arxiv-id": "1204.6081v1", 
    "author": "Jun Yang", 
    "publish": "2012-04-26T23:25:50Z", 
    "summary": "Big array analytics is becoming indispensable in answering important\nscientific and business questions. Most analysis tasks consist of multiple\nsteps, each making one or multiple passes over the arrays to be analyzed and\ngenerating intermediate results. In the big data setting, I/O optimization is a\nkey to efficient analytics. In this paper, we develop a framework and\ntechniques for capturing a broad range of analysis tasks expressible in\nnested-loop forms, representing them in a declarative way, and optimizing their\nI/O by identifying sharing opportunities. Experiment results show that our\noptimizer is capable of finding execution plans that exploit nontrivial I/O\nsharing opportunities with significant savings."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0132v1", 
    "other_authors": "Rakesh Pimplikar, Sunita Sarawagi", 
    "title": "Answering Table Queries on the Web using Column Keywords", 
    "arxiv-id": "1207.0132v1", 
    "author": "Sunita Sarawagi", 
    "publish": "2012-06-30T20:10:21Z", 
    "summary": "We present the design of a structured search engine which returns a\nmulti-column table in response to a query consisting of keywords describing\neach of its columns. We answer such queries by exploiting the millions of\ntables on the Web because these are much richer sources of structured knowledge\nthan free-format text. However, a corpus of tables harvested from arbitrary\nHTML web pages presents huge challenges of diversity and redundancy not seen in\ncentrally edited knowledge bases. We concentrate on one concrete task in this\npaper. Given a set of Web tables T1, . . ., Tn, and a query Q with q sets of\nkeywords Q1, . . ., Qq, decide for each Ti if it is relevant to Q and if so,\nidentify the mapping between the columns of Ti and query columns. We represent\nthis task as a graphical model that jointly maps all tables by incorporating\ndiverse sources of clues spanning matches in different parts of the table,\ncorpus-wide co-occurrence statistics, and content overlap across table columns.\nWe define a novel query segmentation model for matching keywords to table\ncolumns, and a robust mechanism of exploiting content overlap across table\ncolumns. We design efficient inference algorithms based on bipartite matching\nand constrained graph cuts to solve the joint labeling task. Experiments on a\nworkload of 59 queries over a 25 million web table corpus shows significant\nboost in accuracy over baseline IR methods."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0134v1", 
    "other_authors": "Lukas Blunschi, Claudio Jossen, Donald Kossman, Magdalini Mori, Kurt Stockinger", 
    "title": "SODA: Generating SQL for Business Users", 
    "arxiv-id": "1207.0134v1", 
    "author": "Kurt Stockinger", 
    "publish": "2012-06-30T20:15:28Z", 
    "summary": "The purpose of data warehouses is to enable business analysts to make better\ndecisions. Over the years the technology has matured and data warehouses have\nbecome extremely successful. As a consequence, more and more data has been\nadded to the data warehouses and their schemas have become increasingly\ncomplex. These systems still work great in order to generate pre-canned\nreports. However, with their current complexity, they tend to be a poor match\nfor non tech-savvy business analysts who need answers to ad-hoc queries that\nwere not anticipated. This paper describes the design, implementation, and\nexperience of the SODA system (Search over DAta Warehouse). SODA bridges the\ngap between the business needs of analysts and the technical complexity of\ncurrent data warehouses. SODA enables a Google-like search experience for data\nwarehouses by taking keyword queries of business users and automatically\ngenerating executable SQL. The key idea is to use a graph pattern matching\nalgorithm that uses the metadata model of the data warehouse. Our results with\nreal data from a global player in the financial services industry show that\nSODA produces queries with high precision and recall, and makes it much easier\nfor business users to interactively explore highly-complex data warehouses."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0135v1", 
    "other_authors": "Manolis Terrovitis, John Liagouris, Nikos Mamoulis, Spiros Skiadopoulos", 
    "title": "Privacy Preservation by Disassociation", 
    "arxiv-id": "1207.0135v1", 
    "author": "Spiros Skiadopoulos", 
    "publish": "2012-06-30T20:16:16Z", 
    "summary": "In this work, we focus on protection against identity disclosure in the\npublication of sparse multidimensional data. Existing multidimensional\nanonymization techniquesa) protect the privacy of users either by altering the\nset of quasi-identifiers of the original data (e.g., by generalization or\nsuppression) or by adding noise (e.g., using differential privacy) and/or (b)\nassume a clear distinction between sensitive and non-sensitive information and\nsever the possible linkage. In many real world applications the above\ntechniques are not applicable. For instance, consider web search query logs.\nSuppressing or generalizing anonymization methods would remove the most\nvaluable information in the dataset: the original query terms. Additionally,\nweb search query logs contain millions of query terms which cannot be\ncategorized as sensitive or non-sensitive since a term may be sensitive for a\nuser and non-sensitive for another. Motivated by this observation, we propose\nan anonymization technique termed disassociation that preserves the original\nterms but hides the fact that two or more different terms appear in the same\nrecord. We protect the users' privacy by disassociating record terms that\nparticipate in identifying combinations. This way the adversary cannot\nassociate with high probability a record with a rare combination of terms. To\nthe best of our knowledge, our proposal is the first to employ such a technique\nto provide protection against identity disclosure. We propose an anonymization\nalgorithm based on our approach and evaluate its performance on real and\nsynthetic datasets, comparing it against other state-of-the-art methods based\non generalization and differential privacy."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0136v1", 
    "other_authors": "Bhargav Kanagal, Amr Ahmed, Sandeep Pandey, Vanja Josifovski, Jeff Yuan, Lluis Garcia-Pueyo", 
    "title": "Supercharging Recommender Systems using Taxonomies for Learning User   Purchase Behavior", 
    "arxiv-id": "1207.0136v1", 
    "author": "Lluis Garcia-Pueyo", 
    "publish": "2012-06-30T20:17:05Z", 
    "summary": "Recommender systems based on latent factor models have been effectively used\nfor understanding user interests and predicting future actions. Such models\nwork by projecting the users and items into a smaller dimensional space,\nthereby clustering similar users and items together and subsequently compute\nsimilarity between unknown user-item pairs. When user-item interactions are\nsparse (sparsity problem) or when new items continuously appear (cold start\nproblem), these models perform poorly. In this paper, we exploit the\ncombination of taxonomies and latent factor models to mitigate these issues and\nimprove recommendation accuracy. We observe that taxonomies provide structure\nsimilar to that of a latent factor model: namely, it imposes human-labeled\ncategories (clusters) over items. This leads to our proposed taxonomy-aware\nlatent factor model (TF) which combines taxonomies and latent factors using\nadditive models. We develop efficient algorithms to train the TF models, which\nscales to large number of users/items and develop scalable\ninference/recommendation algorithms by exploiting the structure of the\ntaxonomy. In addition, we extend the TF model to account for the temporal\ndynamics of user interests using high-order Markov chains. To deal with\nlarge-scale data, we develop a parallel multi-core implementation of our TF\nmodel. We empirically evaluate the TF model for the task of predicting user\npurchases using a real-world shopping dataset spanning more than a million\nusers and products. Our experiments demonstrate the benefits of using our TF\nmodels over existing approaches, in terms of both prediction accuracy and\nrunning time."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0137v1", 
    "other_authors": "Yanif Ahmad, Oliver Kennedy, Christoph Koch, Milos Nikolic", 
    "title": "DBToaster: Higher-order Delta Processing for Dynamic, Frequently Fresh   Views", 
    "arxiv-id": "1207.0137v1", 
    "author": "Milos Nikolic", 
    "publish": "2012-06-30T20:17:47Z", 
    "summary": "Applications ranging from algorithmic trading to scientific data analysis\nrequire realtime analytics based on views over databases that change at very\nhigh rates. Such views have to be kept fresh at low maintenance cost and\nlatencies. At the same time, these views have to support classical SQL, rather\nthan window semantics, to enable applications that combine current with aged or\nhistorical data. In this paper, we present viewlet transforms, a recursive\nfinite differencing technique applied to queries. The viewlet transform\nmaterializes a query and a set of its higher-order deltas as views. These views\nsupport each other's incremental maintenance, leading to a reduced overall view\nmaintenance cost. The viewlet transform of a query admits efficient evaluation,\nthe elimination of certain expensive query operations, and aggressive\nparallelization. We develop viewlet transforms into a workable query execution\ntechnique, present a heuristic and cost-based optimization framework, and\nreport on experiments with a prototype dynamic data management system that\ncombines viewlet transforms with an optimizing compilation technique. The\nsystem supports tens of thousands of complete view refreshes a second for a\nwide range of queries."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0139v1", 
    "other_authors": "Odysseas Papapetrou, Minos Garofalakis, Antonios Deligiannakis", 
    "title": "Sketch-based Querying of Distributed Sliding-Window Data Streams", 
    "arxiv-id": "1207.0139v1", 
    "author": "Antonios Deligiannakis", 
    "publish": "2012-06-30T20:19:09Z", 
    "summary": "While traditional data-management systems focus on evaluating single, ad-hoc\nqueries over static data sets in a centralized setting, several emerging\napplications require (possibly, continuous) answers to queries on dynamic data\nthat is widely distributed and constantly updated. Furthermore, such query\nanswers often need to discount data that is \"stale\", and operate solely on a\nsliding window of recent data arrivals (e.g., data updates occurring over the\nlast 24 hours). Such distributed data streaming applications mandate novel\nalgorithmic solutions that are both time- and space-efficient (to manage\nhigh-speed data streams), and also communication-efficient (to deal with\nphysical data distribution). In this paper, we consider the problem of complex\nquery answering over distributed, high-dimensional data streams in the\nsliding-window model. We introduce a novel sketching technique (termed\nECM-sketch) that allows effective summarization of streaming data over both\ntime-based and count-based sliding windows with probabilistic accuracy\nguarantees. Our sketch structure enables point as well as inner-product\nqueries, and can be employed to address a broad range of problems, such as\nmaintaining frequency statistics, finding heavy hitters, and computing\nquantiles in the sliding-window model. Focusing on distributed environments, we\ndemonstrate how ECM-sketches of individual, local streams can be composed to\ngenerate a (low-error) ECM-sketch summary of the order-preserving aggregation\nof all streams; furthermore, we show how ECM-sketches can be exploited for\ncontinuous monitoring of sliding-window queries over distributed streams. Our\nextensive experimental study with two real-life data sets validates our\ntheoretical claims and verifies the effectiveness of our techniques. To the\nbest of our knowledge, ours is the first work to address efficient,\nguaranteed-error complex query answ...[truncated]."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0140v1", 
    "other_authors": "Hoang Tam Vo, Sheng Wang, Divyakant Agrawal, Gang Chen, Beng Chin Ooi", 
    "title": "LogBase: A Scalable Log-structured Database System in the Cloud", 
    "arxiv-id": "1207.0140v1", 
    "author": "Beng Chin Ooi", 
    "publish": "2012-06-30T20:19:50Z", 
    "summary": "Numerous applications such as financial transactions (e.g., stock trading)\nare write-heavy in nature. The shift from reads to writes in web applications\nhas also been accelerating in recent years. Write-ahead-logging is a common\napproach for providing recovery capability while improving performance in most\nstorage systems. However, the separation of log and application data incurs\nwrite overheads observed in write-heavy environments and hence adversely\naffects the write throughput and recovery time in the system. In this paper, we\nintroduce LogBase - a scalable log-structured database system that adopts\nlog-only storage for removing the write bottleneck and supporting fast system\nrecovery. LogBase is designed to be dynamically deployed on commodity clusters\nto take advantage of elastic scaling property of cloud environments. LogBase\nprovides in-memory multiversion indexes for supporting efficient access to data\nmaintained in the log. LogBase also supports transactions that bundle read and\nwrite operations spanning across multiple records. We implemented the proposed\nsystem and compared it with HBase and a disk-based log-structured\nrecord-oriented system modeled after RAMCloud. The experimental results show\nthat LogBase is able to provide sustained write throughput, efficient data\naccess out of the cache, and effective system recovery."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0141v1", 
    "other_authors": "Wei Lu, Yanyan Shen, Su Chen, Beng Chin Ooi", 
    "title": "Efficient Processing of k Nearest Neighbor Joins using MapReduce", 
    "arxiv-id": "1207.0141v1", 
    "author": "Beng Chin Ooi", 
    "publish": "2012-06-30T20:20:31Z", 
    "summary": "k nearest neighbor join (kNN join), designed to find k nearest neighbors from\na dataset S for every object in another dataset R, is a primitive operation\nwidely adopted by many data mining applications. As a combination of the k\nnearest neighbor query and the join operation, kNN join is an expensive\noperation. Given the increasing volume of data, it is difficult to perform a\nkNN join on a centralized machine efficiently. In this paper, we investigate\nhow to perform kNN join using MapReduce which is a well-accepted framework for\ndata-intensive applications over clusters of computers. In brief, the mappers\ncluster objects into groups; the reducers perform the kNN join on each group of\nobjects separately. We design an effective mapping mechanism that exploits\npruning rules for distance filtering, and hence reduces both the shuffling and\ncomputational costs. To reduce the shuffling cost, we propose two approximate\nalgorithms to minimize the number of replicas. Extensive experiments on our\nin-house cluster demonstrate that our proposed methods are efficient, robust\nand scalable."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0142v1", 
    "other_authors": "Nikolay Laptev, Kai Zeng, Carlo Zaniolo", 
    "title": "Early Accurate Results for Advanced Analytics on MapReduce", 
    "arxiv-id": "1207.0142v1", 
    "author": "Carlo Zaniolo", 
    "publish": "2012-06-30T20:21:10Z", 
    "summary": "Approximate results based on samples often provide the only way in which\nadvanced analytical applications on very massive data sets can satisfy their\ntime and resource constraints. Unfortunately, methods and tools for the\ncomputation of accurate early results are currently not supported in\nMapReduce-oriented systems although these are intended for `big data'.\nTherefore, we proposed and implemented a non-parametric extension of Hadoop\nwhich allows the incremental computation of early results for arbitrary\nwork-flows, along with reliable on-line estimates of the degree of accuracy\nachieved so far in the computation. These estimates are based on a technique\ncalled bootstrapping that has been widely employed in statistics and can be\napplied to arbitrary functions and data distributions. In this paper, we\ndescribe our Early Accurate Result Library (EARL) for Hadoop that was designed\nto minimize the changes required to the MapReduce framework. Various tests of\nEARL of Hadoop are presented to characterize the frequent situations where EARL\ncan provide major speed-ups over the current version of Hadoop."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0143v1", 
    "other_authors": "Xuan Liu, Meiyu Lu, Beng Chin Ooi, Yanyan Shen, Sai Wu, Meihui Zhang", 
    "title": "CDAS: A Crowdsourcing Data Analytics System", 
    "arxiv-id": "1207.0143v1", 
    "author": "Meihui Zhang", 
    "publish": "2012-06-30T20:21:52Z", 
    "summary": "Some complex problems, such as image tagging and natural language processing,\nare very challenging for computers, where even state-of-the-art technology is\nyet able to provide satisfactory accuracy. Therefore, rather than relying\nsolely on developing new and better algorithms to handle such tasks, we look to\nthe crowdsourcing solution -- employing human participation -- to make good the\nshortfall in current technology. Crowdsourcing is a good supplement to many\ncomputer tasks. A complex job may be divided into computer-oriented tasks and\nhuman-oriented tasks, which are then assigned to machines and humans\nrespectively. To leverage the power of crowdsourcing, we design and implement a\nCrowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to\nsupport the deployment of various crowdsourcing applications. The core part of\nCDAS is a quality-sensitive answering model, which guides the crowdsourcing\nengine to process and monitor the human tasks. In this paper, we introduce the\nprinciples of our quality-sensitive model. To satisfy user required accuracy,\nthe model guides the crowdsourcing query engine for the design and processing\nof the corresponding crowdsourcing jobs. It provides an estimated accuracy for\neach generated result based on the human workers' historical performances. When\nverifying the quality of the result, the model employs an online strategy to\nreduce waiting time. To show the effectiveness of the model, we implement and\ndeploy two analytics jobs on CDAS, a twitter sentiment analytics job and an\nimage tagging job. We use real Twitter and Flickr data as our queries\nrespectively. We compare our approaches with state-of-the-art classification\nand image annotation techniques. The results show that the human-assisted\nmethods can indeed achieve a much higher accuracy. By embedding the\nquality-sensitive model into crowdsourcing query engine, we\neffectiv...[truncated]."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0144v1", 
    "other_authors": "Mayank Sachan, Arnab Bhattacharya", 
    "title": "Mining Statistically Significant Substrings using the Chi-Square   Statistic", 
    "arxiv-id": "1207.0144v1", 
    "author": "Arnab Bhattacharya", 
    "publish": "2012-06-30T20:22:30Z", 
    "summary": "The problem of identification of statistically significant patterns in a\nsequence of data has been applied to many domains such as intrusion detection\nsystems, financial models, web-click records, automated monitoring systems,\ncomputational biology, cryptology, and text analysis. An observed pattern of\nevents is deemed to be statistically significant if it is unlikely to have\noccurred due to randomness or chance alone. We use the chi-square statistic as\na quantitative measure of statistical significance. Given a string of\ncharacters generated from a memoryless Bernoulli model, the problem is to\nidentify the substring for which the empirical distribution of single letters\ndeviates the most from the distribution expected from the generative Bernoulli\nmodel. This deviation is captured using the chi-square measure. The most\nsignificant substring (MSS) of a string is thus defined as the substring having\nthe highest chi-square value. Till date, to the best of our knowledge, there\ndoes not exist any algorithm to find the MSS in better than O(n^2) time, where\nn denotes the length of the string. In this paper, we propose an algorithm to\nfind the most significant substring, whose running time is O(n^{3/2}) with high\nprobability. We also study some variants of this problem such as finding the\ntop-t set, finding all substrings having chi-square greater than a fixed\nthreshold and finding the MSS among substrings greater than a given length. We\nexperimentally demonstrate the asymptotic behavior of the MSS on varying the\nstring size and alphabet size. We also describe some applications of our\nalgorithm on cryptology and real world data from finance and sports. Finally,\nwe compare our technique with the existing heuristics for finding the MSS."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0145v1", 
    "other_authors": "Martina-Cezara Albutiu, Alfons Kemper, Thomas Neumann", 
    "title": "Massively Parallel Sort-Merge Joins in Main Memory Multi-Core Database   Systems", 
    "arxiv-id": "1207.0145v1", 
    "author": "Thomas Neumann", 
    "publish": "2012-06-30T20:23:09Z", 
    "summary": "Two emerging hardware trends will dominate the database system technology in\nthe near future: increasing main memory capacities of several TB per server and\nmassively parallel multi-core processing. Many algorithmic and control\ntechniques in current database technology were devised for disk-based systems\nwhere I/O dominated the performance. In this work we take a new look at the\nwell-known sort-merge join which, so far, has not been in the focus of research\nin scalable massively parallel multi-core data processing as it was deemed\ninferior to hash joins. We devise a suite of new massively parallel sort-merge\n(MPSM) join algorithms that are based on partial partition-based sorting.\nContrary to classical sort-merge joins, our MPSM algorithms do not rely on a\nhard to parallelize final merge step to create one complete sort order. Rather\nthey work on the independently created runs in parallel. This way our MPSM\nalgorithms are NUMA-affine as all the sorting is carried out on local memory\npartitions. An extensive experimental evaluation on a modern 32-core machine\nwith one TB of main memory proves the competitive performance of MPSM on large\nmain memory databases with billions of objects. It scales (almost) linearly in\nthe number of employed cores and clearly outperforms competing hash join\nproposals - in particular it outperforms the \"cutting-edge\" Vectorwise parallel\nquery engine by a factor of four."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.0147v1", 
    "other_authors": "Tian Luo, Rubao Lee, Michael Mesnier, Feng Chen, Xiaodong Zhang", 
    "title": "hStorage-DB: Heterogeneity-aware Data Management to Exploit the Full   Capability of Hybrid Storage Systems", 
    "arxiv-id": "1207.0147v1", 
    "author": "Xiaodong Zhang", 
    "publish": "2012-06-30T20:41:39Z", 
    "summary": "As storage systems become increasingly heterogeneous and complex, it adds\nburdens on DBAs, causing suboptimal performance even after a lot of human\nefforts have been made. In addition, existing monitoring-based storage\nmanagement by access pattern detections has difficulties to handle workloads\nthat are highly dynamic and concurrent. To achieve high performance by best\nutilizing heterogeneous storage devices, we have designed and implemented a\nheterogeneity-aware software framework for DBMS storage management called\nhStorage-DB, where semantic information that is critical for storage I/O is\nidentified and passed to the storage manager. According to the collected\nsemantic information, requests are classified into different types. Each type\nis assigned a proper QoS policy supported by the underlying storage system, so\nthat every request will be served with a suitable storage device. With\nhStorage-DB, we can well utilize semantic information that cannot be detected\nthrough data access monitoring but is particularly important for a hybrid\nstorage system. To show the effectiveness of hStorage-DB, we have implemented a\nsystem prototype that consists of an I/O request classification enabled DBMS,\nand a hybrid storage system that is organized into a two-level caching\nhierarchy. Our performance evaluation shows that hStorage-DB can automatically\nmake proper decisions for data allocation in different storage devices and make\nsubstantial performance improvements in a cost-efficient way."
},{
    "category": "cs.DB", 
    "doi": "10.1109/LICS.2013.61", 
    "link": "http://arxiv.org/pdf/1207.1535v1", 
    "other_authors": "Prof Rudresh Shirwaikar, Nikhil Rajadhyax", 
    "title": "Data Mining on Educational Domain", 
    "arxiv-id": "1207.1535v1", 
    "author": "Nikhil Rajadhyax", 
    "publish": "2012-07-06T07:06:06Z", 
    "summary": "Educational data mining (EDM) is defined as the area of scientific inquiry\ncentered around the development of methods for making discoveries within the\nunique kinds of data that come from educational settings, and using those\nmethods to better understand students and the settings which they learn in.\nData mining enables organizations to use their current reporting capabilities\nto uncover and understand hidden patterns in vast databases. As a result of\nthis insight, institutions are able to allocate resources and staff more\neffectively. In this paper, we present a real-world experiment conducted in\nShree Rayeshwar Institute of Engineering and Information Technology (SRIEIT) in\nGoa, India. Here we found the relevant subjects in an undergraduate syllabus\nand the strength of their relationship. We have also focused on classification\nof students into different categories such as good, average, poor depending on\ntheir marks scored by them by obtaining a decision tree which will predict the\nperformance of the students and accordingly help the weaker section of students\nto improve in their academics. We have also found clusters of students for\nhelping in analyzing student's performance and also improvising the subject\nteaching in that particular subject."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.2189v3", 
    "other_authors": "Daniel Lemire, Owen Kaser, Eduardo Gutarra", 
    "title": "Reordering Rows for Better Compression: Beyond the Lexicographic Order", 
    "arxiv-id": "1207.2189v3", 
    "author": "Eduardo Gutarra", 
    "publish": "2012-07-09T21:47:13Z", 
    "summary": "Sorting database tables before compressing them improves the compression\nrate. Can we do better than the lexicographical order? For minimizing the\nnumber of runs in a run-length encoding compression scheme, the best approaches\nto row-ordering are derived from traveling salesman heuristics, although there\nis a significant trade-off between running time and compression. A new\nheuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades\noff compression for a major running-time speedup, is a good option for very\nlarge tables. However, for some compression schemes, it is more important to\ngenerate long runs rather than few runs. For this case, another novel\nheuristic, Vortex, is promising. We find that we can improve run-length\nencoding up to a factor of 3 whereas we can improve prefix coding by up to 80%:\nthese gains are on top of the gains due to lexicographically sorting the table.\nWe prove that the new row reordering is optimal (within 10%) at minimizing the\nruns of identical values within columns, in a few cases."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.4570v2", 
    "other_authors": "Farzad Parseh, Davood Karimzadgan Moghaddam, Mir Mohsen Pedram, Rohollah Esmaeli Manesh, Mohammad, Jamshidi", 
    "title": "Presentation an Approach for Optimization of Semantic Web Language Based   on the Document Structure", 
    "arxiv-id": "1207.4570v2", 
    "author": "Jamshidi", 
    "publish": "2012-07-19T07:14:15Z", 
    "summary": "Pattern tree are based on integrated rules which are equal to a combination\nof some points connected to each other in a hierarchical structure, called\nEnquiry Hierarchical (EH). The main operation in pattern enquiry seeking is to\nlocate the steps that match the given EH in the dataset. A point of algorithms\nhas offered for EH matching; but the majority of this algorithms seeks all of\nthe enquiry steps to access all EHs in the dataset. A few algorithms such as\nseek only steps that satisfy end points of EH. All of above algorithms are\ntrying to locate a way just for investigating direct testing of steps and to\nlocate the answer of enquiry, directly via these points. In this paper, we\ndescribe a novel algorithm to locate the answer of enquiry without access to\nreal point of the dataset blindly. In this algorithm, first, the enquiry will\nbe executed on enquiry schema and this leads to a schema. Using this plan, it\nwill be clear how to seek end steps and how to achieve enquiry dataset, before\nseeking of the dataset steps. Therefore, none of dataset steps will be seek\nblindly."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.4958v1", 
    "other_authors": "Ashish Gupta, Akshay Mittal, Arnab Bhattacharya", 
    "title": "Minimally Infrequent Itemset Mining using Pattern-Growth Paradigm and   Residual Trees", 
    "arxiv-id": "1207.4958v1", 
    "author": "Arnab Bhattacharya", 
    "publish": "2012-07-11T11:11:54Z", 
    "summary": "Itemset mining has been an active area of research due to its successful\napplication in various data mining scenarios including finding association\nrules. Though most of the past work has been on finding frequent itemsets,\ninfrequent itemset mining has demonstrated its utility in web mining,\nbioinformatics and other fields. In this paper, we propose a new algorithm\nbased on the pattern-growth paradigm to find minimally infrequent itemsets. A\nminimally infrequent itemset has no subset which is also infrequent. We also\nintroduce the novel concept of residual trees. We further utilize the residual\ntrees to mine multiple level minimum support itemsets where different\nthresholds are used for finding frequent itemsets for different lengths of the\nitemset. Finally, we analyze the behavior of our algorithm with respect to\ndifferent parameters and show through experiments that it outperforms the\ncompeting ones."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.5226v2", 
    "other_authors": "George Beskales, Ihab F. Ilyas, Lukasz Golab, Artur Galiullin", 
    "title": "On the Relative Trust between Inconsistent Data and Inaccurate   Constraints", 
    "arxiv-id": "1207.5226v2", 
    "author": "Artur Galiullin", 
    "publish": "2012-07-22T13:10:33Z", 
    "summary": "Functional dependencies (FDs) specify the intended data semantics while\nviolations of FDs indicate deviation from these semantics. In this paper, we\nstudy a data cleaning problem in which the FDs may not be completely correct,\ne.g., due to data evolution or incomplete knowledge of the data semantics. We\nargue that the notion of relative trust is a crucial aspect of this problem: if\nthe FDs are outdated, we should modify them to fit the data, but if we suspect\nthat there are problems with the data, we should modify the data to fit the\nFDs. In practice, it is usually unclear how much to trust the data versus the\nFDs. To address this problem, we propose an algorithm for generating\nnon-redundant solutions (i.e., simultaneous modifications of the data and the\nFDs) corresponding to various levels of relative trust. This can help users\ndetermine the best way to modify their data and/or FDs to achieve consistency."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.5466v1", 
    "other_authors": "Yongge Wang, Xintao Wu", 
    "title": "Approximate Inverse Frequent Itemset Mining: Privacy, Complexity, and   Approximation", 
    "arxiv-id": "1207.5466v1", 
    "author": "Xintao Wu", 
    "publish": "2012-07-23T17:57:47Z", 
    "summary": "In order to generate synthetic basket data sets for better benchmark testing,\nit is important to integrate characteristics from real-life databases into the\nsynthetic basket data sets. The characteristics that could be used for this\npurpose include the frequent itemsets and association rules. The problem of\ngenerating synthetic basket data sets from frequent itemsets is generally\nreferred to as inverse frequent itemset mining. In this paper, we show that the\nproblem of approximate inverse frequent itemset mining is {\\bf NP}-complete.\nThen we propose and analyze an approximate algorithm for approximate inverse\nfrequent itemset mining, and discuss privacy issues related to the synthetic\nbasket data set. In particular, we propose an approximate algorithm to\ndetermine the privacy leakage in a synthetic basket data set."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.6096v1", 
    "other_authors": "Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, Grigory Yaroslavtsev", 
    "title": "Accurate and Efficient Private Release of Datacubes and Contingency   Tables", 
    "arxiv-id": "1207.6096v1", 
    "author": "Grigory Yaroslavtsev", 
    "publish": "2012-07-25T19:31:18Z", 
    "summary": "A central problem in releasing aggregate information about sensitive data is\nto do so accurately while providing a privacy guarantee on the output. Recent\nwork focuses on the class of linear queries, which include basic counting\nqueries, data cubes, and contingency tables. The goal is to maximize the\nutility of their output, while giving a rigorous privacy guarantee. Most\nresults follow a common template: pick a \"strategy\" set of linear queries to\napply to the data, then use the noisy answers to these queries to reconstruct\nthe queries of interest. This entails either picking a strategy set that is\nhoped to be good for the queries, or performing a costly search over the space\nof all possible strategies.\n  In this paper, we propose a new approach that balances accuracy and\nefficiency: we show how to improve the accuracy of a given query set by\nanswering some strategy queries more accurately than others. This leads to an\nefficient optimal noise allocation for many popular strategies, including\nwavelets, hierarchies, Fourier coefficients and more. For the important case of\nmarginal queries we show that this strictly improves on previous methods, both\nanalytically and empirically. Our results also extend to ensuring that the\nreturned query answers are consistent with an (unknown) data set at minimal\nextra cost in terms of time and noise."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1207.6560v1", 
    "other_authors": "Nguyen Duc Thuan", 
    "title": "Covering Rough Sets From a Topological Point of View", 
    "arxiv-id": "1207.6560v1", 
    "author": "Nguyen Duc Thuan", 
    "publish": "2012-07-27T14:49:18Z", 
    "summary": "Covering-based rough set theory is an extension to classical rough set. The\nmain purpose of this paper is to study covering rough sets from a topological\npoint of view. The relationship among upper approximations based on topological\nspaces are explored."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.0191v1", 
    "other_authors": "A B M Moniruzzaman, Syed Akhter Hossain", 
    "title": "NoSQL Database: New Era of Databases for Big data Analytics -   Classification, Characteristics and Comparison", 
    "arxiv-id": "1307.0191v1", 
    "author": "Syed Akhter Hossain", 
    "publish": "2013-06-30T09:53:16Z", 
    "summary": "Digital world is growing very fast and become more complex in the volume\n(terabyte to petabyte), variety (structured and un-structured and hybrid),\nvelocity (high speed in growth) in nature. This refers to as Big Data that is a\nglobal phenomenon. This is typically considered to be a data collection that\nhas grown so large it can not be effectively managed or exploited using\nconventional data management tools: e.g., classic relational database\nmanagement systems (RDBMS) or conventional search engines. To handle this\nproblem, traditional RDBMS are complemented by specifically designed a rich set\nof alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This\npaper motivation is to provide - classification, characteristics and evaluation\nof NoSQL databases in Big Data Analytics. This report is intended to help\nusers, especially to the organizations to obtain an independent understanding\nof the strengths and weaknesses of various NoSQL database approaches to\nsupporting applications that process huge volumes of data."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.0193v1", 
    "other_authors": "Supriya Nirkhiwale, Alin Dobra, Chris Jermaine", 
    "title": "A Sampling Algebra for Aggregate Estimation", 
    "arxiv-id": "1307.0193v1", 
    "author": "Chris Jermaine", 
    "publish": "2013-06-30T10:07:22Z", 
    "summary": "As of 2005, sampling has been incorporated in all major database systems.\nWhile efficient sampling techniques are realizable, determining the accuracy of\nan estimate obtained from the sample is still an unresolved problem. In this\npaper, we present a theoretical framework that allows an elegant treatment of\nthe problem. We base our work on generalized uniform sampling (GUS), a class of\nsampling methods that subsumes a wide variety of sampling techniques. We\nintroduce a key notion of equivalence that allows GUS sampling operators to\ncommute with selection and join, and derivation of confidence intervals. We\nillustrate the theory through extensive examples and give indications on how to\nuse it to provide meaningful estimations in database systems."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.0844v1", 
    "other_authors": "Andrei Todor, Alin Dobra, Tamer Kahveci, Christopher Dudley", 
    "title": "Making massive probabilistic databases practical", 
    "arxiv-id": "1307.0844v1", 
    "author": "Christopher Dudley", 
    "publish": "2013-07-02T20:50:15Z", 
    "summary": "Existence of incomplete and imprecise data has moved the database paradigm\nfrom deterministic to proba- babilistic information. Probabilistic databases\ncontain tuples that may or may not exist with some probability. As a result,\nthe number of possible deterministic database instances that can be observed\nfrom a probabilistic database grows exponentially with the number of\nprobabilistic tuples. In this paper, we consider the problem of answering both\naggregate and non-aggregate queries on massive probabilistic databases. We\nadopt the tuple independence model, in which each tuple is assigned a\nprobability value. We develop a method that exploits Probability Generating\nFunctions (PGF) to answer such queries efficiently. Our method maintains a\npolynomial for each tuple. It incrementally builds a master polynomial that\nexpresses the distribution of the possible result values precisely. We also\ndevelop an approximation method that finds the distribution of the result value\nwith negligible errors. Our experiments suggest that our methods are orders of\nmagnitude faster than the most recent systems that answer such queries,\nincluding MayBMS and SPROUT. In our experiments, we were able to scale up to\nseveral terabytes of data on TPC- H queries, while existing methods could only\nrun for a few gigabytes of data on the same queries."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.1927v1", 
    "other_authors": "Murat Ali Bayir, Ismail Hakki Toroslu", 
    "title": "Link Based Session Reconstruction: Finding All Maximal Paths", 
    "arxiv-id": "1307.1927v1", 
    "author": "Ismail Hakki Toroslu", 
    "publish": "2013-07-07T22:20:27Z", 
    "summary": "This paper introduces a new method for the session construction problem,\nwhich is the first main step of the web usage mining process. Through\nexperiments, it is shown that when our new technique is used, it outperforms\nprevious approaches in web usage mining applications such as next-page\nprediction."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.2554v1", 
    "other_authors": "El Amin Aoulad Abdelouarit", 
    "title": "Les index pour les entrep\u00f4ts de donn\u00e9es : comparaison entre index   arbre-B et Bitmap", 
    "arxiv-id": "1307.2554v1", 
    "author": "El Amin Aoulad Abdelouarit", 
    "publish": "2013-07-09T19:31:50Z", 
    "summary": "With the development of decision systems and specially data warehouses, the\nvisibility of the data warehouse design before its creation has become\nessential, and that because of data warehouse importance as considered as the\nunique data source giving meaning to the decision. In a decision system the\nproper functioning of a data warehouse resides in the smooth running of the\nmiddleware tools ETC step one hand, and the restitution step through the data\nmining, reporting solutions, dashboards... etc other. The large volume of data\nthat passes through these stages require an optimal design for a highly\nefficient decision system, without disregarding the choice of technologies that\nare introduced for the data warehouse implementation such as: database\nmanagement system, the type of server operating systems, physical server\narchitecture (64-bit, for example) that can be a benefit performance of this\nsystem. The designer of the data warehouse should consider the effectiveness of\ndata query, this depends on the selection of relevant indexes and their\ncombination with the materialized views, note that the index selection is a\nNPcomplete problem, because the number of indexes is exponential in the total\nnumber of attributes in the database, So, it is necessary to provide, while the\ndata warehouse design, the suitable type of index for this data warehouse. This\npaper presents a comparative study between the index B-tree type and type\nBitmap, their advantages and disadvantages, with a real experiment showing that\nits index of type Bitmap more advantageous than the index B-tree type."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.2603v1", 
    "other_authors": "Olivier Cur\u00e9, Myriam Lamolle, Chan Le Duc", 
    "title": "Ontology Based Data Integration Over Document and Column Family Oriented   NOSQL", 
    "arxiv-id": "1307.2603v1", 
    "author": "Chan Le Duc", 
    "publish": "2013-07-09T21:08:27Z", 
    "summary": "The World Wide Web infrastructure together with its more than 2 billion users\nenables to store information at a rate that has never been achieved before.\nThis is mainly due to the will of storing almost all end-user interactions\nperformed on some web applications. In order to reply to scalability and\navailability constraints, many web companies involved in this process recently\nstarted to design their own data management systems. Many of them are referred\nto as NOSQL databases, standing for 'Not only SQL'. With their wide adoption\nemerges new needs and data integration is one of them. In this paper, we\nconsider that an ontology-based representation of the information stored in a\nset of NOSQL sources is highly needed. The main motivation of this approach is\nthe ability to reason on elements of the ontology and to retrieve information\nin an efficient and distributed manner. Our contributions are the following:\n(1) we analyze a set of schemaless NOSQL databases to generate local\nontologies, (2) we generate a global ontology based on the discovery of\ncorrespondences between the local ontologies and finally (3) we propose a query\ntranslation solution from SPARQL to query languages of the sources. We are\ncurrently implementing our data integration solution on two popular NOSQL\ndatabases: MongoDB as a document database and Cassandra as a column family\nstore."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.2991v2", 
    "other_authors": "Qiwei Lu, Wenchao Huang, Yan Xiong, Xudong Gong", 
    "title": "Integrity Verification for Outsourcing Uncertain Frequent Itemset Mining", 
    "arxiv-id": "1307.2991v2", 
    "author": "Xudong Gong", 
    "publish": "2013-07-11T07:03:06Z", 
    "summary": "In recent years, due to the wide applications of uncertain data (e.g., noisy\ndata), uncertain frequent itemsets (UFI) mining over uncertain databases has\nattracted much attention, which differs from the corresponding deterministic\nproblem from the generalized definition and resolutions. As the most costly\ntask in association rule mining process, it has been shown that outsourcing\nthis task to a service provider (e.g.,the third cloud party) brings several\nbenefits to the data owner such as cost relief and a less commitment to storage\nand computational resources. However, the correctness integrity of mining\nresults can be corrupted if the service provider is with random fault or not\nhonest (e.g., lazy, malicious, etc). Therefore, in this paper, we focus on the\nintegrity and verification issue in UFI mining problem during outsourcing\nprocess, i.e., how the data owner verifies the mining results. Specifically, we\nexplore and extend the existing work on deterministic FI outsourcing\nverification to uncertain scenario. For this purpose, We extend the existing\noutsourcing FI mining work to uncertain area w.r.t. the two popular UFI\ndefinition criteria and the approximate UFI mining methods. Specifically, We\nconstruct and improve the basic/enhanced verification scheme with such\ndifferent UFI definition respectively. After that, we further discuss the\nscenario of existing approximation UFP mining, where we can see that our\ntechnique can provide good probabilistic guarantees about the correctness of\nthe verification. Finally, we present the comparisons and analysis on the\nschemes proposed in this paper."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2338626.2338627", 
    "link": "http://arxiv.org/pdf/1307.3046v1", 
    "other_authors": "Leila Esheiba, Hoda M. O. Mokhtar, Mohamed El-Sharkawi", 
    "title": "Spatio-Temporal Queries for moving objects Data warehousing", 
    "arxiv-id": "1307.3046v1", 
    "author": "Mohamed El-Sharkawi", 
    "publish": "2013-07-11T10:35:01Z", 
    "summary": "In the last decade, Moving Object Databases (MODs) have attracted a lot of\nattention from researchers. Several research works were conducted to extend\ntraditional database techniques to accommodate the new requirements imposed by\nthe continuous change in location information of moving objects. Managing,\nquerying, storing, and mining moving objects were the key research directions.\nThis extensive interest in moving objects is a natural consequence of the\nrecent ubiquitous location-aware devices, such as PDAs, mobile phones, etc., as\nwell as the variety of information that can be extracted from such new\ndatabases. In this paper we propose a Spatio-Temporal data warehousing (STDW)\nfor efficiently querying location information of moving objects. The proposed\nschema introduces new measures like direction majority and other\ndirection-based measures that enhance the decision making based on location\ninformation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.3061v1", 
    "other_authors": "Dr. Osama E. Sheta, Ahmed Nour Eldeen", 
    "title": "The technology of using a data warehouse to support decision-making in   health care", 
    "arxiv-id": "1307.3061v1", 
    "author": "Ahmed Nour Eldeen", 
    "publish": "2013-07-11T11:30:32Z", 
    "summary": "This paper describes the technology of data warehouse in healthcare\ndecision-making and tools for support of these technologies, which is used to\ncancer diseases. The healthcare executive managers and doctors needs\ninformation about and insight into the existing health data, so as to make\ndecision more efficiently without interrupting the daily work of an On-Line\nTransaction Processing (OLTP) system. This is a complex problem during the\nhealthcare decision-making process. To solve this problem, the building a\nhealthcare data warehouse seems to be efficient. First in this paper we explain\nthe concepts of the data warehouse, On-Line Analysis Processing (OLAP).\nChanging the data in the data warehouse into a multidimensional data cube is\nthen shown. Finally, an application example is given to illustrate the use of\nthe healthcare data warehouse specific to cancer diseases developed in this\nstudy. The executive managers and doctors can view data from more than one\nperspective with reduced query time, thus making decisions faster and more\ncomprehensive."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.3419v2", 
    "other_authors": "Michael Schmidt, Georg Lausen", 
    "title": "Pleasantly Consuming Linked Data with RDF Data Descriptions", 
    "arxiv-id": "1307.3419v2", 
    "author": "Georg Lausen", 
    "publish": "2013-07-12T11:30:58Z", 
    "summary": "Although the intention of RDF is to provide an open, minimally constraining\nway for representing information, there exists an increasing number of\napplications for which guarantees on the structure and values of an RDF data\nset become desirable if not essential. What is missing in this respect are\nmechanisms to tie RDF data to quality guarantees akin to schemata of relational\ndatabases, or DTDs in XML, in particular when translating legacy data coming\nwith a rich set of integrity constraints - like keys or cardinality\nrestrictions - into RDF. Addressing this shortcoming, we present the RDF Data\nDescription language (RDD), which makes it possible to specify instance-level\ndata constraints over RDF. Making such constraints explicit does not only help\nin asserting and maintaining data quality, but also opens up new optimization\nopportunities for query engines and, most importantly, makes query formulation\na lot easier for users and system developers. We present design goals, syntax,\nand a formal, First-order logics based semantics of RDDs and discuss the impact\non consuming Linked Data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.3448v1", 
    "other_authors": "Dr. Osama E. Sheta, Ahmed Nour Eldeen", 
    "title": "Evaluating a healthcare data warehouse for cancer diseases", 
    "arxiv-id": "1307.3448v1", 
    "author": "Ahmed Nour Eldeen", 
    "publish": "2013-07-12T13:16:21Z", 
    "summary": "This paper presents the evaluation of the architecture of healthcare data\nwarehouse specific to cancer diseases. This data warehouse containing relevant\ncancer medical information and patient data. The data warehouse provides the\nsource for all current and historical health data to help executive manager and\ndoctors to improve the decision making process for cancer patients. The\nevaluation model based on Bill Inmon's definition of data warehouse is proposed\nto evaluate the Cancer data warehouse."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.4519v1", 
    "other_authors": "Dhammika Pieris", 
    "title": "Extending the ER Model to relational Model novel transformation   Algorithm: transforming relationship Types among Subtypes", 
    "arxiv-id": "1307.4519v1", 
    "author": "Dhammika Pieris", 
    "publish": "2013-07-17T07:09:30Z", 
    "summary": "A novel approach for creating ER conceptual models and an algorithm for\ntransforming them to the relational model has been developed by modifying and\nextending the existing methods. A part of the new algorithm has previously been\npresented. This paper presents the rest of the algorithm. One of the objectives\nof this paper is to use it as a supportive document for ongoing empirical\nevaluations of the new approach being conducted using the cognitive engagement\nmethod and with the participation of different segments of the field as\nrespondents."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.5437v1", 
    "other_authors": "Chanchal Yadav, Shuliang Wang, Manoj Kumar", 
    "title": "Algorithm and approaches to handle large Data- A Survey", 
    "arxiv-id": "1307.5437v1", 
    "author": "Manoj Kumar", 
    "publish": "2013-07-20T16:19:35Z", 
    "summary": "Data mining environment produces a large amount of data, that need to be\nanalyzed, patterns have to be extracted from that to gain knowledge. In this\nnew era with boom of data both structured and unstructured, in the field of\ngenomics, meteorology, biology, environmental research and many others, it has\nbecome difficult to process, manage and analyze patterns using traditional\ndatabases and architectures. So, a proper architecture should be understood to\ngain knowledge about the Big Data. This paper presents a review of various\nalgorithms from 1994-2013 necessary for handling such large data set. These\nalgorithms define various structures and methods implemented to handle Big\nData, also in the paper are listed various tool that were developed for\nanalyzing them."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.6348v2", 
    "other_authors": "Radu Ciucanu, Slawek Staworko", 
    "title": "Learning Schemas for Unordered XML", 
    "arxiv-id": "1307.6348v2", 
    "author": "Slawek Staworko", 
    "publish": "2013-07-24T09:33:41Z", 
    "summary": "We consider unordered XML, where the relative order among siblings is\nignored, and we investigate the problem of learning schemas from examples given\nby the user. We focus on the schema formalisms proposed in [10]: disjunctive\nmultiplicity schemas (DMS) and its restriction, disjunction-free multiplicity\nschemas (MS). A learning algorithm takes as input a set of XML documents which\nmust satisfy the schema (i.e., positive examples) and a set of XML documents\nwhich must not satisfy the schema (i.e., negative examples), and returns a\nschema consistent with the examples. We investigate a learning framework\ninspired by Gold [18], where a learning algorithm should be sound i.e., always\nreturn a schema consistent with the examples given by the user, and complete\ni.e., able to produce every schema with a sufficiently rich set of examples.\nAdditionally, the algorithm should be efficient i.e., polynomial in the size of\nthe input. We prove that the DMS are learnable from positive examples only, but\nthey are not learnable when we also allow negative examples. Moreover, we show\nthat the MS are learnable in the presence of positive examples only, and also\nin the presence of both positive and negative examples. Furthermore, for the\nlearnable cases, the proposed learning algorithms return minimal schemas\nconsistent with the examples."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.7328v1", 
    "other_authors": "Eiad Basher Alhyasat, Mahmoud Al-Dalahmeh", 
    "title": "Data Warehouse Success and Strategic Oriented Business Intelligence: A   Theoretical Framework", 
    "arxiv-id": "1307.7328v1", 
    "author": "Mahmoud Al-Dalahmeh", 
    "publish": "2013-07-28T03:50:16Z", 
    "summary": "With the proliferation of the data warehouses as supportive decision making\ntools, organizations are increasingly looking forward for a complete data\nwarehouse success model that would manage the enormous amounts of growing data.\nIt is therefore important to measure the success of these massive projects.\nWhile general IS success models have received great deals of attention, few\nresearch has been conducted to assess the success of data warehouses for\nstrategic business intelligence purposes. The framework developed in this study\nconsists of the following nine measures: Vendors and Consultants, Management\nActions, System Quality, Information Quality, Data Warehouse Usage, Perceived\nutility, Individual Decision Making Impact, Organizational Decision Making\nImpact, and Corporate Strategic Goals Attainment."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.7411v3", 
    "other_authors": "Wajdi Dhifli, Mohamed Moussaoui, Rabie Saidi, Engelbert Mephu Nguifo", 
    "title": "Towards an Efficient Discovery of the Topological Representative   Subgraphs", 
    "arxiv-id": "1307.7411v3", 
    "author": "Engelbert Mephu Nguifo", 
    "publish": "2013-07-28T22:17:40Z", 
    "summary": "With the emergence of graph databases, the task of frequent subgraph\ndiscovery has been extensively addressed. Although the proposed approaches in\nthe literature have made this task feasible, the number of discovered frequent\nsubgraphs is still very high to be efficiently used in any further exploration.\nFeature selection for graph data is a way to reduce the high number of frequent\nsubgraphs based on exact or approximate structural similarity. However, current\nstructural similarity strategies are not efficient enough in many real-world\napplications, besides, the combinatorial nature of graphs makes it\ncomputationally very costly. In order to select a smaller yet structurally\nirredundant set of subgraphs, we propose a novel approach that mines the top-k\ntopological representative subgraphs among the frequent ones. Our approach\nallows detecting hidden structural similarities that existing approaches are\nunable to detect such as the density or the diameter of the subgraph. In\naddition, it can be easily extended using any user defined structural or\ntopological attributes depending on the sought properties. Empirical studies on\nreal and synthetic graph datasets show that our approach is fast and scalable."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1307.8269v1", 
    "other_authors": "Serge Abiteboul, \u00c9milien Antoine, Gerome Miklau, Julia Stoyanovich, Vera Zaychik Moffitt", 
    "title": "Introducing Access Control in Webdamlog", 
    "arxiv-id": "1307.8269v1", 
    "author": "Vera Zaychik Moffitt", 
    "publish": "2013-07-31T10:21:33Z", 
    "summary": "We survey recent work on the specification of an access control mechanism in\na collaborative environment. The work is presented in the context of the\nWebdamLog language, an extension of datalog to a distributed context. We\ndiscuss a fine-grained access control mechanism for intentional data based on\nprovenance as well as a control mechanism for delegation, i.e., for deploying\nrules at remote peers."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.0454v1", 
    "other_authors": "Sattam Alsubaiee, Yasser Altowim, Hotham Altwaijry, Alexander Behm, Vinayak Borkar, Yingyi Bu, Michael Carey, Inci Cetindil, Madhusudan Cheelangi, Khurram Faraaz, Eugenia Gabrielova, Raman Grover, Zachary Heilbron, Young-Seok Kim, Chen Li, Guangqiang Li, Ji Mahn Ok, Nicola Onose, Pouria Pirzadeh, Vassilis Tsotras, Rares Vernica, Jian Wen, Till Westmann", 
    "title": "AsterixDB: A Scalable, Open Source BDMS", 
    "arxiv-id": "1407.0454v1", 
    "author": "Till Westmann", 
    "publish": "2014-07-02T04:29:54Z", 
    "summary": "AsterixDB is a new, full-function BDMS (Big Data Management System) with a\nfeature set that distinguishes it from other platforms in today's open source\nBig Data ecosystem. Its features make it well-suited to applications like web\ndata warehousing, social data storage and analysis, and other use cases related\nto Big Data. AsterixDB has a flexible NoSQL style data model; a query language\nthat supports a wide range of queries; a scalable runtime; partitioned,\nLSM-based data storage and indexing (including B+-tree, R-tree, and text\nindexes); support for external as well as natively stored data; a rich set of\nbuilt-in types; support for fuzzy, spatial, and temporal types and queries; a\nbuilt-in notion of data feeds for ingestion of data; and transaction support\nakin to that of a NoSQL store.\n  Development of AsterixDB began in 2009 and led to a mid-2013 initial open\nsource release. This paper is the first complete description of the resulting\nopen source AsterixDB system. Covered herein are the system's data model, its\nquery language, and its software architecture. Also included are a summary of\nthe current status of the project and a first glimpse into how AsterixDB\nperforms when compared to alternative technologies, including a parallel\nrelational DBMS, a popular NoSQL store, and a popular Hadoop-based SQL data\nanalytics platform, for things that both technologies can do. Also included is\na brief description of some initial trials that the system has undergone and\nthe lessons learned (and plans laid) based on those early \"customer\"\nengagements."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.0455v1", 
    "other_authors": "Yingyi Bu, Vinayak Borkar, Jianfeng Jia, Michael J. Carey, Tyson Condie", 
    "title": "Pregelix: Big(ger) Graph Analytics on A Dataflow Engine", 
    "arxiv-id": "1407.0455v1", 
    "author": "Tyson Condie", 
    "publish": "2014-07-02T05:04:28Z", 
    "summary": "There is a growing need for distributed graph processing systems that are\ncapable of gracefully scaling to very large graph datasets. Unfortunately, this\nchallenge has not been easily met due to the intense memory pressure imposed by\nprocess-centric, message passing designs that many graph processing systems\nfollow. Pregelix is a new open source distributed graph processing system that\nis based on an iterative dataflow design that is better tuned to handle both\nin-memory and out-of-core workloads. As such, Pregelix offers improved\nperformance characteristics and scaling properties over current open source\nsystems (e.g., we have seen up to 15x speedup compared to Apache Giraph and up\nto 35x speedup compared to distributed GraphLab), and makes more effective use\nof available machine resources to support Big(ger) Graph Analytics."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.1807v1", 
    "other_authors": "Raed Shatnawi, Qutaibah Althebyan, Baraq Ghalib, Mohammed Al-Maolegi", 
    "title": "Building A Smart Academic Advising System Using Association Rule Mining", 
    "arxiv-id": "1407.1807v1", 
    "author": "Mohammed Al-Maolegi", 
    "publish": "2014-07-04T19:28:35Z", 
    "summary": "In an academic environment, student advising is considered a paramount\nactivity for both advisors and student to improve the academic performance of\nstudents. In universities of large numbers of students, advising is a\ntime-consuming activity that may take a considerable effort of advisors and\nuniversity administration in guiding students to complete their registration\nsuccessfully and efficiently. Current systems are traditional and depend\ngreatly on the effort of the advisor to find the best selection of courses to\nimprove students performance. There is a need for a smart system that can\nadvise a large number of students every semester. In this paper, we propose a\nsmart system that uses association rule mining to help both students and\nadvisors in selecting and prioritizing courses. The system helps students to\nimprove their performance by suggesting courses that meet their current needs\nand at the same time improve their academic performance. The system uses\nassociation rule mining to find associations between courses that have been\nregistered by students in many previous semesters. The system successfully\ngenerates a list of association rules that guide a particular student to select\ncourses registered by similar students."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.2279v1", 
    "other_authors": "Gosta Grahne, Adrian Onet", 
    "title": "The data-exchange chase under the microscope", 
    "arxiv-id": "1407.2279v1", 
    "author": "Adrian Onet", 
    "publish": "2014-07-08T21:31:32Z", 
    "summary": "In this paper we take closer look at recent developments for the chase\nprocedure, and provide additional results. Our analysis allows us create a\ntaxonomy of the chase variations and the properties they satisfy. Two of the\nmost central problems regarding the chase is termination, and discovery of\nrestricted classes of sets of dependencies that guarantee termination of the\nchase. The search for the restricted classes has been motivated by a fairly\nrecent result that shows that it is undecidable to determine whether the chase\nwith a given dependency set will terminate on a given instance. There is a\nsmall dissonance here, since the quest has been for classes of sets of\ndependencies guaranteeing termination of the chase on all instances, even\nthough the latter problem was not known to be undecidable. We resolve the\ndissonance in this paper by showing that determining whether the chase with a\ngiven set of dependencies terminates on all instances is coRE-complete. For the\nhardness proof we use a reduction from word rewriting systems, thereby also\nshowing the close connection between the chase and word rewriting. The same\nreduction also gives us the aforementioned instance-dependent RE-completeness\nresult as a byproduct. For one of the restricted classes guaranteeing\ntermination on all instances, the stratified sets dependencies, we provide new\ncomplexity results for the problem of testing whether a given set of\ndependencies belongs to it. These results rectify some previous claims that\nhave occurred in the literature."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.2899v1", 
    "other_authors": "Gabriela Montoya, Hala Skaf-Molli, Pascal Molli, Maria-Esther Vidal", 
    "title": "Fedra: Query Processing for SPARQL Federations with Divergence", 
    "arxiv-id": "1407.2899v1", 
    "author": "Maria-Esther Vidal", 
    "publish": "2014-07-10T18:39:47Z", 
    "summary": "Data replication and deployment of local SPARQL endpoints improve scalability\nand availability of public SPARQL endpoints, making the consumption of Linked\nData a reality. This solution requires synchronization and specific query\nprocessing strategies to take advantage of replication. However, existing\nreplication aware techniques in federations of SPARQL endpoints do not consider\ndata dynamicity. We propose Fedra, an approach for querying federations of\nendpoints that benefits from replication. Participants in Fedra federations can\ncopy fragments of data from several datasets, and describe them using\nprovenance and views. These descriptions enable Fedra to reduce the number of\nselected endpoints while satisfying user divergence requirements. Experiments\non real-world datasets suggest savings of up to three orders of magnitude."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.3745v1", 
    "other_authors": "Sutanay Choudhury, Lawrence Holder, George Chin, Patrick Mackey, Khushbu Agarwal, John Feo", 
    "title": "Query Optimization for Dynamic Graphs", 
    "arxiv-id": "1407.3745v1", 
    "author": "John Feo", 
    "publish": "2014-07-14T17:53:30Z", 
    "summary": "Given a query graph that represents a pattern of interest, the emerging\npattern detection problem can be viewed as a continuous query problem on a\ndynamic graph. We present an incremental algorithm for continuous query\nprocessing on dynamic graphs. The algorithm is based on the concept of query\ndecomposition; we decompose a query graph into smaller subgraphs and assemble\nthe result of sub-queries to find complete matches with the specified query.\nThe novelty of our work lies in using the subgraph distributional statistics\ncollected from the dynamic graph to generate the decomposition. We introduce a\n\"Lazy Search\" algorithm where the search strategy is decided on a\nvertex-to-vertex basis depending on the likelihood of a match in the vertex\nneighborhood. We also propose a metric named \"Relative Selectivity\" that is\nused to select between different query decomposition strategies. Our\nexperiments performed on real online news, network traffic stream and a\nsynthetic social network benchmark demonstrate 10-100x speedups over competing\napproaches."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.3850v1", 
    "other_authors": "Stephan G\u00fcnnemann, Hardy Kremer, Matthias Hannen, Thomas Seidl", 
    "title": "KDD-SC: Subspace Clustering Extensions for Knowledge Discovery   Frameworks", 
    "arxiv-id": "1407.3850v1", 
    "author": "Thomas Seidl", 
    "publish": "2014-07-15T00:15:11Z", 
    "summary": "Analyzing high dimensional data is a challenging task. For these data it is\nknown that traditional clustering algorithms fail to detect meaningful\npatterns. As a solution, subspace clustering techniques have been introduced.\nThey analyze arbitrary subspace projections of the data to detect clustering\nstructures.\n  In this paper, we present our subspace clustering extension for KDD\nframeworks, termed KDD-SC. In contrast to existing subspace clustering\ntoolkits, our solution neither is a standalone product nor is it tightly\ncoupled to a specific KDD framework. Our extension is realized by a common\ncodebase and easy-to-use plugins for three of the most popular KDD frameworks,\nnamely KNIME, RapidMiner, and WEKA. KDD-SC extends these frameworks such that\nthey offer a wide range of different subspace clustering functionalities. It\nprovides a multitude of algorithms, data generators, evaluation measures, and\nvisualization techniques specifically designed for subspace clustering. These\nfunctionalities integrate seamlessly with the frameworks' existing features\nsuch that they can be flexibly combined. KDD-SC is publicly available on our\nwebsite."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.5661v1", 
    "other_authors": "Scott M. Sawyer, B. David O'Gwynn", 
    "title": "Evaluating Accumulo Performance for a Scalable Cyber Data Processing   Pipeline", 
    "arxiv-id": "1407.5661v1", 
    "author": "B. David O'Gwynn", 
    "publish": "2014-07-21T20:34:32Z", 
    "summary": "Streaming, big data applications face challenges in creating scalable data\nflow pipelines, in which multiple data streams must be collected, stored,\nqueried, and analyzed. These data sources are characterized by their volume (in\nterms of dataset size), velocity (in terms of data rates), and variety (in\nterms of fields and types). For many applications, distributed NoSQL databases\nare effective alternatives to traditional relational database management\nsystems. This paper considers a cyber situational awareness system that uses\nthe Apache Accumulo database to provide scalable data warehousing, real-time\ndata ingest, and responsive querying for human users and analytic algorithms.\nWe evaluate Accumulo's ingestion scalability as a function of number of client\nprocesses and servers. We also describe a flexible data model with effective\ntechniques for query planning and query batching to deliver responsive results.\nQuery performance is evaluated in terms of latency of the client receiving\ninitial result sets. Accumulo performance is measured on a database of up to 8\nnodes using real cyber data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2013.5305", 
    "link": "http://arxiv.org/pdf/1407.8515v1", 
    "other_authors": "Suryakant B. Patil, Vijay S. Suryawanshi, Dipali V. Suryawanshi, Preeti Patil", 
    "title": "Integrated ERP System for Improving the Functional efficiency of the   organization by Customized Architecture", 
    "arxiv-id": "1407.8515v1", 
    "author": "Preeti Patil", 
    "publish": "2014-07-29T05:33:53Z", 
    "summary": "An ERP is a kind of package which consist front end and backend as DBMS like\na collection of DBMSs. You can create DBMS to manage one aspect of your\nbusiness. For example, a publishing house has a database of books that keeps\ninformation about books such as Author Name, Title, Translator Name, etc. But\nthis database app only helps enter books data and search them. It doesn't help\nthem, for example, sell books. They get or develop another DBMS database that\nhas all the Books data plus prices, discount formulas, names of common clients,\netc. Now they connect the Books database to Sales database and maybe also the\ninventory database. Now its DBMS slowly turning into an ERP. They may add\npayroll database and connect it to this ERP. They may develop sales staff and\ncommissions database and connect it to this ERP and so on. In the traditional\nDatabase management system the different databases are used for the various\nCampuses of the JSPM Group of Education like Wagholi Campus, Tathwade Campus,\nNarhe Campus, Hadpsar Campuses, Bhavdhan Campus as well as Corporate office at\nKatraj of same organization so it is not possible to keep different databases\nfor the same so in this paper proposed the use of Integrated Database for the\nEntire organization using ERP system. The Proposed ERP system applied on the\nexisting Architecture of the JSPM Group; the marginal difference observed in\nthe Databases need to be accessed to generate the same number of Reports when\nuse the Traditional DBMS which end up with improvement in the Functional\nefficiency of Organizational Architecture."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0803.3224v1", 
    "other_authors": "Michael Hahsler", 
    "title": "A Model-Based Frequency Constraint for Mining Associations from   Transaction Data", 
    "arxiv-id": "0803.3224v1", 
    "author": "Michael Hahsler", 
    "publish": "2008-03-21T20:39:53Z", 
    "summary": "Mining frequent itemsets is a popular method for finding associated items in\ndatabases. For this method, support, the co-occurrence frequency of the items\nwhich form an association, is used as the primary indicator of the\nassociations's significance. A single user-specified support threshold is used\nto decided if associations should be further investigated. Support has some\nknown problems with rare items, favors shorter itemsets and sometimes produces\nmisleading associations.\n  In this paper we develop a novel model-based frequency constraint as an\nalternative to a single, user-specified minimum support. The constraint\nutilizes knowledge of the process generating transaction data by applying a\nsimple stochastic mixture model (the NB model) which allows for transaction\ndata's typically highly skewed item frequency distribution. A user-specified\nprecision threshold is used together with the model to find local frequency\nthresholds for groups of itemsets. Based on the constraint we develop the\nnotion of NB-frequent itemsets and adapt a mining algorithm to find all\nNB-frequent itemsets in a database. In experiments with publicly available\ntransaction databases we show that the new constraint provides improvements\nover a single minimum support threshold and that the precision threshold is\nmore robust and easier to set and interpret by the user."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.0075v1", 
    "other_authors": "Sherif Sakr", 
    "title": "An Experimental Investigation of XML Compression Tools", 
    "arxiv-id": "0806.0075v1", 
    "author": "Sherif Sakr", 
    "publish": "2008-05-31T14:49:00Z", 
    "summary": "This paper presents an extensive experimental study of the state-of-the-art\nof XML compression tools. The study reports the behavior of nine XML\ncompressors using a large corpus of XML documents which covers the different\nnatures and scales of XML documents. In addition to assessing and comparing the\nperformance characteristics of the evaluated XML compression tools, the study\ntries to assess the effectiveness and practicality of using these tools in the\nreal world. Finally, we provide some guidelines and recommen- dations which are\nuseful for helping developers and users for making an effective decision for\nselecting the most suitable XML compression tool for their needs."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.1071v1", 
    "other_authors": "Graham Cormode, Minos Garofalakis", 
    "title": "Histograms and Wavelets on Probabilistic Data", 
    "arxiv-id": "0806.1071v1", 
    "author": "Minos Garofalakis", 
    "publish": "2008-06-05T23:19:56Z", 
    "summary": "There is a growing realization that uncertain information is a first-class\ncitizen in modern database management. As such, we need techniques to correctly\nand efficiently process uncertain data in database systems. In particular, data\nreduction techniques that can produce concise, accurate synopses of large\nprobabilistic relations are crucial. Similar to their deterministic relation\ncounterparts, such compact probabilistic data synopses can form the foundation\nfor human understanding and interactive data exploration, probabilistic query\nplanning and optimization, and fast approximate query processing in\nprobabilistic database systems.\n  In this paper, we introduce definitions and algorithms for building\nhistogram- and wavelet-based synopses on probabilistic data. The core problem\nis to choose a set of histogram bucket boundaries or wavelet coefficients to\noptimize the accuracy of the approximate representation of a collection of\nprobabilistic tuples under a given error metric. For a variety of different\nerror metrics, we devise efficient algorithms that construct optimal or near\noptimal B-term histogram and wavelet synopses. This requires careful analysis\nof the structure of the probability distributions, and novel extensions of\nknown dynamic-programming-based techniques for the deterministic domain. Our\nexperiments show that this approach clearly outperforms simple ideas, such as\nbuilding summaries for samples drawn from the data distribution, while taking\nequal or less time."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.3115v1", 
    "other_authors": "Dan Hazel", 
    "title": "Using rational numbers to key nested sets", 
    "arxiv-id": "0806.3115v1", 
    "author": "Dan Hazel", 
    "publish": "2008-06-19T02:06:14Z", 
    "summary": "This report details the generation and use of tree node ordering keys in a\nsingle relational database table. The keys for each node are calculated from\nthe keys of its parent, in such a way that the sort order places every node in\nthe tree before all of its descendants and after all siblings having a lower\nindex. The calculation from parent keys to child keys is simple, and reversible\nin the sense that the keys of every ancestor of a node can be calculated from\nthat node's keys without having to consult the database.\n  Proofs of the above properties of the key encoding process and of its\ncorrespondence to a finite continued fraction form are provided."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.4703v2", 
    "other_authors": "Feng Li, Shuigeng Zhou", 
    "title": "Challenging More Updates: Towards Anonymous Re-publication of Fully   Dynamic Datasets", 
    "arxiv-id": "0806.4703v2", 
    "author": "Shuigeng Zhou", 
    "publish": "2008-06-28T16:24:03Z", 
    "summary": "Most existing anonymization work has been done on static datasets, which have\nno update and need only one-time publication. Recent studies consider\nanonymizing dynamic datasets with external updates: the datasets are updated\nwith record insertions and/or deletions. This paper addresses a new problem:\nanonymous re-publication of datasets with internal updates, where the attribute\nvalues of each record are dynamically updated. This is an important and\nchallenging problem for attribute values of records are updating frequently in\npractice and existing methods are unable to deal with such a situation.\n  We initiate a formal study of anonymous re-publication of dynamic datasets\nwith internal updates, and show the invalidation of existing methods. We\nintroduce theoretical definition and analysis of dynamic datasets, and present\na general privacy disclosure framework that is applicable to all anonymous\nre-publication problems. We propose a new counterfeited generalization\nprinciple alled m-Distinct to effectively anonymize datasets with both external\nupdates and internal updates. We also develop an algorithm to generalize\ndatasets to meet m-Distinct. The experiments conducted on real-world data\ndemonstrate the effectiveness of the proposed solution."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.4749v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Nested Ordered Sets and their Use for Data Modelling", 
    "arxiv-id": "0806.4749v1", 
    "author": "Alexandr Savinov", 
    "publish": "2008-06-29T11:38:06Z", 
    "summary": "In this paper we present a new approach to data modelling, called the\nconcept-oriented model (CoM), and describe its main features and\ncharacteristics including data semantics and operations. The distinguishing\nfeature of this model is that it is based on the formalism of nested ordered\nsets where any element participates in two structures simultaneously:\nhierarchical (nested) and multi-dimensional (ordered). An element of the model\nis postulated to consist of two parts, called identity and entity, and the\nwhole approach can be naturally broken into two branches: identity modelling\nand entity modelling. We also propose a new query language with the main\nconstruct, called concept, defined as a pair of two classes: identity class and\nentity class. We describe how its operations of projection, de-projection and\nproduct can be used to solve typical data modelling tasks."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0806.4920v1", 
    "other_authors": "Tuyet-Tram Dang-Ngoc, Georges Gardarin", 
    "title": "Conception et Evaluation de XQuery dans une architecture de m\u00e9diation   \"Tout-XML\"", 
    "arxiv-id": "0806.4920v1", 
    "author": "Georges Gardarin", 
    "publish": "2008-06-30T15:23:20Z", 
    "summary": "XML has emerged as the leading language for representing and exchanging data\nnot only on the Web, but also in general in the enterprise. XQuery is emerging\nas the standard query language for XML. Thus, tools are required to mediate\nbetween XML queries and heterogeneous data sources to integrate data in XML.\nThis paper presents the XMedia mediator, a unique tool for integrating and\nquerying disparate heterogeneous information as unified XML views. It describes\nthe mediator architecture and focuses on the unique distributed query\nprocessing technology implemented in this component. Query evaluation is based\non an original XML algebra simply extending classical operators to process\ntuples of tree elements. Further, we present a set of performance evaluation on\na relational benchmark, which leads to discuss possible performance\nenhancements."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0807.1734v5", 
    "other_authors": "Daniel Lemire", 
    "title": "Faster Sequential Search with a Two-Pass Dynamic-Time-Warping Lower   Bound", 
    "arxiv-id": "0807.1734v5", 
    "author": "Daniel Lemire", 
    "publish": "2008-07-10T20:26:31Z", 
    "summary": "The Dynamic Time Warping (DTW) is a popular similarity measure between time\nseries. The DTW fails to satisfy the triangle inequality and its computation\nrequires quadratic time. Hence, to find closest neighbors quickly, we use\nbounding techniques. We can avoid most DTW computations with an inexpensive\nlower bound (LB_Keogh). We compare LB_Keogh with a tighter lower bound\n(LB_Improved). We find that LB_Improved-based search is faster for sequential\nsearch. As an example, our approach is 3 times faster over random-walk and\nshape time series. We also review some of the mathematical properties of the\nDTW. We derive a tight triangle inequality for the DTW. We show that the DTW\nbecomes the l_1 distance when time series are separated by a constant."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0807.2972v1", 
    "other_authors": "Flavio Rizzolo", 
    "title": "DescribeX: A Framework for Exploring and Querying XML Web Collections", 
    "arxiv-id": "0807.2972v1", 
    "author": "Flavio Rizzolo", 
    "publish": "2008-07-18T14:12:02Z", 
    "summary": "This thesis introduces DescribeX, a powerful framework that is capable of\ndescribing arbitrarily complex XML summaries of web collections, providing\nsupport for more efficient evaluation of XPath workloads. DescribeX permits the\ndeclarative description of document structure using all axes and language\nconstructs in XPath, and generalizes many of the XML indexing and summarization\napproaches in the literature. DescribeX supports the construction of\nheterogeneous summaries where different document elements sharing a common\nstructure can be declaratively defined and refined by means of path regular\nexpressions on axes, or axis path regular expression (AxPREs). DescribeX can\nsignificantly help in the understanding of both the structure of complex,\nheterogeneous XML collections and the behaviour of XPath queries evaluated on\nthem.\n  Experimental results demonstrate the scalability of DescribeX summary\nrefinements and stabilizations (the key enablers for tailoring summaries) with\nmulti-gigabyte web collections. A comparative study suggests that using a\nDescribeX summary created from a given workload can produce query evaluation\ntimes orders of magnitude better than using existing summaries. DescribeX's\nlight-weight approach of combining summaries with a file-at-a-time XPath\nprocessor can be a very competitive alternative, in terms of performance, to\nconventional fully-fledged XML query engines that provide DB-like functionality\nsuch as security, transaction processing, and native storage."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10618-005-0026-2", 
    "link": "http://arxiv.org/pdf/0807.3795v1", 
    "other_authors": "Marshall Spight, Vadim Tropashko", 
    "title": "Relational Lattice Axioms", 
    "arxiv-id": "0807.3795v1", 
    "author": "Vadim Tropashko", 
    "publish": "2008-07-24T05:24:34Z", 
    "summary": "Relational lattice is a formal mathematical model for Relational algebra. It\nreduces the set of six classic relational algebra operators to two: natural\njoin and inner union. We continue to investigate Relational lattice properties\nwith emphasis onto axiomatic definition. New results include additional axioms,\nequational definition for set difference (more generally anti-join), and case\nstudy demonstrating application of the relational lattice theory for query\ntransformations."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0807.4580v1", 
    "other_authors": "Yi-Reun Kim, Kyu-Young Whang, Min-Soo Kim, Il-Yeol Song", 
    "title": "A Logical Model and Data Placement Strategies for MEMS Storage Devices", 
    "arxiv-id": "0807.4580v1", 
    "author": "Il-Yeol Song", 
    "publish": "2008-07-29T06:18:34Z", 
    "summary": "MEMS storage devices are new non-volatile secondary storages that have\noutstanding advantages over magnetic disks. MEMS storage devices, however, are\nmuch different from magnetic disks in the structure and access characteristics.\nThey have thousands of heads called probe tips and provide the following two\nmajor access facilities: (1) flexibility: freely selecting a set of probe tips\nfor accessing data, (2) parallelism: simultaneously reading and writing data\nwith the set of probe tips selected. Due to these characteristics, it is\nnontrivial to find data placements that fully utilize the capability of MEMS\nstorage devices. In this paper, we propose a simple logical model called the\nRegion-Sector (RS) model that abstracts major characteristics affecting data\nretrieval performance, such as flexibility and parallelism, from the physical\nMEMS storage model. We also suggest heuristic data placement strategies based\non the RS model and derive new data placements for relational data and\ntwo-dimensional spatial data by using those strategies. Experimental results\nshow that the proposed data placements improve the data retrieval performance\nby up to 4.0 times for relational data and by up to 4.8 times for\ntwo-dimensional spatial data of approximately 320 Mbytes compared with those of\nexisting data placements. Further, these improvements are expected to be more\nmarked as the database size grows."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0903.1059v1", 
    "other_authors": "Tiberiu Marius Karnyanszky", 
    "title": "Home Heating Systems Design using PHP and MySQL Databases", 
    "arxiv-id": "0903.1059v1", 
    "author": "Tiberiu Marius Karnyanszky", 
    "publish": "2009-03-05T18:33:52Z", 
    "summary": "This paper presents the use of a computer application based on a MySQL\ndatabase, managed by PHP programs, allowing the selection of a heating device\nusing coefficient-based calculus."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0903.1953v1", 
    "other_authors": "Balder ten Cate, Laura Chiticariu, Phokion Kolaitis, Wang-Chiew Tan", 
    "title": "Laconic schema mappings: computing core universal solutions by means of   SQL queries", 
    "arxiv-id": "0903.1953v1", 
    "author": "Wang-Chiew Tan", 
    "publish": "2009-03-11T11:34:20Z", 
    "summary": "We present a new method for computing core universal solutions in data\nexchange settings specified by source-to-target dependencies, by means of SQL\nqueries. Unlike previously known algorithms, which are recursive in nature, our\nmethod can be implemented directly on top of any DBMS. Our method is based on\nthe new notion of a laconic schema mapping. A laconic schema mapping is a\nschema mapping for which the canonical universal solution is the core universal\nsolution. We give a procedure by which every schema mapping specified by FO s-t\ntgds can be turned into a laconic schema mapping specified by FO s-t tgds that\nmay refer to a linear order on the domain of the source instance. We show that\nour results are optimal, in the sense that the linear order is necessary and\nthe method cannot be extended to schema mapping involving target constraints."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0903.3317v2", 
    "other_authors": "Shaoxu Song, Lei Chen", 
    "title": "Discovering Matching Dependencies", 
    "arxiv-id": "0903.3317v2", 
    "author": "Lei Chen", 
    "publish": "2009-03-19T12:33:47Z", 
    "summary": "The concept of matching dependencies (mds) is recently pro- posed for\nspecifying matching rules for object identification. Similar to the functional\ndependencies (with conditions), mds can also be applied to various data quality\napplications such as violation detection. In this paper, we study the problem\nof discovering matching dependencies from a given database instance. First, we\nformally define the measures, support and confidence, for evaluating utility of\nmds in the given database instance. Then, we study the discovery of mds with\ncertain utility requirements of support and confidence. Exact algorithms are\ndeveloped, together with pruning strategies to improve the time performance.\nSince the exact algorithm has to traverse all the data during the computation,\nwe propose an approximate solution which only use some of the data. A bound of\nrelative errors introduced by the approximation is also developed. Finally, our\nexperimental evaluation demonstrates the efficiency of the proposed methods."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0903.4305v2", 
    "other_authors": "Diana Sophia Codat", 
    "title": "Evaluation d'une requete en SQL", 
    "arxiv-id": "0903.4305v2", 
    "author": "Diana Sophia Codat", 
    "publish": "2009-03-25T11:39:31Z", 
    "summary": "The objective of this paper is to show how the interrogation processor\nresponds to SQL interrogation. The interrogation processor is split into two\nparts. The first, called the interrogation compiler translates an SQL query\ninto a plan of physical execution. The second, called evaluation query runs the\nexecution plan."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E92.D.2218", 
    "link": "http://arxiv.org/pdf/0903.5346v1", 
    "other_authors": "Lucja Kot, Christoph Koch", 
    "title": "Cooperative Update Exchange in the Youtopia System", 
    "arxiv-id": "0903.5346v1", 
    "author": "Christoph Koch", 
    "publish": "2009-03-31T00:10:02Z", 
    "summary": "Youtopia is a platform for collaborative management and integration of\nrelational data. At the heart of Youtopia is an update exchange abstraction:\nchanges to the data propagate through the system to satisfy user-specified\nmappings. We present a novel change propagation model that combines a\ndeterministic chase with human intervention. The process is fundamentally\ncooperative and gives users significant control over how mappings are repaired.\nAn additional advantage of our model is that mapping cycles can be permitted\nwithout compromising correctness.\n  We investigate potential harmful interference between updates in our model;\nwe introduce two appropriate notions of serializability that avoid such\ninterference if enforced. The first is very general and related to classical\nfinal-state serializability; the second is more restrictive but highly\npractical and related to conflict-serializability. We present an algorithm to\nenforce the latter notion. Our algorithm is an optimistic one, and as such may\nsometimes require updates to be aborted. We develop techniques for reducing the\nnumber of aborts and we test these experimentally."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1346v8", 
    "other_authors": "Daniel Lemire, Owen Kaser", 
    "title": "Reordering Columns for Smaller Indexes", 
    "arxiv-id": "0909.1346v8", 
    "author": "Owen Kaser", 
    "publish": "2009-09-07T21:34:52Z", 
    "summary": "Column-oriented indexes-such as projection or bitmap indexes-are compressed\nby run-length encoding to reduce storage and increase speed. Sorting the tables\nimproves compression. On realistic data sets, permuting the columns in the\nright order before sorting can reduce the number of runs by a factor of two or\nmore. Unfortunately, determining the best column order is NP-hard. For many\ncases, we prove that the number of runs in table columns is minimized if we\nsort columns by increasing cardinality. Experimentally, sorting based on\nHilbert space-filling curves is poor at minimizing the number of runs."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1760v1", 
    "other_authors": "Xiaodan Wang, Randal Burns, Tanu Malik", 
    "title": "LifeRaft: Data-Driven, Batch Processing for the Exploration of   Scientific Databases", 
    "arxiv-id": "0909.1760v1", 
    "author": "Tanu Malik", 
    "publish": "2009-09-09T18:05:37Z", 
    "summary": "Workloads that comb through vast amounts of data are gaining importance in\nthe sciences. These workloads consist of \"needle in a haystack\" queries that\nare long running and data intensive so that query throughput limits\nperformance. To maximize throughput for data-intensive queries, we put forth\nLifeRaft: a query processing system that batches queries with overlapping data\nrequirements. Rather than scheduling queries in arrival order, LifeRaft\nexecutes queries concurrently against an ordering of the data that maximizes\ndata sharing among queries. This decreases I/O and increases cache utility.\nHowever, such batch processing can increase query response time by starving\ninteractive workloads. LifeRaft addresses starvation using techniques inspired\nby head scheduling in disk drives. Depending upon the workload saturation and\nqueuing times, the system adaptively and incrementally trades-off processing\nqueries in arrival order and data-driven batch processing. Evaluating LifeRaft\nin the SkyQuery federation of astronomy databases reveals a two-fold\nimprovement in query throughput."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1766v1", 
    "other_authors": "Yi Zhang, Herodotos Herodotou, Jun Yang", 
    "title": "RIOT: I/O-Efficient Numerical Computing without SQL", 
    "arxiv-id": "0909.1766v1", 
    "author": "Jun Yang", 
    "publish": "2009-09-09T18:09:27Z", 
    "summary": "R is a numerical computing environment that is widely popular for statistical\ndata analysis. Like many such environments, R performs poorly for large\ndatasets whose sizes exceed that of physical memory. We present our vision of\nRIOT (R with I/O Transparency), a system that makes R programs I/O-efficient in\na way transparent to the users. We describe our experience with RIOT-DB, an\ninitial prototype that uses a relational database system as a backend. Despite\nthe overhead and inadequacy of generic database systems in handling array data\nand numerical computation, RIOT-DB significantly outperforms R in many\nlarge-data scenarios, thanks to a suite of high-level, inter-operation\noptimizations that integrate seamlessly into R. While many techniques in RIOT\nare inspired by databases (and, for RIOT-DB, realized by a database system),\nRIOT users are insulated from anything database related. Compared with previous\napproaches that require users to learn new languages and rewrite their programs\nto interface with a database, RIOT will, we believe, be easier to adopt by the\nmajority of the R users."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1767v1", 
    "other_authors": "Willis Lang, Jignesh Patel", 
    "title": "Towards Eco-friendly Database Management Systems", 
    "arxiv-id": "0909.1767v1", 
    "author": "Jignesh Patel", 
    "publish": "2009-09-09T18:09:31Z", 
    "summary": "Database management systems (DBMSs) have largely ignored the task of managing\nthe energy consumed during query processing. Both economical and environmental\nfactors now require that DBMSs pay close attention to energy consumption. In\nthis paper we approach this issue by considering energy consumption as a\nfirst-class performance goal for query processing in a DBMS. We present two\nconcrete techniques that can be used by a DBMS to directly manage the energy\nconsumption. Both techniques trade energy consumption for performance. The\nfirst technique, called PVC, leverages the ability of modern processors to\nexecute at lower processor voltage and frequency. The second technique, called\nQED, uses query aggregation to leverage common components of queries in a\nworkload. Using experiments run on a commercial DBMS and MySQL, we show that\nPVC can reduce the processor energy consumption by 49% of the original\nconsumption while increasing the response time by only 3%. On MySQL, PVC can\nreduce energy consumption by 20% with a response time penalty of only 6%. For\nsimple selection queries with no predicate overlap, we show that QED can be\nused to gracefully trade response time for energy, reducing energy consumption\nby 54% for a 43% increase in average response time. In this paper we also\nhighlight some research issues in the emerging area of energy-efficient data\nprocessing."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1771v1", 
    "other_authors": "Ken Smith, Michael Morse, Peter Mork, Maya Li, Arnon Rosenthal, David Allen, Len Seligman, Chris Wolf", 
    "title": "The Role of Schema Matching in Large Enterprises", 
    "arxiv-id": "0909.1771v1", 
    "author": "Chris Wolf", 
    "publish": "2009-09-09T18:09:48Z", 
    "summary": "To date, the principal use case for schema matching research has been as a\nprecursor for code generation, i.e., constructing mappings between schema\nelements with the end goal of data transfer. In this paper, we argue that\nschema matching plays valuable roles independent of mapping construction,\nespecially as schemata grow to industrial scales. Specifically, in large\nenterprises human decision makers and planners are often the immediate consumer\nof information derived from schema matchers, instead of schema mapping tools.\nWe list a set of real application areas illustrating this role for schema\nmatching, and then present our experiences tackling a customer problem in one\nof these areas. We describe the matcher used, where the tool was effective,\nwhere it fell short, and our lessons learned about how well current schema\nmatching technology is suited for use in large enterprises. Finally, we suggest\na new agenda for schema matching research based on these experiences."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1773v1", 
    "other_authors": "Andrey Balmin, Latha Colby, Emiran Curtmola, Quanzhong Li, Fatma Ozcan", 
    "title": "Search Driven Analysis of Heterogenous XML Data", 
    "arxiv-id": "0909.1773v1", 
    "author": "Fatma Ozcan", 
    "publish": "2009-09-09T18:09:55Z", 
    "summary": "Analytical processing on XML repositories is usually enabled by designing\ncomplex data transformations that shred the documents into a common data\nwarehousing schema. This can be very time-consuming and costly, especially if\nthe underlying XML data has a lot of variety in structure, and only a subset of\nattributes constitutes meaningful dimensions and facts. Today, there is no tool\nto explore an XML data set, discover interesting attributes, dimensions and\nfacts, and rapidly prototype an OLAP solution.\n  In this paper, we propose a system, called SEDA that enables users to start\nwith simple keyword-style querying, and interactively refine the query based on\nresult summaries. SEDA then maps query results onto a set of known, or newly\ncreated, facts and dimensions, and derives a star schema and its instantiation\nto be fed into an off-the-shelf OLAP tool, for further analysis."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1777v1", 
    "other_authors": "Yanlei Diao, Boduo Li, Anna Liu, Liping Peng, Charles Sutton, Thanh Tran, Michael Zink", 
    "title": "Capturing Data Uncertainty in High-Volume Stream Processing", 
    "arxiv-id": "0909.1777v1", 
    "author": "Michael Zink", 
    "publish": "2009-09-09T18:10:11Z", 
    "summary": "We present the design and development of a data stream system that captures\ndata uncertainty from data collection to query processing to final result\ngeneration. Our system focuses on data that is naturally modeled as continuous\nrandom variables. For such data, our system employs an approach grounded in\nprobability and statistical theory to capture data uncertainty and integrates\nthis approach into high-volume stream processing. The first component of our\nsystem captures uncertainty of raw data streams from sensing devices. Since\nsuch raw streams can be highly noisy and may not carry sufficient information\nfor query processing, our system employs probabilistic models of the data\ngeneration process and stream-speed inference to transform raw data into a\ndesired format with an uncertainty metric. The second component captures\nuncertainty as data propagates through query operators. To efficiently quantify\nresult uncertainty of a query operator, we explore a variety of techniques\nbased on probability and statistical theory to compute the result distribution\nat stream speed. We are currently working with a group of scientists to\nevaluate our system using traces collected from the domains of (and eventually\nin the real systems for) hazardous weather monitoring and object tracking and\nmonitoring."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1778v1", 
    "other_authors": "Nodira Khoussainova, Magda Balazinska, Wolfgang Gatterbauer, YongChul Kwon, Dan Suciu", 
    "title": "A Case for A Collaborative Query Management System", 
    "arxiv-id": "0909.1778v1", 
    "author": "Dan Suciu", 
    "publish": "2009-09-09T18:10:16Z", 
    "summary": "Over the past 40 years, database management systems (DBMSs) have evolved to\nprovide a sophisticated variety of data management capabilities. At the same\ntime, tools for managing queries over the data have remained relatively\nprimitive. One reason for this is that queries are typically issued through\napplications. They are thus debugged once and re-used repeatedly. This mode of\ninteraction, however, is changing. As scientists (and others) store and share\nincreasingly large volumes of data in data centers, they need the ability to\nanalyze the data by issuing exploratory queries. In this paper, we argue that,\nin these new settings, data management systems must provide powerful query\nmanagement capabilities, from query browsing to automatic query\nrecommendations. We first discuss the requirements for a collaborative query\nmanagement system. We outline an early system architecture and discuss the many\nresearch challenges associated with building such an engine."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1779v1", 
    "other_authors": "Philippe Cudre-Mauroux, Eugene Wu, Sam Madden", 
    "title": "The Case for RodentStore, an Adaptive, Declarative Storage System", 
    "arxiv-id": "0909.1779v1", 
    "author": "Sam Madden", 
    "publish": "2009-09-09T18:10:20Z", 
    "summary": "Recent excitement in the database community surrounding new\napplications?analytic, scientific, graph, geospatial, etc.?has led to an\nexplosion in research on database storage systems. New storage systems are\nvital to the database community, as they are at the heart of making database\nsystems perform well in new application domains. Unfortunately, each such\nsystem also represents a substantial engineering effort including a great deal\nof duplication of mechanisms for features such as transactions and caching. In\nthis paper, we make the case for RodentStore, an adaptive and declarative\nstorage system providing a high-level interface for describing the physical\nrepresentation of data. Specifically, RodentStore uses a declarative storage\nalgebra whereby administrators (or database design tools) specify how a logical\nschema should be grouped into collections of rows, columns, and/or arrays, and\nthe order in which those groups should be laid out on disk. We describe the key\noperators and types of our algebra, outline the general architecture of\nRodentStore, which interprets algebraic expressions to generate a physical\nrepresentation of the data, and describe the interface between RodentStore and\nother parts of a database system, such as the query optimizer and executor. We\nprovide a case study of the potential use of RodentStore in representing dense\ngeospatial data collected from a mobile sensor network, showing the ease with\nwhich different storage layouts can be expressed using some of our algebraic\nconstructs and the potential performance gains that a RodentStore-built storage\nsystem can offer."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1782v1", 
    "other_authors": "Shel Finkelstein, Dean Jacobs, Rainer Brendle", 
    "title": "Principles for Inconsistency", 
    "arxiv-id": "0909.1782v1", 
    "author": "Rainer Brendle", 
    "publish": "2009-09-09T18:10:33Z", 
    "summary": "Data consistency is very desirable because strong semantic properties make it\neasier to write correct programs that perform as users expect. However, there\nare good reasons why consistency may have to be weakened to achieve other\nbusiness goals. In this CIDR 2009 Perspectives paper, we present real-world\nreasons inconsistency may be necessary, offer principles for managing\ninconsistency coherently, and describe implementation approaches we are\ninvestigating for sustainably scalable systems that offer comprehensible user\nexperiences despite inconsistency."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.1785v1", 
    "other_authors": "Jayant Madhavan, Loredana Afanasiev, Lyublena Antova, Alon Halevy", 
    "title": "Harnessing the Deep Web: Present and Future", 
    "arxiv-id": "0909.1785v1", 
    "author": "Alon Halevy", 
    "publish": "2009-09-09T18:10:42Z", 
    "summary": "Over the past few years, we have built a system that has exposed large\nvolumes of Deep-Web content to Google.com users. The content that our system\nexposes contributes to more than 1000 search queries per-second and spans over\n50 languages and hundreds of domains. The Deep Web has long been acknowledged\nto be a major source of structured data on the web, and hence accessing\nDeep-Web content has long been a problem of interest in the data management\ncommunity. In this paper, we report on where we believe the Deep Web provides\nvalue and where it does not. We contrast two very different approaches to\nexposing Deep-Web content -- the surfacing approach that we used, and the\nvirtual integration approach that has often been pursued in the data management\nliterature. We emphasize where the values of each of the two approaches lie and\ncaution against potential pitfalls. We outline important areas of future\nresearch and, in particular, emphasize the value that can be derived from\nanalyzing large collections of potentially disparate structured data on the\nweb."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.ins.2011.02.002", 
    "link": "http://arxiv.org/pdf/0909.2062v1", 
    "other_authors": "Rafael Fern\u00e1ndez-Moctezuma, Kristin Tufte, Jin Li", 
    "title": "Inter-Operator Feedback in Data Stream Management Systems via   Punctuation", 
    "arxiv-id": "0909.2062v1", 
    "author": "Jin Li", 
    "publish": "2009-09-10T22:55:16Z", 
    "summary": "High-volume, high-speed data streams may overwhelm the capabilities of stream\nprocessing systems; techniques such as data prioritization, avoidance of\nunnecessary processing and on-demand result production may be necessary to\nreduce processing requirements. However, the dynamic nature of data streams, in\nterms of both rate and content, makes the application of such techniques\nchallenging. Such techniques have been addressed in the context of static and\ncentralized query optimization; however, they have not been fully addressed for\ndata stream management systems. In this work, we present a comprehensive\nframework that supports prioritization, avoidance of unnecessary work, and\non-demand result production over distributed, unreliable, bursty, disordered\ndata sources, typical of many data streams. We propose a form of inter-operator\nfeedback, which flows against the stream direction, to communicate the\ninformation needed to enable execution of these techniques. This feedback\nleverages punctuations to describe the subsets of interest. We identify\npotential sources of feedback information, characterize new types of\npunctuation to support feedback, and describe the roles of producers,\nexploiters, and relayers of feedback that query operators may implement. We\npresent initial experimental observations using the NiagaraST data-stream\nsystem."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/0909.2623v1", 
    "other_authors": "Reza Akbarinia, Esther Pacitti, Patrick Valduriez", 
    "title": "Reducing Network Traffic in Unstructured P2P Systems Using Top-k Queries", 
    "arxiv-id": "0909.2623v1", 
    "author": "Patrick Valduriez", 
    "publish": "2009-09-14T19:07:58Z", 
    "summary": "A major problem of unstructured P2P systems is their heavy network traffic.\nThis is caused mainly by high numbers of query answers, many of which are\nirrelevant for users. One solution to this problem is to use Top-k queries\nwhereby the user can specify a limited number (k) of the most relevant answers.\nIn this paper, we present FD, a (Fully Distributed) framework for executing\nTop-k queries in unstructured P2P systems, with the objective of reducing\nnetwork traffic. FD consists of a family of algorithms that are simple but\neffec-tive. FD is completely distributed, does not depend on the existence of\ncertain peers, and addresses the volatility of peers during query execution. We\nvali-dated FD through implementation over a 64-node cluster and simulation\nusing the BRITE topology generator and SimJava. Our performance evaluation\nshows that FD can achieve major performance gains in terms of communication and\nresponse time."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/0909.4196v1", 
    "other_authors": "Sabah S. Al-Fedaghi, Bernhard Thalheim", 
    "title": "Personal Information Databases", 
    "arxiv-id": "0909.4196v1", 
    "author": "Bernhard Thalheim", 
    "publish": "2009-09-23T12:55:26Z", 
    "summary": "One of the most important aspects of security organization is to establish a\nframework to identify security significant points where policies and procedures\nare declared. The (information) security infrastructure comprises entities,\nprocesses, and technology. All are participants in handling information, which\nis the item that needs to be protected. Privacy and security information\ntechnology is a critical and unmet need in the management of personal\ninformation. This paper proposes concepts and technologies for management of\npersonal information. Two different types of information can be distinguished:\npersonal information and nonpersonal information. Personal information can be\neither personal identifiable information (PII), or nonidentifiable information\n(NII). Security, policy, and technical requirements can be based on this\ndistinction. At the conceptual level, PII is defined and formalized by\npropositions over infons (discrete pieces of information) that specify\ntransformations in PII and NII. PII is categorized into simple infons that\nreflect the proprietor s aspects, relationships with objects, and relationships\nwith other proprietors. The proprietor is the identified person about whom the\ninformation is communicated. The paper proposes a database organization that\nfocuses on the PII spheres of proprietors. At the design level, the paper\ndescribes databases of personal identifiable information built exclusively for\nthis type of information, with their own conceptual scheme, system management,\nand physical structure."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/0909.4409v1", 
    "other_authors": "Mohamed A. El-Zawawy, Mohamed E. El-Sharkawi", 
    "title": "Clustering with Obstacles in Spatial Databases", 
    "arxiv-id": "0909.4409v1", 
    "author": "Mohamed E. El-Sharkawi", 
    "publish": "2009-09-24T10:52:58Z", 
    "summary": "Clustering large spatial databases is an important problem, which tries to\nfind the densely populated regions in a spatial area to be used in data mining,\nknowledge discovery, or efficient information retrieval. However most\nalgorithms have ignored the fact that physical obstacles such as rivers, lakes,\nand highways exist in the real world and could thus affect the result of the\nclustering. In this paper, we propose CPO, an efficient clustering technique to\nsolve the problem of clustering in the presence of obstacles. The proposed\nalgorithm divides the spatial area into rectangular cells. Each cell is\nassociated with statistical information used to label the cell as dense or\nnon-dense. It also labels each cell as obstructed (i.e. intersects any\nobstacle) or nonobstructed. For each obstructed cell, the algorithm finds a\nnumber of non-obstructed sub-cells. Then it finds the dense regions of\nnon-obstructed cells or sub-cells by a breadthfirst search as the required\nclusters with a center to each region."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/0909.4412v1", 
    "other_authors": "Mohamed E. El-Sharkawi, Mohamed A. El-Zawawy", 
    "title": "Algorithm for Spatial Clustering with Obstacles", 
    "arxiv-id": "0909.4412v1", 
    "author": "Mohamed A. El-Zawawy", 
    "publish": "2009-09-24T11:09:15Z", 
    "summary": "In this paper, we propose an efficient clustering technique to solve the\nproblem of clustering in the presence of obstacles. The proposed algorithm\ndivides the spatial area into rectangular cells. Each cell is associated with\nstatistical information that enables us to label the cell as dense or\nnon-dense. We also label each cell as obstructed (i.e. intersects any obstacle)\nor non-obstructed. Then the algorithm finds the regions (clusters) of\nconnected, dense, non-obstructed cells. Finally, the algorithm finds a center\nfor each such region and returns those centers as centers of the relatively\ndense regions (clusters) in the spatial area."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/0909.5530v1", 
    "other_authors": "Xiaokui Xiao, Guozhang Wang, Johannes Gehrke", 
    "title": "Differential Privacy via Wavelet Transforms", 
    "arxiv-id": "0909.5530v1", 
    "author": "Johannes Gehrke", 
    "publish": "2009-09-30T07:16:38Z", 
    "summary": "Privacy preserving data publishing has attracted considerable research\ninterest in recent years. Among the existing solutions, {\\em\n$\\epsilon$-differential privacy} provides one of the strongest privacy\nguarantees. Existing data publishing methods that achieve\n$\\epsilon$-differential privacy, however, offer little data utility. In\nparticular, if the output dataset is used to answer count queries, the noise in\nthe query answers can be proportional to the number of tuples in the data,\nwhich renders the results useless.\n  In this paper, we develop a data publishing technique that ensures\n$\\epsilon$-differential privacy while providing accurate answers for {\\em\nrange-count queries}, i.e., count queries where the predicate on each attribute\nis a range. The core of our solution is a framework that applies {\\em wavelet\ntransforms} on the data before adding noise to it. We present instantiations of\nthe proposed framework for both ordinal and nominal data, and we provide a\ntheoretical analysis on their privacy and utility guarantees. In an extensive\nexperimental study on both real and synthetic data, we show the effectiveness\nand efficiency of our solution."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0198v1", 
    "other_authors": "Houssem Jerbi, Genevi\u00e8ve Pujolle, Franck Ravat, Olivier Teste", 
    "title": "Personnalisation de Syst\u00e8mes OLAP Annot\u00e9s", 
    "arxiv-id": "1005.0198v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T06:36:10Z", 
    "summary": "This paper deals with personalization of annotated OLAP systems. Data\nconstellation is extended to support annotations and user preferences.\nAnnotations reflect the decision-maker experience whereas user preferences\nenable users to focus on the most interesting data. User preferences allow\nannotated contextual recommendations helping the decision-maker during his/her\nmultidimensional navigations."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0201v1", 
    "other_authors": "Franck Ravat, Olivier Teste, Gilles Zurfluh", 
    "title": "Personnalisation de bases de donn\u00e9es multidimensionnelles", 
    "arxiv-id": "1005.0201v1", 
    "author": "Gilles Zurfluh", 
    "publish": "2010-05-03T06:40:36Z", 
    "summary": "This paper deals with decision support systems resting on multidimensional\nmodelling of data. Moreover, we intend to offer a set of concepts and\nmechanisms for personalized multidimensional database specifications. This\npersonalization consists in associating weights to different components of a\nmultidimensional schema. Personalization specifications are specified through\nthe use of a language based on the principle of Event Condition Action. This\npersonalisation determines multidimensional data display as well as their\nanalyses (with the use of drilling or rotating operations)."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0212v1", 
    "other_authors": "Fr\u00e9d\u00e9ric Bret, Olivier Teste", 
    "title": "Construction graphique d'entrep\u00f4ts et de magasins de donn\u00e9es", 
    "arxiv-id": "1005.0212v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T07:43:22Z", 
    "summary": "Nowadays, decisional systems have became a significant research topic in\ndatabases. Data warehouses and data marts are the main elements of such\nsystems. This paper presents our decisional support system. We present\ngraphical interfaces which help the administrator to build data warehouses and\ndata marts. We present a data warehouse building interface based on an\nobject-oriented conceptual model. This model allows the warehouse data\nhistorisation at three levels: attribute, class and environment. Also, we\npresent a data mart building interface which allows warehouse data to be\nreorganised through a multidimensional object-oriented model."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0213v1", 
    "other_authors": "Franck Ravat, Olivier Teste, Gilles Zurfluh", 
    "title": "Alg\u00e8bre OLAP et langage graphique", 
    "arxiv-id": "1005.0213v1", 
    "author": "Gilles Zurfluh", 
    "publish": "2010-05-03T07:43:25Z", 
    "summary": "This article deals with OLAP systems based on multidimensional model. The\nconceptual model we provide, represents data through a constellation\n(multi-facts) composed of several multi-hierarchy dimensions. In this model,\ndata are displayed through multidimensional tables. We define a query algebra\nhandling these tables. This user oriented algebra is composed of a closure core\nof OLAP operators as soon as advanced operators dedicated to complex analysis.\nFinally, we specify a graphical OLAP language based on this algebra. This\nlanguage facilitates analyses of decision makers."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0214v1", 
    "other_authors": "Franck Ravat, Olivier Teste, Zurfluh Gilles", 
    "title": "Mod\u00e9lisation et extraction de donn\u00e9es pour un entrep\u00f4t objet", 
    "arxiv-id": "1005.0214v1", 
    "author": "Zurfluh Gilles", 
    "publish": "2010-05-03T07:43:45Z", 
    "summary": "This paper describes an object-oriented model for designing complex and\ntime-variant data warehouse data. The main contribution is the warehouse class\nconcept, which extends the class concept by temporal and archive filters as\nwell as a mapping function. Filters allow the keeping of relevant data changes\nwhereas the mapping function defines the warehouse class schema from a global\ndata source schema. The approach take into account static properties as well as\ndynamic properties. The behaviour extraction is based on the use-matrix\nconcept."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0217v1", 
    "other_authors": "Gilles Hubert, Olivier Teste", 
    "title": "Analyse multigraduelle OLAP", 
    "arxiv-id": "1005.0217v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T07:47:20Z", 
    "summary": "Decisional systems are based on multidimensional databases improving OLAP\nanalyses. The paper describes a new OLAP operator named \"BLEND\" to perform\nmultigradual analyses. The operation transforms multidimensional structures\nduring querying in order to analyse measures according to various granularity\nlevels, which are reorganised into a single parameter. We study valid\ncombinations of the operation in the context of strict hierarchies. First\nexperimentations implement the operation in an R-OLAP framework showing the\nslight cost of this operation."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0218v1", 
    "other_authors": "Faiza Ghozzi, Franck Ravat, Olivier Teste, Gilles Zurfluh", 
    "title": "Contraintes pour mod\u00e8le et langage multidimensionnels", 
    "arxiv-id": "1005.0218v1", 
    "author": "Gilles Zurfluh", 
    "publish": "2010-05-03T07:47:56Z", 
    "summary": "This paper defines a constraint-based model dedicated to multidimensional\ndatabases. The model we define represents data through a constellation of facts\n(subjects of analyse) associated to dimensions (axis of analyse), which are\npossibly shared. Each dimension is organised according to several hierarchies\n(views of analyse) integrating several levels of data granularity. In order to\ninsure data consistency, we introduce 5 semantic constraints (exclusion,\ninclusion, partition, simultaneity, totality) which can be intra-dimension or\ninter-dimensions; the intra-dimension constraints allow the expression of\nconstraints between hierarchies within a same dimension whereas the\ninter-dimensions constraints focus on hierarchies of distinct dimensions. We\nalso study repercussions of these constraints on multidimensional manipulations\nand we provide extensions of the multidimensional operators."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0219v1", 
    "other_authors": "Franck Ravat, Olivier Teste", 
    "title": "Mod\u00e9lisation et manipulation de donn\u00e9es historis\u00e9es et archiv\u00e9es   dans un entrep\u00f4t orient\u00e9 objet", 
    "arxiv-id": "1005.0219v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T07:48:53Z", 
    "summary": "This paper deals with temporal and archive object-oriented data warehouse\nmodelling and querying. In a first step, we define a data model describing\nwarehouses as central repositories of complex and temporal data extracted from\none information source. The model is based on the concepts of warehouse object\nand environment. A warehouse object is composed of one current state, several\npast states (modelling value changes) and several archive states (summarising\nsome value changes). An environment defines temporal parts in a warehouse\nschema according to a relevant granularity (attribute, class or graph). In a\nsecond step, we provide a query algebra dedicated to data warehouses. This\nalgebra, which is based on common object algebras, integrates temporal\noperators and operators for querying object states. An other important\ncontribution concerns dedicated operators allowing users to transform warehouse\nobjects in temporal series as well as operators facilitating analytical\ntreatments."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0220v1", 
    "other_authors": "Olivier Teste", 
    "title": "Elaboration d'entrep\u00f4ts de donn\u00e9es complexes", 
    "arxiv-id": "1005.0220v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T07:49:14Z", 
    "summary": "In this paper, we study the data warehouse modelling used in decision support\nsystems. We provide an object-oriented data warehouse model allowing data\nwarehouse description as a central repository of relevant, complex and temporal\ndata. Our model integrates three concepts such as warehouse object, environment\nand warehouse class. Each warehouse object is composed of one current state,\nseveral past states (modelling its detailed evolutions) and several archive\nstates (modelling its evolutions within a summarised form). The environment\nconcept defines temporal parts in the data warehouse schema with significant\ngranularities (attribute, class, graph). Finally, we provide five functions\naiming at defining the data warehouse structures and two functions allowing the\nwarehouse class inheritance hierarchy organisation."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0224v1", 
    "other_authors": "Olivier Teste", 
    "title": "Towards Conceptual Multidimensional Design in Decision Support Systems", 
    "arxiv-id": "1005.0224v1", 
    "author": "Olivier Teste", 
    "publish": "2010-05-03T07:55:33Z", 
    "summary": "Multidimensional databases support efficiently on-line analytical processing\n(OLAP). In this paper, we depict a model dedicated to multidimensional\ndatabases. The approach we present designs decisional information through a\nconstellation of facts and dimensions. Each dimension is possibly shared\nbetween several facts and it is organised according to multiple hierarchies. In\naddition, we define a comprehensive query algebra regrouping the more popular\nmultidimensional operations in current commercial systems and research\napproaches. We introduce new operators dedicated to a constellation. Finally,\nwe describe a prototype that allows managers to query constellations of facts,\ndimensions and multiple hierarchies."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0813v1", 
    "other_authors": "R. S. Weigel, D. M. Lindholm, A. Wilson, J. Faden", 
    "title": "TSDS: high-performance merge, subset, and filter software for time   series-like data", 
    "arxiv-id": "1005.0813v1", 
    "author": "J. Faden", 
    "publish": "2010-04-13T23:30:24Z", 
    "summary": "Time Series Data Server (TSDS) is a software package for implementing a\nserver that provides fast super-setting, sub-setting, filtering, and uniform\ngridding of time series-like data. TSDS was developed to respond quickly to\nrequests for long time spans of data. Data may be served from a fast database,\ntypically created by aggregating granules (e.g., data files) from a remote data\nsource and storing them in a local cache that is optimized for serving time\nseries. The system was designed specifically for time series data, and is\noptimized for requests where the longest dimension of the requested data\nstructure is time. Scalar, vector, and spectrogram time series types are\nsupported. The user can interact with the server by requesting a time series, a\ndate range, and an optional filter to apply to the data. Available filters\ninclude strides, block average/minimum/maximum, exclude, and inequality.\nConstraint expressions are supported, which allow such operations as a request\nfor data from one time series when a different time series satisfied a\nspecified relationship. TSDS builds upon DAP (Data Access Protocol), NcML\n(netCDF Mark-up language) and related software libraries. In this work, we\ndescribe the current design of this server, as well as planned features and\npotential implementation strategies."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.0972v1", 
    "other_authors": "S. F. Rodd, U. P. Kulkarni", 
    "title": "Adaptive Tuning Algorithm for Performance tuning of Database Management   System", 
    "arxiv-id": "1005.0972v1", 
    "author": "U. P. Kulkarni", 
    "publish": "2010-05-06T10:47:03Z", 
    "summary": "Performance tuning of Database Management Systems(DBMS) is both complex and\nchallenging as it involves identifying and altering several key performance\ntuning parameters. The quality of tuning and the extent of performance\nenhancement achieved greatly depends on the skill and experience of the\nDatabase Administrator (DBA). As neural networks have the ability to adapt to\ndynamically changing inputs and also their ability to learn makes them ideal\ncandidates for employing them for tuning purpose. In this paper, a novel tuning\nalgorithm based on neural network estimated tuning parameters is presented. The\nkey performance indicators are proactively monitored and fed as input to the\nNeural Network and the trained network estimates the suitable size of the\nbuffer cache, shared pool and redo log buffer size. The tuner alters these\ntuning parameters using the estimated values using a rate change computing\nalgorithm. The preliminary results show that the proposed method is effective\nin improving the query response time for a variety of workload types. ."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s10619-006-8313-5", 
    "link": "http://arxiv.org/pdf/1005.4714v2", 
    "other_authors": "Sushovan De, Subbarao Kambhampati", 
    "title": "Defining and Mining Functional Dependencies in Probabilistic Databases", 
    "arxiv-id": "1005.4714v2", 
    "author": "Subbarao Kambhampati", 
    "publish": "2010-05-26T00:06:09Z", 
    "summary": "Functional dependencies -- traditional, approximate and conditional are of\ncritical importance in relational databases, as they inform us about the\nrelationships between attributes. They are useful in schema normalization, data\nrectification and source selection. Most of these were however developed in the\ncontext of deterministic data. Although uncertain databases have started\nreceiving attention, these dependencies have not been defined for them, nor are\nfast algorithms available to evaluate their confidences. This paper defines the\nlogical extensions of various forms of functional dependencies for\nprobabilistic databases and explores the connections between them. We propose a\npruning-based exact algorithm to evaluate the confidence of functional\ndependencies, a Monte-Carlo based algorithm to evaluate the confidence of\napproximate functional dependencies and algorithms for their conditional\ncounterparts in probabilistic databases. Experiments are performed on both\nsynthetic and real data evaluating the performance of these algorithms in\nassessing the confidence of dependencies and mining them from data. We believe\nthat having these dependencies and algorithms available for probabilistic\ndatabases will drive adoption of probabilistic data storage in the industry."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2202", 
    "link": "http://arxiv.org/pdf/1005.5432v1", 
    "other_authors": "Spits Warnars H. L. H", 
    "title": "Attribute oriented induction with star schema", 
    "arxiv-id": "1005.5432v1", 
    "author": "Spits Warnars H. L. H", 
    "publish": "2010-05-29T07:27:24Z", 
    "summary": "This paper will propose a novel star schema attribute induction as a new\nattribute induction paradigm and as improving from current attribute oriented\ninduction. A novel star schema attribute induction will be examined with\ncurrent attribute oriented induction based on characteristic rule and using non\nrule based concept hierarchy by implementing both of approaches. In novel star\nschema attribute induction some improvements have been implemented like\nelimination threshold number as maximum tuples control for generalization\nresult, there is no ANY as the most general concept, replacement the role\nconcept hierarchy with concept tree, simplification for the generalization\nstrategy steps and elimination attribute oriented induction algorithm. Novel\nstar schema attribute induction is more powerful than the current attribute\noriented induction since can produce small number final generalization tuples\nand there is no ANY in the results."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2204", 
    "link": "http://arxiv.org/pdf/1005.5433v1", 
    "other_authors": "Nouha Arfaoui, Jalel Akaichi", 
    "title": "A Data Warehouse Assistant Design System Based on Clover Model", 
    "arxiv-id": "1005.5433v1", 
    "author": "Jalel Akaichi", 
    "publish": "2010-05-29T07:35:23Z", 
    "summary": "Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision\nmaking. However, their design remains a very delicate and difficult task either\nfor expert or users. The goal of this paper is to propose a new approach based\non the clover model, destined to assist users to design a DW. The proposed\napproach is based on two main steps. The first one aims to guide users in their\nchoice of DW schema model. The second one aims to finalize the chosen model by\noffering to the designer views related to former successful DW design\nexperiences."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2205", 
    "link": "http://arxiv.org/pdf/1005.5434v1", 
    "other_authors": "B. N. Keshavamurthy, Mitesh Sharma, Durga Toshniwal", 
    "title": "Efficient Support Coupled Frequent Pattern Mining Over Progressive   Databases", 
    "arxiv-id": "1005.5434v1", 
    "author": "Durga Toshniwal", 
    "publish": "2010-05-29T07:38:51Z", 
    "summary": "There have been many recent studies on sequential pattern mining. The\nsequential pattern mining on progressive databases is relatively very new, in\nwhich we progressively discover the sequential patterns in period of interest.\nPeriod of interest is a sliding window continuously advancing as the time goes\nby. As the focus of sliding window changes, the new items are added to the\ndataset of interest and obsolete items are removed from it and become up to\ndate. In general, the existing proposals do not fully explore the real world\nscenario, such as items associated with support in data stream applications\nsuch as market basket analysis. Thus mining important knowledge from supported\nfrequent items becomes a non trivial research issue. Our proposed novel\napproach efficiently mines frequent sequential pattern coupled with support\nusing progressive mining tree."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2010.2208", 
    "link": "http://arxiv.org/pdf/1005.5438v1", 
    "other_authors": "Raddad Al King, Abdelkader Hameurlain, Franck Morvan", 
    "title": "Query Routing and Processing in Peer-To-Peer Data Sharing Systems", 
    "arxiv-id": "1005.5438v1", 
    "author": "Franck Morvan", 
    "publish": "2010-05-29T08:16:38Z", 
    "summary": "Sharing musical files via the Internet was the essential motivation of early\nP2P systems. Despite of the great success of the P2P file sharing systems,\nthese systems support only \"simple\" queries. The focus in such systems is how\nto carry out an efficient query routing in order to find the nodes storing a\ndesired file. Recently, several research works have been made to extend P2P\nsystems to be able to share data having a fine granularity (i.e. atomic\nattribute) and to process queries written with a highly expressive language\n(i.e. SQL). These works have led to the emergence of P2P data sharing systems\nthat represent a new generation of P2P systems and, on the other hand, a next\nstage in a long period of the database research area. ? The characteristics of\nP2P systems (e.g. large-scale, node autonomy and instability) make impractical\nto have a global catalog that represents often an essential component in\ntraditional database systems. Usually, such a catalog stores information about\ndata, schemas and data sources. Query routing and processing are two problems\naffected by the absence of a global catalog. Locating relevant data sources and\ngenerating a close to optimal execution plan become more difficult. In this\npaper, we concentrate our study on proposed solutions for the both problems.\nFurthermore, selected case studies of main P2P data sharing systems are\nanalyzed and compared."
},{
    "category": "cs.DB", 
    "doi": "10.1109/SWOD.2007.353199", 
    "link": "http://arxiv.org/pdf/1005.5514v1", 
    "other_authors": "Yannis Delveroudis, Paraskevas V. Lekeas", 
    "title": "Managing Semantic Loss during Query Reformulation in Peer Data   Management Systems", 
    "arxiv-id": "1005.5514v1", 
    "author": "Paraskevas V. Lekeas", 
    "publish": "2010-05-30T11:13:08Z", 
    "summary": "In this paper we deal with the notion of semantic loss in Peer Data\nManagement Systems (PDMS) queries. We define such a notion and we give a\nmechanism that discovers semantic loss in a PDMS network. Next, we propose an\nalgorithm that addresses the problem of restoring such a loss. Further\nevaluation of our proposed algorithm is an ongoing work"
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1005.5732v2", 
    "other_authors": "Foto Afrati, Victor Kyritsis, Paraskevas V. Lekeas, Dora Souliou", 
    "title": "A New Framework for Join Product Skew", 
    "arxiv-id": "1005.5732v2", 
    "author": "Dora Souliou", 
    "publish": "2010-05-31T19:40:01Z", 
    "summary": "Different types of data skew can result in load imbalance in the context of\nparallel joins under the shared nothing architecture. We study one important\ntype of skew, join product skew (JPS). A static approach based on frequency\nclasses is proposed which takes for granted the data distribution of join\nattribute values. It comes from the observation that the join selectivity can\nbe expressed as a sum of products of frequencies of the join attribute values.\nAs a consequence, an appropriate assignment of join sub-tasks, that takes into\nconsideration the magnitude of the frequency products can alleviate the join\nproduct skew. Motivated by the aforementioned remark, we propose an algorithm,\ncalled Handling Join Product Skew (HJPS), to handle join product skew."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.0557v2", 
    "other_authors": "Jes\u00fas Camacho-Rodr\u00edguez, Asterios Katsifodimos, Ioana Manolescu, Alexandra Roatis", 
    "title": "LiquidXML: Adaptive XML Content Redistribution", 
    "arxiv-id": "1008.0557v2", 
    "author": "Alexandra Roatis", 
    "publish": "2010-08-03T14:23:41Z", 
    "summary": "We propose to demonstrate LiquidXML, a platform for managing large corpora of\nXML documents in large-scale P2P networks. All LiquidXML peers may publish XML\ndocuments to be shared with all the network peers. The challenge then is to\nefficiently (re-)distribute the published content in the network, possibly in\noverlapping, redundant fragments, to support efficient processing of queries at\neach peer. The novelty of LiquidXML relies in its adaptive method of choosing\nwhich data fragments are stored where, to improve performance. The \"liquid\"\naspect of XML management is twofold: XML data flows from many sources towards\nmany consumers, and its distribution in the network continuously adapts to\nimprove query performance."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.1337v1", 
    "other_authors": "Zeeshan Ahmed", 
    "title": "PDM based I-SOAS Data Warehouse Design", 
    "arxiv-id": "1008.1337v1", 
    "author": "Zeeshan Ahmed", 
    "publish": "2010-08-07T12:55:26Z", 
    "summary": "This research paper briefly describes the industrial contributions of Product\nData Management in any organization's technical and managerial data management.\nThen focusing on some current major PDM based problems i.e. Static and\nUnintelligent Search, Platform Independent System and Successful PDM System\nImplementation, briefly presents a semantic based solution i.e. I-SOAS. Majorly\nthis research paper is about to present and discuss the contributions of I-SOAS\nin any organization's technical and system data management."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.1339v1", 
    "other_authors": "Zeeshan Ahmed, Sudhir Ganti", 
    "title": "Removal of Communication Gap", 
    "arxiv-id": "1008.1339v1", 
    "author": "Sudhir Ganti", 
    "publish": "2010-08-07T13:02:01Z", 
    "summary": "This research is about an online forum designed and developed to improve the\ncommunication process between alumni, new, old and upcoming students. In this\nresearch paper we present targeted problems, designed architecture, used\ntechnologies in development and final end product in detail."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.2300v1", 
    "other_authors": "Thomas Bernecker, Hans-Peter Kriegel, Matthias Renz, Florian Verhein, Andreas Z\u00fcfle", 
    "title": "Probabilistic Frequent Pattern Growth for Itemset Mining in Uncertain   Databases (Technical Report)", 
    "arxiv-id": "1008.2300v1", 
    "author": "Andreas Z\u00fcfle", 
    "publish": "2010-08-13T12:06:29Z", 
    "summary": "Frequent itemset mining in uncertain transaction databases semantically and\ncomputationally differs from traditional techniques applied on standard\n(certain) transaction databases. Uncertain transaction databases consist of\nsets of existentially uncertain items. The uncertainty of items in transactions\nmakes traditional techniques inapplicable. In this paper, we tackle the problem\nof finding probabilistic frequent itemsets based on possible world semantics.\nIn this context, an itemset X is called frequent if the probability that X\noccurs in at least minSup transactions is above a given threshold. We make the\nfollowing contributions: We propose the first probabilistic FP-Growth algorithm\n(ProFP-Growth) and associated probabilistic FP-Tree (ProFP-Tree), which we use\nto mine all probabilistic frequent itemsets in uncertain transaction databases\nwithout candidate generation. In addition, we propose an efficient technique to\ncompute the support probability distribution of an itemset in linear time using\nthe concept of generating functions. An extensive experimental section\nevaluates the our proposed techniques and shows that our ProFP-Growth approach\nis significantly faster than the current state-of-the-art algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.3743v1", 
    "other_authors": "Leopoldo Bertossi, Solmaz Kolahi, Laks V. S. Lakshmanan", 
    "title": "Data Cleaning and Query Answering with Matching Dependencies and   Matching Functions", 
    "arxiv-id": "1008.3743v1", 
    "author": "Laks V. S. Lakshmanan", 
    "publish": "2010-08-23T03:15:10Z", 
    "summary": "Matching dependencies were recently introduced as declarative rules for data\ncleaning and entity resolution. Enforcing a matching dependency on a database\ninstance identifies the values of some attributes for two tuples, provided that\nthe values of some other attributes are sufficiently similar. Assuming the\nexistence of matching functions for making two attributes values equal, we\nformally introduce the process of cleaning an instance using matching\ndependencies, as a chase-like procedure. We show that matching functions\nnaturally introduce a lattice structure on attribute domains, and a partial\norder of semantic domination between instances. Using the latter, we define the\nsemantics of clean query answering in terms of certain/possible answers as the\ngreatest lower bound/least upper bound of all possible answers obtained from\nthe clean instances. We show that clean query answering is intractable in some\ncases. Then we study queries that behave monotonically wrt semantic domination\norder, and show that we can provide an under/over approximation for clean\nanswers to monotone queries. Moreover, non-monotone positive queries can be\nrelaxed into monotone queries."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.3751v1", 
    "other_authors": "Sudipto Das, Divyakant Agrawal, Amr El Abbadi", 
    "title": "ElasTraS: An Elastic Transactional Data Store in the Cloud", 
    "arxiv-id": "1008.3751v1", 
    "author": "Amr El Abbadi", 
    "publish": "2010-08-23T05:30:01Z", 
    "summary": "Over the last couple of years, \"Cloud Computing\" or \"Elastic Computing\" has\nemerged as a compelling and successful paradigm for internet scale computing.\nOne of the major contributing factors to this success is the elasticity of\nresources. In spite of the elasticity provided by the infrastructure and the\nscalable design of the applications, the elephant (or the underlying database),\nwhich drives most of these web-based applications, is not very elastic and\nscalable, and hence limits scalability. In this paper, we propose ElasTraS\nwhich addresses this issue of scalability and elasticity of the data store in a\ncloud computing environment to leverage from the elastic nature of the\nunderlying infrastructure, while providing scalable transactional data access.\nThis paper aims at providing the design of a system in progress, highlighting\nthe major design choices, analyzing the different guarantees provided by the\nsystem, and identifying several important challenges for the research community\nstriving for computing in the cloud."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.4627v1", 
    "other_authors": "Jaffer Gardezi, Leopoldo Bertossi, Iluju Kiringa", 
    "title": "Matching Dependencies with Arbitrary Attribute Values: Semantics, Query   Answering and Integrity Constraints", 
    "arxiv-id": "1008.4627v1", 
    "author": "Iluju Kiringa", 
    "publish": "2010-08-27T01:30:14Z", 
    "summary": "Matching dependencies (MDs) were introduced to specify the identification or\nmatching of certain attribute values in pairs of database tuples when some\nsimilarity conditions are satisfied. Their enforcement can be seen as a natural\ngeneralization of entity resolution. In what we call the \"pure case\" of MDs,\nany value from the underlying data domain can be used for the value in common\nthat does the matching. We investigate the semantics and properties of data\ncleaning through the enforcement of matching dependencies for the pure case. We\ncharacterize the intended clean instances and also the \"clean answers\" to\nqueries as those that are invariant under the cleaning process. The complexity\nof computing clean instances and clean answers to queries is investigated.\nTractable and intractable cases depending on the MDs and queries are\nidentified. Finally, we establish connections with database \"repairs\" under\nintegrity constraints."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.5073v1", 
    "other_authors": "Everardo Barcenas, Pierre Geneves, Nabil Layaida, Alan Schmitt", 
    "title": "On the Count of Trees", 
    "arxiv-id": "1008.5073v1", 
    "author": "Alan Schmitt", 
    "publish": "2010-08-30T12:58:17Z", 
    "summary": "Regular tree grammars and regular path expressions constitute core constructs\nwidely used in programming languages and type systems. Nevertheless, there has\nbeen little research so far on frameworks for reasoning about path expressions\nwhere node cardinality constraints occur along a path in a tree. We present a\nlogic capable of expressing deep counting along paths which may include\narbitrary recursive forward and backward navigation. The counting extensions\ncan be seen as a generalization of graded modalities that count immediate\nsuccessor nodes. While the combination of graded modalities, nominals, and\ninverse modalities yields undecidable logics over graphs, we show that these\nfeatures can be combined in a decidable tree logic whose main features can be\ndecided in exponential time. Our logic being closed under negation, it may be\nused to decide typical problems on XPath queries such as satisfiability, type\nchecking with relation to regular types, containment, or equivalence."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1008.5357v1", 
    "other_authors": "Denis Mindolin, Jan Chomicki", 
    "title": "Preference Elicitation in Prioritized Skyline Queries", 
    "arxiv-id": "1008.5357v1", 
    "author": "Jan Chomicki", 
    "publish": "2010-08-31T16:41:09Z", 
    "summary": "Preference queries incorporate the notion of binary preference relation into\nrelational database querying. Instead of returning all the answers, such\nqueries return only the best answers, according to a given preference relation.\nPreference queries are a fast growing area of database research. Skyline\nqueries constitute one of the most thoroughly studied classes of preference\nqueries. A well known limitation of skyline queries is that skyline preference\nrelations assign the same importance to all attributes. In this work, we study\np-skyline queries that generalize skyline queries by allowing varying attribute\nimportance in preference relations. We perform an in-depth study of the\nproperties of p-skyline preference relations. In particular,we study the\nproblems of containment and minimal extension. We apply the obtained results to\nthe central problem of the paper: eliciting relative importance of attributes.\nRelative importance is implicit in the constructed p-skyline preference\nrelation. The elicitation is based on user-selected sets of superior (positive)\nand inferior (negative) examples. We show that the computational complexity of\nelicitation depends on whether inferior examples are involved. If they are not,\nelicitation can be achieved in polynomial time. Otherwise, it is NP-complete.\nOur experiments show that the proposed elicitation algorithm has high accuracy\nand good scalability"
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.0335v2", 
    "other_authors": "Sudeepa Roy, Vittorio Perduca, Val Tannen", 
    "title": "Faster Query Answering in Probabilistic Databases using Read-Once   Functions", 
    "arxiv-id": "1012.0335v2", 
    "author": "Val Tannen", 
    "publish": "2010-12-01T22:05:14Z", 
    "summary": "A boolean expression is in read-once form if each of its variables appears\nexactly once. When the variables denote independent events in a probability\nspace, the probability of the event denoted by the whole expression in\nread-once form can be computed in polynomial time (whereas the general problem\nfor arbitrary expressions is #P-complete). Known approaches to checking\nread-once property seem to require putting these expressions in disjunctive\nnormal form. In this paper, we tell a better story for a large subclass of\nboolean event expressions: those that are generated by conjunctive queries\nwithout self-joins and on tuple-independent probabilistic databases. We first\nshow that given a tuple-independent representation and the provenance graph of\nan SPJ query plan without self-joins, we can, without using the DNF of a result\nevent expression, efficiently compute its co-occurrence graph. From this, the\nread-once form can already, if it exists, be computed efficiently using\nexisting techniques. Our second and key contribution is a complete, efficient,\nand simple to implement algorithm for computing the read-once forms (whenever\nthey exist) directly, using a new concept, that of co-table graph, which can be\nsignificantly smaller than the co-occurrence graph."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1565v2", 
    "other_authors": "wided oueslati, jalel akaichi", 
    "title": "A Survey on Data Warehouse Evolution", 
    "arxiv-id": "1012.1565v2", 
    "author": "jalel akaichi", 
    "publish": "2010-12-07T17:47:31Z", 
    "summary": "The data warehouse (DW) technology was developed to integrate heterogeneous\ninformation sources for analysis purposes. Information sources are more and\nmore autonomous and they often change their content due to perpetual\ntransactions (data changes) and may change their structure due to continual\nusers' requirements evolving (schema changes). Handling properly all type of\nchanges is a must. In fact, the DW which is considered as the core component of\nthe modern decision support systems has to be update according to different\ntype of evolution of information sources to reflect the real world subject to\nanalysis. The goal of this paper is to propose an overview and a comparative\nstudy between different works related to the DW evolution problem."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1621v1", 
    "other_authors": "Abdelaali Briache, Kamar Marrakchi, Amine Kerzazi, Ismael Navas-Delgado, Jose F Aldana Montes, Badr D. Rossi Hassani, Khalid Lairini", 
    "title": "YeastMed: an XML-Based System for Biological Data Integration of Yeast", 
    "arxiv-id": "1012.1621v1", 
    "author": "Khalid Lairini", 
    "publish": "2010-12-07T21:53:31Z", 
    "summary": "A key goal of bioinformatics is to create database systems and software\nplatforms capable of storing and analysing large sets of biological data.\nHundreds of biological databases are now available and provide access to huge\namount of biological data. SGD, Yeastract, CYGD-MIPS, BioGrid and PhosphoGrid\nare five of the most visited databases by the yeast community. These sources\nprovide complementary data on biological entities. Biologists are brought\nsystematically to query these data sources in order to analyse the results of\ntheir experiments. Because of the heterogeneity of these sources, querying them\nseparately and then manually combining the returned result is a complex and\nlaborious task. To provide transparent and simultaneous access to these\nsources, we have developed a mediator-based system called YeastMed. In this\npaper, we present YeastMed focusing on its architecture."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1632v1", 
    "other_authors": "Vladimir Mironov, Nirmala Seethappan, Ward Blonde, Erick Antezana, Bjorn Lindi, Martin Kuiper", 
    "title": "Benchmarking triple stores with biological data", 
    "arxiv-id": "1012.1632v1", 
    "author": "Martin Kuiper", 
    "publish": "2010-12-07T22:24:15Z", 
    "summary": "We have compared the performance of five non-commercial triple stores,\nVirtuoso-open source, Jena SDB, Jena TDB, SWIFT-OWLIM and 4Store. We examined\nthree performance aspects: the query execution time, scalability and run-to-run\nreproducibility. The queries we chose addressed different ontological or\nbiological topics, and we obtained evidence that individual store performance\nwas quite query specific. We identified three groups of queries displaying\nsimilar behavior across the different stores: 1) relatively short response\ntime, 2) moderate response time and 3) relatively long response time. OWLIM\nproved to be a winner in the first group, 4Store in the second and Virtuoso in\nthe third. Our benchmarking showed Virtuoso to be a very balanced performer -\nits response time was better than average for all the 24 queries; it showed a\nvery good scalability and a reasonable run-to-run reproducibility."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1645v1", 
    "other_authors": "Alexandru Todor, Adrian Paschke, Stephan Heineke", 
    "title": "ChemCloud: Chemical e-Science Information Cloud", 
    "arxiv-id": "1012.1645v1", 
    "author": "Stephan Heineke", 
    "publish": "2010-12-07T23:52:54Z", 
    "summary": "Our Chemical e-Science Information Cloud (ChemCloud) - a Semantic Web based\neScience infrastructure - integrates and automates a multitude of databases,\ntools and services in the domain of chemistry, pharmacy and bio-chemistry\navailable at the Fachinformationszentrum Chemie (FIZ Chemie), at the Freie\nUniversitaet Berlin (FUB), and on the public Web. Based on the approach of the\nW3C Linked Open Data initiative and the W3C Semantic Web technologies for\nontologies and rules it semantically links and integrates knowledge from our\nW3C HCLS knowledge base hosted at the FUB, our multi-domain knowledge base\nDBpedia (Deutschland) implemented at FUB, which is extracted from Wikipedia\n(De) providing a public semantic resource for chemistry, and our\nwell-established databases at FIZ Chemie such as ChemInform for organic\nreaction data, InfoTherm the leading source for thermophysical data, Chemisches\nZentralblatt, the complete chemistry knowledge from 1830 to 1969, and\nChemgaPedia the largest and most frequented e-Learning platform for Chemistry\nand related sciences in German language."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1660v1", 
    "other_authors": "Jerven Bolleman, Alain Gateau, Sebastien Gehant, Nicole Redaschi", 
    "title": "Provenance and evidence in UniProtKB", 
    "arxiv-id": "1012.1660v1", 
    "author": "Nicole Redaschi", 
    "publish": "2010-12-08T00:51:37Z", 
    "summary": "The primary mission of UniProt is to support biological research by\nmaintaining a stable, comprehensive, fully classified, richly and accurately\nannotated protein sequence knowledgebase, with extensive cross-references to\nexternal resources, that is freely available to the scientific community. To\nenable users of the knowledgebase to accurately assess the reliability of the\ninformation contained in this resource, the evidence for and provenance of the\ninformation must be recorded. This paper discusses the user requirements for\nthis kind of metadata and the manner in which UniProtKB records it."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-27392-6", 
    "link": "http://arxiv.org/pdf/1012.1898v1", 
    "other_authors": "Doug Howe, Christian Pich", 
    "title": "Ontology Usage at ZFIN", 
    "arxiv-id": "1012.1898v1", 
    "author": "Christian Pich", 
    "publish": "2010-12-09T00:03:31Z", 
    "summary": "The Zebrafish Model Organism Database (ZFIN) provides a Web resource of\nzebrafish genomic, genetic, developmental, and phenotypic data. Four different\nontologies are currently used to annotate data to the most specific term\navailable facilitating a better comparison between inter-species data. In\naddition, ontologies are used to help users find and cluster data more quickly\nwithout the need of knowing the exact technical name for a term."
},{
    "category": "cs.DB", 
    "doi": "10.1145/1989284.1989321", 
    "link": "http://arxiv.org/pdf/1012.2858v1", 
    "other_authors": "Tom Ameloot, Frank Neven, Jan Van den Bussche", 
    "title": "Relational transducers for declarative networking", 
    "arxiv-id": "1012.2858v1", 
    "author": "Jan Van den Bussche", 
    "publish": "2010-12-13T20:48:12Z", 
    "summary": "Motivated by a recent conjecture concerning the expressiveness of declarative\nnetworking, we propose a formal computation model for \"eventually consistent\"\ndistributed querying, based on relational transducers. A tight link has been\nconjectured between coordination-freeness of computations, and monotonicity of\nthe queries expressed by such computations. Indeed, we propose a formal\ndefinition of coordination-freeness and confirm that the class of monotone\nqueries is captured by coordination-free transducer networks.\nCoordination-freeness is a semantic property, but the syntactic class that we\ndefine of \"oblivious\" transducers also captures the same class of monotone\nqueries. Transducer networks that are not coordination-free are much more\npowerful."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.1369", 
    "link": "http://arxiv.org/pdf/1012.4074v1", 
    "other_authors": "Woong-Kee Loh, Yang-Sae Moon, Wookey Lee", 
    "title": "A fast divide-and-conquer algorithm for indexing human genome sequences", 
    "arxiv-id": "1012.4074v1", 
    "author": "Wookey Lee", 
    "publish": "2010-12-18T09:46:03Z", 
    "summary": "Since the release of human genome sequences, one of the most important\nresearch issues is about indexing the genome sequences, and the suffix tree is\nmost widely adopted for that purpose. The traditional suffix tree construction\nalgorithms have severe performance degradation due to the memory bottleneck\nproblem. The recent disk-based algorithms also have limited performance\nimprovement due to random disk accesses. Moreover, they do not fully utilize\nthe recent CPUs with multiple cores. In this paper, we propose a fast algorithm\nbased on 'divide-and-conquer' strategy for indexing the human genome sequences.\nOur algorithm almost eliminates random disk accesses by accessing the disk in\nthe unit of contiguous chunks. In addition, our algorithm fully utilizes the\nmulti-core CPUs by dividing the genome sequences into multiple partitions and\nthen assigning each partition to a different core for parallel processing.\nExperimental results show that our algorithm outperforms the previous fastest\nDIGEST algorithm by up to 3.5 times."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.1369", 
    "link": "http://arxiv.org/pdf/1012.4855v1", 
    "other_authors": "Salvatore Raunich, Erhard Rahm", 
    "title": "Target-driven merging of Taxonomies", 
    "arxiv-id": "1012.4855v1", 
    "author": "Erhard Rahm", 
    "publish": "2010-12-22T01:43:14Z", 
    "summary": "The proliferation of ontologies and taxonomies in many domains increasingly\ndemands the integration of multiple such ontologies. The goal of ontology\nintegration is to merge two or more given ontologies in order to provide a\nunified view on the input ontologies while maintaining all information coming\nfrom them. We propose a new taxonomy merging algorithm that, given as input two\ntaxonomies and an equivalence matching between them, can generate an integrated\ntaxonomy in a fully automatic manner. The approach is target-driven, i.e. we\nmerge a source taxonomy into the target taxonomy and preserve the structure of\nthe target ontology as much as possible. We also discuss how to extend the\nmerge algorithm providing auxiliary information, like additional relationships\nbetween source and target concepts, in order to semantically improve the final\nresult. The algorithm was implemented in a working prototype and evaluated\nusing synthetic and real-world scenarios."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.1369", 
    "link": "http://arxiv.org/pdf/1012.5696v1", 
    "other_authors": "Sebastian Maneth, Tom Sebastian", 
    "title": "Fast and Tiny Structural Self-Indexes for XML", 
    "arxiv-id": "1012.5696v1", 
    "author": "Tom Sebastian", 
    "publish": "2010-12-28T04:48:21Z", 
    "summary": "XML document markup is highly repetitive and therefore well compressible\nusing dictionary-based methods such as DAGs or grammars. In the context of\nselectivity estimation, grammar-compressed trees were used before as synopsis\nfor structural XPath queries. Here a fully-fledged index over such grammars is\npresented. The index allows to execute arbitrary tree algorithms with a\nslow-down that is comparable to the space improvement. More interestingly,\ncertain algorithms execute much faster over the index (because no decompression\noccurs). E.g., for structural XPath count queries, evaluating over the index is\nfaster than previous XPath implementations, often by two orders of magnitude.\nThe index also allows to serialize XML results (including texts) faster than\nprevious systems, by a factor of ca. 2-3. This is due to efficient copy\nhandling of grammar repetitions, and because materialization is totally\navoided. In order to compare with twig join implementations, we implemented a\nmaterializer which writes out pre-order numbers of result nodes, and show its\ncompetitiveness."
},{
    "category": "cs.DB", 
    "doi": "10.1587/transinf.E94.D.1369", 
    "link": "http://arxiv.org/pdf/1012.6009v1", 
    "other_authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain", 
    "title": "Cluster Evaluation of Density Based Subspace Clustering", 
    "arxiv-id": "1012.6009v1", 
    "author": "Jasni Mohamad Zain", 
    "publish": "2010-12-29T19:34:11Z", 
    "summary": "Clustering real world data often faced with curse of dimensionality, where\nreal world data often consist of many dimensions. Multidimensional data\nclustering evaluation can be done through a density-based approach. Density\napproaches based on the paradigm introduced by DBSCAN clustering. In this\napproach, density of each object neighbours with MinPoints will be calculated.\nCluster change will occur in accordance with changes in density of each object\nneighbours. The neighbours of each object typically determined using a distance\nfunction, for example the Euclidean distance. In this paper SUBCLU, FIRES and\nINSCY methods will be applied to clustering 6x1595 dimension synthetic\ndatasets. IO Entropy, F1 Measure, coverage, accurate and time consumption used\nas evaluation performance parameters. Evaluation results showed SUBCLU method\nrequires considerable time to process subspace clustering; however, its value\ncoverage is better. Meanwhile INSCY method is better for accuracy comparing\nwith two other methods, although consequence time calculation was longer."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3205", 
    "link": "http://arxiv.org/pdf/1106.0304v1", 
    "other_authors": "Jes\u00fas Pardillo, Jose-Norberto Maz\u00f3n", 
    "title": "Using Ontologies for the Design of Data Warehouses", 
    "arxiv-id": "1106.0304v1", 
    "author": "Jose-Norberto Maz\u00f3n", 
    "publish": "2011-06-01T20:01:00Z", 
    "summary": "Obtaining an implementation of a data warehouse is a complex task that forces\ndesigners to acquire wide knowledge of the domain, thus requiring a high level\nof expertise and becoming it a prone-to-fail task. Based on our experience, we\nhave detected a set of situations we have faced up with in real-world projects\nin which we believe that the use of ontologies will improve several aspects of\nthe design of data warehouses. The aim of this article is to describe several\nshortcomings of current data warehouse design approaches and discuss the\nbenefit of using ontologies to overcome them. This work is a starting point for\ndiscussing the convenience of using ontologies in data warehouse design."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3205", 
    "link": "http://arxiv.org/pdf/1106.1478v1", 
    "other_authors": "M. Andrea Rodr\u00edguez, Leopoldo Bertossi, Monica Caniupan", 
    "title": "Consistent Query Answering under Spatial Semantic Constraints", 
    "arxiv-id": "1106.1478v1", 
    "author": "Monica Caniupan", 
    "publish": "2011-06-08T01:21:04Z", 
    "summary": "Consistent query answering is an inconsistency tolerant approach to obtaining\nsemantically correct answers from a database that may be inconsistent with\nrespect to its integrity constraints. In this work we formalize the notion of\nconsistent query answer for spatial databases and spatial semantic integrity\nconstraints. In order to do this, we first characterize conflicting spatial\ndata, and next, we define admissible instances that restore consistency while\nstaying close to the original instance. In this way we obtain a repair\nsemantics, which is used as an instrumental concept to define and possibly\nderive consistent query answers. We then concentrate on a class of spatial\ndenial constraints and spatial queries for which there exists an efficient\nstrategy to compute consistent query answers. This study applies inconsistency\ntolerance in spatial databases, rising research issues that shift the goal from\nthe consistency of a spatial database to the consistency of query answering."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3205", 
    "link": "http://arxiv.org/pdf/1106.1811v2", 
    "other_authors": "Arnab Bhattacharya, B. Palvali Teja, Sourav Dutta", 
    "title": "Caching Stars in the Sky: A Semantic Caching Approach to Accelerate   Skyline Queries", 
    "arxiv-id": "1106.1811v2", 
    "author": "Sourav Dutta", 
    "publish": "2011-06-09T13:47:34Z", 
    "summary": "Multi-criteria decision making has been made possible with the advent of\nskyline queries. However, processing such queries for high dimensional datasets\nremains a time consuming task. Real-time applications are thus infeasible,\nespecially for non-indexed skyline techniques where the datasets arrive online.\nIn this paper, we propose a caching mechanism that uses the semantics of\nprevious skyline queries to improve the processing time of a new query. In\naddition to exact queries, utilizing such special semantics allow accelerating\nrelated queries. We achieve this by generating partial result sets guaranteed\nto be in the skyline sets. We also propose an index structure for efficient\norganization of the cached queries. Experiments on synthetic and real datasets\nshow the effectiveness and scalability of our proposed methods."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1106.3745v2", 
    "other_authors": "Marcelo Arenas, Ronald Fagin, Alan Nash", 
    "title": "Composition with Target Constraints", 
    "arxiv-id": "1106.3745v2", 
    "author": "Alan Nash", 
    "publish": "2011-06-19T13:37:19Z", 
    "summary": "It is known that the composition of schema mappings, each specified by\nsource-to-target tgds (st-tgds), can be specified by a second-order tgd (SO\ntgd). We consider the question of what happens when target constraints are\nallowed. Specifically, we consider the question of specifying the composition\nof standard schema mappings (those specified by st-tgds, target egds, and a\nweakly acyclic set of target tgds). We show that SO tgds, even with the\nassistance of arbitrary source constraints and target constraints, cannot\nspecify in general the composition of two standard schema mappings. Therefore,\nwe introduce source-to-target second-order dependencies (st-SO dependencies),\nwhich are similar to SO tgds, but allow equations in the conclusion. We show\nthat st-SO dependencies (along with target egds and target tgds) are sufficient\nto express the composition of every finite sequence of standard schema\nmappings, and further, every st-SO dependency specifies such a composition. In\naddition to this expressive power, we show that st-SO dependencies enjoy other\ndesirable properties. In particular, they have a polynomial-time chase that\ngenerates a universal solution. This universal solution can be used to find the\ncertain answers to unions of conjunctive queries in polynomial time. It is easy\nto show that the composition of an arbitrary number of standard schema mappings\nis equivalent to the composition of only two standard schema mappings. We show\nthat surprisingly, the analogous result holds also for schema mappings\nspecified by just st-tgds (no target constraints). This is proven by showing\nthat every SO tgd is equivalent to an unnested SO tgd (one where there is no\nnesting of function symbols). Similarly, we prove unnesting results for st-SO\ndependencies, with the same types of consequences."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1106.5979v1", 
    "other_authors": "Mohammed Eunus Ali, Egemen Tanin, Rui Zhang, Ramamohanarao Kotagiri", 
    "title": "Probabilistic Voronoi Diagrams for Probabilistic Moving Nearest Neighbor   Queries", 
    "arxiv-id": "1106.5979v1", 
    "author": "Ramamohanarao Kotagiri", 
    "publish": "2011-06-29T15:49:36Z", 
    "summary": "A large spectrum of applications such as location based services and\nenvironmental monitoring demand efficient query processing on uncertain\ndatabases. In this paper, we propose the probabilistic Voronoi diagram (PVD)\nfor processing moving nearest neighbor queries on uncertain data, namely the\nprobabilistic moving nearest neighbor (PMNN) queries. A PMNN query finds the\nmost probable nearest neighbor of a moving query point continuously. To process\nPMNN queries efficiently, we provide two techniques: a pre-computation approach\nand an incremental approach. In the pre-computation approach, we develop an\nalgorithm to efficiently evaluate PMNN queries based on the pre-computed PVD\nfor the entire data set. In the incremental approach, we propose an incremental\nprobabilistic safe region based technique that does not require to pre-compute\nthe whole PVD to answer the PMNN query. In this incremental approach, we\nexploit the knowledge for a known region to compute the lower bound of the\nprobability of an object being the nearest neighbor. Experimental results show\nthat our approaches significantly outperform a sampling based approach by\norders of magnitude in terms of I/O, query processing time, and communication\noverheads."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1107.1104v1", 
    "other_authors": "Samur Araujo, Jan Hidders, Daniel Schwabe, Arjen P. de Vries", 
    "title": "SERIMI - Resource Description Similarity, RDF Instance Matching and   Interlinking", 
    "arxiv-id": "1107.1104v1", 
    "author": "Arjen P. de Vries", 
    "publish": "2011-07-06T11:56:34Z", 
    "summary": "The interlinking of datasets published in the Linked Data Cloud is a\nchallenging problem and a key factor for the success of the Semantic Web.\nManual rule-based methods are the most effective solution for the problem, but\nthey require skilled human data publishers going through a laborious, error\nprone and time-consuming process for manually describing rules mapping\ninstances between two datasets. Thus, an automatic approach for solving this\nproblem is more than welcome. In this paper, we propose a novel interlinking\nmethod, SERIMI, for solving this problem automatically. SERIMI matches\ninstances between a source and a target datasets, without prior knowledge of\nthe data, domain or schema of these datasets. Experiments conducted with\nbenchmark collections demonstrate that our approach considerably outperforms\nstate-of-the-art automatic approaches for solving the interlinking problem on\nthe Linked Data Cloud."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1107.1779v1", 
    "other_authors": "Eya Ben Ahmed, Ahlem Nabli, Fa\u00efez Gargouri", 
    "title": "A Survey of User-Centric Data Warehouses: From Personalization to   Recommendation", 
    "arxiv-id": "1107.1779v1", 
    "author": "Fa\u00efez Gargouri", 
    "publish": "2011-07-09T12:26:14Z", 
    "summary": "Providing a customized support for the OLAP brings tremendous challenges to\nthe OLAP technology. Standing at the crossroads of the preferences and the data\nwarehouse, two emerging trends are pointed out; namely: (i) the personalization\nand (ii) the recommendation. Although the panoply of the proposed approaches,\nthe user-centric data warehouse community issues have not been addressed yet.\nIn this paper we draw an overview of several user centric data warehouse\nproposals. We also discuss the two promising concepts in this issue, namely,\nthe personalization and the recommendation of the data warehouses. We compare\nthe current approaches among each others with respect to some criteria."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1107.3606v3", 
    "other_authors": "Hideaki Kimura, Carleton Coffrin, Alexander Rasin, Stanley B. Zdonik", 
    "title": "Optimizing Index Deployment Order for Evolving OLAP (Extended Version)", 
    "arxiv-id": "1107.3606v3", 
    "author": "Stanley B. Zdonik", 
    "publish": "2011-07-19T01:52:52Z", 
    "summary": "Query workloads and database schemas in OLAP applications are becoming\nincreasingly complex. Moreover, the queries and the schemas have to continually\n\\textit{evolve} to address business requirements. During such repetitive\ntransitions, the \\textit{order} of index deployment has to be considered while\ndesigning the physical schemas such as indexes and MVs.\n  An effective index deployment ordering can produce (1) a prompt query runtime\nimprovement and (2) a reduced total deployment time. Both of these are\nessential qualities of design tools for quickly evolving databases, but\noptimizing the problem is challenging because of complex index interactions and\na factorial number of possible solutions.\n  We formulate the problem in a mathematical model and study several techniques\nfor solving the index ordering problem. We demonstrate that Constraint\nProgramming (CP) is a more flexible and efficient platform to solve the problem\nthan other methods such as mixed integer programming and A* search. In addition\nto exact search techniques, we also studied local search algorithms to find\nnear optimal solution very quickly.\n  Our empirical analysis on the TPC-H dataset shows that our pruning techniques\ncan reduce the size of the search space by tens of orders of magnitude. Using\nthe TPC-DS dataset, we verify that our local search algorithm is a highly\nscalable and stable method for quickly finding a near-optimal solution."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1107.4924v1", 
    "other_authors": "Anastasios Arvanitis, Antonios Deligiannakis", 
    "title": "Discovering Attractive Products based on Influence Sets", 
    "arxiv-id": "1107.4924v1", 
    "author": "Antonios Deligiannakis", 
    "publish": "2011-07-25T12:38:15Z", 
    "summary": "Skyline queries have been widely used as a practical tool for multi-criteria\ndecision analysis and for applications involving preference queries. For\nexample, in a typical online retail application, skyline queries can help\ncustomers select the most interesting, among a pool of available, products.\nRecently, reverse skyline queries have been proposed, highlighting the\nmanufacturer's perspective, i.e. how to determine the expected buyers of a\ngiven product. In this work we develop novel algorithms for two important\nclasses of queries involving customer preferences. We first propose a novel\nalgorithm, termed as RSA, for answering reverse skyline queries. We then\nintroduce a new type of queries, namely the k-Most Attractive Candidates k-MAC\nquery. In this type of queries, given a set of existing product specifications\nP, a set of customer preferences C and a set of new candidate products Q, the\nk-MAC query returns the set of k candidate products from Q that jointly\nmaximizes the total number of expected buyers, measured as the cardinality of\nthe union of individual reverse skyline sets (i.e., influence sets). Applying\nexisting approaches to solve this problem would require calculating the reverse\nskyline set for each candidate, which is prohibitively expensive for large data\nsets. We, thus, propose a batched algorithm for this problem and compare its\nperformance against a branch-and-bound variant that we devise. Both of these\nalgorithms use in their core variants of our RSA algorithm. Our experimental\nstudy using both synthetic and real data sets demonstrates that our proposed\nalgorithms outperform existing, or naive solutions to our studied classes of\nqueries."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1109.0086v3", 
    "other_authors": "Qiang Zeng, Hai Zhuge", 
    "title": "Comments on \"Stack-based Algorithms for Pattern Matching on DAGs\"", 
    "arxiv-id": "1109.0086v3", 
    "author": "Hai Zhuge", 
    "publish": "2011-09-01T04:14:34Z", 
    "summary": "The paper \"Stack-based Algorithms for Pattern Matching on DAGs\" generalizes\nthe classical holistic twig join algorithms and proposes PathStackD, TwigStackD\nand DagStackD to respectively evaluate path, twig and DAG pattern queries on\ndirected acyclic graphs. In this paper, we investigate the major results of\nthat paper, pointing out several discrepancies and proposing solutions to\nresolving them. We show that the original algorithms do not find particular\ntypes of query solutions that are common in practice. We also analyze the\neffect of an underlying assumption on the correctness of the algorithms and\ndiscuss the pre-filtering process that the original work proposes to prune\nredundant nodes. Our experimental study on both real and synthetic data\nsubstantiates our conclusions."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1109.0181v1", 
    "other_authors": "J\u00fcrgen Umbrich, Aidan Hogan, Axel Polleres", 
    "title": "Improving the recall of decentralised linked data querying through   implicit knowledge", 
    "arxiv-id": "1109.0181v1", 
    "author": "Axel Polleres", 
    "publish": "2011-09-01T13:14:13Z", 
    "summary": "Aside from crawling, indexing, and querying RDF data centrally, Linked Data\nprinciples allow for processing SPARQL queries on-the-fly by dereferencing\nURIs. Proposed link-traversal query approaches for Linked Data have the\nbenefits of up-to-date results and decentralised (i.e., client-side) execution,\nbut operate on incomplete knowledge available in dereferenced documents, thus\naffecting recall. In this paper, we investigate how implicit knowledge -\nspecifically that found through owl:sameAs and RDFS reasoning - can improve the\nrecall in this setting. We start with an empirical analysis of a large crawl\nfeaturing 4 m Linked Data sources and 1.1 g quadruples: we (1) measure expected\nrecall by only considering dereferenceable information, (2) measure the\nimprovement in recall given by considering rdfs:seeAlso links as previous\nproposals did. We further propose and measure the impact of additionally\nconsidering (3) owl:sameAs links, and (4) applying lightweight RDFS reasoning\n(specifically {\\rho}DF) for finding more results, relying on static schema\ninformation. We evaluate our methods for live queries over our crawl."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1109.0617v1", 
    "other_authors": "C. Komalavalli, Chetna Laroiya", 
    "title": "Metadata Challenge for Query Processing Over Heterogeneous Wireless   Sensor Network", 
    "arxiv-id": "1109.0617v1", 
    "author": "Chetna Laroiya", 
    "publish": "2011-09-03T12:04:00Z", 
    "summary": "Wireless sensor networks become integral part of our life. These networks can\nbe used for monitoring the data in various domain due to their flexibility and\nfunctionality. Query processing and optimization in the WSN is a very\nchallenging task because of their energy and memory constraint. In this paper,\nfirst our focus is to review the different approaches that have significant\nimpacts on the development of query processing techniques for WSN. Finally, we\naim to illustrate the existing approach in popular query processing engines\nwith future research challenges in query optimization."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1109.0736v1", 
    "other_authors": "Hideaki Kimura, Vivek Narasayya, Manoj Syamala", 
    "title": "Compression Aware Physical Database Design", 
    "arxiv-id": "1109.0736v1", 
    "author": "Manoj Syamala", 
    "publish": "2011-09-04T18:11:43Z", 
    "summary": "Modern RDBMSs support the ability to compress data using methods such as null\nsuppression and dictionary encoding. Data compression offers the promise of\nsignificantly reducing storage requirements and improving I/O performance for\ndecision support queries. However, compression can also slow down update and\nquery performance due to the CPU costs of compression and decompression. In\nthis paper, we study how data compression affects choice of appropriate\nphysical database design, such as indexes, for a given workload. We observe\nthat approaches that decouple the decision of whether or not to choose an index\nfrom whether or not to compress the index can result in poor solutions. Thus,\nwe focus on the novel problem of integrating compression into physical database\ndesign in a scalable manner. We have implemented our techniques by modifying\nMicrosoft SQL Server and the Database Engine Tuning Advisor (DTA) physical\ndesign tool. Our techniques are general and are potentially applicable to DBMSs\nthat support other compression methods. Our experimental results on real world\nas well as TPC-H benchmark workloads demonstrate the effectiveness of our\ntechniques."
},{
    "category": "cs.DB", 
    "doi": "10.2168/LMCS-7(3:13)2011", 
    "link": "http://arxiv.org/pdf/1109.1087v1", 
    "other_authors": "A. Martin, M. Manjula, Dr. V. Prasanna Venkatesan", 
    "title": "A Business Intelligence Model to Predict Bankruptcy using Financial   Domain Ontology with Association Rule Mining Algorithm", 
    "arxiv-id": "1109.1087v1", 
    "author": "Dr. V. Prasanna Venkatesan", 
    "publish": "2011-09-06T07:02:35Z", 
    "summary": "Today in every organization financial analysis provides the basis for\nunderstanding and evaluating the results of business operations and delivering\nhow well a business is doing. This means that the organizations can control the\noperational activities primarily related to corporate finance. One way that\ndoing this is by analysis of bankruptcy prediction. This paper develops an\nontological model from financial information of an organization by analyzing\nthe Semantics of the financial statement of a business. One of the best\nbankruptcy prediction models is Altman Z-score model. Altman Z-score method\nuses financial rations to predict bankruptcy. From the financial ontological\nmodel the relation between financial data is discovered by using data mining\nalgorithm. By combining financial domain ontological model with association\nrule mining algorithm and Zscore model a new business intelligence model is\ndeveloped to predict the bankruptcy."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3311", 
    "link": "http://arxiv.org/pdf/1109.1144v1", 
    "other_authors": "M. Venkatesan, Arunkumar Thangavelu, P. Prabhavathy", 
    "title": "Event Centric Modeling Approach in Colocation Pattern Snalysis from   Spatial Data", 
    "arxiv-id": "1109.1144v1", 
    "author": "P. Prabhavathy", 
    "publish": "2011-09-06T11:12:42Z", 
    "summary": "Spatial co-location patterns are the subsets of Boolean spatial features\nwhose instances are often located in close geographic proximity. Co-location\nrules can be identified by spatial statistics or data mining approaches. In\ndata mining method, Association rule-based approaches can be used which are\nfurther divided into transaction-based approaches and distance-based\napproaches. Transaction-based approaches focus on defining transactions over\nspace so that an Apriori algorithm can be used. The natural notion of\ntransactions is absent in spatial data sets which are embedded in continuous\ngeographic space. A new distance -based approach is developed to mine\nco-location patterns from spatial data by using the concept of proximity\nneighborhood. A new interest measure, a participation index, is used for\nspatial co-location patterns as it possesses an anti-monotone property. An\nalgorithm to discover co-location patterns are designed which generates\ncandidate locations and their table instances. Finally the co-location rules\nare generated to identify the patterns."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3311", 
    "link": "http://arxiv.org/pdf/1109.1202v1", 
    "other_authors": "Abhijit Raorane, R. V. Kulkarni", 
    "title": "Data Mining Techniques: A Source for Consumer Behavior Analysis", 
    "arxiv-id": "1109.1202v1", 
    "author": "R. V. Kulkarni", 
    "publish": "2011-09-06T14:32:05Z", 
    "summary": "Various studies on consumer purchasing behaviors have been presented and used\nin real problems. Data mining techniques are expected to be a more effective\ntool for analyzing consumer behaviors. However, the data mining method has\ndisadvantages as well as advantages. Therefore, it is important to select\nappropriate techniques to mine databases. The objective of this paper is to\nknow consumer behavior, his psychological condition at the time of purchase and\nhow suitable data mining method apply to improve conventional method. Moreover,\nin an experiment, association rule is employed to mine rules for trusted\ncustomers using sales data in a super market industry"
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3311", 
    "link": "http://arxiv.org/pdf/1109.1302v1", 
    "other_authors": "Hakik Paci, Elinda Kajo, Igli Tafa, Aleksander Xhuvani", 
    "title": "Adding a new site in an existing Oracle Multimaster replication without   quiescing the replication", 
    "arxiv-id": "1109.1302v1", 
    "author": "Aleksander Xhuvani", 
    "publish": "2011-09-06T21:12:33Z", 
    "summary": "This paper presents a new solution, which adds a new database server on an\nexisting Oracle Multimaster Data replication system with Online Instantiation\nmethod. During this time the system is down, because we cannot execute DML\nstatements on replication objects but we can only make queries. The time for\nadding the new database server depends on the number of objects, on the\nreplication group and on the network conditions. We propose to add a new layer\nbetween replication objects and the database sessions, which contain DML\nstatements. The layer eliminates the system down time exploiting our developed\npackages. The packages will be active only during the addition of a new site\nprocess and will modify all DML statements and queries based on replication\nobjects."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3303", 
    "link": "http://arxiv.org/pdf/1109.1359v1", 
    "other_authors": "Agus Pratondo", 
    "title": "Representation for alphanumeric data type based on space and speed case   study: Student ID of X university", 
    "arxiv-id": "1109.1359v1", 
    "author": "Agus Pratondo", 
    "publish": "2011-09-07T05:57:20Z", 
    "summary": "ID is derived from the word identity, derived from the first two characters\nin the word. ID is used to distinguish between an entity to another entity.\nStudent ID (SID) is the key differentiator between a student with other\nstudents. On the concept of database, the differentiator is unique. SID can be\nnumbers, letters, or a combination of both (alphanumeric). Viewed from the\ndaily context, it is not important to determine which a SID belongs to the type\nof data. However, when reviewed on database design, determining the type of\ndata, including SID in this case, is important. Problems arise because there is\na contradiction between the data type viewed from the data characteristic and\npractical needs. Type of data for SID is a string, if it is evaluated from the\nbasic concepts and its characteristic. It is acceptable because SID consists of\na set of numbers which will not be meaningful if applied arithmetic operations\nlike addition, subtraction, multiplication and division. But in terms of\ncomputer organization, data representation type will determine how much data\nspace requirements, speed of access, and speed of operation. By considering the\nconstraints of space and speed on the experiments conducted, SID is better\nexpressed as an integer rather than a set of characters.\n  KEYWORDS aphanumeric,representation, string, integer, space, speed"
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.2427v1", 
    "other_authors": "M. Rajalakshmi, Dr. T. Purusothaman, Dr. R. Nedunchezhian", 
    "title": "Maximal frequent itemset generation using segmentation approach", 
    "arxiv-id": "1109.2427v1", 
    "author": "Dr. R. Nedunchezhian", 
    "publish": "2011-09-12T10:37:53Z", 
    "summary": "Finding frequent itemsets in a data source is a fundamental operation behind\nAssociation Rule Mining. Generally, many algorithms use either the bottom-up or\ntop-down approaches for finding these frequent itemsets. When the length of\nfrequent itemsets to be found is large, the traditional algorithms find all the\nfrequent itemsets from 1-length to n-length, which is a difficult process. This\nproblem can be solved by mining only the Maximal Frequent Itemsets (MFS).\nMaximal Frequent Itemsets are frequent itemsets which have no proper frequent\nsuperset. Thus, the generation of only maximal frequent itemsets reduces the\nnumber of itemsets and also time needed for the generation of all frequent\nitemsets as each maximal itemset of length m implies the presence of 2m-2\nfrequent itemsets. Furthermore, mining only maximal frequent itemset is\nsufficient in many data mining applications like minimal key discovery and\ntheory extraction. In this paper, we suggest a novel method for finding the\nmaximal frequent itemset from huge data sources using the concept of\nsegmentation of data source and prioritization of segments. Empirical\nevaluation shows that this method outperforms various other known methods."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.4288v4", 
    "other_authors": "Qiang Zeng, Xiaorui Jiang, Hai Zhuge", 
    "title": "Adding Logical Operators to Tree Pattern Queries on Graph-Structured   Data", 
    "arxiv-id": "1109.4288v4", 
    "author": "Hai Zhuge", 
    "publish": "2011-09-20T13:30:32Z", 
    "summary": "As data are increasingly modeled as graphs for expressing complex\nrelationships, the tree pattern query on graph-structured data becomes an\nimportant type of queries in real-world applications. Most practical query\nlanguages, such as XQuery and SPARQL, support logical expressions using\nlogical-AND/OR/NOT operators to define structural constraints of tree patterns.\nIn this paper, (1) we propose generalized tree pattern queries (GTPQs) over\ngraph-structured data, which fully support propositional logic of structural\nconstraints. (2) We make a thorough study of fundamental problems including\nsatisfiability, containment and minimization, and analyze the computational\ncomplexity and the decision procedures of these problems. (3) We propose a\ncompact graph representation of intermediate results and a pruning approach to\nreduce the size of intermediate results and the number of join operations --\ntwo factors that often impair the efficiency of traditional algorithms for\nevaluating tree pattern queries. (4) We present an efficient algorithm for\nevaluating GTPQs using 3-hop as the underlying reachability index. (5)\nExperiments on both real-life and synthetic data sets demonstrate the\neffectiveness and efficiency of our algorithm, from several times to orders of\nmagnitude faster than state-of-the-art algorithms in terms of evaluation time,\neven for traditional tree pattern queries with only conjunctive operations."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6299v1", 
    "other_authors": "Radim Belohlavek, Lucie Urbanova, Vilem Vychodil", 
    "title": "Sensitivity Analysis for Declarative Relational Query Languages with   Ordinal Ranks", 
    "arxiv-id": "1109.6299v1", 
    "author": "Vilem Vychodil", 
    "publish": "2011-09-28T19:04:18Z", 
    "summary": "We present sensitivity analysis for results of query executions in a\nrelational model of data extended by ordinal ranks. The underlying model of\ndata results from the ordinary Codd's model of data in which we consider\nordinal ranks of tuples in data tables expressing degrees to which tuples match\nqueries. In this setting, we show that ranks assigned to tuples are insensitive\nto small changes, i.e., small changes in the input data do not yield large\nchanges in the results of queries."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6880v1", 
    "other_authors": "Daniel Fabbri, Kristen LeFevre", 
    "title": "Explanation-Based Auditing", 
    "arxiv-id": "1109.6880v1", 
    "author": "Kristen LeFevre", 
    "publish": "2011-09-30T16:24:41Z", 
    "summary": "To comply with emerging privacy laws and regulations, it has become common\nfor applications like electronic health records systems (EHRs) to collect\naccess logs, which record each time a user (e.g., a hospital employee) accesses\na piece of sensitive data (e.g., a patient record). Using the access log, it is\neasy to answer simple queries (e.g., Who accessed Alice's medical record?), but\nthis often does not provide enough information. In addition to learning who\naccessed their medical records, patients will likely want to understand why\neach access occurred. In this paper, we introduce the problem of generating\nexplanations for individual records in an access log. The problem is motivated\nby user-centric auditing applications, and it also provides a novel approach to\nmisuse detection. We develop a framework for modeling explanations which is\nbased on a fundamental observation: For certain classes of databases, including\nEHRs, the reason for most data accesses can be inferred from data stored\nelsewhere in the database. For example, if Alice has an appointment with Dr.\nDave, this information is stored in the database, and it explains why Dr. Dave\nlooked at Alice's record. Large numbers of data accesses can be explained using\ngeneral forms called explanation templates. Rather than requiring an\nadministrator to manually specify explanation templates, we propose a set of\nalgorithms for automatically discovering frequent templates from the database\n(i.e., those that explain a large number of accesses). We also propose\ntechniques for inferring collaborative user groups, which can be used to\nenhance the quality of the discovered explanations. Finally, we have evaluated\nour proposed techniques using an access log and data from the University of\nMichigan Health System. Our results demonstrate that in practice we can provide\nexplanations for over 94% of data accesses in the log."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6881v1", 
    "other_authors": "Adam Marcus, Eugene Wu, David Karger, Samuel Madden, Robert Miller", 
    "title": "Human-powered Sorts and Joins", 
    "arxiv-id": "1109.6881v1", 
    "author": "Robert Miller", 
    "publish": "2011-09-30T16:24:47Z", 
    "summary": "Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible\nto task people with small jobs, such as labeling images or looking up phone\nnumbers, via a programmatic interface. MTurk tasks for processing datasets with\nhumans are currently designed with significant reimplementation of common\nworkflows and ad-hoc selection of parameters such as price to pay per task. We\ndescribe how we have integrated crowds into a declarative workflow engine\ncalled Qurk to reduce the burden on workflow designers. In this paper, we focus\non how to use humans to compare items for sorting and joining data, two of the\nmost common operations in DBMSs. We describe our basic query interface and the\nuser interface of the tasks we post to MTurk. We also propose a number of\noptimizations, including task batching, replacing pairwise comparisons with\nnumerical ratings, and pre-filtering tables before joining them, which\ndramatically reduce the overall cost of running sorts and joins on the crowd.\nIn an experiment joining two sets of images, we reduce the overall cost from\n$67 in a naive implementation to about $3, without substantially affecting\naccuracy or latency. In an end-to-end experiment, we reduced cost by a factor\nof 14.5."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6882v1", 
    "other_authors": "Graham Cormode, Justin Thaler, Ke Yi", 
    "title": "Verifying Computations with Streaming Interactive Proofs", 
    "arxiv-id": "1109.6882v1", 
    "author": "Ke Yi", 
    "publish": "2011-09-30T16:24:53Z", 
    "summary": "When computation is outsourced, the data owner would like to be assured that\nthe desired computation has been performed correctly by the service provider.\nIn theory, proof systems can give the necessary assurance, but prior work is\nnot sufficiently scalable or practical. In this paper, we develop new proof\nprotocols for verifying computations which are streaming in nature: the\nverifier (data owner) needs only logarithmic space and a single pass over the\ninput, and after observing the input follows a simple protocol with a prover\n(service provider) that takes logarithmic communication spread over a\nlogarithmic number of rounds. These ensure that the computation is performed\ncorrectly: that the service provider has not made any errors or missed out some\ndata. The guarantee is very strong: even if the service provider deliberately\ntries to cheat, there is only vanishingly small probability of doing so\nundetected, while a correct computation is always accepted. We first observe\nthat some theoretical results can be modified to work with streaming verifiers,\nshowing that there are efficient protocols for problems in the complexity\nclasses NP and NC. Our main results then seek to bridge the gap between theory\nand practice by developing usable protocols for a variety of problems of\ncentral importance in streaming and database processing. All these problems\nrequire linear space in the traditional streaming model, and therefore our\nprotocols demonstrate that adding a prover can exponentially reduce the effort\nneeded by the verifier. Our experimental results show that our protocols are\npractical and scalable."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6883v1", 
    "other_authors": "Dan Lin, Christian S. Jensen, Rui Zhang, Lu Xiao, Jiaheng Lu", 
    "title": "A MovingObject Index for Efficient Query Processing with Peer-Wise   Location Privacy", 
    "arxiv-id": "1109.6883v1", 
    "author": "Jiaheng Lu", 
    "publish": "2011-09-30T16:24:58Z", 
    "summary": "With the growing use of location-based services, location privacy attracts\nincreasing attention from users, industry, and the research community. While\nconsiderable effort has been devoted to inventing techniques that prevent\nservice providers from knowing a user's exact location, relatively little\nattention has been paid to enabling so-called peer-wise privacy--the protection\nof a user's location from unauthorized peer users. This paper identifies an\nimportant efficiency problem in existing peer-privacy approaches that simply\napply a filtering step to identify users that are located in a query range, but\nthat do not want to disclose their location to the querying peer. To solve this\nproblem, we propose a novel, privacy-policy enabled index called the PEB-tree\nthat seamlessly integrates location proximity and policy compatibility. We\npropose efficient algorithms that use the PEB-tree for processing privacy-aware\nrange and kNN queries. Extensive experiments suggest that the PEB-tree enables\nefficient query processing."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6884v1", 
    "other_authors": "Essam Mansour, Amin Allam, Spiros Skiadopoulos, Panos Kalnis", 
    "title": "ERA: Efficient Serial and Parallel Suffix Tree Construction for Very   Long Strings", 
    "arxiv-id": "1109.6884v1", 
    "author": "Panos Kalnis", 
    "publish": "2011-09-30T16:25:02Z", 
    "summary": "The suffix tree is a data structure for indexing strings. It is used in a\nvariety of applications such as bioinformatics, time series analysis,\nclustering, text editing and data compression. However, when the string and the\nresulting suffix tree are too large to fit into the main memory, most existing\nconstruction algorithms become very inefficient. This paper presents a\ndisk-based suffix tree construction method, called Elastic Range (ERa), which\nworks efficiently with very long strings that are much larger than the\navailable memory. ERa partitions the tree construction process horizontally and\nvertically and minimizes I/Os by dynamically adjusting the horizontal\npartitions independently for each vertical partition, based on the evolving\nshape of the tree and the available memory. Where appropriate, ERa also groups\nvertical partitions together to amortize the I/O cost. We developed a serial\nversion; a parallel version for shared-memory and shared-disk multi-core\nsystems; and a parallel version for shared-nothing architectures. ERa indexes\nthe entire human genome in 19 minutes on an ordinary desktop computer. For\ncomparison, the fastest existing method needs 15 minutes using 1024 CPUs on an\nIBM BlueGene supercomputer."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6885v1", 
    "other_authors": "Jens Krueger, Changkyu Kim, Martin Grund, Nadathur Satish, David Schwalb, Jatin Chhugani, Hasso Plattner, Pradeep Dubey, Alexander Zeier", 
    "title": "Fast Updates on Read-Optimized Databases Using Multi-Core CPUs", 
    "arxiv-id": "1109.6885v1", 
    "author": "Alexander Zeier", 
    "publish": "2011-09-30T16:25:08Z", 
    "summary": "Read-optimized columnar databases use differential updates to handle writes\nby maintaining a separate write-optimized delta partition which is periodically\nmerged with the read-optimized and compressed main partition. This merge\nprocess introduces significant overheads and unacceptable downtimes in update\nintensive systems, aspiring to combine transactional and analytical workloads\ninto one system. In the first part of the paper, we report data analyses of 12\nSAP Business Suite customer systems. In the second half, we present an\noptimized merge process reducing the merge overhead of current systems by a\nfactor of 30. Our linear-time merge algorithm exploits the underlying high\ncompute and bandwidth resources of modern multi-core CPUs with\narchitecture-aware optimizations and efficient parallelization. This enables\ncompressed in-memory column stores to handle the transactional update rate\nrequired by enterprise applications, while keeping properties of read-optimized\ndatabases for analytic-style queries."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1109.6886v1", 
    "other_authors": "Amit Goyal, Francesco Bonchi, Laks V. S. Lakshmanan", 
    "title": "A Data-Based Approach to Social Influence Maximization", 
    "arxiv-id": "1109.6886v1", 
    "author": "Laks V. S. Lakshmanan", 
    "publish": "2011-09-30T16:25:13Z", 
    "summary": "Influence maximization is the problem of finding a set of users in a social\nnetwork, such that by targeting this set, one maximizes the expected spread of\ninfluence in the network. Most of the literature on this topic has focused\nexclusively on the social graph, overlooking historical data, i.e., traces of\npast action propagations. In this paper, we study influence maximization from a\nnovel data-based perspective. In particular, we introduce a new model, which we\ncall credit distribution, that directly leverages available propagation traces\nto learn how influence flows in the network and uses this to estimate expected\ninfluence spread. Our approach also learns the different levels of\ninfluenceability of users, and it is time-aware in the sense that it takes the\ntemporal nature of influence into account. We show that influence maximization\nunder the credit distribution model is NP-hard and that the function that\ndefines expected spread under our model is submodular. Based on these, we\ndevelop an approximation algorithm for solving the influence maximization\nproblem that at once enjoys high accuracy compared to the standard approach,\nwhile being several orders of magnitude faster and more scalable."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1111.2530v1", 
    "other_authors": "C. Ramesh, K. V. Chalapati Rao, A. Govardhan", 
    "title": "A semantically enriched web usage based recommendation model", 
    "arxiv-id": "1111.2530v1", 
    "author": "A. Govardhan", 
    "publish": "2011-11-10T17:34:05Z", 
    "summary": "With the rapid growth of internet technologies, Web has become a huge\nrepository of information and keeps growing exponentially under no editorial\ncontrol. However the human capability to read, access and understand Web\ncontent remains constant. This motivated researchers to provide Web\npersonalized online services such as Web recommendations to alleviate the\ninformation overload problem and provide tailored Web experiences to the Web\nusers. Recent studies show that Web usage mining has emerged as a popular\napproach in providing Web personalization. However conventional Web usage based\nrecommender systems are limited in their ability to use the domain knowledge of\nthe Web application. The focus is only on Web usage data. As a consequence the\nquality of the discovered patterns is low. In this paper, we propose a novel\nframework integrating semantic information in the Web usage mining process.\nSequential Pattern Mining technique is applied over the semantic space to\ndiscover the frequent sequential patterns. The frequent navigational patterns\nare extracted in the form of Ontology instances instead of Web page views and\nthe resultant semantic patterns are used for generating Web page\nrecommendations to the user. Experimental results shown are promising and\nproved that incorporating semantic information into Web usage mining process\ncan provide us with more interesting patterns which consequently make the\nrecommendation system more functional, smarter and comprehensive."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2011.3302", 
    "link": "http://arxiv.org/pdf/1111.2669v1", 
    "other_authors": "R. B. Geeta, Omkar Mamillapalli, Shasikumar G. Totad, Prasad Reddy P. V. G. D", 
    "title": "A Novel Approach for Web Page Set Mining", 
    "arxiv-id": "1111.2669v1", 
    "author": "Prasad Reddy P. V. G. D", 
    "publish": "2011-11-11T06:24:49Z", 
    "summary": "The one of the most time consuming steps for association rule mining is the\ncomputation of the frequency of the occurrences of itemsets in the database.\nThe hash table index approach converts a transaction database to an hash index\ntree by scanning the transaction database only once. Whenever user requests for\nany Uniform Resource Locator (URL), the request entry is stored in the Log File\nof the server. This paper presents the hash index table structure, a general\nand dense structure which provides web page set extraction from Log File of\nserver. This hash table provides information about the original database. Web\nPage set mining (WPs-Mine) provides a complete representation of the original\ndatabase. This approach works well for both sparse and dense data\ndistributions. Web page set mining supported by hash table index shows the\nperformance always comparable with and often better than algorithms accessing\ndata on flat files. Incremental update is feasible without reaccessing the\noriginal transactional database."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.2852v1", 
    "other_authors": "Patrick Valduriez", 
    "title": "Principles of Distributed Data Management in 2020?", 
    "arxiv-id": "1111.2852v1", 
    "author": "Patrick Valduriez", 
    "publish": "2011-11-11T20:44:38Z", 
    "summary": "With the advents of high-speed networks, fast commodity hardware, and the\nweb, distributed data sources have become ubiquitous. The third edition of the\n\\\"Ozsu-Valduriez textbook Principles of Distributed Database Systems [10]\nreflects the evolution of distributed data management and distributed database\nsystems. In this new edition, the fundamental principles of distributed data\nmanagement could be still presented based on the three dimensions of earlier\neditions: distribution, heterogeneity and autonomy of the data sources. In\nretrospect, the focus on fundamental principles and generic techniques has been\nuseful not only to understand and teach the material, but also to enable an\ninfinite number of variations. The primary application of these generic\ntechniques has been obviously for distributed and parallel DBMS versions.\nToday, to support the requirements of important data-intensive applications\n(e.g. social networks, web data analytics, scientific applications, etc.), new\ndistributed data management techniques and systems (e.g. MapReduce, Hadoop,\nSciDB, Peanut, Pig latin, etc.) are emerging and receiving much attention from\nthe research community. Although they do well in terms of\nconsistency/flexibility/performance trade-offs for specific applications, they\nseem to be ad-hoc and might hurt data interoperability. The key questions I\ndiscuss are: What are the fundamental principles behind the emerging solutions?\nIs there any generic architectural model, to explain those principles? Do we\nneed new foundations to look at data distribution?"
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.3069v1", 
    "other_authors": "Laika Satish, Sami Halawani", 
    "title": "A fusion algorithm for joins based on collections in Odra (Object   Database for Rapid Application development)", 
    "arxiv-id": "1111.3069v1", 
    "author": "Sami Halawani", 
    "publish": "2011-11-13T22:06:36Z", 
    "summary": "In this paper we present the functionality of a currently under development\ndatabase programming methodology called ODRA (Object Database for Rapid\nApplication development) which works fully on the object oriented principles.\nThe database programming language is called SBQL (Stack based query language).\nWe discuss some concepts in ODRA for e.g. the working of ODRA, how ODRA runtime\nenvironment operates, the interoperability of ODRA with .net and java .A view\nof ODRA's working with web services and xml. Currently the stages under\ndevelopment in ODRA are query optimization. So we present the prior work that\nis done in ODRA related to Query optimization and we also present a new fusion\nalgorithm of how ODRA can deal with joins based on collections like set, lists,\nand arrays for query optimization."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.3689v1", 
    "other_authors": "Anish Das Sarma, Ankur Jain, Ashwin Machanavajjhala, Philip Bohannon", 
    "title": "CBLOCK: An Automatic Blocking Mechanism for Large-Scale De-duplication   Tasks", 
    "arxiv-id": "1111.3689v1", 
    "author": "Philip Bohannon", 
    "publish": "2011-11-15T23:32:34Z", 
    "summary": "De-duplication---identification of distinct records referring to the same\nreal-world entity---is a well-known challenge in data integration. Since very\nlarge datasets prohibit the comparison of every pair of records, {\\em blocking}\nhas been identified as a technique of dividing the dataset for pairwise\ncomparisons, thereby trading off {\\em recall} of identified duplicates for {\\em\nefficiency}. Traditional de-duplication tasks, while challenging, typically\ninvolved a fixed schema such as Census data or medical records. However, with\nthe presence of large, diverse sets of structured data on the web and the need\nto organize it effectively on content portals, de-duplication systems need to\nscale in a new dimension to handle a large number of schemas, tasks and data\nsets, while handling ever larger problem sizes. In addition, when working in a\nmap-reduce framework it is important that canopy formation be implemented as a\n{\\em hash function}, making the canopy design problem more challenging. We\npresent CBLOCK, a system that addresses these challenges. CBLOCK learns hash\nfunctions automatically from attribute domains and a labeled dataset consisting\nof duplicates. Subsequently, CBLOCK expresses blocking functions using a\nhierarchical tree structure composed of atomic hash functions. The application\nmay guide the automated blocking process based on architectural constraints,\nsuch as by specifying a maximum size of each block (based on memory\nrequirements), impose disjointness of blocks (in a grid environment), or\nspecify a particular objective function trading off recall for efficiency. As a\npost-processing step to automatically generated blocks, CBLOCK {\\em rolls-up}\nsmaller blocks to increase recall. We present experimental results on two\nlarge-scale de-duplication datasets at Yahoo!---consisting of over 140K movies\nand 40K restaurants respectively---and demonstrate the utility of CBLOCK."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.5639v1", 
    "other_authors": "Seifedine Kadry, Mohamad Smaili, Hussam Kassem, Hassan Hayek", 
    "title": "A New Technique to Backup and Restore DBMS using XML and .NET   Technologies", 
    "arxiv-id": "1111.5639v1", 
    "author": "Hassan Hayek", 
    "publish": "2011-11-23T22:28:38Z", 
    "summary": "In this paper, we proposed a new technique for backing up and restoring\ndifferent Database Management Systems (DBMS). The technique is enabling to\nbackup and restore a part of or the whole database using a unified interface\nusing ASP.NET and XML technologies. It presents a Web Solution allowing the\nadministrators to do their jobs from everywhere, locally or remotely. To show\nthe importance of our solution, we have taken two case studies, oracle 11g and\nSQL Server 2008."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.5687v1", 
    "other_authors": "Baptiste Ducatel, Mehdi Kaytoue, Florent Marcuola, Amedeo Napoli, Laszlo Szathmary", 
    "title": "Coron : Plate-forme d'extraction de connaissances dans les bases de   donn\u00e9es", 
    "arxiv-id": "1111.5687v1", 
    "author": "Laszlo Szathmary", 
    "publish": "2011-11-24T07:52:59Z", 
    "summary": "Coron is a domain and platform independent, multi-purposed data mining\ntoolkit, which incorporates not only a rich collection of data mining\nalgorithms, but also allows a number of auxiliary operations. To the best of\nour knowledge, a data mining toolkit designed specifically for itemset\nextraction and association rule generation like Coron does not exist elsewhere.\nCoron also provides support for preparing and filtering data, and for\ninterpreting the extracted units of knowledge."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.5690v1", 
    "other_authors": "Mehdi Kaytoue, Florent Marcuola, Amedeo Napoli, Laszlo Szathmary, Jean Villerd", 
    "title": "The Coron System", 
    "arxiv-id": "1111.5690v1", 
    "author": "Jean Villerd", 
    "publish": "2011-11-24T07:56:18Z", 
    "summary": "Coron is a domain and platform independent, multi-purposed data mining\ntoolkit, which incorporates not only a rich collection of data mining\nalgorithms, but also allows a number of auxiliary operations. To the best of\nour knowledge, a data mining toolkit designed specifically for itemset\nextraction and association rule generation like Coron does not exist elsewhere.\nCoron also provides support for preparing and filtering data, and for\ninterpreting the extracted units of knowledge."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.6552v8", 
    "other_authors": "Souad Bouasker, Tarek Hamrouni, Sadok Ben Yahia", 
    "title": "Nouvelle repr\u00e9sentation concise exacte des motifs corr\u00e9l\u00e9s rares :   Application \u00e0 la d\u00e9tection d'intrusions", 
    "arxiv-id": "1111.6552v8", 
    "author": "Sadok Ben Yahia", 
    "publish": "2011-11-28T19:17:40Z", 
    "summary": "Correlated rare pattern mining is an interesting issue in Data mining. In\nthis respect, the set of correlated rare patterns w.r.t. to the bond\ncorrelation measure was studied in a recent work, in which the RCPR concise\nexact representation of the set of correlated rare patterns was proposed.\nHowever, none algorithm was proposed in order to mine this representation and\nnone experiment was carried out to evaluate it. In this paper, we introduce the\nnew RcprMiner algorithm allowing an efficient extraction of RCPR. We also\npresent the IsRCP algorithm allowing the query of the RCPR representation in\naddition to the RCPRegeneration algorithm allowing the regeneration of the\nwhole set RCP of rare correlated patterns starting from this representation.\nThe carried out experiments highlight interesting compactness rates offered by\nRCPR. The effectiveness of the proposed classification method, based on generic\nrare correlated association rules derived from RCPR, has also been proved in\nthe context of intrusion detection."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7164v1", 
    "other_authors": "Fabian M. Suchanek, Serge Abiteboul, Pierre Senellart", 
    "title": "PARIS: Probabilistic Alignment of Relations, Instances, and Schema", 
    "arxiv-id": "1111.7164v1", 
    "author": "Pierre Senellart", 
    "publish": "2011-11-30T14:08:40Z", 
    "summary": "One of the main challenges that the Semantic Web faces is the integration of\na growing number of independently designed ontologies. In this work, we present\nPARIS, an approach for the automatic alignment of ontologies. PARIS aligns not\nonly instances, but also relations and classes. Alignments at the instance\nlevel cross-fertilize with alignments at the schema level. Thereby, our system\nprovides a truly holistic solution to the problem of ontology alignment. The\nheart of the approach is probabilistic, i.e., we measure degrees of matchings\nbased on probability estimates. This allows PARIS to run without any parameter\ntuning. We demonstrate the efficiency of the algorithm and its precision\nthrough extensive experiments. In particular, we obtain a precision of around\n90% in experiments with some of the world's largest ontologies."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7165v1", 
    "other_authors": "Sayan Ranu, Ambuj K. Singh", 
    "title": "Answering Top-k Queries Over a Mixture of Attractive and Repulsive   Dimensions", 
    "arxiv-id": "1111.7165v1", 
    "author": "Ambuj K. Singh", 
    "publish": "2011-11-30T14:09:11Z", 
    "summary": "In this paper, we formulate a top-k query that compares objects in a database\nto a user-provided query object on a novel scoring function. The proposed\nscoring function combines the idea of attractive and repulsive dimensions into\na general framework to overcome the weakness of traditional distance or\nsimilarity measures. We study the properties of the proposed class of scoring\nfunctions and develop efficient and scalable index structures that index the\nisolines of the function. We demonstrate various scenarios where the query\nfinds application. Empirical evaluation demonstrates a performance gain of one\nto two orders of magnitude on querying time over existing state-of-the-art\ntop-k techniques. Further, a qualitative analysis is performed on a real\ndataset to highlight the potential of the proposed query in discovering hidden\ndata characteristics."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7166v1", 
    "other_authors": "Michael Armbrust, Kristal Curtis, Tim Kraska, Armando Fox, Michael J. Franklin, David A. Patterson", 
    "title": "PIQL: Success-Tolerant Query Processing in the Cloud", 
    "arxiv-id": "1111.7166v1", 
    "author": "David A. Patterson", 
    "publish": "2011-11-30T14:09:39Z", 
    "summary": "Newly-released web applications often succumb to a \"Success Disaster,\" where\noverloaded database machines and resulting high response times destroy a\npreviously good user experience. Unfortunately, the data independence provided\nby a traditional relational database system, while useful for agile\ndevelopment, only exacerbates the problem by hiding potentially expensive\nqueries under simple declarative expressions. As a result, developers of these\napplications are increasingly abandoning relational databases in favor of\nimperative code written against distributed key/value stores, losing the many\nbenefits of data independence in the process. Instead, we propose PIQL, a\ndeclarative language that also provides scale independence by calculating an\nupper bound on the number of key/value store operations that will be performed\nfor any query. Coupled with a service level objective (SLO) compliance\nprediction model and PIQL's scalable database architecture, these bounds make\nit easy for developers to write success-tolerant applications that support an\narbitrarily large number of users while still providing acceptable performance.\nIn this paper, we present the PIQL query processing system and evaluate its\nscale independence on hundreds of machines using two benchmarks, TPC-W and\nSCADr."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7167v1", 
    "other_authors": "Peixiang Zhao, Charu C. Aggarwal, Min Wang", 
    "title": "gSketch: On Query Estimation in Graph Streams", 
    "arxiv-id": "1111.7167v1", 
    "author": "Min Wang", 
    "publish": "2011-11-30T14:10:01Z", 
    "summary": "Many dynamic applications are built upon large network infrastructures, such\nas social networks, communication networks, biological networks and the Web.\nSuch applications create data that can be naturally modeled as graph streams,\nin which edges of the underlying graph are received and updated sequentially in\na form of a stream. It is often necessary and important to summarize the\nbehavior of graph streams in order to enable effective query processing.\nHowever, the sheer size and dynamic nature of graph streams present an enormous\nchallenge to existing graph management techniques. In this paper, we propose a\nnew graph sketch method, gSketch, which combines well studied synopses for\ntraditional data streams with a sketch partitioning technique, to estimate and\noptimize the responses to basic queries on graph streams. We consider two\ndifferent scenarios for query estimation: (1) A graph stream sample is\navailable; (2) Both a graph stream sample and a query workload sample are\navailable. Algorithms for different scenarios are designed respectively by\npartitioning a global sketch to a group of localized sketches in order to\noptimize the query estimation accuracy. We perform extensive experimental\nstudies on both real and synthetic data sets and demonstrate the power and\nrobustness of gSketch in comparison with the state-of-the-art global sketch\nmethod."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7168v1", 
    "other_authors": "Brian E. Ruttenberg, Ambuj K. Singh", 
    "title": "Indexing the Earth Mover's Distance Using Normal Distributions", 
    "arxiv-id": "1111.7168v1", 
    "author": "Ambuj K. Singh", 
    "publish": "2011-11-30T14:10:17Z", 
    "summary": "Querying uncertain data sets (represented as probability distributions)\npresents many challenges due to the large amount of data involved and the\ndifficulties comparing uncertainty between distributions. The Earth Mover's\nDistance (EMD) has increasingly been employed to compare uncertain data due to\nits ability to effectively capture the differences between two distributions.\nComputing the EMD entails finding a solution to the transportation problem,\nwhich is computationally intensive. In this paper, we propose a new lower bound\nto the EMD and an index structure to significantly improve the performance of\nEMD based K-nearest neighbor (K-NN) queries on uncertain databases. We propose\na new lower bound to the EMD that approximates the EMD on a projection vector.\nEach distribution is projected onto a vector and approximated by a normal\ndistribution, as well as an accompanying error term. We then represent each\nnormal as a point in a Hough transformed space. We then use the concept of\nstochastic dominance to implement an efficient index structure in the\ntransformed space. We show that our method significantly decreases K-NN query\ntime on uncertain databases. The index structure also scales well with database\ncardinality. It is well suited for heterogeneous data sets, helping to keep EMD\nbased queries tractable as uncertain data sets become larger and more complex."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7169v1", 
    "other_authors": "Georgios J. Fakas, Zhi Cai, Nikos Mamoulis", 
    "title": "Size-l Object Summaries for Relational Keyword Search", 
    "arxiv-id": "1111.7169v1", 
    "author": "Nikos Mamoulis", 
    "publish": "2011-11-30T14:11:28Z", 
    "summary": "A previously proposed keyword search paradigm produces, as a query result, a\nranked list of Object Summaries (OSs). An OS is a tree structure of related\ntuples that summarizes all data held in a relational database about a\nparticular Data Subject (DS). However, some of these OSs are very large in size\nand therefore unfriendly to users that initially prefer synoptic information\nbefore proceeding to more comprehensive information about a particular DS. In\nthis paper, we investigate the effective and efficient retrieval of concise and\ninformative OSs. We argue that a good size-l OS should be a stand-alone and\nmeaningful synopsis of the most important information about the particular DS.\nMore precisely, we define a size-l OS as a partial OS composed of l important\ntuples. We propose three algorithms for the efficient generation of size-l OSs\n(in addition to the optimal approach which requires exponential time).\nExperimental evaluation on DBLP and TPC-H databases verifies the effectiveness\nand efficiency of our approach."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7170v1", 
    "other_authors": "Lujun Fang, Anish Das Sarma, Cong Yu, Philip Bohannon", 
    "title": "REX: Explaining Relationships between Entity Pairs", 
    "arxiv-id": "1111.7170v1", 
    "author": "Philip Bohannon", 
    "publish": "2011-11-30T14:11:53Z", 
    "summary": "Knowledge bases of entities and relations (either constructed manually or\nautomatically) are behind many real world search engines, including those at\nYahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs\nwith nodes representing entities and edges representing (primary)\nrelationships, and various studies have been conducted on how to leverage them\nto answer entity seeking queries. Meanwhile, in a complementary direction,\nanalyses over the query logs have enabled researchers to identify entity pairs\nthat are statistically correlated. Such entity relationships are then presented\nto search users through the \"related searches\" feature in modern search\nengines. However, entity relationships thus discovered can often be \"puzzling\"\nto the users because why the entities are connected is often indescribable. In\nthis paper, we propose a novel problem called \"entity relationship\nexplanation\", which seeks to explain why a pair of entities are connected, and\nsolve this challenging problem by integrating the above two complementary\napproaches, i.e., we leverage the knowledge base to \"explain\" the connections\ndiscovered between entity pairs. More specifically, we present REX, a system\nthat takes a pair of entities in a given knowledge base as input and\nefficiently identifies a ranked list of relationship explanations. We formally\ndefine relationship explanations and analyze their desirable properties.\nFurthermore, we design and implement algorithms to efficiently enumerate and\nrank all relationship explanations based on multiple measures of\n\"interestingness.\" We perform extensive experiments over real web-scale data\ngathered from DBpedia and a commercial search engine, demonstrating the\nefficiency and scalability of REX. We also perform user studies to corroborate\nthe effectiveness of explanations generated by REX."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7171v1", 
    "other_authors": "Guoliang Li, Dong Deng, Jiannan Wang, Jianhua Feng", 
    "title": "PASS-JOIN: A Partition-based Method for Similarity Joins", 
    "arxiv-id": "1111.7171v1", 
    "author": "Jianhua Feng", 
    "publish": "2011-11-30T14:12:22Z", 
    "summary": "As an essential operation in data cleaning, the similarity join has attracted\nconsiderable attention from the database community. In this paper, we study\nstring similarity joins with edit-distance constraints, which find similar\nstring pairs from two large sets of strings whose edit distance is within a\ngiven threshold. Existing algorithms are efficient either for short strings or\nfor long strings, and there is no algorithm that can efficiently and adaptively\nsupport both short strings and long strings. To address this problem, we\npropose a partition-based method called Pass-Join. Pass-Join partitions a\nstring into a set of segments and creates inverted indices for the segments.\nThen for each string, Pass-Join selects some of its substrings and uses the\nselected substrings to find candidate pairs using the inverted indices. We\ndevise efficient techniques to select the substrings and prove that our method\ncan minimize the number of selected substrings. We develop novel pruning\ntechniques to efficiently verify the candidate pairs. Experimental results show\nthat our algorithms are efficient for both short strings and long strings, and\noutperform state-of-the-art methods on real datasets."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1111.7224v1", 
    "other_authors": "Rani Qumsiyeh, Maria S. Pera, Yiu-Kai Ng", 
    "title": "Generating Exact- and Ranked Partially-Matched Answers to Questions in   Advertisements", 
    "arxiv-id": "1111.7224v1", 
    "author": "Yiu-Kai Ng", 
    "publish": "2011-11-30T16:08:06Z", 
    "summary": "Taking advantage of the Web, many advertisements (ads for short) websites,\nwhich aspire to increase client's transactions and thus profits, offer\nsearching tools which allow users to (i) post keyword queries to capture their\ninformation needs or (ii) invoke form-based interfaces to create queries by\nselecting search options, such as a price range, filled-in entries, check\nboxes, or drop-down menus. These search mechanisms, however, are inadequate,\nsince they cannot be used to specify a natural-language query with rich\nsyntactic and semantic content, which can only be handled by a question\nanswering (QA) system. Furthermore, existing ads websites are incapable of\nevaluating arbitrary Boolean queries or retrieving partiallymatched answers\nthat might be of interest to the user whenever a user's search yields only a\nfew or no results at all. In solving these problems, we present a QA system for\nads, called CQAds, which (i) allows users to post a natural-language question Q\nfor retrieving relevant ads, if they exist, (ii) identifies ads as answers that\npartially-match the requested information expressed in Q, if insufficient or no\nanswers to Q can be retrieved, which are ordered using a similarity-ranking\napproach, and (iii) analyzes incomplete or ambiguous questions to perform the\n\"best guess\" in retrieving answers that \"best match\" the selection criteria\nspecified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean\noperators that are either explicitly or implicitly specified in Q, i.e., with\nor without Boolean operators specified by the users, respectively. CQAds is\neasy to use, scalable to all ads domains, and more powerful than search tools\nprovided by existing ads websites, since its query-processing strategy\nretrieves relevant ads of higher quality and quantity. We have verified the\naccuracy of CQAds in retrieving ads on eight ads domains and compared\nit...[truncated]."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.2335v1", 
    "other_authors": "Beth Trushkowsky, Tim Kraska, Michael J. Franklin, Purnamrita Sarkar", 
    "title": "Getting It All from the Crowd", 
    "arxiv-id": "1202.2335v1", 
    "author": "Purnamrita Sarkar", 
    "publish": "2012-02-10T19:47:57Z", 
    "summary": "Hybrid human/computer systems promise to greatly expand the usefulness of\nquery processing by incorporating the crowd for data gathering and other tasks.\nSuch systems raise many database system implementation questions. Perhaps most\nfundamental is that the closed world assumption underlying relational query\nsemantics does not hold in such systems. As a consequence the meaning of even\nsimple queries can be called into question. Furthermore query progress\nmonitoring becomes difficult due to non-uniformities in the arrival of\ncrowdsourced data and peculiarities of how people work in crowdsourcing\nsystems. To address these issues, we develop statistical tools that enable\nusers and systems developers to reason about tradeoffs between time/cost and\ncompleteness. These tools can also help drive query execution and crowdsourcing\nstrategies. We evaluate our techniques using experiments on a popular\ncrowdsourcing platform."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.2926v1", 
    "other_authors": "Anjana K. Mahanta, Mala Dutta", 
    "title": "Detection of Calendar-Based Periodicities of Interval-Based Temporal   Patterns", 
    "arxiv-id": "1202.2926v1", 
    "author": "Mala Dutta", 
    "publish": "2012-02-14T03:13:10Z", 
    "summary": "We present a novel technique to identify calendar-based (annual, monthly and\ndaily) periodicities of an interval-based temporal pattern. An interval-based\ntemporal pattern is a pattern that occurs across a time-interval, then\ndisappears for some time, again recurs across another time-interval and so on\nand so forth. Given the sequence of time-intervals in which an interval-based\ntemporal pattern has occurred, we propose a method for identifying the extent\nto which the pattern is periodic with respect to a calendar cycle. In\ncomparison to previous work, our method is asymptotically faster. We also show\nan interesting relationship between periodicities across different levels of\nany hierarchical timestamp (year/month/day, hour/minute/second etc.)."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3179v1", 
    "other_authors": "Ke Wang, Chao Han, Ada Waichee Fu", 
    "title": "Randomization Resilient To Sensitive Reconstruction", 
    "arxiv-id": "1202.3179v1", 
    "author": "Ada Waichee Fu", 
    "publish": "2012-02-15T00:23:24Z", 
    "summary": "With the randomization approach, sensitive data items of records are\nrandomized to protect privacy of individuals while allowing the distribution\ninformation to be reconstructed for data analysis. In this paper, we\ndistinguish between reconstruction that has potential privacy risk, called\nmicro reconstruction, and reconstruction that does not, called aggregate\nreconstruction. We show that the former could disclose sensitive information\nabout a target individual, whereas the latter is more useful for data analysis\nthan for privacy breaches. To limit the privacy risk of micro reconstruction,\nwe propose a privacy definition, called (epsilon,delta)-reconstruction-privacy.\nIntuitively, this privacy notion requires that micro reconstruction has a large\nerror with a large probability. The promise of this approach is that micro\nreconstruction is more sensitive to the number of independent trials in the\nrandomization process than aggregate reconstruction is; therefore, reducing the\nnumber of independent trials helps achieve\n(epsilon,delta)-reconstruction-privacy while preserving the accuracy of\naggregate reconstruction. We present an algorithm based on this idea and\nevaluate the effectiveness of this approach using real life data sets."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3215v1", 
    "other_authors": "J. Malar Vizhi, T. Bhuvaneswari", 
    "title": "Data quality measurement on categorical data using genetic algorithm", 
    "arxiv-id": "1202.3215v1", 
    "author": "T. Bhuvaneswari", 
    "publish": "2012-02-15T06:32:26Z", 
    "summary": "Data quality on categorical attribute is a difficult problem that has not\nreceived as much attention as numerical counterpart. Our basic idea is to\nemploy association rule for the purpose of data quality measurement. Strong\nrule generation is an important area of data mining. Association rule mining\nproblems can be considered as a multi objective problem rather than as a single\nobjective one. The main area of concentration was the rules generated by\nassociation rule mining using genetic algorithm. The advantage of using genetic\nalgorithm is to discover high level prediction rules is that they perform a\nglobal search and cope better with attribute interaction than the greedy rule\ninduction algorithm often used in data mining. Genetic algorithm based approach\nutilizes the linkage between association rule and feature selection. In this\npaper, we put forward a Multi objective genetic algorithm approach for data\nquality on categorical attributes. The result shows that our approach is\noutperformed by the objectives like accuracy, completeness, comprehensibility\nand interestingness."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3253v1", 
    "other_authors": "Ada Wai-Chee Fu, Jia Wang, Ke Wang, Raymond Chi-Wing Wong", 
    "title": "Small Count Privacy and Large Count Utility in Data Publishing", 
    "arxiv-id": "1202.3253v1", 
    "author": "Raymond Chi-Wing Wong", 
    "publish": "2012-02-15T10:04:25Z", 
    "summary": "While the introduction of differential privacy has been a major breakthrough\nin the study of privacy preserving data publication, some recent work has\npointed out a number of cases where it is not possible to limit inference about\nindividuals. The dilemma that is intrinsic in the problem is the simultaneous\nrequirement of data utility in the published data. Differential privacy does\nnot aim to protect information about an individual that can be uncovered even\nwithout the participation of the individual. However, this lack of coverage may\nviolate the principle of individual privacy. Here we propose a solution by\nproviding protection to sensitive information, by which we refer to the answers\nfor aggregate queries with small counts. Previous works based on\n$\\ell$-diversity can be seen as providing a special form of this kind of\nprotection. Our method is developed with another goal which is to provide\ndifferential privacy guarantee, and for that we introduce a more refined form\nof differential privacy to deal with certain practical issues. Our empirical\nstudies show that our method can preserve better utilities than a number of\nstate-of-the-art methods although these methods do not provide the protections\nthat we provide."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3461v2", 
    "other_authors": "Liyue Fan, Li Xiong", 
    "title": "Adaptively Sharing Time-Series with Differential Privacy", 
    "arxiv-id": "1202.3461v2", 
    "author": "Li Xiong", 
    "publish": "2012-02-15T22:14:31Z", 
    "summary": "Sharing real-time aggregate statistics of private data is of great value to\nthe public to perform data mining for understanding important phenomena, such\nas Influenza outbreaks and traffic congestion. However, releasing time-series\ndata with standard differential privacy mechanism has limited utility due to\nhigh correlation between data values. We propose FAST, a novel framework to\nrelease real-time aggregate statistics under differential privacy based on\nfiltering and adaptive sampling. To minimize the overall privacy cost, FAST\nadaptively samples long time-series according to the detected data dynamics. To\nimprove the accuracy of data release per time stamp, FAST predicts data values\nat non-sampling points and corrects noisy observations at sampling points. Our\nexperiments with real-world as well as synthetic data sets confirm that FAST\nimproves the accuracy of released aggregates even under small privacy cost and\ncan be used to enable a wide range of monitoring applications."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3667v3", 
    "other_authors": "Juan F. Sequeda, Marcelo Arenas, Daniel P. Miranker", 
    "title": "On Directly Mapping Relational Databases to RDF and OWL (Extended   Version)", 
    "arxiv-id": "1202.3667v3", 
    "author": "Daniel P. Miranker", 
    "publish": "2012-02-16T19:04:49Z", 
    "summary": "Mapping relational databases to RDF is a fundamental problem for the\ndevelopment of the Semantic Web. We present a solution, inspired by draft\nmethods defined by the W3C where relational databases are directly mapped to\nRDF and OWL. Given a relational database schema and its integrity constraints,\nthis direct mapping produces an OWL ontology, which, provides the basis for\ngenerating RDF instances. The semantics of this mapping is defined using\nDatalog. Two fundamental properties are information preservation and query\npreservation. We prove that our mapping satisfies both conditions, even for\nrelational databases that contain null values. We also consider two desirable\nproperties: monotonicity and semantics preservation. We prove that our mapping\nis monotone and also prove that no monotone mapping, including ours, is\nsemantic preserving. We realize that monotonicity is an obstacle for semantic\npreservation and thus present a non-monotone direct mapping that is semantics\npreserving."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3686v1", 
    "other_authors": "Ke Wang, Peng Wang, Ada Waichee Fu, Raywong Chi-Wing Wong", 
    "title": "Inferential or Differential: Privacy Laws Dictate", 
    "arxiv-id": "1202.3686v1", 
    "author": "Raywong Chi-Wing Wong", 
    "publish": "2012-02-16T20:18:48Z", 
    "summary": "So far, privacy models follow two paradigms. The first paradigm, termed\ninferential privacy in this paper, focuses on the risk due to statistical\ninference of sensitive information about a target record from other records in\nthe database. The second paradigm, known as differential privacy, focuses on\nthe risk to an individual when included in, versus when not included in, the\ndatabase. The contribution of this paper consists of two parts. The first part\npresents a critical analysis on differential privacy with two results: (i) the\ndifferential privacy mechanism does not provide inferential privacy, (ii) the\nimpossibility result about achieving Dalenius's privacy goal [5] is based on an\nadversary simulated by a Turing machine, but a human adversary may behave\ndifferently; consequently, the practical implication of the impossibility\nresult remains unclear. The second part of this work is devoted to a solution\naddressing three major drawbacks in previous approaches to inferential privacy:\nlack of flexibility for handling variable sensitivity, poor utility, and\nvulnerability to auxiliary information."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.3807v1", 
    "other_authors": "Chao Li, Gerome Miklau", 
    "title": "An Adaptive Mechanism for Accurate Query Answering under Differential   Privacy", 
    "arxiv-id": "1202.3807v1", 
    "author": "Gerome Miklau", 
    "publish": "2012-02-16T22:00:09Z", 
    "summary": "We propose a novel mechanism for answering sets of count- ing queries under\ndifferential privacy. Given a workload of counting queries, the mechanism\nautomatically selects a different set of \"strategy\" queries to answer\nprivately, using those answers to derive answers to the workload. The main\nalgorithm proposed in this paper approximates the optimal strategy for any\nworkload of linear counting queries. With no cost to the privacy guarantee, the\nmechanism improves significantly on prior approaches and achieves near-optimal\nerror for many workloads, when applied under (\\epsilon, \\delta)-differential\nprivacy. The result is an adaptive mechanism which can help users achieve good\nutility without requiring that they reason carefully about the best formulation\nof their task."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.4818v1", 
    "other_authors": "Sanober Shaikh, Madhuri rao", 
    "title": "Association Rule Mining Based On Trade List", 
    "arxiv-id": "1202.4818v1", 
    "author": "Madhuri rao", 
    "publish": "2012-02-22T05:07:44Z", 
    "summary": "In this paper a new mining algorithm is defined based on frequent item set.\nApriori Algorithm scans the database every time when it finds the frequent item\nset so it is very time consuming and at each step it generates candidate item\nset. So for large databases it takes lots of space to store candidate item set\n.In undirected item set graph, it is improvement on apriori but it takes time\nand space for tree generation. The defined algorithm scans the database at the\nstart only once and then from that scanned data base it generates the Trade\nList. It contains the information of whole database. By considering minimum\nsupport it finds the frequent item set and by considering the minimum\nconfidence it generates the association rule. If database and minimum support\nis changed, the new algorithm finds the new frequent items by scanning Trade\nList. That is why it's executing efficiency is improved distinctly compared to\ntraditional algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.5358v1", 
    "other_authors": "Yonghui Xiao, Li Xiong, Liyue Fan, Slawomir Goryczka", 
    "title": "DPCube: Differentially Private Histogram Release through   Multidimensional Partitioning", 
    "arxiv-id": "1202.5358v1", 
    "author": "Slawomir Goryczka", 
    "publish": "2012-02-24T01:29:34Z", 
    "summary": "Differential privacy is a strong notion for protecting individual privacy in\nprivacy preserving data analysis or publishing. In this paper, we study the\nproblem of differentially private histogram release for random workloads. We\nstudy two multidimensional partitioning strategies including: 1) a baseline\ncell-based partitioning strategy for releasing an equi-width cell histogram,\nand 2) an innovative 2-phase kd-tree based partitioning strategy for releasing\na v-optimal histogram. We formally analyze the utility of the released\nhistograms and quantify the errors for answering linear queries such as\ncounting queries. We formally characterize the property of the input data that\nwill guarantee the optimality of the algorithm. Finally, we implement and\nexperimentally evaluate several applications using the released histograms,\nincluding counting queries, classification, and blocking for record linkage and\nshow the benefit of our approach."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1202.6677v1", 
    "other_authors": "Alin Deutsch, Richard Hull, Avinash Vyas, Kevin Keliang Zhao", 
    "title": "Trajectory and Policy Aware Sender Anonymity in Location Based Services", 
    "arxiv-id": "1202.6677v1", 
    "author": "Kevin Keliang Zhao", 
    "publish": "2012-02-29T20:37:33Z", 
    "summary": "We consider Location-based Service (LBS) settings, where a LBS provider logs\nthe requests sent by mobile device users over a period of time and later wants\nto publish/share these logs. Log sharing can be extremely valuable for\nadvertising, data mining research and network management, but it poses a\nserious threat to the privacy of LBS users. Sender anonymity solutions prevent\na malicious attacker from inferring the interests of LBS users by associating\nthem with their service requests after gaining access to the anonymized logs.\nWith the fast-increasing adoption of smartphones and the concern that historic\nuser trajectories are becoming more accessible, it becomes necessary for any\nsender anonymity solution to protect against attackers that are\ntrajectory-aware (i.e. have access to historic user trajectories) as well as\npolicy-aware (i.e they know the log anonymization policy). We call such\nattackers TP-aware.\n  This paper introduces a first privacy guarantee against TP-aware attackers,\ncalled TP-aware sender k-anonymity. It turns out that there are many possible\nTP-aware anonymizations for the same LBS log, each with a different utility to\nthe consumer of the anonymized log. The problem of finding the optimal TP-aware\nanonymization is investigated. We show that trajectory-awareness renders the\nproblem computationally harder than the trajectory-unaware variants found in\nthe literature (NP-complete in the size of the log, versus PTIME). We describe\na PTIME l-approximation algorithm for trajectories of length l and empirically\nshow that it scales to large LBS logs (up to 2 million users)."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1205.0724v1", 
    "other_authors": "Phuc V. Nguyen", 
    "title": "Using Data Warehouse to Support Building Strategy or Forecast Business   Tend", 
    "arxiv-id": "1205.0724v1", 
    "author": "Phuc V. Nguyen", 
    "publish": "2011-12-14T11:46:13Z", 
    "summary": "The data warehousing is becoming increasingly important in terms of strategic\ndecision making through their capacity to integrate heterogeneous data from\nmultiple information sources in a common storage space, for querying and\nanalysis. So it can evolve into a multi-tier structure where parts of the\norganization take information from the main data warehouse into their own\nsystems. These may include analysis databases or dependent data marts. As the\ndata warehouse evolves and the organization gets better at capturing\ninformation on all interactions with the customer. Data warehouse can track\ncustomer interactions over the whole of the customer's lifetime."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1205.1126v1", 
    "other_authors": "Md. Rashid Farooqi, Khalid Raza", 
    "title": "A Comprehensive Study of CRM through Data Mining Techniques", 
    "arxiv-id": "1205.1126v1", 
    "author": "Khalid Raza", 
    "publish": "2012-05-05T12:29:35Z", 
    "summary": "In today's competitive scenario in corporate world, \"Customer Retention\"\nstrategy in Customer Relationship Management (CRM) is an increasingly pressed\nissue. Data mining techniques play a vital role in better CRM. This paper\nattempts to bring a new perspective by focusing the issue of data mining\napplications, opportunities and challenges in CRM. It covers the topic such as\ncustomer retention, customer services, risk assessment, fraud detection and\nsome of the data mining tools which are widely used in CRM."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-23088-2", 
    "link": "http://arxiv.org/pdf/1205.1609v1", 
    "other_authors": "Jyothi Pillai, O. P. Vyas", 
    "title": "CSHURI - Modified HURI algorithm for Customer Segmentation and   Transaction Profitability", 
    "arxiv-id": "1205.1609v1", 
    "author": "O. P. Vyas", 
    "publish": "2012-05-08T07:21:44Z", 
    "summary": "Association rule mining (ARM) is the process of generating rules based on the\ncorrelation between the set of items that the customers purchase.Of late, data\nmining researchers have improved upon the quality of association rule mining\nfor business development by incorporating factors like value (utility),\nquantity of items sold (weight) and profit. The rules mined without considering\nutility values (profit margin) will lead to a probable loss of profitable\nrules. The advantage of wealth of the customers' needs information and rules\naids the retailer in designing his store layout[9]. An algorithm CSHURI,\nCustomer Segmentation using HURI, is proposed, a modified version of HURI [6],\nfinds customers who purchase high profitable rare items and accordingly\nclassify the customers based on some criteria; for example, a retail business\nmay need to identify valuable customers who are major contributors to a\ncompany's overall profit. For a potential customer arriving in the store, which\ncustomer group one should belong to according to customer needs, what are the\npreferred functional features or products that the customer focuses on and what\nkind of offers will satisfy the customer, etc., finds the key in targeting\ncustomers to improve sales [9], which forms the base for customer utility\nmining."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.1796v1", 
    "other_authors": "Azedine Boulmakoul, Lamia Karim, Ahmed Lbath", 
    "title": "Moving Object Trajectories Meta-Model And Spatio-Temporal Queries", 
    "arxiv-id": "1205.1796v1", 
    "author": "Ahmed Lbath", 
    "publish": "2012-05-08T19:58:28Z", 
    "summary": "In this paper, a general moving object trajectories framework is put forward\nto allow independent applications processing trajectories data benefit from a\nhigh level of interoperability, information sharing as well as an efficient\nanswer for a wide range of complex trajectory queries. Our proposed meta-model\nis based on ontology and event approach, incorporates existing presentations of\ntrajectory and integrates new patterns like space-time path to describe\nactivities in geographical space-time. We introduce recursive Region of\nInterest concepts and deal mobile objects trajectories with diverse\nspatio-temporal sampling protocols and different sensors available that\ntraditional data model alone are incapable for this purpose."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.1923v1", 
    "other_authors": "Shweta Kharya", 
    "title": "Using data mining techniques for diagnosis and prognosis of cancer   disease", 
    "arxiv-id": "1205.1923v1", 
    "author": "Shweta Kharya", 
    "publish": "2012-05-09T09:37:52Z", 
    "summary": "Breast cancer is one of the leading cancers for women in developed countries\nincluding India. It is the second most common cause of cancer death in women.\nThe high incidence of breast cancer in women has increased significantly in the\nlast years. In this paper we have discussed various data mining approaches that\nhave been utilized for breast cancer diagnosis and prognosis. Breast Cancer\nDiagnosis is distinguishing of benign from malignant breast lumps and Breast\nCancer Prognosis predicts when Breast Cancer is to recur in patients that have\nhad their cancers excised. This study paper summarizes various review and\ntechnical articles on breast cancer diagnosis and prognosis also we focus on\ncurrent research being carried out using the data mining techniques to enhance\nthe breast cancer diagnosis and prognosis."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.2320v1", 
    "other_authors": "Theodore Dalamagas, Nikos Bikakis, George Papastefanatos, Yannis Stavrakas, Artemis G. Hatzigeorgiou", 
    "title": "Publishing Life Science Data as Linked Open Data: the Case Study of   miRBase", 
    "arxiv-id": "1205.2320v1", 
    "author": "Artemis G. Hatzigeorgiou", 
    "publish": "2012-05-10T17:29:05Z", 
    "summary": "This paper presents our Linked Open Data (LOD) infrastructures for genomic\nand experimental data related to microRNA biomolecules. Legacy data from two\nwell-known microRNA databases with experimental data and observations, as well\nas change and version information about microRNA entities, are fused and\nexported as LOD. Our LOD server assists biologists to explore biological\nentities and their evolution, and provides a SPARQL endpoint for applications\nand services to query historical miRNA data and track changes, their causes and\neffects."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.2465v1", 
    "other_authors": "Julian Eberius, Katrin Braunschweig, Maik Thiele, Wolfgang Lehner", 
    "title": "Identifying And Weighting Integration Hypotheses On Open Data Platforms", 
    "arxiv-id": "1205.2465v1", 
    "author": "Wolfgang Lehner", 
    "publish": "2012-05-11T09:31:27Z", 
    "summary": "Open data platforms such as data.gov or opendata.socrata. com provide a huge\namount of valuable information. Their free-for-all nature, the lack of\npublishing standards and the multitude of domains and authors represented on\nthese platforms lead to new integration and standardization problems. At the\nsame time, crowd-based data integration techniques are emerging as new way of\ndealing with these problems. However, these methods still require input in form\nof specific questions or tasks that can be passed to the crowd. This paper\ndiscusses integration problems on Open Data Platforms, and proposes a method\nfor identifying and ranking integration hypotheses in this context. We will\nevaluate our findings by conducting a comprehensive evaluation using on one of\nthe largest Open Data platforms."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.2691v2", 
    "other_authors": "Ahmad Assaf, Eldad Louw, Aline Senart, Corentin Follenfant, Rapha\u00ebl Troncy, David Trastour", 
    "title": "Improving Schema Matching with Linked Data", 
    "arxiv-id": "1205.2691v2", 
    "author": "David Trastour", 
    "publish": "2012-05-11T17:20:02Z", 
    "summary": "With today's public data sets containing billions of data items, more and\nmore companies are looking to integrate external data with their traditional\nenterprise data to improve business intelligence analysis. These distributed\ndata sources however exhibit heterogeneous data formats and terminologies and\nmay contain noisy data. In this paper, we present a novel framework that\nenables business users to semi-automatically perform data integration on\npotentially noisy tabular data. This framework offers an extension to Google\nRefine with novel schema matching algorithms leveraging Freebase rich types.\nFirst experiments show that using Linked Data to map cell values with instances\nand column headers with types improves significantly the quality of the\nmatching results and therefore should lead to more informed decisions."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.2726v1", 
    "other_authors": "David Leoni", 
    "title": "Non-Interactive Differential Privacy: a Survey", 
    "arxiv-id": "1205.2726v1", 
    "author": "David Leoni", 
    "publish": "2012-05-11T21:38:16Z", 
    "summary": "OpenData movement around the globe is demanding more access to information\nwhich lies locked in public or private servers. As recently reported by a\nMcKinsey publication, this data has significant economic value, yet its release\nhas potential to blatantly conflict with people privacy. Recent UK government\ninquires have shown concern from various parties about publication of\nanonymized databases, as there is concrete possibility of user identification\nby means of linkage attacks. Differential privacy stands out as a model that\nprovides strong formal guarantees about the anonymity of the participants in a\nsanitized database. Only recent results demonstrated its applicability on\nreal-life datasets, though. This paper covers such breakthrough discoveries, by\nreviewing applications of differential privacy for non-interactive publication\nof anonymized real-life datasets. Theory, utility and a data-aware comparison\nare discussed on a variety of principles and concrete applications."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.2880v1", 
    "other_authors": "Gao Cong, Hua Lu, Beng Chin Ooi, Dongxiang Zhang, Meihui Zhang", 
    "title": "Efficient Spatial Keyword Search in Trajectory Databases", 
    "arxiv-id": "1205.2880v1", 
    "author": "Meihui Zhang", 
    "publish": "2012-05-13T16:35:40Z", 
    "summary": "An increasing amount of trajectory data is being annotated with text\ndescriptions to better capture the semantics associated with locations. The\nfusion of spatial locations and text descriptions in trajectories engenders a\nnew type of top-$k$ queries that take into account both aspects. Each\ntrajectory in consideration consists of a sequence of geo-spatial locations\nassociated with text descriptions. Given a user location $\\lambda$ and a\nkeyword set $\\psi$, a top-$k$ query returns $k$ trajectories whose text\ndescriptions cover the keywords $\\psi$ and that have the shortest match\ndistance. To the best of our knowledge, previous research on querying\ntrajectory databases has focused on trajectory data without any text\ndescription, and no existing work has studied such kind of top-$k$ queries on\ntrajectories. This paper proposes one novel method for efficiently computing\ntop-$k$ trajectories. The method is developed based on a new hybrid index,\ncell-keyword conscious B$^+$-tree, denoted by \\cellbtree, which enables us to\nexploit both text relevance and location proximity to facilitate efficient and\neffective query processing. The results of our extensive empirical studies with\nan implementation of the proposed algorithms on BerkeleyDB demonstrate that our\nproposed methods are capable of achieving excellent performance and good\nscalability."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.3231v1", 
    "other_authors": "Rekha Sunny T, Sabu M. Thampi", 
    "title": "Survey on Distributed Data Mining in P2P Networks", 
    "arxiv-id": "1205.3231v1", 
    "author": "Sabu M. Thampi", 
    "publish": "2012-05-15T01:00:07Z", 
    "summary": "The exponential increase of availability of digital data and the necessity to\nprocess it in business and scientific fields has literally forced upon us the\nneed to analyze and mine useful knowledge from it. Traditionally data mining\nhas used a data warehousing model of gathering all data into a central site,\nand then running an algorithm upon that data. Such a centralized approach is\nfundamentally inappropriate due to many reasons like huge amount of data,\ninfeasibility to centralize data stored at multiple sites, bandwidth limitation\nand privacy concerns. To solve these problems, Distributed Data Mining (DDM)\nhas emerged as a hot research area. Careful attention in the usage of\ndistributed resources of data, computing, communication, and human factors in a\nnear optimal fashion are paid by distributed data mining. DDM is gaining\nattention in peer-to-peer (P2P) systems which are emerging as a choice of\nsolution for applications such as file sharing, collaborative movie and song\nscoring, electronic commerce, and surveillance using sensor networks. The main\nintension of this draft paper is to provide an overview of DDM and P2P Data\nMining. The paper discusses the need for DDM, taxonomy of DDM architectures,\nvarious DDM approaches, DDM related works in P2P systems and issues and\nchallenges in P2P data mining."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.3321v2", 
    "other_authors": "Gianluigi Greco, Francesco Scarcello", 
    "title": "Tree Projections and Structural Decomposition Methods: The Power of   Local Consistency and Larger Islands of Tractability", 
    "arxiv-id": "1205.3321v2", 
    "author": "Francesco Scarcello", 
    "publish": "2012-05-15T11:00:48Z", 
    "summary": "Evaluating conjunctive queries and solving constraint satisfaction problems\nare fundamental problems in database theory and artificial intelligence,\nrespectively. These problems are NP-hard, so that several research efforts have\nbeen made in the literature for identifying tractable classes, known as islands\nof tractability, as well as for devising clever heuristics for solving\nefficiently real-world instances. Many heuristic approaches are based on\nenforcing on the given instance a property called local consistency, where (in\ndatabase terms) each tuple in every query atom matches at least one tuple in\nevery other query atom. Interestingly, it turns out that, for many well-known\nclasses of queries, such as for the acyclic queries, enforcing local\nconsistency is even sufficient to solve the given instance correctly. However,\nthe precise power of such a procedure was unclear, but for some very restricted\ncases. The paper provides full answers to the long-standing questions about the\nprecise power of algorithms based on enforcing local consistency. The classes\nof instances where enforcing local consistency turns out to be a correct\nquery-answering procedure are however not efficiently recognizable. In fact,\nthe paper finally focuses on certain subclasses defined in terms of the novel\nnotion of greedy tree projections. These latter classes are shown to be\nefficiently recognizable and strictly larger than most islands of tractability\nknown so far, both in the general case of tree projections and for specific\nstructural decomposition methods."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.5921v1", 
    "other_authors": "Noreddine GHERABI, Mohamed BAHAJ", 
    "title": "Robust representation for conversion UML class into XML Document using   DOM", 
    "arxiv-id": "1205.5921v1", 
    "author": "Mohamed BAHAJ", 
    "publish": "2012-05-26T22:37:10Z", 
    "summary": "This paper presents a Framework for converting a class diagram into an XML\nstructure and shows how to use Web files for the design of data warehouses\nbased on the classification UML. Extensible Markup Language (XML) has become a\nstandard for representing data over the Internet. We use XSD schema for define\nthe structure of XML documents and validate XML documents.\n  A prototype has been developed, which migrates successfully UML Class into\nXML document based on the formulation mathematics model. The experimental\nresults were very encouraging, demonstrating that the proposed approach is\nfeasible efficient and correct."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.5922v1", 
    "other_authors": "Noreddine Gherabi, Khaoula Addakiri, Mohamed Bahaj", 
    "title": "Mapping relational database into OWL Structure with data semantic   preservation", 
    "arxiv-id": "1205.5922v1", 
    "author": "Mohamed Bahaj", 
    "publish": "2012-05-26T22:41:44Z", 
    "summary": "This paper proposes a solution for migrating an RDB into Web semantic. The\nsolution takes an existing RDB as input, and extracts its metadata\nrepresentation (MTRDB). Based on the MTRDB, a Canonical Data Model (CDM) is\ngenerated. Finally, the structure of the classification scheme in the CDM model\nis converted into OWL ontology and the recordsets of database are stored in owl\ndocument. A prototype has been implemented, which migrates a RDB into OWL\nstructure, for demonstrate the practical applicability of our approach by\nshowing how the results of reasoning of this technique can help improve the Web\nsystems."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6691v1", 
    "other_authors": "Zhao Sun, Hongzhi Wang, Haixun Wang, Bin Shao, Jianzhong Li", 
    "title": "Efficient Subgraph Matching on Billion Node Graphs", 
    "arxiv-id": "1205.6691v1", 
    "author": "Jianzhong Li", 
    "publish": "2012-05-30T14:32:16Z", 
    "summary": "The ability to handle large scale graph data is crucial to an increasing\nnumber of applications. Much work has been dedicated to supporting basic graph\noperations such as subgraph matching, reachability, regular expression\nmatching, etc. In many cases, graph indices are employed to speed up query\nprocessing. Typically, most indices require either super-linear indexing time\nor super-linear indexing space. Unfortunately, for very large graphs,\nsuper-linear approaches are almost always infeasible. In this paper, we study\nthe problem of subgraph matching on billion-node graphs. We present a novel\nalgorithm that supports efficient subgraph matching for graphs deployed on a\ndistributed memory store. Instead of relying on super-linear indices, we use\nefficient graph exploration and massive parallel computing for query\nprocessing. Our experimental results demonstrate the feasibility of performing\nsubgraph matching on web-scale graph data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6692v1", 
    "other_authors": "Ye Yuan, Guoren Wang, Lei Chen, Haixun Wang", 
    "title": "Efficient Subgraph Similarity Search on Large Probabilistic Graph   Databases", 
    "arxiv-id": "1205.6692v1", 
    "author": "Haixun Wang", 
    "publish": "2012-05-30T14:32:39Z", 
    "summary": "Many studies have been conducted on seeking the efficient solution for\nsubgraph similarity search over certain (deterministic) graphs due to its wide\napplication in many fields, including bioinformatics, social network analysis,\nand Resource Description Framework (RDF) data management. All these works\nassume that the underlying data are certain. However, in reality, graphs are\noften noisy and uncertain due to various factors, such as errors in data\nextraction, inconsistencies in data integration, and privacy preserving\npurposes. Therefore, in this paper, we study subgraph similarity search on\nlarge probabilistic graph databases. Different from previous works assuming\nthat edges in an uncertain graph are independent of each other, we study the\nuncertain graphs where edges' occurrences are correlated. We formally prove\nthat subgraph similarity search over probabilistic graphs is #P-complete, thus,\nwe employ a filter-and-verify framework to speed up the search. In the\nfiltering phase,we develop tight lower and upper bounds of subgraph similarity\nprobability based on a probabilistic matrix index, PMI. PMI is composed of\ndiscriminative subgraph features associated with tight lower and upper bounds\nof subgraph isomorphism probability. Based on PMI, we can sort out a large\nnumber of probabilistic graphs and maximize the pruning capability. During the\nverification phase, we develop an efficient sampling algorithm to validate the\nremaining candidates. The efficiency of our proposed solutions has been\nverified through extensive experiments."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6693v1", 
    "other_authors": "Jia Wang, James Cheng", 
    "title": "Truss Decomposition in Massive Networks", 
    "arxiv-id": "1205.6693v1", 
    "author": "James Cheng", 
    "publish": "2012-05-30T14:32:46Z", 
    "summary": "The k-truss is a type of cohesive subgraphs proposed recently for the study\nof networks. While the problem of computing most cohesive subgraphs is NP-hard,\nthere exists a polynomial time algorithm for computing k-truss. Compared with\nk-core which is also efficient to compute, k-truss represents the \"core\" of a\nk-core that keeps the key information of, while filtering out less important\ninformation from, the k-core. However, existing algorithms for computing\nk-truss are inefficient for handling today's massive networks. We first improve\nthe existing in-memory algorithm for computing k-truss in networks of moderate\nsize. Then, we propose two I/O-efficient algorithms to handle massive networks\nthat cannot fit in main memory. Our experiments on real datasets verify the\nefficiency of our algorithms and the value of k-truss."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6694v1", 
    "other_authors": "Ju Fan, Guoliang Li, Lizhu Zhou, Shanshan Chen, Jun Hu", 
    "title": "SEAL: Spatio-Textual Similarity Search", 
    "arxiv-id": "1205.6694v1", 
    "author": "Jun Hu", 
    "publish": "2012-05-30T14:32:51Z", 
    "summary": "Location-based services (LBS) have become more and more ubiquitous recently.\nExisting methods focus on finding relevant points-of-interest (POIs) based on\nusers' locations and query keywords. Nowadays, modern LBS applications generate\na new kind of spatio-textual data, regions-of-interest (ROIs), containing\nregion-based spatial information and textual description, e.g., mobile user\nprofiles with active regions and interest tags. To satisfy search requirements\non ROIs, we study a new research problem, called spatio-textual similarity\nsearch: Given a set of ROIs and a query ROI, we find the similar ROIs by\nconsidering spatial overlap and textual similarity. Spatio-textual similarity\nsearch has many important applications, e.g., social marketing in\nlocation-aware social networks. It calls for an efficient search method to\nsupport large scales of spatio-textual data in LBS systems. To this end, we\nintroduce a filter-and-verification framework to compute the answers. In the\nfilter step, we generate signatures for the ROIs and the query, and utilize the\nsignatures to generate candidates whose signatures are similar to that of the\nquery. In the verification step, we verify the candidates and identify the\nfinal answers. To achieve high performance, we generate effective high-quality\nsignatures, and devise efficient filtering algorithms as well as pruning\ntechniques. Experimental results on real and synthetic datasets show that our\nmethod achieves high performance."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6695v1", 
    "other_authors": "Theodoros Lappas, Marcos R. Vieira, Dimitrios Gunopulos, Vassilis J. Tsotras", 
    "title": "On The Spatiotemporal Burstiness of Terms", 
    "arxiv-id": "1205.6695v1", 
    "author": "Vassilis J. Tsotras", 
    "publish": "2012-05-30T14:32:56Z", 
    "summary": "Thousands of documents are made available to the users via the web on a daily\nbasis. One of the most extensively studied problems in the context of such\ndocument streams is burst identification. Given a term t, a burst is generally\nexhibited when an unusually high frequency is observed for t. While spatial and\ntemporal burstiness have been studied individually in the past, our work is the\nfirst to simultaneously track and measure spatiotemporal term burstiness. In\naddition, we use the mined burstiness information toward an efficient\ndocument-search engine: given a user's query of terms, our engine returns a\nranked list of documents discussing influential events with a strong\nspatiotemporal impact. We demonstrate the efficiency of our methods with an\nextensive experimental evaluation on real and synthetic datasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6696v1", 
    "other_authors": "Houtan Shirani-Mehr, Farnoush Banaei Kashani, Cyrus Shahabi", 
    "title": "Efficient Reachability Query Evaluation in Large Spatiotemporal Contact   Datasets", 
    "arxiv-id": "1205.6696v1", 
    "author": "Cyrus Shahabi", 
    "publish": "2012-05-30T14:33:01Z", 
    "summary": "With the advent of reliable positioning technologies and prevalence of\nlocation-based services, it is now feasible to accurately study the propagation\nof items such as infectious viruses, sensitive information pieces, and malwares\nthrough a population of moving objects, e.g., individuals, mobile devices, and\nvehicles. In such application scenarios, an item passes between two objects\nwhen the objects are sufficiently close (i.e., when they are, so-called, in\ncontact), and hence once an item is initiated, it can penetrate the object\npopulation through the evolving network of contacts among objects, termed\ncontact network. In this paper, for the first time we define and study\nreachability queries in large (i.e., disk-resident) contact datasets which\nrecord the movement of a (potentially large) set of objects moving in a spatial\nenvironment over an extended time period. A reachability query verifies whether\ntwo objects are \"reachable\" through the evolving contact network represented by\nsuch contact datasets. We propose two contact-dataset indexes that enable\nefficient evaluation of such queries despite the potentially humongous size of\nthe contact datasets. With the first index, termed ReachGrid, at the query time\nonly a small necessary portion of the contact network which is required for\nreachability evaluation is constructed and traversed. With the second approach,\ntermed ReachGraph, we precompute reachability at different scales and leverage\nthese precalculations at the query time for efficient query processing. We\noptimize the placement of both indexes on disk to enable efficient index\ntraversal during query processing. We study the pros and cons of our proposed\napproaches by performing extensive experiments with both real and synthetic\ndata. Based on our experimental results, our proposed approaches outperform\nexisting reachability query processing techniques in contact n...[truncated]."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6697v1", 
    "other_authors": "Thi Nguyen, Zhen He, Rui Zhang, Phillip Ward", 
    "title": "Boosting Moving Object Indexing through Velocity Partitioning", 
    "arxiv-id": "1205.6697v1", 
    "author": "Phillip Ward", 
    "publish": "2012-05-30T14:33:06Z", 
    "summary": "There have been intense research interests in moving object indexing in the\npast decade. However, existing work did not exploit the important property of\nskewed velocity distributions. In many real world scenarios, objects travel\npredominantly along only a few directions. Examples include vehicles on road\nnetworks, flights, people walking on the streets, etc. The search space for a\nquery is heavily dependent on the velocity distribution of the objects grouped\nin the nodes of an index tree. Motivated by this observation, we propose the\nvelocity partitioning (VP) technique, which exploits the skew in velocity\ndistribution to speed up query processing using moving object indexes. The VP\ntechnique first identifies the \"dominant velocity axes (DVAs)\" using a\ncombination of principal components analysis (PCA) and k-means clustering.\nThen, a moving object index (e.g., a TPR-tree) is created based on each DVA,\nusing the DVA as an axis of the underlying coordinate system. An object is\nmaintained in the index whose DVA is closest to the object's current moving\ndirection. Thus, all the objects in an index are moving in a near 1-dimensional\nspace instead of a 2-dimensional space. As a result, the expansion of the\nsearch space with time is greatly reduced, from a quadratic function of the\nmaximum speed (of the objects in the search range) to a near linear function of\nthe maximum speed. The VP technique can be applied to a wide range of moving\nobject index structures. We have implemented the VP technique on two\nrepresentative ones, the TPR*-tree and the Bx-tree. Extensive experiments\nvalidate that the VP technique consistently improves the performance of those\nindex structures."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6698v1", 
    "other_authors": "Nicole Bidoit-Tollu, Dario Colazzo, Federico Ulliana", 
    "title": "Type-Based Detection of XML Query-Update Independence", 
    "arxiv-id": "1205.6698v1", 
    "author": "Federico Ulliana", 
    "publish": "2012-05-30T14:33:10Z", 
    "summary": "This paper presents a novel static analysis technique to detect XML\nquery-update independence, in the presence of a schema. Rather than types, our\nsystem infers chains of types. Each chain represents a path that can be\ntraversed on a valid document during query/update evaluation. The resulting\nindependence analysis is precise, although it raises a challenging issue:\nrecursive schemas may lead to infer infinitely many chains. A sound and\ncomplete approximation technique ensuring a finite analysis in any case is\npresented, together with an efficient implementation performing the chain-based\nanalysis in polynomial space and time."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6699v1", 
    "other_authors": "Benjamin Sowell, Wojciech Golab, Mehul A. Shah", 
    "title": "Minuet: A Scalable Distributed Multiversion B-Tree", 
    "arxiv-id": "1205.6699v1", 
    "author": "Mehul A. Shah", 
    "publish": "2012-05-30T14:33:38Z", 
    "summary": "Data management systems have traditionally been designed to support either\nlong-running analytics queries or short-lived transactions, but an increasing\nnumber of applications need both. For example, online games, socio-mobile apps,\nand e-commerce sites need to not only maintain operational state, but also\nanalyze that data quickly to make predictions and recommendations that improve\nuser experience. In this paper, we present Minuet, a distributed, main-memory\nB-tree that supports both transactions and copy-on-write snapshots for in-situ\nanalytics. Minuet uses main-memory storage to enable low-latency transactional\noperations as well as analytics queries without compromising transaction\nperformance. In addition to supporting read-only analytics queries on\nsnapshots, Minuet supports writable clones, so that users can create branching\nversions of the data. This feature can be quite useful, e.g. to support complex\n\"what-if\" analysis or to facilitate wide-area replication. Our experiments show\nthat Minuet outperforms a commercial main-memory database in many ways. It\nscales to hundreds of cores and TBs of memory, and can process hundreds of\nthousands of B-tree operations per second while executing long-running scans."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4203", 
    "link": "http://arxiv.org/pdf/1205.6700v1", 
    "other_authors": "Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, Chen Chen", 
    "title": "Challenging the Long Tail Recommendation", 
    "arxiv-id": "1205.6700v1", 
    "author": "Chen Chen", 
    "publish": "2012-05-30T14:33:56Z", 
    "summary": "The success of \"infinite-inventory\" retailers such as Amazon.com and Netflix\nhas been largely attributed to a \"long tail\" phenomenon. Although the majority\nof their inventory is not in high demand, these niche products, unavailable at\nlimited-inventory competitors, generate a significant fraction of total revenue\nin aggregate. In addition, tail product availability can boost head sales by\noffering consumers the convenience of \"one-stop shopping\" for both their\nmainstream and niche tastes. However, most of existing recommender systems,\nespecially collaborative filter based methods, can not recommend tail products\ndue to the data sparsity issue. It has been widely acknowledged that to\nrecommend popular products is easier yet more trivial while to recommend long\ntail products adds more novelty yet it is also a more challenging task. In this\npaper, we propose a novel suite of graph-based algorithms for the long tail\nrecommendation. We first represent user-item information with undirected\nedge-weighted graph and investigate the theoretical foundation of applying\nHitting Time algorithm for long tail item recommendation. To improve\nrecommendation diversity and accuracy, we extend Hitting Time and propose\nefficient Absorbing Time algorithm to help users find their favorite long tail\nitems. Finally, we refine the Absorbing Time algorithm and propose two\nentropy-biased Absorbing Cost algorithms to distinguish the variation on\ndifferent user-item rating pairs, which further enhances the effectiveness of\nlong tail recommendation. Empirical experiments on two real life datasets show\nthat our proposed algorithms are effective to recommend long tail items and\noutperform state-of-the-art recommendation techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1206.0021v1", 
    "other_authors": "Casey C. Bennett", 
    "title": "Clinical Productivity System - A Decision Support Model", 
    "arxiv-id": "1206.0021v1", 
    "author": "Casey C. Bennett", 
    "publish": "2012-05-31T20:15:35Z", 
    "summary": "Purpose: This goal of this study was to evaluate the effects of a data-driven\nclinical productivity system that leverages Electronic Health Record (EHR) data\nto provide productivity decision support functionality in a real-world clinical\nsetting. The system was implemented for a large behavioral health care provider\nseeing over 75,000 distinct clients a year. Design/methodology/approach: The\nkey metric in this system is a \"VPU\", which simultaneously optimizes multiple\naspects of clinical care. The resulting mathematical value of clinical\nproductivity was hypothesized to tightly link the organization's performance to\nits expectations and, through transparency and decision support tools at the\nclinician level, affect significant changes in productivity, quality, and\nconsistency relative to traditional models of clinical productivity. Findings:\nIn only 3 months, every single variable integrated into the VPU system showed\nsignificant improvement, including a 30% rise in revenue, 10% rise in clinical\npercentage, a 25% rise in treatment plan completion, a 20% rise in case rate\neligibility, along with similar improvements in compliance/audit issues,\noutcomes collection, access, etc. Practical implications: A data-driven\nclinical productivity system employing decision support functionality is\neffective because of the impact on clinician behavior relative to traditional\nclinical productivity systems. Critically, the model is also extensible to\nintegration with outcomes-based productivity. Originality/Value: EHR's are only\na first step - the problem is turning that data into useful information.\nTechnology can leverage the data in order to produce actionable information\nthat can inform clinical practice and decision-making. Without additional\ntechnology, EHR's are essentially just copies of paper-based records stored in\nelectronic form."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1206.0217v1", 
    "other_authors": "Mohamed A. El-Zawawy", 
    "title": "Efficient techniques for mining spatial databases", 
    "arxiv-id": "1206.0217v1", 
    "author": "Mohamed A. El-Zawawy", 
    "publish": "2012-06-01T15:06:58Z", 
    "summary": "Clustering is one of the major tasks in data mining. In the last few years,\nClustering of spatial data has received a lot of research attention. Spatial\ndatabases are components of many advanced information systems like geographic\ninformation systems VLSI design systems. In this thesis, we introduce several\nefficient algorithms for clustering spatial data. First, we present a\ngrid-based clustering algorithm that has several advantages and comparable\nperformance to the well known efficient clustering algorithm. The algorithm has\nseveral advantages. The algorithm does not require many input parameters. It\nrequires only three parameters, the number of the points in the data space, the\nnumber of the cells in the grid and a percentage. The number of the cells in\nthe grid reflects the accuracy that should be achieved by the algorithm. The\nalgorithm is capable of discovering clusters of arbitrary shapes. The\ncomputational complexity of the algorithm is comparable to the complexity of\nthe most efficient clustering algorithm. The algorithm has been implemented and\ntested against different ranges of database sizes. The performance results show\nthat the running time of the algorithm is superior to the most well known\nalgorithms (CLARANS [23]). The results show also that the performance of the\nalgorithm do not degrade as the number of the data points increases."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1206.1032v1", 
    "other_authors": "Manel Zarrouk, Med Salah Gouider", 
    "title": "Frequent Patterns mining in time-sensitive Data Stream", 
    "arxiv-id": "1206.1032v1", 
    "author": "Med Salah Gouider", 
    "publish": "2012-06-05T19:26:24Z", 
    "summary": "Mining frequent itemsets through static Databases has been extensively\nstudied and used and is always considered a highly challenging task. For this\nreason it is interesting to extend it to data streams field. In the streaming\ncase, the frequent patterns' mining has much more information to track and much\ngreater complexity to manage. Infrequent items can become frequent later on and\nhence cannot be ignored. The output structure needs to be dynamically\nincremented to reflect the evolution of itemset frequencies over time. In this\npaper, we study this problem and specifically the methodology of mining\ntime-sensitive data streams. We tried to improve an existing algorithm by\nincreasing the temporal accuracy and discarding the out-of-date data by adding\na new concept called the \"Shaking Point\". We presented as well some experiments\nillustrating the time and space required."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1206.6646v1", 
    "other_authors": "Arnab Bhattacharya, B. Palvali Teja", 
    "title": "Aggregate Skyline Join Queries: Skylines with Aggregate Operations over   Multiple Relations", 
    "arxiv-id": "1206.6646v1", 
    "author": "B. Palvali Teja", 
    "publish": "2012-06-28T12:06:51Z", 
    "summary": "The multi-criteria decision making, which is possible with the advent of\nskyline queries, has been applied in many areas. Though most of the existing\nresearch is concerned with only a single relation, several real world\napplications require finding the skyline set of records over multiple\nrelations. Consequently, the join operation over skylines where the preferences\nare local to each relation, has been proposed. In many of those cases, however,\nthe join often involves performing aggregate operations among some of the\nattributes from the different relations. In this paper, we introduce such\nqueries as \"aggregate skyline join queries\". Since the naive algorithm is\nimpractical, we propose three algorithms to efficiently process such queries.\nThe algorithms utilize certain properties of skyline sets, and processes the\nskylines as much as possible locally before computing the join. Experiments\nwith real and synthetic datasets exhibit the practicality and scalability of\nthe algorithms with respect to the cardinality and dimensionality of the\nrelations."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0073v1", 
    "other_authors": "Dong-Wan Choi, Chin-Wan Chung, Yufei Tao", 
    "title": "A Scalable Algorithm for Maximizing Range Sum in Spatial Databases", 
    "arxiv-id": "1208.0073v1", 
    "author": "Yufei Tao", 
    "publish": "2012-08-01T03:42:19Z", 
    "summary": "This paper investigates the MaxRS problem in spatial databases. Given a set O\nof weighted points and a rectangular region r of a given size, the goal of the\nMaxRS problem is to find a location of r such that the sum of the weights of\nall the points covered by r is maximized. This problem is useful in many\nlocation-based applications such as finding the best place for a new franchise\nstore with a limited delivery range and finding the most attractive place for a\ntourist with a limited reachable range. However, the problem has been studied\nmainly in theory, particularly, in computational geometry. The existing\nalgorithms from the computational geometry community are in-memory algorithms\nwhich do not guarantee the scalability. In this paper, we propose a scalable\nexternal-memory algorithm (ExactMaxRS) for the MaxRS problem, which is optimal\nin terms of the I/O complexity. Furthermore, we propose an approximation\nalgorithm (ApproxMaxCRS) for the MaxCRS problem that is a circle version of the\nMaxRS problem. We prove the correctness and optimality of the ExactMaxRS\nalgorithm along with the approximation bound of the ApproxMaxCRS algorithm.\nFrom extensive experimental results, we show that the ExactMaxRS algorithm is\ntwo orders of magnitude faster than methods adapted from existing algorithms,\nand the approximation bound in practice is much better than the theoretical\nbound of the ApproxMaxCRS algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0074v1", 
    "other_authors": "Ahmed M. Aly, Walid G. Aref, Mourad Ouzzani", 
    "title": "Spatial Queries with Two kNN Predicates", 
    "arxiv-id": "1208.0074v1", 
    "author": "Mourad Ouzzani", 
    "publish": "2012-08-01T03:43:08Z", 
    "summary": "The widespread use of location-aware devices has led to countless\nlocation-based services in which a user query can be arbitrarily complex, i.e.,\none that embeds multiple spatial selection and join predicates. Amongst these\npredicates, the k-Nearest-Neighbor (kNN) predicate stands as one of the most\nimportant and widely used predicates. Unlike related research, this paper goes\nbeyond the optimization of queries with single kNN predicates, and shows how\nqueries with two kNN predicates can be optimized. In particular, the paper\naddresses the optimization of queries with: (i) two kNN-select predicates, (ii)\ntwo kNN-join predicates, and (iii) one kNN-join predicate and one kNN-select\npredicate. For each type of queries, conceptually correct query evaluation\nplans (QEPs) and new algorithms that optimize the query execution time are\npresented. Experimental results demonstrate that the proposed algorithms\noutperform the conceptually correct QEPs by orders of magnitude."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0075v1", 
    "other_authors": "Cheng Sheng, Nan Zhang, Yufei Tao, Xin Jin", 
    "title": "Optimal Algorithms for Crawling a Hidden Database in the Web", 
    "arxiv-id": "1208.0075v1", 
    "author": "Xin Jin", 
    "publish": "2012-08-01T03:43:52Z", 
    "summary": "A hidden database refers to a dataset that an organization makes accessible\non the web by allowing users to issue queries through a search interface. In\nother words, data acquisition from such a source is not by following static\nhyper-links. Instead, data are obtained by querying the interface, and reading\nthe result page dynamically generated. This, with other facts such as the\ninterface may answer a query only partially, has prevented hidden databases\nfrom being crawled effectively by existing search engines. This paper remedies\nthe problem by giving algorithms to extract all the tuples from a hidden\ndatabase. Our algorithms are provably efficient, namely, they accomplish the\ntask by performing only a small number of queries, even in the worst case. We\nalso establish theoretical results indicating that these algorithms are\nasymptotically optimal -- i.e., it is impossible to improve their efficiency by\nmore than a constant factor. The derivation of our upper and lower bound\nresults reveals significant insight into the characteristics of the underlying\nproblem. Extensive experiments confirm the proposed techniques work very well\non all the real datasets examined."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0076v1", 
    "other_authors": "Lu Qin, Jeffrey Xu Yu, Lijun Chang", 
    "title": "Diversifying Top-K Results", 
    "arxiv-id": "1208.0076v1", 
    "author": "Lijun Chang", 
    "publish": "2012-08-01T03:44:46Z", 
    "summary": "Top-k query processing finds a list of k results that have largest scores\nw.r.t the user given query, with the assumption that all the k results are\nindependent to each other. In practice, some of the top-k results returned can\nbe very similar to each other. As a result some of the top-k results returned\nare redundant. In the literature, diversified top-k search has been studied to\nreturn k results that take both score and diversity into consideration. Most\nexisting solutions on diversified top-k search assume that scores of all the\nsearch results are given, and some works solve the diversity problem on a\nspecific problem and can hardly be extended to general cases. In this paper, we\nstudy the diversified top-k search problem. We define a general diversified\ntop-k search problem that only considers the similarity of the search results\nthemselves. We propose a framework, such that most existing solutions for top-k\nquery processing can be extended easily to handle diversified top-k search, by\nsimply applying three new functions, a sufficient stop condition sufficient(),\na necessary stop condition necessary(), and an algorithm for diversified top-k\nsearch on the current set of generated results, div-search-current(). We\npropose three new algorithms, namely, div-astar, div-dp, and div-cut to solve\nthe div-search-current() problem. div-astar is an A* based algorithm, div-dp is\nan algorithm that decomposes the results into components which are searched\nusing div-astar independently and combined using dynamic programming. div-cut\nfurther decomposes the current set of generated results using cut points and\ncombines the results using sophisticated operations. We conducted extensive\nperformance studies using two real datasets, enwiki and reuters. Our div-cut\nalgorithm finds the optimal solution for diversified top-k search problem in\nseconds even for k as large as 2,000."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0077v1", 
    "other_authors": "Xin Cao, Lisi Chen, Gao Cong, Xiaokui Xiao", 
    "title": "Keyword-aware Optimal Route Search", 
    "arxiv-id": "1208.0077v1", 
    "author": "Xiaokui Xiao", 
    "publish": "2012-08-01T03:45:38Z", 
    "summary": "Identifying a preferable route is an important problem that finds\napplications in map services. When a user plans a trip within a city, the user\nmay want to find \"a most popular route such that it passes by shopping mall,\nrestaurant, and pub, and the travel time to and from his hotel is within 4\nhours.\" However, none of the algorithms in the existing work on route planning\ncan be used to answer such queries. Motivated by this, we define the problem of\nkeyword-aware optimal route query, denoted by KOR, which is to find an optimal\nroute such that it covers a set of user-specified keywords, a specified budget\nconstraint is satisfied, and an objective score of the route is optimal. The\nproblem of answering KOR queries is NP-hard. We devise an approximation\nalgorithm OSScaling with provable approximation bounds. Based on this\nalgorithm, another more efficient approximation algorithm BucketBound is\nproposed. We also design a greedy approximation algorithm. Results of empirical\nstudies show that all the proposed algorithms are capable of answering KOR\nqueries efficiently, while the BucketBound and Greedy algorithms run faster.\nThe empirical studies also offer insight into the accuracy of the proposed\nalgorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0078v1", 
    "other_authors": "Bogdan Cautis, Evgeny Kharlamov", 
    "title": "Answering Queries using Views over Probabilistic XML: Complexity and   Tractability", 
    "arxiv-id": "1208.0078v1", 
    "author": "Evgeny Kharlamov", 
    "publish": "2012-08-01T03:46:21Z", 
    "summary": "We study the complexity of query answering using views in a probabilistic XML\nsetting, identifying large classes of XPath queries -- with child and\ndescendant navigation and predicates -- for which there are efficient (PTime)\nalgorithms. We consider this problem under the two possible semantics for XML\nquery results: with persistent node identifiers and in their absence.\nAccordingly, we consider rewritings that can exploit a single view, by means of\ncompensation, and rewritings that can use multiple views, by means of\nintersection. Since in a probabilistic setting queries return answers with\nprobabilities, the problem of rewriting goes beyond the classic one of\nretrieving XML answers from views. For both semantics of XML queries, we show\nthat, even when XML answers can be retrieved from views, their probabilities\nmay not be computable. For rewritings that use only compensation, we describe a\nPTime decision procedure, based on easily verifiable criteria that distinguish\nbetween the feasible cases -- when probabilistic XML results are computable --\nand the unfeasible ones. For rewritings that can use multiple views, with\ncompensation and intersection, we identify the most permissive conditions that\nmake probabilistic rewriting feasible, and we describe an algorithm that is\nsound in general, and becomes complete under fairly permissive restrictions,\nrunning in PTime modulo worst-case exponential time equivalence tests. This is\nthe best we can hope for since intersection makes query equivalence intractable\nalready over deterministic data. Our algorithm runs in PTime whenever\ndeterministic rewritings can be found in PTime."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0079v1", 
    "other_authors": "Abhay Jha, Dan Suciu", 
    "title": "Probabilistic Databases with MarkoViews", 
    "arxiv-id": "1208.0079v1", 
    "author": "Dan Suciu", 
    "publish": "2012-08-01T03:47:10Z", 
    "summary": "Most of the work on query evaluation in probabilistic databases has focused\non the simple tuple-independent data model, where tuples are independent random\nevents. Several efficient query evaluation techniques exists in this setting,\nsuch as safe plans, algorithms based on OBDDs, tree-decomposition and a variety\nof approximation algorithms. However, complex data analytics tasks often\nrequire complex correlations, and query evaluation then is significantly more\nexpensive, or more restrictive. In this paper, we propose MVDB as a framework\nboth for representing complex correlations and for efficient query evaluation.\nAn MVDB specifies correlations by views, called MarkoViews, on the\nprobabilistic relations and declaring the weights of the view's outputs. An\nMVDB is a (very large) Markov Logic Network. We make two sets of contributions.\nFirst, we show that query evaluation on an MVDB is equivalent to evaluating a\nUnion of Conjunctive Query(UCQ) over a tuple-independent database. The\ntranslation is exact (thus allowing the techniques developed for tuple\nindependent databases to be carried over to MVDB), yet it is novel and quite\nnon-obvious (some resulting probabilities may be negative!). This translation\nin itself though may not lead to much gain since the translated query gets\ncomplicated as we try to capture more correlations. Our second contribution is\nto propose a new query evaluation strategy that exploits offline compilation to\nspeed up online query evaluation. Here we utilize and extend our prior work on\ncompilation of UCQ. We validate experimentally our techniques on a large\nprobabilistic database with MarkoViews inferred from the DBLP data."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0080v1", 
    "other_authors": "Konstantinos Mamouras, Sigal Oren, Lior Seeman, Lucja Kot, Johannes Gehrke", 
    "title": "The Complexity of Social Coordination", 
    "arxiv-id": "1208.0080v1", 
    "author": "Johannes Gehrke", 
    "publish": "2012-08-01T03:47:57Z", 
    "summary": "Coordination is a challenging everyday task; just think of the last time you\norganized a party or a meeting involving several people. As a growing part of\nour social and professional life goes online, an opportunity for an improved\ncoordination process arises. Recently, Gupta et al. proposed entangled queries\nas a declarative abstraction for data-driven coordination, where the difficulty\nof the coordination task is shifted from the user to the database.\nUnfortunately, evaluating entangled queries is very hard, and thus previous\nwork considered only a restricted class of queries that satisfy safety (the\ncoordination partners are fixed) and uniqueness (all queries need to be\nsatisfied). In this paper we significantly extend the class of feasible\nentangled queries beyond uniqueness and safety. First, we show that we can\nsimply drop uniqueness and still efficiently evaluate a set of safe entangled\nqueries. Second, we show that as long as all users coordinate on the same set\nof attributes, we can give an efficient algorithm for coordination even if the\nset of queries does not satisfy safety. In an experimental evaluation we show\nthat our algorithms are feasible for a wide spectrum of coordination scenarios."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0081v1", 
    "other_authors": "Xiaofei Zhang, Lei Chen, Min Wang", 
    "title": "Efficient Multi-way Theta-Join Processing Using MapReduce", 
    "arxiv-id": "1208.0081v1", 
    "author": "Min Wang", 
    "publish": "2012-08-01T03:48:44Z", 
    "summary": "Multi-way Theta-join queries are powerful in describing complex relations and\ntherefore widely employed in real practices. However, existing solutions from\ntraditional distributed and parallel databases for multi-way Theta-join queries\ncannot be easily extended to fit a shared-nothing distributed computing\nparadigm, which is proven to be able to support OLAP applications over immense\ndata volumes. In this work, we study the problem of efficient processing of\nmulti-way Theta-join queries using MapReduce from a cost-effective perspective.\nAlthough there have been some works using the (key,value) pair-based\nprogramming model to support join operations, efficient processing of multi-way\nTheta-join queries has never been fully explored. The substantial challenge\nlies in, given a number of processing units (that can run Map or Reduce tasks),\nmapping a multi-way Theta-join query to a number of MapReduce jobs and having\nthem executed in a well scheduled sequence, such that the total processing time\nspan is minimized. Our solution mainly includes two parts: 1) cost metrics for\nboth single MapReduce job and a number of MapReduce jobs executed in a certain\norder; 2) the efficient execution of a chain-typed Theta-join with only one\nMapReduce job. Comparing with the query evaluation strategy proposed in [23]\nand the widely adopted Pig Latin and Hive SQL solutions, our method achieves\nsignificant improvement of the join processing efficiency."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0082v1", 
    "other_authors": "Harold Lim, Herodotos Herodotou, Shivnath Babu", 
    "title": "Stubby: A Transformation-based Optimizer for MapReduce Workflows", 
    "arxiv-id": "1208.0082v1", 
    "author": "Shivnath Babu", 
    "publish": "2012-08-01T03:49:32Z", 
    "summary": "There is a growing trend of performing analysis on large datasets using\nworkflows composed of MapReduce jobs connected through producer-consumer\nrelationships based on data. This trend has spurred the development of a number\nof interfaces--ranging from program-based to query-based interfaces--for\ngenerating MapReduce workflows. Studies have shown that the gap in performance\ncan be quite large between optimized and unoptimized workflows. However,\nautomatic cost-based optimization of MapReduce workflows remains a challenge\ndue to the multitude of interfaces, large size of the execution plan space, and\nthe frequent unavailability of all types of information needed for\noptimization. We introduce a comprehensive plan space for MapReduce workflows\ngenerated by popular workflow generators. We then propose Stubby, a cost-based\noptimizer that searches selectively through the subspace of the full plan space\nthat can be enumerated correctly and costed based on the information available\nin any given setting. Stubby enumerates the plan space based on plan-to-plan\ntransformations and an efficient search algorithm. Stubby is designed to be\nextensible to new interfaces and new types of optimizations, which is a\ndesirable feature given how rapidly MapReduce systems are evolving. Stubby's\nefficiency and effectiveness have been evaluated using representative workflows\nfrom many domains."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0083v1", 
    "other_authors": "Zhuowei Bao, Susan B. Davidson, Tova Milo", 
    "title": "Labeling Workflow Views with Fine-Grained Dependencies", 
    "arxiv-id": "1208.0083v1", 
    "author": "Tova Milo", 
    "publish": "2012-08-01T03:50:17Z", 
    "summary": "This paper considers the problem of efficiently answering reachability\nqueries over views of provenance graphs, derived from executions of workflows\nthat may include recursion. Such views include composite modules and model\nfine-grained dependencies between module inputs and outputs. A novel\nview-adaptive dynamic labeling scheme is developed for efficient query\nevaluation, in which view specifications are labeled statically (i.e. as they\nare created) and data items are labeled dynamically as they are produced during\na workflow execution. Although the combination of fine-grained dependencies and\nrecursive workflows entail, in general, long (linear-size) data labels, we show\nthat for a large natural class of workflows and views, labels are compact\n(logarithmic-size) and reachability queries can be evaluated in constant time.\nExperimental results demonstrate the benefit of this approach over the\nstate-of-the-art technique when applied for labeling multiple views."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0084v1", 
    "other_authors": "Jaroslaw Szlichta, Parke Godfrey, Jarek Gryz", 
    "title": "Fundamentals of Order Dependencies", 
    "arxiv-id": "1208.0084v1", 
    "author": "Jarek Gryz", 
    "publish": "2012-08-01T03:51:05Z", 
    "summary": "Dependencies have played a significant role in database design for many\nyears. They have also been shown to be useful in query optimization. In this\npaper, we discuss dependencies between lexicographically ordered sets of\ntuples. We introduce formally the concept of order dependency and present a set\nof axioms (inference rules) for them. We show how query rewrites based on these\naxioms can be used for query optimization. We present several interesting\ntheorems that can be derived using the inference rules. We prove that\nfunctional dependencies are subsumed by order dependencies and that our set of\naxioms for order dependencies is sound and complete."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0086v1", 
    "other_authors": "Yu Cao, Chee-Yong Chan, Jie Li, Kian-Lee Tan", 
    "title": "Optimization of Analytic Window Functions", 
    "arxiv-id": "1208.0086v1", 
    "author": "Kian-Lee Tan", 
    "publish": "2012-08-01T03:52:40Z", 
    "summary": "Analytic functions represent the state-of-the-art way of performing complex\ndata analysis within a single SQL statement. In particular, an important class\nof analytic functions that has been frequently used in commercial systems to\nsupport OLAP and decision support applications is the class of window\nfunctions. A window function returns for each input tuple a value derived from\napplying a function over a window of neighboring tuples. However, existing\nwindow function evaluation approaches are based on a naive sorting scheme. In\nthis paper, we study the problem of optimizing the evaluation of window\nfunctions. We propose several efficient techniques, and identify optimization\nopportunities that allow us to optimize the evaluation of a set of window\nfunctions. We have integrated our scheme into PostgreSQL. Our comprehensive\nexperimental study on the TPC-DS datasets as well as synthetic datasets and\nqueries demonstrate significant speedup over existing approaches."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0087v1", 
    "other_authors": "Fabian Hueske, Mathias Peters, Matthias Sax, Astrid Rheinl\u00e4nder, Rico Bergmann, Aljoscha Krettek, Kostas Tzoumas", 
    "title": "Opening the Black Boxes in Data Flow Optimization", 
    "arxiv-id": "1208.0087v1", 
    "author": "Kostas Tzoumas", 
    "publish": "2012-08-01T03:53:27Z", 
    "summary": "Many systems for big data analytics employ a data flow abstraction to define\nparallel data processing tasks. In this setting, custom operations expressed as\nuser-defined functions are very common. We address the problem of performing\ndata flow optimization at this level of abstraction, where the semantics of\noperators are not known. Traditionally, query optimization is applied to\nqueries with known algebraic semantics. In this work, we find that a handful of\nproperties, rather than a full algebraic specification, suffice to establish\nreordering conditions for data processing operators. We show that these\nproperties can be accurately estimated for black box operators by statically\nanalyzing the general-purpose code of their user-defined functions. We design\nand implement an optimizer for parallel data flows that does not assume\nknowledge of semantics or algebraic properties of operators. Our evaluation\nconfirms that the optimizer can apply common rewritings such as selection\nreordering, bushy join-order enumeration, and limited forms of aggregation\npush-down, hence yielding similar rewriting power as modern relational DBMS\noptimizers. Moreover, it can optimize the operator order of non-relational data\nflows, a unique feature among today's systems."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0088v1", 
    "other_authors": "Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, Volker Markl", 
    "title": "Spinning Fast Iterative Data Flows", 
    "arxiv-id": "1208.0088v1", 
    "author": "Volker Markl", 
    "publish": "2012-08-01T03:54:11Z", 
    "summary": "Parallel dataflow systems are a central part of most analytic pipelines for\nbig data. The iterative nature of many analysis and machine learning\nalgorithms, however, is still a challenge for current systems. While certain\ntypes of bulk iterative algorithms are supported by novel dataflow frameworks,\nthese systems cannot exploit computational dependencies present in many\nalgorithms, such as graph algorithms. As a result, these algorithms are\ninefficiently executed and have led to specialized systems based on other\nparadigms, such as message passing or shared memory. We propose a method to\nintegrate incremental iterations, a form of workset iterations, with parallel\ndataflows. After showing how to integrate bulk iterations into a dataflow\nsystem and its optimizer, we present an extension to the programming model for\nincremental iterations. The extension alleviates for the lack of mutable state\nin dataflows and allows for exploiting the sparse computational dependencies\ninherent in many iterative algorithms. The evaluation of a prototypical\nimplementation shows that those aspects lead to up to two orders of magnitude\nspeedup in algorithm runtime, when exploited. In our experiments, the improved\ndataflow system is highly competitive with specialized systems while\nmaintaining a transparent and unified dataflow abstraction."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0089v1", 
    "other_authors": "Svilen R. Mihaylov, Zachary G. Ives, Sudipto Guha", 
    "title": "REX: Recursive, Delta-Based Data-Centric Computation", 
    "arxiv-id": "1208.0089v1", 
    "author": "Sudipto Guha", 
    "publish": "2012-08-01T03:54:58Z", 
    "summary": "In today's Web and social network environments, query workloads include ad\nhoc and OLAP queries, as well as iterative algorithms that analyze data\nrelationships (e.g., link analysis, clustering, learning). Modern DBMSs support\nad hoc and OLAP queries, but most are not robust enough to scale to large\nclusters. Conversely, \"cloud\" platforms like MapReduce execute chains of batch\ntasks across clusters in a fault tolerant way, but have too much overhead to\nsupport ad hoc queries.\n  Moreover, both classes of platform incur significant overhead in executing\niterative data analysis algorithms. Most such iterative algorithms repeatedly\nrefine portions of their answers, until some convergence criterion is reached.\nHowever, general cloud platforms typically must reprocess all data in each\nstep. DBMSs that support recursive SQL are more efficient in that they\npropagate only the changes in each step -- but they still accumulate each\niteration's state, even if it is no longer useful. User-defined functions are\nalso typically harder to write for DBMSs than for cloud platforms.\n  We seek to unify the strengths of both styles of platforms, with a focus on\nsupporting iterative computations in which changes, in the form of deltas, are\npropagated from iteration to iteration, and state is efficiently updated in an\nextensible way. We present a programming model oriented around deltas, describe\nhow we execute and optimize such programs in our REX runtime system, and\nvalidate that our platform also handles failures gracefully. We experimentally\nvalidate our techniques, and show speedups over the competing methods ranging\nfrom 2.5 to nearly 100 times."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0090v1", 
    "other_authors": "James Cheng, Zechao Shang, Hong Cheng, Haixun Wang, Jeffrey Xu Yu", 
    "title": "K-Reach: Who is in Your Small World", 
    "arxiv-id": "1208.0090v1", 
    "author": "Jeffrey Xu Yu", 
    "publish": "2012-08-01T03:55:46Z", 
    "summary": "We study the problem of answering k-hop reachability queries in a directed\ngraph, i.e., whether there exists a directed path of length k, from a source\nquery vertex to a target query vertex in the input graph. The problem of k-hop\nreachability is a general problem of the classic reachability (where\nk=infinity). Existing indexes for processing classic reachability queries, as\nwell as for processing shortest path queries, are not applicable or not\nefficient for processing k-hop reachability queries. We propose an index for\nprocessing k-hop reachability queries, which is simple in design and efficient\nto construct. Our experimental results on a wide range of real datasets show\nthat our index is more efficient than the state-of-the-art indexes even for\nprocessing classic reachability queries, for which these indexes are primarily\ndesigned. We also show that our index is efficient in answering k-hop\nreachability queries."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0091v1", 
    "other_authors": "Wenfei Fan, Xin Wang, Yinghui Wu", 
    "title": "Performance Guarantees for Distributed Reachability Queries", 
    "arxiv-id": "1208.0091v1", 
    "author": "Yinghui Wu", 
    "publish": "2012-08-01T03:56:31Z", 
    "summary": "In the real world a graph is often fragmented and distributed across\ndifferent sites. This highlights the need for evaluating queries on distributed\ngraphs. This paper proposes distributed evaluation algorithms for three classes\nof queries: reachability for determining whether one node can reach another,\nbounded reachability for deciding whether there exists a path of a bounded\nlength between a pair of nodes, and regular reachability for checking whether\nthere exists a path connecting two nodes such that the node labels on the path\nform a string in a given regular expression. We develop these algorithms based\non partial evaluation, to explore parallel computation. When evaluating a query\nQ on a distributed graph G, we show that these algorithms possess the following\nperformance guarantees, no matter how G is fragmented and distributed: (1) each\nsite is visited only once; (2) the total network traffic is determined by the\nsize of Q and the fragmentation of G, independent of the size of G; and (3) the\nresponse time is decided by the largest fragment of G rather than the entire G.\nIn addition, we show that these algorithms can be readily implemented in the\nMapReduce framework. Using synthetic and real-life data, we experimentally\nverify that these algorithms are scalable on large graphs, regardless of how\nthe graphs are distributed."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0092v1", 
    "other_authors": "Pirooz Chubak, Davood Rafiei", 
    "title": "Efficient Indexing and Querying over Syntactically Annotated Trees", 
    "arxiv-id": "1208.0092v1", 
    "author": "Davood Rafiei", 
    "publish": "2012-08-01T03:57:16Z", 
    "summary": "Natural language text corpora are often available as sets of syntactically\nparsed trees. A wide range of expressive tree queries are possible over such\nparsed trees that open a new avenue in searching over natural language text.\nThey not only allow for querying roles and relationships within sentences, but\nalso improve search effectiveness compared to flat keyword queries. One major\ndrawback of current systems supporting querying over parsed text is the\nperformance of evaluating queries over large data. In this paper we propose a\nnovel indexing scheme over unique subtrees as index keys. We also propose a\nnovel root-split coding scheme that stores subtree structural information only\npartially, thus reducing index size and improving querying performance. Our\nextensive set of experiments show that root-split coding reduces the index size\nof any interval coding which stores individual node numbers by a factor of 50%\nto 80%, depending on the sizes of subtrees indexed. Moreover, We show that our\nindex using root-split coding, outperforms previous approaches by at least an\norder of magnitude in terms of the response time of queries."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0093v1", 
    "other_authors": "Ninghui Li, Wahbeh Qardaji, Dong Su, Jianneng Cao", 
    "title": "PrivBasis: Frequent Itemset Mining with Differential Privacy", 
    "arxiv-id": "1208.0093v1", 
    "author": "Jianneng Cao", 
    "publish": "2012-08-01T03:58:50Z", 
    "summary": "The discovery of frequent itemsets can serve valuable economic and research\npurposes. Releasing discovered frequent itemsets, however, presents privacy\nchallenges. In this paper, we study the problem of how to perform frequent\nitemset mining on transaction databases while satisfying differential privacy.\nWe propose an approach, called PrivBasis, which leverages a novel notion called\nbasis sets. A theta-basis set has the property that any itemset with frequency\nhigher than theta is a subset of some basis. We introduce algorithms for\nprivately constructing a basis set and then using it to find the most frequent\nitemsets. Experiments show that our approach greatly outperforms the current\nstate of the art."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0094v1", 
    "other_authors": "Ganzhao Yuan, Zhenjie Zhang, Marianne Winslett, Xiaokui Xiao, Yin Yang, Zhifeng Hao", 
    "title": "Low-Rank Mechanism: Optimizing Batch Queries under Differential Privacy", 
    "arxiv-id": "1208.0094v1", 
    "author": "Zhifeng Hao", 
    "publish": "2012-08-01T03:59:34Z", 
    "summary": "Differential privacy is a promising privacy-preserving paradigm for\nstatistical query processing over sensitive data. It works by injecting random\nnoise into each query result, such that it is provably hard for the adversary\nto infer the presence or absence of any individual record from the published\nnoisy results. The main objective in differentially private query processing is\nto maximize the accuracy of the query results, while satisfying the privacy\nguarantees. Previous work, notably the matrix mechanism, has suggested that\nprocessing a batch of correlated queries as a whole can potentially achieve\nconsiderable accuracy gains, compared to answering them individually. However,\nas we point out in this paper, the matrix mechanism is mainly of theoretical\ninterest; in particular, several inherent problems in its design limit its\naccuracy in practice, which almost never exceeds that of naive methods. In\nfact, we are not aware of any existing solution that can effectively optimize a\nquery batch under differential privacy. Motivated by this, we propose the\nLow-Rank Mechanism (LRM), the first practical differentially private technique\nfor answering batch queries with high accuracy, based on a low rank\napproximation of the workload matrix. We prove that the accuracy provided by\nLRM is close to the theoretical lower bound for any mechanism to answer a batch\nof queries under differential privacy. Extensive experiments using real data\ndemonstrate that LRM consistently outperforms state-of-the-art query processing\nsolutions under differential privacy, by large margins."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0163v1", 
    "other_authors": "Saida Aissi, Mohamed Salah Gouider", 
    "title": "Spatial and Spatio-Temporal Multidimensional Data Modelling: A Survey", 
    "arxiv-id": "1208.0163v1", 
    "author": "Mohamed Salah Gouider", 
    "publish": "2012-08-01T10:21:55Z", 
    "summary": "Data warehouse store and provide access to large volume of historical data\nsupporting the strategic decisions of organisations. Data warehouse is based on\na multidimensional model which allow to express user's needs for supporting the\ndecision making process. Since it is estimated that 80% of data used for\ndecision making has a spatial or location component [1, 2], spatial data have\nbeen widely integrated in Data Warehouses and in OLAP systems. Extending a\nmultidimensional data model by the inclusion of spatial data provides a concise\nand organised spatial datawarehouse representation. This paper aims to provide\na comprehensive review of litterature on developed and suggested spatial and\nspatio-temporel multidimensional models. A benchmarking study of the proposed\nmodels is presented. Several evaluation criterias are used to identify the\nexistence of trends as well as potential needs for further investigations."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0203v1", 
    "other_authors": "Saida Aissi, Mohamed Salah Gouider", 
    "title": "Towards the Next Generation of Data Warehouse Personalization System: A   Survey and a Comparative Study", 
    "arxiv-id": "1208.0203v1", 
    "author": "Mohamed Salah Gouider", 
    "publish": "2012-08-01T13:27:20Z", 
    "summary": "Multidimensional databases are a great asset for decision making. Their users\nexpress complex OLAP (On-Line Analytical Processing) queries, often returning\nhuge volumes of facts, sometimes providing little or no information.\nFurthermore, due to the huge volume of historical data stored in DWs, the OLAP\napplications may return a big amount of irrelevant information that could make\nthe data exploration process not efficient and tardy. OLAP personalization\nsystems play a major role in reducing the effort of decision-makers to find the\nmost interesting information. Several works dealing with OLAP personalization\nwere presented in the last few years. This paper aims to provide a\ncomprehensive review of literature on OLAP personalization approaches. A\nbenchmarking study of OLAP personalization methods is proposed. Several\nevaluation criteria are used to identify the existence of trends as well as\npotential needs for further investigations."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0219v1", 
    "other_authors": "Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, Marianne Winslett", 
    "title": "Functional Mechanism: Regression Analysis under Differential Privacy", 
    "arxiv-id": "1208.0219v1", 
    "author": "Marianne Winslett", 
    "publish": "2012-08-01T14:11:04Z", 
    "summary": "\\epsilon-differential privacy is the state-of-the-art model for releasing\nsensitive information while protecting privacy. Numerous methods have been\nproposed to enforce epsilon-differential privacy in various analytical tasks,\ne.g., regression analysis. Existing solutions for regression analysis, however,\nare either limited to non-standard types of regression or unable to produce\naccurate regression results. Motivated by this, we propose the Functional\nMechanism, a differentially private method designed for a large class of\noptimization-based analyses. The main idea is to enforce epsilon-differential\nprivacy by perturbing the objective function of the optimization problem,\nrather than its results. As case studies, we apply the functional mechanism to\naddress two most widely used regression models, namely, linear regression and\nlogistic regression. Both theoretical analysis and thorough experimental\nevaluations show that the functional mechanism is highly effective and\nefficient, and it significantly outperforms existing solutions."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0220v1", 
    "other_authors": "Jianneng Cao, Panagiotis Karras", 
    "title": "Publishing Microdata with a Robust Privacy Guarantee", 
    "arxiv-id": "1208.0220v1", 
    "author": "Panagiotis Karras", 
    "publish": "2012-08-01T14:11:45Z", 
    "summary": "Today, the publication of microdata poses a privacy threat. Vast research has\nstriven to define the privacy condition that microdata should satisfy before it\nis released, and devise algorithms to anonymize the data so as to achieve this\ncondition. Yet, no method proposed to date explicitly bounds the percentage of\ninformation an adversary gains after seeing the published data for each\nsensitive value therein. This paper introduces beta-likeness, an appropriately\nrobust privacy model for microdata anonymization, along with two anonymization\nschemes designed therefor, the one based on generalization, and the other based\non perturbation. Our model postulates that an adversary's confidence on the\nlikelihood of a certain sensitive-attribute (SA) value should not increase, in\nrelative difference terms, by more than a predefined threshold. Our techniques\naim to satisfy a given beta threshold with little information loss. We\nexperimentally demonstrate that (i) our model provides an effective privacy\nguarantee in a way that predecessor models cannot, (ii) our generalization\nscheme is more effective and efficient in its task than methods adapting\nalgorithms for the k-anonymity model, and (iii) our perturbation method\noutperforms a baseline approach. Moreover, we discuss in detail the resistance\nof our model and methods to attacks proposed in previous research."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0221v1", 
    "other_authors": "Ziyu Guan, Xifeng Yan, Lance M. Kaplan", 
    "title": "Measuring Two-Event Structural Correlations on Graphs", 
    "arxiv-id": "1208.0221v1", 
    "author": "Lance M. Kaplan", 
    "publish": "2012-08-01T14:12:02Z", 
    "summary": "Real-life graphs usually have various kinds of events happening on them,\ne.g., product purchases in online social networks and intrusion alerts in\ncomputer networks. The occurrences of events on the same graph could be\ncorrelated, exhibiting either attraction or repulsion. Such structural\ncorrelations can reveal important relationships between different events.\nUnfortunately, correlation relationships on graph structures are not well\nstudied and cannot be captured by traditional measures. In this work, we design\na novel measure for assessing two-event structural correlations on graphs.\nGiven the occurrences of two events, we choose uniformly a sample of \"reference\nnodes\" from the vicinity of all event nodes and employ the Kendall's tau rank\ncorrelation measure to compute the average concordance of event density\nchanges. Significance can be efficiently assessed by tau's nice property of\nbeing asymptotically normal under the null hypothesis. In order to compute the\nmeasure in large scale networks, we develop a scalable framework using\ndifferent sampling strategies. The complexity of these strategies is analyzed.\nExperiments on real graph datasets with both synthetic and real events\ndemonstrate that the proposed framework is not only efficacious, but also\nefficient and scalable."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0222v1", 
    "other_authors": "Jeffrey Jestes, Jeff M. Phillips, Feifei Li, Mingwang Tang", 
    "title": "Ranking Large Temporal Data", 
    "arxiv-id": "1208.0222v1", 
    "author": "Mingwang Tang", 
    "publish": "2012-08-01T14:12:21Z", 
    "summary": "Ranking temporal data has not been studied until recently, even though\nranking is an important operator (being promoted as a firstclass citizen) in\ndatabase systems. However, only the instant top-k queries on temporal data were\nstudied in, where objects with the k highest scores at a query time instance t\nare to be retrieved. The instant top-k definition clearly comes with\nlimitations (sensitive to outliers, difficult to choose a meaningful query time\nt). A more flexible and general ranking operation is to rank objects based on\nthe aggregation of their scores in a query interval, which we dub the aggregate\ntop-k query on temporal data. For example, return the top-10 weather stations\nhaving the highest average temperature from 10/01/2010 to 10/07/2010; find the\ntop-20 stocks having the largest total transaction volumes from 02/05/2011 to\n02/07/2011. This work presents a comprehensive study to this problem by\ndesigning both exact and approximate methods (with approximation quality\nguarantees). We also provide theoretical analysis on the construction cost, the\nindex size, the update and the query costs of each approach. Extensive\nexperiments on large real datasets clearly demonstrate the efficiency, the\neffectiveness, and the scalability of our methods compared to the baseline\nmethods."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0224v1", 
    "other_authors": "Florian Funke, Alfons Kemper, Thomas Neumann", 
    "title": "Compacting Transactional Data in Hybrid OLTP & OLAP Databases", 
    "arxiv-id": "1208.0224v1", 
    "author": "Thomas Neumann", 
    "publish": "2012-08-01T14:13:12Z", 
    "summary": "Growing main memory sizes have facilitated database management systems that\nkeep the entire database in main memory. The drastic performance improvements\nthat came along with these in-memory systems have made it possible to reunite\nthe two areas of online transaction processing (OLTP) and online analytical\nprocessing (OLAP): An emerging class of hybrid OLTP and OLAP database systems\nallows to process analytical queries directly on the transactional data. By\noffering arbitrarily current snapshots of the transactional data for OLAP,\nthese systems enable real-time business intelligence. Despite memory sizes of\nseveral Terabytes in a single commodity server, RAM is still a precious\nresource: Since free memory can be used for intermediate results in query\nprocessing, the amount of memory determines query performance to a large\nextent. Consequently, we propose the compaction of memory-resident databases.\nCompaction consists of two tasks: First, separating the mutable working set\nfrom the immutable \"frozen\" data. Second, compressing the immutable data and\noptimizing it for efficient, memory-consumption-friendly snapshotting. Our\napproach reorganizes and compresses transactional data online and yet hardly\naffects the mission-critical OLTP throughput. This is achieved by unburdening\nthe OLTP threads from all additional processing and performing these tasks\nasynchronously."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0225v1", 
    "other_authors": "Alexander Hall, Olaf Bachmann, Robert B\u00fcssow, Silviu G\u0103nceanu, Marc Nunkesser", 
    "title": "Processing a Trillion Cells per Mouse Click", 
    "arxiv-id": "1208.0225v1", 
    "author": "Marc Nunkesser", 
    "publish": "2012-08-01T14:13:23Z", 
    "summary": "Column-oriented database systems have been a real game changer for the\nindustry in recent years. Highly tuned and performant systems have evolved that\nprovide users with the possibility of answering ad hoc queries over large\ndatasets in an interactive manner. In this paper we present the column-oriented\ndatastore developed as one of the central components of PowerDrill. It combines\nthe advantages of columnar data layout with other known techniques (such as\nusing composite range partitions) and extensive algorithmic engineering on key\ndata structures. The main goal of the latter being to reduce the main memory\nfootprint and to increase the efficiency in processing typical user queries. In\nthis combination we achieve large speed-ups. These enable a highly interactive\nWeb UI where it is common that a single mouse click leads to processing a\ntrillion values in the underlying dataset."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0227v1", 
    "other_authors": "Danica Porobic, Ippokratis Pandis, Miguel Branco, P\u0131nar T\u00f6z\u00fcn, Anastasia Ailamaki", 
    "title": "OLTP on Hardware Islands", 
    "arxiv-id": "1208.0227v1", 
    "author": "Anastasia Ailamaki", 
    "publish": "2012-08-01T14:13:33Z", 
    "summary": "Modern hardware is abundantly parallel and increasingly heterogeneous. The\nnumerous processing cores have non-uniform access latencies to the main memory\nand to the processor caches, which causes variability in the communication\ncosts. Unfortunately, database systems mostly assume that all processing cores\nare the same and that microarchitecture differences are not significant enough\nto appear in critical database execution paths. As we demonstrate in this\npaper, however, hardware heterogeneity does appear in the critical path and\nconventional database architectures achieve suboptimal and even worse,\nunpredictable performance. We perform a detailed performance analysis of OLTP\ndeployments in servers with multiple cores per CPU (multicore) and multiple\nCPUs per server (multisocket). We compare different database deployment\nstrategies where we vary the number and size of independent database instances\nrunning on a single server, from a single shared-everything instance to\nfine-grained shared-nothing configurations. We quantify the impact of\nnon-uniform hardware on various deployments by (a) examining how efficiently\neach deployment uses the available hardware resources and (b) measuring the\nimpact of distributed transactions and skewed requests on different workloads.\nFinally, we argue in favor of shared-nothing deployments that are topology- and\nworkload-aware and take advantage of fast on-chip communication between islands\nof cores on the same socket."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0270v1", 
    "other_authors": "Stacy Patterson, Aaron J. Elmore, Faisal Nawab, Divyakant Agrawal, Amr El Abbadi", 
    "title": "Serializability, not Serial: Concurrency Control and Availability in   Multi-Datacenter Datastores", 
    "arxiv-id": "1208.0270v1", 
    "author": "Amr El Abbadi", 
    "publish": "2012-08-01T16:48:57Z", 
    "summary": "We present a framework for concurrency control and availability in\nmulti-datacenter datastores. While we consider Google's Megastore as our\nmotivating example, we define general abstractions for key components, making\nour solution extensible to any system that satisfies the abstraction\nproperties. We first develop and analyze a transaction management and\nreplication protocol based on a straightforward implementation of the Paxos\nalgorithm. Our investigation reveals that this protocol acts as a concurrency\nprevention mechanism rather than a concurrency control mechanism. We then\npropose an enhanced protocol called Paxos with Combination and Promotion\n(Paxos-CP) that provides true transaction concurrency while requiring the same\nper instance message complexity as the basic Paxos protocol. Finally, we\ncompare the performance of Paxos and Paxos-CP in a multi-datacenter\nexperimental study, and we demonstrate that Paxos-CP results in significantly\nfewer aborted transactions than basic Paxos."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0271v1", 
    "other_authors": "Alvin Cheung, Owen Arden, Samuel Madden, Andrew C. Myers", 
    "title": "Automatic Partitioning of Database Applications", 
    "arxiv-id": "1208.0271v1", 
    "author": "Andrew C. Myers", 
    "publish": "2012-08-01T16:49:06Z", 
    "summary": "Database-backed applications are nearly ubiquitous in our daily lives.\nApplications that make many small accesses to the database create two\nchallenges for developers: increased latency and wasted resources from numerous\nnetwork round trips. A well-known technique to improve transactional database\napplication performance is to convert part of the application into stored\nprocedures that are executed on the database server. Unfortunately, this\nconversion is often difficult. In this paper we describe Pyxis, a system that\ntakes database-backed applications and automatically partitions their code into\ntwo pieces, one of which is executed on the application server and the other on\nthe database server. Pyxis profiles the application and server loads,\nstatically analyzes the code's dependencies, and produces a partitioning that\nminimizes the number of control transfers as well as the amount of data sent\nduring each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is\nable to generate partitions with up to 3x reduction in latency and 1.7x\nimprovement in throughput when compared to a traditional non-partitioned\nimplementation and has comparable performance to that of a custom stored\nprocedure implementation."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0273v1", 
    "other_authors": "Caleb Chen Cao, Jieying She, Yongxin Tong, Lei Chen", 
    "title": "Whom to Ask? Jury Selection for Decision Making Tasks on Micro-blog   Services", 
    "arxiv-id": "1208.0273v1", 
    "author": "Lei Chen", 
    "publish": "2012-08-01T16:49:31Z", 
    "summary": "It is universal to see people obtain knowledge on micro-blog services by\nasking others decision making questions. In this paper, we study the Jury\nSelection Problem(JSP) by utilizing crowdsourcing for decision making tasks on\nmicro-blog services. Specifically, the problem is to enroll a subset of crowd\nunder a limited budget, whose aggregated wisdom via Majority Voting scheme has\nthe lowest probability of drawing a wrong answer(Jury Error Rate-JER). Due to\nvarious individual error-rates of the crowd, the calculation of JER is\nnon-trivial. Firstly, we explicitly state that JER is the probability when the\nnumber of wrong jurors is larger than half of the size of a jury. To avoid the\nexponentially increasing calculation of JER, we propose two efficient\nalgorithms and an effective bounding technique. Furthermore, we study the Jury\nSelection Problem on two crowdsourcing models, one is for altruistic\nusers(AltrM) and the other is for incentive-requiring users(PayM) who require\nextra payment when enrolled into a task. For the AltrM model, we prove the\nmonotonicity of JER on individual error rate and propose an efficient exact\nalgorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM\nand propose an efficient greedy-based heuristic algorithm. Finally, we conduct\na series of experiments to investigate the traits of JSP, and validate the\nefficiency and effectiveness of our proposed algorithms on both synthetic and\nreal micro-blog data."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0274v1", 
    "other_authors": "Xiaochun Yang, Honglei Liu, Bin Wang", 
    "title": "ALAE: Accelerating Local Alignment with Affine Gap Exactly in   Biosequence Databases", 
    "arxiv-id": "1208.0274v1", 
    "author": "Bin Wang", 
    "publish": "2012-08-01T16:49:39Z", 
    "summary": "We study the problem of local alignment, which is finding pairs of similar\nsubsequences with gaps. The problem exists in biosequence databases. BLAST is a\ntypical software for finding local alignment based on heuristic, but could miss\nresults. Using the Smith-Waterman algorithm, we can find all local alignments\nin O(mn) time, where m and n are lengths of a query and a text, respectively. A\nrecent exact approach BWT-SW improves the complexity of the Smith-Waterman\nalgorithm under constraints, but still much slower than BLAST. This paper takes\non the challenge of designing an accurate and efficient algorithm for\nevaluating local-alignment searches, especially for long queries. In this\npaper, we propose an efficient software called ALAE to speed up BWT-SW using a\ncompressed suffix array. ALAE utilizes a family of filtering techniques to\nprune meaningless calculations and an algorithm for reusing score calculations.\nWe also give a mathematical analysis and show that the upper bound of the total\nnumber of calculated entries using ALAE could vary from 4.50mn0.520 to\n9.05mn0.896 for random DNA sequences and vary from 8.28mn0.364 to 7.49mn0.723\nfor random protein sequences. We demonstrate the significant performance\nimprovement of ALAE on BWT-SW using a thorough experimental study on real\nbiosequences. ALAE guarantees correctness and accelerates BLAST for most of\nparameters."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0275v1", 
    "other_authors": "K. Sel\u00e7uk Candan, Rosaria Rossini, Maria Luisa Sapino, Xiaolan Wang", 
    "title": "sDTW: Computing DTW Distances using Locally Relevant Constraints based   on Salient Feature Alignments", 
    "arxiv-id": "1208.0275v1", 
    "author": "Xiaolan Wang", 
    "publish": "2012-08-01T16:49:49Z", 
    "summary": "Many applications generate and consume temporal data and retrieval of time\nseries is a key processing step in many application domains. Dynamic time\nwarping (DTW) distance between time series of size N and M is computed relying\non a dynamic programming approach which creates and fills an NxM grid to search\nfor an optimal warp path. Since this can be costly, various heuristics have\nbeen proposed to cut away the potentially unproductive portions of the DTW\ngrid. In this paper, we argue that time series often carry structural features\nthat can be used for identifying locally relevant constraints to eliminate\nredundant work. Relying on this observation, we propose salient feature based\nsDTW algorithms which first identify robust salient features in the given time\nseries and then find a consistent alignment of these to establish the\nboundaries for the warp path search. More specifically, we propose alternative\nfixed core&adaptive width, adaptive core&fixed width, and adaptive\ncore&adaptive width strategies which enforce different constraints reflecting\nthe high level structural characteristics of the series in the data set.\nExperiment results show that the proposed sDTW algorithms help achieve much\nhigher accuracy in DTWcomputation and time series retrieval than fixed core &\nfixed width algorithms that do not leverage local features of the given time\nseries."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0276v1", 
    "other_authors": "Farhan Tauheed, Thomas Heinis, Felix Sh\u00fcrmann, Henry Markram, Anastasia Ailamaki", 
    "title": "SCOUT: Prefetching for Latent Feature Following Queries", 
    "arxiv-id": "1208.0276v1", 
    "author": "Anastasia Ailamaki", 
    "publish": "2012-08-01T16:49:56Z", 
    "summary": "Today's scientists are quickly moving from in vitro to in silico\nexperimentation: they no longer analyze natural phenomena in a petri dish, but\ninstead they build models and simulate them. Managing and analyzing the massive\namounts of data involved in simulations is a major task. Yet, they lack the\ntools to efficiently work with data of this size. One problem many scientists\nshare is the analysis of the massive spatial models they build. For several\ntypes of analysis they need to interactively follow the structures in the\nspatial model, e.g., the arterial tree, neuron fibers, etc., and issue range\nqueries along the way. Each query takes long to execute, and the total time for\nexecuting a sequence of queries significantly delays data analysis. Prefetching\nthe spatial data reduces the response time considerably, but known approaches\ndo not prefetch with high accuracy. We develop SCOUT, a structure-aware method\nfor prefetching data along interactive spatial query sequences. SCOUT uses an\napproximate graph model of the structures involved in past queries and attempts\nto identify what particular structure the user follows. Our experiments with\nneuroscience data show that SCOUT prefetches with an accuracy from 71% to 92%,\nwhich translates to a speedup of 4x-15x. SCOUT also improves the prefetching\naccuracy on datasets from other scientific domains, such as medicine and\nbiology."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0277v1", 
    "other_authors": "Kaibo Wang, Yin Huai, Rubao Lee, Fusheng Wang, Xiaodong Zhang, Joel H. Saltz", 
    "title": "Accelerating Pathology Image Data Cross-Comparison on CPU-GPU Hybrid   Systems", 
    "arxiv-id": "1208.0277v1", 
    "author": "Joel H. Saltz", 
    "publish": "2012-08-01T16:50:06Z", 
    "summary": "As an important application of spatial databases in pathology imaging\nanalysis, cross-comparing the spatial boundaries of a huge amount of segmented\nmicro-anatomic objects demands extremely data- and compute-intensive\noperations, requiring high throughput at an affordable cost. However, the\nperformance of spatial database systems has not been satisfactory since their\nimplementations of spatial operations cannot fully utilize the power of modern\nparallel hardware. In this paper, we provide a customized software solution\nthat exploits GPUs and multi-core CPUs to accelerate spatial cross-comparison\nin a cost-effective way. Our solution consists of an efficient GPU algorithm\nand a pipelined system framework with task migration support. Extensive\nexperiments with real-world data sets demonstrate the effectiveness of our\nsolution, which improves the performance of spatial cross-comparison by over 18\ntimes compared with a parallelized spatial database approach."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0278v1", 
    "other_authors": "Jiexing Li, Arnd Christian K\u00f6nig, Vivek Narasayya, Surajit Chaudhuri", 
    "title": "Robust Estimation of Resource Consumption for SQL Queries using   Statistical Techniques", 
    "arxiv-id": "1208.0278v1", 
    "author": "Surajit Chaudhuri", 
    "publish": "2012-08-01T16:50:15Z", 
    "summary": "The ability to estimate resource consumption of SQL queries is crucial for a\nnumber of tasks in a database system such as admission control, query\nscheduling and costing during query optimization. Recent work has explored the\nuse of statistical techniques for resource estimation in place of the manually\nconstructed cost models used in query optimization. Such techniques, which\nrequire as training data examples of resource usage in queries, offer the\npromise of superior estimation accuracy since they can account for factors such\nas hardware characteristics of the system or bias in cardinality estimates.\nHowever, the proposed approaches lack robustness in that they do not generalize\nwell to queries that are different from the training examples, resulting in\nsignificant estimation errors. Our approach aims to address this problem by\ncombining knowledge of database query processing with statistical models. We\nmodel resource-usage at the level of individual operators, with different\nmodels and features for each operator type, and explicitly model the asymptotic\nbehavior of each operator. This results in significantly better estimation\naccuracy and the ability to estimate resource usage of arbitrary plans, even\nwhen they are very different from the training instances. We validate our\napproach using various large scale real-life and benchmark workloads on\nMicrosoft SQL Server."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0285v1", 
    "other_authors": "Mahashweta Das, Saravanan Thirumuruganathan, Sihem Amer-Yahia, Gautam Das, Cong Yu", 
    "title": "Who Tags What? An Analysis Framework", 
    "arxiv-id": "1208.0285v1", 
    "author": "Cong Yu", 
    "publish": "2012-08-01T17:20:05Z", 
    "summary": "The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and\nYouTube, and social tagging is essential to their success. A typical tagging\naction involves three components, user, item (e.g., photos in Flickr), and tags\n(i.e., words or phrases). Analyzing how tags are assigned by certain users to\ncertain items has important implications in helping users search for desired\ninformation. In this paper, we explore common analysis tasks and propose a dual\nmining framework for social tagging behavior mining. This framework is centered\naround two opposing measures, similarity and diversity, being applied to one or\nmore tagging components, and therefore enables a wide range of analysis\nscenarios such as characterizing similar users tagging diverse items with\nsimilar tags, or diverse users tagging similar items with diverse tags, etc. By\nadopting different concrete measures for similarity and diversity in the\nframework, we show that a wide range of concrete analysis problems can be\ndefined and they are NP-Complete in general. We design efficient algorithms for\nsolving many of those problems and demonstrate, through comprehensive\nexperiments over real data, that our algorithms significantly out-perform the\nexact brute-force approach without compromising analysis result quality."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0286v1", 
    "other_authors": "Haohan Zhu, George Kollios, Vassilis Athitsos", 
    "title": "A Generic Framework for Efficient and Effective Subsequence Retrieval", 
    "arxiv-id": "1208.0286v1", 
    "author": "Vassilis Athitsos", 
    "publish": "2012-08-01T17:20:11Z", 
    "summary": "This paper proposes a general framework for matching similar subsequences in\nboth time series and string databases. The matching results are pairs of query\nsubsequences and database subsequences. The framework finds all possible pairs\nof similar subsequences if the distance measure satisfies the \"consistency\"\nproperty, which is a property introduced in this paper. We show that most\npopular distance functions, such as the Euclidean distance, DTW, ERP, the\nFrechet distance for time series, and the Hamming distance and Levenshtein\ndistance for strings, are all \"consistent\". We also propose a generic index\nstructure for metric spaces named \"reference net\". The reference net occupies\nO(n) space, where n is the size of the dataset and is optimized to work well\nwith our framework. The experiments demonstrate the ability of our method to\nimprove retrieval performance when combined with diverse distance measures. The\nexperiments also illustrate that the reference net scales well in terms of\nspace overhead and query time."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0287v1", 
    "other_authors": "Jens Dittrich, Jorge-Arnulfo Quian\u00e9-Ruiz, Stefan Richter, Stefan Schuh, Alekh Jindal, J\u00f6rg Schad", 
    "title": "Only Aggressive Elephants are Fast Elephants", 
    "arxiv-id": "1208.0287v1", 
    "author": "J\u00f6rg Schad", 
    "publish": "2012-08-01T17:20:18Z", 
    "summary": "Yellow elephants are slow. A major reason is that they consume their inputs\nentirely before responding to an elephant rider's orders. Some clever riders\nhave trained their yellow elephants to only consume parts of the inputs before\nresponding. However, the teaching time to make an elephant do that is high. So\nhigh that the teaching lessons often do not pay off. We take a different\napproach. We make elephants aggressive; only this will make them very fast. We\npropose HAIL (Hadoop Aggressive Indexing Library), an enhancement of HDFS and\nHadoop MapReduce that dramatically improves runtimes of several classes of\nMapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create\ndifferent clustered indexes on each data block replica. An interesting feature\nof HAIL is that we typically create a win-win situation: we improve both data\nupload to HDFS and the runtime of the actual Hadoop MapReduce job. In terms of\ndata upload, HAIL improves over HDFS by up to 60% with the default replication\nfactor of three. In terms of query execution, we demonstrate that HAIL runs up\nto 68x faster than Hadoop. In our experiments, we use six clusters including\nphysical and EC2 clusters of up to 100 nodes. A series of scalability\nexperiments also demonstrates the superiority of HAIL."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0288v1", 
    "other_authors": "Rui Li, Shengjie Wang, Kevin Chen-Chuan Chang", 
    "title": "Multiple Location Profiling for Users and Relationships from Social   Network and Content", 
    "arxiv-id": "1208.0288v1", 
    "author": "Kevin Chen-Chuan Chang", 
    "publish": "2012-08-01T17:20:26Z", 
    "summary": "Users' locations are important for many applications such as personalized\nsearch and localized content delivery. In this paper, we study the problem of\nprofiling Twitter users' locations with their following network and tweets. We\npropose a multiple location profiling model (MLP), which has three key\nfeatures: 1) it formally models how likely a user follows another user given\ntheir locations and how likely a user tweets a venue given his location, 2) it\nfundamentally captures that a user has multiple locations and his following\nrelationships and tweeted venues can be related to any of his locations, and\nsome of them are even noisy, and 3) it novelly utilizes the home locations of\nsome users as partial supervision. As a result, MLP not only discovers users'\nlocations accurately and completely, but also \"explains\" each following\nrelationship by revealing users' true locations in the relationship.\nExperiments on a large-scale data set demonstrate those advantages.\nParticularly, 1) for predicting users' home locations, MLP successfully places\n62% users and outperforms two state-of-the-art methods by 10% in accuracy, 2)\nfor discovering users' multiple locations, MLP improves the baseline methods by\n14% in recall, and 3) for explaining following relationships, MLP achieves 57%\naccuracy."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0289v1", 
    "other_authors": "Woon-Hak Kang, Sang-Won Lee, Bongki Moon", 
    "title": "Flash-based Extended Cache for Higher Throughput and Faster Recovery", 
    "arxiv-id": "1208.0289v1", 
    "author": "Bongki Moon", 
    "publish": "2012-08-01T17:20:34Z", 
    "summary": "Considering the current price gap between disk and flash memory drives, for\napplications dealing with large scale data, it will be economically more\nsensible to use flash memory drives to supplement disk drives rather than to\nreplace them. This paper presents FaCE, which is a new low-overhead caching\nstrategy that uses flash memory as an extension to the DRAM buffer. FaCE aims\nat improving the transaction throughput as well as shortening the recovery time\nfrom a system failure. To achieve the goals, we propose two novel algorithms\nfor flash cache management, namely, Multi-Version FIFO replacement and Group\nSecond Chance. One striking result from FaCE is that using a small flash memory\ndrive as a caching device could deliver even higher throughput than using a\nlarge flash memory drive to store the entire database tables. This was possible\ndue to flash write optimization as well as disk access reduction obtained by\nthe FaCE caching methods. In addition, FaCE takes advantage of the\nnon-volatility of flash memory to fully support database recovery by extending\nthe scope of a persistent database to include the data pages stored in the\nflash cache. We have implemented FaCE in the PostgreSQL open source database\nserver and demonstrated its effectiveness for TPC-C benchmarks."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0290v1", 
    "other_authors": "Michael A. Bender, Martin Farach-Colton, Rob Johnson, Russell Kraner, Bradley C. Kuszmaul, Dzejla Medjedovic, Pablo Montes, Pradeep Shetty, Richard P. Spillane, Erez Zadok", 
    "title": "Don't Thrash: How to Cache Your Hash on Flash", 
    "arxiv-id": "1208.0290v1", 
    "author": "Erez Zadok", 
    "publish": "2012-08-01T17:21:23Z", 
    "summary": "This paper presents new alternatives to the well-known Bloom filter data\nstructure. The Bloom filter, a compact data structure supporting set insertion\nand membership queries, has found wide application in databases, storage\nsystems, and networks. Because the Bloom filter performs frequent random reads\nand writes, it is used almost exclusively in RAM, limiting the size of the sets\nit can represent. This paper first describes the quotient filter, which\nsupports the basic operations of the Bloom filter, achieving roughly comparable\nperformance in terms of space and time, but with better data locality.\nOperations on the quotient filter require only a small number of contiguous\naccesses. The quotient filter has other advantages over the Bloom filter: it\nsupports deletions, it can be dynamically resized, and two quotient filters can\nbe efficiently merged. The paper then gives two data structures, the buffered\nquotient filter and the cascade filter, which exploit the quotient filter\nadvantages and thus serve as SSD-optimized alternatives to the Bloom filter.\nThe cascade filter has better asymptotic I/O performance than the buffered\nquotient filter, but the buffered quotient filter outperforms the cascade\nfilter on small to medium data sets. Both data structures significantly\noutperform recently-proposed SSD-optimized Bloom filter variants, such as the\nelevator Bloom filter, buffered Bloom filter, and forest-structured Bloom\nfilter. In experiments, the cascade filter and buffered quotient filter\nperformed insertions 8.6-11 times faster than the fastest Bloom filter variant\nand performed lookups 0.94-2.56 times faster."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0291v1", 
    "other_authors": "Robert Isele, Christian Bizer", 
    "title": "Learning Expressive Linkage Rules using Genetic Programming", 
    "arxiv-id": "1208.0291v1", 
    "author": "Christian Bizer", 
    "publish": "2012-08-01T17:21:32Z", 
    "summary": "A central problem in data integration and data cleansing is to find entities\nin different data sources that describe the same real-world object. Many\nexisting methods for identifying such entities rely on explicit linkage rules\nwhich specify the conditions that entities must fulfill in order to be\nconsidered to describe the same real-world object. In this paper, we present\nthe GenLink algorithm for learning expressive linkage rules from a set of\nexisting reference links using genetic programming. The algorithm is capable of\ngenerating linkage rules which select discriminative properties for comparison,\napply chains of data transformations to normalize property values, choose\nappropriate distance measures and thresholds and combine the results of\nmultiple comparisons using non-linear aggregation functions. Our experiments\nshow that the GenLink algorithm outperforms the state-of-the-art genetic\nprogramming approach to learning linkage rules recently presented by Carvalho\net. al. and is capable of learning linkage rules which achieve a similar\naccuracy as human written rules for the same problem."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0292v1", 
    "other_authors": "Yongxin Tong, Lei Chen, Yurong Cheng, Philip S. Yu", 
    "title": "Mining Frequent Itemsets over Uncertain Databases", 
    "arxiv-id": "1208.0292v1", 
    "author": "Philip S. Yu", 
    "publish": "2012-08-01T17:22:19Z", 
    "summary": "In recent years, due to the wide applications of uncertain data, mining\nfrequent itemsets over uncertain databases has attracted much attention. In\nuncertain databases, the support of an itemset is a random variable instead of\na fixed occurrence counting of this itemset. Thus, unlike the corresponding\nproblem in deterministic databases where the frequent itemset has a unique\ndefinition, the frequent itemset under uncertain environments has two different\ndefinitions so far. The first definition, referred as the expected\nsupport-based frequent itemset, employs the expectation of the support of an\nitemset to measure whether this itemset is frequent. The second definition,\nreferred as the probabilistic frequent itemset, uses the probability of the\nsupport of an itemset to measure its frequency. Thus, existing work on mining\nfrequent itemsets over uncertain databases is divided into two different groups\nand no study is conducted to comprehensively compare the two different\ndefinitions. In addition, since no uniform experimental platform exists,\ncurrent solutions for the same definition even generate inconsistent results.\nIn this paper, we firstly aim to clarify the relationship between the two\ndifferent definitions. Through extensive experiments, we verify that the two\ndefinitions have a tight connection and can be unified together when the size\nof data is large enough. Secondly, we provide baseline implementations of eight\nexisting representative algorithms and test their performances with uniform\nmeasures fairly. Finally, according to the fair tests over many different\nbenchmark data sets, we clarify several existing inconsistent conclusions and\ndiscuss some new findings."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.0684v1", 
    "other_authors": "Mahnoosh Kholghi, MohammadReza Keyvanpour", 
    "title": "Comparative Evaluation of Data Stream Indexing Models", 
    "arxiv-id": "1208.0684v1", 
    "author": "MohammadReza Keyvanpour", 
    "publish": "2012-08-03T08:17:57Z", 
    "summary": "In recent years, the management and processing of data streams has become a\ntopic of active research in several fields of computer science such as,\ndistributed systems, database systems, and data mining. A data stream can be\nthought of as a transient, continuously increasing sequence of data. In data\nstreams' applications, because of online monitoring, answering to the user's\nqueries should be time and space efficient. In this paper, we consider the\nspecial requirements of indexing to determine the performance of different\ntechniques in data stream processing environments. Stream indexing has main\ndifferences with approaches in traditional databases. Also, we compare data\nstream indexing models analytically that can provide a suitable method for\nstream indexing."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.1231v1", 
    "other_authors": "Foteini Alvanaki, Sebastian Michel, Aleksandar Stupar", 
    "title": "Building and Maintaining Halls of Fame over a Database", 
    "arxiv-id": "1208.1231v1", 
    "author": "Aleksandar Stupar", 
    "publish": "2012-08-06T18:26:17Z", 
    "summary": "Halls of Fame are fascinating constructs. They represent the elite of an\noften very large amount of entities---persons, companies, products, countries\netc. Beyond their practical use as static rankings, changes to them are\nparticularly interesting---for decision making processes, as input to common\nmedia or novel narrative science applications, or simply consumed by users. In\nthis work, we aim at detecting events that can be characterized by changes to a\nHall of Fame ranking in an automated way. We describe how the schema and data\nof a database can be used to generate Halls of Fame. In this database scenario,\nby Hall of Fame we refer to distinguished tuples; entities, whose\ncharacteristics set them apart from the majority. We define every Hall of Fame\nas one specific instance of an SQL query, such that a change in its result is\nconsidered a noteworthy event. Identified changes (i.e., events) are ranked\nusing lexicographic tradeoffs over event and query properties and presented to\nusers or fed in higher-level applications. We have implemented a full-fledged\nprototype system that uses either database triggers or a Java based middleware\nfor event identification. We report on an experimental evaluation using a\nreal-world dataset of basketball statistics."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.1927v1", 
    "other_authors": "Jiannan Wang, Tim Kraska, Michael J. Franklin, Jianhua Feng", 
    "title": "CrowdER: Crowdsourcing Entity Resolution", 
    "arxiv-id": "1208.1927v1", 
    "author": "Jianhua Feng", 
    "publish": "2012-08-09T14:46:38Z", 
    "summary": "Entity resolution is central to data integration and data cleaning.\nAlgorithmic approaches have been improving in quality, but remain far from\nperfect. Crowdsourcing platforms offer a more accurate but expensive (and slow)\nway to bring human insight into the process. Previous work has proposed\nbatching verification tasks for presentation to human workers but even with\nbatching, a human-only approach is infeasible for data sets of even moderate\nsize, due to the large numbers of matches to be tested. Instead, we propose a\nhybrid human-machine approach in which machines are used to do an initial,\ncoarse pass over all the data, and people are used to verify only the most\nlikely matching pairs. We show that for such a hybrid system, generating the\nminimum number of verification tasks of a given size is NP-Hard, but we develop\na novel two-tiered heuristic approach for creating batched tasks. We describe\nthis method, and present the results of extensive experiments on real data sets\nusing a popular crowdsourcing platform. The experiments show that our hybrid\napproach achieves both good efficiency and high accuracy compared to\nmachine-only or human-only alternatives."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.1931v1", 
    "other_authors": "Michele Dallachiesa, Besmira Nushi, Katsiaryna Mirylenka, Themis Palpanas", 
    "title": "Uncertain Time-Series Similarity: Return to the Basics", 
    "arxiv-id": "1208.1931v1", 
    "author": "Themis Palpanas", 
    "publish": "2012-08-09T14:52:01Z", 
    "summary": "In the last years there has been a considerable increase in the availability\nof continuous sensor measurements in a wide range of application domains, such\nas Location-Based Services (LBS), medical monitoring systems, manufacturing\nplants and engineering facilities to ensure efficiency, product quality and\nsafety, hydrologic and geologic observing systems, pollution management, and\nothers. Due to the inherent imprecision of sensor observations, many\ninvestigations have recently turned into querying, mining and storing uncertain\ndata. Uncertainty can also be due to data aggregation, privacy-preserving\ntransforms, and error-prone mining algorithms. In this study, we survey the\ntechniques that have been proposed specifically for modeling and processing\nuncertain time series, an important model for temporal data. We provide an\nanalytical evaluation of the alternatives that have been proposed in the\nliterature, highlighting the advantages and disadvantages of each approach, and\nfurther compare these alternatives with two additional techniques that were\ncarefully studied before. We conduct an extensive experimental evaluation with\n17 real datasets, and discuss some surprising results, which suggest that a\nfruitful research direction is to take into account the temporal correlations\nin the time series. Based on our evaluations, we also provide guidelines useful\nfor the practitioners in the field."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.1932v1", 
    "other_authors": "Tamraparni Dasu, Ji Meng Loh", 
    "title": "Statistical Distortion: Consequences of Data Cleaning", 
    "arxiv-id": "1208.1932v1", 
    "author": "Ji Meng Loh", 
    "publish": "2012-08-09T14:52:19Z", 
    "summary": "We introduce the notion of statistical distortion as an essential metric for\nmeasuring the effectiveness of data cleaning strategies. We use this metric to\npropose a widely applicable yet scalable experimental framework for evaluating\ndata cleaning strategies along three dimensions: glitch improvement,\nstatistical distortion and cost-related criteria. Existing metrics focus on\nglitch improvement and cost, but not on the statistical impact of data cleaning\nstrategies. We illustrate our framework on real world data, with a\ncomprehensive suite of experiments and analyses."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.1933v1", 
    "other_authors": "Willis Lang, Stavros Harizopoulos, Jignesh M. Patel, Mehul A. Shah, Dimitris Tsirogiannis", 
    "title": "Towards Energy-Efficient Database Cluster Design", 
    "arxiv-id": "1208.1933v1", 
    "author": "Dimitris Tsirogiannis", 
    "publish": "2012-08-09T14:54:33Z", 
    "summary": "Energy is a growing component of the operational cost for many \"big data\"\ndeployments, and hence has become increasingly important for practitioners of\nlarge-scale data analysis who require scale-out clusters or parallel DBMS\nappliances. Although a number of recent studies have investigated the energy\nefficiency of DBMSs, none of these studies have looked at the architectural\ndesign space of energy-efficient parallel DBMS clusters. There are many\nchallenges to increasing the energy efficiency of a DBMS cluster, including\ndealing with the inherent scaling inefficiency of parallel data processing, and\nchoosing the appropriate energy-efficient hardware. In this paper, we\nexperimentally examine and analyze a number of key parameters related to these\nchallenges for designing energy-efficient database clusters. We explore the\ncluster design space using empirical results and propose a model that considers\nthe key bottlenecks to energy efficiency in a parallel DBMS. This paper\nrepresents a key first step in designing energy-efficient database clusters,\nwhich is increasingly important given the trend toward parallel database\nappliances."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.2448v3", 
    "other_authors": "Yong Zeng, Zhifeng Bao, Guoliang Li, Tok Wang Ling, Jiaheng Lu", 
    "title": "Breaking Out The XML MisMatch Trap", 
    "arxiv-id": "1208.2448v3", 
    "author": "Jiaheng Lu", 
    "publish": "2012-08-12T18:51:23Z", 
    "summary": "In keyword search, when user cannot get what she wants, query refinement is\nneeded and reason can be various. We first give a thorough categorization of\nthe reason, then focus on solving one category of query refinement problem in\nthe context of XML keyword search, where what user searches for does not exist\nin the data. We refer to it as the MisMatch problem in this paper. Then we\npropose a practical way to detect the MisMatch problem and generate helpful\nsuggestions to users. Our approach can be viewed as a post-processing job of\nquery evaluation, and has three main features: (1) it adopts both the suggested\nqueries and their sample results as the output to user, helping user judge\nwhether the MisMatch problem is solved without consuming all query results; (2)\nit is portable in the sense that it can work with any LCA-based matching\nsemantics and orthogonal to the choice of result retrieval method adopted; (3)\nit is lightweight in the way that it occupies a very small proportion of the\nwhole query evaluation time. Extensive experiments on three real datasets\nverify the effectiveness, efficiency and scalability of our approach. An online\nXML keyword search engine called XClear that embeds the MisMatch problem\ndetector and suggester has been built."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.2773v1", 
    "other_authors": "Luca Bonomi, Li Xiong, Rui Chen, Benjamin C. M. Fung", 
    "title": "Privacy Preserving Record Linkage via grams Projections", 
    "arxiv-id": "1208.2773v1", 
    "author": "Benjamin C. M. Fung", 
    "publish": "2012-08-14T02:42:01Z", 
    "summary": "Record linkage has been extensively used in various data mining applications\ninvolving sharing data. While the amount of available data is growing, the\nconcern of disclosing sensitive information poses the problem of utility vs\nprivacy. In this paper, we study the problem of private record linkage via\nsecure data transformations. In contrast to the existing techniques in this\narea, we propose a novel approach that provides strong privacy guarantees under\nthe formal framework of differential privacy. We develop an embedding strategy\nbased on frequent variable length grams mined in a private way from the\noriginal data. We also introduce personalized threshold for matching individual\nrecords in the embedded space which achieves better linkage accuracy than the\nexisting global threshold approach. Compared with the state-of-the-art secure\nmatching schema, our approach provides formal, provable privacy guarantees and\nachieves better scalability while providing comparable utility."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.3307v3", 
    "other_authors": "Grigoriev Evgeny", 
    "title": "Impedance mismatch is not an \"Objects vs. Relations\" problem", 
    "arxiv-id": "1208.3307v3", 
    "author": "Grigoriev Evgeny", 
    "publish": "2012-08-16T07:43:28Z", 
    "summary": "A problem of impedance mismatch between applications written in OO languages\nand relational DB is not a problem of discrepancy between object-oriented and\nrelational approaches themselves. Its real causes can be found in usual\nimplementation of the OO approach. Direct comparison of the two approaches\ncannot be used as a base for the conclusion that they are discrepant or\nmismatched. Experimental proof of absence of contradiction between\nobject-oriented paradigm and relational data model is also presented"
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.3533v2", 
    "other_authors": "Marina Drosou, Evaggelia Pitoura", 
    "title": "DisC Diversity: Result Diversification based on Dissimilarity and   Coverage", 
    "arxiv-id": "1208.3533v2", 
    "author": "Evaggelia Pitoura", 
    "publish": "2012-08-17T05:45:18Z", 
    "summary": "Recently, result diversification has attracted a lot of attention as a means\nto improve the quality of results retrieved by user queries. In this paper, we\npropose a new, intuitive definition of diversity called DisC diversity. A DisC\ndiverse subset of a query result contains objects such that each object in the\nresult is represented by a similar object in the diverse subset and the objects\nin the diverse subset are dissimilar to each other. We show that locating a\nminimum DisC diverse subset is an NP-hard problem and provide heuristics for\nits approximation. We also propose adapting DisC diverse subsets to a different\ndegree of diversification. We call this operation zooming. We present efficient\nimplementations of our algorithms based on the M-tree, a spatial index\nstructure, and experimentally evaluate their performance."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.3812v1", 
    "other_authors": "Pritam Chanda, Aidong Zhang, Murali Ramanathan", 
    "title": "Algorithms for Efficient Mining of Statistically Significant Attribute   Association Information", 
    "arxiv-id": "1208.3812v1", 
    "author": "Murali Ramanathan", 
    "publish": "2012-08-19T06:09:49Z", 
    "summary": "Knowledge of the association information between the attributes in a data set\nprovides insight into the underlying structure of the data and explains the\nrelationships (independence, synergy, redundancy) between the attributes and\nclass (if present). Complex models learnt computationally from the data are\nmore interpretable to a human analyst when such interdependencies are known. In\nthis paper, we focus on mining two types of association information among the\nattributes - correlation information and interaction information for both\nsupervised (class attribute present) and unsupervised analysis (class attribute\nabsent). Identifying the statistically significant attribute associations is a\ncomputationally challenging task - the number of possible associations\nincreases exponentially and many associations contain redundant information\nwhen a number of correlated attributes are present. In this paper, we explore\nefficient data mining methods to discover non-redundant attribute sets that\ncontain significant association information indicating the presence of\ninformative patterns in the data."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.3876v1", 
    "other_authors": "Saravanan Thirumuruganathan, Nan Zhang, Gautam Das", 
    "title": "Digging Deeper into Deep Web Databases by Breaking Through the Top-k   Barrier", 
    "arxiv-id": "1208.3876v1", 
    "author": "Gautam Das", 
    "publish": "2012-08-19T18:34:28Z", 
    "summary": "A large number of web databases are only accessible through proprietary\nform-like interfaces which require users to query the system by entering\ndesired values for a few attributes. A key restriction enforced by such an\ninterface is the top-k output constraint - i.e., when there are a large number\nof matching tuples, only a few (top-k) of them are preferentially selected and\nreturned by the website, often according to a proprietary ranking function.\nSince most web database owners set k to be a small value, the top-k output\nconstraint prevents many interesting third-party (e.g., mashup) services from\nbeing developed over real-world web databases. In this paper we consider the\nnovel problem of \"digging deeper\" into such web databases. Our main\ncontribution is the meta-algorithm GetNext that can retrieve the next ranked\ntuple from the hidden web database using only the restrictive interface of a\nweb database without any prior knowledge of its ranking function. This\nalgorithm can then be called iteratively to retrieve as many top ranked tuples\nas necessary. We develop principled and efficient algorithms that are based on\ngenerating and executing multiple reformulated queries and inferring the next\nranked tuple from their returned results. We provide theoretical analysis of\nour algorithms, as well as extensive experimental results over synthetic and\nreal-world databases that illustrate the effectiveness of our techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4016v1", 
    "other_authors": "Gowri Shankar Ramaswamy, F Sagayaraj Francis", 
    "title": "Concept driven framework for Latent Table Discovery", 
    "arxiv-id": "1208.4016v1", 
    "author": "F Sagayaraj Francis", 
    "publish": "2012-08-20T14:09:01Z", 
    "summary": "Database systems have to cater to the growing demands of the information age.\nThe growth of the new age information retrieval powerhouses like search engines\nhas thrown a challenge to the data management community to come up with novel\nmechanisms for feeding information to end users. The burgeoning use of natural\nlanguage query interfaces compels system designers to present meaningful and\ncustomised information. Conventional query languages like SQL do not cater to\nthese requirements due to syntax oriented design. Providing a semantic cover\nover these systems was the aim of latent table discovery focusing on\nsemantically connecting unrelated tables that were not syntactically related by\ndesign and document the discovered knowledge. This paper throws a new direction\ntowards improving the semantic capabilities of database systems by introducing\na concept driven framework over the latent table discovery method."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4145v1", 
    "other_authors": "Paolo Boldi, Francesco Bonchi, Aris Gionis, Tamir Tassa", 
    "title": "Injecting Uncertainty in Graphs for Identity Obfuscation", 
    "arxiv-id": "1208.4145v1", 
    "author": "Tamir Tassa", 
    "publish": "2012-08-21T00:13:56Z", 
    "summary": "Data collected nowadays by social-networking applications create fascinating\nopportunities for building novel services, as well as expanding our\nunderstanding about social structures and their dynamics. Unfortunately,\npublishing social-network graphs is considered an ill-advised practice due to\nprivacy concerns. To alleviate this problem, several anonymization methods have\nbeen proposed, aiming at reducing the risk of a privacy breach on the published\ndata, while still allowing to analyze them and draw relevant conclusions. In\nthis paper we introduce a new anonymization approach that is based on injecting\nuncertainty in social graphs and publishing the resulting uncertain graphs.\nWhile existing approaches obfuscate graph data by adding or removing edges\nentirely, we propose using a finer-grained perturbation that adds or removes\nedges partially: this way we can achieve the same desired level of obfuscation\nwith smaller changes in the data, thus maintaining higher utility. Our\nexperiments on real-world networks confirm that at the same level of identity\nobfuscation our method provides higher usefulness than existing randomized\nmethods that publish standard graphs."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4165v1", 
    "other_authors": "Joe Hellerstein, Christopher R\u00e9, Florian Schoppmann, Daisy Zhe Wang, Eugene Fratkin, Aleksander Gorajek, Kee Siong Ng, Caleb Welton, Xixuan Feng, Kun Li, Arun Kumar", 
    "title": "The MADlib Analytics Library or MAD Skills, the SQL", 
    "arxiv-id": "1208.4165v1", 
    "author": "Arun Kumar", 
    "publish": "2012-08-21T02:52:27Z", 
    "summary": "MADlib is a free, open source library of in-database analytic methods. It\nprovides an evolving suite of SQL-based algorithms for machine learning, data\nmining and statistics that run at scale within a database engine, with no need\nfor data import/export to other tools. The goal is for MADlib to eventually\nserve a role for scalable database systems that is similar to the CRAN library\nfor R: a community repository of statistical methods, this time written with\nscale and parallelism in mind. In this paper we introduce the MADlib project,\nincluding the background that led to its beginnings, and the motivation for its\nopen source nature. We provide an overview of the library's architecture and\ndesign patterns, and provide a description of various statistical methods in\nthat context. We include performance and speedup results of a core design\npattern from one of those methods over the Greenplum parallel DBMS on a\nmodest-sized test cluster. We then report on two initial efforts at\nincorporating academic research into MADlib, which is one of the project's\ngoals. MADlib is freely available at http://madlib.net, and the project is open\nfor contributions of both new methods, and ports to additional database\nplatforms."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4166v1", 
    "other_authors": "Avrilia Floratou, Nikhil Teletia, David J. Dewitt, Jignesh M. Patel, Donghui Zhang", 
    "title": "Can the Elephants Handle the NoSQL Onslaught?", 
    "arxiv-id": "1208.4166v1", 
    "author": "Donghui Zhang", 
    "publish": "2012-08-21T02:52:46Z", 
    "summary": "In this new era of \"big data\", traditional DBMSs are under attack from two\nsides. At one end of the spectrum, the use of document store NoSQL systems\n(e.g. MongoDB) threatens to move modern Web 2.0 applications away from\ntraditional RDBMSs. At the other end of the spectrum, big data DSS analytics\nthat used to be the domain of parallel RDBMSs is now under attack by another\nclass of NoSQL data analytics systems, such as Hive on Hadoop. So, are the\ntraditional RDBMSs, aka \"big elephants\", doomed as they are challenged from\nboth ends of this \"big data\" spectrum? In this paper, we compare one\nrepresentative NoSQL system from each end of this spectrum with SQL Server, and\nanalyze the performance and scalability aspects of each of these approaches\n(NoSQL vs. SQL) on two workloads (decision support analysis and interactive\ndata-serving) that represent the two ends of the application spectrum. We\npresent insights from this evaluation and speculate on potential trends for the\nfuture."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4167v1", 
    "other_authors": "Tilmann Rabl, Mohammad Sadoghi, Hans-Arno Jacobsen, Sergio G\u00f3mez-Villamor, Victor Munt\u00e9s-Mulero, Serge Mankowskii", 
    "title": "Solving Big Data Challenges for Enterprise Application Performance   Management", 
    "arxiv-id": "1208.4167v1", 
    "author": "Serge Mankowskii", 
    "publish": "2012-08-21T02:52:55Z", 
    "summary": "As the complexity of enterprise systems increases, the need for monitoring\nand analyzing such systems also grows. A number of companies have built\nsophisticated monitoring tools that go far beyond simple resource utilization\nreports. For example, based on instrumentation and specialized APIs, it is now\npossible to monitor single method invocations and trace individual transactions\nacross geographically distributed systems. This high-level of detail enables\nmore precise forms of analysis and prediction but comes at the price of high\ndata rates (i.e., big data). To maximize the benefit of data monitoring, the\ndata has to be stored for an extended period of time for ulterior analysis.\nThis new wave of big data analytics imposes new challenges especially for the\napplication performance monitoring systems. The monitoring data has to be\nstored in a system that can sustain the high data rates and at the same time\nenable an up-to-date view of the underlying infrastructure. With the advent of\nmodern key-value stores, a variety of data storage systems have emerged that\nare built with a focus on scalability and high data rates as predominant in\nthis monitoring use case. In this work, we present our experience and a\ncomprehensive performance evaluation of six modern (open-source) data stores in\nthe context of application performance monitoring as part of CA Technologies\ninitiative. We evaluated these systems with data and workloads that can be\nfound in application performance monitoring, as well as, on-line advertisement,\npower monitoring, and many other use cases. We present our insights not only as\nperformance results but also as lessons learned and our experience relating to\nthe setup and configuration complexity of these data stores in an industry\nsetting."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4168v1", 
    "other_authors": "Avraham Shinnar, David Cunningham, Benjamin Herta, Vijay Saraswat", 
    "title": "M3R: Increased performance for in-memory Hadoop jobs", 
    "arxiv-id": "1208.4168v1", 
    "author": "Vijay Saraswat", 
    "publish": "2012-08-21T02:53:00Z", 
    "summary": "Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce\n(HMR) API targeted at online analytics on high mean-time-to-failure clusters.\nIt does not support resilience, and supports only those workloads which can fit\ninto cluster memory. In return, it can run HMR jobs unchanged -- including jobs\nproduced by compilers for higher-level languages such as Pig, Jaql, and\nSystemML and interactive front-ends like IBM BigSheets -- while providing\nsignificantly better performance than the Hadoop engine on several workloads\n(e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also\nsupports extensions to the HMR API which can enable Map Reduce jobs to run\nfaster on the M3R engine, while not affecting their performance under the\nHadoop engine."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4169v1", 
    "other_authors": "Philipp R\u00f6sch, Lars Dannecker, Gregor Hackenbroich, Franz Faerber", 
    "title": "A Storage Advisor for Hybrid-Store Databases", 
    "arxiv-id": "1208.4169v1", 
    "author": "Franz Faerber", 
    "publish": "2012-08-21T02:53:09Z", 
    "summary": "With the SAP HANA database, SAP offers a high-performance in-memory\nhybrid-store database. Hybrid-store databases---that is, databases supporting\nrow- and column-oriented data management---are getting more and more prominent.\nWhile the columnar management offers high-performance capabilities for\nanalyzing large quantities of data, the row-oriented store can handle\ntransactional point queries as well as inserts and updates more efficiently. To\neffectively take advantage of both stores at the same time the novel question\nwhether to store the given data row- or column-oriented arises. We tackle this\nproblem with a storage advisor tool that supports database administrators at\nthis decision. Our proposed storage advisor recommends the optimal store based\non data and query characteristics; its core is a cost model to estimate and\ncompare query execution times for the different stores. Besides a per-table\ndecision, our tool also considers to horizontally and vertically partition the\ndata and manage the partitions on different stores. We evaluated the storage\nadvisor for the use in the SAP HANA database; we show the recommendation\nquality as well as the benefit of having the data in the optimal store with\nrespect to increased query performance."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4170v1", 
    "other_authors": "Micha\u0142 \u015awitakowski, Peter Boncz, Marcin \u017bukowski", 
    "title": "From Cooperative Scans to Predictive Buffer Management", 
    "arxiv-id": "1208.4170v1", 
    "author": "Marcin \u017bukowski", 
    "publish": "2012-08-21T02:53:17Z", 
    "summary": "In analytical applications, database systems often need to sustain workloads\nwith multiple concurrent scans hitting the same table. The Cooperative Scans\n(CScans) framework, which introduces an Active Buffer Manager (ABM) component\ninto the database architecture, has been the most effective and elaborate\nresponse to this problem, and was initially developed in the X100 research\nprototype. We now report on the the experiences of integrating Cooperative\nScans into its industrial-strength successor, the Vectorwise database product.\nDuring this implementation we invented a simpler optimization of concurrent\nscan buffer management, called Predictive Buffer Management (PBM). PBM is based\non the observation that in a workload with long-running scans, the buffer\nmanager has quite a bit of information on the workload in the immediate future,\nsuch that an approximation of the ideal OPT algorithm becomes feasible. In the\nevaluation on both synthetic benchmarks as well as a TPC-H throughput run we\ncompare the benefits of naive buffer management (LRU) versus CScans, PBM and\nOPT; showing that PBM achieves benefits close to Cooperative Scans, while\nincurring much lower architectural impact."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4171v1", 
    "other_authors": "George Lee, Jimmy Lin, Chuang Liu, Andrew Lorek, Dmitriy Ryaboy", 
    "title": "The Unified Logging Infrastructure for Data Analytics at Twitter", 
    "arxiv-id": "1208.4171v1", 
    "author": "Dmitriy Ryaboy", 
    "publish": "2012-08-21T02:53:45Z", 
    "summary": "In recent years, there has been a substantial amount of work on large-scale\ndata analytics using Hadoop-based platforms running on large clusters of\ncommodity machines. A less-explored topic is how those data, dominated by\napplication logs, are collected and structured to begin with. In this paper, we\npresent Twitter's production logging infrastructure and its evolution from\napplication-specific logging to a unified \"client events\" log format, where\nmessages are captured in common, well-formatted, flexible Thrift messages.\nSince most analytics tasks consider the user session as the basic unit of\nanalysis, we pre-materialize \"session sequences\", which are compact summaries\nthat can answer a large class of common queries quickly. The development of\nthis infrastructure has streamlined log collection and data analysis, thereby\nimproving our ability to rapidly experiment and iterate on various aspects of\nthe service."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4172v1", 
    "other_authors": "Tomas Talius, Robin Dhamankar, Andrei Dumitrache, Hanuma Kodavalla", 
    "title": "Transaction Log Based Application Error Recovery and Point In-Time Query", 
    "arxiv-id": "1208.4172v1", 
    "author": "Hanuma Kodavalla", 
    "publish": "2012-08-21T02:53:49Z", 
    "summary": "Database backups have traditionally been used as the primary mechanism to\nrecover from hardware and user errors. High availability solutions maintain\nredundant copies of data that can be used to recover from most failures except\nuser or application errors. Database backups are neither space nor time\nefficient for recovering from user errors which typically occur in the recent\npast and affect a small portion of the database. Moreover periodic full backups\nimpact user workload and increase storage costs. In this paper we present a\nscheme that can be used for both user and application error recovery starting\nfrom the current state and rewinding the database back in time using the\ntransaction log. While we provide a consistent view of the entire database as\nof a point in time in the past, the actual prior versions are produced only for\ndata that is accessed. We make the as of data accessible to arbitrary point in\ntime queries by integrating with the database snapshot feature in Microsoft SQL\nServer."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4173v1", 
    "other_authors": "Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandier, Lyric Doshi, Chuck Bear", 
    "title": "The Vertica Analytic Database: C-Store 7 Years Later", 
    "arxiv-id": "1208.4173v1", 
    "author": "Chuck Bear", 
    "publish": "2012-08-21T02:53:52Z", 
    "summary": "This paper describes the system architecture of the Vertica Analytic Database\n(Vertica), a commercialization of the design of the C-Store research prototype.\nVertica demonstrates a modern commercial RDBMS system that presents a classical\nrelational interface while at the same time achieving the high performance\nexpected from modern \"web scale\" analytic systems by making appropriate\narchitectural choices. Vertica is also an instructive lesson in how academic\nsystems research can be directly commercialized into a successful product."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4174v1", 
    "other_authors": "Yanpei Chen, Sara Alspaugh, Randy Katz", 
    "title": "Interactive Analytical Processing in Big Data Systems: A Cross-Industry   Study of MapReduce Workloads", 
    "arxiv-id": "1208.4174v1", 
    "author": "Randy Katz", 
    "publish": "2012-08-21T02:53:55Z", 
    "summary": "Within the past few years, organizations in diverse industries have adopted\nMapReduce-based systems for large-scale data processing. Along with these new\nusers, important new workloads have emerged which feature many small, short,\nand increasingly interactive jobs in addition to the large, long-running batch\njobs for which MapReduce was originally designed. As interactive, large-scale\nquery processing is a strength of the RDBMS community, it is important that\nlessons from that field be carried over and applied where possible in this new\ndomain. However, these new workloads have not yet been described in the\nliterature. We fill this gap with an empirical analysis of MapReduce traces\nfrom six separate business-critical deployments inside Facebook and at Cloudera\ncustomers in e-commerce, telecommunications, media, and retail. Our key\ncontribution is a characterization of new MapReduce workloads which are driven\nin part by interactive analysis, and which make heavy use of query-like\nprogramming frameworks on top of MapReduce. These workloads display diverse\nbehaviors which invalidate prior assumptions about MapReduce such as uniform\ndata access, regular diurnal patterns, and prevalence of large jobs. A\nsecondary contribution is a first step towards creating a TPC-like data\nprocessing benchmark for MapReduce."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4175v1", 
    "other_authors": "Wang Lam, Lu Liu, STS Prasad, Anand Rajaraman, Zoheb Vacheri, AnHai Doan", 
    "title": "Muppet: MapReduce-Style Processing of Fast Data", 
    "arxiv-id": "1208.4175v1", 
    "author": "AnHai Doan", 
    "publish": "2012-08-21T02:53:58Z", 
    "summary": "MapReduce has emerged as a popular method to process big data. In the past\nfew years, however, not just big data, but fast data has also exploded in\nvolume and availability. Examples of such data include sensor data streams, the\nTwitter Firehose, and Facebook updates. Numerous applications must process fast\ndata. Can we provide a MapReduce-style framework so that developers can quickly\nwrite such applications and execute them over a cluster of machines, to achieve\nlow latency and high scalability? In this paper we report on our investigation\nof this question, as carried out at Kosmix and WalmartLabs. We describe\nMapUpdate, a framework like MapReduce, but specifically developed for fast\ndata. We describe Muppet, our implementation of MapUpdate. Throughout the\ndescription we highlight the key challenges, argue why MapReduce is not well\nsuited to address them, and briefly describe our current solutions. Finally, we\ndescribe our experience and lessons learned with Muppet, which has been used\nextensively at Kosmix and WalmartLabs to power a broad range of applications in\nsocial media and e-commerce."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4176v1", 
    "other_authors": "Gabriela Jacques-Silva, Bu\u011fra Gedik, Rohit Wagle, Kun-Lung Wu, Vibhore Kumar", 
    "title": "Building User-defined Runtime Adaptation Routines for Stream Processing   Applications", 
    "arxiv-id": "1208.4176v1", 
    "author": "Vibhore Kumar", 
    "publish": "2012-08-21T02:54:03Z", 
    "summary": "Stream processing applications are deployed as continuous queries that run\nfrom the time of their submission until their cancellation. This deployment\nmode limits developers who need their applications to perform runtime\nadaptation, such as algorithmic adjustments, incremental job deployment, and\napplication-specific failure recovery. Currently, developers do runtime\nadaptation by using external scripts and/or by inserting operators into the\nstream processing graph that are unrelated to the data processing logic. In\nthis paper, we describe a component called orchestrator that allows users to\nwrite routines for automatically adapting the application to runtime\nconditions. Developers build an orchestrator by registering and handling events\nas well as specifying actuations. Events can be generated due to changes in the\nsystem state (e.g., application component failures), built-in system metrics\n(e.g., throughput of a connection), or custom application metrics (e.g.,\nquality score). Once the orchestrator receives an event, users can take\nadaptation actions by using the orchestrator actuation APIs. We demonstrate the\nuse of the orchestrator in IBM's System S in the context of three different\napplications, illustrating application adaptation to changes on the incoming\ndata distribution, to application failures, and on-demand dynamic composition."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4178v1", 
    "other_authors": "Junchen Jiang, Hongji Bao, Edward Y. Chang, Yuqian Li", 
    "title": "MOIST: A Scalable and Parallel Moving Object Indexer with School   Tracking", 
    "arxiv-id": "1208.4178v1", 
    "author": "Yuqian Li", 
    "publish": "2012-08-21T02:56:43Z", 
    "summary": "Location-Based Service (LBS) is rapidly becoming the next ubiquitous\ntechnology for a wide range of mobile applications. To support applications\nthat demand nearest-neighbor and history queries, an LBS spatial indexer must\nbe able to efficiently update, query, archive and mine location records, which\ncan be in contention with each other. In this work, we propose MOIST, whose\nbaseline is a recursive spatial partitioning indexer built upon BigTable. To\nreduce update and query contention, MOIST groups nearby objects of similar\ntrajectory into the same school, and keeps track of only the history of school\nleaders. This dynamic clustering scheme can eliminate redundant updates and\nhence reduce update latency. To improve history query processing, MOIST keeps\nsome history data in memory, while it flushes aged data onto parallel disks in\na locality-preserving way. Through experimental studies, we show that MOIST can\nsupport highly efficient nearest-neighbor and history queries and can scale\nwell with an increasing number of users and update frequency."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4179v1", 
    "other_authors": "Dan R. K. Ports, Kevin Grittner", 
    "title": "Serializable Snapshot Isolation in PostgreSQL", 
    "arxiv-id": "1208.4179v1", 
    "author": "Kevin Grittner", 
    "publish": "2012-08-21T02:56:46Z", 
    "summary": "This paper describes our experience implementing PostgreSQL's new\nserializable isolation level. It is based on the recently-developed\nSerializable Snapshot Isolation (SSI) technique. This is the first\nimplementation of SSI in a production database release as well as the first in\na database that did not previously have a lock-based serializable isolation\nlevel. We reflect on our experience and describe how we overcame some of the\nresulting challenges, including the implementation of a new lock manager, a\ntechnique for ensuring memory usage is bounded, and integration with other\nPostgreSQL features. We also introduce an extension to SSI that improves\nperformance for read-only transactions. We evaluate PostgreSQL's serializable\nisolation level using several benchmarks and show that it achieves performance\nonly slightly below that of snapshot isolation, and significantly outperforms\nthe traditional two-phase locking approach on read-intensive workloads."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4270v1", 
    "other_authors": "Kyu-Young Whang, Tae-Seob Yun, Yeon-Mi Yeo, Il-Yeol Song, Hyuk-Yoon Kwon, In-Joong Kim", 
    "title": "ODYS: A Massively-Parallel Search Engine Using a DB-IR   Tightly-Integrated Parallel DBMS", 
    "arxiv-id": "1208.4270v1", 
    "author": "In-Joong Kim", 
    "publish": "2012-08-21T14:01:36Z", 
    "summary": "Recently, parallel search engines have been implemented based on scalable\ndistributed file systems such as Google File System. However, we claim that\nbuilding a massively-parallel search engine using a parallel DBMS can be an\nattractive alternative since it supports a higher-level (i.e., SQL-level)\ninterface than that of a distributed file system for easy and less error-prone\napplication development while providing scalability. In this paper, we propose\na new approach of building a massively-parallel search engine using a DB-IR\ntightly-integrated parallel DBMS and demonstrate its commercial-level\nscalability and performance. In addition, we present a hybrid (i.e., analytic\nand experimental) performance model for the parallel search engine. We have\nbuilt a five-node parallel search engine according to the proposed architecture\nusing a DB-IR tightly-integrated DBMS. Through extensive experiments, we show\nthe correctness of the model by comparing the projected output with the\nexperimental results of the five-node engine. Our model demonstrates that ODYS\nis capable of handling 1 billion queries per day (81 queries/sec) for 30\nbillion web pages by using only 43,472 nodes with an average query response\ntime of 211 ms, which is equivalent to or better than those of commercial\nsearch engines. We also show that, by using twice as many (86,944) nodes, ODYS\ncan provide an average query response time of 162 ms, which is significantly\nlower than those of commercial search engines."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.4809v1", 
    "other_authors": "H. Venkateswara Reddy, Dr. S. Viswanadha Raju, B. Ramasubba Reddy", 
    "title": "Comparing N-Node Set Importance Representative results with Node   Importance Representative results for Categorical Clustering: An exploratory   study", 
    "arxiv-id": "1208.4809v1", 
    "author": "B. Ramasubba Reddy", 
    "publish": "2012-08-23T17:32:32Z", 
    "summary": "The proportionate increase in the size of the data with increase in space\nimplies that clustering a very large data set becomes difficult and is a time\nconsuming process.Sampling is one important technique to scale down the size of\ndataset and to improve the efficiency of clustering. After sampling allocating\nunlabeled objects into proper clusters is impossible in the categorical\ndomain.To address the problem, Chen employed a method called MAximal\nRepresentative Data Labeling to allocate each unlabeled data point to the\nappropriate cluster based on Node Importance Representative and N-Node\nImportance Representative algorithms. This paper took off from Chen s\ninvestigation and analyzed and compared the results of NIR and NNIR leading to\nthe conclusion that the two processes contradict each other when it comes to\nfinding the resemblance between an unlabeled data point and a cluster.A new and\nbetter way of solving the problem was arrived at that finds resemblance between\nunlabeled data point within all clusters, while also providing maximal\nresemblance for allocation of data in the required cluster."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.5443v1", 
    "other_authors": "Bing-Rong Lin, Daniel Kifer", 
    "title": "A Framework for Extracting Semantic Guarantees from Privacy", 
    "arxiv-id": "1208.5443v1", 
    "author": "Daniel Kifer", 
    "publish": "2012-08-27T16:50:53Z", 
    "summary": "Statistical privacy views privacy definitions as contracts that guide the\nbehavior of algorithms that take in sensitive data and produce sanitized data.\nFor most existing privacy definitions, it is not clear what they actually\nguarantee.\n  In this paper, we propose the first (to the best of our knowledge) framework\nfor extracting semantic guarantees from privacy definitions. That is, instead\nof answering narrow questions such as \"does privacy definition Y protect X?\"\nthe goal is to answer the more general question \"what does privacy definition Y\nprotect?\"\n  The privacy guarantees we can extract are Bayesian in nature and deal with\nchanges in an attacker's beliefs. The key to our framework is an object we call\nthe row cone. Every privacy definition has a row cone, which is a convex set\nthat describes all the ways an attacker's prior beliefs can be turned into\nposterior beliefs after observing an output of an algorithm satisfying that\nprivacy definition.\n  The framework can be applied to privacy definitions or even to individual\nalgorithms to identify the types of inferences they defend against. We\nillustrate the use of our framework with analyses of several definitions and\nalgorithms for which we can derive previously unknown semantics. These include\nrandomized response, FRAPP, and several algorithms that add integer-valued\nnoise to their inputs."
},{
    "category": "cs.DB", 
    "doi": "10.1108/17410401111112014", 
    "link": "http://arxiv.org/pdf/1208.5745v1", 
    "other_authors": "Rohit Raghunathan, Sushovan De, Subbarao Kambhampati", 
    "title": "Bayes Networks for Supporting Query Processing Over Incomplete   Autonomous Databases", 
    "arxiv-id": "1208.5745v1", 
    "author": "Subbarao Kambhampati", 
    "publish": "2012-08-28T18:52:12Z", 
    "summary": "As the information available to lay users through autonomous data sources\ncontinues to increase, mediators become important to ensure that the wealth of\ninformation available is tapped effectively. A key challenge that these\ninformation mediators need to handle is the varying levels of incompleteness in\nthe underlying databases in terms of missing attribute values. Existing\napproaches such as QPIAD aim to mine and use Approximate Functional\nDependencies (AFDs) to predict and retrieve relevant incomplete tuples. These\napproaches make independence assumptions about missing values---which\ncritically hobbles their performance when there are tuples containing missing\nvalues for multiple correlated attributes. In this paper, we present a\nprincipled probabilistic alternative that views an incomplete tuple as defining\na distribution over the complete tuples that it stands for. We learn this\ndistribution in terms of Bayes networks. Our approach involves\nmining/\"learning\" Bayes networks from a sample of the database, and using it to\ndo both imputation (predict a missing value) and query rewriting (retrieve\nrelevant results with incompleteness on the query-constrained attributes, when\nthe data sources are autonomous). We present empirical studies to demonstrate\nthat (i) at higher levels of incompleteness, when multiple attribute values are\nmissing, Bayes networks do provide a significantly higher classification\naccuracy and (ii) the relevant possible answers retrieved by the queries\nreformulated using Bayes networks provide higher precision and recall than AFDs\nwhile keeping query processing costs manageable."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.0378v2", 
    "other_authors": "C. V. Dam\u00e1sio, A. Analyti, G. Antoniou", 
    "title": "Provenance for SPARQL queries", 
    "arxiv-id": "1209.0378v2", 
    "author": "G. Antoniou", 
    "publish": "2012-09-03T15:04:55Z", 
    "summary": "Determining trust of data available in the Semantic Web is fundamental for\napplications and users, in particular for linked open data obtained from SPARQL\nendpoints. There exist several proposals in the literature to annotate SPARQL\nquery results with values from abstract models, adapting the seminal works on\nprovenance for annotated relational databases. We provide an approach capable\nof providing provenance information for a large and significant fragment of\nSPARQL 1.1, including for the first time the major non-monotonic constructs\nunder multiset semantics. The approach is based on the translation of SPARQL\ninto relational queries over annotated relations with values of the most\ngeneral m-semiring, and in this way also refuting a claim in the literature\nthat the OPTIONAL construct of SPARQL cannot be captured appropriately with the\nknown abstract models."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.1794v1", 
    "other_authors": "Saida Aissa, Mohamed Salah Gouider", 
    "title": "A New Similairty Measure For Spatial Personalization", 
    "arxiv-id": "1209.1794v1", 
    "author": "Mohamed Salah Gouider", 
    "publish": "2012-09-09T11:54:39Z", 
    "summary": "Extracting the relevant information by exploiting the spatial data warehouse\nbecomes increasingly hard. In fact, because of the enormous amount of data\nstored in the spatial data warehouse, the user, usually, don't know what part\nof the cube contain the relevant information and what the forthcoming query\nshould be. As a solution, we propose to study the similarity between the\nbehaviors of the users, in term of the spatial MDX queries launched on the\nsystem, as a basis to recommend the next relevant MDX query to the current\nuser. This paper introduces a new similarity measure for comparing spatial MDX\nqueries. The proposed similarity measure could directly support the development\nof spatial personalization approaches. The proposed similarity measure takes\ninto account the basic components of the similarity assessment models: the\ntopology, the direction and the distance."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.2647v1", 
    "other_authors": "Jason T. Liu", 
    "title": "Shadow Theory, data model design for data integration", 
    "arxiv-id": "1209.2647v1", 
    "author": "Jason T. Liu", 
    "publish": "2012-09-12T15:48:33Z", 
    "summary": "For data integration in information ecosystems, semantic heterogeneity is a\nknown difficulty. In this paper, we propose Shadow Theory as the philosophical\nfoundation to address this issue. It is based on the notion of shadows in\nPlato's Allegory of the Cave. What we can observe are just shadows, and\nmeanings of shadows are mental entities that only exist in viewers' cognitive\nstructures. With enterprise customer data integration example, we proposed six\ndesign principles and algebra to support required operations."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.2794v1", 
    "other_authors": "Hakik Paci, Elinda Kajo Mece, Aleksander Xhuvani", 
    "title": "Protecting oracle pl/sql source code from a dba user", 
    "arxiv-id": "1209.2794v1", 
    "author": "Aleksander Xhuvani", 
    "publish": "2012-09-13T07:07:24Z", 
    "summary": "In this paper we are presenting a new way to disable DDL statements on some\nspecific PL/SQL procedures to a dba user in the Oracle database. Nowadays dba\nusers have access to a lot of data and source code even if they do not have\nlegal permissions to see or modify them. With this method we can disable the\nability to execute DDL and DML statements on some specific pl/sql procedures\nfrom every Oracle database user even if it has a dba role. Oracle gives to\ndeveloper the possibility to wrap the pl/sql procedures, functions and packages\nbut those wrapped scripts can be unwrapped by using third party tools. The\nscripts that we have developed analyzes all database sessions, and if they\ndetect a DML or a DDL statement from an unauthorized user to procedure,\nfunction or package which should be protected then the execution of the\nstatement is denied. Furthermore, these scripts do not allow a dba user to drop\nor disable the scripts themselves. In other words by managing sessions prior to\nthe execution of an eventual statement from a dba user, we can prevent the\nexecution of eventual statements which target our scripts."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.3756v3", 
    "other_authors": "Charalampos Nikolaou, Manolis Koubarakis", 
    "title": "Incomplete Information in RDF", 
    "arxiv-id": "1209.3756v3", 
    "author": "Manolis Koubarakis", 
    "publish": "2012-09-17T19:10:38Z", 
    "summary": "We extend RDF with the ability to represent property values that exist, but\nare unknown or partially known, using constraints. Following ideas from the\nincomplete information literature, we develop a semantics for this extension of\nRDF, called RDFi, and study SPARQL query evaluation in this framework."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.3943v1", 
    "other_authors": "Wafa Tebourski Ourida Ben Boubaker Saidi", 
    "title": "Formal Concept Analysis Based Association Rules Extraction", 
    "arxiv-id": "1209.3943v1", 
    "author": "Wafa Tebourski Ourida Ben Boubaker Saidi", 
    "publish": "2012-09-18T13:04:01Z", 
    "summary": "Generating a huge number of association rules reduces their utility in the\ndecision making process, done by domain experts. In this context, based on the\ntheory of Formal Concept Analysis, we propose to extend the notion of Formal\nConcept through the generalization of the notion of itemset in order to\nconsider the itemset as an intent, its support as the cardinality of the extent\nand its relevance which is related to the confidence of rule. Accordingly, we\npropose a new approach to extract interesting itemsets through the concept\ncoverage. This approach uses a new quality-criteria of a rule: the relevance\nbringing a semantic added value to formal concept analysis approach to discover\nassociation rules."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.3944v1", 
    "other_authors": "Wafa Tebourski Wahiba Ben Abdessalem Karaa", 
    "title": "Cyclic Association Rules Mining under Constraints", 
    "arxiv-id": "1209.3944v1", 
    "author": "Wafa Tebourski Wahiba Ben Abdessalem Karaa", 
    "publish": "2012-09-18T13:09:24Z", 
    "summary": "Several researchers have explored the temporal aspect of association rules\nmining. In this paper, we focus on the cyclic association rules, in order to\ndiscover correlations among items characterized by regular cyclic variation\novertime. The overview of the state of the art has revealed the drawbacks of\nproposed algorithm literatures, namely the excessive number of generated rules\nwhich are not meeting the expert's expectations. To overcome these\nrestrictions, we have introduced our approach dedicated to generate the cyclic\nassociation rules under constraints through a new method called\nConstraint-Based Cyclic Association Rules CBCAR. The carried out experiments\nunderline the usefulness and the performance of our new approach."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.4169v1", 
    "other_authors": "Doreswamy, Hemanth K. S", 
    "title": "Hybrid Data Mining Technique for Knowledge Discovery from Engineering   Materials' Data sets", 
    "arxiv-id": "1209.4169v1", 
    "author": "Hemanth K. S", 
    "publish": "2012-09-19T07:23:02Z", 
    "summary": "Studying materials informatics from a data mining perspective can be\nbeneficial for manufacturing and other industrial engineering applications.\nPredictive data mining technique and machine learning algorithm are combined to\ndesign a knowledge discovery system for the selection of engineering materials\nthat meet the design specifications. Predictive method-Naive Bayesian\nclassifier and Machine learning Algorithm - Pearson correlation coefficient\nmethod were implemented respectively for materials classification and\nselection. The knowledge extracted from the engineering materials data sets is\nproposed for effective decision making in advanced engineering materials design\napplications."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.5244v1", 
    "other_authors": "K. C. Srikantaiah, P. L. Srikanth, V. Tejaswi, K. Shaila, K. R. Venugopal, L. M. Patnaik", 
    "title": "Ranking Search Engine Result Pages based on Trustworthiness of Websites", 
    "arxiv-id": "1209.5244v1", 
    "author": "L. M. Patnaik", 
    "publish": "2012-09-24T12:24:49Z", 
    "summary": "The World Wide Web (WWW) is the repository of large number of web pages which\ncan be accessed via Internet by multiple users at the same time and therefore\nit is Ubiquitous in nature. The search engine is a key application used to\nsearch the web pages from this huge repository, which uses the link analysis\nfor ranking the web pages without considering the facts provided by them. A new\nalgorithm called Probability of Correctness of Facts(PCF)-Engine is proposed to\nfind the accuracy of the facts provided by the web pages. It uses the\nProbability based similarity function (SIM) which performs the string matching\nbetween the true facts and the facts of web pages to find their probability of\ncorrectness. The existing semantic search engines, may give the relevant result\nto the user query but may not be 100% accurate. Our algorithm computes\ntrustworthiness of websites to rank the web pages. Simulation results show that\nour approach is efficient when compared with existing Voting and Truthfinder[1]\nalgorithms with respect to the trustworthiness of the websites."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.5426v2", 
    "other_authors": "Tanvir Ahmed, Mohammad Saiedur Rahaman, Mohammad Saidur Rahman, Manzur H. Khan", 
    "title": "A Coherent Distributed Grid Service for Assimilation and Unification of   Heterogeneous Data Source", 
    "arxiv-id": "1209.5426v2", 
    "author": "Manzur H. Khan", 
    "publish": "2012-09-24T21:17:09Z", 
    "summary": "Grid services are heavily used for handling large distributed computations.\nThey are also very useful to handle heavy data intensive applications where\ndata are distributed in different sites. Most of the data grid services used in\nsuch situations are meant for homogeneous data source. In case of Heterogeneous\ndata sources, most of the grid services that are available are designed such a\nway that they must be identical in schema definition for their smooth\noperation. But there can be situations where the grid site databases are\nheterogeneous and their schema definition is different from the central schema\ndefinition. In this paper we propose a light weight coherent grid service for\nheterogeneous data sources that is very easily install. It can map and convert\nthe central SQL schema into that of the grid members and send queries to get\naccording results from heterogeneous data sources."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.5598v3", 
    "other_authors": "Fan Min", 
    "title": "Granular association rules on two universes with four measures", 
    "arxiv-id": "1209.5598v3", 
    "author": "Fan Min", 
    "publish": "2012-09-25T13:13:11Z", 
    "summary": "Relational association rules reveal patterns hide in multiple tables.\nExisting rules are usually evaluated through two measures, namely support and\nconfidence. However, these two measures may not be enough to describe the\nstrength of a rule. In this paper, we introduce granular association rules with\nfour measures to reveal connections between granules in two universes, and\npropose three algorithms for rule mining. An example of such a rule might be\n\"40% men like at least 30% kinds of alcohol; 45% customers are men and 6%\nproducts are alcohol.\" Here 45%, 6%, 40%, and 30% are the source coverage, the\ntarget coverage, the source confidence, and the target confidence,\nrespectively. With these measures, our rules are semantically richer than\nexisting ones. Three subtypes of rules are obtained through considering special\nrequirements on the source/target confidence. Then we define a rule mining\nproblem, and design a sandwich algorithm with different rule checking\napproaches for different subtypes. Experiments on a real world dataset show\nthat the approaches dedicated to three subtypes are 2-3 orders of magnitudes\nfaster than the one for the general case. A forward algorithm and a backward\nalgorithm for one particular subtype can speed up the mining process further.\nThis work opens a new research trend concerning relational association rule\nmining, granular computing and rough sets."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.5625v1", 
    "other_authors": "Robert Smith", 
    "title": "Managing Complex Structured Data In a Fast Evolving Environment", 
    "arxiv-id": "1209.5625v1", 
    "author": "Robert Smith", 
    "publish": "2012-09-25T14:37:48Z", 
    "summary": "Criminal data comes in a variety of formats, mandated by state, federal, and\ninternational standards. Specifying the data in a unified fashion is necessary\nfor any system that intends to integrate with state, federal, and international\nlaw enforcement agencies. However, the contents, format, and structure of the\ndata is highly inconsistent across jurisdictions, and each datum requires\ndifferent ways of being printed, transmitted, and displayed. The goal was to\ndesign a system that is unified in its approach to specify data, and is\namenable to future \"unknown unknowns\". We have developed a domain-specific\nlanguage in Common Lisp which allows the specification of complex data with\nevolving formats and structure, and is inter-operable with the Common Lisp\nlanguage. The resultant system has enabled the easy handling of complex\nevolving information in the general criminal data environment and has made it\npossible to manage and extend the system in a high-paced market. The language\nhas allowed the principal product of Secure Outcomes Inc. to enjoy success with\nover 50 users throughout the United States."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.6297v1", 
    "other_authors": "Pratima Gautam, Rahul Shukla", 
    "title": "An Efficient Algorithm for Mining Multilevel Association Rule Based on   Pincer Search", 
    "arxiv-id": "1209.6297v1", 
    "author": "Rahul Shukla", 
    "publish": "2012-09-27T17:29:58Z", 
    "summary": "Discovering frequent itemset is a key difficulty in significant data mining\napplications, such as the discovery of association rules, strong rules,\nepisodes, and minimal keys. The problem of developing models and algorithms for\nmultilevel association mining poses for new challenges for mathematics and\ncomputer science. In this paper, we present a model of mining multilevel\nassociation rules which satisfies the different minimum support at each level,\nwe have employed princer search concepts, multilevel taxonomy and different\nminimum supports to find multilevel association rules in a given transaction\ndata set. This search is used only for maintaining and updating a new data\nstructure. It is used to prune early candidates that would normally encounter\nin the top-down search. A main characteristic of the algorithms is that it does\nnot require explicit examination of every frequent itemsets, an example is also\ngiven to demonstrate and support that the proposed mining algorithm can derive\nthe multiple-level association rules under different supports in a simple and\neffective manner"
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1209.6490v1", 
    "other_authors": "Istv\u00e1n Csabai, M\u00e1rton Trencs\u00e9ni, G\u00e9za Herczegh, L\u00e1szl\u00f3 Dobos, P\u00e9ter J\u00f3zsa, Norbert Purger, Tam\u00e1s Budav\u00e1ri, Alexander Szalay", 
    "title": "Spatial Indexing of Large Multidimensional Databases", 
    "arxiv-id": "1209.6490v1", 
    "author": "Alexander Szalay", 
    "publish": "2012-09-28T11:47:00Z", 
    "summary": "Scientific endeavors such as large astronomical surveys generate databases on\nthe terabyte scale. These, usually multidimensional databases must be\nvisualized and mined in order to find interesting objects or to extract\nmeaningful and qualitatively new relationships. Many statistical algorithms\nrequired for these tasks run reasonably fast when operating on small sets of\nin-memory data, but take noticeable performance hits when operating on large\ndatabases that do not fit into memory. We utilize new software technologies to\ndevelop and evaluate fast multidimensional indexing schemes that inherently\nfollow the underlying, highly non-uniform distribution of the data: they are\nlayered uniform grid indices, hierarchical binary space partitioning, and\nsampled flat Voronoi tessellation of the data. Our working database is the\n5-dimensional magnitude space of the Sloan Digital Sky Survey with more than\n270 million data points, where we show that these techniques can dramatically\nspeed up data mining operations such as finding similar objects by example,\nclassifying objects or comparing extensive simulation sets with observations.\nWe are also developing tools to interact with the multidimensional database and\nvisualize the data at multiple resolutions in an adaptive manner."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.1040v1", 
    "other_authors": "Sharon Christa, K. Lakshmi Madhuri, V. Suma", 
    "title": "A Comparative Analysis of Data Mining Tools in Agent Based Systems", 
    "arxiv-id": "1210.1040v1", 
    "author": "V. Suma", 
    "publish": "2012-10-03T09:19:19Z", 
    "summary": "World wide technological advancement has brought in a widespread change in\nadoption and utilization of open source tools. Since, most of the organizations\nacross the globe deal with a large amount of data to be updated online and\ntransactions are made every second, managing, mining and processing this\ndynamic data is very complex. Successful implementation of the data mining\ntechnique requires a careful assessment of the various tools and algorithms\navailable to mining experts. This paper provides a comparative study of open\nsource data mining tools available to the professionals. Parameters influencing\nthe choice of apt tools in addition to the real time challenges are discussed.\nHowever, it is well proven that agents aid in improving the performance of data\nmining tools. This paper provides information on an agent-based framework for\ndata preprocessing with implementation details for the development of better\ntool in the market. An integration of open source data mining tools with agent\nsimulation enable one to implement an effective data pre processing\narchitecture thereby providing robust capabilities of the application which can\nbe upgraded using a minimum of pre planning requirement from the application\ndeveloper."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.2872v1", 
    "other_authors": "Tipawan Silwattananusarn, Kulthida Tuamsuk", 
    "title": "Data Mining and Its Applications for Knowledge Management: A Literature   Review from 2007 to 2012", 
    "arxiv-id": "1210.2872v1", 
    "author": "Kulthida Tuamsuk", 
    "publish": "2012-10-10T11:12:13Z", 
    "summary": "Data mining is one of the most important steps of the knowledge discovery in\ndatabases process and is considered as significant subfield in knowledge\nmanagement. Research in data mining continues growing in business and in\nlearning organization over coming decades. This review paper explores the\napplications of data mining techniques which have been developed to support\nknowledge management process. The journal articles indexed in ScienceDirect\nDatabase from 2007 to 2012 are analyzed and classified. The discussion on the\nfindings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types\nand/or knowledge datasets; (iii) data mining tasks; and (iv) data mining\ntechniques and applications used in knowledge management. The article first\nbriefly describes the definition of data mining and data mining functionality.\nThen the knowledge management rationale and major knowledge management tools\nintegrated in knowledge management cycle are described. Finally, the\napplications of data mining techniques in the process of knowledge management\nare summarized and discussed."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.4405v2", 
    "other_authors": "Hong Sun, Kristof Depraetere, Jos De Roo, Boris De Vloed, Giovanni Mels, Dirk Colaert", 
    "title": "Semantic integration and analysis of clinical data", 
    "arxiv-id": "1210.4405v2", 
    "author": "Dirk Colaert", 
    "publish": "2012-10-05T09:49:35Z", 
    "summary": "There is a growing need to semantically process and integrate clinical data\nfrom different sources for Clinical Data Management and Clinical Decision\nSupport in the healthcare IT industry. In the clinical practice domain, the\nsemantic gap between clinical information systems and domain ontologies is\nquite often difficult to bridge in one step. In this paper, we report our\nexperience in using a two-step formalization approach to formalize clinical\ndata, i.e. from database schemas to local formalisms and from local formalisms\nto domain (unifying) formalisms. We use N3 rules to explicitly and formally\nstate the mapping from local ontologies to domain ontologies. The resulting\ndata expressed in domain formalisms can be integrated and analyzed, though\noriginating from very distinct sources. Practices of applying the two-step\napproach in the infectious disorders and cancer domains are introduced."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.5403v1", 
    "other_authors": "Andreas Schwarte, Peter Haase, Michael Schmidt, Katja Hose, Ralf Schenkel", 
    "title": "An Experience Report of Large Scale Federations", 
    "arxiv-id": "1210.5403v1", 
    "author": "Ralf Schenkel", 
    "publish": "2012-10-19T12:39:17Z", 
    "summary": "We present an experimental study of large-scale RDF federations on top of the\nBio2RDF data sources, involving 29 data sets with more than four billion RDF\ntriples deployed in a local federation. Our federation is driven by FedX, a\nhighly optimized federation mediator for Linked Data. We discuss design\ndecisions, technical aspects, and experiences made in setting up and optimizing\nthe Bio2RDF federation, and present an exhaustive experimental evaluation of\nthe federation scenario. In addition to a controlled setting with local\nfederation members, we study implications arising in a hybrid setting, where\nlocal federation members interact with remote federation members exhibiting\nhigher network latency. The outcome demonstrates the feasibility of federated\nsemantic data management in general and indicates remaining bottlenecks and\nresearch opportunities that shall serve as a guideline for future work in the\narea of federated semantic data processing."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.5980v1", 
    "other_authors": "Tim Furche, Georg Gottlob, Giovanni Grasso, Xiaonan Guo, Giorgio Orsi, Christian Schallhart", 
    "title": "The Ontological Key: Automatically Understanding and Integrating Forms   to Access the Deep Web", 
    "arxiv-id": "1210.5980v1", 
    "author": "Christian Schallhart", 
    "publish": "2012-10-22T17:38:29Z", 
    "summary": "Forms are our gates to the web. They enable us to access the deep content of\nweb sites. Automatic form understanding provides applications, ranging from\ncrawlers over meta-search engines to service integrators, with a key to this\ncontent. Yet, it has received little attention other than as component in\nspecific applications such as crawlers or meta-search engines. No comprehensive\napproach to form understanding exists, let alone one that produces rich models\nfor semantic services or integration with linked open data.\n  In this paper, we present OPAL, the first comprehensive approach to form\nunderstanding and integration. We identify form labeling and form\ninterpretation as the two main tasks involved in form understanding. On both\nproblems OPAL pushes the state of the art: For form labeling, it combines\nfeatures from the text, structure, and visual rendering of a web page. In\nextensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern\nweb forms OPAL outperforms previous approaches for form labeling by a\nsignificant margin. For form interpretation, OPAL uses a schema (or ontology)\nof forms in a given domain. Thanks to this domain schema, it is able to produce\nnearly perfect (more than 97 percent accuracy in the evaluation domains) form\ninterpretations. Yet, the effort to produce a domain schema is very low, as we\nprovide a Datalog-based template language that eases the specification of such\nschemata and a methodology for deriving a domain schema largely automatically\nfrom an existing domain ontology. We demonstrate the value of the form\ninterpretations in OPAL through a light-weight form integration system that\nsuccessfully translates and distributes master queries to hundreds of forms\nwith no error, yet is implemented with only a handful translation rules."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.5984v1", 
    "other_authors": "Tim Furche, Georg Gottlob, Giovanni Grasso, Giorgio Orsi, Christian Schallhart, Cheng Wang", 
    "title": "AMBER: Automatic Supervision for Multi-Attribute Extraction", 
    "arxiv-id": "1210.5984v1", 
    "author": "Cheng Wang", 
    "publish": "2012-10-22T17:58:07Z", 
    "summary": "The extraction of multi-attribute objects from the deep web is the bridge\nbetween the unstructured web and structured data. Existing approaches either\ninduce wrappers from a set of human-annotated pages or leverage repeated\nstructures on the page without supervision. What the former lack in automation,\nthe latter lack in accuracy. Thus accurate, automatic multi-attribute object\nextraction has remained an open challenge.\n  AMBER overcomes both limitations through mutual supervision between the\nrepeated structure and automatically produced annotations. Previous approaches\nbased on automatic annotations have suffered from low quality due to the\ninherent noise in the annotations and have attempted to compensate by exploring\nmultiple candidate wrappers. In contrast, AMBER compensates for this noise by\nintegrating repeated structure analysis with annotation-based induction: The\nrepeated structure limits the search space for wrapper induction, and\nconversely, annotations allow the repeated structure analysis to distinguish\nnoise from relevant data. Both, low recall and low precision in the annotations\nare mitigated to achieve almost human quality (more than 98 percent)\nmulti-attribute object extraction.\n  To achieve this accuracy, AMBER needs to be trained once for an entire\ndomain. AMBER bootstraps its training from a small, possibly noisy set of\nattribute instances and a few unannotated sites of the domain."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.6242v1", 
    "other_authors": "Lena Wiese", 
    "title": "Enhancing Algebraic Query Relaxation with Semantic Similarity", 
    "arxiv-id": "1210.6242v1", 
    "author": "Lena Wiese", 
    "publish": "2012-10-23T14:22:02Z", 
    "summary": "Cooperative database systems support a database user by searching for answers\nthat are closely related to his query and hence are informative answers. Common\noperators to relax the user query are Dropping Condition, Anti-Instantiation\nand Goal Replacement. In this article, we provide an algebraic version of these\noperators. Moreover we propose some heuristics to assign a degree of similarity\nto each tuple of an answer table; this degree can help the user to determine\nwhether this answer is relevant for him or not."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.6272v2", 
    "other_authors": "Rebeca Schroeder, Ronaldo Santos Mello, Carmem Satie Hara", 
    "title": "Affinity-based XML Fragmentation", 
    "arxiv-id": "1210.6272v2", 
    "author": "Carmem Satie Hara", 
    "publish": "2012-10-23T15:46:23Z", 
    "summary": "In this paper we tackle the fragmentation problem for highly distributed\ndatabases. In such an environment, a suitable fragmentation strategy may\nprovide scalability and availability by minimizing distributed transactions. We\npropose an approach for XML fragmentation that takes as input both the\napplication's expected workload and a storage threshold, and produces as output\nan XML fragmentation schema. Our workload-aware method aims to minimize the\nexecution of distributed transactions by packing up related data in a small set\nof fragments. We present experiments that compare alternative fragmentation\nschemas, showing that the one produced by our technique provides a\nfiner-grained result and better system throughput."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.6746v1", 
    "other_authors": "Hossain Mahmud, Ashfaq Mahmood Amin, Mohammed Eunus Ali, Tanzima Hashem", 
    "title": "Shared Execution of Path Queries on Road Networks", 
    "arxiv-id": "1210.6746v1", 
    "author": "Tanzima Hashem", 
    "publish": "2012-10-25T06:30:51Z", 
    "summary": "The advancement of mobile technologies and the proliferation of map-based\napplications have enabled a user to access a wide variety of services that\nrange from information queries to navigation systems. Due to the popularity of\nmap-based applications among the users, the service provider often requires to\nanswer a large number of simultaneous queries. Thus, processing queries\nefficiently on spatial networks (i.e., road networks) have become an important\nresearch area in recent years. In this paper, we focus on path queries that\nfind the shortest path between a source and a destination of the user. In\nparticular, we address the problem of finding the shortest paths for a large\nnumber of simultaneous path queries in road networks. Traditional systems that\nconsider one query at a time are not suitable for many applications due to high\ncomputational and service costs. These systems cannot guarantee required\nresponse time in high load conditions. We propose an efficient group based\napproach that provides a practical solution with reduced cost. The key concept\nfor our approach is to group queries that share a common travel path and then\ncompute the shortest path for the group. Experimental results show that our\napproach is on an average ten times faster than the traditional approach in\nreturn of sacrificing the accuracy by 0.5% in the worst case, which is\nacceptable for most of the users."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1210.8229v1", 
    "other_authors": "Jnanamurthy H. K., Vishesh H. V., Vishruth Jain, Preetham Kumar, Radhika M. Pai", 
    "title": "Top Down Approach to find Maximal Frequent Item Sets using Subset   Creation", 
    "arxiv-id": "1210.8229v1", 
    "author": "Radhika M. Pai", 
    "publish": "2012-10-31T05:09:32Z", 
    "summary": "Association rule has been an area of active research in the field of\nknowledge discovery. Data mining researchers had improved upon the quality of\nassociation rule mining for business development by incorporating influential\nfactors like value (utility), quantity of items sold (weight) and more for the\nmining of association patterns. In this paper, we propose an efficient approach\nto find maximal frequent itemset first. Most of the algorithms in literature\nused to find minimal frequent item first, then with the help of minimal\nfrequent itemsets derive the maximal frequent itemsets. These methods consume\nmore time to find maximal frequent itemsets. To overcome this problem, we\npropose a navel approach to find maximal frequent itemset directly using the\nconcepts of subsets. The proposed method is found to be efficient in finding\nmaximal frequent itemsets."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.0176v1", 
    "other_authors": "Matteo Magnani, Danilo Montesi", 
    "title": "Joining relations under discrete uncertainty", 
    "arxiv-id": "1211.0176v1", 
    "author": "Danilo Montesi", 
    "publish": "2012-11-01T13:45:04Z", 
    "summary": "In this paper we introduce and experimentally compare alternative algorithms\nto join uncertain relations. Different algorithms are based on specific\nprinciples, e.g., sorting, indexing, or building intermediate relational tables\nto apply traditional approaches. As a consequence their performance is affected\nby different features of the input data, and each algorithm is shown to be more\nefficient than the others in specific cases. In this way statistics explicitly\nrepresenting the amount and kind of uncertainty in the input uncertain\nrelations can be used to choose the most efficient algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.0224v1", 
    "other_authors": "Lorena Etcheverry, Alejandro A. Vaisman", 
    "title": "Views over RDF Datasets: A State-of-the-Art and Open Challenges", 
    "arxiv-id": "1211.0224v1", 
    "author": "Alejandro A. Vaisman", 
    "publish": "2012-11-01T17:00:27Z", 
    "summary": "Views on RDF datasets have been discussed in several works, nevertheless\nthere is no consensus on their definition nor the requirements they should\nfulfill. In traditional data management systems, views have proved to be useful\nin different application scenarios such as data integration, query answering,\ndata security, and query modularization.\n  In this work we have reviewed existent work on views over RDF datasets, and\ndiscussed the application of existent view definition mechanisms to four\nscenarios in which views have proved to be useful in traditional (relational)\ndata management systems. To give a framework for the discussion we provided a\ndefinition of views over RDF datasets, an issue over which there is no\nconsensus so far. We finally chose the three proposals closer to this\ndefinition, and analyzed them with respect to four selected goals."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.1565v1", 
    "other_authors": "Michael Hausenblas, Boris Villazon-Terrazas, Richard Cyganiak", 
    "title": "Data Shapes and Data Transformations", 
    "arxiv-id": "1211.1565v1", 
    "author": "Richard Cyganiak", 
    "publish": "2012-11-07T14:54:46Z", 
    "summary": "Nowadays, information management systems deal with data originating from\ndifferent sources including relational databases, NoSQL data stores, and Web\ndata formats, varying not only in terms of data formats, but also in the\nunderlying data model. Integrating data from heterogeneous data sources is a\ntime-consuming and error-prone engineering task; part of this process requires\nthat the data has to be transformed from its original form to other forms,\nrepeating all along the life cycle. With this report we provide a principled\noverview on the fundamental data shapes tabular, tree, and graph as well as\ntransformations between them, in order to gain a better understanding for\nperforming said transformations more efficiently and effectively."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.1788v1", 
    "other_authors": "Dipti Patil, Dr. Vijay M. Wadhai, Mayuri Gund, Richa Biyani, Snehal Andhalkar, Bhagyashree Agrawal", 
    "title": "An Adaptive parameter free data mining approach for healthcare   application", 
    "arxiv-id": "1211.1788v1", 
    "author": "Bhagyashree Agrawal", 
    "publish": "2012-11-08T08:29:50Z", 
    "summary": "In today's world, healthcare is the most important factor affecting human\nlife. Due to heavy work load it is not possible for personal healthcare. The\nproposed system acts as a preventive measure for determining whether a person\nis fit or unfit based on person's historical and real time data by applying\nclustering algorithms like K-means and D-stream. The Density-based clustering\nalgorithm i.e. the D-stream algorithm overcomes drawbacks of K-Means algorithm.\nBy calculating their performance measures we finally find out effectiveness and\nefficiency of both the algorithms. Both clustering algorithms are applied on\npatient's bio-medical historical database. To check the correctness of both the\nalgorithms, we apply them on patient's current bio-medical data."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.2367v1", 
    "other_authors": "Ada Wai-Chee Fu, Huanhuan Wu, James Cheng, Shumo Chu, Raymond Chi-Wing Wong", 
    "title": "IS-LABEL: an Independent-Set based Labeling Scheme for Point-to-Point   Distance Querying on Large Graphs", 
    "arxiv-id": "1211.2367v1", 
    "author": "Raymond Chi-Wing Wong", 
    "publish": "2012-11-11T01:59:11Z", 
    "summary": "We study the problem of computing shortest path or distance between two query\nvertices in a graph, which has numerous important applications. Quite a number\nof indexes have been proposed to answer such distance queries. However, all of\nthese indexes can only process graphs of size barely up to 1 million vertices,\nwhich is rather small in view of many of the fast-growing real-world graphs\ntoday such as social networks and Web graphs. We propose an efficient index,\nwhich is a novel labeling scheme based on the independent set of a graph. We\nshow that our method can handle graphs of size three orders of magnitude larger\nthan those existing indexes."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-642-35176-1_39", 
    "link": "http://arxiv.org/pdf/1211.3016v1", 
    "other_authors": "Enrico Franconi, Paolo Guagliardo", 
    "title": "The View Update Problem Revisited", 
    "arxiv-id": "1211.3016v1", 
    "author": "Paolo Guagliardo", 
    "publish": "2012-11-13T15:07:48Z", 
    "summary": "In this paper, we revisit the view update problem in a relational setting and\npropose a framework based on the notion of determinacy under constraints.\nWithin such a framework, we characterise when a view mapping is invertible,\nestablishing that this is the case precisely when each database symbol has an\nexact rewriting in terms of the view symbols under the given constraints, and\nwe provide a general effective criterion to understand whether the changes\nintroduced by a view update can be propagated to the underlying database\nrelations in a unique and unambiguous way.\n  Afterwards, we show how determinacy under constraints can be checked, and\nrewritings effectively found, in three different relevant scenarios in the\nabsence of view constraints. First, we settle the long-standing open issue of\nhow to solve the view update problem in a multi-relational database with views\nthat are projections of joins of relations, and we do so in a more general\nsetting where views are defined by arbitrary conjunctive queries and database\nconstraints are stratified embedded dependencies. Next, we study a setting\nbased on horizontal decompositions of a single database relation, where views\nare defined by selections on possibly interpreted attributes (e.g., arithmetic\ncomparisons) in the presence of domain constraints over the database schema.\nLastly, we look into another multi-relational database setting, where views are\ndefined in an expressive \"Type\" Relational Algebra based on the n-ary\nDescription Logic DLR and database constraints are inclusions of expressions in\nthat algebra."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9207-3742", 
    "link": "http://arxiv.org/pdf/1211.3871v1", 
    "other_authors": "Neelamadhab Padhy, Rasmita Panigrahi", 
    "title": "Multi Relational Data Mining Approaches: A Data Mining Technique", 
    "arxiv-id": "1211.3871v1", 
    "author": "Rasmita Panigrahi", 
    "publish": "2012-11-16T12:07:11Z", 
    "summary": "The multi relational data mining approach has developed as an alternative way\nfor handling the structured data such that RDBMS. This will provides the mining\nin multiple tables directly. In MRDM the patterns are available in multiple\ntables (relations) from a relational database. As the data are available over\nthe many tables which will affect the many problems in the practice of the data\nmining. To deal with this problem, one either constructs a single table by\nPropositionalisation, or uses a Multi-Relational Data Mining algorithm. MRDM\napproaches have been successfully applied in the area of bioinformatics. Three\npopular pattern finding techniques classification, clustering and association\nare frequently used in MRDM. Multi relational approach has developed as an\nalternative for analyzing the structured data such as relational database. MRDM\nallowing applying directly in the data mining in multiple tables. To avoid the\nexpensive joining operations and semantic losses we used the MRDM technique.\nThis paper focuses some of the application areas of MRDM and feature directions\nas well as the comparison of ILP, GM, SSDM and MRDM"
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4503", 
    "link": "http://arxiv.org/pdf/1211.4371v1", 
    "other_authors": "Osama El-Sayed Sheta, Ahmed Nour Eldeen", 
    "title": "Building a health care data warehouse for cancer diseases", 
    "arxiv-id": "1211.4371v1", 
    "author": "Ahmed Nour Eldeen", 
    "publish": "2012-11-19T11:50:32Z", 
    "summary": "This paper presents architecture for health care data warehouse specific to\ncancer diseases which could be used by executive managers, doctors, physicians\nand other health professionals to support the healthcare process. The data\ntoday existing in multi-sources with different formats makes it necessary to\nhave some techniques for data integration. Executive managers need access to\nInformation so that decision makers can react in real time to changing needs.\nInformation is one of the most factors to an organization success that\nexecutive managers or physicians would need to base their decisions on, during\ndecision making. A health care data warehouse is therefore necessary to\nintegrate the different data sources into a central data repository and\nanalysis this data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4503", 
    "link": "http://arxiv.org/pdf/1211.5009v1", 
    "other_authors": "Seyed-Mehdi-Reza Beheshti, Hamid Reza Motahari-Nezhad, Boualem Benatallah", 
    "title": "Temporal Provenance Model (TPM): Model and Query Language", 
    "arxiv-id": "1211.5009v1", 
    "author": "Boualem Benatallah", 
    "publish": "2012-11-21T11:58:47Z", 
    "summary": "Provenance refers to the documentation of an object's lifecycle. This\ndocumentation (often represented as a graph) should include all the information\nnecessary to reproduce a certain piece of data or the process that led to it.\nIn a dynamic world, as data changes, it is important to be able to get a piece\nof data as it was, and its provenance graph, at a certain point in time.\nSupporting time-aware provenance querying is challenging and requires: (i)\nexplicitly representing the time information in the provenance graphs, and (ii)\nproviding abstractions and efficient mechanisms for time-aware querying of\nprovenance graphs over an ever growing volume of data. The existing provenance\nmodels treat time as a second class citizen (i.e. as an optional annotation).\nThis makes time-aware querying of provenance data inefficient and sometimes\ninaccessible. We introduce an extended provenance graph model to explicitly\nrepresent time as an additional dimension of provenance data. We also provide a\nquery language, novel abstractions and efficient mechanisms to query and\nanalyze timed provenance graphs. The main contributions of the paper include:\n(i) proposing a Temporal Provenance Model (TPM) as a timed provenance model;\nand (ii) introducing two concepts of timed folder, as a container of related\nset of objects and their provenance relationship over time, and timed paths, to\nrepresent the evolution of objects tracing information over time, for analyzing\nand querying TPM graphs. We have implemented the approach on top of FPSPARQL, a\nquery engine for large graphs, and have evaluated for querying TPM models. The\nevaluation shows the viability and efficiency of our approach."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2012.4501", 
    "link": "http://arxiv.org/pdf/1211.5418v1", 
    "other_authors": "D. Roselin Selvarani, T. N. Ravi", 
    "title": "A survey on data and transaction management in mobile databases", 
    "arxiv-id": "1211.5418v1", 
    "author": "T. N. Ravi", 
    "publish": "2012-11-23T06:29:45Z", 
    "summary": "The popularity of the Mobile Database is increasing day by day as people need\ninformation even on the move in the fast changing world. This database\ntechnology permits employees using mobile devices to connect to their corporate\nnetworks, hoard the needed data, work in the disconnected mode and reconnect to\nthe network to synchronize with the corporate database. In this scenario, the\ndata is being moved closer to the applications in order to improve the\nperformance and autonomy. This leads to many interesting problems in mobile\ndatabase research and Mobile Database has become a fertile land for many\nresearchers. In this paper a survey is presented on data and Transaction\nmanagement in Mobile Databases from the year 2000 onwards. The survey focuses\non the complete study on the various types of Architectures used in Mobile\ndatabases and Mobile Transaction Models. It also addresses the data management\nissues namely Replication and Caching strategies and the transaction management\nfunctionalities such as Concurrency Control and Commit protocols,\nSynchronization, Query Processing, Recovery and Security. It also provides\nResearch Directions in Mobile databases."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcseit.2012.2503", 
    "link": "http://arxiv.org/pdf/1211.5724v1", 
    "other_authors": "Neelamadhab Padhy, Rasmita Panigrahi", 
    "title": "Data Mining: A prediction Technique for the workers in the PR Department   of Orissa (Block and Panchayat)", 
    "arxiv-id": "1211.5724v1", 
    "author": "Rasmita Panigrahi", 
    "publish": "2012-11-25T04:24:02Z", 
    "summary": "This paper presents the method of mining the data and which contains the\ninformation about the large information about the PR (Panchayat Raj\nDepartment)of Orissa.We have focused some of the techniques,approaches and\ndifferent methodologies of the demand forecasting. Every organizations are\noperated in different places of the country. Each place of operation may\ngenerate a huge amount of data. In an organization, worker prediction is the\ndifficult task of the manager. It is the complex process not only because its\nnature of feature prediction but also various approaches methodologies always\nmakes user confused. This paper aims to deal with the problem selection\nprocess. In this paper we have used some of the approaches from literature are\nbeen introduced and analyzed to find its suitable organization and situation.\nBased on this we have designed with automatic selection function to help users\nmake a prejudgment. This information about each approach will be showed to\nusers with examples to help understanding. This system also provides\ncalculation function to help users work out a predication result. Generally the\nnew developed system has a more comprehensive functions compared with existing\nones. It aims to improve the accuracy of demand forecasting by implementing the\nforecasting algorithm. While it is still a decision support system with no\nability of make the final judgment.This type of huge amount of data are are\navailable in the form of different ways which has drastically changed in the\nareas of science and engineering.To analyze, manage and make a decision of such\ntype of huge amount of data we need techniques called the data mining which\nwill transforming in many fields. We have implemented the algorithms in JAVA\ntechnology. This paper provides the prediction algorithm Linear Regression,\nresult which will helpful in the further research."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcseit.2012.2503", 
    "link": "http://arxiv.org/pdf/1211.5817v1", 
    "other_authors": "Seyed-Mehdi-Reza Beheshti, Sherif Sakr, Boualem Benatallah, Hamid Reza Motahari-Nezhad", 
    "title": "Extending SPARQL to Support Entity Grouping and Path Queries", 
    "arxiv-id": "1211.5817v1", 
    "author": "Hamid Reza Motahari-Nezhad", 
    "publish": "2012-11-21T10:55:36Z", 
    "summary": "The ability to efficiently find relevant subgraphs and paths in a large graph\nto a given query is important in many applications including scientific data\nanalysis, social networks, and business intelligence. Currently, there is\nlittle support and no efficient approaches for expressing and executing such\nqueries. This paper proposes a data model and a query language to address this\nproblem. The contributions include supporting the construction and selection\nof: (i) folder nodes, representing a set of related entities, and (ii) path\nnodes, representing a set of paths in which a path is the transitive\nrelationship of two or more entities in the graph. Folders and paths can be\nstored and used for future queries. We introduce FPSPARQL which is an extension\nof the SPARQL supporting folder and path nodes. We have implemented a query\nengine that supports FPSPARQL and the evaluation results shows its viability\nand efficiency for querying large graph datasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcseit.2012.2503", 
    "link": "http://arxiv.org/pdf/1211.6176v1", 
    "other_authors": "Reynold Xin, Josh Rosen, Matei Zaharia, Michael J. Franklin, Scott Shenker, Ion Stoica", 
    "title": "Shark: SQL and Rich Analytics at Scale", 
    "arxiv-id": "1211.6176v1", 
    "author": "Ion Stoica", 
    "publish": "2012-11-27T01:36:58Z", 
    "summary": "Shark is a new data analysis system that marries query processing with\ncomplex analytics on large clusters. It leverages a novel distributed memory\nabstraction to provide a unified engine that can run SQL queries and\nsophisticated analytics functions (e.g., iterative machine learning) at scale,\nand efficiently recovers from failures mid-query. This allows Shark to run SQL\nqueries up to 100x faster than Apache Hive, and machine learning programs up to\n100x faster than Hadoop. Unlike previous systems, Shark shows that it is\npossible to achieve these speedups while retaining a MapReduce-like execution\nengine, and the fine-grained fault tolerance properties that such engines\nprovide. It extends such an engine in several ways, including column-oriented\nin-memory storage and dynamic mid-query replanning, to effectively execute SQL.\nThe result is a system that matches the speedups reported for MPP analytic\ndatabases over MapReduce, while offering fault tolerance properties and complex\nanalytics capabilities that they lack."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcseit.2012.2503", 
    "link": "http://arxiv.org/pdf/1211.6273v1", 
    "other_authors": "Amineh Amini, Hadi Saboohi, Nasser Nemat bakhsh", 
    "title": "A RDF-based Data Integration Framework", 
    "arxiv-id": "1211.6273v1", 
    "author": "Nasser Nemat bakhsh", 
    "publish": "2012-11-27T11:17:09Z", 
    "summary": "Data integration is one of the main problems in distributed data sources. An\napproach is to provide an integrated mediated schema for various data sources.\nThis research work aims at developing a framework for defining an integrated\nschema and querying on it. The basic idea is to employ recent standard\nlanguages and tools to provide a unified data integration framework. RDF is\nused for integrated schema descriptions as well as providing a unified view of\ndata. RDQL is used for query reformulation. Furthermore, description logic\ninference services provide necessary means for satisfiability checking of\nconcepts in integrated schema. The framework has tools to display integrated\nschema, query on it, and provides enough flexibilities to be used in different\napplication domains."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcseit.2012.2503", 
    "link": "http://arxiv.org/pdf/1212.0254v1", 
    "other_authors": "Bruno Marnette", 
    "title": "Resolution and Datalog Rewriting Under Value Invention and Equality   Constraints", 
    "arxiv-id": "1212.0254v1", 
    "author": "Bruno Marnette", 
    "publish": "2012-12-02T22:50:25Z", 
    "summary": "This paper present several refinements of the Datalog +/- framework based on\nresolution and Datalog-rewriting. We first present a resolution algorithm which\nis complete for arbitrary sets of tgds and egds. We then show that a technique\nof saturation can be used to achieve completeness with respect to First-Order\n(FO) query rewriting. We then investigate the class of guarded tgds (with a\nloose definition of guardedness), and show that every set of tgds in this class\ncan be rewritten into an equivalent set of standard Datalog rules. On the\nnegative side, this implies that Datalog +/- has (only) the same expressive\npower as standard Datalog in terms of query answering. On the positive side\nhowever, this mean that known results and existing optimization techniques\n(such as Magic-Set) may be applied in the context of Datalog +/- despite its\nricher syntax."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.0317v1", 
    "other_authors": "B. Adinarayana Reddy, O. Srinivasa Rao, M. H. M. Krishna Prasad", 
    "title": "An Improved UP-Growth High Utility Itemset Mining", 
    "arxiv-id": "1212.0317v1", 
    "author": "M. H. M. Krishna Prasad", 
    "publish": "2012-12-03T08:50:50Z", 
    "summary": "Efficient discovery of frequent itemsets in large datasets is a crucial task\nof data mining. In recent years, several approaches have been proposed for\ngenerating high utility patterns, they arise the problems of producing a large\nnumber of candidate itemsets for high utility itemsets and probably degrades\nmining performance in terms of speed and space. Recently proposed compact tree\nstructure, viz., UP Tree, maintains the information of transactions and\nitemsets, facilitate the mining performance and avoid scanning original\ndatabase repeatedly. In this paper, UP Tree (Utility Pattern Tree) is adopted,\nwhich scans database only twice to obtain candidate items and manage them in an\nefficient data structured way. Applying UP Tree to the UP Growth takes more\nexecution time for Phase II. Hence this paper presents modified algorithm\naiming to reduce the execution time by effectively identifying high utility\nitemsets."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.1469v1", 
    "other_authors": "Marc Moreau, Wendy Osborn", 
    "title": "mqr-tree: A 2-dimensional Spatial Access Method", 
    "arxiv-id": "1212.1469v1", 
    "author": "Wendy Osborn", 
    "publish": "2012-12-06T21:05:47Z", 
    "summary": "In this paper, we propose the mqr-tree, a two-dimensional spatial access\nmethod that organizes spatial objects in a two-dimensional node and based on\ntheir spatial relationships. Previously proposed spatial access methods that\nattempt to maintain spatial relationships between objects in their structures\nare limited in their incorporation of existing one-dimensional spatial access\nmethods, or have lower space utilization in its nodes, and higher tree height,\novercoverage and overlap than is necessary. The mqr-tree utilizes a node\norganization, set of spatial relationship rules and insertion strategy in order\nto gain significant improvements in overlap and overcoverage. In addition,\nother desirable properties are identified as a result of the chosen node\norganization and insertion strategies. In particular, zero overlap is achieved\nwhen the mqr-tree is used to index point data. A comparison of the mqr-tree\ninsertion strategy versus the R-tree shows significant improvements in overlap\nand overcoverage, with comparable space utilization. In addition, a comparison\nof region searching shows that the mqr-tree achieves a lower number of disk\naccesses in many cases"
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.2071v1", 
    "other_authors": "Youssef Bassil", 
    "title": "A Data Warehouse Design for a Typical University Information System", 
    "arxiv-id": "1212.2071v1", 
    "author": "Youssef Bassil", 
    "publish": "2012-12-10T14:09:03Z", 
    "summary": "Presently, large enterprises rely on database systems to manage their data\nand information. These databases are useful for conducting daily business\ntransactions. However, the tight competition in the marketplace has led to the\nconcept of data mining in which data are analyzed to derive effective business\nstrategies and discover better ways in carrying out business. In order to\nperform data mining, regular databases must be converted into what so called\ninformational databases also known as data warehouse. This paper presents a\ndesign model for building data warehouse for a typical university information\nsystem. It is based on transforming an operational database into an\ninformational warehouse useful for decision makers to conduct data analysis,\npredication, and forecasting. The proposed model is based on four stages of\ndata migration: Data extraction, data cleansing, data transforming, and data\nindexing and loading. The complete system is implemented under MS Access 2010\nand is meant to serve as a repository of data for data mining operations."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.2251v1", 
    "other_authors": "Susan B. Davidson, Tova Milo, Sudeepa Roy", 
    "title": "A Propagation Model for Provenance Views of Public/Private Workflows", 
    "arxiv-id": "1212.2251v1", 
    "author": "Sudeepa Roy", 
    "publish": "2012-12-10T23:47:25Z", 
    "summary": "We study the problem of concealing functionality of a proprietary or private\nmodule when provenance information is shown over repeated executions of a\nworkflow which contains both `public' and `private' modules. Our approach is to\nuse `provenance views' to hide carefully chosen subsets of data over all\nexecutions of the workflow to ensure G-privacy: for each private module and\neach input x, the module's output f(x) is indistinguishable from G -1 other\npossible values given the visible data in the workflow executions. We show that\nG-privacy cannot be achieved simply by combining solutions for individual\nprivate modules; data hiding must also be `propagated' through public modules.\nWe then examine how much additional data must be hidden and when it is safe to\nstop propagating data hiding. The answer depends strongly on the workflow\ntopology as well as the behavior of public modules on the visible data. In\nparticular, for a class of workflows (which include the common tree and chain\nworkflows), taking private solutions for each private module, augmented with a\n`public closure' that is `upstream-downstream safe', ensures G-privacy. We\ndefine these notions formally and show that the restrictions are necessary. We\nalso study the related optimization problems of minimizing the amount of hidden\ndata."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.2338v1", 
    "other_authors": "St\u00e9phane Martin, Mehdi Ahmed-Nacer, Pascal Urso", 
    "title": "Controlled conflict resolution for replicated document", 
    "arxiv-id": "1212.2338v1", 
    "author": "Pascal Urso", 
    "publish": "2012-12-11T09:02:45Z", 
    "summary": "Collaborative working is increasingly popular, but it presents challenges due\nto the need for high responsiveness and disconnected work support. To address\nthese challenges the data is optimistically replicated at the edges of the\nnetwork, i.e. personal computers or mobile devices. This replication requires a\nmerge mechanism that preserves the consistency and structure of the shared data\nsubject to concurrent modifications. In this paper, we propose a generic design\nto ensure eventual consistency (every replica will eventually view the same\ndata) and to maintain the specific constraints of the replicated data. Our\nlayered design provides to the application engineer the complete control over\nsystem scalability and behavior of the replicated data in face of concurrent\nmodifications. We show that our design allows replication of complex data types\nwith acceptable performances."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.3501v1", 
    "other_authors": "Henrik Bj\u00f6rklund, Martin Schuster, Thomas Schwentick, Joscha Kulbatzki", 
    "title": "On optimum left-to-right strategies for active context-free games", 
    "arxiv-id": "1212.3501v1", 
    "author": "Joscha Kulbatzki", 
    "publish": "2012-12-14T15:28:45Z", 
    "summary": "Active context-free games are two-player games on strings over finite\nalphabets with one player trying to rewrite the input string to match a target\nspecification. These games have been investigated in the context of exchanging\nActive XML (AXML) data. While it was known that the rewriting problem is\nundecidable in general, it is shown here that it is EXPSPACE-complete to decide\nfor a given context-free game, whether all safely rewritable strings can be\nsafely rewritten in a left-to-right manner, a problem that was previously\nconsidered by Abiteboul et al. Furthermore, it is shown that the corresponding\nproblem for games with finite replacement languages is EXPTIME-complete."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.5636v1", 
    "other_authors": "Luis Gal\u00e1rraga, Katja Hose, Ralf Schenkel", 
    "title": "Partout: A Distributed Engine for Efficient RDF Processing", 
    "arxiv-id": "1212.5636v1", 
    "author": "Ralf Schenkel", 
    "publish": "2012-12-21T23:51:05Z", 
    "summary": "The increasing interest in Semantic Web technologies has led not only to a\nrapid growth of semantic data on the Web but also to an increasing number of\nbackend applications with already more than a trillion triples in some cases.\nConfronted with such huge amounts of data and the future growth, existing\nstate-of-the-art systems for storing RDF and processing SPARQL queries are no\nlonger sufficient. In this paper, we introduce Partout, a distributed engine\nfor efficient RDF processing in a cluster of machines. We propose an effective\napproach for fragmenting RDF data sets based on a query log, allocating the\nfragments to nodes in a cluster, and finding the optimal configuration. Partout\ncan efficiently handle updates and its query optimizer produces efficient query\nexecution plans for ad-hoc SPARQL queries. Our experiments show the superiority\nof our approach to state-of-the-art approaches for partitioning and distributed\nSPARQL query processing."
},{
    "category": "cs.DB", 
    "doi": "10.5120/9255-3424", 
    "link": "http://arxiv.org/pdf/1212.6051v1", 
    "other_authors": "Wided Bakari, Mouez Ali, Hanene Ben-Abdallah", 
    "title": "Automatic approach for generating ETL operators", 
    "arxiv-id": "1212.6051v1", 
    "author": "Hanene Ben-Abdallah", 
    "publish": "2012-12-25T14:07:44Z", 
    "summary": "This article addresses the generation of the ETL\noperators(Extract-Transform-Load) for supplying a Data Warehouse from a\nrelational data source. As a first step, we add new rules to those proposed by\nthe authors of [1], these rules deal with the combination of ETL operators. In\na second step, we propose an automatic approach based on model transformations\nto generate the ETL operations needed for loading a data warehouse. This\napproach offers the possibility to set some designer requirements for loading."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1212.6383v1", 
    "other_authors": "Andrea Burattin, Alessandro Sperduti, Wil M. P. van der Aalst", 
    "title": "Heuristics Miners for Streaming Event Data", 
    "arxiv-id": "1212.6383v1", 
    "author": "Wil M. P. van der Aalst", 
    "publish": "2012-12-27T15:15:48Z", 
    "summary": "More and more business activities are performed using information systems.\nThese systems produce such huge amounts of event data that existing systems are\nunable to store and process them. Moreover, few processes are in steady-state\nand due to changing circumstances processes evolve and systems need to adapt\ncontinuously. Since conventional process discovery algorithms have been defined\nfor batch processing, it is difficult to apply them in such evolving\nenvironments. Existing algorithms cannot cope with streaming event data and\ntend to generate unreliable and obsolete results.\n  In this paper, we discuss the peculiarities of dealing with streaming event\ndata in the context of process mining. Subsequently, we present a general\nframework for defining process mining algorithms in settings where it is\nimpossible to store all events over an extended period or where processes\nevolve while being analyzed. We show how the Heuristics Miner, one of the most\neffective process discovery algorithms for practical applications, can be\nmodified using this framework. Different stream-aware versions of the\nHeuristics Miner are defined and implemented in ProM. Moreover, experimental\nresults on artificial and real logs are reported."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1212.6636v2", 
    "other_authors": "Paraschos Koutris, Dan Suciu", 
    "title": "A Dichotomy on the Complexity of Consistent Query Answering for Atoms   with Simple Keys", 
    "arxiv-id": "1212.6636v2", 
    "author": "Dan Suciu", 
    "publish": "2012-12-29T15:07:50Z", 
    "summary": "We study the problem of consistent query answering under primary key\nviolations. In this setting, the relations in a database violate the key\nconstraints and we are interested in maximal subsets of the database that\nsatisfy the constraints, which we call repairs. For a boolean query Q, the\nproblem CERTAINTY(Q) asks whether every such repair satisfies the query or not;\nthe problem is known to be always in coNP for conjunctive queries. However,\nthere are queries for which it can be solved in polynomial time. It has been\nconjectured that there exists a dichotomy on the complexity of CERTAINTY(Q) for\nconjunctive queries: it is either in PTIME or coNP-complete. In this paper, we\nprove that the conjecture is indeed true for the case of conjunctive queries\nwithout self-joins, where each atom has as a key either a single attribute\n(simple key) or all attributes of the atom."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.1575v1", 
    "other_authors": "Anna Pyayt, Michael Gubanov", 
    "title": "BigDB: Automatic Machine Learning Optimizer", 
    "arxiv-id": "1301.1575v1", 
    "author": "Michael Gubanov", 
    "publish": "2013-01-06T04:03:29Z", 
    "summary": "In this short vision paper, we introduce a machine learning optimizer for\ndata management and describe its architecture and main functionality."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.2236v1", 
    "other_authors": "Rym Khemiri, Fadila Bentayeb", 
    "title": "User Profile-Driven Data Warehouse Summary for Adaptive OLAP Queries", 
    "arxiv-id": "1301.2236v1", 
    "author": "Fadila Bentayeb", 
    "publish": "2013-01-10T20:09:36Z", 
    "summary": "Data warehousing is an essential element of decision support systems. It aims\nat enabling the user knowledge to make better and faster daily business\ndecisions. To improve this decision support system and to give more and more\nrelevant information to the user, the need to integrate user's profiles into\nthe data warehouse process becomes crucial. In this paper, we propose to\nexploit users' preferences as a basis for adapting OLAP (On-Line Analytical\nProcessing) queries to the user. For this, we present a user profile-driven\ndata warehouse approach that allows dening user's profile composed by his/her\nidentifier and a set of his/her preferences. Our approach is based on a general\ndata warehouse architecture and an adaptive OLAP analysis system. Our main idea\nconsists in creating a data warehouse materialized view for each user with\nrespect to his/her profile. This task is performed off-line when the user\ndefines his/her profile for the first time. Then, when a user query is\nsubmitted to the data warehouse, the system deals with his/her data warehouse\nmaterialized view instead of the whole data warehouse. In other words, the data\nwarehouse view summaries the data warehouse content for the user by taking into\naccount his/her preferences. Moreover, we are implementing our data warehouse\npersonalization approach under the SQL Server 2005 DBMS (DataBase Management\nSystem)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.2362v1", 
    "other_authors": "Jianxin Li, Chengfei Liu, Rui Zhou, Jeffrey Xu Yu", 
    "title": "Quasi-SLCA based Keyword Query Processing over Probabilistic XML Data", 
    "arxiv-id": "1301.2362v1", 
    "author": "Jeffrey Xu Yu", 
    "publish": "2013-01-11T00:05:46Z", 
    "summary": "The probabilistic threshold query is one of the most common queries in\nuncertain databases, where a result satisfying the query must be also with\nprobability meeting the threshold requirement. In this paper, we investigate\nprobabilistic threshold keyword queries (PrTKQ) over XML data, which is not\nstudied before. We first introduce the notion of quasi-SLCA and use it to\nrepresent results for a PrTKQ with the consideration of possible world\nsemantics. Then we design a probabilistic inverted (PI) index that can be used\nto quickly return the qualified answers and filter out the unqualified ones\nbased on our proposed lower/upper bounds. After that, we propose two efficient\nand comparable algorithms: Baseline Algorithm and PI index-based Algorithm. To\naccelerate the performance of algorithms, we also utilize probability density\nfunction. An empirical study using real and synthetic data sets has verified\nthe effectiveness and the efficiency of our approaches."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.2375v1", 
    "other_authors": "Jianxin Li, Chengfei Liu, Liang Yao, Jeffrey Xu Yu", 
    "title": "Context-based Diversification for Keyword Queries over XML Data", 
    "arxiv-id": "1301.2375v1", 
    "author": "Jeffrey Xu Yu", 
    "publish": "2013-01-11T01:33:50Z", 
    "summary": "While keyword query empowers ordinary users to search vast amount of data,\nthe ambiguity of keyword query makes it difficult to effectively answer keyword\nqueries, especially for short and vague keyword queries. To address this\nchallenging problem, in this paper we propose an approach that automatically\ndiversifies XML keyword search based on its different contexts in the XML data.\nGiven a short and vague keyword query and XML data to be searched, we firstly\nderive keyword search candidates of the query by a classifical feature\nselection model. And then, we design an effective XML keyword search\ndiversification model to measure the quality of each candidate. After that,\nthree efficient algorithms are proposed to evaluate the possible generated\nquery candidates representing the diversified search intentions, from which we\ncan find and return top-$k$ qualified query candidates that are most relevant\nto the given keyword query while they can cover maximal number of distinct\nresults.At last, a comprehensive evaluation on real and synthetic datasets\ndemonstrates the effectiveness of our proposed diversification model and the\nefficiency of our algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.2378v1", 
    "other_authors": "Jianxin Li, Chengfei Liu, Liang Yao, Jeffrey Xu Yu, Rui Zhou", 
    "title": "Query-driven Frequent Co-occurring Term Extraction over Relational Data   using MapReduce", 
    "arxiv-id": "1301.2378v1", 
    "author": "Rui Zhou", 
    "publish": "2013-01-11T01:55:10Z", 
    "summary": "In this paper we study how to efficiently compute \\textit{frequent\nco-occurring terms} (FCT) in the results of a keyword query in parallel using\nthe popular MapReduce framework. Taking as input a keyword query q and an\ninteger k, an FCT query reports the k terms that are not in q, but appear most\nfrequently in the results of the keyword query q over multiple joined\nrelations. The returned terms of FCT search can be used to do query expansion\nand query refinement for traditional keyword search. Different from the method\nof FCT search in a single platform, our proposed approach can efficiently\nanswer a FCT query using the MapReduce Paradigm without pre-computing the\nresults of the original keyword query, which is run in parallel platform. In\nthis work, we can output the final FCT search results by two MapReduce jobs:\nthe first is to extract the statistical information of the data; and the second\nis to calculate the total frequency of each term based on the output of the\nfirst job. At the two MapReduce jobs, we would guarantee the load balance of\nmappers and the computational balance of reducers as much as possible.\nAnalytical and experimental evaluations demonstrate the efficiency and\nscalability of our proposed approach using TPC-H benchmark datasets with\ndifferent sizes."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1301.7015v2", 
    "other_authors": "Entong Shen, Ting Yu", 
    "title": "Mining Frequent Graph Patterns with Differential Privacy", 
    "arxiv-id": "1301.7015v2", 
    "author": "Ting Yu", 
    "publish": "2013-01-29T18:37:35Z", 
    "summary": "Discovering frequent graph patterns in a graph database offers valuable\ninformation in a variety of applications. However, if the graph dataset\ncontains sensitive data of individuals such as mobile phone-call graphs and\nweb-click graphs, releasing discovered frequent patterns may present a threat\nto the privacy of individuals. {\\em Differential privacy} has recently emerged\nas the {\\em de facto} standard for private data analysis due to its provable\nprivacy guarantee. In this paper we propose the first differentially private\nalgorithm for mining frequent graph patterns.\n  We first show that previous techniques on differentially private discovery of\nfrequent {\\em itemsets} cannot apply in mining frequent graph patterns due to\nthe inherent complexity of handling structural information in graphs. We then\naddress this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling\nbased algorithm. Unlike previous work on frequent itemset mining, our\ntechniques do not rely on the output of a non-private mining algorithm.\nInstead, we observe that both frequent graph pattern mining and the guarantee\nof differential privacy can be unified into an MCMC sampling framework. In\naddition, we establish the privacy and utility guarantee of our algorithm and\npropose an efficient neighboring pattern counting technique as well.\nExperimental results show that the proposed algorithm is able to output\nfrequent patterns with good precision."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.0567v1", 
    "other_authors": "Antonis Loizou, Paul Groth", 
    "title": "On the Formulation of Performant SPARQL Queries", 
    "arxiv-id": "1304.0567v1", 
    "author": "Paul Groth", 
    "publish": "2013-04-02T09:02:48Z", 
    "summary": "The combination of the flexibility of RDF and the expressiveness of SPARQL\nprovides a powerful mechanism to model, integrate and query data. However,\nthese properties also mean that it is nontrivial to write performant SPARQL\nqueries. Indeed, it is quite easy to create queries that tax even the most\noptimised triple stores. Currently, application developers have little concrete\nguidance on how to write \"good\" queries. The goal of this paper is to begin to\nbridge this gap. It describes 5 heuristics that can be applied to create\noptimised queries. The heuristics are informed by formal results in the\nliterature on the semantics and complexity of evaluating SPARQL queries, which\nensures that queries following these rules can be optimised effectively by an\nunderlying RDF store. Moreover, we empirically verify the efficacy of the\nheuristics using a set of openly available datasets and corresponding SPARQL\nqueries developed by a large pharmacology data integration project. The\nexperimental results show improvements in performance across 6 state-of-the-art\nRDF stores."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.0959v1", 
    "other_authors": "Gosta Grahne, Adrian Onet, Nihat Tartal", 
    "title": "Conditional Tables in practice", 
    "arxiv-id": "1304.0959v1", 
    "author": "Nihat Tartal", 
    "publish": "2013-04-03T14:11:04Z", 
    "summary": "Due to the ever increasing importance of the internet, interoperability of\nheterogeneous data sources is as well of ever increasing importance.\nInteroperability can be achieved e.g. through data integration and data\nexchange. Common to both approaches is the need for the DBMS to be able to\nstore and query incomplete databases. In this report we present PossDB, a DBMS\ncapable of storing and querying incomplete databases. The system is wrapper\nover PostgreSQL, and the query language is an extension of a subset of standard\nSQL. Our experimental results show that our system scales well, actually better\nthan comparable systems."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.1411v2", 
    "other_authors": "Quoc Trung Tran, Ivo Jimenez, Rui Wang, Neoklis Polyzotis, Anastasia Ailamaki", 
    "title": "RITA: An Index-Tuning Advisor for Replicated Databases", 
    "arxiv-id": "1304.1411v2", 
    "author": "Anastasia Ailamaki", 
    "publish": "2013-04-04T15:54:48Z", 
    "summary": "Given a replicated database, a divergent design tunes the indexes in each\nreplica differently in order to specialize it for a specific subset of the\nworkload. This specialization brings significant performance gains compared to\nthe common practice of having the same indexes in all replicas, but requires\nthe development of new tuning tools for database administrators. In this paper\nwe introduce RITA (Replication-aware Index Tuning Advisor), a novel\ndivergent-tuning advisor that offers several essential features not found in\nexisting tools: it generates robust divergent designs that allow the system to\nadapt gracefully to replica failures; it computes designs that spread the load\nevenly among specialized replicas, both during normal operation and when\nreplicas fail; it monitors the workload online in order to detect changes that\nrequire a recomputation of the divergent design; and, it offers suggestions to\nelastically reconfigure the system (by adding/removing replicas or\nadding/dropping indexes) to respond to workload changes. The key technical\ninnovation behind RITA is showing that the problem of selecting an optimal\ndesign can be formulated as a Binary Integer Program (BIP). The BIP has a\nrelatively small number of variables, which makes it feasible to solve it\nefficiently using any off-the-shelf linear-optimization software. Experimental\nresults demonstrate that RITA computes better divergent designs compared to\nexisting tools, offers more features, and has fast execution times."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.2184v1", 
    "other_authors": "Evgeniy Grigoriev", 
    "title": "Object-Oriented Translation for Programmable Relational System (DRAFT)", 
    "arxiv-id": "1304.2184v1", 
    "author": "Evgeniy Grigoriev", 
    "publish": "2013-04-08T12:29:31Z", 
    "summary": "The paper introduces the principles of object-oriented translation for target\nmachine which provides executing the sequences of elementary operations on\npersistent data presented as a set of relations (programmable relational\nsystem). The language of this target machine bases on formal operations of\nrelational data model. An approach is given to convert both the description of\ncomplex object-oriented data structures and operations on these data, into a\ndescription of relational structures and operations on them. The proposed\napproach makes possible to extend the target relational language with commands\nallowing data be described as a set of complex persistent objects of different\nclasses. Object views are introduced which allow relational operations be\napplied to the data of complex objects. It is shown that any operation and\nmethod can be executed on any group of the objects without explicit and\nimplicit iterators. Binding of both attributes and methods with their\npolymorphic implementations are discussed. Classes can be co-used with\nrelations as scalar domains, in referential integrity constraints and in data\nquery operations."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.2637v2", 
    "other_authors": "Juan L. Reutter", 
    "title": "Containment of Nested Regular Expressions", 
    "arxiv-id": "1304.2637v2", 
    "author": "Juan L. Reutter", 
    "publish": "2013-04-09T15:40:21Z", 
    "summary": "Nested regular expressions (NREs) have been proposed as a powerful formalism\nfor querying RDFS graphs, but research in a more general graph database context\nhas been scarce, and static analysis results are currently lacking. In this\npaper we investigate the problem of containment of NREs, and show that it can\nbe solved in PSPACE, i.e., the same complexity as the problem of containment of\nregular expressions or regular path queries (RPQs)."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.3120v1", 
    "other_authors": "J. A. Quaye-Ballard, R. An, A. B. Agyemang, N. Y. Oppong-Quayson, J. E. N. Ablade", 
    "title": "GUI Database for the Equipment Store of the Department of Geomatic   Engineering, KNUST", 
    "arxiv-id": "1304.3120v1", 
    "author": "J. E. N. Ablade", 
    "publish": "2013-04-10T00:25:01Z", 
    "summary": "The geospatial analyst is required to apply art, science, and technology to\nmeasure relative positions of natural and man-made features above or beneath\nthe earths surface, and to present this information either graphically or\nnumerically. The reference positions for these measurements need to be well\narchived and managed to effectively sustain the activities in the spatial\nindustry. The research herein described highlights the need for an information\nsystem for the Land Surveyors Equipment Store. Such a system is a database\nmanagement system with a user friendly graphical interface. This paper\ndescribes one such system that has been developed for the Equipment Store of\nthe Department of Geomatic Engineering, Kwame Nkrumah University of Science and\nTechnology, Ghana. The system facilitates efficient management and location of\ninstruments, as well as easy location of beacons together with their attribute\ninformation, it provides multimedia information about instruments in an\nEquipment Store. Digital camera was used capture the pictorial descriptions of\nthe beacons. Geographic Information System software was employed to visualize\nthe spatial location of beacons and to publish the various layers for the\nGraphical User Interface. The aesthetics of the interface was developed with\nuser interface design tools and coded by programming. The developed Suite,\npowered by a reliable and fully scalable database, provides an efficient way of\nbooking and analyzing transactions in an Equipment Store."
},{
    "category": "cs.DB", 
    "doi": "10.1109/CEC.2014.6900341", 
    "link": "http://arxiv.org/pdf/1304.3603v1", 
    "other_authors": "Sunita Jahirabadkar, Parag Kulkarni", 
    "title": "SCAF An effective approach to Classify Subspace Clustering algorithms", 
    "arxiv-id": "1304.3603v1", 
    "author": "Parag Kulkarni", 
    "publish": "2013-04-12T11:02:56Z", 
    "summary": "Subspace clustering discovers the clusters embedded in multiple, overlapping\nsubspaces of high dimensional data. Many significant subspace clustering\nalgorithms exist, each having different characteristics caused by the use of\ndifferent techniques, assumptions, heuristics used etc. A comprehensive\nclassification scheme is essential which will consider all such characteristics\nto divide subspace clustering approaches in various families. The algorithms\nbelonging to same family will satisfy common characteristics. Such a\ncategorization will help future developers to better understand the quality\ncriteria to be used and similar algorithms to be used to compare results with\ntheir proposed clustering algorithms. In this paper, we first proposed the\nconcept of SCAF (Subspace Clustering Algorithms Family). Characteristics of\nSCAF will be based on the classes such as cluster orientation, overlap of\ndimensions etc. As an illustration, we further provided a comprehensive,\nsystematic description and comparison of few significant algorithms belonging\nto 'Axis parallel, overlapping, density based' SCAF."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3204", 
    "link": "http://arxiv.org/pdf/1304.4184v1", 
    "other_authors": "K. C. Srikantaiah, N. Krishna Kumar, K. R. Venugopal, L. M. Patnaik", 
    "title": "Bidirectional Growth based Mining and Cyclic Behaviour Analysis of Web   Sequential Patterns", 
    "arxiv-id": "1304.4184v1", 
    "author": "L. M. Patnaik", 
    "publish": "2013-04-15T17:36:57Z", 
    "summary": "Web sequential patterns are important for analyzing and understanding users\nbehaviour to improve the quality of service offered by the World Wide Web. Web\nPrefetching is one such technique that utilizes prefetching rules derived\nthrough Cyclic Model Analysis of the mined Web sequential patterns. The more\naccurate the prediction and more satisfying the results of prefetching if we\nuse a highly efficient and scalable mining technique such as the Bidirectional\nGrowth based Directed Acyclic Graph. In this paper, we propose a novel\nalgorithm called Bidirectional Growth based mining Cyclic behavior Analysis of\nweb sequential Patterns (BGCAP) that effectively combines these strategies to\ngenerate prefetching rules in the form of 2-sequence patterns with Periodicity\nand threshold of Cyclic Behaviour that can be utilized to effectively prefetch\nWeb pages, thus reducing the users perceived latency. As BGCAP is based on\nBidirectional pattern growth, it performs only (log n+1) levels of recursion\nfor mining n Web sequential patterns. Our experimental results show that\nprefetching rules generated using BGCAP is 5-10 percent faster for different\ndata sizes and 10-15% faster for a fixed data size than TD-Mine. In addition,\nBGCAP generates about 5-15 percent more prefetching rules than TD-Mine."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3204", 
    "link": "http://arxiv.org/pdf/1304.4187v1", 
    "other_authors": "Serge Abiteboul, \u00c9milien Antoine, Julia Stoyanovich", 
    "title": "The Webdamlog System Managing Distributed Knowledge on the Web", 
    "arxiv-id": "1304.4187v1", 
    "author": "Julia Stoyanovich", 
    "publish": "2013-04-15T18:00:40Z", 
    "summary": "We study the use of WebdamLog, a declarative high-level lan- guage in the\nstyle of datalog, to support the distribution of both data and knowledge (i.e.,\nprograms) over a network of au- tonomous peers. The main novelty of WebdamLog\ncompared to datalog is its use of delegation, that is, the ability for a peer\nto communicate a program to another peer. We present results of a user study,\nshowing that users can write WebdamLog programs quickly and correctly, and with\na minimal amount of training. We present an implementation of the WebdamLog\ninference engine relying on the Bud dat- alog engine. We describe an\nexperimental evaluation of the WebdamLog engine, demonstrating that WebdamLog\ncan be im- plemented efficiently. We conclude with a discussion of ongoing and\nfuture work."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3204", 
    "link": "http://arxiv.org/pdf/1304.4303v1", 
    "other_authors": "Azza Abouzied, Dana Angluin, Christos Papadimitriou, Joseph M. Hellerstein, Avi Silberschatz", 
    "title": "Learning and Verifying Quantified Boolean Queries by Example", 
    "arxiv-id": "1304.4303v1", 
    "author": "Avi Silberschatz", 
    "publish": "2013-04-16T00:21:25Z", 
    "summary": "To help a user specify and verify quantified queries --- a class of database\nqueries known to be very challenging for all but the most expert users --- one\ncan question the user on whether certain data objects are answers or\nnon-answers to her intended query. In this paper, we analyze the number of\nquestions needed to learn or verify qhorn queries, a special class of Boolean\nquantified queries whose underlying form is conjunctions of quantified Horn\nexpressions. We provide optimal polynomial-question and polynomial-time\nlearning and verification algorithms for two subclasses of the class qhorn with\nupper constant limits on a query's causal density."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3204", 
    "link": "http://arxiv.org/pdf/1304.4795v1", 
    "other_authors": "Shixi Chen, Shuigeng Zhou", 
    "title": "Recursive Mechanism: Towards Node Differential Privacy and Unrestricted   Joins [Full Version, Draft 0.1]", 
    "arxiv-id": "1304.4795v1", 
    "author": "Shuigeng Zhou", 
    "publish": "2013-04-17T12:40:15Z", 
    "summary": "Existing studies on differential privacy mainly consider aggregation on data\nsets where each entry corresponds to a particular participant to be protected.\nIn many situations, a user may pose a relational algebra query on a sensitive\ndatabase, and desires differentially private aggregation on the result of the\nquery. However, no known work is capable to release this kind of aggregation\nwhen the query contains unrestricted join operations. This severely limits the\napplications of existing differential privacy techniques because many data\nanalysis tasks require unrestricted joins. One example is subgraph counting on\na graph. Existing methods for differentially private subgraph counting address\nonly edge differential privacy and are subject to very simple subgraphs. Before\nthis work, whether any nontrivial graph statistics can be released with\nreasonable accuracy under node differential privacy is still an open problem.\n  In this paper, we propose a novel differentially private mechanism to release\nan approximation to a linear statistic of the result of some positive\nrelational algebra calculation over a sensitive database. Unrestricted joins\nare supported in our mechanism. The error bound of the approximate answer is\nroughly proportional to the \\emph{empirical sensitivity} of the query --- a new\nnotion that measures the maximum possible change to the query answer when a\nparticipant withdraws its data from the sensitive database. For subgraph\ncounting, our mechanism provides the first solution to achieve node\ndifferential privacy, for any kind of subgraphs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3204", 
    "link": "http://arxiv.org/pdf/1304.7285v1", 
    "other_authors": "Minyar Sassi-Hidri, Soukaina Ben Bdira", 
    "title": "Traitement approximatif des requ\u00eates flexibles avec groupement   d'attributs et jointure", 
    "arxiv-id": "1304.7285v1", 
    "author": "Soukaina Ben Bdira", 
    "publish": "2013-04-25T09:27:18Z", 
    "summary": "This paper addresses the problem of approximate processing for flexible\nqueries in the form SELECT-FROM-WHERE-GROUP BY with join condition. It offers a\nflexible framework for online aggregation while promoting response time at the\nexpense of result accuracy."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2723372.2746483", 
    "link": "http://arxiv.org/pdf/1304.7799v3", 
    "other_authors": "Medha Atre", 
    "title": "Left Bit Right: For SPARQL Join Queries with OPTIONAL Patterns   (Left-outer-joins)", 
    "arxiv-id": "1304.7799v3", 
    "author": "Medha Atre", 
    "publish": "2013-04-29T20:49:49Z", 
    "summary": "SPARQL basic graph pattern (BGP) (a.k.a. SQL inner-join) query optimization\nis a well researched area. However, optimization of OPTIONAL pattern queries\n(a.k.a. SQL left-outer-joins) poses additional challenges, due to the\nrestrictions on the \\textit{reordering} of left-outer-joins. The occurrence of\nsuch queries tends to be as high as 50% of the total queries (e.g., DBPedia\nquery logs).\n  In this paper, we present \\textit{Left Bit Right} (LBR), a technique for\n\\textit{well-designed} nested BGP and OPTIONAL pattern queries. Through LBR, we\npropose a novel method to represent such queries using a graph of\n\\textit{supernodes}, which is used to aggressively prune the RDF triples, with\nthe help of compressed indexes. We also propose novel optimization strategies\n-- first of a kind, to the best of our knowledge -- that combine together the\ncharacteristics of \\textit{acyclicity} of queries, \\textit{minimality}, and\n\\textit{nullification}, \\textit{best-match} operators. In this paper, we focus\non OPTIONAL patterns without UNIONs or FILTERs, but we also show how UNIONs and\nFILTERs can be handled with our technique using a \\textit{query rewrite}. Our\nevaluation on RDF graphs of up to and over one billion triples, on a commodity\nlaptop with 8 GB memory, shows that LBR can process \\textit{well-designed}\nlow-selectivity complex queries up to 11 times faster compared to the\nstate-of-the-art RDF column-stores as Virtuoso and MonetDB, and for highly\nselective queries, LBR is at par with them."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2723372.2746483", 
    "link": "http://arxiv.org/pdf/1304.7854v2", 
    "other_authors": "Leopoldo Bertossi, Jaffer Gardezi", 
    "title": "On the Complexity of Query Answering under Matching Dependencies for   Entity Resolution", 
    "arxiv-id": "1304.7854v2", 
    "author": "Jaffer Gardezi", 
    "publish": "2013-04-30T04:05:44Z", 
    "summary": "Matching Dependencies (MDs) are a relatively recent proposal for declarative\nentity resolution. They are rules that specify, given the similarities\nsatisfied by values in a database, what values should be considered duplicates,\nand have to be matched. On the basis of a chase-like procedure for MD\nenforcement, we can obtain clean (duplicate-free) instances; actually possibly\nseveral of them. The resolved answers to queries are those that are invariant\nunder the resulting class of resolved instances. In previous work we identified\nsome tractable cases (i.e. for certain classes of queries and MDs) of resolved\nquery answering. In this paper we further investigate the complexity of this\nproblem, identifying some intractable cases. For a special case we obtain a\ndichotomy complexity result."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2723372.2746483", 
    "link": "http://arxiv.org/pdf/1305.0502v2", 
    "other_authors": "Ruoming Jin, Guan Wang", 
    "title": "Simple, Fast, and Scalable Reachability Oracle", 
    "arxiv-id": "1305.0502v2", 
    "author": "Guan Wang", 
    "publish": "2013-05-02T16:39:04Z", 
    "summary": "A reachability oracle (or hop labeling) assigns each vertex v two sets of\nvertices: Lout(v) and Lin(v), such that u reaches v iff Lout(u) \\cap Lin(v)\n\\neq \\emptyset. Despite their simplicity and elegance, reachability oracles\nhave failed to achieve efficiency in more than ten years since their\nintroduction: the main problem is high construction cost, which stems from a\nset-cover framework and the need to materialize transitive closure. In this\npaper, we present two simple and efficient labeling algorithms,\nHierarchical-Labeling and Distribution-Labeling, which can work onmassive\nreal-world graphs: their construction time is an order of magnitude faster than\nthe setcover based labeling approach, and transitive closure materialization is\nnot needed. On large graphs, their index sizes and their query performance can\nnow beat the state-of-the-art transitive closure compression and online search\napproaches."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2723372.2746483", 
    "link": "http://arxiv.org/pdf/1305.1609v1", 
    "other_authors": "Yu Cheng, Florin Rusu", 
    "title": "Formal Representation of the SS-DB Benchmark and Experimental Evaluation   in EXTASCID", 
    "arxiv-id": "1305.1609v1", 
    "author": "Florin Rusu", 
    "publish": "2013-05-07T19:07:28Z", 
    "summary": "Evaluating the performance of scientific data processing systems is a\ndifficult task considering the plethora of application-specific solutions\navailable in this landscape and the lack of a generally-accepted benchmark. The\ndual structure of scientific data coupled with the complex nature of processing\ncomplicate the evaluation procedure further. SS-DB is the first attempt to\ndefine a general benchmark for complex scientific processing over raw and\nderived data. It fails to draw sufficient attention though because of the\nambiguous plain language specification and the extraordinary SciDB results. In\nthis paper, we remedy the shortcomings of the original SS-DB specification by\nproviding a formal representation in terms of ArrayQL algebra operators and\nArrayQL/SciQL constructs. These are the first formal representations of the\nSS-DB benchmark. Starting from the formal representation, we give a reference\nimplementation and present benchmark results in EXTASCID, a novel system for\nscientific data processing. EXTASCID is complete in providing native support\nboth for array and relational data and extensible in executing any user code\ninside the system by the means of a configurable metaoperator. These features\nresult in an order of magnitude improvement over SciDB at data loading,\nextracting derived data, and operations over derived data."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2723372.2746483", 
    "link": "http://arxiv.org/pdf/1305.1713v1", 
    "other_authors": "Meenesh Bhardwaj", 
    "title": "Optimization of stochastic database cracking", 
    "arxiv-id": "1305.1713v1", 
    "author": "Meenesh Bhardwaj", 
    "publish": "2013-05-08T04:26:10Z", 
    "summary": "Variant Stochastic cracking is a significantly more resilient approach to\nadaptive indexing. It showed [1]that Stochastic cracking uses each query as a\nhint on how to reorganize data, but not blindly so; it gains resilience and\navoids performance bottlenecks by deliberately applying certain arbitrary\nchoices in its decision making. Therefore bring, adaptive indexing forward to a\nmature formulation that confers the workload-robustness that previous\napproaches lacked. Original cracking relies on the randomness of the workloads\nto converge well. [2][3] However, where the workload is non-random, cracking\nneeds to introduce randomness on its own. Stochastic Cracking clearly improves\nover original cracking by being robust in workload changes while maintaining\nall original cracking features when it comes to adaptation. But looking at both\ntypes of cracking, it conveyed an incomplete picture as at some point of time\nit is must to know whether the workload is random or sequential. In this paper\nour focus is on optimization of variant stochastic cracking, that could be\nachieved in two ways either by reducing the initialization cost to make\nstochastic cracking even more transparent to the user, especially for queries\nthat initiate a workload change and hence incur a higher cost or by combining\nthe strengths of the various stochastic cracking algorithms via a dynamic\ncomponent that decides which algorithm to choose for a query on the fly. The\nefforts have been put in to make an algorithm that reduces the initialization\ncost by using the main notion of both cracking, while considering the\nrequirements of adaptive indexing [2]."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TKDE.2015.2397440", 
    "link": "http://arxiv.org/pdf/1305.2103v2", 
    "other_authors": "Jacek Sroka, Adrian Panasiuk, Krzysztof Stencel, Jerzy Tyszkiewicz", 
    "title": "Translating Relational Queries into Spreadsheets", 
    "arxiv-id": "1305.2103v2", 
    "author": "Jerzy Tyszkiewicz", 
    "publish": "2013-05-09T14:30:12Z", 
    "summary": "Spreadsheets are among the most commonly used applications for data\nmanagement and analysis. Perhaps they are even among the most widely used\ncomputer applications of all kinds. They combine in a natural and intuitive way\ndata processing with very diverse supplementary features: statistical\nfunctions, visualization tools, pivot tables, pivot charts, linear programming\nsolvers, Web queries periodically downloading data from external sources, etc.\nHowever, the spreadsheet paradigm of computation still lacks sufficient\nanalysis.\n  In this article we demonstrate that a spreadsheet can implement all data\ntransformations definable in SQL, without any use of macros or built-in\nprogramming languages, merely by utilizing spreadsheet formulas. We provide a\nquery compiler, which translates any given SQL query into a worksheet of the\nsame semantics, including NULL values.\n  Thereby database operations become available to the users who do not want to\nmigrate to a database. They can define their queries using a high-level\nlanguage and then get their execution plans in a plain vanilla spreadsheet. No\nsophisticated database system, no spreadsheet plugins or macros are needed.\n  The functions available in spreadsheets impose severe limitations on the\nalgorithms one can implement. In this paper we offer $O(n\\log^2n)$ sorting\nspreadsheet, but using a non-constant number of rows, improving on the\npreviously known $O(n^2)$ ones.\n  It is therefore surprising, that a spreadsheet can implement, as we\ndemonstrate, Depth-First-Search and Breadth-First-Search on graphs, thereby\nreaching beyond queries definable in SQL-92."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TKDE.2015.2397440", 
    "link": "http://arxiv.org/pdf/1305.2758v1", 
    "other_authors": "1Oumair Naseer, 2Ayesha Naseer, 3Atif Ali Khan, 4Humza Naseer", 
    "title": "Using Page Size for Controlling Duplicate Query Results in Semantic Web", 
    "arxiv-id": "1305.2758v1", 
    "author": "4Humza Naseer", 
    "publish": "2013-05-13T12:33:58Z", 
    "summary": "Semantic web is a web of future. The Resource Description Framework (RDF) is\na language to represent resources in the World Wide Web. When these resources\nare queried the problem of duplicate query results occurs. The present\ntechniques used hash index comparison to remove duplicate query results. The\nmajor drawback of using the hash index to remove duplicate query results is\nthat, if there is a slight change in formatting or word order, then hash index\nis changed and query results are no more considered as duplicate even though\nthey have same contents. We presented an algorithm for detection and\nelimination of duplicate query results from semantic web using hash index and\npage size comparisons. Experimental results showed that the proposed technique\nremoved duplicate query results from semantic web efficiently, solved the\nproblems of using hash index for duplicate handling and could be embedded in\nexisting SQL-Based query system for semantic web. Research could be carried out\nfor certain flexibilities in existing SQL-Based query system of semantic web to\naccommodate other duplicate detection techniques as well."
},{
    "category": "cs.DB", 
    "doi": "10.1109/TKDE.2015.2397440", 
    "link": "http://arxiv.org/pdf/1305.3058v1", 
    "other_authors": "Serge Abiteboul, \u00c9milien Antoine, Gerome Miklau, Julia Stoyanovich, Jules Testard", 
    "title": "Rule-Based Application Development using Webdamlog", 
    "arxiv-id": "1305.3058v1", 
    "author": "Jules Testard", 
    "publish": "2013-05-14T08:31:10Z", 
    "summary": "We present the WebdamLog system for managing distributed data on the Web in a\npeer-to-peer manner. We demonstrate the main features of the system through an\napplication called Wepic for sharing pictures between attendees of the sigmod\nconference. Using Wepic, the attendees will be able to share, download, rate\nand annotate pictures in a highly decentralized manner. We show how WebdamLog\nhandles heterogeneity of the devices and services used to share data in such a\nWeb setting. We exhibit the simple rules that define the Wepic application and\nshow how to easily modify the Wepic application."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.3082v1", 
    "other_authors": "Jialong Han, Ji-Rong Wen", 
    "title": "Mining Frequent Neighborhood Patterns in Large Labeled Graphs", 
    "arxiv-id": "1305.3082v1", 
    "author": "Ji-Rong Wen", 
    "publish": "2013-05-14T09:46:17Z", 
    "summary": "Over the years, frequent subgraphs have been an important sort of targeted\npatterns in the pattern mining literatures, where most works deal with\ndatabases holding a number of graph transactions, e.g., chemical structures of\ncompounds. These methods rely heavily on the downward-closure property (DCP) of\nthe support measure to ensure an efficient pruning of the candidate patterns.\nWhen switching to the emerging scenario of single-graph databases such as\nGoogle Knowledge Graph and Facebook social graph, the traditional support\nmeasure turns out to be trivial (either 0 or 1). However, to the best of our\nknowledge, all attempts to redefine a single-graph support resulted in measures\nthat either lose DCP, or are no longer semantically intuitive.\n  This paper targets mining patterns in the single-graph setting. We resolve\nthe \"DCP-intuitiveness\" dilemma by shifting the mining target from frequent\nsubgraphs to frequent neighborhoods. A neighborhood is a specific topological\npattern where a vertex is embedded, and the pattern is frequent if it is shared\nby a large portion (above a given threshold) of vertices. We show that the new\npatterns not only maintain DCP, but also have equally significant semantics as\nsubgraph patterns. Experiments on real-life datasets display the feasibility of\nour algorithms on relatively large graphs, as well as the capability of mining\ninteresting knowledge that is not discovered in prior works."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.3407v2", 
    "other_authors": "Johannes Niedermayer, Andreas Z\u00fcfle, Tobias Emrich, Matthias Renz, Nikos Mamoulis, Lei Chen, Hans-Peter Kriegel", 
    "title": "Probabilistic Nearest Neighbor Queries on Uncertain Moving Object   Trajectories", 
    "arxiv-id": "1305.3407v2", 
    "author": "Hans-Peter Kriegel", 
    "publish": "2013-05-15T09:54:58Z", 
    "summary": "Nearest neighbor (NN) queries in trajectory databases have received\nsignificant attention in the past, due to their application in spatio-temporal\ndata analysis. Recent work has considered the realistic case where the\ntrajectories are uncertain; however, only simple uncertainty models have been\nproposed, which do not allow for accurate probabilistic search. In this paper,\nwe fill this gap by addressing probabilistic nearest neighbor queries in\ndatabases with uncertain trajectories modeled by stochastic processes,\nspecifically the Markov chain model. We study three nearest neighbor query\nsemantics that take as input a query state or trajectory $q$ and a time\ninterval. For some queries, we show that no polynomial time solution can be\nfound. For problems that can be solved in PTIME, we present exact query\nevaluation algorithms, while for the general case, we propose a sophisticated\nsampling approach, which uses Bayesian inference to guarantee that sampled\ntrajectories conform to the observation data stored in the database. This\nsampling approach can be used in Monte-Carlo based approximation solutions. We\ninclude an extensive experimental study to support our theoretical results."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.4195v2", 
    "other_authors": "Susan B. Davidson, Xiaocheng Huang, Julia Stoyanovich, Xiaojie Yuan", 
    "title": "Search and Result Presentation in Scientific Workflow Repositories", 
    "arxiv-id": "1305.4195v2", 
    "author": "Xiaojie Yuan", 
    "publish": "2013-05-17T21:22:14Z", 
    "summary": "We study the problem of searching a repository of complex hierarchical\nworkflows whose component modules, both composite and atomic, have been\nannotated with keywords. Since keyword search does not use the graph structure\nof a workflow, we develop a model of workflows using context-free bag grammars.\nWe then give efficient polynomial-time algorithms that, given a workflow and a\nkeyword query, determine whether some execution of the workflow matches the\nquery. Based on these algorithms we develop a search and ranking solution that\nefficiently retrieves the top-k grammars from a repository. Finally, we propose\na novel result presentation method for grammars matching a keyword query, based\non representative parse-trees. The effectiveness of our approach is validated\nthrough an extensive experimental evaluation."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.5653v1", 
    "other_authors": "George Garbis, Kostis Kyzirakos, Manolis Koubarakis", 
    "title": "Geographica: A Benchmark for Geospatial RDF Stores", 
    "arxiv-id": "1305.5653v1", 
    "author": "Manolis Koubarakis", 
    "publish": "2013-05-24T08:54:46Z", 
    "summary": "Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently\nbeen defined and corresponding geospatial RDF stores have been implemented.\nHowever, there is no widely used benchmark for evaluating geospatial RDF stores\nwhich takes into account recent advances to the state of the art in this area.\nIn this paper, we develop a benchmark, called Geographica, which uses both\nreal-world and synthetic data to test the offered functionality and the\nperformance of some prominent geospatial RDF stores."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.5824v1", 
    "other_authors": "Slim Bouker, Rabie Saidi, Sadok Ben Yahia, Engelbert Mephu Nguifo", 
    "title": "Towards a semantic and statistical selection of association rules", 
    "arxiv-id": "1305.5824v1", 
    "author": "Engelbert Mephu Nguifo", 
    "publish": "2013-05-24T18:46:34Z", 
    "summary": "The increasing growth of databases raises an urgent need for more accurate\nmethods to better understand the stored data. In this scope, association rules\nwere extensively used for the analysis and the comprehension of huge amounts of\ndata. However, the number of generated rules is too large to be efficiently\nanalyzed and explored in any further process. Association rules selection is a\nclassical topic to address this issue, yet, new innovated approaches are\nrequired in order to provide help to decision makers. Hence, many interesting-\nness measures have been defined to statistically evaluate and filter the\nassociation rules. However, these measures present two major problems. On the\none hand, they do not allow eliminating irrelevant rules, on the other hand,\ntheir abun- dance leads to the heterogeneity of the evaluation results which\nleads to confusion in decision making. In this paper, we propose a two-winged\napproach to select statistically in- teresting and semantically incomparable\nrules. Our statis- tical selection helps discovering interesting association\nrules without favoring or excluding any measure. The semantic comparability\nhelps to decide if the considered association rules are semantically related\ni.e comparable. The outcomes of our experiments on real datasets show promising\nresults in terms of reduction in the number of rules."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.6506v1", 
    "other_authors": "Michael Hausenblas", 
    "title": "Notes on Physical & Logical Data Layouts", 
    "arxiv-id": "1305.6506v1", 
    "author": "Michael Hausenblas", 
    "publish": "2013-05-28T14:16:11Z", 
    "summary": "In this short note I review and discuss fundamental options for physical and\nlogical data layouts as well as the impact of the choices on data processing. I\nshould say in advance that these notes offer no new insights, that is,\neverything stated here has already been published elsewhere. In fact, it has\nbeen published in so many different places, such as blog posts, in the\nliterature, etc. that the main contribution is to bring it all together in one\nplace."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1305.7006v1", 
    "other_authors": "Walaa Eldin Moustafa, Angelika Kimmig, Amol Deshpande, Lise Getoor", 
    "title": "Subgraph Pattern Matching over Uncertain Graphs with Identity Linkage   Uncertainty", 
    "arxiv-id": "1305.7006v1", 
    "author": "Lise Getoor", 
    "publish": "2013-05-30T05:27:23Z", 
    "summary": "There is a growing need for methods which can capture uncertainties and\nanswer queries over graph-structured data. Two common types of uncertainty are\nuncertainty over the attribute values of nodes and uncertainty over the\nexistence of edges. In this paper, we combine those with identity uncertainty.\nIdentity uncertainty represents uncertainty over the mapping from objects\nmentioned in the data, or references, to the underlying real-world entities. We\npropose the notion of a probabilistic entity graph (PEG), a probabilistic graph\nmodel that defines a distribution over possible graphs at the entity level. The\nmodel takes into account node attribute uncertainty, edge existence\nuncertainty, and identity uncertainty, and thus enables us to systematically\nreason about all three types of uncertainties in a uniform manner. We introduce\na general framework for constructing a PEG given uncertain data at the\nreference level and develop highly efficient algorithms to answer subgraph\npattern matching queries in this setting. Our algorithms are based on two novel\nideas: context-aware path indexing and reduction by join-candidates, which\ndrastically reduce the query search space. A comprehensive experimental\nevaluation shows that our approach outperforms baseline implementations by\norders of magnitude."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1306.1334v1", 
    "other_authors": "Hitesh Chhinkaniwala, Sanjay Garg", 
    "title": "Tuple Value Based Multiplicative Data Perturbation Approach To Preserve   Privacy In Data Stream Mining", 
    "arxiv-id": "1306.1334v1", 
    "author": "Sanjay Garg", 
    "publish": "2013-06-06T08:15:22Z", 
    "summary": "Huge volume of data from domain specific applications such as medical,\nfinancial, library, telephone, shopping records and individual are regularly\ngenerated. Sharing of these data is proved to be beneficial for data mining\napplication. On one hand such data is an important asset to business decision\nmaking by analyzing it. On the other hand data privacy concerns may prevent\ndata owners from sharing information for data analysis. In order to share data\nwhile preserving privacy, data owner must come up with a solution which\nachieves the dual goal of privacy preservation as well as an accuracy of data\nmining task - clustering and classification. An efficient and effective\napproach has been proposed that aims to protect privacy of sensitive\ninformation and obtaining data clustering with minimum information loss."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2505515.2505530", 
    "link": "http://arxiv.org/pdf/1306.1689v1", 
    "other_authors": "Simon Razniewski, Marco Montali, Werner Nutt", 
    "title": "Verification of Query Completeness over Processes [Extended Version]", 
    "arxiv-id": "1306.1689v1", 
    "author": "Werner Nutt", 
    "publish": "2013-06-07T11:17:41Z", 
    "summary": "Data completeness is an essential aspect of data quality, and has in turn a\nhuge impact on the effective management of companies. For example, statistics\nare computed and audits are conducted in companies by implicitly placing the\nstrong assumption that the analysed data are complete. In this work, we are\ninterested in studying the problem of completeness of data produced by business\nprocesses, to the aim of automatically assessing whether a given database query\ncan be answered with complete information in a certain state of the process. We\nformalize so-called quality-aware processes that create data in the real world\nand store it in the company's information system possibly at a later point."
},{
    "category": "cs.DB", 
    "doi": "10.5121/IJDKP.2013.3306", 
    "link": "http://arxiv.org/pdf/1306.1730v1", 
    "other_authors": "M. Laxmaiah, A. Govardhan", 
    "title": "A Conceptual Metadata Framework for Spatial Data Warehouse", 
    "arxiv-id": "1306.1730v1", 
    "author": "A. Govardhan", 
    "publish": "2013-06-07T13:56:43Z", 
    "summary": "Metadata represents the information about data to be stored in Data\nWarehouses.It is a mandatory element of Data Warehouse to build an efficient\nData Warehouse.Metadata helps in data integration,lineage,data quality and\npopulating transformed data into data warehouse.Spatial data warehouses are\nbased on spatial data mostly collected from Geographical Information\nSystems(GIS)and the transactional systems that are specific to an application\nor enterprise.Metadata design and deployment is the most critical phase in\nbuilding of data warehouse where it is mandatory to bring the spatial\ninformation and data modeling together.In this paper,we present a holistic\nmetadata framework that drives metadata creation for spatial data warehouse.\nTheoretically, the proposed metadata framework improves the efficiency of\naccessing of data in response to frequent queries on SDWs.In other words, the\nproposed framework decreases the response time of the query and accurate\ninformation is fetched from Data Warehouse including the spatial information."
},{
    "category": "cs.DB", 
    "doi": "10.5121/IJDKP.2013.3306", 
    "link": "http://arxiv.org/pdf/1306.2459v1", 
    "other_authors": "Sutanay Choudhury, Lawrence Holder, George Chin, John Feo", 
    "title": "Fast Search for Dynamic Multi-Relational Graphs", 
    "arxiv-id": "1306.2459v1", 
    "author": "John Feo", 
    "publish": "2013-06-11T09:21:42Z", 
    "summary": "Acting on time-critical events by processing ever growing social media or\nnews streams is a major technical challenge. Many of these data sources can be\nmodeled as multi-relational graphs. Continuous queries or techniques to search\nfor rare events that typically arise in monitoring applications have been\nstudied extensively for relational databases. This work is dedicated to answer\nthe question that emerges naturally: how can we efficiently execute a\ncontinuous query on a dynamic graph? This paper presents an exact subgraph\nsearch algorithm that exploits the temporal characteristics of representative\nqueries for online news or social media monitoring. The algorithm is based on a\nnovel data structure called the Subgraph Join Tree (SJ-Tree) that leverages the\nstructural and semantic characteristics of the underlying multi-relational\ngraph. The paper concludes with extensive experimentation on several real-world\ndatasets that demonstrates the validity of this approach."
},{
    "category": "cs.DB", 
    "doi": "10.5121/IJDKP.2013.3306", 
    "link": "http://arxiv.org/pdf/1306.2460v1", 
    "other_authors": "Sutanay Choudhury, Lawrence Holder, George Chin, Abhik Ray, Sherman Beus, John Feo", 
    "title": "StreamWorks - A system for Dynamic Graph Search", 
    "arxiv-id": "1306.2460v1", 
    "author": "John Feo", 
    "publish": "2013-06-11T09:24:06Z", 
    "summary": "Acting on time-critical events by processing ever growing social media, news\nor cyber data streams is a major technical challenge. Many of these data\nsources can be modeled as multi-relational graphs. Mining and searching for\nsubgraph patterns in a continuous setting requires an efficient approach to\nincremental graph search. The goal of our work is to enable real-time search\ncapabilities for graph databases. This demonstration will present a dynamic\ngraph query system that leverages the structural and semantic characteristics\nof the underlying multi-relational graph."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1306.2691v1", 
    "other_authors": "Ivan Gazeau, Dale Miller, Catuscia Palamidessi", 
    "title": "Preserving differential privacy under finite-precision semantics", 
    "arxiv-id": "1306.2691v1", 
    "author": "Catuscia Palamidessi", 
    "publish": "2013-06-12T01:55:18Z", 
    "summary": "The approximation introduced by finite-precision representation of continuous\ndata can induce arbitrarily large information leaks even when the computation\nusing exact semantics is secure. Such leakage can thus undermine design efforts\naimed at protecting sensitive information. We focus here on differential\nprivacy, an approach to privacy that emerged from the area of statistical\ndatabases and is now widely applied also in other domains. In this approach,\nprivacy is protected by the addition of noise to a true (private) value. To\ndate, this approach to privacy has been proved correct only in the ideal case\nin which computations are made using an idealized, infinite-precision\nsemantics. In this paper, we analyze the situation at the implementation level,\nwhere the semantics is necessarily finite-precision, i.e. the representation of\nreal numbers and the operations on them, are rounded according to some level of\nprecision. We show that in general there are violations of the differential\nprivacy property, and we study the conditions under which we can still\nguarantee a limited (but, arguably, totally acceptable) variant of the\nproperty, under only a minor degradation of the privacy level. Finally, we\nillustrate our results on two cases of noise-generating distributions: the\nstandard Laplacian mechanism commonly used in differential privacy, and a\nbivariate version of the Laplacian recently introduced in the setting of\nprivacy-aware geolocation."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1306.5690v1", 
    "other_authors": "Dhammika Pieris", 
    "title": "Modifying the Entity relationship modelling notation: towards high   quality relational databases from better notated ER models", 
    "arxiv-id": "1306.5690v1", 
    "author": "Dhammika Pieris", 
    "publish": "2013-06-21T06:47:48Z", 
    "summary": "The entity relationship modelling using the original ER notation has been\napplauded providing a natural view of data in conceptual modelling of\ninformation systems. However, the current ER to relational model transformation\nalgorithm is known to be insufficient in providing a complete and accurate\nrepresentation of the ER model undertaken for transformation. In an effort to\nderive better transformations from ER models, we have understood that\nmodifications should be introduced to both of the existing transformation\nalgorithm as well as to the ER notation. Introducing some new concepts, we have\nadapted the original ER notation and developed a new transformation algorithm\nbased on the existing one. This paper presents the modified ER notation with an\nER diagram drawn based on the new notation."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1306.5972v1", 
    "other_authors": "Paul Beame, Paraschos Koutris, Dan Suciu", 
    "title": "Communication Steps for Parallel Query Processing", 
    "arxiv-id": "1306.5972v1", 
    "author": "Dan Suciu", 
    "publish": "2013-06-25T14:04:49Z", 
    "summary": "We consider the problem of computing a relational query $q$ on a large input\ndatabase of size $n$, using a large number $p$ of servers. The computation is\nperformed in rounds, and each server can receive only $O(n/p^{1-\\varepsilon})$\nbits of data, where $\\varepsilon \\in [0,1]$ is a parameter that controls\nreplication. We examine how many global communication steps are needed to\ncompute $q$. We establish both lower and upper bounds, in two settings. For a\nsingle round of communication, we give lower bounds in the strongest possible\nmodel, where arbitrary bits may be exchanged; we show that any algorithm\nrequires $\\varepsilon \\geq 1-1/\\tau^*$, where $\\tau^*$ is the fractional vertex\ncover of the hypergraph of $q$. We also give an algorithm that matches the\nlower bound for a specific class of databases. For multiple rounds of\ncommunication, we present lower bounds in a model where routing decisions for a\ntuple are tuple-based. We show that for the class of tree-like queries there\nexists a tradeoff between the number of rounds and the space exponent\n$\\varepsilon$. The lower bounds for multiple rounds are the first of their\nkind. Our results also imply that transitive closure cannot be computed in O(1)\nrounds of communication."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1306.6670v1", 
    "other_authors": "Olivier Cur\u00e9, David Faye, Guillaume Blin", 
    "title": "Towards a better insight of RDF triples Ontology-guided Storage system   abilities", 
    "arxiv-id": "1306.6670v1", 
    "author": "Guillaume Blin", 
    "publish": "2013-06-27T22:06:27Z", 
    "summary": "The vision of the Semantic Web is becoming a reality with billions of RDF\ntriples being distributed over multiple queryable end-points (e.g. Linked\nData). Although there has been a body of work on RDF triples persistent\nstorage, it seems that, considering reasoning dependent queries, the problem of\nproviding an efficient, in terms of performance, scalability and data\nredundancy, partitioning of the data is still open. In regards to recent data\npartitioning studies, it seems reasonable to think that data partitioning\nshould be guided considering several directions (e.g. ontology, data,\napplication queries). This paper proposes several contributions: describe an\noverview of what a road map for data partitioning for RDF data efficient and\npersistent storage should contain, present some preliminary results and\nanalysis on the particular case of ontology-guided (property hierarchy)\npartitioning and finally introduce a set of semantic query rewriting rules to\nsupport querying RDF data needing OWL inferences"
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1306.6734v1", 
    "other_authors": "Dhammika Pieris", 
    "title": "A novel ER model to relational model transformation algorithm for   semantically clear high quality database design", 
    "arxiv-id": "1306.6734v1", 
    "author": "Dhammika Pieris", 
    "publish": "2013-06-28T07:12:23Z", 
    "summary": "Conceptual modelling using the entity relationship (ER) model has been widely\nused for database design for a long period of time. However, studies indicate\nthat creating a satisfactory relational model representation from an ER model\nis uncertain due to the insufficiencies both in the transformation methods used\nand in the relational model itself. In an effort to solve the issue the\noriginal ER notation has been modified, and accordingly, a new transformation\nalgorithm has been developed. This paper presents the proposed transformation\nalgorithm. Using a real world example it shows how the algorithm can be applied\nin practice. The paper also discusses how to validate the resulted database and\nreclaim the information that it represents."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1309.0373v1", 
    "other_authors": "Sebastiaan J. van Schaik, Dan Olteanu, Robert Fink", 
    "title": "ENFrame: A Platform for Processing Probabilistic Data", 
    "arxiv-id": "1309.0373v1", 
    "author": "Robert Fink", 
    "publish": "2013-09-02T12:02:34Z", 
    "summary": "This paper introduces ENFrame, a unified data processing platform for\nquerying and mining probabilistic data. Using ENFrame, users can write programs\nin a fragment of Python with constructs such as bounded-range loops, list\ncomprehension, aggregate operations on lists, and calls to external database\nengines. The program is then interpreted probabilistically by ENFrame.\n  The realisation of ENFrame required novel contributions along several\ndirections. We propose an event language that is expressive enough to\nsuccinctly encode arbitrary correlations, trace the computation of user\nprograms, and allow for computation of discrete probability distributions of\nprogram variables. We exemplify ENFrame on three clustering algorithms:\nk-means, k-medoids, and Markov Clustering. We introduce sequential and\ndistributed algorithms for computing the probability of interconnected events\nexactly or approximately with error guarantees. Experiments with k-medoids\nclustering of sensor readings from energy networks show orders-of-magnitude\nimprovements of exact clustering using ENFrame over na\\\"ive clustering in each\npossible world, of approximate over exact, and of distributed over sequential\nalgorithms."
},{
    "category": "cs.DB", 
    "doi": "10.4204/EPTCS.117.1", 
    "link": "http://arxiv.org/pdf/1309.1556v1", 
    "other_authors": "Yu cao, Xiaoyan Guo, Stephen Todd", 
    "title": "Hyper-Graph Based Database Partitioning for Transactional Workloads", 
    "arxiv-id": "1309.1556v1", 
    "author": "Stephen Todd", 
    "publish": "2013-09-06T07:35:53Z", 
    "summary": "A common approach to scaling transactional databases in practice is\nhorizontal partitioning, which increases system scalability, high availability\nand self-manageability. Usu- ally it is very challenging to choose or design an\noptimal partitioning scheme for a given workload and database. In this\ntechnical report, we propose a fine-grained hyper-graph based database\npartitioning system for transactional work- loads. The partitioning system\ntakes a database, a workload, a node cluster and partitioning constraints as\ninput and out- puts a lookup-table encoding the final database partitioning\ndecision. The database partitioning problem is modeled as a multi-constraints\nhyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our\nsystem can min- imize the total number of distributed transactions in the\nworkload, balance the sizes and workload accesses of the partitions and satisfy\nall the partition constraints imposed. Our system is highly interactive as it\nallows users to im- pose partition constraints, watch visualized partitioning\nef- fects, and provide feedback based on human expertise and indirect domain\nknowledge for generating better partition- ing schemes."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.2371v1", 
    "other_authors": "Arpna Shrivastava, R. C. Jain", 
    "title": "Performance analysis of modified algorithm for finding multilevel   association rules", 
    "arxiv-id": "1309.2371v1", 
    "author": "R. C. Jain", 
    "publish": "2013-09-10T04:52:58Z", 
    "summary": "Multilevel association rules explore the concept hierarchy at multiple levels\nwhich provides more specific information. Apriori algorithm explores the single\nlevel association rules. Many implementations are available of Apriori\nalgorithm. Fast Apriori implementation is modified to develop new algorithm for\nfinding multilevel association rules. In this study the performance of this new\nalgorithm is analyzed in terms of running time in seconds."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.2517v1", 
    "other_authors": "R. H. Vishwanath, S. Leena, K. C. Srikantaiah, K. Shreekrishna Kumar, P. Deepa Shenoy, K. R. Venugopal, S. S. Iyengar, L. M. Patnaik", 
    "title": "Forecasting Stock Time-Series using Data Approximation and Pattern   Sequence Similarity", 
    "arxiv-id": "1309.2517v1", 
    "author": "L. M. Patnaik", 
    "publish": "2013-09-10T14:05:09Z", 
    "summary": "Time series analysis is the process of building a model using statistical\ntechniques to represent characteristics of time series data. Processing and\nforecasting huge time series data is a challenging task. This paper presents\nApproximation and Prediction of Stock Time-series data (APST), which is a two\nstep approach to predict the direction of change of stock price indices. First,\nperforms data approximation by using the technique called Multilevel Segment\nMean (MSM). In second phase, prediction is performed for the approximated data\nusing Euclidian distance and Nearest-Neighbour technique. The computational\ncost of data approximation is O(n ni) and computational cost of prediction task\nis O(m |NN|). Thus, the accuracy and the time required for prediction in the\nproposed method is comparatively efficient than the existing Label Based\nForecasting (LBF) method [1]."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.2597v1", 
    "other_authors": "Bondu Venkateswarlu, Prof G. S. V. Prasad Raju", 
    "title": "Mine Blood Donors Information through Improved K-Means Clustering", 
    "arxiv-id": "1309.2597v1", 
    "author": "Prof G. S. V. Prasad Raju", 
    "publish": "2013-09-10T18:07:58Z", 
    "summary": "The number of accidents and health diseases which are increasing at an\nalarming rate are resulting in a huge increase in the demand for blood. There\nis a necessity for the organized analysis of the blood donor database or blood\nbanks repositories. Clustering analysis is one of the data mining applications\nand K-means clustering algorithm is the fundamental algorithm for modern\nclustering techniques. K-means clustering algorithm is traditional approach and\niterative algorithm. At every iteration, it attempts to find the distance from\nthe centroid of each cluster to each and every data point. This paper gives the\nimprovement to the original k-means algorithm by improving the initial\ncentroids with distribution of data. Results and discussions show that improved\nK-means algorithm produces accurate clusters in less computation time to find\nthe donors information."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.2687v1", 
    "other_authors": "Han Su", 
    "title": "CrowdPlanner: A Crowd-Based Route Recommendation System", 
    "arxiv-id": "1309.2687v1", 
    "author": "Han Su", 
    "publish": "2013-09-10T23:06:57Z", 
    "summary": "CrowdPlanner -- a novel crowd-based route recommendation system has been\ndeveloped, which requests human workers to evaluate candidates routes\nrecommended by different sources and methods, and determine the best route\nbased on the feedbacks of these workers. Our system addresses two critical\nissues in its core components: a) task generation component generates a series\nof informative and concise questions with optimized ordering for a given\ncandidate route set so that workers feel comfortable and easy to answer; and b)\nworker selection component utilizes a set of selection criteria and an\nefficient algorithm to find the most eligible workers to answer the questions\nwith high accuracy."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.3733v1", 
    "other_authors": "Jixue Liu, Selasi Kwashie, Jiuyong Li, Feiyue Ye, Millist Vincent", 
    "title": "Discovery of Approximate Differential Dependencies", 
    "arxiv-id": "1309.3733v1", 
    "author": "Millist Vincent", 
    "publish": "2013-09-15T06:21:30Z", 
    "summary": "Differential dependencies (DDs) capture the relationships between data\ncolumns of relations. They are more general than functional dependencies (FDs)\nand and the difference is that DDs are defined on the distances between values\nof two tuples, not directly on the values. Because of this difference, the\nalgorithms for discovering FDs from data find only special DDs, not all DDs and\ntherefore are not applicable to DD discovery. In this paper, we propose an\nalgorithm to discover DDs from data following the way of fixing the left hand\nside of a candidate DD to determine the right hand side. We also show some\nproperties of DDs and conduct a comprehensive analysis on how sampling affects\nthe DDs discovered from data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1309.5821v1", 
    "other_authors": "Jonathan Stuart Ward, Adam Barker", 
    "title": "Undefined By Data: A Survey of Big Data Definitions", 
    "arxiv-id": "1309.5821v1", 
    "author": "Adam Barker", 
    "publish": "2013-09-20T13:51:18Z", 
    "summary": "The term big data has become ubiquitous. Owing to a shared origin between\nacademia, industry and the media there is no single unified definition, and\nvarious stakeholders provide diverse and often contradictory definitions. The\nlack of a consistent definition introduces ambiguity and hampers discourse\nrelating to big data. This short paper attempts to collate the various\ndefinitions which have gained some degree of traction and to furnish a clear\nand concise definition of an otherwise ambiguous term."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.0141v4", 
    "other_authors": "Alexander Russakovsky", 
    "title": "Hopping over Big Data: Accelerating Ad-hoc OLAP Queries with Grasshopper   Algorithms", 
    "arxiv-id": "1310.0141v4", 
    "author": "Alexander Russakovsky", 
    "publish": "2013-10-01T05:02:14Z", 
    "summary": "This paper presents a family of algorithms for fast subset filtering within\nordered sets of integers representing composite keys. Applications include\nsignificant acceleration of (ad-hoc) analytic queries against a data warehouse\nwithout any additional indexing. The algorithms work for point, range and set\nrestrictions on multiple attributes, in any combination, and are inherently\nmultidimensional. The main idea consists in intelligent combination of\nsequential crawling with jumps over large portions of irrelevant keys. The way\nto combine them is adaptive to characteristics of the underlying data store."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.1161v1", 
    "other_authors": "Bibudh Lahiri, Arko Provo Mukherjee, Srikanta Tirthapura", 
    "title": "Identifying Correlated Heavy-Hitters in a Two-Dimensional Data Stream", 
    "arxiv-id": "1310.1161v1", 
    "author": "Srikanta Tirthapura", 
    "publish": "2013-10-04T03:48:41Z", 
    "summary": "We consider online mining of correlated heavy-hitters from a data stream.\nGiven a stream of two-dimensional data, a correlated aggregate query first\nextracts a substream by applying a predicate along a primary dimension, and\nthen computes an aggregate along a secondary dimension. Prior work on\nidentifying heavy-hitters in streams has almost exclusively focused on\nidentifying heavy-hitters on a single dimensional stream, and these yield\nlittle insight into the properties of heavy-hitters along other dimensions. In\ntypical applications however, an analyst is interested not only in identifying\nheavy-hitters, but also in understanding further properties such as: what other\nitems appear frequently along with a heavy-hitter, or what is the frequency\ndistribution of items that appear along with the heavy-hitters. We consider\nqueries of the following form: In a stream S of (x, y) tuples, on the substream\nH of all x values that are heavy-hitters, maintain those y values that occur\nfrequently with the x values in H. We call this problem as Correlated\nHeavy-Hitters (CHH). We formulate an approximate formulation of CHH\nidentification, and present an algorithm for tracking CHHs on a data stream.\nThe algorithm is easy to implement and uses workspace which is orders of\nmagnitude smaller than the stream itself. We present provable guarantees on the\nmaximum error, as well as detailed experimental results that demonstrate the\nspace-accuracy trade-off."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.1190v1", 
    "other_authors": "Priyanka Dash, Ranjita Rout, Satya Bhusan Pratihari, Sanjay Kumar Padhi", 
    "title": "Review on Fragment Allocation by using Clustering Technique in   Distributed Database System", 
    "arxiv-id": "1310.1190v1", 
    "author": "Sanjay Kumar Padhi", 
    "publish": "2013-10-04T07:43:00Z", 
    "summary": "Considerable Progress has been made in the last few years in improving the\nperformance of the distributed database systems. The development of Fragment\nallocation models in Distributed database is becoming difficult due to the\ncomplexity of huge number of sites and their communication considerations.\nUnder such conditions, simulation of clustering and data allocation is adequate\ntools for understanding and evaluating the performance of data allocation in\nDistributed databases. Clustering sites and fragment allocation are key\nchallenges in Distributed database performance, and are considered to be\nefficient methods that have a major role in reducing transferred and accessed\ndata during the execution of applications. In this paper a review on Fragment\nallocation by using Clustering technique is given in Distributed Database\nSystem."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.2367v1", 
    "other_authors": "Mrs. Dhanamma Jagli, Ms. Priyanka Gaikwad, Ms. Shubhangi Gunjal, Mr. Chaitanya Bilaware", 
    "title": "Handy Annotations within Oracle 10g", 
    "arxiv-id": "1310.2367v1", 
    "author": "Mr. Chaitanya Bilaware", 
    "publish": "2013-10-09T06:22:15Z", 
    "summary": "This paper describes practical observations during the Database system Lab.\nOracle 10g DBMS is used in the data base system lab and performed SQL queries\nbased many concepts like Data Definition Language Commands (DDL), Data\nModification Language Commands ((DML), Views, Integrity Constraints, Aggregate\nfunctions, Joins and Abstract type . While performing practical during the lab\nsession, many problems occurred, in order to solve them many text books and\nwebsites referred but could not obtain expected help from them. Even though by\nspending much time in the database labs with Oracle 10g, tried in numerous\nways, as a final point expected output is achieved. This paper describes\nannotations which were experimentally proved in the Database lab."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.3939v1", 
    "other_authors": "Domenico Sacca', Edoardo Serra, Pietro Dicosta, Antonio Piccolo", 
    "title": "Multi-Sorted Inverse Frequent Itemsets Mining", 
    "arxiv-id": "1310.3939v1", 
    "author": "Antonio Piccolo", 
    "publish": "2013-10-15T07:38:36Z", 
    "summary": "The development of novel platforms and techniques for emerging \"Big Data\"\napplications requires the availability of real-life datasets for data-driven\nexperiments, which are however out of reach for academic research in most cases\nas they are typically proprietary. A possible solution is to use synthesized\ndatasets that reflect patterns of real ones in order to ensure high quality\nexperimental findings. A first step in this direction is to use inverse mining\ntechniques such as inverse frequent itemset mining (IFM) that consists of\ngenerating a transactional database satisfying given support constraints on the\nitemsets in an input set, that are typically the frequent ones. This paper\nintroduces an extension of IFM, called many-sorted IFM, where the schemes for\nthe datasets to be generated are those typical of Big Tables as required in\nemerging big data applications, e.g., social network analytics."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.5254v1", 
    "other_authors": "Syed Ijaz Ahmad Bukhari", 
    "title": "Real Time Data Warehouse", 
    "arxiv-id": "1310.5254v1", 
    "author": "Syed Ijaz Ahmad Bukhari", 
    "publish": "2013-10-19T17:30:48Z", 
    "summary": "Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged\nas a fast growing reporting and analysis technique in early 1980s. Today, it\nhas almost replaced relational databases. However, with passage of time, static\nand historic data of DWs could not produce Real Time reporting and analysis,\nthus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).\nAlthough, there are problems with RTDWs, but with advancement in technology and\nresearchers focus, RTDWs will be able to generate real time reports, analysis\nand forecasting."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.5841v1", 
    "other_authors": "Naoual Mouhni, Abderrafiaa Elkalay", 
    "title": "Ontology based data warehouses federation management system", 
    "arxiv-id": "1310.5841v1", 
    "author": "Abderrafiaa Elkalay", 
    "publish": "2013-10-22T08:50:41Z", 
    "summary": "Data warehouses are nowadays an important component in every competitive\nsystem, it's one of the main components on which business intelligence is\nbased. We can even say that many companies are climbing to the next level and\nuse a set of Data warehouses to provide the complete information or it's\ngenerally due to fusion of two or many companies. these Data warehouses can be\nheterogeneous and geographically separated, this structure is what we call\nfederation, and even if the components are physically separated, they are\nlogically seen as a single component. generally, these items are heterogeneous\nwhich make it difficult to create the logical federation schema,and the\nexecution of user queries a complicated mission. In this paper, we will fill\nthis gap by proposing an extension of an existent algorithm in order to treat\ndifferent schema types (star, snow flack) including the treatment of\nhierarchies dimension using ontology"
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.6833v1", 
    "other_authors": "A. M. Sowjanya, M. Shashi", 
    "title": "New Proximity Estimate for Incremental Update of Non-uniformly   Distributed Clusters", 
    "arxiv-id": "1310.6833v1", 
    "author": "M. Shashi", 
    "publish": "2013-10-25T06:50:48Z", 
    "summary": "The conventional clustering algorithms mine static databases and generate a\nset of patterns in the form of clusters. Many real life databases keep growing\nincrementally. For such dynamic databases, the patterns extracted from the\noriginal database become obsolete. Thus the conventional clustering algorithms\nare not suitable for incremental databases due to lack of capability to modify\nthe clustering results in accordance with recent updates. In this paper, the\nauthor proposes a new incremental clustering algorithm called CFICA(Cluster\nFeature-Based Incremental Clustering Approach for numerical data) to handle\nnumerical data and suggests a new proximity metric called Inverse Proximity\nEstimate (IPE) which considers the proximity of a data point to a cluster\nrepresentative as well as its proximity to a farthest point in its vicinity.\nCFICA makes use of the proposed proximity metric to determine the membership of\na data point into a cluster."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1310.7297v1", 
    "other_authors": "Farhana Murtaza Choudhury, Mohammed Eunus Ali, Sarah Masud, Suman Nath, Ishat E Rabban", 
    "title": "Scalable Visibility Color Map Construction in Spatial Databases", 
    "arxiv-id": "1310.7297v1", 
    "author": "Ishat E Rabban", 
    "publish": "2013-10-28T02:38:26Z", 
    "summary": "Recent advances in 3D modeling provide us with real 3D datasets to answer\nqueries, such as \"What is the best position for a new billboard?\" and \"Which\nhotel room has the best view?\" in the presence of obstacles. These applications\nrequire measuring and differentiating the visibility of an object (target) from\ndifferent viewpoints in a dataspace, e.g., a billboard may be seen from two\nviewpoints but is readable only from the viewpoint closer to the target. In\nthis paper, we formulate the above problem of quantifying the visibility of\n(from) a target object from (of) the surrounding area with a visibility color\nmap (VCM). A VCM is essentially defined as a surface color map of the space,\nwhere each viewpoint of the space is assigned a color value that denotes the\nvisibility measure of the target from that viewpoint. Measuring the visibility\nof a target even from a single viewpoint is an expensive operation, as we need\nto consider factors such as distance, angle, and obstacles between the\nviewpoint and the target. Hence, a straightforward approach to construct the\nVCM that requires visibility computation for every viewpoint of the surrounding\nspace of the target, is prohibitively expensive in terms of both I/Os and\ncomputation, especially for a real dataset comprising of thousands of\nobstacles. We propose an efficient approach to compute the VCM based on a key\nproperty of the human vision that eliminates the necessity of computing the\nvisibility for a large number of viewpoints of the space. To further reduce the\ncomputational overhead, we propose two approximations; namely, minimum bounding\nrectangle and tangential approaches with guaranteed error bounds. Our extensive\nexperiments demonstrate the effectiveness and efficiency of our solutions to\nconstruct the VCM for real 2D and 3D datasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0059v1", 
    "other_authors": "Jian Wen, Vinayak R. Borkar, Michael J. Carey, Vassilis J. Tsotras", 
    "title": "Revisiting Aggregation for Data Intensive Applications: A Performance   Study", 
    "arxiv-id": "1311.0059v1", 
    "author": "Vassilis J. Tsotras", 
    "publish": "2013-10-31T22:51:50Z", 
    "summary": "Aggregation has been an important operation since the early days of\nrelational databases. Today's Big Data applications bring further challenges\nwhen processing aggregation queries, demanding adaptive aggregation algorithms\nthat can process large volumes of data relative to a potentially limited memory\nbudget (especially in multiuser settings). Despite its importance, the design\nand evaluation of aggregation algorithms has not received the same attention\nthat other basic operators, such as joins, have received in the literature. As\na result, when considering which aggregation algorithm(s) to implement in a new\nparallel Big Data processing platform (AsterixDB), we faced a lack of \"off the\nshelf\" answers that we could simply read about and then implement based on\nprior performance studies.\n  In this paper we revisit the engineering of efficient local aggregation\nalgorithms for use in Big Data platforms. We discuss the salient implementation\ndetails of several candidate algorithms and present an in-depth experimental\nperformance study to guide future Big Data engine developers. We show that the\nefficient implementation of the aggregation operator for a Big Data platform is\nnon-trivial and that many factors, including memory usage, spilling strategy,\nand I/O and CPU cost, should be considered. Further, we introduce precise cost\nmodels that can help in choosing an appropriate algorithm based on input\nparameters including memory budget, grouping key cardinality, and data skew."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0320v2", 
    "other_authors": "Zhi Jie Wang, Bin Yao, Minyi Guo", 
    "title": "Explicit and Implicit Constrained-Space Probabilistic Threshold Range   Queries for Moving Objects", 
    "arxiv-id": "1311.0320v2", 
    "author": "Minyi Guo", 
    "publish": "2013-11-01T22:52:00Z", 
    "summary": "This paper studies the constrained-space probabilistic threshold range query\n(CSPTRQ) for moving objects. We differentiate two kinds of CSPTRQs: implicit\nand explicit ones. Specifically, for each moving object $o$, we assume $o$\ncannot be located in some specific areas, we model its location as a closed\nregion, $u$, together with a probability density function, and model a query\nrange, $R$, as an arbitrary polygon. An implicit CSPTRQ can be reduced to a\nsearch (over all the $u$) that returns a set of objects, which have\nprobabilities higher than a probability threshold $p_t$ to be located in $R$,\nwhere $0\\leq p_t\\leq 1$. In contrast, an explicit CSPTRQ returns a set of\ntuples in form of ($o$, $p$) such that $p\\geq p_t$, where $p$ is the\nprobability of $o$ being located in $R$. A straightforward adaptation of\nexisting method is inefficient due to its weak pruning/validating capability.\nIn order to efficiently process such queries, we propose targeted solutions, in\nwhich three main ideas are incorporated: (1) swapping the order of geometric\noperations based on the computation duality; (2) pruning unrelated objects in\nthe early stages using the location unreachability; and (3) computing the\nprobability using the multi-step mechanism. Extensive experimental results\ndemonstrate the efficiency and effectiveness of the proposed algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0350v1", 
    "other_authors": "Thabet Slimani, Amor Lazzez", 
    "title": "Sequential Mining: Patterns and Algorithms Analysis", 
    "arxiv-id": "1311.0350v1", 
    "author": "Amor Lazzez", 
    "publish": "2013-11-02T06:55:10Z", 
    "summary": "This paper presents and analysis the common existing sequential pattern\nmining algorithms. It presents a classifying study of sequential pattern-mining\nalgorithms into five extensive classes. First, on the basis of Apriori-based\nalgorithm, second on Breadth First Search-based strategy, third on Depth First\nSearch strategy, fourth on sequential closed-pattern algorithm and five on the\nbasis of incremental pattern mining algorithms. At the end, a comparative\nanalysis is done on the basis of important key features supported by various\nalgorithms. This study gives an enhancement in the understanding of the\napproaches of sequential pattern mining."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0505v1", 
    "other_authors": "Dang-Hoan Tran", 
    "title": "Automated Change Detection and Reactive Clustering in Multivariate   Streaming Data", 
    "arxiv-id": "1311.0505v1", 
    "author": "Dang-Hoan Tran", 
    "publish": "2013-11-03T18:48:50Z", 
    "summary": "Many automated systems need the capability of automatic change detection\nwithout the given detection threshold. This paper presents an automated change\ndetection algorithm in streaming multivariate data. Two overlapping windows are\nused to quantify the changes. While a window is used as the reference window\nfrom which the clustering is created, the other called the current window\ncaptures the newly incoming data points. A newly incoming data point can be\nconsidered a change point if it is not a member of any cluster. As our\nclustering-based change detector does not require detection threshold, it is an\nautomated detector. Based on this change detector, we propose a reactive\nclustering algorithm for streaming data. Our empirical results show that, our\nclustering-based change detector works well with multivariate streaming data.\nThe detection accuracy depends on the number of clusters in the reference\nwindow, the window width."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0536v3", 
    "other_authors": "Nikos Bikakis, Chrisa Tsinaraki, Ioannis Stavrakantonakis, Nektarios Gioldasis, Stavros Christodoulakis", 
    "title": "The SPARQL2XQuery Interoperability Framework. Utilizing Schema Mapping,   Schema Transformation and Query Translation to Integrate XML and the Semantic   Web", 
    "arxiv-id": "1311.0536v3", 
    "author": "Stavros Christodoulakis", 
    "publish": "2013-11-03T21:57:48Z", 
    "summary": "The Web of Data is an open environment consisting of a great number of large\ninter-linked RDF datasets from various domains. In this environment,\norganizations and companies adopt the Linked Data practices utilizing Semantic\nWeb (SW) technologies, in order to publish their data and offer SPARQL\nendpoints (i.e., SPARQL-based search services). On the other hand, the dominant\nstandard for information exchange in the Web today is XML. The SW and XML\nworlds and their developed infrastructures are based on different data models,\nsemantics and query languages. Thus, it is crucial to develop interoperability\nmechanisms that allow the Web of Data users to access XML datasets, using\nSPARQL, from their own working environments. It is unrealistic to expect that\nall the existing legacy data (e.g., Relational, XML, etc.) will be transformed\ninto SW data. Therefore, publishing legacy data as Linked Data and providing\nSPARQL endpoints over them has become a major research challenge. In this\ndirection, we introduce the SPARQL2XQuery Framework which creates an\ninteroperable environment, where SPARQL queries are automatically translated to\nXQuery queries, in order to access XML data across the Web. The SPARQL2XQuery\nFramework provides a mapping model for the expression of OWL-RDF/S to XML\nSchema mappings as well as a method for SPARQL to XQuery translation. To this\nend, our Framework supports both manual and automatic mapping specification\nbetween ontologies and XML Schemas. In the automatic mapping specification\nscenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML\nSchemas into OWL ontologies. Finally, extensive experiments have been conducted\nin order to evaluate the schema transformation, mapping generation, query\ntranslation and query evaluation efficiency, using both real and synthetic\ndatasets."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0805v2", 
    "other_authors": "Todor Ivanov, Nikolaos Korfiatis, Roberto V. Zicari", 
    "title": "On the inequality of the 3V's of Big Data Architectural Paradigms: A   case for heterogeneity", 
    "arxiv-id": "1311.0805v2", 
    "author": "Roberto V. Zicari", 
    "publish": "2013-11-04T18:29:45Z", 
    "summary": "The well-known 3V architectural paradigm for Big Data introduced by Laney\n(2011), provides a simplified framework for defining the architecture of a big\ndata platform to be deployed in various scenarios tackling processing of\nmassive datasets. While additional components such as Variability and Veracity\nhave been discussed as an extension to the 3V model, the basic components\n(volume, variety, velocity) provide a quantitative framework while variability\nand veracity target a more qualitative approach. In this paper we argue why the\nbasic 3V's are not equal due to the different requirements that need to be\ncovered in case higher demands for a particular \"V\". Similar to other\nconjectures such as the CAP theorem 3V based architectures differ on their\nimplementation. We call this paradigm heterogeneity and we provide a taxonomy\nof the existing tools (as of 2013) covering the Hadoop ecosystem from the\nperspective of heterogeneity. This paper contributes on the understanding of\nthe Hadoop ecosystem from the perspective of different workloads and aims to\nhelp researchers and practitioners on the design of scalable platforms\ntargeting different operational needs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.0841v2", 
    "other_authors": "L\u00e1szl\u00f3 Dobos, J\u00e1nos Sz\u00fcle, Tam\u00e1s Bodn\u00e1r, Tam\u00e1s Hanyecz, Tam\u00e1s Seb\u0151k, D\u00e1niel Kondor, Zs\u00f3fia Kallus, J\u00f3zsef St\u00e9ger, Istv\u00e1n Csabai, G\u00e1bor Vattay", 
    "title": "A multi-terabyte relational database for geo-tagged social network data", 
    "arxiv-id": "1311.0841v2", 
    "author": "G\u00e1bor Vattay", 
    "publish": "2013-11-04T20:39:16Z", 
    "summary": "Despite their relatively low sampling factor, the freely available, randomly\nsampled status streams of Twitter are very useful sources of geographically\nembedded social network data. To statistically analyze the information Twitter\nprovides via these streams, we have collected a year's worth of data and built\na multi-terabyte relational database from it. The database is designed for fast\ndata loading and to support a wide range of studies focusing on the statistics\nand geographic features of social networks, as well as on the linguistic\nanalysis of tweets. In this paper we present the method of data collection, the\ndatabase design, the data loading procedure and special treatment of geo-tagged\nand multi-lingual data. We also provide some SQL recipes for computing network\nstatistics."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.2100v1", 
    "other_authors": "Nandish Jayaram, Arijit Khan, Chengkai Li, Xifeng Yan, Ramez Elmasri", 
    "title": "Querying Knowledge Graphs by Example Entity Tuples", 
    "arxiv-id": "1311.2100v1", 
    "author": "Ramez Elmasri", 
    "publish": "2013-11-08T22:47:39Z", 
    "summary": "We witness an unprecedented proliferation of knowledge graphs that record\nmillions of entities and their relationships. While knowledge graphs are\nstructure-flexible and content rich, they are difficult to use. The challenge\nlies in the gap between their overwhelming complexity and the limited database\nknowledge of non-professional users. If writing structured queries over simple\ntables is difficult, complex graphs are only harder to query. As an initial\nstep toward improving the usability of knowledge graphs, we propose to query\nsuch data by example entity tuples, without requiring users to form complex\ngraph queries. Our system, GQBE (Graph Query By Example), automatically derives\na weighted hidden maximal query graph based on input query tuples, to capture a\nuser's query intent. It efficiently finds and ranks the top approximate answer\ntuples. For fast query processing, GQBE only partially evaluates query graphs.\nWe conducted experiments and user studies on the large Freebase and DBpedia\ndatasets and observed appealing accuracy and efficiency. Our system provides a\ncomplementary approach to the existing keyword-based methods, facilitating\nuser-friendly graph querying. To the best of our knowledge, there was no such\nproposal in the past in the context of graphs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.2342v1", 
    "other_authors": "Carlos R. Rivero, Hasan M. Jamil", 
    "title": "Anatomy of Graph Matching based on an XQuery and RDF Implementation", 
    "arxiv-id": "1311.2342v1", 
    "author": "Hasan M. Jamil", 
    "publish": "2013-11-11T03:15:38Z", 
    "summary": "Graphs are becoming one of the most popular data modeling paradigms since\nthey are able to model complex relationships that cannot be easily captured\nusing traditional data models. One of the major tasks of graph management is\ngraph matching, which aims to find all of the subgraphs in a data graph that\nmatch a query graph. In the literature, proposals in this context are\nclassified into two different categories: graph-at-a-time, which process the\nwhole query graph at the same time, and vertex-at-a-time, which process a\nsingle vertex of the query graph at the same time. In this paper, we propose a\nnew vertex-at-a-time proposal that is based on graphlets, each of which\ncomprises a vertex of a graph, all of the immediate neighbors of that vertex,\nand all of the edges that relate those neighbors. Furthermore, we also use the\nconcept of minimum hub covers, each of which comprises a subset of vertices in\nthe query graph that account for all of the edges in that graph. We present the\nalgorithms of our proposal and describe an implementation based on XQuery and\nRDF. Our evaluation results show that our proposal is appealing to perform\ngraph matching."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.3312v1", 
    "other_authors": "Vanessa Ayala-Rivera, Patrick McDonagh, Thomas Cerqueus, Liam Murphy", 
    "title": "Synthetic Data Generation using Benerator Tool", 
    "arxiv-id": "1311.3312v1", 
    "author": "Liam Murphy", 
    "publish": "2013-11-13T21:14:40Z", 
    "summary": "Datasets of different characteristics are needed by the research community\nfor experimental purposes. However, real data may be difficult to obtain due to\nprivacy concerns. Moreover, real data may not meet specific characteristics\nwhich are needed to verify new approaches under certain conditions. Given these\nlimitations, the use of synthetic data is a viable alternative to complement\nthe real data. In this report, we describe the process followed to generate\nsynthetic data using Benerator, a publicly available tool. The results show\nthat the synthetic data preserves a high level of accuracy compared to the\noriginal data. The generated datasets correspond to microdata containing\nrecords with social, economic and demographic data which mimics the\ndistribution of aggregated statistics from the 2011 Irish Census data."
},{
    "category": "cs.DB", 
    "doi": "10.5121/cseij.2013.3401", 
    "link": "http://arxiv.org/pdf/1311.3879v1", 
    "other_authors": "Faisal Alkhateeb, J\u00e9r\u00f4me Euzenat", 
    "title": "Answering SPARQL queries modulo RDF Schema with paths", 
    "arxiv-id": "1311.3879v1", 
    "author": "J\u00e9r\u00f4me Euzenat", 
    "publish": "2013-11-15T15:34:26Z", 
    "summary": "SPARQL is the standard query language for RDF graphs. In its strict\ninstantiation, it only offers querying according to the RDF semantics and would\nthus ignore the semantics of data expressed with respect to (RDF) schemas or\n(OWL) ontologies. Several extensions to SPARQL have been proposed to query RDF\ndata modulo RDFS, i.e., interpreting the query with RDFS semantics and/or\nconsidering external ontologies. We introduce a general framework which allows\nfor expressing query answering modulo a particular semantics in an homogeneous\nway. In this paper, we discuss extensions of SPARQL that use regular\nexpressions to navigate RDF graphs and may be used to answer queries\nconsidering RDFS semantics. We also consider their embedding as extensions of\nSPARQL. These SPARQL extensions are interpreted within the proposed framework\nand their drawbacks are presented. In particular, we show that the PSPARQL\nquery language, a strict extension of SPARQL offering transitive closure,\nallows for answering SPARQL queries modulo RDFS graphs with the same complexity\nas SPARQL through a simple transformation of the queries. We also consider\nlanguages which, in addition to paths, provide constraints. In particular, we\npresent and compare nSPARQL and our proposal CPSPARQL. We show that CPSPARQL is\nexpressive enough to answer full SPARQL queries modulo RDFS. Finally, we\ncompare the expressiveness and complexity of both nSPARQL and the corresponding\nfragment of CPSPARQL, that we call cpSPARQL. We show that both languages have\nthe same complexity through cpSPARQL, being a proper extension of SPARQL graph\npatterns, is more expressive than nSPARQL."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijwest.2013.4401", 
    "link": "http://arxiv.org/pdf/1311.4040v1", 
    "other_authors": "Miklos Kalman, Ferenc Havasi", 
    "title": "Enhanced XML Validation using SRML", 
    "arxiv-id": "1311.4040v1", 
    "author": "Ferenc Havasi", 
    "publish": "2013-11-16T09:59:05Z", 
    "summary": "Data validation is becoming more and more important with the ever-growing\namount of data being consumed and transmitted by systems over the Internet. It\nis important to ensure that the data being sent is valid as it may contain\nentry errors, which may be consumed by different systems causing further\nerrors. XML has become the defacto standard for data transfer. The XML Schema\nDefinition language (XSD) was created to help XML structural validation and\nprovide a schema for data type restrictions, however it does not allow for more\ncomplex situations. In this article we introduce a way to provide rule based\nXML validation and correction through the extension and improvement of our SRML\nmetalanguage. We also explore the option of applying it in a database as a\ntrigger for CRUD operations allowing more granular dataset validation on an\natomic level allowing for more complex dataset record validation rules."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijwest.2013.4401", 
    "link": "http://arxiv.org/pdf/1311.4121v1", 
    "other_authors": "Thabet Slimani", 
    "title": "Application of Rough Set Theory in Data Mining", 
    "arxiv-id": "1311.4121v1", 
    "author": "Thabet Slimani", 
    "publish": "2013-11-17T06:27:50Z", 
    "summary": "Rough set theory is a new method that deals with vagueness and uncertainty\nemphasized in decision making. Data mining is a discipline that has an\nimportant contribution to data analysis, discovery of new meaningful knowledge,\nand autonomous decision making. The rough set theory offers a viable approach\nfor decision rule extraction from data.This paper, introduces the fundamental\nconcepts of rough set theory and other aspects of data mining, a discussion of\ndata representation with rough set theory including pairs of attribute-value\nblocks, information tables reducts, indiscernibility relation and decision\ntables. Additionally, the rough set approach to lower and upper approximations\nand certain possible rule sets concepts are introduced. Finally, some\ndescription about applications of the data mining system with rough set theory\nis included."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijwest.2013.4401", 
    "link": "http://arxiv.org/pdf/1311.4529v2", 
    "other_authors": "Afroza Sultana, Naeemul Hassan, Chengkai Li, Jun Yang, Cong Yu", 
    "title": "Incremental Discovery of Prominent Situational Facts", 
    "arxiv-id": "1311.4529v2", 
    "author": "Cong Yu", 
    "publish": "2013-11-18T20:44:13Z", 
    "summary": "We study the novel problem of finding new, prominent situational facts, which\nare emerging statements about objects that stand out within certain contexts.\nMany such facts are newsworthy---e.g., an athlete's outstanding performance in\na game, or a viral video's impressive popularity. Effective and efficient\nidentification of these facts assists journalists in reporting, one of the main\ngoals of computational journalism. Technically, we consider an ever-growing\ntable of objects with dimension and measure attributes. A situational fact is a\n\"contextual\" skyline tuple that stands out against historical tuples in a\ncontext, specified by a conjunctive constraint involving dimension attributes,\nwhen a set of measure attributes are compared. New tuples are constantly added\nto the table, reflecting events happening in the real world. Our goal is to\ndiscover constraint-measure pairs that qualify a new tuple as a contextual\nskyline tuple, and discover them quickly before the event becomes yesterday's\nnews. A brute-force approach requires exhaustive comparison with every tuple,\nunder every constraint, and in every measure subspace. We design algorithms in\nresponse to these challenges using three corresponding ideas---tuple reduction,\nconstraint pruning, and sharing computation across measure subspaces. We also\nadopt a simple prominence measure to rank the discovered facts when they are\nnumerous. Experiments over two real datasets validate the effectiveness and\nefficiency of our techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s13222-012-0100-z", 
    "link": "http://arxiv.org/pdf/1311.4610v2", 
    "other_authors": "V\u00edctor Cuevas-Vicentt\u00edn, Saumen Dey, Sven K\u00f6hler, Sean Riddle, Bertram Lud\u00e4scher", 
    "title": "Scientific Workflows and Provenance: Introduction and Research   Opportunities", 
    "arxiv-id": "1311.4610v2", 
    "author": "Bertram Lud\u00e4scher", 
    "publish": "2013-11-19T02:23:03Z", 
    "summary": "Scientific workflows are becoming increasingly popular for compute-intensive\nand data-intensive scientific applications. The vision and promise of\nscientific workflows includes rapid, easy workflow design, reuse, scalable\nexecution, and other advantages, e.g., to facilitate \"reproducible science\"\nthrough provenance (e.g., data lineage) support. However, as described in the\npaper, important research challenges remain. While the database community has\nstudied (business) workflow technologies extensively in the past, most current\nwork in scientific workflows seems to be done outside of the database\ncommunity, e.g., by practitioners and researchers in the computational sciences\nand eScience. We provide a brief introduction to scientific workflows and\nprovenance, and identify areas and problems that suggest new opportunities for\ndatabase research."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2534732.2534742", 
    "link": "http://arxiv.org/pdf/1311.5204v1", 
    "other_authors": "Georgios Skoumas, Dieter Pfoser, Anastasios Kyrillidis", 
    "title": "On Quantifying Qualitative Geospatial Data: A Probabilistic Approach", 
    "arxiv-id": "1311.5204v1", 
    "author": "Anastasios Kyrillidis", 
    "publish": "2013-11-18T09:06:59Z", 
    "summary": "Living in the era of data deluge, we have witnessed a web content explosion,\nlargely due to the massive availability of User-Generated Content (UGC). In\nthis work, we specifically consider the problem of geospatial information\nextraction and representation, where one can exploit diverse sources of\ninformation (such as image and audio data, text data, etc), going beyond\ntraditional volunteered geographic information. Our ambition is to include\navailable narrative information in an effort to better explain geospatial\nrelationships: with spatial reasoning being a basic form of human cognition,\nnarratives expressing such experiences typically contain qualitative spatial\ndata, i.e., spatial objects and spatial relationships.\n  To this end, we formulate a quantitative approach for the representation of\nqualitative spatial relations extracted from UGC in the form of texts. The\nproposed method quantifies such relations based on multiple text observations.\nSuch observations provide distance and orientation features which are utilized\nby a greedy Expectation Maximization-based (EM) algorithm to infer a\nprobability distribution over predefined spatial relationships; the latter\nrepresent the quantified relationships under user-defined probabilistic\nassumptions. We evaluate the applicability and quality of the proposed approach\nusing real UGC data originating from an actual travel blog text corpus. To\nverify the quality of the result, we generate grid-based maps visualizing the\nspatial extent of the various relations."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2534732.2534742", 
    "link": "http://arxiv.org/pdf/1311.5663v1", 
    "other_authors": "Zhengkui Wang, Yan Chu, Kian-Lee Tan, Divyakant Agrawal, Amr EI Abbadi, Xiaolong Xu", 
    "title": "Scalable Data Cube Analysis over Big Data", 
    "arxiv-id": "1311.5663v1", 
    "author": "Xiaolong Xu", 
    "publish": "2013-11-22T07:48:25Z", 
    "summary": "Data cubes are widely used as a powerful tool to provide multidimensional\nviews in data warehousing and On-Line Analytical Processing (OLAP). However,\nwith increasing data sizes, it is becoming computationally expensive to perform\ndata cube analysis. The problem is exacerbated by the demand of supporting more\ncomplicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as\nwell as supporting frequent view updates in data cubes. This calls for new\nscalable and efficient data cube analysis systems. In this paper, we introduce\nHaCube, an extension of MapReduce, designed for efficient parallel data cube\nanalysis on large-scale data by taking advantages from both MapReduce (in terms\nof scalability) and parallel DBMS (in terms of efficiency). We also provide a\ngeneral data cube materialization algorithm which is able to facilitate the\nfeatures in MapReduce-like systems towards an efficient data cube computation.\nFurthermore, we demonstrate how HaCube supports view maintenance through either\nincremental computation (e.g. used for SUM or COUNT) or recomputation (e.g.\nused for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and\nevaluate it based on the TPC-D benchmark over billions of tuples on a cluster\nwith over 320 cores. The experimental results demonstrate the efficiency,\nscalability and practicality of HaCube for cube analysis over a large amount of\ndata in a distributed environment."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2534732.2534742", 
    "link": "http://arxiv.org/pdf/1311.6335v1", 
    "other_authors": "Astrid Rheinl\u00e4nder, Arvid Heise, Fabian Hueske, Ulf Leser, Felix Naumann", 
    "title": "SOFA: An Extensible Logical Optimizer for UDF-heavy Dataflows", 
    "arxiv-id": "1311.6335v1", 
    "author": "Felix Naumann", 
    "publish": "2013-11-25T15:26:49Z", 
    "summary": "Recent years have seen an increased interest in large-scale analytical\ndataflows on non-relational data. These dataflows are compiled into execution\ngraphs scheduled on large compute clusters. In many novel application areas the\npredominant building blocks of such dataflows are user-defined predicates or\nfunctions (UDFs). However, the heavy use of UDFs is not well taken into account\nfor dataflow optimization in current systems.\n  SOFA is a novel and extensible optimizer for UDF-heavy dataflows. It builds\non a concise set of properties for describing the semantics of Map/Reduce-style\nUDFs and a small set of rewrite rules, which use these properties to find a\nmuch larger number of semantically equivalent plan rewrites than possible with\ntraditional techniques. A salient feature of our approach is extensibility: We\narrange user-defined operators and their properties into a subsumption\nhierarchy, which considerably eases integration and optimization of new\noperators. We evaluate SOFA on a selection of UDF-heavy dataflows from\ndifferent domains and compare its performance to three other algorithms for\ndataflow optimization. Our experiments reveal that SOFA finds efficient plans,\noutperforming the best plans found by its competitors by a factor of up to 6."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2014.6816714", 
    "link": "http://arxiv.org/pdf/1311.6570v2", 
    "other_authors": "Shizuya Hakuta, Sebastian Maneth, Keisuke Nakano, Hideya Iwasaki", 
    "title": "XQuery Streaming by Forest Transducers", 
    "arxiv-id": "1311.6570v2", 
    "author": "Hideya Iwasaki", 
    "publish": "2013-11-26T07:17:23Z", 
    "summary": "Streaming of XML transformations is a challenging task and only very few\nsystems support streaming. Research approaches generally define custom\nfragments of XQuery and XPath that are amenable to streaming, and then design\ncustom algorithms for each fragment. These languages have several shortcomings.\nHere we take a more principles approach to the problem of streaming\nXQuery-based transformations. We start with an elegant transducer model for\nwhich many static analysis problems are well-understood: the Macro Forest\nTransducer (MFT). We show that a large fragment of XQuery can be translated\ninto MFTs --- indeed, a fragment of XQuery, that can express important features\nthat are missing from other XQuery stream engines, such as GCX: our fragment of\nXQuery supports XPath predicates and let-statements. We then rely on a\nstreaming execution engine for MFTs, one which uses a well-founded set of\noptimizations from functional programming, such as strictness analysis and\ndeforestation. Our prototype achieves time and memory efficiency comparable to\nthe fastest known engine for XQuery streaming, GCX. This is surprising because\nour engine relies on the OCaml built in garbage collector and does not use any\nspecialized buffer management, while GCX's efficiency is due to clever and\nexplicit buffer management."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2014.6816714", 
    "link": "http://arxiv.org/pdf/1311.6714v1", 
    "other_authors": "Stefan B\u00f6ttcher, Rita Hartel, Jonathan Rabe", 
    "title": "Efficient XML Keyword Search based on DAG-Compression", 
    "arxiv-id": "1311.6714v1", 
    "author": "Jonathan Rabe", 
    "publish": "2013-11-26T15:55:41Z", 
    "summary": "In contrast to XML query languages as e.g. XPath which require knowledge on\nthe query language as well as on the document structure, keyword search is open\nto anybody. As the size of XML sources grows rapidly, the need for efficient\nsearch indices on XML data that support keyword search increases. In this\npaper, we present an approach of XML keyword search which is based on the DAG\nof the XML data, where repeated substructures are considered only once, and\ntherefore, have to be searched only once. As our performance evaluation shows,\nthis DAG-based extension of the set intersection search algorithm[1], [2], can\nlead to search times that are on large documents more than twice as fast as the\nsearch times of the XML-based approach. Additionally, we utilize a smaller\nindex, i.e., we consume less main memory to compute the results."
},{
    "category": "cs.DB", 
    "doi": "10.1109/ICDE.2014.6816714", 
    "link": "http://arxiv.org/pdf/1311.7219v2", 
    "other_authors": "Trupti M. Kodinariya Dr. Prashant R. Makwana", 
    "title": "Partitioning Clustering algorithms for handling numerical and   categorical data: a review", 
    "arxiv-id": "1311.7219v2", 
    "author": "Trupti M. Kodinariya Dr. Prashant R. Makwana", 
    "publish": "2013-11-28T07:06:56Z", 
    "summary": "Clustering is widely used in different field such as biology, psychology, and\neconomics. Most traditional clustering algorithms are limited to handling\ndatasets that contain either numeric or categorical attributes. However,\ndatasets with mixed types of attributes are common in real life data mining\napplications. In this paper, we review partitioning based algorithm such as\nK-prototype, Extension of K-prototype, K-histogram, Fuzzy approaches, genetic\napproaches, etc. These algorithm works on both numerical and categorical data.\nThe approaches has been proposed to handle mixed data are based on four\ndifferent perceptive: i) split data set into two part such that each part\ncontain either numerical or categorical data, then apply separate clustering\nalgorithm on each data set, finally combined the result of both clustering\nalgorithm, ii) converting categorical attribute into numerical attribute and\napply numerical attribute clustering algorithm; iii) discrimination of\nnumerical attribute and apply categorical based clustering algorithm; iv)\nConversion of the categorical attributes into binary ones and apply any\nnumerical based clustering algorithm"
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1311.7307v4", 
    "other_authors": "Iovka Boneva, Radu Ciucanu, S\u0142awek Staworko", 
    "title": "Schemas for Unordered XML on a DIME", 
    "arxiv-id": "1311.7307v4", 
    "author": "S\u0142awek Staworko", 
    "publish": "2013-11-28T13:03:12Z", 
    "summary": "We investigate schema languages for unordered XML having no relative order\namong siblings. First, we propose unordered regular expressions (UREs),\nessentially regular expressions with unordered concatenation instead of\nstandard concatenation, that define languages of unordered words to model the\nallowed content of a node (i.e., collections of the labels of children).\nHowever, unrestricted UREs are computationally too expensive as we show the\nintractability of two fundamental decision problems for UREs: membership of an\nunordered word to the language of a URE and containment of two UREs.\nConsequently, we propose a practical and tractable restriction of UREs,\ndisjunctive interval multiplicity expressions (DIMEs).\n  Next, we employ DIMEs to define languages of unordered trees and propose two\nschema languages: disjunctive interval multiplicity schema (DIMS), and its\nrestriction, disjunction-free interval multiplicity schema (IMS). We study the\ncomplexity of the following static analysis problems: schema satisfiability,\nmembership of a tree to the language of a schema, schema containment, as well\nas twig query satisfiability, implication, and containment in the presence of\nschema. Finally, we study the expressive power of the proposed schema languages\nand compare them with yardstick languages of unordered trees (FO, MSO, and\nPresburger constraints) and DTDs under commutative closure. Our results show\nthat the proposed schema languages are capable of expressing many practical\nlanguages of unordered trees and enjoy desirable computational properties."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.0001v1", 
    "other_authors": "Ayan Chakraborty, Shiladitya Munshi, Debajyoti Mukhopadhyay", 
    "title": "A Proposal for the Characterization of Multi-Dimensional   Inter-relationships of RDF Graphs Based on Set Theoretic Approach", 
    "arxiv-id": "1312.0001v1", 
    "author": "Debajyoti Mukhopadhyay", 
    "publish": "2013-11-28T04:11:36Z", 
    "summary": "In this paper a Set Theoretic approach has been reported for analyzing\ninter-relationship between any numbers of RDF Graphs. An RDF Graph represents\ntriples in Resource Description Format of semantic web. So the identification\nand characterization of criteria for inter-relationship of RDF Graphs shows a\nnew road in semantic search. Using set theoretic approach, a sound framing\ncriteria can be designed that examine whether two RDF Graphs are related and if\nyes, how these relationships could be described with formal set theory. Along\nwith this, by introducing RDF Schema, the inter-relationship status is refined\ninto n-dimensional induced relationships."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.0156v1", 
    "other_authors": "Verena Kantere", 
    "title": "Datom: Towards modular data management", 
    "arxiv-id": "1312.0156v1", 
    "author": "Verena Kantere", 
    "publish": "2013-11-30T21:33:49Z", 
    "summary": "Recent technology breakthroughs have enabled data collection of unprecedented\nscale, rate, variety and complexity that has led to an explosion in data\nmanagement requirements. Existing theories and techniques are not adequate to\nfulfil these requirements. We endeavour to rethink the way data management\nresearch is being conducted and we propose to work towards modular data\nmanagement that will allow for unification of the expression of data management\nproblems and systematization of their solution. The core of such an approach is\nthe novel notion of a datom, i.e. a data management atom, which encapsulates\ngeneric data management provision. The datom is the foundation for comparison,\ncustomization and re-usage of data management problems and solutions. The\nproposed approach can signal a revolution in data management research and a\nlong anticipated evolution in data management engineering."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.0285v1", 
    "other_authors": "Lukasz Golab, Marios Hadjieleftheriou, Howard Karloff, Barna Saha", 
    "title": "Distributed Data Placement via Graph Partitioning", 
    "arxiv-id": "1312.0285v1", 
    "author": "Barna Saha", 
    "publish": "2013-12-01T23:16:42Z", 
    "summary": "With the widespread use of shared-nothing clusters of servers, there has been\na proliferation of distributed object stores that offer high availability,\nreliability and enhanced performance for MapReduce-style workloads. However,\nrelational workloads cannot always be evaluated efficiently using MapReduce\nwithout extensive data migrations, which cause network congestion and reduced\nquery throughput. We study the problem of computing data placement strategies\nthat minimize the data communication costs incurred by typical relational query\nworkloads in a distributed setting.\n  Our main contribution is a reduction of the data placement problem to the\nwell-studied problem of {\\sc Graph Partitioning}, which is NP-Hard but for\nwhich efficient approximation algorithms exist. The novelty and significance of\nthis result lie in representing the communication cost exactly and using\nstandard graphs instead of hypergraphs, which were used in prior work on data\nplacement that optimized for different objectives (not communication cost).\n  We study several practical extensions of the problem: with load balancing,\nwith replication, with materialized views, and with complex query plans\nconsisting of sequences of intermediate operations that may be computed on\ndifferent servers. We provide integer linear programs (IPs) that may be used\nwith any IP solver to find an optimal data placement. For the no-replication\ncase, we use publicly available graph partitioning libraries (e.g., METIS) to\nefficiently compute nearly-optimal solutions. For the versions with\nreplication, we introduce two heuristics that utilize the {\\sc Graph\nPartitioning} solution of the no-replication case. Using the TPC-DS workload,\nit may take an IP solver weeks to compute an optimal data placement, whereas\nour reduction produces nearly-optimal solutions in seconds."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.2065v1", 
    "other_authors": "S. Hanumanth Sastry, Prof. M. S. Prasada Babu", 
    "title": "Implementation of CRISP Methodology for ERP Systems", 
    "arxiv-id": "1312.2065v1", 
    "author": "Prof. M. S. Prasada Babu", 
    "publish": "2013-12-07T07:29:56Z", 
    "summary": "ERP systems contain huge amounts of data related to the actual execution of\nbusiness processes. These systems have a particular way of recording activities\nwhich results in an unclear display of business processes in event logs.\nSeveral works have been conducted on ERP systems, most of them focusing on the\ndevelopment of new algorithms for the automatic discovery of business\nprocesses. We focused on addressing issues like, how can organizations with ERP\nsystems apply process mining for analyzing their business processes in order to\nimprove them. The data handling aspect of ERP systems contrasts with those of\nBPMS or workflow based systems, whose systematical storage of events\nfacilitates the application of process mining techniques. CRISP-DM has emerged\nas the de facto standard for developing data mining and knowledge discovery\nprojects. Successful data mining requires three families of analytical\ncapabilities namely reporting, classification and forecasting. A data miner\nuses more than one analytical method to get the best results. The objective of\nthis paper is to improve the usability and understandability of process mining\ntechniques, by implementing CRISP-DM methodology for their application in ERP\ncontexts, detailed in terms of specific implementation tools and step by step\ncoordination. Our study confirms that data discovery from ERP system improves\nstrategic and operational decision making."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.2353v1", 
    "other_authors": "Davide Martinenghi", 
    "title": "On the difference between checking integrity constraints before or after   updates", 
    "arxiv-id": "1312.2353v1", 
    "author": "Davide Martinenghi", 
    "publish": "2013-12-09T09:34:05Z", 
    "summary": "Integrity checking is a crucial issue, as databases change their instance all\nthe time and therefore need to be checked continuously and rapidly. Decades of\nresearch have produced a plethora of methods for checking integrity constraints\nof a database in an incremental manner. However, not much has been said about\nwhen to check integrity. In this paper, we study the differences and\nsimilarities between checking integrity before an update (a.k.a. pre-test) or\nafter (a.k.a. post-test) in order to assess the respective convenience and\nproperties."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.2355v1", 
    "other_authors": "Davide Martinenghi", 
    "title": "On the dependency on the size of the data when chasing under conceptual   dependencies", 
    "arxiv-id": "1312.2355v1", 
    "author": "Davide Martinenghi", 
    "publish": "2013-12-09T09:37:16Z", 
    "summary": "Conceptual dependencies (CDs) are particular kinds of key dependencies (KDs)\nand inclusion dependencies (IDs) that precisely characterize relational\nschemata modeled according to the main features of the Entity-Relationship (ER)\nmodel. An instance for such a schema may be inconsistent (data violate the\ndependencies) and incomplete (data constitute a piece of correct information,\nbut not necessarily all the relevant information). While undecidable under\ngeneral KDs and IDs, query answering under incomplete data is known to be\ndecidable for CDs. The known techniques are based on the chase -- a special\ninstance, organized in levels of depth, that is a representative of all the\ninstances that satisfy the dependencies and that include the initial instance.\nAlthough the chase generally has infinite size, query answering can be\naddressed by posing the query (or a rewriting thereof) on a finite, initial\npart of the chase. Contrary to previous claims, we show that the maximum level\nof such an initial part cannot be bounded by a constant that does not depend on\nthe size of the initial instance."
},{
    "category": "cs.DB", 
    "doi": "10.1007/s00224-014-9593-1", 
    "link": "http://arxiv.org/pdf/1312.2378v1", 
    "other_authors": "Ramachandra Rao Kurada", 
    "title": "Unsupervised classification of uncertain data objects in spatial   databases using computational geometry and indexing techniques", 
    "arxiv-id": "1312.2378v1", 
    "author": "Ramachandra Rao Kurada", 
    "publish": "2013-12-09T10:49:08Z", 
    "summary": "Unsupervised classification called clustering is a process of organizing\nobjects into groups whose members are similar in some way. Clustering of\nuncertain data objects is a challenge in spatial data bases. In this paper we\nuse Probability Density Functions (PDF) to represent these uncertain data\nobjects, and apply Uncertain K-Means algorithm to generate the clusters. This\nclustering algorithm uses the Expected Distance (ED) to compute the distance\nbetween objects and cluster representatives. To further improve the performance\nof UK-Means we propose a novel technique called Voronoi Diagrams from\nComputational Geometry to prune the number of computations of ED. This\ntechnique works efficiently but results pruning overheads. In order to reduce\nthese in pruning overhead we introduce R*-tree indexing over these uncertain\ndata objects, so that it reduces the computational cost and pruning overheads.\nOur novel approach of integrating UK-Means with voronoi diagrams and R* Tree\napplied over uncertain data objects generates imposing outcome when compared\nwith the accessible methods."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdkp.2013.3607", 
    "link": "http://arxiv.org/pdf/1312.2669v1", 
    "other_authors": "R H Vishwanath, T V Samartha, K C Srikantaiah, K R Venugopal, L M Patnaik", 
    "title": "DRSP : Dimension Reduction For Similarity Matching And Pruning Of Time   Series Data Streams", 
    "arxiv-id": "1312.2669v1", 
    "author": "L M Patnaik", 
    "publish": "2013-12-10T05:14:08Z", 
    "summary": "Similarity matching and join of time series data streams has gained a lot of\nrelevance in today's world that has large streaming data. This process finds\nwide scale application in the areas of location tracking, sensor networks,\nobject positioning and monitoring to name a few. However, as the size of the\ndata stream increases, the cost involved to retain all the data in order to aid\nthe process of similarity matching also increases. We develop a novel framework\nto addresses the following objectives. Firstly, Dimension reduction is\nperformed in the preprocessing stage, where large stream data is segmented and\nreduced into a compact representation such that it retains all the crucial\ninformation by a technique called Multi-level Segment Means (MSM). This reduces\nthe space complexity associated with the storage of large time-series data\nstreams. Secondly, it incorporates effective Similarity Matching technique to\nanalyze if the new data objects are symmetric to the existing data stream. And\nfinally, the Pruning Technique that filters out the pseudo data object pairs\nand join only the relevant pairs. The computational cost for MSM is O(l*ni) and\nthe cost for pruning is O(DRF*wsize*d), where DRF is the Dimension Reduction\nFactor. We have performed exhaustive experimental trials to show that the\nproposed framework is both efficient and competent in comparison with earlier\nworks."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsity.2013.1407", 
    "link": "http://arxiv.org/pdf/1312.2678v1", 
    "other_authors": "S. Hanumanth Sastry, Prof. M. S. Prasada Babu", 
    "title": "Analysis & Prediction of Sales Data in SAP-ERP System using Clustering   Algorithms", 
    "arxiv-id": "1312.2678v1", 
    "author": "Prof. M. S. Prasada Babu", 
    "publish": "2013-12-10T05:58:43Z", 
    "summary": "Clustering is an important data mining technique where we will be interested\nin maximizing intracluster distance and also minimizing intercluster distance.\nWe have utilized clustering techniques for detecting deviation in product sales\nand also to identify and compare sales over a particular period of time.\nClustering is suited to group items that seem to fall naturally together, when\nthere is no specified class for any new item. We have utilizedannual sales data\nof a steel major to analyze Sales Volume & Value with respect to dependent\nattributes like products, customers and quantities sold. The demand for steel\nproducts is cyclical and depends on many factors like customer profile,\nprice,Discounts and tax issues. In this paper, we have analyzed sales data with\nclustering algorithms like K-Means&EMwhichrevealed many interesting\npatternsuseful for improving sales revenue and achieving higher sales volume.\nOur study confirms that partition methods like K-Means & EM algorithms are\nbetter suited to analyze our sales data in comparison to Density based methods\nlike DBSCAN & OPTICS or Hierarchical methods like COBWEB."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2274576.2274588", 
    "link": "http://arxiv.org/pdf/1312.2919v1", 
    "other_authors": "Daniel Zinn, Todd J Green, Bertram Lud\u00e4scher", 
    "title": "Win-Move is Coordination-Free (Sometimes)", 
    "arxiv-id": "1312.2919v1", 
    "author": "Bertram Lud\u00e4scher", 
    "publish": "2013-12-10T19:27:41Z", 
    "summary": "In a recent paper by Hellerstein [15], a tight relationship was conjectured\nbetween the number of strata of a Datalog${}^\\neg$ program and the number of\n\"coordination stages\" required for its distributed computation. Indeed, Ameloot\net al. [9] showed that a query can be computed by a coordination-free\nrelational transducer network iff it is monotone, thus answering in the\naffirmative a variant of Hellerstein's CALM conjecture, based on a particular\ndefinition of coordination-free computation. In this paper, we present three\nadditional models for declarative networking. In these variants, relational\ntransducers have limited access to the way data is distributed. This variation\nallows transducer networks to compute more queries in a coordination-free\nmanner: e.g., a transducer can check whether a ground atom $A$ over the input\nschema is in the \"scope\" of the local node, and then send either $A$ or $\\neg\nA$ to other nodes.\n  We show the surprising result that the query given by the well-founded\nsemantics of the unstratifiable win-move program is coordination-free in some\nof the models we consider. We also show that the original transducer network\nmodel [9] and our variants form a strict hierarchy of classes of\ncoordination-free queries. Finally, we identify different syntactic fragments\nof Datalog${}^{\\neg\\neg}_{\\forall}$, called semi-monotone programs, which can\nbe used as declarative network programming languages, whose distributed\ncomputation is guaranteed to be eventually consistent and coordination-free."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2274576.2274588", 
    "link": "http://arxiv.org/pdf/1312.2990v2", 
    "other_authors": "Foto N. Afrati, Dimitris Fotakis, Angelos Vasilakopoulos", 
    "title": "Efficient Lineage for SUM Aggregate Queries", 
    "arxiv-id": "1312.2990v2", 
    "author": "Angelos Vasilakopoulos", 
    "publish": "2013-12-10T22:48:02Z", 
    "summary": "AI systems typically make decisions and find patterns in data based on the\ncomputation of aggregate and specifically sum functions, expressed as queries,\non data's attributes. This computation can become costly or even inefficient\nwhen these queries concern the whole or big parts of the data and especially\nwhen we are dealing with big data. New types of intelligent analytics require\nalso the explanation of why something happened. In this paper we present a\nrandomised algorithm that constructs a small summary of the data, called\nAggregate Lineage, which can approximate well and explain all sums with large\nvalues in time that depends only on its size. The size of Aggregate Lineage is\npractically independent on the size of the original data. Our algorithm does\nnot assume any knowledge on the set of sum queries to be approximated."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2588555.2588581", 
    "link": "http://arxiv.org/pdf/1312.3913v5", 
    "other_authors": "Xi He, Ashwin Machanavajjhala, Bolin Ding", 
    "title": "Blowfish Privacy: Tuning Privacy-Utility Trade-offs using Policies", 
    "arxiv-id": "1312.3913v5", 
    "author": "Bolin Ding", 
    "publish": "2013-12-13T19:23:12Z", 
    "summary": "Privacy definitions provide ways for trading-off the privacy of individuals\nin a statistical database for the utility of downstream analysis of the data.\nIn this paper, we present Blowfish, a class of privacy definitions inspired by\nthe Pufferfish framework, that provides a rich interface for this trade-off. In\nparticular, we allow data publishers to extend differential privacy using a\npolicy, which specifies (a) secrets, or information that must be kept secret,\nand (b) constraints that may be known about the data. While the secret\nspecification allows increased utility by lessening protection for certain\nindividual properties, the constraint specification provides added protection\nagainst an adversary who knows correlations in the data (arising from\nconstraints). We formalize policies and present novel algorithms that can\nhandle general specifications of sensitive information and certain count\nconstraints. We show that there are reasonable policies under which our privacy\nmechanisms for k-means clustering, histograms and range queries introduce\nsignificantly lesser noise than their differentially private counterparts. We\nquantify the privacy-utility trade-offs for various policies analytically and\nempirically on real datasets."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2588555.2588581", 
    "link": "http://arxiv.org/pdf/1312.4012v1", 
    "other_authors": "Arvind Arasu, Raghav Kaushik", 
    "title": "Oblivious Query Processing", 
    "arxiv-id": "1312.4012v1", 
    "author": "Raghav Kaushik", 
    "publish": "2013-12-14T07:14:09Z", 
    "summary": "Motivated by cloud security concerns, there is an increasing interest in\ndatabase systems that can store and support queries over encrypted data. A\ncommon architecture for such systems is to use a trusted component such as a\ncryptographic co-processor for query processing that is used to securely\ndecrypt data and perform computations in plaintext. The trusted component has\nlimited memory, so most of the (input and intermediate) data is kept encrypted\nin an untrusted storage and moved to the trusted component on ``demand.''\n  In this setting, even with strong encryption, the data access pattern from\nuntrusted storage has the potential to reveal sensitive information; indeed,\nall existing systems that use a trusted component for query processing over\nencrypted data have this vulnerability. In this paper, we undertake the first\nformal study of secure query processing, where an adversary having full\nknowledge of the query (text) and observing the query execution learns nothing\nabout the underlying database other than the result size of the query on the\ndatabase. We introduce a simpler notion, oblivious query processing, and show\nformally that a query admits secure query processing iff it admits oblivious\nquery processing. We present oblivious query processing algorithms for a rich\nclass of database queries involving selections, joins, grouping and\naggregation. For queries not handled by our algorithms, we provide some initial\nevidence that designing oblivious (and therefore secure) algorithms would be\nhard via reductions from two simple, well-studied problems that are generally\nbelieved to be hard. Our study of oblivious query processing also reveals\ninteresting connections to database join theory."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2588555.2588581", 
    "link": "http://arxiv.org/pdf/1312.4283v1", 
    "other_authors": "Yeye He, Siddharth Barman, Jeffrey F. Naughton", 
    "title": "On Load Shedding in Complex Event Processing", 
    "arxiv-id": "1312.4283v1", 
    "author": "Jeffrey F. Naughton", 
    "publish": "2013-12-16T10:03:03Z", 
    "summary": "Complex Event Processing (CEP) is a stream processing model that focuses on\ndetecting event patterns in continuous event streams. While the CEP model has\ngained popularity in the research communities and commercial technologies, the\nproblem of gracefully degrading performance under heavy load in the presence of\nresource constraints, or load shedding, has been largely overlooked. CEP is\nsimilar to \"classical\" stream data management, but addresses a substantially\ndifferent class of queries. This unfortunately renders the load shedding\nalgorithms developed for stream data processing inapplicable. In this paper we\nstudy CEP load shedding under various resource constraints. We formalize broad\nclasses of CEP load-shedding scenarios as different optimization problems. We\ndemonstrate an array of complexity results that reveal the hardness of these\nproblems and construct shedding algorithms with performance guarantees. Our\nresults shed some light on the difficulty of developing load-shedding\nalgorithms that maximize utility."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.4477v1", 
    "other_authors": "Ghazi Al-Naymat", 
    "title": "GCG: Mining Maximal Complete Graph Patterns from Large Spatial Data", 
    "arxiv-id": "1312.4477v1", 
    "author": "Ghazi Al-Naymat", 
    "publish": "2013-12-13T15:00:50Z", 
    "summary": "Recent research on pattern discovery has progressed from mining frequent\npatterns and sequences to mining structured patterns, such as trees and graphs.\nGraphs as general data structure can model complex relations among data with\nwide applications in web exploration and social networks. However, the process\nof mining large graph patterns is a challenge due to the existence of large\nnumber of subgraphs. In this paper, we aim to mine only frequent complete graph\npatterns. A graph g in a database is complete if every pair of distinct\nvertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining\nalgorithm developed to explore interesting pruning techniques to extract\nmaximal complete graphs from large spatial dataset existing in Sloan Digital\nSky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high\nefficiency especially in the presence of large number of patterns. In this\npaper, we describe GCG that can mine not only simple co-location spatial\npatterns but also complex ones. To the best of our knowledge, this is the first\nalgorithm used to exploit the extraction of maximal complete graphs in the\nprocess of mining complex co-location patterns in large spatial dataset."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.4800v1", 
    "other_authors": "Thabet Slimani", 
    "title": "New Approach to Optimize the Time of Association Rules Extraction", 
    "arxiv-id": "1312.4800v1", 
    "author": "Thabet Slimani", 
    "publish": "2013-12-17T14:30:38Z", 
    "summary": "The knowledge discovery algorithms have become ineffective at the abundance\nof data and the need for fast algorithms or optimizing methods is required. To\naddress this limitation, the objective of this work is to adapt a new method\nfor optimizing the time of association rules extractions from large databases.\nIndeed, given a relational database (one relation) represented as a set of\ntuples, also called set of attributes, we transform the original database as a\nbinary table (Bitmap table) containing binary numbers. Then, we use this Bitmap\ntable to construct a data structure called Peano Tree stored as a binary file\non which we apply a new algorithm called BF-ARM (extension of the well known\nApriori algorithm). Since the database is loaded into a binary file, our\nproposed algorithm will traverse this file, and the processes of association\nrules extractions will be based on the file stored on disk. The BF-ARM\nalgorithm is implemented and compared with Apriori, Apriori+ and RS-Rules+\nalgorithms. The evaluation process is based on three benchmarks (Mushroom, Car\nEvaluation and Adult). Our preliminary experimental results showed that our\nalgorithm produces association rules with a minimum time compared to other\nalgorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.5148v2", 
    "other_authors": "Xiaolu Lu, Dongxu Li, Xiang Li, Ling Feng", 
    "title": "Object Selection under Team Context", 
    "arxiv-id": "1312.5148v2", 
    "author": "Ling Feng", 
    "publish": "2013-12-18T14:17:09Z", 
    "summary": "Context-aware database has drawn increasing attention from both industry and\nacademia recently by taking users' current situation and environment into\nconsideration. However, most of the literature focus on individual context,\noverlooking the team users. In this paper, we investigate how to integrate team\ncontext into database query process to help the users' get top-ranked database\ntuples and make the team more competitive. We introduce naive and optimized\nquery algorithm to select the suitable records and show that they output the\nsame results while the latter is more computational efficient. Extensive\nempirical studies are conducted to evaluate the query approaches and\ndemonstrate their effectiveness and efficiency."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.5912v2", 
    "other_authors": "Andrea Cal\u00ec, Riccardo Torlone", 
    "title": "Containment of Schema Mappings for Data Exchange (Preliminary Report)", 
    "arxiv-id": "1312.5912v2", 
    "author": "Riccardo Torlone", 
    "publish": "2013-12-20T12:13:11Z", 
    "summary": "In data exchange, data are materialised from a source schema to a target\nschema, according to suitable source-to-target constraints. Constraints are\nalso expressed on the target schema to represent the domain of interest. A\nschema mapping is the union of the source-to-target and of the target\nconstraints.\n  In this paper, we address the problem of containment of schema mappings for\ndata exchange, which has been recently proposed in this framework as a step\ntowards the optimization of data exchange settings. We refer to a natural\nnotion of containment that relies on the behaviour of schema mappings with\nrespect to conjunctive query answering, in the presence of so-called LAV TGDs\nas target constraints. Our contribution is a practical technique for testing\nthe containment based on the existence of a homomorphism between special\n\"dummy\" instances, which can be easily built from schema mappings.\n  We argue that containment of schema mappings is decidable for most practical\ncases, and we set the basis for further investigations in the topic. This paper\nextends our preliminary results."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.5914v2", 
    "other_authors": "Andrea Cal\u00ec, Marco Console, Riccardo Frosini", 
    "title": "Deep Separability of Ontological Constraints", 
    "arxiv-id": "1312.5914v2", 
    "author": "Riccardo Frosini", 
    "publish": "2013-12-20T12:18:29Z", 
    "summary": "When data schemata are enriched with expressive constraints that aim at\nrepresenting the domain of interest, in order to answer queries one needs to\nconsider the logical theory consisting of both the data and the constraints.\nQuery answering in such a context is called ontological query answering.\nCommonly adopted database constraints in this field are tuple-generating\ndependencies (TGDs) and equality-generating dependencies (EGDs). It is well\nknown that their interaction leads to intractability or undecidability of query\nanswering even in the case of simple subclasses. Several conditions have been\nfound to guarantee separability, that is lack of interaction, between TGDs and\nEGDs. Separability makes EGDs (mostly) irrelevant for query answering and\ntherefore often guarantees tractability, as long as the theory is satisfiable.\nIn this paper we review the two notions of separability found in the\nliterature, as well as several syntactic conditions that are sufficient to\nprove them. We then shed light on the issue of satisfiability checking, showing\nthat under a sufficient condition called deep separability it can be done by\nconsidering the TGDs only.\n  We show that, fortunately, in the case of TGDs and EGDs, separability implies\ndeep separability. This result generalizes several analogous ones, proved ad\nhoc for particular classes of constraints. Applications include the class of\nsticky TGDs and EGDs, for which we provide a syntactic separability condition\nwhich extends the analogous one for linear TGDs; preliminary experiments show\nthe feasibility of query answering in this case."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.7373v3", 
    "other_authors": "Mostafa Milani, Leopoldo Bertossi, Sina Ariyan", 
    "title": "Extending Contexts with Ontologies for Multidimensional Data Quality   Assessment", 
    "arxiv-id": "1312.7373v3", 
    "author": "Sina Ariyan", 
    "publish": "2013-12-27T23:59:54Z", 
    "summary": "Data quality and data cleaning are context dependent activities. Starting\nfrom this observation, in previous work a context model for the assessment of\nthe quality of a database instance was proposed. In that framework, the context\ntakes the form of a possibly virtual database or data integration system into\nwhich a database instance under quality assessment is mapped, for additional\nanalysis and processing, enabling quality assessment. In this work we extend\ncontexts with dimensions, and by doing so, we make possible a multidimensional\nassessment of data quality assessment. Multidimensional contexts are\nrepresented as ontologies written in Datalog+-. We use this language for\nrepresenting dimensional constraints, and dimensional rules, and also for doing\nquery answering based on dimensional navigation, which becomes an important\nauxiliary activity in the assessment of data. We show ideas and mechanisms by\nmeans of examples."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1312.7542v1", 
    "other_authors": "Daniel Ritter", 
    "title": "Towards Connected Enterprises: The Business Network System", 
    "arxiv-id": "1312.7542v1", 
    "author": "Daniel Ritter", 
    "publish": "2013-12-29T14:38:24Z", 
    "summary": "The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable. We present a novel\ndata discovery, mining and network inference system, called Business Network\nSystem (BNS), that reconstructs the BN--integration and business process\nnetworks--from raw data, hidden in the enterprises' landscapes. BNS provides a\nnew, declarative foundation for gathering information, defining a network\nmodel, inferring the network and check its conformance to the real-world\n\"as-is\" network. The paper covers both the foundation and the key features of\nBNS, including its underlying technologies, its overall system architecture,\nand its most interesting capabilities."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1401.0069v2", 
    "other_authors": "Davide Martinenghi", 
    "title": "Determining Relevant Relations for Datalog Queries under Access   Limitations is Undecidable", 
    "arxiv-id": "1401.0069v2", 
    "author": "Davide Martinenghi", 
    "publish": "2013-12-31T01:59:21Z", 
    "summary": "Access limitations are restrictions in the way in which the tuples of a\nrelation can be accessed. Under access limitations, query answering becomes\nmore complex than in the traditional case, with no guarantee that the answer\ntuples that can be extracted (aka maximal answer) are all those that would be\nfound without access limitations (aka complete answer). The field of query\nanswering under access limitations has been broadly investigated in the past.\nAttention has been devoted to the problem of determining relations that are\nrelevant for a query, i.e., those (possibly off-query) relations that might\nneed to be accessed in order to find all tuples in the maximal answer. In this\nshort paper, we show that relevance is undecidable for Datalog queries."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1401.0494v1", 
    "other_authors": "Ines Benali-Sougui, Minyar Sassi-Hidri, Amel Grissa-Touzi", 
    "title": "Flexible SQLf query based on fuzzy linguistic summaries", 
    "arxiv-id": "1401.0494v1", 
    "author": "Amel Grissa-Touzi", 
    "publish": "2014-01-02T18:14:33Z", 
    "summary": "Data is often partially known, vague or ambiguous in many real world\napplications. To deal with such imprecise information, fuzziness is introduced\nin the classical model. SQLf is one of the practical language to deal with\nflexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount\nof fuzzy data, the necessity to work with synthetic views became a challenge\nfor many DB community researchers. The present work deals with Flexible SQLf\nquery based on fuzzy linguistic summaries. We use the fuzzy summaries produced\nby our Fuzzy-SaintEtiq approach. It provides a description of objects depending\non the fuzzy linguistic labels specified as selection criteria."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1401.1043v2", 
    "other_authors": "A. Ibrahim, Shivakumar Sastry, P. S. Sastry", 
    "title": "Discovering Compressing Serial Episodes from Event Sequences", 
    "arxiv-id": "1401.1043v2", 
    "author": "P. S. Sastry", 
    "publish": "2014-01-06T11:16:01Z", 
    "summary": "Most pattern mining methods output a very large number of frequent patterns\nand isolating a small but relevant subset is a challenging problem of current\ninterest in frequent pattern mining. In this paper we consider discovery of a\nsmall set of relevant frequent episodes from data sequences. We make use of the\nMinimum Description Length principle to formulate the problem of selecting a\nsubset of episodes. Using an interesting class of serial episodes with\ninter-event constraints and a novel encoding scheme for data using such\nepisodes, we present algorithms for discovering small set of episodes that\nachieve good data compression. Using an example of the data streams obtained\nfrom distributed sensors in a composable coupled conveyor system, we show that\nour method is very effective in unearthing highly relevant episodes and that\nour scheme also achieves good data compression."
},{
    "category": "cs.DB", 
    "doi": "10.1109/AICCSA.2013.6616417", 
    "link": "http://arxiv.org/pdf/1401.1174v1", 
    "other_authors": "Hessam Zakerzadeh, Charu C. Aggrawal, Ken Barker", 
    "title": "Towards Breaking the Curse of Dimensionality for High-Dimensional   Privacy: An Extended Version", 
    "arxiv-id": "1401.1174v1", 
    "author": "Ken Barker", 
    "publish": "2014-01-06T19:35:23Z", 
    "summary": "The curse of dimensionality has remained a challenge for a wide variety of\nalgorithms in data mining, clustering, classification and privacy. Recently, it\nwas shown that an increasing dimensionality makes the data resistant to\neffective privacy. The theoretical results seem to suggest that the\ndimensionality curse is a fundamental barrier to privacy preservation. However,\nin practice, we show that some of the common properties of real data can be\nleveraged in order to greatly ameliorate the negative effects of the curse of\ndimensionality. In real data sets, many dimensions contain high levels of\ninter-attribute correlations. Such correlations enable the use of a process\nknown as vertical fragmentation in order to decompose the data into vertical\nsubsets of smaller dimensionality. An information-theoretic criterion of mutual\ninformation is used in the vertical decomposition process. This allows the use\nof an anonymization process, which is based on combining results from multiple\nindependent fragments. We present a general approach which can be applied to\nthe k-anonymity, l-diversity, and t-closeness models. In the presence of\ninter-attribute correlations, such an approach continues to be much more robust\nin higher dimensionality, without losing accuracy. We present experimental\nresults illustrating the effectiveness of the approach. This approach is\nresilient enough to prevent identity, attribute, and membership disclosure\nattack."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPCA.2014.6835958", 
    "link": "http://arxiv.org/pdf/1401.1406v2", 
    "other_authors": "Lei Wang, Jianfeng Zhan, Chunjie Luo, Yuqing Zhu, Qiang Yang, Yongqiang He, Wanling Gao, Zhen Jia, Yingjie Shi, Shujie Zhang, Chen Zheng, Gang Lu, Kent Zhan, Xiaona Li, Bizhu Qiu", 
    "title": "BigDataBench: a Big Data Benchmark Suite from Internet Services", 
    "arxiv-id": "1401.1406v2", 
    "author": "Bizhu Qiu", 
    "publish": "2014-01-06T12:35:31Z", 
    "summary": "As architecture, systems, and data management communities pay greater\nattention to innovative big data systems and architectures, the pressure of\nbenchmarking and evaluating these systems rises. Considering the broad use of\nbig data systems, big data benchmarks must include diversity of data and\nworkloads. Most of the state-of-the-art big data benchmarking efforts target\nevaluating specific types of applications or system software stacks, and hence\nthey are not qualified for serving the purposes mentioned above. This paper\npresents our joint research efforts on this issue with several industrial\npartners. Our big data benchmark suite BigDataBench not only covers broad\napplication scenarios, but also includes diverse and representative data sets.\nBigDataBench is publicly available from http://prof.ict.ac.cn/BigDataBench .\nAlso, we comprehensively characterize 19 big data workloads included in\nBigDataBench with varying data inputs. On a typical state-of-practice\nprocessor, Intel Xeon E5645, we have the following observations: First, in\ncomparison with the traditional benchmarks: including PARSEC, HPCC, and\nSPECCPU, big data applications have very low operation intensity; Second, the\nvolume of data input has non-negligible impact on micro-architecture\ncharacteristics, which may impose challenges for simulation-based big data\narchitecture research; Last but not least, corroborating the observations in\nCloudSuite and DCBench (which use smaller data inputs), we find that the\nnumbers of L1 instruction cache misses per 1000 instructions of the big data\napplications are higher than in the traditional benchmarks; also, we find that\nL3 caches are effective for the big data applications, corroborating the\nobservation in DCBench."
},{
    "category": "cs.DB", 
    "doi": "10.1109/HPCA.2014.6835958", 
    "link": "http://arxiv.org/pdf/1401.2101v1", 
    "other_authors": "Massimo Carro", 
    "title": "NoSQL Databases", 
    "arxiv-id": "1401.2101v1", 
    "author": "Massimo Carro", 
    "publish": "2014-01-09T17:55:33Z", 
    "summary": "In this document, I present the main notions of NoSQL databases and compare\nfour selected products (Riak, MongoDB, Cassandra, Neo4J) according to their\ncapabilities with respect to consistency, availability, and partition\ntolerance, as well as performance. I also propose a few criteria for selecting\nthe right tool for the right situation."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.2250v1", 
    "other_authors": "Md. Palash Uddin, Ashfaque Ahmed, Md. Delowar Hossain, Masud Ibn Afjal, Shah Md. Tanvir Siddiquee", 
    "title": "High speed data retrieval from national data center (ndc) reducing time   and ignoring spelling error in search key based on double metaphone algorithm", 
    "arxiv-id": "1401.2250v1", 
    "author": "Shah Md. Tanvir Siddiquee", 
    "publish": "2014-01-10T08:21:22Z", 
    "summary": "Fast and efficient data management is one of the demanding technologies of\ntodays aspect. This paper proposes a system which makes the working procedures\nof present manual system of storing and retrieving huge citizens information of\nBangladesh automated and increases its effectiveness. The implemented search\nmethodology is user friendly and efficient enough for high speed data retrieval\nignoring spelling error in the input keywords used for searching a particular\ncitizen. The main concern in this research is minimizing the total searching\ntime for a given keyword. This can be done if we can pre-establish the idea of\ngetting the data belonging to the searching keyword. The primary and secondary\nkey-code generated by the Double Metaphone Algorithm for each word is used to\nestablish that idea about the word. This algorithm is used for creating the map\nof the original database, through which the keyword is matched against the\ndata."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.2690v1", 
    "other_authors": "Shuai Ma, Kaiyu Feng, Haixun Wang, Jianxin Li, Jinpeng Huai", 
    "title": "Distance Landmarks Revisited for Road Graphs", 
    "arxiv-id": "1401.2690v1", 
    "author": "Jinpeng Huai", 
    "publish": "2014-01-13T00:56:12Z", 
    "summary": "Computing shortest distances is one of the fundamental problems on graphs,\nand remains a {\\em challenging} task today. {\\em Distance} {\\em landmarks} have\nbeen recently studied for shortest distance queries with an auxiliary data\nstructure, referred to as {\\em landmark} {\\em covers}. This paper studies how\nto apply distance landmarks for fast {\\em exact} shortest distance query\nanswering on large road graphs. However, the {\\em direct} application of\ndistance landmarks is {\\em impractical} due to the high space and time cost. To\nrectify this problem, we investigate novel techniques that can be seamlessly\ncombined with distance landmarks. We first propose a notion of {\\em hybrid\nlandmark covers}, a revision of landmark covers. Second, we propose a notion of\n{\\em agents}, each of which represents a small subgraph and holds good\nproperties for fast distance query answering. We also show that agents can be\ncomputed in {\\em linear time}. Third, we introduce graph partitions to deal\nwith the remaining subgraph that cannot be captured by agents. Fourth, we\ndevelop a unified framework that seamlessly integrates our proposed techniques\nand existing optimization techniques, for fast shortest distance query\nanswering. Finally, we experimentally verify that our techniques significantly\nimprove the efficiency of shortest distance queries, using real-life road\ngraphs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.2911v1", 
    "other_authors": "Raju Dara, Dr. Ch. Satyanarayana, Dr. A. Govardhan", 
    "title": "Front End Data Cleaning And Transformation In Standard Printed Form   Using Neural Models", 
    "arxiv-id": "1401.2911v1", 
    "author": "Dr. A. Govardhan", 
    "publish": "2014-01-09T13:55:29Z", 
    "summary": "Front end of data collection and loading into database manually may cause\npotential errors in data sets and a very time consuming process. Scanning of a\ndata document in the form of an image and recognition of corresponding\ninformation in that image can be considered as a possible solution of this\nchallenge. This paper presents an automated solution for the problem of data\ncleansing and recognition of user written data to transform into standard\nprinted format with the help of artificial neural networks. Three different\nneural models namely direct, correlation based and hierarchical have been\ndeveloped to handle this issue. In a very hostile input environment, the\nsolution is developed to justify the proposed logic."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.4840v2", 
    "other_authors": "Tomasz Gogacz, Jerzy Marcinkowski", 
    "title": "Termination of oblivious chase is undecidable", 
    "arxiv-id": "1401.4840v2", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2014-01-20T09:43:59Z", 
    "summary": "We show that all--instances termination of chase is undecidable. More\nprecisely, there is no algorithm deciding, for a given set $\\cal T$ consisting\nof Tuple Generating Dependencies (a.k.a. Datalog$^\\exists$ program), whether\nthe $\\cal T$-chase on $D$ will terminate for every finite database instance\n$D$. Our method applies to Oblivious Chase, Semi-Oblivious Chase and -- after a\nslight modification -- also for Standard Chase. This means that we give a\n(negative) solution to the all--instances termination problem for all version\nof chase that are usually considered.\n  The arity we need for our undecidability proof is three. We also show that\nthe problem is EXPSPACE-hard for binary signatures, but decidability for this\ncase is left open.\n  Both the proofs -- for ternary and binary signatures -- are easy. Once you\nknow them."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.5051v1", 
    "other_authors": "Olivier Cur\u00e9, Guillaume Blin, Dominique Revuz, David Faye", 
    "title": "WaterFowl, a Compact, Self-indexed RDF Store with Inference-enabled   Dictionaries", 
    "arxiv-id": "1401.5051v1", 
    "author": "David Faye", 
    "publish": "2014-01-20T20:48:05Z", 
    "summary": "In this paper, we present a novel approach -- called WaterFowl -- for the\nstorage of RDF triples that addresses some key issues in the contexts of big\ndata and the Semantic Web. The architecture of our prototype, largely based on\nthe use of succinct data structures, enables the representation of triples in a\nself-indexed, compact manner without requiring decompression at query answering\ntime. Moreover, it is adapted to efficiently support RDF and RDFS entailment\nregimes thanks to an optimized encoding of ontology concepts and properties\nthat does not require a complete inference materialization or extensive query\nrewriting algorithms. This approach implies to make a distinction between the\nterminological and the assertional components of the knowledge base early in\nthe process of data preparation, i.e., preprocessing the data before storing it\nin our structures. The paper describes the complete architecture of this system\nand presents some preliminary results obtained from evaluations conducted on\nour first prototype."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.5465v3", 
    "other_authors": "Zijian Ming, Chunjie Luo, Wanling Gao, Rui Han, Qiang Yang, Lei Wang, Jianfeng Zhan", 
    "title": "BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking", 
    "arxiv-id": "1401.5465v3", 
    "author": "Jianfeng Zhan", 
    "publish": "2014-01-22T02:17:52Z", 
    "summary": "Data generation is a key issue in big data benchmarking that aims to generate\napplication-specific data sets to meet the 4V requirements of big data.\nSpecifically, big data generators need to generate scalable data (Volume) of\ndifferent types (Variety) under controllable generation rates (Velocity) while\nkeeping the important characteristics of raw data (Veracity). This gives rise\nto various new challenges about how we design generators efficiently and\nsuccessfully. To date, most existing techniques can only generate limited types\nof data and support specific big data systems such as Hadoop. Hence we develop\na tool, called Big Data Generator Suite (BDGS), to efficiently generate\nscalable big data while employing data models derived from real data to\npreserve data veracity. The effectiveness of BDGS is demonstrated by developing\nsix data generators covering three representative data types (structured,\nsemi-structured and unstructured) and three data sources (text, graph, and\ntable data)."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.6360v1", 
    "other_authors": "Niv Dayan, Martin Kjaer Svendsen, Matias Bjorling, Philippe Bonnet, Luc Bouganim", 
    "title": "EagleTree: Exploring the Design Space of SSD-Based Algorithms", 
    "arxiv-id": "1401.6360v1", 
    "author": "Luc Bouganim", 
    "publish": "2014-01-24T14:54:41Z", 
    "summary": "Solid State Drives (SSDs) are a moving target for system designers: they are\nblack boxes, their internals are undocumented, and their performance\ncharacteristics vary across models. There is no appropriate analytical model\nand experimenting with commercial SSDs is cumbersome, as it requires a careful\nexperimental methodology to ensure repeatability. Worse, performance results\nobtained on a given SSD cannot be generalized. Overall, it is impossible to\nexplore how a given algorithm, say a hash join or LSM-tree insertions,\nleverages the intrinsic parallelism of a modern SSD, or how a slight change in\nthe internals of an SSD would impact its overall performance. In this paper, we\npropose a new SSD simulation framework, named EagleTree, which addresses these\nproblems, and enables a principled study of SSD-Based algorithms. The\ndemonstration scenario illustrates the design space for algorithms based on an\nSSD-based IO stack, and shows how researchers and practitioners can use\nEagleTree to perform tractable explorations of this complex design space."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.6887v1", 
    "other_authors": "Kifayat Ullah Khan, Kamran Najeebullah, Waqas Nawaz, Young-Koo Lee", 
    "title": "OLAP on Structurally Significant Data in Graphs", 
    "arxiv-id": "1401.6887v1", 
    "author": "Young-Koo Lee", 
    "publish": "2014-01-27T15:16:17Z", 
    "summary": "Summarized data analysis of graphs using OLAP (Online Analytical Processing)\nis very popular these days. However due to high dimensionality and large size,\nit is not easy to decide which data should be aggregated for OLAP analysis.\nThough iceberg cubing is useful, but it is unaware of the significance of\ndimensional values with respect to the structure of the graph. In this paper,\nwe propose a Structural Significance, SS, measure to identify the structurally\nsignificant dimensional values in each dimension. This leads to structure aware\npruning. We then propose an algorithm, iGraphCubing, to compute the graph cube\nto analyze the structurally significant data using the proposed measure. We\nevaluated the proposed ideas on real and synthetic data sets and observed very\nencouraging results."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.7584v1", 
    "other_authors": "Michael Kohlhase, Corneliu Prodescu, Christian Liguda", 
    "title": "XLSearch: A Search Engine for Spreadsheets", 
    "arxiv-id": "1401.7584v1", 
    "author": "Christian Liguda", 
    "publish": "2014-01-28T20:38:05Z", 
    "summary": "Spreadsheets are end-user programs and domain models that are heavily\nemployed in administration, financial forecasting, education, and science\nbecause of their intuitive, flexible, and direct approach to computation. As a\nresult, institutions are swamped by millions of spreadsheets that are becoming\nincreasingly difficult to manage, access, and control.\n  This note presents the XLSearch system, a novel search engine for\nspreadsheets. It indexes spreadsheet formulae and efficiently answers formula\nqueries via unification (a complex query language that allows metavariables in\nboth the query as well as the index). But a web-based search engine is only one\napplication of the underlying technology: Spreadsheet formula export to web\nstandards like MathML combined with formula indexing can be used to find\nsimilar spreadsheets or common formula errors."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1401.7733v1", 
    "other_authors": "C. Sunil Kumar, J. Seetha, S. R. Vinotha", 
    "title": "Security Implications of Distributed Database Management System Models", 
    "arxiv-id": "1401.7733v1", 
    "author": "S. R. Vinotha", 
    "publish": "2014-01-30T04:43:22Z", 
    "summary": "Security features must be addressed when escalating a distributed database.\nThe choice between the object oriented and the relational data model, several\nfactors should be considered. The most important of these factors are single\nand multilevel access controls (MAC), protection and integrity maintenance.\nWhile determining which distributed database replica will be more secure for a\nparticular function, the choice should not be made exclusively on the basis of\navailable security features. One should also query the effectiveness and\nefficiency of the delivery of these characteristics. In this paper, the\nsecurity strengths and weaknesses of both database models and the thorough\nproblems initiate in the distributed environment are conversed."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1403.0017v2", 
    "other_authors": "Zoran Majkic", 
    "title": "Intensional RDB Manifesto: a Unifying NewSQL Model for Flexible Big Data", 
    "arxiv-id": "1403.0017v2", 
    "author": "Zoran Majkic", 
    "publish": "2014-02-28T21:48:30Z", 
    "summary": "In this paper we present a new family of Intensional RDBs (IRDBs) which\nextends the traditional RDBs with the Big Data and flexible and 'Open schema'\nfeatures, able to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. We define a query\nrewriting algorithm, based on the GAV Data Integration methods, so that each\nuser-defined SQL query is rewritten into a SQL query over this vector relation,\nand hence the user-defined standard RDB schema is maintained as an empty global\nschema for the RDB schema modeling of data and as the SQL interface to stored\nvector relation. Such an IRDB architecture is adequate for the massive\nmigrations from the existing slow RDBMSs into this new family of fast IRDBMSs\nby offering a Big Data and new flexible schema features as well."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1403.0230v1", 
    "other_authors": "Ashiq Anjum, Peter Bloodsworth, Andrew Branson, Irfan Habib, Richard McClatchey, Tony Solomonides, the neuGRID Consortium", 
    "title": "Research Traceability using Provenance Services for Biomedical Analysis", 
    "arxiv-id": "1403.0230v1", 
    "author": "the neuGRID Consortium", 
    "publish": "2014-03-02T16:11:58Z", 
    "summary": "We outline the approach being developed in the neuGRID project to use\nprovenance management techniques for the purposes of capturing and preserving\nthe provenance data that emerges in the specification and execution of\nworkflows in biomedical analyses. In the neuGRID project a provenance service\nhas been designed and implemented that is intended to capture, store, retrieve\nand reconstruct the workflow information needed to facilitate users in\nconducting user analyses. We describe the architecture of the neuGRID\nprovenance service and discuss how the CRISTAL system from CERN is being\nadapted to address the requirements of the project and then consider how a\ngeneralised approach for provenance management could emerge for more generic\napplication to the (Health)Grid community."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1403.0701v1", 
    "other_authors": "Aapo Kyrola, Carlos Guestrin", 
    "title": "GraphChi-DB: Simple Design for a Scalable Graph Database System -- on   Just a PC", 
    "arxiv-id": "1403.0701v1", 
    "author": "Carlos Guestrin", 
    "publish": "2014-03-04T07:05:06Z", 
    "summary": "We propose a new data structure, Parallel Adjacency Lists (PAL), for\nefficiently managing graphs with billions of edges on disk. The PAL structure\nis based on the graph storage model of GraphChi (Kyrola et. al., OSDI 2012),\nbut we extend it to enable online database features such as queries and fast\ninsertions. In addition, we extend the model with edge and vertex attributes.\nCompared to previous data structures, PAL can store graphs more compactly while\nallowing fast access to both the incoming and the outgoing edges of a vertex,\nwithout duplicating data. Based on PAL, we design a graph database management\nsystem, GraphChi-DB, which can also execute powerful analytical graph\ncomputation.\n  We evaluate our design experimentally and demonstrate that GraphChi-DB\nachieves state-of-the-art performance on graphs that are much larger than the\navailable memory. GraphChi-DB enables anyone with just a laptop or a PC to work\nwith extremely large graphs."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsea.2013.3601", 
    "link": "http://arxiv.org/pdf/1403.0779v2", 
    "other_authors": "Minhao Jiang, Ada Wai-Chee Fu, Raymond Chi-Wing Wong, Yanyan Xu", 
    "title": "Hop Doubling Label Indexing for Point-to-Point Distance Querying on   Scale-Free Networks", 
    "arxiv-id": "1403.0779v2", 
    "author": "Yanyan Xu", 
    "publish": "2014-03-04T13:09:09Z", 
    "summary": "We study the problem of point-to-point distance querying for massive\nscale-free graphs, which is important for numerous applications. Given a\ndirected or undirected graph, we propose to build an index for answering such\nqueries based on a hop-doubling labeling technique. We derive bounds on the\nindex size, the computation costs and I/O costs based on the properties of\nunweighted scale-free graphs. We show that our method is much more efficient\ncompared to the state-of-the-art technique, in terms of both querying time and\nindexing time. Our empirical study shows that our method can handle graphs that\nare orders of magnitude larger than existing methods."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-662-43984-5_27", 
    "link": "http://arxiv.org/pdf/1403.0783v1", 
    "other_authors": "Antoine Amarilli, Yael Amsterdamer, Tova Milo", 
    "title": "Uncertainty in Crowd Data Sourcing under Structural Constraints", 
    "arxiv-id": "1403.0783v1", 
    "author": "Tova Milo", 
    "publish": "2014-03-04T13:21:39Z", 
    "summary": "Applications extracting data from crowdsourcing platforms must deal with the\nuncertainty of crowd answers in two different ways: first, by deriving\nestimates of the correct value from the answers; second, by choosing crowd\nquestions whose answers are expected to minimize this uncertainty relative to\nthe overall data collection goal. Such problems are already challenging when we\nassume that questions are unrelated and answers are independent, but they are\neven more complicated when we assume that the unknown values follow hard\nstructural constraints (such as monotonicity).\n  In this vision paper, we examine how to formally address this issue with an\napproach inspired by [Amsterdamer et al., 2013]. We describe a generalized\nsetting where we model constraints as linear inequalities, and use them to\nguide the choice of crowd questions and the processing of answers. We present\nthe main challenges arising in this setting, and propose directions to solve\nthem."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-662-43984-5_27", 
    "link": "http://arxiv.org/pdf/1403.2307v2", 
    "other_authors": "Sudip Roy, Lucja Kot, Gabriel Bender, Bailu Ding, Hossein Hojjat, Christoph Koch, Nate Foster, Johannes Gehrke", 
    "title": "The Homeostasis Protocol: Avoiding Transaction Coordination Through   Program Analysis", 
    "arxiv-id": "1403.2307v2", 
    "author": "Johannes Gehrke", 
    "publish": "2014-03-10T17:12:09Z", 
    "summary": "Datastores today rely on distribution and replication to achieve improved\nperformance and fault-tolerance. But correctness of many applications depends\non strong consistency properties - something that can impose substantial\noverheads, since it requires coordinating the behavior of multiple nodes. This\npaper describes a new approach to achieving strong consistency in distributed\nsystems while minimizing communication between nodes. The key insight is to\nallow the state of the system to be inconsistent during execution, as long as\nthis inconsistency is bounded and does not affect transaction correctness. In\ncontrast to previous work, our approach uses program analysis to extract\nsemantic information about permissible levels of inconsistency and is fully\nautomated. We then employ a novel homeostasis protocol to allow sites to\noperate independently, without communicating, as long as any inconsistency is\ngoverned by appropriate treaties between the nodes. We discuss mechanisms for\noptimizing treaties based on workload characteristics to minimize\ncommunication, as well as a prototype implementation and experiments that\ndemonstrate the benefits of our approach on common transactional benchmarks."
},{
    "category": "cs.DB", 
    "doi": "10.1007/978-3-662-43984-5_27", 
    "link": "http://arxiv.org/pdf/1403.2763v2", 
    "other_authors": "Weimo Liu, Saravanan Thirumuruganathan, Nan Zhang, Gautam Das", 
    "title": "Aggregate Estimation Over Dynamic Hidden Web Databases", 
    "arxiv-id": "1403.2763v2", 
    "author": "Gautam Das", 
    "publish": "2014-03-11T21:32:59Z", 
    "summary": "Many databases on the web are \"hidden\" behind (i.e., accessible only through)\ntheir restrictive, form-like, search interfaces. Recent studies have shown that\nit is possible to estimate aggregate query answers over such hidden web\ndatabases by issuing a small number of carefully designed search queries\nthrough the restrictive web interface. A problem with these existing work,\nhowever, is that they all assume the underlying database to be static, while\nmost real-world web databases (e.g., Amazon, eBay) are frequently updated. In\nthis paper, we study the novel problem of estimating/tracking aggregates over\ndynamic hidden web databases while adhering to the stringent query-cost\nlimitation they enforce (e.g., at most 1,000 search queries per day).\nTheoretical analysis and extensive real-world experiments demonstrate the\neffectiveness of our proposed algorithms and their superiority over baseline\nsolutions (e.g., the repeated execution of algorithms designed for static web\ndatabases)."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V9P101", 
    "link": "http://arxiv.org/pdf/1403.2958v1", 
    "other_authors": "Deepa S", 
    "title": "An Approach for Normalizing Fuzzy Relational Databases Based on Join   Dependency", 
    "arxiv-id": "1403.2958v1", 
    "author": "Deepa S", 
    "publish": "2014-03-11T07:30:43Z", 
    "summary": "Fuzziness in databases is used to denote uncertain or incomplete data.\nRelational Databases stress on the nature of the data to be certain. This\ncertainty based data is used as the basis of the normalization approach\ndesigned for traditional relational databases. But real world data may not\nalways be certain, thereby making it necessary to design an approach for\nnormalization that deals with fuzzy data. This paper focuses on the approach\nfor designing the fifth normal form (5NF) based on join dependencies for fuzzy\ndata. The basis of join dependency for fuzzy relational databases is derived\nfrom the basic relational database concepts. As join dependency implies an\nmultivalued dependency by symmetry the proof of join dependency based\nnormalization is stated from the perspective of multivalued dependency based\nnormalization on fuzzy relational databases."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V9P101", 
    "link": "http://arxiv.org/pdf/1403.3304v1", 
    "other_authors": "Hadi Hajari, Farshad Hakimpour", 
    "title": "A Spatial Data Model for Moving Object Databases", 
    "arxiv-id": "1403.3304v1", 
    "author": "Farshad Hakimpour", 
    "publish": "2014-03-13T15:40:54Z", 
    "summary": "Moving Object Databases will have significant role in Geospatial Information\nSystems as they allow users to model continuous movements of entities in the\ndatabases and perform spatio-temporal analysis. For representing and querying\nmoving objects, and algebra with a comprehensive framework of User Defined\nTypes together with a set of functions on those types is needed. Moreover,\nconcerning real world applications, moving objects move along constrained\nenvironments like transportation networks so that an extra algebra for modeling\nnetworks is demanded, too. These algebras can be inserted in any data model if\ntheir designs are based on available standards such as Open Geospatial\nConsortium that provides a common model for existing DBMS's. In this paper, we\nfocus on extending a spatial data model for constrained moving objects. Static\nand moving geometries in our model are based on Open Geospatial Consortium\nstandards. We also extend Structured Query Language for retrieving, querying,\nand manipulating spatio-temporal data related to moving objects as a simple and\nexpressive query language. Finally as a proof of concept, we implement a\ngenerator to generate data for moving objects constrained by a transportation\nnetwork. Such a generator primarily aims at traffic planning applications."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V9P101", 
    "link": "http://arxiv.org/pdf/1403.3948v1", 
    "other_authors": "Mohammed Al-Maolegi, Bassam Arkok", 
    "title": "An Improved Apriori Algorithm for Association Rules", 
    "arxiv-id": "1403.3948v1", 
    "author": "Bassam Arkok", 
    "publish": "2014-03-16T18:50:54Z", 
    "summary": "There are several mining algorithms of association rules. One of the most\npopular algorithms is Apriori that is used to extract frequent itemsets from\nlarge database and getting the association rule for discovering the knowledge.\nBased on this algorithm, this paper indicates the limitation of the original\nApriori algorithm of wasting time for scanning the whole database searching on\nthe frequent itemsets, and presents an improvement on Apriori by reducing that\nwasted time depending on scanning only some transactions. The paper shows by\nexperimental results with several groups of transactions, and with several\nvalues of minimum support that applied on the original Apriori and our\nimplemented improved Apriori that our improved Apriori reduces the time\nconsumed by 67.38% in comparison with the original Apriori, and makes the\nApriori algorithm more efficient and less time consuming."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V9P101", 
    "link": "http://arxiv.org/pdf/1403.5381v1", 
    "other_authors": "Silu Huang, Ada Wai-Chee Fu", 
    "title": "(\u03b1, k)-Minimal Sorting and Skew Join in MPI and MapReduce", 
    "arxiv-id": "1403.5381v1", 
    "author": "Ada Wai-Chee Fu", 
    "publish": "2014-03-21T07:10:16Z", 
    "summary": "As computer clusters are found to be highly effective for handling massive\ndatasets, the design of efficient parallel algorithms for such a computing\nmodel is of great interest. We consider ({\\alpha}, k)-minimal algorithms for\nsuch a purpose, where {\\alpha} is the number of rounds in the algorithm, and k\nis a bound on the deviation from perfect workload balance. We focus on new\n({\\alpha}, k)-minimal algorithms for sorting and skew equijoin operations for\ncomputer clusters. To the best of our knowledge the proposed sorting and skew\njoin algorithms achieve the best workload balancing guarantee when compared to\nprevious works. Our empirical study shows that they are close to optimal in\nworkload balancing. In particular, our proposed sorting algorithm is around 25%\nmore efficient than the state-of-the-art Terasort algorithm and achieves\nsignificantly more even workload distribution by over 50%."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22312803/IJCTT-V9P101", 
    "link": "http://arxiv.org/pdf/1403.5645v1", 
    "other_authors": "Todd L. Veldhuizen", 
    "title": "Transaction Repair: Full Serializability Without Locks", 
    "arxiv-id": "1403.5645v1", 
    "author": "Todd L. Veldhuizen", 
    "publish": "2014-03-22T11:04:03Z", 
    "summary": "Transaction Repair is a method for lock-free, scalable transaction processing\nthat achieves full serializability. It demonstrates parallel speedup even in\ninimical scenarios where all pairs of transactions have significant read-write\nconflicts. In the transaction repair approach, each transaction runs in\ncomplete isolation in a branch of the database; when conflicts occur, we detect\nand repair them. These repairs are performed efficiently in parallel, and the\nnet effect is that of serial processing. Within transactions, we use no locks.\nThis frees users from the complications and performance hazards of locks, and\nfrom the anomalies of sub-SERIALIZABLE isolation levels. Our approach builds on\nan incrementalized variant of leapfrog triejoin, a worst-case optimal algorithm\nfor $\\exists_1$ formulae, and on well-established techniques from programming\nlanguages: declarative languages, purely functional data structures,\nincremental computation, and fixpoint equations."
},{
    "category": "cs.DB", 
    "doi": "10.1109/COMPSACW.2014.97", 
    "link": "http://arxiv.org/pdf/1403.5946v3", 
    "other_authors": "Jack Kelly, William Knottenbelt", 
    "title": "Metadata for Energy Disaggregation", 
    "arxiv-id": "1403.5946v3", 
    "author": "William Knottenbelt", 
    "publish": "2014-03-24T13:29:04Z", 
    "summary": "Energy disaggregation is the process of estimating the energy consumed by\nindividual electrical appliances given only a time series of the whole-home\npower demand. Energy disaggregation researchers require datasets of the power\ndemand from individual appliances and the whole-home power demand. Multiple\nsuch datasets have been released over the last few years but provide metadata\nin a disparate array of formats including CSV files and plain-text README\nfiles. At best, the lack of a standard metadata schema makes it unnecessarily\ntime-consuming to write software to process multiple datasets and, at worse,\nthe lack of a standard means that crucial information is simply absent from\nsome datasets. We propose a metadata schema for representing appliances,\nmeters, buildings, datasets, prior knowledge about appliances and appliance\nmodels. The schema is relational and provides a simple but powerful inheritance\nmechanism."
},{
    "category": "cs.DB", 
    "doi": "10.1109/COMPSACW.2014.97", 
    "link": "http://arxiv.org/pdf/1403.6089v2", 
    "other_authors": "Zoran Majkic", 
    "title": "Intensional RDB for Big Data Interoperability", 
    "arxiv-id": "1403.6089v2", 
    "author": "Zoran Majkic", 
    "publish": "2014-03-24T19:15:37Z", 
    "summary": "A new family of Intensional RDBs (IRDBs), introduced in [1], extends the\ntraditional RDBs with the Big Data and flexible and 'Open schema' features,\nable to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. Such an IRDB architecture\nis adequate for the massive migrations from the existing slow RDBMSs into this\nnew family of fast IRDBMSs by offering a Big Data and new flexible schema\nfeatures as well. Here we present the interoperability features of the IRDBs by\npermitting the queries also over the internal vector relations created by\nparsing of each federated database in a given Multidatabase system. We show\nthat the SchemaLog with the second-order syntax and ad hoc Logic Programming\nand its querying fragment can be embedded into the standard SQL IRDBMSs, so\nthat we obtain a full interoperabilty features of IRDBs by using only the\nstandard relational SQL for querying both data and meta-data."
},{
    "category": "cs.DB", 
    "doi": "10.1109/COMPSACW.2014.97", 
    "link": "http://arxiv.org/pdf/1403.6985v3", 
    "other_authors": "Kostyantyn Demchuk, Douglas J. Leith", 
    "title": "A Fast Minimal Infrequent Itemset Mining Algorithm", 
    "arxiv-id": "1403.6985v3", 
    "author": "Douglas J. Leith", 
    "publish": "2014-03-27T11:54:27Z", 
    "summary": "A novel fast algorithm for finding quasi identifiers in large datasets is\npresented. Performance measurements on a broad range of datasets demonstrate\nsubstantial reductions in run-time relative to the state of the art and the\nscalability of the algorithm to realistically-sized datasets up to several\nmillion records."
},{
    "category": "cs.DB", 
    "doi": "10.1109/COMPSACW.2014.97", 
    "link": "http://arxiv.org/pdf/1403.7248v1", 
    "other_authors": "Albin Ahmeti, Diego Calvanese, Axel Polleres", 
    "title": "Updating RDFS ABoxes and TBoxes in SPARQL", 
    "arxiv-id": "1403.7248v1", 
    "author": "Axel Polleres", 
    "publish": "2014-03-27T23:43:38Z", 
    "summary": "Updates in RDF stores have recently been standardised in the SPARQL 1.1\nUpdate specification. However, computing answers entailed by ontologies in\ntriple stores is usually treated orthogonal to updates. Even the W3C's recent\nSPARQL 1.1 Update language and SPARQL 1.1 Entailment Regimes specifications\nexplicitly exclude a standard behaviour how SPARQL endpoints should treat\nentailment regimes other than simple entailment in the context of updates. In\nthis paper, we take a first step to close this gap. We define a fragment of\nSPARQL basic graph patterns corresponding to (the RDFS fragment of) DL-Lite and\nthe corresponding SPARQL update language, dealing with updates both of ABox and\nof TBox statements. We discuss possible semantics along with potential\nstrategies for implementing them. We treat both, (i) materialised RDF stores,\nwhich store all entailed triples explicitly, and (ii) reduced RDF Stores, that\nis, redundancy-free RDF stores that do not store any RDF triples (corresponding\nto DL-Lite ABox statements) entailed by others already."
},{
    "category": "cs.DB", 
    "doi": "10.1109/COMPSACW.2014.97", 
    "link": "http://arxiv.org/pdf/1403.7729v1", 
    "other_authors": "Minos Garofalakis, Yannis Ioannidis", 
    "title": "Multi-Resource Parallel Query Scheduling and Optimization", 
    "arxiv-id": "1403.7729v1", 
    "author": "Yannis Ioannidis", 
    "publish": "2014-03-30T10:15:06Z", 
    "summary": "Scheduling query execution plans is a particularly complex problem in\nshared-nothing parallel systems, where each site consists of a collection of\nlocal time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory)\nresources and communicates with remote sites by message-passing. Earlier work\non parallel query scheduling employs either (a) one-dimensional models of\nparallel task scheduling, effectively ignoring the potential benefits of\nresource sharing, or (b) models of globally accessible resource units, which\nare appropriate only for shared-memory architectures, since they cannot capture\nthe affinity of system resources to sites. In this paper, we develop a general\napproach capturing the full complexity of scheduling distributed,\nmulti-dimensional resource units for all forms of parallelism within and across\nqueries and operators. We present a level-based list scheduling heuristic\nalgorithm for independent query tasks (i.e., physical operator pipelines) that\nis provably near-optimal for given degrees of partitioned parallelism (with a\nworst-case performance ratio that depends on the number of time-shared and\nspace-shared resources per site and the granularity of the clones). We also\npropose extensions to handle blocking constraints in logical operator (e.g.,\nhash-join) pipelines and bushy query plans as well as on-line task arrivals\n(e.g., in a dynamic or multi-query execution environment). Experiments with our\nscheduling algorithms implemented on top of a detailed simulation model verify\ntheir effectiveness compared to existing approaches in a realistic setting.\nBased on our analytical and experimental results, we revisit the open problem\nof designing efficient cost models for parallel query optimization and propose\na solution that captures all the important parameters of parallel execution."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1403.7928v1", 
    "other_authors": "J. Urban, J. Pipek, M. Hron, F. Janky, R. Pap\u0159ok, M. Peterka, A. S. Duarte", 
    "title": "Integrated Data Acquisition, Storage, Retrieval and Processing Using the   COMPASS DataBase (CDB)", 
    "arxiv-id": "1403.7928v1", 
    "author": "A. S. Duarte", 
    "publish": "2014-03-31T09:38:37Z", 
    "summary": "We present a complex data handling system for the COMPASS tokamak, operated\nby IPP ASCR Prague, Czech Republic [1]. The system, called CDB (Compass\nDataBase), integrates different data sources as an assortment of data\nacquisition hardware and software from different vendors is used. Based on\nwidely available open source technologies wherever possible, CDB is vendor and\nplatform independent and it can be easily scaled and distributed. The data is\ndirectly stored and retrieved using a standard NAS (Network Attached Storage),\nhence independent of the particular technology; the description of the data\n(the metadata) is recorded in a relational database. Database structure is\ngeneral and enables the inclusion of multi-dimensional data signals in multiple\nrevisions (no data is overwritten). This design is inherently distributed as\nthe work is off-loaded to the clients. Both NAS and database can be implemented\nand optimized for fast local access as well as secure remote access. CDB is\nimplemented in Python language; bindings for Java, C/C++, IDL and Matlab are\nprovided. Independent data acquisitions systems as well as nodes managed by\nFireSignal [2] are all integrated using CDB. An automated data post-processing\nserver is a part of CDB. Based on dependency rules, the server executes, in\nparallel if possible, prescribed post-processing tasks."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.0046v1", 
    "other_authors": "Immanuel Trummer, Christoph Koch", 
    "title": "Approximation Schemes for Many-Objective Query Optimization", 
    "arxiv-id": "1404.0046v1", 
    "author": "Christoph Koch", 
    "publish": "2014-03-31T21:39:29Z", 
    "summary": "The goal of multi-objective query optimization (MOQO) is to find query plans\nthat realize a good compromise between conflicting objectives such as\nminimizing execution time and minimizing monetary fees in a Cloud scenario. A\npreviously proposed exhaustive MOQO algorithm needs hours to optimize even\nsimple TPC-H queries. This is why we propose several approximation schemes for\nMOQO that generate guaranteed near-optimal plans in seconds where exhaustive\noptimization takes hours.\n  We integrated all MOQO algorithms into the Postgres optimizer and present\nexperimental results for TPC-H queries; we extended the Postgres cost model and\noptimize for up to nine conflicting objectives in our experiments. The proposed\nalgorithms are based on a formal analysis of typical cost functions that occur\nin the context of MOQO. We identify properties that hold for a broad range of\nobjectives and can be exploited for the design of future MOQO algorithms."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.0696v1", 
    "other_authors": "S. Sioutas, E. Sakkopoulos, A. Panaretos, D. Tsoumakos, P. Gerolymatos, G. Tzimas, Y. Manolopoulos", 
    "title": "D-P2P-Sim+:A Novel Distributed Framework for P2P Protocols Performance   Testing", 
    "arxiv-id": "1404.0696v1", 
    "author": "Y. Manolopoulos", 
    "publish": "2014-04-02T20:31:12Z", 
    "summary": "In recent IoT (Internet of Things) and Web 2.0 technologies, a critical\nproblem arises with respect to storing and processing the large amount of\ncollected data. In this paper we develop and evaluate distributed\ninfrastructures for storing and processing large amount of such data. We\npresent a distributed framework that supports customized deployment of a\nvariety of indexing engines over million-node overlays. The proposed framework\nprovides the appropriate integrated set of tools that allows applications\nprocessing large amount of data, to evaluate and test the performance of\nvarious application protocols for very large scale deployments (multi million\nnodes - billions of keys). The key aim is to provide the appropriate\nenvironment that contributes in taking decisions regarding the choice of the\nprotocol in storage P2P systems for a variety of big data applications. Using\nlightweight and efficient collection mechanisms, our system enables real-time\nregistration of multiple measures, integrating support for real-life parameters\nsuch as node failure models and recovery strategies. Experiments have been\nperformed at the PlanetLab network and at a typical research laboratory in\norder to verify scalability and show maximum re-usability of our setup.\nD-P2P-Sim+ framework is publicly available at\nhttp://code.google.com/p/d-p2p-sim/downloads/list."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.1270v2", 
    "other_authors": "Iovka Boneva, Jose Emilio Labra Gayo, Samuel Hym, Eric G. Prud'hommeau, Harold Solbrig, S\u0142awek Staworko", 
    "title": "Validating RDF with Shape Expressions", 
    "arxiv-id": "1404.1270v2", 
    "author": "S\u0142awek Staworko", 
    "publish": "2014-04-04T14:39:48Z", 
    "summary": "We propose shape expression schema (ShEx), a novel schema formalism for\ndescribing the topology of an RDF graph that uses regular bag expressions\n(RBEs) to define constraints on the admissible neighborhood for the nodes of a\ngiven type. We provide two alternative semantics, multi- and single-type,\ndepending on whether or not a node may have more than one type. We study the\nexpressive power of ShEx and study the complexity of the validation problem. We\nshow that the single-type semantics is strictly more expressive than the\nmulti-type semantics, single-type validation is generally intractable and\nmulti-type validation is feasible for a small class of RBEs. To further curb\nthe high computational complexity of validation, we propose a natural notion of\ndeterminism and show that multi-type validation for the class of deterministic\nschemas using single-occurrence regular bag expressions (SORBEs) is tractable.\nFinally, we consider the problem of validating only a fragment of a graph with\npreassigned types for some of its nodes, and argue that for deterministic ShEx\nusing SORBEs, multi-type validation can be performed efficiently and\nsingle-type validation can be performed with a single pass over the graph."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.2034v1", 
    "other_authors": "Victor Alvarez, Felix Martin Schuhknecht, Jens Dittrich, Stefan Richter", 
    "title": "Main Memory Adaptive Indexing for Multi-core Systems", 
    "arxiv-id": "1404.2034v1", 
    "author": "Stefan Richter", 
    "publish": "2014-04-08T08:11:32Z", 
    "summary": "Adaptive indexing is a concept that considers index creation in databases as\na by-product of query processing; as opposed to traditional full index creation\nwhere the indexing effort is performed up front before answering any queries.\nAdaptive indexing has received a considerable amount of attention, and several\nalgorithms have been proposed over the past few years; including a recent\nexperimental study comparing a large number of existing methods. Until now,\nhowever, most adaptive indexing algorithms have been designed single-threaded,\nyet with multi-core systems already well established, the idea of designing\nparallel algorithms for adaptive indexing is very natural. In this regard only\none parallel algorithm for adaptive indexing has recently appeared in the\nliterature: The parallel version of standard cracking. In this paper we\ndescribe three alternative parallel algorithms for adaptive indexing, including\na second variant of a parallel standard cracking algorithm. Additionally, we\ndescribe a hybrid parallel sorting algorithm, and a NUMA-aware method based on\nsorting. We then thoroughly compare all these algorithms experimentally; along\na variant of a recently published parallel version of radix sort. Parallel\nsorting algorithms serve as a realistic baseline for multi-threaded adaptive\nindexing techniques. In total we experimentally compare seven parallel\nalgorithms. Additionally, we extensively profile all considered algorithms. The\ninitial set of experiments considered in this paper indicates that our parallel\nalgorithms significantly improve over previously known ones. Our results\nsuggest that, although adaptive indexing algorithms are a good design choice in\nsingle-threaded environments, the rules change considerably in the parallel\ncase. That is, in future highly-parallel environments, sorting algorithms could\nbe serious alternatives to adaptive indexing."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.2160v1", 
    "other_authors": "Timur Mirzoev, Craig Brockman", 
    "title": "SAP HANA and its performance benefits", 
    "arxiv-id": "1404.2160v1", 
    "author": "Craig Brockman", 
    "publish": "2014-04-08T14:46:03Z", 
    "summary": "In-memory computing has changed the landscape of database technology. Within\nthe database and technology field, advancements occur over the course of time\nthat has had the capacity to transform some fundamental tenants of the\ntechnology and how it is applied. The concept of Database Management Systems\n(DBMS) was realized in industry during the 1960s, allowing users and developers\nto use a navigational model to access the data stored by the computers of that\nday as they grew in speed and capability. This manuscript is specifically\nexamines the SAPHigh Performance Analytics Appliance(HANA) approach, which is\none of the commonly used technologies today. Additionally, this manuscript\nprovides the analysis of the first two of the four common main usecases to\nutilize SAP HANA's in-memory computing database technology. The performance\nbenefits are important factors for DB calculations.Some of the benefits are\nquantified and the demonstrated by the defined sets of data."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.3461v1", 
    "other_authors": "Xiaolu Lu, Dongxu Li, Xiang Li, Ling Feng", 
    "title": "A 2D based Partition Strategy for Solving Ranking under Team Context   (RTP)", 
    "arxiv-id": "1404.3461v1", 
    "author": "Ling Feng", 
    "publish": "2014-04-14T05:20:48Z", 
    "summary": "In this paper, we propose a 2D based partition method for solving the problem\nof Ranking under Team Context(RTC) on datasets without a priori. We first map\nthe data into 2D space using its minimum and maximum value among all\ndimensions. Then we construct window queries with consideration of current team\ncontext. Besides, during the query mapping procedure, we can pre-prune some\ntuples which are not top ranked ones. This pre-classified step will defer\nprocessing those tuples and can save cost while providing solutions for the\nproblem. Experiments show that our algorithm performs well especially on large\ndatasets with correctness."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.fusengdes.2014.03.032", 
    "link": "http://arxiv.org/pdf/1404.4286v1", 
    "other_authors": "Behrouz Minaei Bidgoli, Maryam Nazaridoust", 
    "title": "Case study: Data Mining of Associate Degree Accepted Candidates by   Modular Method", 
    "arxiv-id": "1404.4286v1", 
    "author": "Maryam Nazaridoust", 
    "publish": "2014-04-16T15:31:38Z", 
    "summary": "Since about 10 years ago, University of Applied Science and Technology (UAST)\nin Iran has admitted students in discontinuous associate degree by modular\nmethod, so that almost 100,000 students are accepted every year. Although the\nfirst aim of holding such courses was to improve scientific and skill level of\nemployees, over time a considerable group of unemployed people have been\ninterested to participate in these courses. According to this fact, in this\npaper, we mine and analyze a sample data of accepted candidates in modular 2008\nand 2009 courses by using unsupervised and supervised learning paradigms. In\nthe first step, by using unsupervised paradigm, we grouped (clustered) set of\nmodular accepted candidates based on their student status and labeled data sets\nby three classes so that each class somehow shows educational and student\nstatus of modular accepted candidates. In the second step, by using supervised\nand unsupervised algorithms, we generated predicting models in 2008 data sets.\nThen, by making a comparison between performances of generated models, we\nselected predicting model of association rules through which some rules were\nextracted. Finally, this model is executed for Test set which includes accepted\ncandidates of next course then by evaluation of results, the percentage of\ncorrectness and confidentiality of obtained results can be viewed."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1404.4963v2", 
    "other_authors": "Antonio Badia, Daniel Lemire", 
    "title": "Functional dependencies with null markers", 
    "arxiv-id": "1404.4963v2", 
    "author": "Daniel Lemire", 
    "publish": "2014-04-19T15:46:01Z", 
    "summary": "Functional dependencies are an integral part of database design. However,\nthey are only defined when we exclude null markers. Yet we commonly use null\nmarkers in practice. To bridge this gap between theory and practice,\nresearchers have proposed definitions of functional dependencies over relations\nwith null markers. Though sound, these definitions lack some qualities that we\nfind desirable. For example, some fail to satisfy Armstrong's axioms---while\nthese axioms are part of the foundation of common database methodologies. We\npropose a set of properties that any extension of functional dependencies over\nrelations with null markers should possess. We then propose two new extensions\nhaving these properties. These extensions attempt to allow null markers where\nthey make sense to practitioners.\n  They both support Armstrong's axioms and provide realizable null markers: at\nany time, some or all of the null markers can be replaced by actual values\nwithout causing an anomaly. Our proposals may improve database designs."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1404.6570v1", 
    "other_authors": "Jayanta Mondal, Amol Deshpande", 
    "title": "EAGr: Supporting Continuous Ego-centric Aggregate Queries over Large   Dynamic Graphs", 
    "arxiv-id": "1404.6570v1", 
    "author": "Amol Deshpande", 
    "publish": "2014-04-25T22:12:40Z", 
    "summary": "In this work, we present EAGr, a system for supporting large numbers of\ncontinuous neighborhood-based (\"ego-centric\") aggregate queries over large,\nhighly dynamic, and rapidly evolving graphs. Examples of such queries include\ncomputation of personalized, tailored trends in social networks, anomaly/event\ndetection in financial transaction networks, local search and alerts in\nspatio-temporal networks, to name a few. Key challenges in supporting such\ncontinuous queries include high update rates typically seen in these\nsituations, large numbers of queries that need to be executed simultaneously,\nand stringent low latency requirements. We propose a flexible, general, and\nextensible in-memory framework for executing different types of ego-centric\naggregate queries over large dynamic graphs with low latencies. Our framework\nis built around the notion of an aggregation overlay graph, a pre-compiled data\nstructure that encodes the computations to be performed when an update/query is\nreceived. The overlay graph enables sharing of partial aggregates across\nmultiple ego-centric queries (corresponding to the nodes in the graph), and\nalso allows partial pre-computation of the aggregates to minimize the query\nlatencies. We present several highly scalable techniques for constructing an\noverlay graph given an aggregation function, and also design incremental\nalgorithms for handling structural changes to the underlying graph. We also\npresent an optimal, polynomial-time algorithm for making the pre-computation\ndecisions given an overlay graph, and evaluate an approach to incrementally\nadapt those decisions as the workload changes. Although our approach is\nnaturally parallelizable, we focus on a single-machine deployment and show that\nour techniques can easily handle graphs of size up to 320 million nodes and\nedges, and achieve update/query throughputs of over 500K/s using a single,\npowerful machine."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1404.6664v2", 
    "other_authors": "Marc Burdon", 
    "title": "How to extract data from proprietary software database systems using   TCP/IP?", 
    "arxiv-id": "1404.6664v2", 
    "author": "Marc Burdon", 
    "publish": "2014-04-26T17:11:22Z", 
    "summary": "This document is an white paper about how to connect reverse engineering and\nprograming skills to extract data from a proprietary implementation of a\ndatabase system to build EML-Tools for data format conversion into raw data.\nThis article shows how to access data of a source software system without any\ninterface for data conversion. We discuss how raw data can be transfered into\nstructural format by using XML or any other custom designed software solution.\nFor demonstration purposes only, we will use a CRM system called Harmony(r) by\nHarmony(r) Software AG, the programing language Python and methods of computer\nsecurity, which are used to get quick access to the raw data.\n  All trademarks are property of their owners, as Harmony(r) is of Harmony\nSoftware AG."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1404.6857v2", 
    "other_authors": "Babak Salimi, Leopoldo Bertossi", 
    "title": "Causality in Databases: The Diagnosis and Repair Connections", 
    "arxiv-id": "1404.6857v2", 
    "author": "Leopoldo Bertossi", 
    "publish": "2014-04-28T02:58:31Z", 
    "summary": "In this work we establish and investigate the connections between causality\nfor query answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new problems in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes and the\nother way around. The vast body of research on database repairs can be applied\nto the newer problem of determining actual causes for query answers. By\nformulating a causality problem as a diagnosis problem, we manage to\ncharacterize causes in terms of a system's diagnoses."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1404.7571v1", 
    "other_authors": "Mina Ghashami, Jeff M. Phillips, Feifei Li", 
    "title": "Continuous Matrix Approximation on Distributed Data", 
    "arxiv-id": "1404.7571v1", 
    "author": "Feifei Li", 
    "publish": "2014-04-30T01:57:40Z", 
    "summary": "Tracking and approximating data matrices in streaming fashion is a\nfundamental challenge. The problem requires more care and attention when data\ncomes from multiple distributed sites, each receiving a stream of data. This\npaper considers the problem of \"tracking approximations to a matrix\" in the\ndistributed streaming model. In this model, there are m distributed sites each\nobserving a distinct stream of data (where each element is a row of a\ndistributed matrix) and has a communication channel with a coordinator, and the\ngoal is to track an eps-approximation to the norm of the matrix along any\ndirection. To that end, we present novel algorithms to address the matrix\napproximation problem. Our algorithms maintain a smaller matrix B, as an\napproximation to a distributed streaming matrix A, such that for any unit\nvector x: | ||A x||^2 - ||B x||^2 | <= eps ||A||_F^2. Our algorithms work in\nstreaming fashion and incur small communication, which is critical for\ndistributed computation. Our best method is deterministic and uses only\nO((m/eps) log(beta N)) communication, where N is the size of stream (at the\ntime of the query) and beta is an upper-bound on the squared norm of any row of\nthe matrix. In addition to proving all algorithmic properties theoretically,\nextensive experiments with real large datasets demonstrate the efficiency of\nthese protocols."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1405.0253v2", 
    "other_authors": "Federica Panella", 
    "title": "Approximate Query Answering in Inconsistent Databases", 
    "arxiv-id": "1405.0253v2", 
    "author": "Federica Panella", 
    "publish": "2014-05-01T19:09:39Z", 
    "summary": "Classical algorithms for query optimization presuppose the absence of\ninconsistencies or uncertainties in the database and exploit only valid\nsemantic knowledge provided, e.g., by integrity constraints. Data inconsistency\nor uncertainty, however, is a widespread critical issue in ordinary databases:\ntotal integrity is often, in fact, an unrealistic assumption and violations to\nintegrity constraints may be introduced in several ways.\n  In this report we present an approach for semantic query optimization that,\ndifferently from the traditional ones, relies on not necessarily valid semantic\nknowledge, e.g., provided by violated or soft integrity constraints, or induced\nby applying data mining techniques. Query optimization that leverages invalid\nsemantic knowledge cannot guarantee the semantic equivalence between the\noriginal user's query and its rewriting: thus a query optimized by our approach\nyields approximate answers that can be provided to the users whenever fast but\npossibly partial responses are required. Also, we evaluate the impact of use of\ninvalid semantic knowledge in the rewriting of a query by computing a measure\nof the quality of the answer returned to the user, and we rely on the recent\ntheory of Belief Logic Programming to deal with the presence of possible\ncorrelation in the semantic knowledge used in the rewriting."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1405.1339v1", 
    "other_authors": "Wilhelmiina H\u00e4m\u00e4l\u00e4inen", 
    "title": "General upper bounds for well-behaving goodness measures on dependency   rules", 
    "arxiv-id": "1405.1339v1", 
    "author": "Wilhelmiina H\u00e4m\u00e4l\u00e4inen", 
    "publish": "2014-05-06T16:22:38Z", 
    "summary": "In the search for statistical dependency rules, a crucial task is to restrict\nthe search space by estimating upper bounds for the goodness of yet\nundiscovered rules. In this paper, we show that all well-behaving goodness\nmeasures achieve their maximal values in the same points. Therefore, the same\ngeneric search strategy can be applied with any of these measures. The notion\nof well-behaving measures is based on the classical axioms for any proper\ngoodness measures, and extended to negative dependencies, as well. As an\nexample, we show that several commonly used goodness measures are\nwell-behaving."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1405.1360v1", 
    "other_authors": "Wilhelmiina H\u00e4m\u00e4l\u00e4inen", 
    "title": "Assessing the statistical significance of association rules", 
    "arxiv-id": "1405.1360v1", 
    "author": "Wilhelmiina H\u00e4m\u00e4l\u00e4inen", 
    "publish": "2014-05-06T16:54:20Z", 
    "summary": "An association rule is statistically significant, if it has a small\nprobability to occur by chance. It is well-known that the traditional\nfrequency-confidence framework does not produce statistically significant\nrules. It can both accept spurious rules (type 1 error) and reject significant\nrules (type 2 error). The same problem concerns other commonly used\ninterestingness measures and pruning heuristics.\n  In this paper, we inspect the most common measure functions - frequency,\nconfidence, degree of dependence, $\\chi^2$, correlation coefficient, and\n$J$-measure - and redundancy reduction techniques. For each technique, we\nanalyze whether it can make type 1 or type 2 error and the conditions under\nwhich the error occurs. In addition, we give new theoretical results which can\nbe use to guide the search for statistically significant association rules."
},{
    "category": "cs.DB", 
    "doi": "10.1093/comjnl/bxu039", 
    "link": "http://arxiv.org/pdf/1405.1705v1", 
    "other_authors": "Raman Grover, Michael J. Carey", 
    "title": "Scalable Fault-Tolerant Data Feeds in AsterixDB", 
    "arxiv-id": "1405.1705v1", 
    "author": "Michael J. Carey", 
    "publish": "2014-05-07T19:14:42Z", 
    "summary": "In this paper we describe the support for data feed ingestion in AsterixDB,\nan open-source Big Data Management System (BDMS) that provides a platform for\nstorage and analysis of large volumes of semi-structured data. Data feeds are a\nmechanism for having continuous data arrive into a BDMS from external sources\nand incrementally populate a persisted dataset and associated indexes. The need\nto persist and index \"fast-flowing\" high-velocity data (and support ad hoc\nanalytical queries) is ubiquitous. However, the state of the art today involves\n'gluing' together different systems. AsterixDB is different in being a unified\nsystem with \"native support\" for data feed ingestion.\n  We discuss the challenges and present the design and implementation of the\nconcepts involved in modeling and managing data feeds in AsterixDB. AsterixDB\nallows the runtime behavior, allocation of resources and the offered degree of\nrobustness to be customized to suit the high-level application(s) that wish to\nconsume the ingested data. Initial experiments that evaluate scalability and\nfault-tolerance of AsterixDB data feeds facility are reported."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijcsit.2014.6210", 
    "link": "http://arxiv.org/pdf/1405.1851v1", 
    "other_authors": "P. Cynthia Selvi, A. R. Mohammed Shanavas", 
    "title": "Output Privacy Protection With Pattern-Based Heuristic Algorithm", 
    "arxiv-id": "1405.1851v1", 
    "author": "A. R. Mohammed Shanavas", 
    "publish": "2014-05-08T09:26:52Z", 
    "summary": "Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at\nbridging the gap between the collaborative data mining and data confidentiality\nThere are many different approaches which have been adopted for PPDM, of them\nthe rule hiding approach is used in this article. This approach ensures output\nprivacy that prevent the mined patterns(itemsets) from malicious inference\nproblems. An efficient algorithm named as Pattern-based Maxcover Algorithm is\nproposed with experimental results. This algorithm minimizes the dissimilarity\nbetween the source and the released database; Moreover the patterns protected\ncannot be retrieved from the released database by an adversary or counterpart\neven with an arbitrarily low support threshold."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2014.6201", 
    "link": "http://arxiv.org/pdf/1405.1912v1", 
    "other_authors": "M\u00e1rta Czenky", 
    "title": "The Efficiency Examination of Teaching of Different Normalization   Methods", 
    "arxiv-id": "1405.1912v1", 
    "author": "M\u00e1rta Czenky", 
    "publish": "2014-05-08T13:06:34Z", 
    "summary": "Normalization is an important database design method, in the course of the\nteaching of data modeling the understanding and applying of this method cause\nproblems for students the most. For improving the efficiency of learning\nnormalization we looked for alternative normalization methods and introduced\nthem into education. We made a survey among engineer students how efficient\ncould they execute the normalization with different methods. We executed\nstatistical and data mining examinations to decide whether any of the methods\nresulted significantly better solutions."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2014.6201", 
    "link": "http://arxiv.org/pdf/1405.2848v1", 
    "other_authors": "Georg Gottlob, Giorgio Orsi, Andreas Pieris", 
    "title": "Query Rewriting and Optimization for Ontological Databases", 
    "arxiv-id": "1405.2848v1", 
    "author": "Andreas Pieris", 
    "publish": "2014-05-12T17:34:45Z", 
    "summary": "Ontological queries are evaluated against a knowledge base consisting of an\nextensional database and an ontology (i.e., a set of logical assertions and\nconstraints which derive new intensional knowledge from the extensional\ndatabase), rather than directly on the extensional database. The evaluation and\noptimization of such queries is an intriguing new problem for database\nresearch. In this paper, we discuss two important aspects of this problem:\nquery rewriting and query optimization. Query rewriting consists of the\ncompilation of an ontological query into an equivalent first-order query\nagainst the underlying extensional database. We present a novel query rewriting\nalgorithm for rather general types of ontological constraints which is\nwell-suited for practical implementations. In particular, we show how a\nconjunctive query against a knowledge base, expressed using linear and sticky\nexistential rules, that is, members of the recently introduced Datalog+/-\nfamily of ontology languages, can be compiled into a union of conjunctive\nqueries (UCQ) against the underlying database. Ontological query optimization,\nin this context, attempts to improve this rewriting process so to produce\npossibly small and cost-effective UCQ rewritings for an input query."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2014.6201", 
    "link": "http://arxiv.org/pdf/1405.3631v8", 
    "other_authors": "Kian Win Ong, Yannis Papakonstantinou, Romain Vernoux", 
    "title": "The SQL++ Query Language: Configurable, Unifying and Semi-structured", 
    "arxiv-id": "1405.3631v8", 
    "author": "Romain Vernoux", 
    "publish": "2014-05-14T19:17:36Z", 
    "summary": "NoSQL databases support semi-structured data, typically modeled as JSON. They\nalso provide limited (but expanding) query languages. Their idiomatic, non-SQL\nlanguage constructs, the many variations, and the lack of formal semantics\ninhibit deep understanding of the query languages, and also impede progress\ntowards clean, powerful, declarative query languages.\n  This paper specifies the syntax and semantics of SQL++, which is applicable\nto both JSON native stores and SQL databases. The SQL++ semi-structured data\nmodel is a superset of both JSON and the SQL data model. SQL++ offers powerful\ncomputational capabilities for processing semi-structured data akin to prior\nnon-relational query languages, notably OQL and XQuery. Yet, SQL++ is SQL\nbackwards compatible and is generalized towards JSON by introducing only a\nsmall number of query language extensions to SQL.\n  Recognizing that a query language standard is probably premature for the fast\nevolving area of NoSQL databases, SQL++ includes configuration options that\nformally itemize the semantics variations that language designers may choose\nfrom. The options often pertain to the treatment of semi-structuredness\n(missing attributes, heterogeneous types, etc), where more than one sensible\napproaches are possible.\n  SQL++ is unifying: By appropriate choices of configuration options, the SQL++\nsemantics can morph into the semantics of existing semi-structured database\nquery languages. The extensive experimental validation shows how SQL and four\nsemi-structured database query languages (MongoDB, Cassandra CQL, Couchbase\nN1QL and AsterixDB AQL) are formally described by appropriate settings of the\nconfiguration options.\n  Early adoption signs of SQL++ are positive: Version 4 of Couchbase's N1QL is\nexplained as syntactic sugar over SQL++. AsterixDB will soon support the full\nSQL++ and Apache Drill is in the process of aligning with SQL++."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2014.6201", 
    "link": "http://arxiv.org/pdf/1405.4228v2", 
    "other_authors": "Leopoldo Bertossi, Babak Salimi", 
    "title": "Unifying Causality, Diagnosis, Repairs and View-Updates in Databases", 
    "arxiv-id": "1405.4228v2", 
    "author": "Babak Salimi", 
    "publish": "2014-05-16T16:18:56Z", 
    "summary": "In this work we establish and point out connections between the notion of\nquery-answer causality in databases and database repairs, model-based diagnosis\nin its consistency-based and abductive versions, and database updates through\nviews. The mutual relationships among these areas of data management and\nknowledge representation shed light on each of them and help to share notions\nand results they have in common. In one way or another, these are all\napproaches to uncertainty management, which becomes even more relevant in the\ncontext of big data that have to be made sense of."
},{
    "category": "cs.DB", 
    "doi": "10.5121/ijdms.2014.6201", 
    "link": "http://arxiv.org/pdf/1405.4979v1", 
    "other_authors": "Razen Al-Harbi, Yasser Ebrahim, Panos Kalnis", 
    "title": "PHD-Store: An Adaptive SPARQL Engine with Dynamic Partitioning for   Distributed RDF Repositories", 
    "arxiv-id": "1405.4979v1", 
    "author": "Panos Kalnis", 
    "publish": "2014-05-20T07:44:03Z", 
    "summary": "Many repositories utilize the versatile RDF model to publish data.\nRepositories are typically distributed and geographically remote, but data are\ninterconnected (e.g., the Semantic Web) and queried globally by a language such\nas SPARQL. Due to the network cost and the nature of the queries, the execution\ntime can be prohibitively high. Current solutions attempt to minimize the\nnetwork cost by redistributing all data in a preprocessing phase, but here are\ntwo drawbacks: (i) redistribution is based on heuristics that may not benefit\nmany of the future queries; and (ii) the preprocessing phase is very expensive\neven for moderate size datasets. In this paper we propose PHD-Store, a SPARQL\nengine for distributed RDF repositories. Our system does not assume any\nparticular initial data placement and does not require prepartitioning; hence,\nit minimizes the startup cost. Initially, PHD-Store answers queries using a\npotentially slow distributed semi-join algorithm, but adapts dynamically to the\nquery load by incrementally redistributing frequently accessed data.\nRedistribution is done in a way that future queries can benefit from fast\nhash-based parallel execution. Our experiments with synthetic and real data\nverify that PHD-Store scales to very large datasets; many repositories;\nconverges to comparable or better quality of partitioning than existing\nmethods; and executes large query loads 1 to 2 orders of magnitude faster than\nour competitors."
},{
    "category": "cs.DB", 
    "doi": "10.1109/MCSE.2015.102", 
    "link": "http://arxiv.org/pdf/1405.5905v2", 
    "other_authors": "Bernardo Gon\u00e7alves, Fabio Porto", 
    "title": "Managing large-scale scientific hypotheses as uncertain and   probabilistic data with support for predictive analytics", 
    "arxiv-id": "1405.5905v2", 
    "author": "Fabio Porto", 
    "publish": "2014-05-22T20:43:51Z", 
    "summary": "The sheer scale of high-resolution raw data generated by simulation has\nmotivated non-conventional approaches for data exploration referred as\n`immersive' and `in situ' query processing of the raw simulation data. Another\nstep towards supporting scientific progress is to enable data-driven hypothesis\nmanagement and predictive analytics out of simulation results. We present a\nsynthesis method and tool for encoding and managing competing hypotheses as\nuncertain data in a probabilistic database that can be conditioned in the\npresence of observations."
},{
    "category": "cs.DB", 
    "doi": "10.1109/MCSE.2015.102", 
    "link": "http://arxiv.org/pdf/1405.7264v2", 
    "other_authors": "Matteo Interlandi, Letizia Tanca", 
    "title": "On the CALM Principle for Bulk Synchronous Parallel Computation", 
    "arxiv-id": "1405.7264v2", 
    "author": "Letizia Tanca", 
    "publish": "2014-05-28T14:47:50Z", 
    "summary": "In the recent years a lot of emphasis has been placed on two apparently\ndisjoined fields: data-parallel and eventually consistent distributed systems.\nIn this paper we propose a theoretical study over an eventually consistent\ndata-parallel computational model. The keystone is provided by the recent\nfinding that a class of programs exists which can be computed in an eventually\nconsistent, coordination-free way: monotonic programs. This principle is called\nCALM and has been proven for distributed asynchronous settings. We make the\ncase that, using the techniques developed by Ameloot et al., CALM does not hold\nin general for data-parallel systems, wherein computation usually proceeds\nsynchronously in rounds and where communication is reliable. We then show that\nusing novel techniques subsuming the one of Ameloot et al., the satisfiability\nof the CALM principle is directly related with the assumptions imposed on the\nbehavior of the system."
},{
    "category": "cs.DB", 
    "doi": "10.1109/MCSE.2015.102", 
    "link": "http://arxiv.org/pdf/1406.0435v1", 
    "other_authors": "Jun-Sung Kim, Kyu-Young Whang, Hyuk-Yoon Kwon, Il-Yeol Song", 
    "title": "Odysseus/DFS: Integration of DBMS and Distributed File System for   Transaction Processing of Big Data", 
    "arxiv-id": "1406.0435v1", 
    "author": "Il-Yeol Song", 
    "publish": "2014-06-02T16:32:05Z", 
    "summary": "The relational DBMS (RDBMS) has been widely used since it supports various\nhigh-level functionalities such as SQL, schemas, indexes, and transactions that\ndo not exist in the O/S file system. But, a recent advent of big data\ntechnology facilitates development of new systems that sacrifice the DBMS\nfunctionality in order to efficiently manage large-scale data. Those so-called\nNoSQL systems use a distributed file system, which support scalability and\nreliability. They support scalability of the system by storing data into a\nlarge number of low-cost commodity hardware and support reliability by storing\nthe data in replica. However, they have a drawback that they do not adequately\nsupport high-level DBMS functionality. In this paper, we propose an\narchitecture of a DBMS that uses the DFS as storage. With this novel\narchitecture, the DBMS is capable of supporting scalability and reliability of\nthe DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a\nvirtually unlimited storage space provided by the DFS, rendering it to be\nsuitable for big data analytics. As part of the architecture of the DBMS, we\npropose the notion of the meta DFS file, which allows the DBMS to use the DFS\nas the storage, and an efficient transaction management method including\nrecovery and concurrency control. We implement this architecture in\nOdysseus/DFS, an integration of the Odysseus relational DBMS, that has been\nbeing developed at KAIST for over 24 years, with the DFS. Our experiments on\ntransaction processing show that, due to the high-level functionality of\nOdysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL\nsystem. We also show that, compared with an RDBMS with local storage, the\nperformance of Odysseus/DFS is comparable or marginally degraded, showing that\nthe overhead of Odysseus/DFS for supporting scalability by using the DFS as the\nstorage is not significant."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.0905v1", 
    "other_authors": "Paolo Missier, Simon Woodman, Hugo Hiden, Paul Watson", 
    "title": "Provenance and data differencing for workflow reproducibility analysis", 
    "arxiv-id": "1406.0905v1", 
    "author": "Paul Watson", 
    "publish": "2014-06-03T23:22:59Z", 
    "summary": "One of the foundations of science is that researchers must publish the\nmethodology used to achieve their results so that others can attempt to\nreproduce them. This has the added benefit of allowing methods to be adopted\nand adapted for other purposes. In the field of e-Science, services -- often\nchoreographed through workflow, process data to generate results. The\nreproduction of results is often not straightforward as the computational\nobjects may not be made available or may have been updated since the results\nwere generated. For example, services are often updated to fix bugs or improve\nalgorithms. This paper addresses these problems in three ways. Firstly, it\nintroduces a new framework to clarify the range of meanings of\n\"reproducibility\". Secondly, it describes a new algorithm, \\PDIFF, that uses a\ncomparison of workflow provenance traces to determine whether an experiment has\nbeen reproduced; the main innovation is that if this is not the case then the\nspecific point(s) of divergence are identified through graph analysis,\nassisting any researcher wishing to understand those differences. One key\nfeature is support for user-defined, semantic data comparison operators.\nFinally, the paper describes an implementation of \\PDIFF that leverages the\npower of the e-Science Central platform which enacts workflows in the cloud. As\nwell as automatically generating a provenance trace for consumption by \\PDIFF,\nthe platform supports the storage and re-use of old versions of workflows, data\nand services; the paper shows how this can be powerfully exploited in order to\nachieve reproduction and re-use."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.1423v1", 
    "other_authors": "Joshua Amavi, Jacques Chabin, Mirian Halfeld Ferrari, Pierre R\u00e9ty", 
    "title": "A ToolBox for Conservative XML Schema Evolution and Document Adaptation", 
    "arxiv-id": "1406.1423v1", 
    "author": "Pierre R\u00e9ty", 
    "publish": "2014-06-05T15:59:42Z", 
    "summary": "This paper proposes a set of tools to help dealing with XML database\nevolution. It aims at establishing a multi-system environment where a global\nintegrated system works in harmony with some local original ones, allowing data\ntranslation in both directions and, thus, activities on both levels. To deal\nwith schemas, we propose an algorithm that computes a mapping capable of\nobtaining a global schema which is a conservative extension of original local\nschemas. The role of the obtained mapping is then twofold: it ensures schema\nevolution, via composition and inversion, and it guides the construction of a\ndocument translator, allowing automatic data adaptation w.r.t. type evolution.\nThis paper applies, extends and put together some of our previous\ncontributions."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.1431v1", 
    "other_authors": "Ali Mansouri, Youssef Amghar", 
    "title": "Int\u00e9gration des r\u00e8gles actives dans des documents", 
    "arxiv-id": "1406.1431v1", 
    "author": "Youssef Amghar", 
    "publish": "2014-06-03T20:08:18Z", 
    "summary": "The management of technical documentation is an unavoidable activity\ninteresting for the enterprises. Indeed, the need to manage documents during\nall the life cycle is an important issue. For that, the need to enhance the\nability of document management systems is an interesting challenge. Despite\nexisting systems on market (electronic document management systems), they are\nconsidered as non-flexible systems which are based on data models preventing\nany extension or improvement. In addition, those systems do not allow a slight\ndescription of documents elements and propose an insufficient mechanisms for\nboth links and consistency management. LIRIS laboratory has developed research\nin this area and proposed an active system, termed SAGED, whose objectives is\nto manage link and consistency using active rules. However SAGED is based on an\napproach that split rules (for consistency management) and documents\ndescription. The main drawback is the rigidity of such approach which is\nhighlighted whenever documents are moved from one server to another or during\nexchanges of documents. To contribute to solve this problem, we propose to\ndevelop an approach aiming at improve the document management including\nconsistency. This approach is based on the introduction of rules with the XML\ndescription of the documents [BoCP01]. In this context we proposed a\nXML-oriented storage level allowing the storing of documents and rules\nuniformly through a native XML database. We defined an intelligent system\ntermed SIGED according a client/server architecture built around an intelligent\ncomponent for active rules execution. These rules are extracted from XML\ndocument, compiled and executed."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.1998v1", 
    "other_authors": "Paolo Missier, Jeremy Bryans, Carl Gamble, Vasa Curcin, Roxana Danger", 
    "title": "ProvAbs: model, policy, and tooling for abstracting PROV graphs", 
    "arxiv-id": "1406.1998v1", 
    "author": "Roxana Danger", 
    "publish": "2014-06-08T16:26:53Z", 
    "summary": "Provenance metadata can be valuable in data sharing settings, where it can be\nused to help data consumers form judgements regarding the reliability of the\ndata produced by third parties. However, some parts of provenance may be\nsensitive, requiring access control, or they may need to be simplified for the\nintended audience. Both these issues can be addressed by a single mechanism for\ncreating abstractions over provenance, coupled with a policy model to drive the\nabstraction. Such mechanism, which we refer to as abstraction by grouping,\nsimultaneously achieves partial disclosure of provenance, and facilitates its\nconsumption. In this paper we introduce a formal foundation for this type of\nabstraction, grounded in the W3C PROV model; describe the associated policy\nmodel; and briefly present its implementation, the Provabs tool for interactive\nexperimentation with policies and abstractions."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.2495v1", 
    "other_authors": "Hugo Firth, Paolo Missier", 
    "title": "ProvGen: generating synthetic PROV graphs with predictable structure", 
    "arxiv-id": "1406.2495v1", 
    "author": "Paolo Missier", 
    "publish": "2014-06-10T10:20:33Z", 
    "summary": "This paper introduces provGen, a generator aimed at producing large synthetic\nprovenance graphs with predictable properties and of arbitrary size. Synthetic\nprovenance graphs serve two main purposes. Firstly, they provide a variety of\ncontrolled workloads that can be used to test storage and query capabilities of\nprovenance management systems at scale. Secondly, they provide challenging\ntestbeds for experimenting with graph algorithms for provenance analytics, an\narea of increasing research interest. provGen produces PROV graphs and stores\nthem in a graph DBMS (Neo4J). A key feature is to let users control the\nrelationship makeup and topological features of the graph, by providing a seed\nprovenance pattern along with a set of constraints, expressed using a custom\nDomain Specific Language. We also propose a simple method for evaluating the\nquality of the generated graphs, by measuring how realistically they simulate\nthe structure of real-world patterns."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.3399v1", 
    "other_authors": "Olaf Hartig, Bryan Thompson", 
    "title": "Foundations of an Alternative Approach to Reification in RDF", 
    "arxiv-id": "1406.3399v1", 
    "author": "Bryan Thompson", 
    "publish": "2014-06-13T01:07:50Z", 
    "summary": "This document defines extensions of the RDF data model and of the SPARQL\nquery language that capture an alternative approach to represent\nstatement-level metadata. While this alternative approach is backwards\ncompatible with RDF reification as defined by the RDF standard, the approach\naims to address usability and data management shortcomings of RDF reification.\nOne of the great advantages of the proposed approach is that it clarifies a\nmeans to (i) understand sparse matrices, the property graph model, hypergraphs,\nand other data structures with an emphasis on link attributes, (ii) map such\ndata onto RDF, and (iii) query such data using SPARQL. Further, the proposal\ngreatly expands both the freedom that database designers enjoy when creating\nphysical indexing schemes and query plans for graph data annotated with link\nattributes and the interoperability of those database solutions."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.5917v1", 
    "other_authors": "Abdelwaheb Ferchichi, Mohamed Salah Gouider", 
    "title": "BSTree: an Incremental Indexing Structure for Similarity Search and Real   Time Monitoring of Data Streams", 
    "arxiv-id": "1406.5917v1", 
    "author": "Mohamed Salah Gouider", 
    "publish": "2014-06-23T14:21:13Z", 
    "summary": "In this work, a new indexing technique of data streams called BSTree is\nproposed. This technique uses the method of data discretization, SAX [4], to\nreduce online the dimensionality of data streams. It draws on Btree to build\nthe index and finally uses an LRV (least Recently visited) pruning technique to\nrid the index structure from data whose last visit time exceeds a threshold\nvalue and thus minimizes response time for similarity search queries."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.6490v1", 
    "other_authors": "Edith Cohen", 
    "title": "Variance Competitiveness for Monotone Estimation: Tightening the Bounds", 
    "arxiv-id": "1406.6490v1", 
    "author": "Edith Cohen", 
    "publish": "2014-06-25T08:18:39Z", 
    "summary": "Random samples are extensively used to summarize massive data sets and\nfacilitate scalable analytics. Coordinated sampling, where samples of different\ndata sets \"share\" the randomization, is a powerful method which facilitates\nmore accurate estimation of many aggregates and similarity measures. We\nrecently formulated a model of {\\it Monotone Estimation Problems} (MEP), which\ncan be applied to coordinated sampling, projected on a single item. MEP\nestimators can then be used to estimate sum aggregates, such as distances, over\ncoordinated samples. For MEP, we are interested in estimators that are unbiased\nand nonnegative. We proposed {\\it variance competitiveness} as a quality\nmeasure of estimators: For each data vector, we consider the minimum variance\nattainable on it by an unbiased and nonnegative estimator. We then define the\ncompetitiveness of an estimator as the maximum ratio, over data, of the\nexpectation of the square to the minimum possible. We also presented a general\nconstruction of the L$^*$ estimator, which is defined for any MEP for which a\nnonnegative unbiased estimator exists, and is at most 4-competitive.\n  Our aim here is to obtain tighter bounds on the {\\em universal ratio}, which\nwe define to be the smallest competitive ratio that can be obtained for any\nMEP. We obtain an upper bound of 3.375, improving over the bound of $4$ of the\nL$^*$ estimator. We also establish a lower bound of 1.44. The lower bound is\nobtained by constructing the {\\it optimally competitive} estimator for\nparticular MEPs. The construction is of independent interest, as it facilitates\nestimation with instance-optimal competitiveness."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.6667v2", 
    "other_authors": "Andrew Crotty, Alex Galakatos, Kayhan Dursun, Tim Kraska, Ugur Cetintemel, Stan Zdonik", 
    "title": "Tupleware: Redefining Modern Analytics", 
    "arxiv-id": "1406.6667v2", 
    "author": "Stan Zdonik", 
    "publish": "2014-06-25T19:06:15Z", 
    "summary": "There is a fundamental discrepancy between the targeted and actual users of\ncurrent analytics frameworks. Most systems are designed for the data and\ninfrastructure of the Googles and Facebooks of the world---petabytes of data\ndistributed across large cloud deployments consisting of thousands of cheap\ncommodity machines. Yet, the vast majority of users operate clusters ranging\nfrom a few to a few dozen nodes, analyze relatively small datasets of up to a\nfew terabytes, and perform primarily compute-intensive operations. Targeting\nthese users fundamentally changes the way we should build analytics systems.\n  This paper describes the design of Tupleware, a new system specifically aimed\nat the challenges faced by the typical user. Tupleware's architecture brings\ntogether ideas from the database, compiler, and programming languages\ncommunities to create a powerful end-to-end solution for data analysis. We\npropose novel techniques that consider the data, computations, and hardware\ntogether to achieve maximum performance on a case-by-case basis. Our\nexperimental evaluation quantifies the impact of our novel techniques and shows\norders of magnitude performance improvement over alternative systems."
},{
    "category": "cs.DB", 
    "doi": "10.1002/cpe.3035", 
    "link": "http://arxiv.org/pdf/1406.6778v1", 
    "other_authors": "Chandrakant Mahobiya, M. Kumar", 
    "title": "Performance Comparison of Two Streaming Data Clustering Algorithms", 
    "arxiv-id": "1406.6778v1", 
    "author": "M. Kumar", 
    "publish": "2014-06-26T06:09:26Z", 
    "summary": "The weighted fuzzy c-mean clustering algorithm and weighted fuzzy\nc-mean-adaptive cluster number are extension of traditional fuzzy c-mean\nAlgorithm to stream data clustering algorithm."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1406.7371v1", 
    "other_authors": "Paresh Tanna, Yogesh Ghodasara", 
    "title": "Using Apriori with WEKA for Frequent Pattern Mining", 
    "arxiv-id": "1406.7371v1", 
    "author": "Yogesh Ghodasara", 
    "publish": "2014-06-28T08:18:57Z", 
    "summary": "Knowledge exploration from the large set of data,generated as a result of the\nvarious data processing activities due to data mining only. Frequent Pattern\nMining is a very important undertaking in data mining. Apriori approach applied\nto generate frequent item set generally espouse candidate generation and\npruning techniques for the satisfaction of the desired objective. This paper\nshows how the different approaches achieve the objective of frequent mining\nalong with the complexities required to perform the job. This paper\ndemonstrates the use of WEKA tool for association rule mining using Apriori\nalgorithm."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1406.7685v1", 
    "other_authors": "Mehwish Aziz, Shabnam Nawaz, Pakeeza Batool", 
    "title": "Efficiency Analysis of Materialized views in DataWarehouse Using   selfmaintenance", 
    "arxiv-id": "1406.7685v1", 
    "author": "Pakeeza Batool", 
    "publish": "2014-06-30T11:56:20Z", 
    "summary": "A data warehouse is a large data repository for the purpose of analysis and\ndecision making in organizations. To improve the query performance and to get\nfast access to the data, data is stored as materialized views (MV) in the data\nwarehouse. When data at source gets updated, the materialized views also need\nto be updated. In this paper, we focus on the problem of maintenance of these\nmaterialized views and address the issue of finding such auxiliary views (AV)\nthat together with the materialized views make the data self-maintainable and\ntake minimal space. We propose an algorithm that uses key and referential\nconstraints which reduces the total number of tuples in auxiliary views and\nuses idea of information sharing between these auxiliary views to further\nreduce number of auxiliary views."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.0528v2", 
    "other_authors": "Xiaocheng Huang, Zhuowei Bao, Susan B. Davidson, Tova Milo, Xiaojie Yuan", 
    "title": "Answering Regular Path Queries on Workflow Provenance", 
    "arxiv-id": "1408.0528v2", 
    "author": "Xiaojie Yuan", 
    "publish": "2014-08-03T19:02:51Z", 
    "summary": "This paper proposes a novel approach for efficiently evaluating regular path\nqueries over provenance graphs of workflows that may include recursion. The\napproach assumes that an execution g of a workflow G is labeled with\nquery-agnostic reachability labels using an existing technique. At query time,\ngiven g, G and a regular path query R, the approach decomposes R into a set of\nsubqueries R1, ..., Rk that are safe for G. For each safe subquery Ri, G is\nrewritten so that, using the reachability labels of nodes in g, whether or not\nthere is a path which matches Ri between two nodes can be decided in constant\ntime. The results of each safe subquery are then composed, possibly with some\nsmall unsafe remainder, to produce an answer to R. The approach results in an\nalgorithm that significantly reduces the number of subqueries k over existing\ntechniques by increasing their size and complexity, and that evaluates each\nsubquery in time bounded by its input and output size. Experimental results\ndemonstrate the benefit of this approach."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.0926v1", 
    "other_authors": "Harry Halpin, James Cheney", 
    "title": "Dynamic Provenance for SPARQL Update", 
    "arxiv-id": "1408.0926v1", 
    "author": "James Cheney", 
    "publish": "2014-08-05T11:17:13Z", 
    "summary": "While the Semantic Web currently can exhibit provenance information by using\nthe W3C PROV standards, there is a \"missing link\" in connecting PROV to storing\nand querying for dynamic changes to RDF graphs using SPARQL. Solving this\nproblem would be required for such clear use-cases as the creation of version\ncontrol systems for RDF. While some provenance models and annotation techniques\nfor storing and querying provenance data originally developed with databases or\nworkflows in mind transfer readily to RDF and SPARQL, these techniques do not\nreadily adapt to describing changes in dynamic RDF datasets over time. In this\npaper we explore how to adapt the dynamic copy-paste provenance model of\nBuneman et al. [2] to RDF datasets that change over time in response to SPARQL\nupdates, how to represent the resulting provenance records themselves as RDF in\na manner compatible with W3C PROV, and how the provenance information can be\ndefined by reinterpreting SPARQL updates. The primary contribution of this\npaper is a semantic framework that enables the semantics of SPARQL Update to be\nused as the basis for a 'cut-and-paste' provenance model in a principled\nmanner."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.2081v1", 
    "other_authors": "Tomasz Gogacz, Jerzy Marcinkowski", 
    "title": "On the BDD/FC Conjecture", 
    "arxiv-id": "1408.2081v1", 
    "author": "Jerzy Marcinkowski", 
    "publish": "2014-08-09T11:09:00Z", 
    "summary": "Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are\ntwo properties of sets of datalog rules and tuple generating dependencies\n(known as Datalog +/- programs), which recently attracted some attention. We\nconjecture that the first of these properties implies the second, and support\nthis conjecture by some evidence proving, among other results, that it holds\ntrue for all theories over binary signature."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.2800v2", 
    "other_authors": "Nikos Bikakis, Chrisa Tsinaraki, Ioannis Stavrakantonakis, Stavros Christodoulakis", 
    "title": "Supporting SPARQL Update Queries in RDF-XML Integration", 
    "arxiv-id": "1408.2800v2", 
    "author": "Stavros Christodoulakis", 
    "publish": "2014-08-12T18:53:23Z", 
    "summary": "The Web of Data encourages organizations and companies to publish their data\naccording to the Linked Data practices and offer SPARQL endpoints. On the other\nhand, the dominant standard for information exchange is XML. The SPARQL2XQuery\nFramework focuses on the automatic translation of SPARQL queries in XQuery\nexpressions in order to access XML data across the Web. In this paper, we\noutline our ongoing work on supporting update queries in the RDF-XML\nintegration scenario."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.3148v1", 
    "other_authors": "Nikos Bikakis, Melina Skourla, George Papastefanatos", 
    "title": "rdf:SynopsViz - A Framework for Hierarchical Linked Data Visual   Exploration and Analysis", 
    "arxiv-id": "1408.3148v1", 
    "author": "George Papastefanatos", 
    "publish": "2014-08-13T21:16:31Z", 
    "summary": "The purpose of data visualization is to offer intuitive ways for information\nperception and manipulation, especially for non-expert users. The Web of Data\nhas realized the availability of a huge amount of datasets. However, the volume\nand heterogeneity of available information make it difficult for humans to\nmanually explore and analyse large datasets. In this paper, we present\nrdf:SynopsViz, a tool for hierarchical charting and visual exploration of\nLinked Open Data (LOD). Hierarchical LOD exploration is based on the creation\nof multiple levels of hierarchically related groups of resources based on the\nvalues of one or more properties. The adopted hierarchical model provides\neffective information abstraction and summarization. Also, it allows efficient\n-on the fly- statistic computations, using aggregations over the hierarchy\nlevels."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.3175v1", 
    "other_authors": "V. Sidda Reddy, Dr. T. V. Rao, Dr. A. Govardhan", 
    "title": "Mining Frequent Itemsets (MFI) over Data Streams: Variable Window Size   (VWS) by Context Variation Analysis (CVA) of the Streaming Transactions", 
    "arxiv-id": "1408.3175v1", 
    "author": "Dr. A. Govardhan", 
    "publish": "2014-08-14T00:43:46Z", 
    "summary": "The challenges with respect to mining frequent items over data streaming\nengaging variable window size and low memory space are addressed in this\nresearch paper. To check the varying point of context change in streaming\ntransaction we have developed a window structure which will be in two levels\nand supports in fixing the window size instantly and controls the\nheterogeneities and assures homogeneities among transactions added to the\nwindow. To minimize the memory utilization, computational cost and improve the\nprocess scalability, this design will allow fixing the coverage or support at\nwindow level. Here in this document, an incremental mining of frequent\nitem-sets from the window and a context variation analysis approach are being\nintroduced. The complete technology that we are presenting in this document is\nnamed as Mining Frequent Item-sets using Variable Window Size fixed by Context\nVariation Analysis (MFI-VWS-CVA). There are clear boundaries among frequent and\ninfrequent item-sets in specific item-sets. In this design we have used window\nsize change to represent the conceptual drift in an information stream. As it\nwere, whenever there is a problem in setting window size effectively the\nitem-set will be infrequent. The experiments that we have executed and\ndocumented proved that the algorithm that we have designed is much efficient\nthan that of existing."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.4793v1", 
    "other_authors": "Luca Matteis", 
    "title": "Restpark: Minimal RESTful API for Retrieving RDF Triples", 
    "arxiv-id": "1408.4793v1", 
    "author": "Luca Matteis", 
    "publish": "2014-08-19T22:57:41Z", 
    "summary": "How do RDF datasets currently get published on the Web? They are either\navailable as large RDF files, which need to be downloaded and processed\nlocally, or they exist behind complex SPARQL endpoints. By providing a RESTful\nAPI that can access triple data, we allow users to query a dataset through a\nsimple interface based on just a couple of HTTP parameters. If RDF resources\nwere published this way we could quickly build applications that depend on\nthese datasets, without having to download and process them locally. This is\nwhat Restpark is: a set of HTTP GET parameters that servers need to handle, and\nrespond with JSON-LD."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.5460v1", 
    "other_authors": "Priyanka Verma, Nishtha Kesswani", 
    "title": "Web Usage mining framework for Data Cleaning and IP address   Identification", 
    "arxiv-id": "1408.5460v1", 
    "author": "Nishtha Kesswani", 
    "publish": "2014-08-23T05:31:35Z", 
    "summary": "The World Wide Web is the most wide known information source that is easily\navailable and searchable. It consists of billions of interconnected documents\nWeb pages are authored by millions of people. Accesses made by various users to\npages are recorded inside web logs. These log files exist in various formats.\nBecause of increase in usage of web, size of web log files is increasing at a\nmuch faster rate. Web mining is application of data mining technique to these\nlog files. It can be of three types Web usage mining, Web structure mining and\nWeb content mining. Web Usage mining is mining of usage patterns of users which\ncan then be used to personalize web sites and create attractive web sites. It\nconsists of three main phases: Preprocessing, Pattern discovery and Pattern\nanalysis. In this paper we focus on Data cleaning and IP Address identification\nstages of preprocessing. Methodology has been proposed for both the stages. At\nthe end conclusion is made about number of users left after IP address\nidentification."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.5894v1", 
    "other_authors": "Georgios Skoumas, Dieter Pfoser, Anastasios Kyrillidis", 
    "title": "Location Estimation Using Crowdsourced Geospatial Narratives", 
    "arxiv-id": "1408.5894v1", 
    "author": "Anastasios Kyrillidis", 
    "publish": "2014-08-25T15:05:54Z", 
    "summary": "The \"crowd\" has become a very important geospatial data provider. Subsumed\nunder the term Volunteered Geographic Information (VGI), non-expert users have\nbeen providing a wealth of quantitative geospatial data online. With spatial\nreasoning being a basic form of human cognition, narratives expressing\ngeospatial experiences, e.g., travel blogs, would provide an even bigger source\nof geospatial data. Textual narratives typically contain qualitative data in\nthe form of objects and spatial relationships. The scope of this work is (i) to\nextract these relationships from user-generated texts, (ii) to quantify them\nand (iii) to reason about object locations based only on this qualitative data.\nWe use information extraction methods to identify toponyms and spatial\nrelationships and to formulate a quantitative approach based on distance and\norientation features to represent the latter. Positional probability\ndistributions for spatial relationships are determined by means of a greedy\nExpectation Maximization-based (EM) algorithm. These estimates are then used to\n\"triangulate\" the positions of unknown object locations. Experiments using a\ntext corpus harvested from travel blog sites establish the considerable\nlocation estimation accuracy of the proposed approach."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.6395v2", 
    "other_authors": "Fariz Darari, Simon Razniewski, Werner Nutt", 
    "title": "Bridging the Semantic Gap between RDF and SPARQL using Completeness   Statements [Extended Version]", 
    "arxiv-id": "1408.6395v2", 
    "author": "Werner Nutt", 
    "publish": "2014-08-27T12:31:17Z", 
    "summary": "RDF data is often treated as incomplete, following the Open-World Assumption.\nOn the other hand, SPARQL, the standard query language over RDF, usually\nfollows the Closed-World Assumption, assuming RDF data to be complete. This\ngives rise to a semantic gap between RDF and SPARQL. In this paper, we address\nhow to close the semantic gap between RDF and SPARQL in terms of certain\nanswers and possible answers using completeness statements."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.6589v1", 
    "other_authors": "Wentao Wu, Xi Wu, Hakan Hac\u0131g\u00fcm\u00fc\u015f, Jeffrey F. Naughton", 
    "title": "Uncertainty Aware Query Execution Time Prediction", 
    "arxiv-id": "1408.6589v1", 
    "author": "Jeffrey F. Naughton", 
    "publish": "2014-08-27T23:21:58Z", 
    "summary": "Predicting query execution time is a fundamental issue underlying many\ndatabase management tasks. Existing predictors rely on information such as\ncardinality estimates and system performance constants that are difficult to\nknow exactly. As a result, accurate prediction still remains elusive for many\nqueries. However, existing predictors provide a single, point estimate of the\ntrue execution time, but fail to characterize the uncertainty in the\nprediction. In this paper, we take a first step towards providing uncertainty\ninformation along with query execution time predictions. We use the query\noptimizer's cost model to represent the query execution time as a function of\nthe selectivities of operators in the query plan as well as the constants that\ndescribe the cost of CPU and I/O operations in the system. By treating these\nquantities as random variables rather than constants, we show that with low\noverhead we can infer the distribution of likely prediction errors. We further\nshow that the estimated prediction errors by our proposed techniques are\nstrongly correlated with the actual prediction errors."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1408.6916v2", 
    "other_authors": "Jiannan Wang, Guoliang Li, Tim Kraska, Michael J. Franklin, Jianhua Feng", 
    "title": "Leveraging Transitive Relations for Crowdsourced Joins", 
    "arxiv-id": "1408.6916v2", 
    "author": "Jianhua Feng", 
    "publish": "2014-08-29T03:36:29Z", 
    "summary": "The development of crowdsourced query processing systems has recently\nattracted a significant attention in the database community. A variety of\ncrowdsourced queries have been investigated. In this paper, we focus on the\ncrowdsourced join query which aims to utilize humans to find all pairs of\nmatching objects from two collections. As a human-only solution is expensive,\nwe adopt a hybrid human-machine approach which first uses machines to generate\na candidate set of matching pairs, and then asks humans to label the pairs in\nthe candidate set as either matching or non-matching. Given the candidate\npairs, existing approaches will publish all pairs for verification to a\ncrowdsourcing platform. However, they neglect the fact that the pairs satisfy\ntransitive relations. As an example, if $o_1$ matches with $o_2$, and $o_2$\nmatches with $o_3$, then we can deduce that $o_1$ matches with $o_3$ without\nneeding to crowdsource $(o_1, o_3)$. To this end, we study how to leverage\ntransitive relations for crowdsourced joins. We propose a hybrid\ntransitive-relations and crowdsourcing labeling framework which aims to\ncrowdsource the minimum number of pairs to label all the candidate pairs. We\nprove the optimal labeling order in an ideal setting and propose a heuristic\nlabeling order in practice. We devise a parallel labeling algorithm to\nefficiently crowdsource the pairs following the order. We evaluate our\napproaches in both simulated environment and a real crowdsourcing platform.\nExperimental results show that our approaches with transitive relations can\nsave much more money and time than existing methods, with a little loss in the\nresult quality."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1409.0651v1", 
    "other_authors": "Koninika Pal, Sebastian Michel", 
    "title": "An LSH Index for Computing Kendall's Tau over Top-k Lists", 
    "arxiv-id": "1409.0651v1", 
    "author": "Sebastian Michel", 
    "publish": "2014-09-02T10:07:27Z", 
    "summary": "We consider the problem of similarity search within a set of top-k lists\nunder the Kendall's Tau distance function. This distance describes how related\ntwo rankings are in terms of concordantly and discordantly ordered items. As\ntop-k lists are usually very short compared to the global domain of possible\nitems to be ranked, creating an inverted index to look up overlapping lists is\npossible but does not capture tight enough the similarity measure. In this\nwork, we investigate locality sensitive hashing schemes for the Kendall's Tau\ndistance and evaluate the proposed methods using two real-world datasets."
},{
    "category": "cs.DB", 
    "doi": "10.14445/22315381/IJETT-V12P223", 
    "link": "http://arxiv.org/pdf/1409.0798v1", 
    "other_authors": "Anant Bhardwaj, Souvik Bhattacherjee, Amit Chavan, Amol Deshpande, Aaron J. Elmore, Samuel Madden, Aditya G. Parameswaran", 
    "title": "DataHub: Collaborative Data Science & Dataset Version Management at   Scale", 
    "arxiv-id": "1409.0798v1", 
    "author": "Aditya G. Parameswaran", 
    "publish": "2014-09-02T17:16:47Z", 
    "summary": "Relational databases have limited support for data collaboration, where teams\ncollaboratively curate and analyze large datasets. Inspired by software version\ncontrol systems like git, we propose (a) a dataset version control system,\ngiving users the ability to create, branch, merge, difference and search large,\ndivergent collections of datasets, and (b) a platform, DataHub, that gives\nusers the ability to perform collaborative data analysis building on this\nversion control system. We outline the challenges in providing dataset version\ncontrol at scale."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.0980v2", 
    "other_authors": "Vilem Vychodil", 
    "title": "Monoidal functional dependencies", 
    "arxiv-id": "1409.0980v2", 
    "author": "Vilem Vychodil", 
    "publish": "2014-09-03T08:06:22Z", 
    "summary": "We present a complete logic for reasoning with functional dependencies (FDs)\nwith semantics defined over classes of commutative integral partially ordered\nmonoids and complete residuated lattices. The dependencies allow us to express\nstronger relationships between attribute values than the ordinary FDs. In our\nsetting, the dependencies not only express that certain values are determined\nby others but also express that similar values of attributes imply similar\nvalues of other attributes. We show complete axiomatization using a system of\nArmstrong-like rules, comment on related computational issues, and the\nrelational vs. propositional semantics of the dependencies."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.1152v1", 
    "other_authors": "Tanay Kumar Saha, Mohammad Al Hasan", 
    "title": "FS^3: A Sampling based method for top-k Frequent Subgraph Mining", 
    "arxiv-id": "1409.1152v1", 
    "author": "Mohammad Al Hasan", 
    "publish": "2014-09-02T18:45:43Z", 
    "summary": "Mining labeled subgraph is a popular research task in data mining because of\nits potential application in many different scientific domains. All the\nexisting methods for this task explicitly or implicitly solve the subgraph\nisomorphism task which is computationally expensive, so they suffer from the\nlack of scalability problem when the graphs in the input database are large. In\nthis work, we propose FS^3, which is a sampling based method. It mines a small\ncollection of subgraphs that are most frequent in the probabilistic sense. FS^3\nperforms a Markov Chain Monte Carlo (MCMC) sampling over the space of a\nfixed-size subgraphs such that the potentially frequent subgraphs are sampled\nmore often. Besides, FS^3 is equipped with an innovative queue manager. It\nstores the sampled subgraph in a finite queue over the course of mining in such\na manner that the top-k positions in the queue contain the most frequent\nsubgraphs. Our experiments on database of large graphs show that FS^3 is\nefficient, and it obtains subgraphs that are the most frequent amongst the\nsubgraphs of a given size."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.1292v1", 
    "other_authors": "Mohan Yang, Bolin Ding, Surajit Chaudhuri, Kaushik Chakrabarti", 
    "title": "Finding Patterns in a Knowledge Base using Keywords to Compose Table   Answers", 
    "arxiv-id": "1409.1292v1", 
    "author": "Kaushik Chakrabarti", 
    "publish": "2014-09-04T00:54:16Z", 
    "summary": "We aim to provide table answers to keyword queries against knowledge bases.\nFor queries referring to multiple entities, like \"Washington cities population\"\nand \"Mel Gibson movies\", it is better to represent each relevant answer as a\ntable which aggregates a set of entities or entity-joins within the same table\nscheme or pattern. In this paper, we study how to find highly relevant patterns\nin a knowledge base for user-given keyword queries to compose table answers. A\nknowledge base can be modeled as a directed graph called knowledge graph, where\nnodes represent entities in the knowledge base and edges represent the\nrelationships among them. Each node/edge is labeled with type and text. A\npattern is an aggregation of subtrees which contain all keywords in the texts\nand have the same structure and types on node/edges. We propose efficient\nalgorithms to find patterns that are relevant to the query for a class of\nscoring functions. We show the hardness of the problem in theory, and propose\npath-based indexes that are affordable in memory. Two query-processing\nalgorithms are proposed: one is fast in practice for small queries (with small\npatterns as answers) by utilizing the indexes; and the other one is better in\ntheory, with running time linear in the sizes of indexes and answers, which can\nhandle large queries better. We also conduct extensive experimental study to\ncompare our approaches with a naive adaption of known techniques."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.1636v2", 
    "other_authors": "Xiufeng Liu", 
    "title": "Two-level Data Staging ETL for Transaction Data", 
    "arxiv-id": "1409.1636v2", 
    "author": "Xiufeng Liu", 
    "publish": "2014-09-05T01:11:03Z", 
    "summary": "In data warehousing, Extract-Transform-Load (ETL) extracts the data from data\nsources into a central data warehouse regularly for the support of business\ndecision-makings. The data from transaction processing systems are featured\nwith the high frequent changes of insertion, update, and deletion. It is\nchallenging for ETL to propagate the changes to the data warehouse, and\nmaintain the change history. Moreover, ETL jobs typically run in a sequential\norder when processing the data with dependencies, which is not optimal, \\eg,\nwhen processing early-arriving data. In this paper, we propose a two-level data\nstaging ETL for handling transaction data. The proposed method detects the\nchanges of the data from transactional processing systems, identifies the\ncorresponding operation codes for the changes, and uses two staging databases\nto facilitate the data processing in an ETL process. The proposed ETL provides\nthe \"one-stop\" method for fast-changing, slowly-changing and early-arriving\ndata processing."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.1639v1", 
    "other_authors": "Xiufeng Liu", 
    "title": "Optimizing ETL Dataflow Using Shared Caching and Parallelization Methods", 
    "arxiv-id": "1409.1639v1", 
    "author": "Xiufeng Liu", 
    "publish": "2014-09-05T01:24:34Z", 
    "summary": "Extract-Transform-Load (ETL) handles large amount of data and manages\nworkload through dataflows. ETL dataflows are widely regarded as complex and\nexpensive operations in terms of time and system resources. In order to\nminimize the time and the resources required by ETL dataflows, this paper\npresents a framework to optimize dataflows using shared cache and\nparallelization techniques. The framework classifies the components in an ETL\ndataflow into different categories based on their data operation properties.\nThe framework then partitions the dataflow based on the classification at\ndifferent granularities. Furthermore, the framework applies optimization\ntechniques such as cache re-using, pipelining and multi-threading to the\nalready-partitioned dataflows. The proposed techniques reduce system memory\nfootprint and the frequency of copying data between different components, and\nalso take full advantage of the computing power of multi-core processors. The\nexperimental results show that the proposed optimization framework is 4.7 times\nfaster than the ordinary ETL dataflows (without using the proposed optimization\ntechniques), and outperforms the similar tool (Kettle)."
},{
    "category": "cs.DB", 
    "doi": "10.1016/j.jcss.2015.03.006", 
    "link": "http://arxiv.org/pdf/1409.2553v1", 
    "other_authors": "Yodsawalai Chodpathumwan, Jose Picado, Arash Termehchy, Alan Fern, Yizhou Sun", 
    "title": "Representation Independent Analytics Over Structured Data", 
    "arxiv-id": "1409.2553v1", 
    "author": "Yizhou Sun", 
    "publish": "2014-09-09T00:01:23Z", 
    "summary": "Database analytics algorithms leverage quantifiable structural properties of\nthe data to predict interesting concepts and relationships. The same\ninformation, however, can be represented using many different structures and\nthe structural properties observed over particular representations do not\nnecessarily hold for alternative structures. Thus, there is no guarantee that\ncurrent database analytics algorithms will still provide the correct insights,\nno matter what structures are chosen to organize the database. Because these\nalgorithms tend to be highly effective over some choices of structure, such as\nthat of the databases used to validate them, but not so effective with others,\ndatabase analytics has largely remained the province of experts who can find\nthe desired forms for these algorithms. We argue that in order to make database\nanalytics usable, we should use or develop algorithms that are effective over a\nwide range of choices of structural organizations. We introduce the notion of\nrepresentation independence, study its fundamental properties for a wide range\nof data analytics algorithms, and empirically analyze the amount of\nrepresentation independence of some popular database analytics algorithms. Our\nresults indicate that most algorithms are not generally representation\nindependent and find the characteristics of more representation independent\nheuristics under certain representational shifts."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.2585v1", 
    "other_authors": "Georgios Skoumas, Klaus Arthur Schmid, Gregor Joss\u00e9, Andreas Z\u00fcfle, Mario A. Nascimento, Matthias Renz, Dieter Pfoser", 
    "title": "Towards Knowledge-Enriched Path Computation", 
    "arxiv-id": "1409.2585v1", 
    "author": "Dieter Pfoser", 
    "publish": "2014-09-09T09:51:01Z", 
    "summary": "Directions and paths, as commonly provided by navigation systems, are usually\nderived considering absolute metrics, e.g., finding the shortest path within an\nunderlying road network. With the aid of crowdsourced geospatial data we aim at\nobtaining paths that do not only minimize distance but also lead through more\npopular areas using knowledge generated by users. We extract spatial relations\nsuch as \"nearby\" or \"next to\" from travel blogs, that define closeness between\npairs of points of interest (PoIs) and quantify each of these relations using a\nprobabilistic model. Subsequently, we create a relationship graph where each\nnode corresponds to a PoI and each edge describes the spatial connection\nbetween the respective PoIs. Using Bayesian inference we obtain a probabilistic\nmeasure of spatial closeness according to the crowd. Applying this measure to\nthe corresponding road network, we obtain an altered cost function which does\nnot exclusively rely on distance, and enriches an actual road networks taking\ncrowdsourced spatial relations into account. Finally, we propose two routing\nalgorithms on the enriched road networks. To evaluate our approach, we use\nFlickr photo data as a ground truth for popularity. Our experimental results --\nbased on real world datasets -- show that the paths computed w.r.t.\\ our\nalternative cost function yield competitive solutions in terms of path length\nwhile also providing more \"popular\" paths, making routing easier and more\ninformative for the user."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.2819v2", 
    "other_authors": "Bagher Saberi, Nasser Ghadiri", 
    "title": "A Sample-Based Approach to Data Quality Assessment in Spatial Databases   with Application to Mobile Trajectory Nearest-Neighbor Search", 
    "arxiv-id": "1409.2819v2", 
    "author": "Nasser Ghadiri", 
    "publish": "2014-09-09T17:10:11Z", 
    "summary": "Spatial data is playing an emerging role in new technologies such as web and\nmobile mapping and Geographic Information Systems (GIS). Important decisions in\npolitical, social and many other aspects of modern human life are being made\nusing location data. Decision makers in many countries are exploiting spatial\ndatabases for collecting information, analyzing them and planning for the\nfuture. In fact, not every spatial database is suitable for this type of\napplication. Inaccuracy, imprecision and other deficiencies are present in\nlocation data just as any other type of data and may have a negative impact on\ncredibility of any action taken based on unrefined information. So we need a\nmethod for evaluating the quality of spatial data and separating usable data\nfrom misleading data which leads to weak decisions. On the other hand, spatial\ndatabases are usually huge in size and therefore working with this type of data\nhas a negative impact on efficiency. To improve the efficiency of working with\nspatial big data, we need a method for shrinking the volume of data. Sampling\nis one of these methods, but its negative effects on the quality of data are\ninevitable. In this paper we are trying to show and assess this change in\nquality of spatial data that is a consequence of sampling. We used this\napproach for evaluating the quality of sampled spatial data related to mobile\nuser trajectories in China which are available in a well-known spatial\ndatabase. The results show that sample-based control of data quality will\nincrease the query performance significantly, without losing too much accuracy.\nBased on this results some future improvements are pointed out which will help\nto process location-based queries faster than before and to make more accurate\nlocation-based decisions in limited times."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.3288v2", 
    "other_authors": "Olaf Hartig", 
    "title": "Reconciliation of RDF* and Property Graphs", 
    "arxiv-id": "1409.3288v2", 
    "author": "Olaf Hartig", 
    "publish": "2014-09-11T01:15:06Z", 
    "summary": "Both the notion of Property Graphs (PG) and the Resource Description\nFramework (RDF) are commonly used models for representing graph-shaped data.\nWhile there exist some system-specific solutions to convert data from one model\nto the other, these solutions are not entirely compatible with one another and\nnone of them appears to be based on a formal foundation. In fact, for the PG\nmodel, there does not even exist a commonly agreed-upon formal definition.\n  The aim of this document is to reconcile both models formally. To this end,\nthe document proposes a formalization of the PG model and introduces\nwell-defined transformations between PGs and RDF. As a result, the document\nprovides a basis for the following two innovations: On one hand, by\nimplementing the RDF-to-PG transformations defined in this document, PG-based\nsystems can enable their users to load RDF data and make it accessible in a\ncompatible, system-independent manner using, e.g., the graph traversal language\nGremlin or the declarative graph query language Cypher. On the other hand, the\nPG-to-RDF transformation in this document enables RDF data management systems\nto support compatible, system-independent queries over the content of Property\nGraphs by using the standard RDF query language SPARQL. Additionally, this\ndocument represents a foundation for systematic research on relationships\nbetween the two models and between their query languages."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.3530v1", 
    "other_authors": "Alexandr Savinov", 
    "title": "Concept-oriented model: inference in hierarchical multidimensional space", 
    "arxiv-id": "1409.3530v1", 
    "author": "Alexandr Savinov", 
    "publish": "2014-09-11T18:29:32Z", 
    "summary": "In spite of its fundamental importance, inference has not been an inherent\nfunction of multidimensional models and analytical applications. These models\nare mainly aimed at numeric (quantitative) analysis where the notions of\ninference and semantics are not well defined. In this paper we argue that\ninference can be and should be integral part of multidimensional data models\nand analytical applications. It is demonstrated how inference can be defined\nusing only multidimensional terms like axes and coordinates as opposed to using\nlogic-based approaches. We propose a novel approach to inference in\nmultidimensional space based on the concept-oriented model of data and\nintroduce elementary operations which are then used to define constraint\npropagation and inference procedures. We describe a query language with\ninference operator and demonstrate its usefulness in solving complex analytical\ntasks."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.3682v1", 
    "other_authors": "Caetano Sauer, Theo H\u00e4rder", 
    "title": "A novel recovery mechanism enabling fine-granularity locking and fast,   REDO-only recovery", 
    "arxiv-id": "1409.3682v1", 
    "author": "Theo H\u00e4rder", 
    "publish": "2014-09-12T08:57:24Z", 
    "summary": "We present a series of novel techniques and algorithms for transaction\ncommit, logging, recovery, and propagation control. In combination, they\nprovide a recovery component that maintains the persistent state of the\ndatabase (both log and data pages) always in a committed state. Recovery from\nsystem and media failures only requires only REDO operations, which can happen\nconcurrently with the processing of new transactions. The mechanism supports\nfine-granularity locking, partial rollbacks, and snapshot isolation for reader\ntransactions. Our design does not assume a specific hardware configuration such\nas non-volatile RAM or flash---it is designed for traditional disk\nenvironments. Nevertheless, it can exploit modern I/O devices for higher\ntransaction throughput and reduced recovery time with a high degree of\nflexibility."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.3809v2", 
    "other_authors": "Daniel Crankshaw, Peter Bailis, Joseph E. Gonzalez, Haoyuan Li, Zhao Zhang, Michael J. Franklin, Ali Ghodsi, Michael I. Jordan", 
    "title": "The Missing Piece in Complex Analytics: Low Latency, Scalable Model   Management and Serving with Velox", 
    "arxiv-id": "1409.3809v2", 
    "author": "Michael I. Jordan", 
    "publish": "2014-09-12T18:12:24Z", 
    "summary": "To support complex data-intensive applications such as personalized\nrecommendations, targeted advertising, and intelligent services, the data\nmanagement community has focused heavily on the design of systems to support\ntraining complex models on large datasets. Unfortunately, the design of these\nsystems largely ignores a critical component of the overall analytics process:\nthe deployment and serving of models at scale. In this work, we present Velox,\na new component of the Berkeley Data Analytics Stack. Velox is a data\nmanagement system for facilitating the next steps in real-world, large-scale\nanalytics pipelines: online model management, maintenance, and serving. Velox\nprovides end-user applications and services with a low-latency, intuitive\ninterface to models, transforming the raw statistical models currently trained\nusing existing offline large-scale compute frameworks into full-blown,\nend-to-end data products capable of recommending products, targeting\nadvertisements, and personalizing web content. To provide up-to-date results\nfor these complex models, Velox also facilitates lightweight online model\nmaintenance and selection (i.e., dynamic weighting). In this paper, we describe\nthe challenges and architectural considerations required to achieve this\nfunctionality, including the abilities to span online and offline systems, to\nadaptively adjust model materialization strategies, and to exploit inherent\nstatistical properties such as model error tolerance, all while operating at\n\"Big Data\" scale."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.4507v1", 
    "other_authors": "Awny Sayed, Amal Almaqrashi", 
    "title": "Scalable and Efficient Self-Join Processing technique in RDF data", 
    "arxiv-id": "1409.4507v1", 
    "author": "Amal Almaqrashi", 
    "publish": "2014-09-16T05:21:06Z", 
    "summary": "Efficient management of RDF data plays an important role in successfully\nunderstanding and fast querying data. Although the current approaches of\nindexing in RDF Triples such as property tables and vertically partitioned\nsolved many issues; however, they still suffer from the performance in the\ncomplex self-join queries and insert data in the same table. As an improvement\nin this paper, we propose an alternative solution to facilitate flexibility and\nefficiency in that queries and try to reach to the optimal solution to decrease\nthe self-joins as much as possible, this solution based on the idea of\n\"Recursive Mapping of Twin Tables\". Our main goal of Recursive Mapping of Twin\nTables (RMTT) approach is divided the main RDF Triple into two tables which\nhave the same structure of RDF Triple and insert the RDF data recursively. Our\nexperimental results compared the performance of join queries in vertically\npartitioned approach and the RMTT approach using very large RDF data, like DBLP\nand DBpedia datasets. Our experimental results with a number of complex\nsubmitted queries shows that our approach is highly scalable compared with\nRDF-3X approach and RMTT reduces the number of self-joins especially in complex\nqueries 3-4 times than RDF-3X approach"
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.6288v1", 
    "other_authors": "Mengmeng Liu, Zachary G. Ives, Boon Thau Loo", 
    "title": "Enabling Incremental Query Re-Optimization", 
    "arxiv-id": "1409.6288v1", 
    "author": "Boon Thau Loo", 
    "publish": "2014-09-22T19:38:31Z", 
    "summary": "As declarative query processing techniques expand in scope --- to the Web,\ndata streams, network routers, and cloud platforms --- there is an increasing\nneed for adaptive query processing techniques that can re-plan in the presence\nof failures or unanticipated performance changes. A status update on the data\ndistributions or the compute nodes may have significant repercussions on the\nchoice of which query plan should be running. Ideally, new system architectures\nwould be able to make cost-based decisions about reallocating work, migrating\ndata, etc., and react quickly as real-time status information becomes\navailable. Existing cost-based query optimizers are not incremental in nature,\nand must be run \"from scratch\" upon each status or cost update. Hence, they\ngenerally result in adaptive schemes that can only react slowly to updates.\n  An open question has been whether it is possible to build a cost-based\nre-optimization architecture for adaptive query processing in a streaming or\nrepeated query execution environment, e.g., by incrementally updating optimizer\nstate given new cost information. We show that this can be achieved\nbeneficially, especially for stream processing workloads. Our techniques build\nupon the recently proposed approach of formulating query plan enumeration as a\nset of recursive datalog queries; we develop a variety of novel optimization\napproaches to ensure effective pruning in both static and incremental cases. We\nimplement our solution within an existing research query processing system, and\nshow that it effectively supports cost-based initial optimization as well as\nfrequent adaptivity."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.6428v1", 
    "other_authors": "Dalia Attia Waguih, Laure Berti-Equille", 
    "title": "Truth Discovery Algorithms: An Experimental Evaluation", 
    "arxiv-id": "1409.6428v1", 
    "author": "Laure Berti-Equille", 
    "publish": "2014-09-23T07:11:31Z", 
    "summary": "A fundamental problem in data fusion is to determine the veracity of\nmulti-source data in order to resolve conflicts. While previous work in truth\ndiscovery has proved to be useful in practice for specific settings, sources'\nbehavior or data set characteristics, there has been limited systematic\ncomparison of the competing methods in terms of efficiency, usability, and\nrepeatability. We remedy this deficit by providing a comprehensive review of 12\nstate-of-the art algorithms for truth discovery. We provide reference\nimplementations and an in-depth evaluation of the methods based on extensive\nexperiments on synthetic and real-world data. We analyze aspects of the problem\nthat have not been explicitly studied before, such as the impact of\ninitialization and parameter setting, convergence, and scalability. We provide\nan experimental framework for extensively comparing the methods in a wide range\nof truth discovery scenarios where source coverage, numbers and distributions\nof conflicts, and true positive claims can be controlled and used to evaluate\nthe quality and performance of the algorithms. Finally, we report comprehensive\nfindings obtained from the experiments and provide new insights for future\nresearch."
},{
    "category": "cs.DB", 
    "doi": "10.1145/2666310.2666485", 
    "link": "http://arxiv.org/pdf/1409.6548v1", 
    "other_authors": "Eshref Januzaj, Hans-Peter Kriegel, Martin Pfeifle", 
    "title": "Scalable Density-Based Distributed Clustering", 
    "arxiv-id": "1409.6548v1", 
    "author": "Martin Pfeifle", 
    "publish": "2014-09-22T17:23:36Z", 
    "summary": "Clustering has become an increasingly important task in analysing huge\namounts of data. Traditional applications require that all data has to be\nlocated at the site where it is scrutinized. Nowadays, large amounts of\nheterogeneous, complex data reside on different, independently working\ncomputers which are connected to each other via local or wide area networks. In\nthis paper, we propose a scalable density-based distributed clustering\nalgorithm which allows a user-defined trade-off between clustering quality and\nthe number of transmitted objects from the different local sites to a global\nserver site. Our approach consists of the following steps: First, we order all\nobjects located at a local site according to a quality criterion reflecting\ntheir suitability to serve as local representatives. Then we send the best of\nthese representatives to a server site where they are clustered with a slightly\nenhanced density-based clustering algorithm. This approach is very efficient,\nbecause the local detemination of suitable representatives can be carried out\nquickly and independently from each other. Furthermore, based on the scalable\nnumber of the most suitable local representatives, the global clustering can be\ndone very effectively and efficiently. In our experimental evaluation, we will\nshow that our new scalable density-based distributed clustering approach\nresults in high quality clusterings with scalable transmission cost."
}]